{
  "title": "Создание инференса для использования в Managed RAG",
  "chapters": [
    {
      "name": "Создание инференса для использования в Managed RAG",
      "content": "Практические руководства Evolution    \n\n # Создание инференса для использования в Managed RAG   Эта статья полезна?          \nС помощью этого руководства вы последовательно создадите три типа инференса в ML Inference для использования их в базе знаний Managed RAG, затем проверите работоспособность базы знаний.\nВы будете использовать следующие сервисы:\n- Evolution Managed RAG — сервис для создания и управления базами знаний, используемыми при генерации ответов языковыми моделями.\n- Evolution Object Storage — объектное хранилище для размещения документов, из которых будет формироваться база знаний.\n- Evolution ML Inference — сервис для запуска ML-моделей в облаке.\n- Huggingface — платформа для публикации и использования моделей машинного обучения.\nШаги:\n1. Создайте бакет и загрузите файл.\n2. Получите токен Huggingface.\n3. Создайте инференс для модели-эмбеддера.\n4. Создайте инференс для модели-реранкера.\n5. Создайте инференс для LLM.\n6. Создайте базу знаний.\n7. Проверьте работу базу знаний.\n\n## Перед началом работы\n1. Зарегистрируйтесь в личном кабинете Cloud.ru.\nЕсли вы уже зарегистрированы, войдите под своей учетной записью.\n2. Убедитесь, что в личном кабинете Cloud.ru подключены сервисы Managed RAG, ML Inference, Object Storage.\n3. Скачайте текстовый файл faq_products.txt.\n\n## 1. Создайте бакет и загрузите файл\n1. Создайте бакет в Object Storage:\n\n1. Укажите название бакета, например rag-inference-bucket.\nОстальные параметры оставьте по умолчанию.\n2. Нажмите Создать.\n2. Создайте папку в бакете со следующими параметрами:\n\n1. Перейдите в бакет rag-inference-bucket.\n2. Нажмите Создать папку.\n3. Укажите название rag-inference-kb/ и нажмите Создать.\n3. Загрузите папку текстовый файл faq_products.txt.\n\n## 2. Получите токен Huggingface\n1. Войдите или зарегистрируйтесь на https://huggingface.co.\n2. Перейдите в раздел Access Tokens.\n3. Нажмите Create new token.\n4. Выберите тип Write.\n5. Введите название токена, например rag_with_mlinference.\n6. Нажмите Create token.\n7. Скопируйте токен и сохраните его, например в блокнот.\nПосле закрытия страницы он будет недоступен.\n\n## 3. Создайте инференс для модели-эмбеддера\nИнференс создаетcя на примере модели с Huggingface Qwen/Qwen3-Embedding-0.6B.\n1. Перейдите в личный кабинет Cloud.ru, AI Factory → ML Inference.\n2. На вкладке Model RUN нажмите Создать.\n3. Укажите название embedder-for-rag.\n4. Выберите для Runtime значение vLLM.\n5. Добавьте модель.\n1. Нажмите Добавить из Hugging Face.\n2. В поле Репозиторий с моделью Hugging Face вставьте скопированное название модели Qwen/Qwen3-Embedding-0.6B.\n3. Нажмите Добавить токен в Secret Management, если токен еще не добавлен.\n4. Укажите путь, например rag_with_mlinferece.\n5. Введите описание, например Huggingface access token.\n6. В поле Значение секрета выберите Стандартный режим и вставьте токен Huggingface, полученный на шаге 2.\n7. Нажмите Создать.\nТокен сохранен в Secret Management.\nВернитесь к созданию инференса.\n6. В поле Токен доступа в Hugging Face выберите созданный токен rag_with_mlinferece → версия 1.\n7. Нажмите Добавить.\nДождитесь расчета ресурсов.\n8. В поле Задача ML модели выберите Embedding — отличительная черта инференса такого типа.\n9. Остальные параметры оставьте по умолчанию и нажмите Продолжить.\n10. Включите опцию Не выключать модель.\n11. (Опционально) Настройте масштабирование.\n12. (Опционально) В настройке Аутентификация выберите сервисный аккаунт.\n13. (Опционально) В настройке Логирование укажите лог‑группу.\n14. Нажмите Создать.\nДождитесь, когда инференс перейдет в статус «Запущен».\n15. Перейдите на вкладку Информация и скопируйте идентификатор инференса — часть публичного URL между https:// и .modelrun.\nНапример, в публичном URL https://12345c60-xxx-4527-xxxx-f789f789fb11.modelrun.inference.cloud.ru нужный идентификатор — 12345c60-xxx-4527-xxxx-f789f789fb11.\n\n## 4. Создайте инференс для модели-реранкера\nИнференс создаетcя на примере модели с Huggingface Qwen/Qwen3-Reranker-0.6B.\n1. Перейдите в личный кабинет Cloud.ru, AI Factory → ML Inference.\n2. На вкладке Model RUN нажмите Создать.\n3. Укажите название reranker-for-rag.\n4. Выберите для Runtime значение vLLM.\n5. Добавьте модель.\n1. Нажмите Добавить из Hugging Face.\n2. В поле Репозиторий с моделью Hugging Face вставьте скопированное название модели Qwen/Qwen3-Reranker-0.6B.\n3. Нажмите Добавить токен в Secret management, если токен еще не добавлен.\n4. Укажите путь, например rag_with_mlinferece.\n5. Введите описание.\n6. В поле Значение секрета выберите Стандартный режим и вставьте токен Huggingface, полученный на шаге 2.\n7. Нажмите Создать.\nТокен сохранен в Secret Management.\nВернитесь к созданию инференса.\n6. В поле Токен доступа в Hugging Face выберите созданный токен rag_with_mlinferece → версия 1.\n7. Нажмите Добавить.\nДождитесь расчета ресурсов.\n8. В поле Задача ML модели выберите Score — отличительная черта инференса такого типа.\n9. Остальные параметры оставьте по умолчанию и нажмите Продолжить.\n10. Включите опцию Не выключать модель.\n11. (Опционально) Настройте масштабирование.\n12. (Опционально) В настройке Аутентификация выберите сервисный аккаунт.\n13. (Опционально) В настройке Логирование укажите лог‑группу.\n14. Нажмите Создать.\nДождитесь, когда инференс перейдет в статус «Запущен».\n15. Перейдите на вкладку Информация и скопируйте идентификатор инференса — часть публичного URL между https:// и .modelrun.\nНапример, в публичном URL https://12345c60-xxx-4527-xxxx-f789f789fb11.modelrun.inference.cloud.ru нужный идентификатор — 12345c60-xxx-4527-xxxx-f789f789fb11.\n\n## 5. Создайте инференс для LLM\nИнференс создаетcя на примере модели с Huggingface t-tech/T-lite-it-1.0.\n1. Перейдите в личный кабинет Cloud.ru, AI Factory → ML Inference.\n2. На вкладке Model RUN нажмите Создать.\n3. Укажите название llm-for-rag.\n4. Выберите для Runtime значение vLLM.\n5. Добавьте модель.\n1. Нажмите Добавить из Hugging Face.\n2. В поле Репозиторий с моделью Hugging Face вставьте скопированное название модели t-tech/T-lite-it-1.0.\n3. Нажмите Добавить токен в Secret Management, если токен еще не добавлен.\n4. Укажите путь, например rag_with_mlinferece.\n5. Введите описание.\n6. В поле Значение секрета выберите Стандартный режим и вставьте токен Huggingface, полученный на шаге 2.\n7. Нажмите Создать.\nТокен сохранен в Secret Management.\nВернитесь к созданию инференса.\n6. В поле Токен доступа в Hugging Face выберите созданный токен rag_with_mlinferece → версия 1.\n7. Нажмите Добавить.\nДождитесь расчета ресурсов.\n8. В поле Задача ML модели выберите Generate — отличительная черта инференса такого типа.\n9. Остальные параметры оставьте по умолчанию и нажмите Продолжить.\n10. Включите опцию Не выключать модель.\n11. (Опционально) Настройте масштабирование.\n12. (Опционально) В настройке Аутентификация выберите сервисный аккаунт.\n13. (Опционально) В настройке Логирование укажите лог‑группу.\n14. Нажмите Создать.\nДождитесь, когда инференс перейдет в статус «Запущен».\n15. Перейдите на вкладку Информация и скопируйте идентификатор инференса — часть публичного URL между https:// и .modelrun.\nНапример, в публичном URL https://12345c60-xxx-4527-xxxx-f789f789fb11.modelrun.inference.cloud.ru нужный идентификатор — 12345c60-xxx-4527-xxxx-f789f789fb11.\n\n## 6. Создайте базу знаний с использованием инференса\nНа этом шаге вы создадите базу знаний на основе загруженных документов и проиндексируете ее для использования с языковыми моделями.\n1. В личном кабинете перейдите в AI Factory → Managed RAG.\n2. Нажмите Создать базу знаний.\n3. В поле Название укажите имя базы знаний, например kb-rag-with-inference.\n4. При необходимости введите описание.\n5. В поле Путь к папке в бакете выберите папку rag-inference-kb, в бакете Object Storage, куда вы загрузили файл faq_products.txt.\n6. В поле Расширение файлов введите txt и выберите его.\n7. Включите опцию Вручную настроить обработку документов и модель.\n8. (Опционально) В настройке Аутентификация выберите сервисный аккаунт.\n9. (Опционально) В настройке Логирование укажите лог‑группу.\n10. Нажмите Продолжить.\n11. Пропустите настройку экстрактора и нажмите Продолжить.\n12. Выберите источник модели ML Inference.\n13. В списке выберите созданный инференс embedder-for-rag.\n14. Нажмите Создать.\nДождитесь завершения индексации базы знаний и ее версии — это займет несколько минут.\n15. Перейдите в созданную версию базы знаний.\n16. Скопируйте значения полей ID версии и ID базы знаний.\n\n## 7. Проверьте работу базы знаний\nВы можете дополнительно проверить работу с базой знаний с помощью личного кабинета или API.\nРекомендуется использовать оба способа.\nЛичный кабинетAPI1. Перейдите в созданную версию базы знаний.\n2. Перейдите на вкладку Чат.\n3. Включите опцию Использовать модель-реранкер.\n4. В качестве источника модели‑реранкера выберите ML Inference.\n5. Выберите созданный инференс reranker-for-rag.\n6. В качестве Модель‑LLM выберите ML Inference и из списка выберите инференс llm-for-rag.\n7. Отправьте сообщение в чате и получите ответ.\n\n## Что дальше\nС этим руководством вы создали базу знаний на основе нескольких инференсов моделей.\nТеперь можно отправлять запросы к инференсу.\nУзнавайте больше о прикладных сценариях и примерах решения бизнес-задач, получайте навыки управления облаком, выполняя практические руководства.\n\n  [© 2025 Cloud.ru](https://cloud.ru)",
      "debug": {
        "start_page": 1,
        "end_page": 1
      }
    }
  ]
}
{
  "title": "Настройка Time-Slicing GPU",
  "chapters": [
    {
      "name": "Настройка Time-Slicing GPU",
      "content": "Практические руководства Evolution    \n\n # Настройка Time-Slicing GPU   Эта статья полезна?          \nNVIDIA GPU Operator поддерживает возможность настройки Time-Slicing — механизма виртуального разделения одной физической GPU между несколькими подами на уровне рабочего узла.\nНапример, если на узле установлена одна GPU V100, а в кластере есть пять подов, каждый из которых запрашивает всю GPU, то без использования Time-Slicing на узел будет назначен только один под.\nОстальные останутся в статусе «Pending» из-за нехватки ресурсов.\nПри включении Time-Slicing ресурсы одной физической GPU делятся между пятью подами.\nТаким образом, все пять подов смогут быть запущены на одном узле одновременно, несмотря на то, что физически доступна только одна GPU.\nВ сценарии настроим Time-Slicing, развернем пять реплик приложения, которое требует для своей работы GPU-ресурсов, проверим состояние подов и логи.\n\n## Перед началом работы\n1. Зарегистрируйтесь в личном кабинете Cloud.ru.\nЕсли вы уже зарегистрированы, войдите под своей учетной записью.\n2. Создайте кластер Managed Kubernetes.\n3. В кластере создайте группу узлов с параметрами для GPU:\n1. Графический процессор (GPU) — активно.\n2. Модель GPU — GPU NVIDIA Tesla V100.\n3. GPU — 1.\nПо умолчанию в Managed Kubernetes установлена нулевая квота на создание узлов с GPU. Чтобы запросить увеличение квоты, обратитесь в техническую поддержку.\n4. Подключитесь к кластеру Managed Kubernetes.\n\n## Шаг 1. Настройте Time-Slicing\n1. Создайте пространство имен gpu-operator:\n```\nkubectl create ns gpu-operator\n```\n2. Перезапишите метку:\n```\nkubectl label --overwrite ns gpu-operator pod-security.kubernetes.io/enforce=privileged\n```\n3. Создайте файл cloudru-time-slicing.yaml со следующим содержимым:\n```\napiVersion: v1kind: ConfigMapmetadata:  name: time-slicing-config  namespace: gpu-operatordata:  tesla-v100: |-    version: v1    sharing:      timeSlicing:        resources:        - name: nvidia.com/gpu          replicas: 5\n```\n4. Выполните команду:\n```\nkubectl apply -f cloudru-time-slicing.yaml\n```\n\nРезультат:\n```\nconfigmap/time-slicing-config created\n```\n5. Проверьте статус:\n```\nkubectl get cm time-slicing-config -n gpu-operator\n```\n\nРезультат:\n```\nNAME                  DATA   AGEtime-slicing-config   1      114s\n```\nВ дополнение к стандартным меткам, которые применяются к узлам после настройки Time-Slicing, для узла применяется метка:\n```\nnvidia.com/gpu.replicas = <replicas-count>\n```\n\nЗдесь <replicas-count> указывает, сколько раз выделенный ресурс gpu может быть переподписан на узле.\nТакже по умолчанию модифицируется метка nvidia.com/gpu.product:\n```\nnvidia.com/gpu.product = <product-name>-SHARED\n```\n\nСуффикс -SHARED помогает отличать узлы с поддержкой Time-Slicing.\n\n## Шаг 2. Установите NVIDIA GPU Operator\nЛичный кабинетAPI1. В личном кабинете перейдите в кластер, для которого создали группу узлов с GPU.\n2. Перейдите в раздел Плагины и справа над списком установленных плагинов нажмите Добавить плагин.\n3. Выберите NVIDIA GPU Operator.\n4. Нажмите Установить.\n5. В разделе Расширенная конфигурация → YAML укажите параметры:\n```\ndevicePlugin:  config:    name: time-slicing-config    default: tesla-v100\n```\n6. Нажмите Установить.\n\nДождитесь, когда состояние плагина изменится на «Установлен».\n\n## Шаг 3. Протестируйте настройку Time-Slicing\n1. Создайте файл cloudru-time-slicing-check.yaml со следующим содержимым:\n```\napiVersion: apps/v1kind: Deploymentmetadata:  name: cloudru-time-slicing-check  labels:    app: cloudru-time-slicing-checkspec:  replicas: 5  selector:    matchLabels:      app: cloudru-time-slicing-check  template:    metadata:      labels:        app: cloudru-time-slicing-check    spec:      tolerations:        - key: nvidia.com/gpu          operator: Exists          effect: NoSchedule      hostPID: true      containers:        - name: cuda-sample-vector-add          image: \"nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04\"          command: [\"/bin/bash\", \"-c\", \"--\"]          args:            - while true; do /cuda-samples/vectorAdd; done          resources:           limits:             nvidia.com/gpu: 1\n```\n2. Выполните команду:\n```\nkubectl apply -f cloudru-time-slicing-check.yaml\n```\n\nРезультат:\n```\ndeployment.apps/cloudru-time-slicing-check created\n```\n3. Проверьте, что все пять реплик в статусе «Running»:\n```\nkubectl get pods\n```\n\nПримерный результат:\n```\nNAME                                          READY   STATUS    RESTARTS   AGEcloudru-time-slicing-check-6dcc7495bc-6dt4k   1/1     Running   0          6m25scloudru-time-slicing-check-6dcc7495bc-7vdvw   1/1     Running   0          6m25scloudru-time-slicing-check-6dcc7495bc-g5xdr   1/1     Running   0          6m25scloudru-time-slicing-check-6dcc7495bc-txbd9   1/1     Running   0          6m25scloudru-time-slicing-check-6dcc7495bc-zxdx8   1/1     Running   0          6m25s\n```\n4. Посмотрите логи одного из подов:\n```\nkubectl logs deploy/cloudru-time-slicing-check\n```\n\nПримерный результат:\n```\nFound 5 pods, using pod/cloudru-time-slicing-check-6dcc7495bc-7vdvw[Vector addition of 50000 elements]Copy input data from the host memory to the CUDA deviceCUDA kernel launch with 196 blocks of 256 threadsCopy output data from the CUDA device to the host memoryTest PASSED...\n```\nСм.такжеTime-Slicing GPUs in Kubernetes\n\n  [© 2025 Cloud.ru](https://cloud.ru)",
      "debug": {
        "start_page": 1,
        "end_page": 1
      }
    }
  ]
}
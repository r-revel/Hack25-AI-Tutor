{
  "title": "Аксенов С.В., Новосельцев В.Б. Организация и использование нейронных сетей (методы и технологии)",
  "chapters": [
    {
      "name": "Глава 1 . Предыстория вопроса",
      "content": "--- Страница 3 ---\nГлава 1. Предыстория вопроса 3 Глава 1 ПРЕДЫСТОРИЯ ВОПРОСА 1.1. Биологический прототип Представление о детальном устройстве головного мозга появилось только около ста лет назад. В 1888 г. испанский доктор Рамони Кайалэкспериментально показал, что мозговая ткань состоит из большогочисла связанных друг с другом однотипных узлов – нейронов. Более поздние исследования при помощи электронного микроскопа показали, что все нейроны, независимо от типа, имеют схожую органи-зационную структуру (рис. 1.1). От центральной части нейрона – сомыотходят древовидные отростки – дендриты. Они играют роль рецепто- ров – сигналов от других нейронов. Задача аксона, самого крупного от- ростка, заключается в том, чтобы передавать сигнал активности от сомыдругим нейронам. Место соединения аксона с дендритами других ней-ронов разделено малым, порядка 200 нм, расстоянием. Этот промежу- ток принято называть синапсом. Дендриты ()dendrites Аксон ( ) axon Синапс ( ) synapseЯдро ()soma Рис. 1.1. Структура нейрона\n--- Страница 4 ---\n4 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Сигналы между нейронами передаются химическим и электриче- ским путем. В состоянии покоя протоплазма нейрона заряжена отрица- тельно, с потенциалом порядка 70 мВ. По мере поступления сигналов от дендритов заряд деполяризуется до значения потенциала 60 мВ, проис-ходит диффузия в сому положительно заряженных ионов натрия Na+,нейрон «срабатывает» – его заряд резко повышается до положительно- го, и затем возбуждение распространяется через аксон на другие нейро- ны. Затем ядро постепенно приходит в свое исходное состояние. Ней-роны головного мозга «срабатывают» несинхронно, подобно цифровымустройствам. Частота срабатывания для разных нейронов может варьи- роваться от 1 до 100 Гц. Скорость распространения сигнала лежит в диапазоне от 0,5 до 2 м/с. Помимо этого было замечено, что сила синап-тической связи для разных нейронов различна и более того – не посто-янна. Если нейрон «срабатывает» чаще остальных, то сила его синапти-ческой связи возрастает. Подобная адаптивность синапса носит назва- ние правила Хебба (см. далее п. 2.4). Общая структура головного мозга весьма сложна. Число нейронов можно оценить приблизительно как 10 11. Каждый нейрон в среднем име- ет 104 синаптических связей, а всего мозг имеет 1015 соединений. Более подробно о биологических аспектах мозга можно узнать в [1, 2]. Из всего изложенного выше видно, что головной мозг – это очень сложная систе-ма с множеством параметров, связей и внешних сенсоров, и до конца по-нять, как происходит процесс мышления, мы пока не в состоянии. 1.2. История В 1943 г. В. Маккаллох и В. Питтс предложили систему обработки информации в виде сети, состоящей из простых вычислителей, создан-ных по принципу биологического нейрона. Каждый такой элемент 1, 2 in= имеет входы и выходы, принимающие значения 0 или 1. Со- стояние отдельного нейрона определяется влиянием остальных как взвешенная линейная комбинация ij jwn∑ их выходов nj. К сумме затем применяется пороговая функция вида (0 ) 0g <=∑ ; (0 ) 1g ≥=∑ . Мак- каллох и Питтс показали, что такие сети могут производить произволь- ные вычисления, подобно известной машине Тьюринга. При этом един-ственной нерешенной проблемой оставался подбор весов w ij – на- страиваемых параметров для задачи. В 1962 г. Ф. Розенблатт для реше-\n--- Страница 5 ---\nГлава 1. Предыстория вопроса 5 ния проблемы классификации символов предложил использовать осо- бый тип искусственной нейронной сети, названный им персептроном [3]. В этой же работе им был предложен итеративный алгоритм получе- ния весов wij, моделирующих силу синаптической связи. Значения весов получались по известным входным значениям и соответствующим же-лаемым выходам сети. Однако уже в 1969 г. М. Минский и С. Паперт в своей известной ра- боте [4] показали, что персептрон не может решать целый класс про-стых задач, таких как, например, реализация логической операции XOR(исключающее ИЛИ). Появление этой работы сыграло роковую роль для теории нейронных сетей, все исследования в этой области фактиче- ски были приостановлены вплоть до середины 80-х годов. В 1986 г. ситуация изменилась – Д. Румельхарт, Г. Хинтон и Р. Виль- ямс предложили эффективный алгоритм для обучения более совершен-ного, так называемого многослойного, персептрона [5]. Алгоритм полу- чил название алгоритма обратного распространения ошибки (он де- тально обсуждается в п. 2.2). Начиная с конца 80-х и по настоящее время теория нейронных сетей испытывает настоящий бум по количеству научных публикаций. В со- временный период нейронные сети все чаще применяются для решения разнообразных практических задач. Более того, можно с уверенностьюутверждать, что программно-аппаратные устройства, построенные набазе нейронных сетей, активно внедряются в нашу жизнь и все чаще используются в медицинской диагностике, промышленности, финансо- вой системе и в повседневной жизни – там, где необходимо решать за-дачи, до этого подвластные только человеку. 1.3. Формальный нейрон В 1962 г. Розенблатт предложил следующую модель нейронной сети (персептрона – [3]). Нейронная сеть (рис. 1.2) состоит из k нейронов, имеет d входов, k выходов и только один слой настраиваемых весов wij. Нейрон имеет структуру, представленную на рис. 1.3. Каждый j-й нейрон сети вычисляет взвешенную сумму своих входов: 0 1()d ji i j iyxx w w ==+∑ , где w0 – порог нейрона.\n--- Страница 6 ---\n6 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ xd x1 x0=1wijΣ Σ Σnk n2 n1 xdx1x0=1 w0 – порог w1 wdg()Σ Рис. 1.2. Персептрон Розенблатта Рис. 1.3. Структура нейрона Обычно для удобства входной вектор расширяется до 0 (1 , , , )d xx x= и порог w0 вносится под знак суммы: 0()d j ii j iyxx w ==∑ . (1.1) Выходной сигнал нейрона определяется нелинейной пороговой функцией g(a), изображенной на рис. 1.4: {}1, 0,()1, 0.agaa−<=+≥(1.2) +1 aga() 1ga() =–1, +а< 0, 1, а≥ 0. Рис. 1.4. Пороговая функция\n--- Страница 7 ---\nГлава 1. Предыстория вопроса 7 Розенблатт предложил использовать персептрон для задач класси- фикации. Рассмотрим простой случай и его геометрическую интерпре- тацию. Имеем обучающий набор 2-компонентных входных векторов вида 12(, )xx x= , каждый из которых принадлежит к одному из двух разных классов C1 и C2. Требуется: для любых новых векторов опреде- лить их принадлежность к классу. Для этого можно использовать пер-септрон всего с одним нейроном. Из уравнения (1.1) видно, что нейронстроит на плоскости прямую (в случае d-мерного пространства – гипер- плоскость). Входные векторы соответственно будут попадать в ту или иную полуплоскость. Применение пороговой функции (1.2) позволяетопределить полуплоскость, принявшую вектор. Вид прямой будет пол-ностью определяться настраиваемыми весами w ij и порогом w0. На рис. 1.5 сплошной линией обозначена прямая, которая непра- вильно разбивает векторы на классы (точки разных видов попадают в один класс), прямая с пунктирной линией будет правильно классифи-цировать обучающий набор и, с высокой вероятностью, новые векторы.Такую прямую назовем границей решения персептрона. Процесс опре- деления весов задается определенным алгоритмом, который принято называть обучением персептрона. x1 x2C1 C2– Веса настроены (нейрон обучен) { } ∈C1 { } C ∈ 2– Веса не настроены Рис. 1.5. Геометрическая интерпретация\n--- Страница 8 ---\n8 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Обучение персептрона основано на способе обучения «с учителем». Имеем набор обучающих входных векторов { xn} и соответствующий им набор значений желаемых выходов персептрона { tn}. Будем полагать, что tn = +1 если входной вектор xn принадлежит классу C1, и tn = –1, ес- ли xn принадлежит классу C2. Из (1.1) и (1.2) видно, что для правильной классификации входных образов необходимо для xn из C1 (tn = +1) иметь WTx > 0, где WTx – матричная форма записи (1.1). Для xn из C2 (tn = –1) необходимо WTx < 0. В общем случае имеем ∀x WT(xt) > 0. Отсюда сле- дует, что для правильной работы персептрона необходимо минимизи-ровать функцию ошибки, названную критерием персептрона: () ( ) nT xMEw W x t ∈=−∑ . (1.3) Здесь M – набор неверно классифицированных данных для весов w. Из определения ошибки (1.3) следует очень простая процедура обу- чения персептрона [6]. Процедура обучения персептрона Ш0. На начальном этапе веса wij инициализируются случайными значениями. Ш1. Подаем на вход персептрона вектор xn из обучающей выборки. Ш2. На выходе возможны три варианта: 1) если персептрон классифицировал входной вектор правильно, то ничего не делаем; 2) если неправильно и xn должен принадлежать классу C1 (tn = +1), то прибавляем xn к весу; 3) если неправильно и xn должен принадлежать классу C2 (tn = –1), то отнимаем xn от веса. Кратко процесс изменения весов по вариантам 2 и 3 можно записать формулой 1() WW w tτ+ τ=+ η , (1.4), где η > 0 – коэффициент, задающий скорость обучения; 1Wτ+ – матрица новых значений весов. Покажем, что при обучении по правилу (1.4) ошибка (1.3) будет уменьшаться. Так как 0η> и (xt)2 > 0, имеем 11( ) () () () () ( ) ( )E w W xt W xt xt xt W w E wτ+ τ+ τ τ τ=− =− −η < = . (1.5)\n--- Страница 9 ---\nГлава 1. Предыстория вопроса 9 Факт уменьшения ошибки еще не доказывает сходимость алгоритма. В 1973 г. Р. Дуда и П. Харт доказали теорему сходимости персептрона [7]. Для любых линейно разделяемых входных данных правило (1.4) на- ходит решение за конечное число шагов. Ключевым моментом в формулировке теоремы является факт линей- ной разделимости данных. Как отмечалось выше, Минский и Паперт в своей работе [4] показали, что персептрон не может решать целый класс простых задач. Из рис. 1.6. видно, что для XOR персептрон, в принципе,не сможет правильно разместить точки по соответствующим классам. XOR 100 0 01 1 10 1 1 0x1 x2C1 C2C2 C1x1x2 Рис. 1.6. Проблема XOR Заметим, что добавление нейронов в персептрон не решает проблему в силу того, что это ведет только к увеличению числа классов, к которымможно отнести входные векторы. Как отмечалось выше, такое серьез-ное ограничение персептрона привело к длительному застою в теориинейронных сетей, пока в середине 80-х XX в. годов не была разработана процедура обучения многослойного персептрона. 1.4. Возможности многослойного персептрона Недостатки персептрона можно преодолеть, если увеличить число слоев нейронов в персептроне. Было доказано, что уже для двух слоев настраиваемых весов граница решения принимает вид выпуклой ком- бинации (рис. 1.7, б). При трех и более слоях персептрона граница ре-шения может состоять из несмежных областей, ограниченных гипер-\n--- Страница 10 ---\n10 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ плоскостями (рис. 1.7, в). Таким образом, с помощью многослойного персептрона можно решать задачи произвольной сложности, а не толь- ко линейно разделяемые. в) 3-уровнеый перcептронб) 2-уровневый перcептрона) персептрон Рис. 1.7. Решающие границы персептрона 1.5. Классификация нейронных сетей В настоящее время кроме многослойного персептрона существует множество способов задания нейроподобных структур. Все виды ней-ронных сетей можно условно разделить на сети прямого распростране-ния и сети с обратными связями. Как следует из названия, в сетях пер-вого типа сигналы от нейрона к нейрону распространяются в четко за- данном направлении – от входов сети к ее выходам. В сетях второго ти- па выходные значения любого нейрона сети могут передаваться к егоже входам. Это позволяет нейронной сети моделировать более сложныепроцессы, например временные, но делает выходы подобной сети не- стабильными, зависящими от состояния сети на предыдущем цикле (см. далее п. 2.3). На рис. 1.8. собраны наиболее распространенные ти-\n--- Страница 11 ---\nГлава 1. Предыстория вопроса 11 пы нейронных сетей. Разнообразие нейронных сетей увеличивается еще больше, благодаря огромному количеству алгоритмов и методик обуче- ния, а также наличию нескольких видов пороговых функций. прямого распространения с обратными связямиСамые известные виды нейронных сетей АДАЛИНcети каскадной корреляцииRBF-сетиMLP(многослойный перcептрон)перcептрон Time Delay-сетистохастические сети (машина Больцмана)АRТ-сетисети Элманаaссоциативная память, сети ХопфидаSOM, сети Кохоненавстречного распространения Рис. 1.8. Классификация нейронных сетей 1.6. Пороговые функции На практике пороговую функцию вида (1.2) очень часто заменяют похожими по форме, но обладающими лучшими свойствами функция-ми: логическим сигмоидом (1.6) (рис. 1.9) и гиперболическим танген-сом (рис. 1.10). Эти функции фактически реализуют пороговую функ- цию, но при этом являются гладкими, дифференцируемыми, что явля- ется принципиально важным для реализации многих алгоритмов обуче-ния: 1() 1aga e−= −; (1.6) ()aa aaeega ee− −−= +. (1.7)\n--- Страница 12 ---\n12 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ –1 –,08 –,06 –,04 –,02 0 02, 04, 06, 08, 101,02,03,04,06,07,08,09,ga() a1 05,ga() =1 1 + е–аlgsig( ) a Рис. 1.9. Логический сигмоид –1 – , 08 – , 06 – , 04 – , 02 0 02 ,0 4 ,0 6 ,0 8 ,102,04,06,08,ga() a1 ga() =еа + е–аth( )a –0 2, –0 4, –0 6, –0 8, –1еа– е–а Рис. 1.10. Гиперболический тангенс\n--- Страница 13 ---\nГлава 1. Предыстория вопроса 13 1.7. Обучение нейронных сетей Настройка нейронных сетей производится при помощи трех пара- дигм обучения: - обучение «с учителем». При данном подходе считается, что для каждого примера xn из обучающей выборки { xn} заранее известен же- лаемый результат t ∈ {tn} работы нейронной сети. Это позволяет эф- фективно корректировать веса сети. Примером обучения «с учителем» может служить процедура обучения персептрона (п. 1.3). Очевиднымнедостатком подобного подхода является то, что не всегда имеется дос-таточное количество примеров «с ответами», а порой их и вовсе невоз- можно получить. В этом случае используется: - обучение «без учителя». Этот тип обучения предполагает, что же- лаемые выходы сети не определены, и алгоритм обучения нейроннойсети подстраивает веса по своему усмотрению. Фактически при этом нейронная сеть ищет закономерности в обучающих данных и выполня- ет группирование схожих входных векторов по неявным признакам.Чаще всего этот метод используется для задач классификации. Приме-ром такого типа обучения служит алгоритм обучения Кохонена (см. да- лее п. 3.4); - обучение методом критики. Данный подход является фактически промежуточным между первыми двумя. Предполагается, что имеетсявозможность только оценивать правильность работы сети и указыватьжелаемое направление обучения. Подобная ситуация часто встречается в системах, связанных с оптимальным управлением. Данный подход, например, используется при стратегии «кнута и пряника», предложен-ной Г. Барто в 1985 г. [8]. Подобная нейронная сеть состоит из так на-зываемых эгоистичных нейронов. Процедура обучения таких нейронов поощряет каждый отдельный нейрон к увеличению только собственной «награды», а не производительности всей сети, как реализовано в мето-де обратного распространения ошибки (см. далее п. 2.2).",
      "debug": {
        "start_page": 3,
        "end_page": 13
      }
    },
    {
      "name": "Глава 2 . Сети прямого распространения",
      "content": "--- Страница 14 --- (продолжение)\nГлава 2 СЕТИ ПРЯМОГО РАСПРОСТРАНЕНИЯ 2.1. Теорема Колмогорова В разд. 1.4 было показано, что задачи, с которыми не в состоянии работать персептрон, можно решать при помощи многослойного пер-септрона. Однако формального математического доказательства не бы-ло приведено. В 1957 г. Колмогоровым была доказана теорема, позво- ляющая говорить о том, что для решения любой задачи возможно по- строить нейронную сеть. Теорема Колмогорова . К а ж д а я н е п р е р ы в н а я ф у н к ц и я d пе- ременных, заданная на единичном кубе d-мерного пространства, пред- ставима в виде 21 1 11( , , ) ( )dn p dq q p qpfx x h x+ ==⎧⎫⎪⎪=ϕ ⎨⎬ ⎪⎪⎩⎭∑∑ , (2.1) где hq– непрерывные негладкие функции; ()p qpxϕ – стандартные функ- ции, не зависящие от вида функции f. Доказательство этой теоремы и ее применимость к теории нейронных сетей приведено в [9]. В терминах нейронных сетей теорему можно перефразировать следующим обра-зом [6]: Любое отображение входов нейронной сети в ее выходы может быть реализовано трехслойной нейронной сетью прямогораспространения с d(2d + 1) нейронов на первом и 2 d + 1 на втором слое (рис. 2.1). Рис. 2.1. Нейронная сеть по теореме Колмогорова: g(x) – функции, зависящие от вида fy gx()2+ 1d gx()1 h2+ 1d h1 xd x1 входыh2+ 1d h1λ1 λd λ1 λd\nГлава 2 СЕТИ ПРЯМОГО РАСПРОСТРАНЕНИЯ 2.1. Теорема Колмогорова В разд. 1.4 было показано, что задачи, с которыми не в состоянии работать персептрон, можно решать при помощи многослойного пер-септрона. Однако формального математического доказательства не бы-ло приведено. В 1957 г. Колмогоровым была доказана теорема, позво- ляющая говорить о том, что для решения любой задачи возможно по- строить нейронную сеть. Теорема Колмогорова . К а ж д а я н е п р е р ы в н а я ф у н к ц и я d пе- ременных, заданная на единичном кубе d-мерного пространства, пред- ставима в виде 21 1 11( , , ) ( )dn p dq q p qpfx x h x+ ==⎧⎫⎪⎪=ϕ ⎨⎬ ⎪⎪⎩⎭∑∑ , (2.1) где hq– непрерывные негладкие функции; ()p qpxϕ – стандартные функ- ции, не зависящие от вида функции f. Доказательство этой теоремы и ее применимость к теории нейронных сетей приведено в [9]. В терминах нейронных сетей теорему можно перефразировать следующим обра-зом [6]: Любое отображение входов нейронной сети в ее выходы может быть реализовано трехслойной нейронной сетью прямогораспространения с d(2d + 1) нейронов на первом и 2 d + 1 на втором слое (рис. 2.1). Рис. 2.1. Нейронная сеть по теореме Колмогорова: g(x) – функции, зависящие от вида fy gx()2+ 1d gx()1 h2+ 1d h1 xd x1 входыh2+ 1d h1λ1 λd λ1 λd\n--- Страница 15 ---\nГлава 2. Сети прямого распространения 15 К сожалению, практического применения для построения нейрон- ных сетей теорема Колмогорова не имеет [6]. Она только гарантирует существование такой сети, но не определяет алгоритм ее построения. Вдобавок к этому, по условию, функции g обязательно должны быть негладкие и их вид зависит от f. А в нейронных сетях вид пороговой функции обычно фиксируется, оставляя для задачи только подстройку весов. Тем не менее важность этой теоремы трудно переоценить, так как она теоретически обосновывает всю мощь нейронных сетей. Болееобщий результат, теоретически обосновывающий эффективность ней-ронных сетей, приведен в [10]. 2.2. Алгоритм обратного распространения ошибки Как показано в п. 1.4, многослойный персептрон в состоянии решать произвольные задачи. Но долгое время его использование было затруд-нительно из-за отсутствия эффективного алгоритма обучения. При не-скольких слоях настраиваемых весов становится непонятным, какие именно веса подстраивать в зависимости от ошибки сети на выходе. Процедура обучения персептрона (п. 1.3) при этом переставала рабо-тать. В 1986 г. Румельхарт, Хинтон и Вильяме предложили так называе- мый алгоритм обратного распространения ошибки. Обучение много- слойного персептрона основано на минимизации функции ошибки сетиE(w). Ошибка определяет отклонение желаемых выходов сети t n от по- лучившихся yn. Обычно функция E(w) определяется методом наимень- ших квадратов: 2 1() ( )2nn nEw t y =−∑ . (2.2) Функция ошибки (2.2) минимизируется методом градиентного спус- ка, известным из теории численных методов. Суть метода заключается в том, что двигаясь в направлении, противоположном градиенту функции∆w ji, в конце концов приближаемся к минимуму, возможно локальному, функции E(w): ji jiEww∂∆= − η∂. (2.3) Здесь 0 < η < 1 задает скорость обучения; wji – коэффициент связи i ней- рона слоя n – 1 с j нейроном слоя n. Найдем способ вычисления гради-\n--- Страница 16 ---\n16 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ ента ∂ E/∂w. Запишем формально схему работы многослойного персеп- трона. Каждый нейрон рассчитывает взвешенную сумму своих входов: ij i i naw z=∑ . (2.4) Здесь zi – это входы нейрона и соответственно выходы предыдущего слоя нейронов (рис. 2.2, а). Выход нейрона j – это преобразование сум- мы ai пороговой функцией g: ()j j zg a= . (2.5) k j zxii()wij б azjyk Рис. 2.2. Схема нейронной сети для алгоритма обратного распространения Вернемся к ∂ E/∂wji и заметим, что E(w) является сложной функцией, для которой w = w(aj) является функцией от aj. Отсюда по правилу вы- числения производной сложной функции имеем j ji j jia EE wa w∂ ∂∂=∂∂ ∂. Используя (2.4), получаем jj i i i ji jiaw z zww∂∂ ==∂∂∑. Выражение δ j = ∂E/∂aj назовем ошибкой на слое j. В итоге можно заме- тить, что для вычисления градиента на каждом слое i вычисляется про- изведение величины ошибки предыдущего i, находящегося сверху слоя j, на входное значение слоя: ji jiEzw∂=δ∂. (2.6)\n--- Страница 17 ---\nГлава 2. Сети прямого распространения 17 Найдем теперь значение ошибки δ j для каждого слоя. Для выходного слоя δ k можно получить довольно просто: ()kj k jkjk kk kkkaw zy EE ay a yg a=∂ ∂∂δ= = =∂∂ ∂ =∑ (2.7) Заметим, что ∂ yk/∂ak есть ни что иное, как производная пороговой функции ∂ yk/∂ak = g'(a). Для логического сигмоида (1.6) такая производ- ная будет иметь довольно простой вид: g'(a) = g(a)(1 – g(a)). Второй со- множитель ∂ E/∂yk вычисляется по формуле (2.2) и приводится к про- стому выражению: ∂ E/∂yk = (yk – tk). В итоге формула вычисления ошибки на выходном слое имеет вид ()kk k gyt′ δ= − . (2.8) Для промежуточного слоя δ j получается аналогичным способом: ()kj i jj kjj k j kj j jjaw zzy EE ay z a zg a=∂∂ ∂∂δ= = =∂∂ ∂ ∂ =∑ ∑ . (2.9) Суммирование происходит по всем k, к которым нейрон j посылает сигнал (рис. 2.2, б). Здесь ∂ E/∂yk есть не что иное, как δ k, а ∂zj/∂aj – это производная пороговой функции g'(aj). От множителя ∂ ak/∂zj остается просто вес wjk. В итоге получаем ()j jk j k kgaw′ δ= δ ∑ . (2.10) Таким образом, ошибка на каждом слое вычисляется рекурсивно че- рез значения ошибки на предыдущих слоях: коррекция ошибки как быраспространяется обратно по нейронной сети. Алгоритм обратного распространения ошибки Ш0. Перед началом работы алгоритма веса wji инициализируем слу- чайными значениями. Ш1. Подаем на вход персептрона вектор xn из обучающей выборки и получаем значение yn на выходе по формулам (2.5) и (2.4). Ш2. Вычисляем ошибки δ k для выходов сети по (2.8). Ш3. Вычисляем ошибки δ j для всех скрытых слоев по (2.10). Ш4. Находим значение градиента по (2.6). Ш5. Корректируем значения синаптических весов по (2.3).Ш6. Вычисляем значение ошибки (2.2). Если величина ошибки не устраивает, то идем на Ш1.\n--- Страница 18 ---\n18 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Один прогон распространения ошибки принято называть эпохой. Трудоемкость алгоритма обратного распространения ошибки определя- ется, как величина порядка O(W)N. Здесь W – число настраиваемых ве- сов сети; N – размер обучающей выборки. Функция ошибки E(w), благодаря тому, что в нейронной сети ис- пользуются нелинейные пороговые функции, представляет собой до- вольно сложную «овражистую» поверхность с большим числом ло- кальных минимумов. Если при градиентном спуске попасть в такой ми-нимум, то, очевидно, сеть не будет настроена на оптимальную произво-дительность. К счастью, способы оптимизации поиска минимума функ- ций подробно исследованы в теории численных методов, и некоторые из них перенесены на алгоритм обратного распространения. Усовершенствования алгоритма обратного распространения 1) Простейший способ – это использование переменной скорости обучения η. В начале работы алгоритма ее величина представляет собойбольшое значение, близкое к 1, по мере сходимости η последовательноуменьшается. Это позволяет быстро подойти к минимуму, а затем точно попасть в него без лишнего «стробирования»; 2) «Овражный» метод. Учитываются тенденции в поверхности до- бавлением момента инерции µ: 1 nnWE W− ∆= − η ∇ + µ . Здесь Wn – матри- ца значений новых весов; Wn–1 – матрица значений весов на предыду- щей эпохе; /n EE W∇≡ ∂ ∂ . Идея заключается в скачке через резкие «провалы» в поверхности ошибки – локальные минимумы; 3) Метод сопряженных градиентов. Флетчер и Ривс предложили вы- бирать направление, сопряженное градиенту, более точно указывающееименно на минимум функции: 1 nn nWE W− ∆= − η ∇ + β , при 2 2 1n nE E−∇β= ∇; (2.11) 4) Наиболее точное решение – это решение, которое позволяют по- лучить так называемые методы второго порядка. Общий принцип рабо-ты основан на использовании матрицы вторых производных – гессиана H = ∇ 2E. Градиент не всегда может указывать в сторону, противопо- ложную минимуму, а величина – H–1∇E, направление Ньютона, всегда будет направлена в сторону минимума (рис. 2.3). Отсюда изменение весов происходит по правилу ∆ W = –η H–1∇E.\n--- Страница 19 ---\nГлава 2. Сети прямого распространения 19 Но сам гессиан довольно сложен для вычисления, поэтому используются мето- ды, где значение H считается приближенно. Лучшим по производительности являетсяметод Левенберга – Маркварта, в которомгессиан аппроксимируется как H = J TJ + αJ; J – это якобиан, который вычислять намно- го легче. Процедура коррекции весов приобретает вид ∆ W = –η( JTJ)–1(JT). Трудоемкость данного метода – величина порядка O(W2)N. Метод явля- ется очень эффективным и для нейронной сети среднего размера – ми- нимум ошибки, как правило, находится за несколько эпох. Сравнение производительности этих методов обучения приводится в [11]. Многослойный персептрон является на сегодняшний день наиболее распространенным и исследованным типом нейронных сетей. Он широ-ко применяется для задач классификации, построения экспертных се- тей. Одной из нерешенных проблем многослойного персептрона, как и большинства классов нейронных сетей, является неопределенность вы-бора топологии (числа слоев, нейронов в слоях) (см. далее п. 4.3). Пример использования многослойного персептрона З а д а ч а: Имеется зашифрованное сообщение. Известен шифр: каж- дой букве ставилась в соответствие предыдущая по алфавиту буква. Каждая буква находится в области размером 6 × 6 пикселей. Требуется расшифровать сообщение. Обучающие входные и выходные векторы создадим следующим об- разом. Если некоторый пиксель буквы закрашен, то соответствующий ему компонент входного или выходного вектора принимает значение 1, в противном случае – 0. Примеры букв представлены на рис. 2.4. Так как входной и выходной векторы кодируются 36 битами, то сеть имеет 36 входных и 36 выходных нейронов. Для обучения используем алгоритм обратного распространения с последующей редукцией методом OBD (подробнее в п. 4.4.6). Пусть в скрытом слое имеется 16 нейронов. После обучения сети следует про-цесс исключения из нее весов и нейронов, оказывающих наименьшеевлияние на классификацию образов, для увеличения обобщающей спо- собности сети. В результате обучения в скрытом слое сети остается 8 нейронов, таким образом минимизируется влияние шумов на результатклассификации.wмин w∇E –Н–1∇E Рис. 2.3. Гессиан\n--- Страница 20 ---\n20 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Примеры работы обученной сети на некоторых зашумленных сим- волах представлены на рис. 2.5. Входной образ Выходной образ Зашумленный образ Результат Рис. 2.4. Примеры из обучающего набораРис. 2.5. Результаты расшифрования 2.3. Сети радиально-базисных функций Сети радиально-базисных функций – RBF-сети кардинально отли- чаются от многослойного персептрона. Многослойный персептрон ре- шает задачу с помощью нейронов, производящих нелинейные преобра- зования своих взвешенных входов, алгоритм его обучения сложен итрудоемок. В RBF-сети активизация нейронов задается дистанцией ме-жду входным вектором и заданным в процессе обучения вектором- прототипом, а обучение происходит быстро и носит элементы как обу- чения «с учителем», так и «без учителя». RBF-сети берут свое начало оттеории точного приближения функций, предложенной Пауэлом в1987 г. Пусть задан набор из N входных векторов x n с соответствующими выходами tn. Задача точного приближения функций – найти такую функцию h, чтобы ()nnhx t=, 1 , , .nN= Для этого Пауэл предложил\n--- Страница 21 ---\nГлава 2. Сети прямого распространения 21 использовать набор базисных функций вида ()nxxΦ− , тогда получаем () ( )nn n nhx w x x t=Φ − =∑ , 1 , ,nN= . (2.12) Здесь wn – свободно настраиваемые параметры. Обычно в качестве базисной берут экспоненциальную функцию 22/(2 )()xxe−σΦ= , где σ – регулирующий параметр. Такая функция обладает необходимым свой- ством базисной функции lim ( ( )) 0 xx →∞Φ→ . Но такое точное приближение дает плохие результаты для зашум- ленных данных (рис. 2.6, а), поэтому в 1988 г. Д. Брумхеад и Д. Лоув предложили модель RBF-сети. 1,0 аб1,0 1,0 1,0 00 Рис. 2.6. Аппроксимация функции RBF-сетью Правила задания RBF-сети 1. Число M базисных функций выбирается много меньше числа обу- чающих данных: M << N. 2. Центры базисных функций µ j не опираются на точки входных данных. Определение центров функций становится частью процесса обучения. 3. Для каждой из M базисных функций задается свой регулирующий параметр σ j, который также определяется в процессе обучения RBF-сети. 4. В сумму (2.12) добавляется константа wk0 – порог нейрона. В итоге RBF-сеть будет описываться формулой 0 10() () ()MM k k jj k k jj jjyx w x w w x ===Φ + =Φ∑∑ . (2.13)\n--- Страница 22 ---\n22 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Здесь базисные функции: 2 2|| 2() .j jx jxe−µ− σΦ= Для удобства представления добавлена функция 0() 1 .xΦ= В 1993 г. Д. Парк и И. Сандберг показали, что R BF-сети, построен- ные подобным образом, обладают свойством аппроксимации (рис. 2.6, б) любой произвольно гладкой функции при условии доста- точного количества базисных функций [6]. На рис. 2.7. приведена архи-тектура RBF-сети. Каждая базисная функция выполняет роль нейрона. y1 Ф0wkj Ф1 ФМyс xd x1µji Рис. 2.7. Сеть радиально-базисных функций Обучение RBF-сети Имеем обучающий набор: { xn} → { tn}. Обучение происходит в два этапа: 1) определяются параметры базисных функций: µ j, σ j. Причем, как правило, используются только входные векторы { xn}, т.е. обучение происходит по схеме «без учителя» (п. 1.7). Для определения σ j используется алгоритм «ближайшего соседа». Выбирается количество центров (базисов) M. Алгоритм ищет разбиение множества { xn} на M несмежных подмножеств Sj таким образом, чтобы минимизировать функцию J: 2 1jM n j jn SJx =∈=− µ∑∑ , где 1 jn j jnSxN∈µ=∑ . (2.14) Здесь µ j фактически является средней точкой Sj. Первоначальное раз-\n--- Страница 23 ---\nГлава 2. Сети прямого распространения 23 биение производится случайным образом. Вычисляется µ j. Затем раз- биение изменяется, пока существует возможность уменьшить J. Параметр σ j – выбирается эвристическим путем. Как правило, σ j де- лают чуть большим расстояния между центрами соответствующих ба-зисных функций µ j; 2) на втором этапе фиксируются базисные функции. RBF-сеть полу- чается равной однослойной нейронной сети. Затем обучение происхо- дит по правилу обучения «с учителем». Как обычно, минимизируется ошибка 2 1() ( )2n kj j k nkEw w t=Φ −∑∑ . Так как E является квадратической функцией от весов w, то минимум E может быть найден решением сис- темы линейных уравнений () 0n kj j k j nkEwtw⎧⎫ ∂=Φ − Φ =⎨⎬∂ ⎩⎭∑∑ . (2.15) Решение системы (2.15) находится крайне быстро. Следовательно, алгоритм обучения RBF-сетей является очень эффективным. В заключение приведем краткую таблицу сравнительных характери- стик RBF-сетей и многослойного персептрона: Многослойный персептрон RBF-сети Граница решения представляет собой пересечение гиперплоскостей (п. 1.4)Граница решения – это пересечение ги- персфер, что задает границу более слож- ной формы Сложная топология связей нейронов и слоевПростая 2-слойная нейронная сеть Сложный и медленно сходящийся ал- горитм обучения (п. 2.2)Быстрая процедура обучения: решение системы уравнений + кластеризация Работает на небольшой обучающей выборкеТребуется значительное число обучаю- щих данных для приемлемого результата Универсальность применения: класте-ризация, аппроксимация, управление и проч.Как правило, только аппроксимация функций и кластеризация 2.4. Обучение без учителя. Правило Хебба Можно утверждать, что большая часть информации из окружающего мира усваивается головным мозгом по процедуре обучения «без учите-ля» (п. 1.7), когда явно правильный «ответ» не подсказывается. Осно- вываясь на этом соображении и исследованиях в области структуры\n--- Страница 24 ---\n24 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ мозга (п. 1.1), еще в 1949 г. Д. Хебб предложил следующую процедуру обучения: увеличивать вес связи между двумя нейронами, если они возбуждаются чаще других нейронов сети. Правило Хебба может при- меняться для обучения нейронных сетей прямого распространения в за-даче классификации. Простейшая реализация правила Хебба называетсясигнальным методом: 1nn ij i jwz z−∆= η . (2.16) Здесь ∆ wij – приращение веса, соединяющего выход i-го нейрона предыдущего слоя n – 1 и нейрона j текущего слоя n; zin–1 и zjn – соот- ветственно выходные значения нейронов i и j; величина η > 0 задает скорость обучения. Более совершенной вариацией правила Хебба явля- ется дифференциальный метод: 1* 1 *() ( )nn n n ij i i j jwz z z z−−∆= η − − . (2.17) Здесь добавляется zi*n–1 и zj*n – выходные значения нейронов i и j на предыдущей итерации обучения. Таким образом, дифференциальный метод усиливает сильнее всего те веса, которые связывают нейроны,выходы которых увеличиваются наиболее динамично. Существует третий метод обучения – обучение с забыванием. Вве- дем понятие коэффициента забывания γ – часть синаптического веса, которая «забывается» каждым нейроном на каждой итерации [12]. То-гда обучение выполняется согласно выражению 1(1 ) ( 1 ) ( )nn ij ij i jwt wt z z−+=− γ ⋅ + η ⋅ . (2.18) Коэффициент γ выбирается из расчета < 0,1, таким образом при обу- чении нейрон сохраняет большую часть информации. Алгоритм обучения нейронной сети по правилу Хебба В начале работы алгоритма имеем: многослойный персептрон (п. 1.4); набор входных образов { xn}, которые необходимо классифици- ровать (т.е. разбить на классы по подобию). Ш0. Инициируем веса wij случайными величинами малой величины. Ш1. Подаем на входы сети все векторы из { xn} по очереди и получа- ем выходные значения z для всех нейронов сети. Ш2. На основании полученных значений z по формулам (2.16) или (2.17), или (2.18) производится изменение весов wij всей сети. Ш3. Переходим на Ш1 до тех пор, пока выходные значения сети не стабилизируются с заданной точностью. Так как нейронная сеть – сеть\n--- Страница 25 ---\nГлава 2. Сети прямого распространения 25 прямого распространения, то она рано или поздно придет в устойчивое состояние. Следует заметить, что для обучения по правилу Хебба значения вы- ходов сети для каждого класса входных образов заранее неизвестны. Восновном это будет зависеть от случайного распределения весов на Ш0.Обычно соответствие выходов сети классам векторов определяется на этапе тестирования обученной сети. Иногда все-таки требуется, чтобы выходы сети были четко опреде- лены – { t n}. Тогда на основе сетей, обучающихся «без учителя», строят сети встречного распространения [13]. Для этого к последнему слою се- ти, функционирующей «без учителя», добавляют один слой нейронов и по любому алгоритму обучения «с учителем», например критерию пер-септрона, переводят полученные выходы { y n} для класса входных векто- ров в желаемые { tn}. Подробнее обучение Хебба рассмотрено в [13, 14]. Рассмотрим пример обучения однослойной нейронной сети по мо- дифицированному алгоритму Хебба с забыванием. Сеть имеет 4 нейрона, размерности входного и выходного вектора равны также 4. Проинициализируем веса сети следующим образом: 000 0 , 5 00 , 50 000 0 , 5 0 0 , 5 000W⎡ ⎤ ⎢ ⎥=⎢ ⎥ ⎢ ⎥⎣ ⎦, где числа в строке являются значениями синаптических весов. Сеть будет обучаться на следующих векторах: 11 00 1x⎡⎤ ⎢⎥=⎢⎥ ⎢⎥⎣⎦, 20 0 1 0x⎡⎤ ⎢⎥=⎢⎥ ⎢⎥⎣⎦, 30 00 1x⎡⎤ ⎢⎥=⎢⎥ ⎢⎥⎣⎦. Обучение производилось при η = 0,1 и γ = 0,025. После двадцати итера- ции матрица весов приобрела вид 0,592 0 0 1,343 00 , 30 000 0 , 5 4 1 0 0,752 0 0 0,586W⎡ ⎤ ⎢ ⎥=⎢ ⎥ ⎢ ⎥⎣ ⎦. Сеть значительно усилила связи первого и четвертого нейронов с пер- выми и четвертыми входами.\n--- Страница 26 ---\n26 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ 2.5. Сети Кохонена. SOM Сети Кохонена – еще один вид сетей, обучающихся «без учителя». В 1987 г. Т. Кохонен предложил нейронную сеть следующего вида (рис. 2.8). Сеть Кохонена состоит из одного слоя настраиваемых весов ифункционирует в духе стратегии, согласно которой победитель забира-ет все, т.е. только один нейрон возбуждается, остальные выходы слоя подавляются, выходы z j сети при этом: j ij i iz wx=∑ ,1 , 0kj kkz z≠ ∃= = . (2.19) wij xd x1 x2zm z1 z2 Рис. 2.8. Сеть Кохонена Сеть Кохонена осуществляет классификацию входных векторов в группы схожих, подстраивая веса таким образом, что входные образы,принадлежащие одному классу, будут активизировать один и тот же выходной нейрон z j. Обучение нейронной сети Кохонена Для работы алгоритма важно, чтобы все входные векторы { xn} были нормализованы, т.е. представляли собой единичный вектор в простран-стве: 2 ii i ix xx′=∑ . (2.20) Ш0. Инициируются начальные значения wij (см. ниже). Ш1. Вариант 1. На вход подается нормализованный вектор xi и вы- числяется его скалярное произведение с вектором весов каждого нейро- на 12( , , , )j jj d j ww w w= : ^cos( ) wx x w x w= . (2.21)\n--- Страница 27 ---\nГлава 2. Сети прямого распространения 27 Выбирается вектор весов, дающий максимальное скалярное произ- ведение. Напомним, что cos(0°) = 1, отсюда максимальное значение скалярного произведения будет давать наиболее сходный с входным вектором вектор весов. Вариант 2. На вход также подается нормализованный вектор xi. Вы- числяется расстояние между векторами wj и x в пространстве и выбира- ется вектор весов, расстояние до которого меньше всего: 2()jjDx w=− . (2.22) Ш2. Проводится аккредитация – настройка весов только выбранного на Ш1 нейрона по правилу 1()tt tww x w+=+ η − , где 1tw+ – новое значение веса. (2.23) Подстройка весов подобным образом сводится к минимизации раз- ницы между входным вектором и векторами весов выбранного нейрона.Существует простая геометрическая интерпретация для двумерного случая (рис. 2.9). Находится вектор ( x – w t) и уменьшается на величину η, задающую скорость обучения η ≈ 0,7. wt + 1 xwt1 1x1x2 () x – wt η()x – wt Рис. 2.9. Подстройка весов нейрона Таким образом, обучение сводится к вращению вектора весов ней- рона в направлении входного вектора без существенного изменения егодлины. Ш3. Повторяются шаги Ш1 и Ш2 до тех пор, пока выходные значе- ния сети не стабилизируются с заданной точностью.\n--- Страница 28 ---\n28 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Выбор начальных значений на Ш0 Обычно перед началом работы обучающего алгоритма нейронной сети веса wij инициируются случайными величинами. В случае сети Ко- хонена получаются векторы весов выходных нейронов, равномернораспределенные по поверхности гиперсферы. Но, как правило, входные векторы x i распределены неравномерно и группируются кучками на ма- лой части поверхности сферы. Векторы весов большинства нейроновочень удалены от входных векторов и никогда не дадут лучшего соот-ветствия, т.е. не будут участвовать в алгоритме. Оставшихся, попавшихв группы входных векторов, будет недостаточно для правильного раз- деления x i на классы. Очевидным решением является инициирование весов, при помещении большей их части в скопления входных векто-ров. Такую возможность дает метод выпуклой комбинации. Все веса нейронной сети делаются равными 1ijwN= , где N – раз- мерность входного вектора. Входные векторы модифицируются по пра- вилу (1 ) /iixx N=α + −α . Начальное значение коэффициента α → 0, поэтому все входные векторы сосредоточены в одной области с весами wij и имеют вид 1ixN≈ . Затем, в процессе обучения α увеличивается до 1, что возвращает входным векторам их истинные значения и трени- рует сеть Кохонена на них. Метод несколько замедляет обычную про- цедуру обучения, но дает лучшие результаты. На основе сети Кохонена строятся более сложные структуры, назы- ваемые самоорганизующиеся сети – SOM [15]. SOM строится из не-скольких слоев, построенных по принципу сети Кохонена. Чаще всего SOM выстраиваются в решетчатую структуру. Выбор ближайшего ней- рона осуществляется по (2.21). Но аккредитируется, то есть обучаетсяпо (2.22), не только он, но и его соседи в окрестности R. Величина R на первых порах работы алгоритма обучения очень большая, но постепен- но уменьшается до 0. Результатом работы будет не один выходной ней- рон, а группа нейронов, соответствующая каждому классу. Это позво-ляет осуществлять более точную классификацию входных образов. Пример использования самоорганизующейся карты признаков Одно из важнейших свойств обученной сети Кохонена – способ- ность к обобщению. Вектор каждого из нейронов сети заменяет группусоответствующих ему классифицируемых векторов. Это позволяет ис-\n--- Страница 29 ---\nГлава 2. Сети прямого распространения 29 пользовать данный вид сети в области сжатия данных. Покажем это на примере компрессии изображения. Входное изображение высотой Nx пикселей и шириной Ny пикселей разбивается на более мелкие кадры размером nx × ny пикселей. Далее значения компонент красного, синего и зеленого цвета каждого кадраслужат обучающим набором для сети Кохонена. После самообучения на вход сети последовательно поступают все кадры исходного изображе- ния. Веса нейрона-победителя определяют значения новых компонентосновных трех цветов кадра. На рис. 2.10 представлено изображение200 на 345 пикселей. Для компрессии использовалась сеть из 150 ней- ронов. Рис. 2.10. Исходное изображение После обучения было получено сжатое изображение (рис. 2.11). Можно заметить, что различия между ними невелики. Рис. 2.11. Сжатое изображение",
      "debug": {
        "start_page": 14,
        "end_page": 29
      }
    },
    {
      "name": "Глава 3 . Рекуррентные нейронные сети",
      "content": "--- Страница 30 --- (продолжение)\nГлава 3 РЕКУРРЕНТНЫЕ НЕЙРОННЫЕ СЕТИ 3.1. Сети Хопфилда У сетей прямого распространения: многослойного персептрона и се- тей радиально-базисных функций, как следует из названия, обратныхсвязей нет, то есть сигнал всегда распространяется в направлении отвходов к выходам сети. В рекуррентных нейронных сетях появляются так называемые обратные связи от выходов нейронов обратно на входы. За счет обратных связей выходы всей сети при одних и тех же входахсети могут принимать произвольные состояния в различные моментывремени и не обязательно стабилизируются на одном [13]. Общая схема работы рекуррентных нейронных сетей (РНС) Рассмотрим отдельный нейрон с обратной связью. На первом этапе работы (рис. 3.1) на вход нейрона подаются входные значения xj и вы- числяется выход нейрона z. Затем получившееся выходное значение подается на вход нейрона на-ряду с прочими значениями и вычисляется новое выходное значение. Этот процесс повторяется до тех пор, пока выходное значение нейрона будетмало изменяться от итерации к итерации. Есливернуться к нейронным сетям, то можно ввестиследующую классификацию. Рекуррентные ней- ронные сети, для которых возможно получить стабилизирующиеся к определенному значению выходы, называются устойчивыми, а есливыходы сети не стабильны, то неустойчивыми. В общем случае, боль- шая часть рекуррентных нейронных сетей являются неустойчивыми. Неустойчивые сети мало пригодны для практического применения, и поэтому в рамках данной монографии не рассматриваются.zz xi Рис. 3.1. Нейрон с обратной связью\nГлава 3 РЕКУРРЕНТНЫЕ НЕЙРОННЫЕ СЕТИ 3.1. Сети Хопфилда У сетей прямого распространения: многослойного персептрона и се- тей радиально-базисных функций, как следует из названия, обратныхсвязей нет, то есть сигнал всегда распространяется в направлении отвходов к выходам сети. В рекуррентных нейронных сетях появляются так называемые обратные связи от выходов нейронов обратно на входы. За счет обратных связей выходы всей сети при одних и тех же входахсети могут принимать произвольные состояния в различные моментывремени и не обязательно стабилизируются на одном [13]. Общая схема работы рекуррентных нейронных сетей (РНС) Рассмотрим отдельный нейрон с обратной связью. На первом этапе работы (рис. 3.1) на вход нейрона подаются входные значения xj и вы- числяется выход нейрона z. Затем получившееся выходное значение подается на вход нейрона на-ряду с прочими значениями и вычисляется новое выходное значение. Этот процесс повторяется до тех пор, пока выходное значение нейрона будетмало изменяться от итерации к итерации. Есливернуться к нейронным сетям, то можно ввестиследующую классификацию. Рекуррентные ней- ронные сети, для которых возможно получить стабилизирующиеся к определенному значению выходы, называются устойчивыми, а есливыходы сети не стабильны, то неустойчивыми. В общем случае, боль- шая часть рекуррентных нейронных сетей являются неустойчивыми. Неустойчивые сети мало пригодны для практического применения, и поэтому в рамках данной монографии не рассматриваются.zz xi Рис. 3.1. Нейрон с обратной связью\n--- Страница 31 ---\nГлава 3. Рекуррентные нейронные сети 31 В 1982 г. Д. Хопфилд предложил устойчивую РНС следующего ви- да (рис. 3.2). Сеть является биполярной, то есть оперирует только ве- личинами {–1, 1}. В сети имеется один слой настраиваемых весов wji. xd x2 x1wij слой (0)zm z2 z1 Рис. 3.2. Сеть Хопфилда Все нейроны единственного слоя возвращают свои выходы на свой вход и входы всех остальных нейронов сети посредством распределителей (не нейронов) слоя (0). Каждый нейрон реализует следующие шаги: 1) вычисляет взвешенную сумму a своих входов: ()M jj i i j ijaw z x ≠=+∑ ; (3.1) 2) к сумме применяется нелинейная пороговая функция g(a): 1, () 1 , не меняется,jj jj j j jaT zg a a T aT> ⎧⎫⎪⎪== −< ⎨⎬ ⎪⎪ =⎩⎭, jT – порог нейрона. (3.2) Сеть Хопфилда функционирует следующим образом. Входные сиг- налы подаются один раз в начале работы, затем сеть работает только за счет возвращенных выходов. Состояние всех нейронов сети изменяетсяодновременно. В конце работы сеть стабилизируется в устойчивомсостоянии. Так как сеть Хопфилда работает с биполярными числами,то состояние ее выходов всегда можно описать бинарным числом z j ∈ {–1, 1}. Существует простая геометрическая интерпретация. Для системы из трех выходов состояние сети описывается трехзначным биполярным числом, которое может иметь 23 = 8 состояний. Пусть каждое состояние\n--- Страница 32 ---\n32 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ сети – это вершина куба в пространстве. Для n выходов соответственно существует 2n состояний и n-мерный гиперкуб. Когда подается новый входной вектор, сеть начинает переходить из вершины в вершину, пока не стабилизируется в какой-либо вершине. Устойчивая вершина z для данного входа x задается матрицей на- страиваемых весов wij и порогами Tj. Если входной вектор x частично подвергнуть искажению, то сеть Хопфилда приходит именно в состоя- ние z, то есть можно говорить, что сеть Хопфилда обладает свойством восстановления запомненного состояния исходя из неполной информа-ции. Устойчивость сети Хопфилда Сеть Хопфилда гарантированно будет устойчивой при выполнении следующих условий: 1) матрица весов W симметрична wij = wji; 2) имеет нули на главной диагонали wii = 0, ∀i. Приведем упрощенное доказательство. Рекуррентную нейронную сеть можно рассматривать как динамическую систему, имеющую неко- торое энергетическое состояние. Если описать энергию системы функ-цией E и показать, что E при любом изменении состояния всегда убыва- ет или не изменяется, то система (нейронная сеть) будет по определе- нию устойчивой. Используя условия 1 и 2, можно описать энергетиче- ское состояние системы E функцией Ляпунова: 1 2jij i jj jj ij j jEw z z x z T z=− − +∑∑ ∑ ∑ . (3.3) Здесь wij – веса, zj – выходное значение нейрона j, x j – j-й элемент вход- ного вектора, Tj – порог нейрона j. Изменение энергии Ej, вызванное из- менением состояния zj нейрона j, есть ()j ji i j j j ijEw z x T z ≠∆= − +− ∆ ⎡⎤ ⎢⎥⎣⎦∑ . Используя формулы (3.1) и (3.2), это выражение можно переписать: ()j jjj Ea T z∆= − − ∆ . (3.4) Возможны три варианта: 1) aj > Tj, ∀j. Отсюда следует, что выражение в скобках ( ) > 0, так- же подставляя это условие в (3.2), видно, что ∆ zj изменится в положи- тельную сторону ∆ zj > 0 либо не изменится. В итоге ∆ E ≤ 0 убывает или стабилизируется;\n--- Страница 33 ---\nГлава 3. Рекуррентные нейронные сети 33 2) aj < Tj, ∀j. Отсюда следует, что выражение в скобках ( ) < 0, так- же подставляя это условие в (3.2), ∆ zj изменится в отрицательную сто- рону ∆ zj ≤ 0, либо не изменится. В итоге ∆ E ≤ 0 убывает или стабилизи- руется; 3) aj = Tj, ∀j. Следовательно, ∆ E = 0, то есть стабилизируется. Таким образом, любое изменение состояний сети уменьшает энер- гию системы, и сеть Хопфида является устойчивой. 3.2. Ассоциативная память. ДАП Компьютерная память является адресуемой, то есть работает по принципу: сначала предъявляется адрес, а устройство хранения возвра- щает информацию, содержащуюся по данному адресу. Если сформиро-ван неверный адрес, то будут получены совершенно произвольные дан-ные, которые будут ошибочно интерпретированы как требуемые дан-ные. Человеческая память является ассоциативной: мозг воспринимает какую-то информацию (например имя человека) и в ответ возвращает целую гамму воспоминаний (внешность, место, эмоции и т.п.), то есть,задавая некоторую часть информации, получаем всю остальную. Оче-видно, что ассоциативная память имеет очень сложную внутреннюю структуру зависимостей и связей образов. Было показано, что сети Хопфилда могут формировать упрощенную модель ассоциативной па-мяти [9]. Алгоритм работы ассоциативной памяти Ш0. Первоначально все запоминаемые образы xj, j = 1, , M кодиру- ются биполярными векторами длины N. Ш1. Затем веса сети Хопфилда настраиваются следующим образом: 1M ji ij d d dwx x ==∑ , где вектор 12(, , . . . , )N dd ddxx xx= . (3.5) Фактически вычисляется произведение каждого запоминаемого вектора с самим собой и суммированием матриц, полученных таким образом. Ш2. После запоминания образов становится возможным восстанов- ление ассоциаций. Входам придают значение образа, возможно частич-но искаженного, и сети предоставляется возможность колебаться до своего устойчивого состояния. Сеть в конечном итоге стабилизируется\n--- Страница 34 ---\n34 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ в одном из запомненных состояний. Значения выходов при этом и есть восстановленная ассоциация. Следует заметить, что если входной век- тор искажен сильно, то результат может быть неверным. Особый интерес представляет емкость сети. Теоретически можно за- поминать 2N образов, но реально получается много меньше. Экспери- ментально было получено, а затем теоретически показано, что в сред- нем для слабо коррелирующих образов сеть из N нейронов в состоянии запомнить ≈ 0,15 N образов и в идеальном случае – не больше N. Строго говоря, ассоциативная память, реализуемая сетью Хопфилда, является автоассоциативной: образ, хранящийся в сети, может быть восстановлен по искаженному входу (этому же вектору), но не ассоции- рован с произвольным другим образом. Пример использования сети Хопфилда З а д а ч а: Существует несколько черно-белых символов-эталонов, размер каждого из которых 6 × 6 пикселей, требуется по введенномузашумленному символу восстановить его до наиболее подходящегоэталона. Примеры символов эталонов представлены на рис. 3.3. Рис. 3.3. Эталоны символов Кодирование эталонов происходило следующим образом: каждому из пикселей эталона соответствовала координата вектора, принимаю-щая значение 1, если пиксель имел черный цвет, и значение –1 в про- тивном случае. По окончании запоминания сетью Хопфилда эталонов на вход сети для проверки функционирования были предъявлены зашумленные об-разы, по завершении всех вычислений был получен результат, пред-ставленный на рис.3.4. В 1980 – 83 гг. С. Гроссберг опубликовал серию работ по так назы- ваемой двунаправленной ассоциативной памяти. В этом случае проис- ходит гетерассоциация двух векторов A и B.\n--- Страница 35 ---\nГлава 3. Рекуррентные нейронные сети 35 Входной образ Результат Рис. 3.4. Результаты восстановления Двунаправленная ассоциативная память (ДАП) фактически пред- ставляет собой объединение двух слоев сети Хопфилда (рис. 3.5.). Функционирование нейронов сети ДАП совпадает со схемой работы се- ти Хопфилда. Слой (2) Слой (1) Слой (0)B AW WT z2 z1 x2 x1zm xd Рис. 3.5. Двунаправленная ассоциативная память\n--- Страница 36 ---\n36 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Алгоритм работы ДАП Ш0. Для удобства будем работать в матрично-векторной форме. Ас- социируемые образы A и B кодируются двоичными векторами длины N. Ш1. Затем веса сети ДАП задаются по формуле ()T ii iWA B=∑ , где вектор 12( , , , )d AAA A= ; (3.6) Ш2. Для восстановления ассоциации B входной вектор A подается на входы слоя (1) и обрабатывается матрицей весов W, в результате на вы- ходе слоя должен формироваться вектор B. В матричном виде это вы- глядит так: ()Bg A W= , где ()g – пороговая функция (3.2) ; (3.7) Ш3. Входной вектор слоя (2) B обрабатывается матрицей весов WT, в результате на выходе слоя (2) и формируется вектор A, который затем передается на входы слоя (1): ()TAg B W= , где ()g – пороговая функция (3.2); (3.8) Ш4. Шаги Ш2 и Ш3 повторяются до тех пор, пока сеть ДАП не ста- билизируется, то есть ни вектор A, ни вектор B не изменятся от итера- ции к итерации. В результате значение вектора B будет искомой ассо- циацией вектору A. Было показано, что сеть ДАП, подобно сети Хопфилда, является ус- тойчивой сетью, т.е. функция энергии Ляпунова либо уменьшается, ли-бо не изменяется. Емкость памяти ДАП, представленной сетью с N нейронами в мень- шем слое, можно оценить следующей величиной: 2logNLN< , где L – емкость память ДАП. (3.9) К сожалению, емкость ДАП невысока, для N = 1024 нейронов сеть будет в состоянии запомнить L < 25 образов. По деталям практической реализации ДАП можно также обратиться к [16]. Пример использования ДАП На рис. 3.6 представлены связанные пары векторов (первый вектор xi – образ самолета, второй yi – его название). Размерности xi равны 120, а векторов yi – 145. После определения всех параметров сети, произво- дится тестирование – на вход подаются зашумленные образы.\n--- Страница 37 ---\nГлава 3. Рекуррентные нейронные сети 37 Первый векто р Второй векто р Рис. 3.6. Ассоциированные пары векторов Результат работы сети представлен на рис. 3.7. Входной образ Результат восстановления Рис. 3.7. Восстановление эталона и образа, ассоциированного с ним 3.5. Теория адаптивного резонанса Человеческий мозг постоянно обрабатывает данные из окружающего мира. Именно основываясь на предыдущем опыте и воспринятой ин-формации, мозг принимает решения. При этом постоянно возникает не-обходимость запоминать и усваивать новые факты. Мозг обладает за- мечательной способностью запоминать новое так, что ранее запомнен- ное не искажается и не забывается. В теории нейронных сетей подобнаяспособность сети воспринимать новые знания без потери прежних на-зывается дилеммой стабильности-пластичности.\n--- Страница 38 ---\n38 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Традиционные нейронные сети не обладают свойством пластично- сти. Более того, процесс обучения и использования практически всегда разделен. Новый обучающий образ требует переобучения всей сети (рис. 3.8). Фактически, при работе «вреальном времени» сеть будет непре-рывно обучаться, то есть бездейство- вать. Результатом исследования этой проблемы стала теория адаптивного ре-зонанса (Adaptive Resonance Theory –ART) , созданная Гроссбергом в 1986 г. В основе теории ART лежит фено- мен фонетического восстановления.Например, если мы слышим шум и об-рывок фразы: « чка на », то смыслфразы ускользает. Если ее завершить чем-то вроде: « чка на плоскости», то дополнительная информация порожда- ет фокусировку – ожидание определенного понятия. В результате изсигнала « чка на плоскости» имеем «точку на плоскости». Говорят, что произошел резонанс образа в памяти с поступившим сигналом. В случае, если резонанса не наступило, то считаем поступившую инфор-мацию новыми данными и запоминаем как отдельный образ. Так, фраза« чка на память», конечно после дополнительного уточнения, может потом восстанавливаться как «фотокарточка на память». Теория адаптивного резонанса существует в двух парадигмах: ART-1, которая работает с бинарными числами, и более сложной реализацииART-2 для работы с вещественными числами. Мы будем рассматривать простой для понимания вариант ART-1, предложенный Липпманом в 1987 г. [17]. Структура ART-сети Рассмотрим структуру и схему работы ART-сети (рис. 3.9). Сеть ART состоит из двух основных слоев нейронов: слоя распознавания F1 и слоя сравнения F2. Два слоя F1 и F2 соединены настраиваемыми весами tji и bij. Веса bij, идущие «снизу вверх» – от выходов нейронов слоя F1 к вхо- дам слоя F2, называются долговременной памятью. Веса tji, идущие «сверху вниз» – от выходов слоя F2 к входам слоя F1, называются ожи- даемыми стимулами. Значение si выходов слоя F1 – это результат обра-– Новый x2x1 Рис. 3.8. Новый обучающий образ\n--- Страница 39 ---\nГлава 3. Рекуррентные нейронные сети 39 ботки нейронами входов xi и сигналов «сверху вниз» от F2 Вместе с ПУ – подсистемой управления – выходы si называются кратковременной памятью и, собственно, отвечают за стабильность-пластичность. N ПУПУ si xibji F1F2 tji +–ОСДПСигнал сброса Слой сравнения ПОM yjСлой распознавания Рис. 3.9. Структура ART-сети Выходы yi нейронов слоя F2 будут представлять собой результат об- работки входных сигналов нейронами слоя F2 и взаимодействия с ПО – подсистемой ориентации. Слой F2 осуществляет классификацию вход- ных векторов. Слой функционирует по принципу «победитель забирает всё»: только один нейрон, имеющий вектор весов bij, наиболее соответ- ствующий по признакам входному вектору, возбуждается, а все осталь-ные нейроны затормаживаются (п. 2.5). Этот эффект достигается за счет специальных тормозящих связей (рис. 3.10). От нейрона с высоким выходным значением на все нейроны слоя подается полученное высокое значение, но со знаком «–», что при-нудительно затормаживает остальные нейроны. Дополнительно нейрон-победитель усиливает себя положительной обратной связью. + yj– ––– Рис. 3.10. Тормозящие связи\n--- Страница 40 ---\n40 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Схема работы ART-сети Рассмотрим три возможных состояния ART-сети: 1) если входной вектор, поданный на слой F1, максимально соответ- ствует сигналам «сверху-вниз» (возникает резонанс) , т.е. похож на век- тор весов bij одного из нейронов слоя F2 (нейрона-победителя), то гово- рят, что входной сигнал соответствует ожиданиям системы и система приходит в устойчивое состояние, определив класс входного вектора.При этом автоматически происходит дообучение сети, с устранениемслабых различий запомненного и входного образов. Получается, что впроцессе работы ART-сети запомненный образ постоянно уточняется; 2) в случае, если входной образ, поданный на F 1, не соответствует сигналам «сверху-вниз», то ПО подавляет нейрон-победитель, блокиру-ет все его связи и инициирует повторную подачу входного вектора. По-добный поиск среди запомненных образов повторяется до тех пор, пока мы не добьемся эффекта резонанса или не превысим некоторый предел поиска. Если в результате поиска резонанс не был достигнут, то полу-чается третий вариант; 3) входной образ не соответствует ожиданиям системы, и в слое F 2 достаточно нейронов, не задействованных для определения классов входных векторов. В этом случае ПУ заставит нейроны в F2 настроить- ся на новый класс входных образов. Процесс настройки проходит попроцедуре обучения «без учителя». Формально алгоритм работы ART-сети выглядит следующим обра- зом: Алгоритм работы ART-сети Вводится дополнительный параметр – параметр сходства p, опреде- ляющий чувствительность сети к шуму и ответственный за созданиеновых классов образов. Ш0. Проводится начальная инициализация весов: t ji = 1, i = 1, , N, j = 1, , M; все входные векторы приводятся в равные условия bij = L/(L – 1 + N). Здесь L ≥ 1 (обычно L = 2), N – число нейронов в слое сравнения F1, a M – число нейронов в слое распознавания F2; Ш1. На вход слоя F1 подается входной вектор 1( , , )N xx x= ; Ш2. Вычисляются выходы слоя F2: 1N j ij i iy bx ==∑ , 1 , ,jM= ; (3.10)\n--- Страница 41 ---\nГлава 3. Рекуррентные нейронные сети 41 Ш3. Выбирается лучший по соответствию входному вектору нейрон yk слоя F2. Такой, что max{ }kjjy y = ; (3.11) Ш4. Нейрон-победитель тестируеся на сходство: 1 1N ik i i N i itx x= =µ=∑ ∑. (3.12) Здесь µ дает такую долю единиц во входном векторе, которая имеет- ся и в запомненном в нейроне yk образе. Если µ > p, сходство считаем достаточным для идентификации и далее на Ш6. Иначе считается, что поступил новый входной образ, переход на Ш5. Ш5. Временно блокируется лучший по соответствию нейрон yk, т.е. yk = 0, и переход на Ш3. Причем на данном шаге этот нейрон остается заблокированным (не учитывается); Ш6. В yk получен результат работы сети ART – определенный класс входного вектора. Происходит настройка нейрон-победителя . Если вы- полнен переход с Ш5, то добавляется незадействованный нейрон: ik ik itt x= ; (3.13) 10,5ik i ik N ik i itxb tx == +∑; (3.14) Ш7. Нейрон, запрещенный на Ш5, снова делается активным, и алго- ритм снова начинается с Ш1. Заметим, что выхода из алгоритма нет, т.е процесс обучения нераз- рывно связан с процессом работы ART-сети. Липпман показал, что данный алгоритм хорошо работает при незна- чительном уровне шума. При высокой зашумленности приходится де-лать параметр сходства p очень малым, иначе два подобных входных образа будут отнесены к разным классам. Существуют усовершенство- вания алгоритма ART-1, когда p является переменной величиной, но за это приходится платить сложностью реализации и увеличением време-ни выполнения.\n--- Страница 42 ---\n42 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Пример обучения ART-сети На этапе 1 подается символ «С». Так как для него отсутствуют за- помненные образы, то выделяется нейрон в слое F2, веса tji которого подстраиваются соответствующим образом (рис. 3.11). Аналогично для «В» и «Е»: так как эти символы не вступают в резонанс с уже запомненной «С», то под них выделяются новые нейроны (этапы 2 и 3). Приподаче символа «Е», но другого шрифта, он оп-ределяется как уже существующий в системе.Причем происходит корректировка запомненно- го образа (этап 4). Если подается искаженный символ «Е», для которого η < p, параметр сход- ства недостаточно мал, и для искаженного сим- вола выделится новый нейрон (этап 5). Отсюда видно, что правильный выбор параметра сходства p очень важен. К сожалению, теоретического обоснования для выбора p не существует. Сеть ART является представителем рекуррентных нейронных сетей, которые являются в общем случае неустойчивыми. Для ART-сети было доказано несколько теорем, показывающих, что она будет устойчивой. Теорема: процесс обучения ART-сети конечен и устойчив. Любая последовательность произвольных входных векторов будет произво-дить стабильный набор весов после конечного количества обучающихсерий. Повторяющиеся последовательности входных векторов не будут приводить к циклическим изменениям. ART-сеть не лишена недостатков. Она очень чувствительна к шу- му. Также имеет место связность всех запомненных образов в t ji и bij: при потере одного фактически будут потеряны все остальные. Голов- ной мозг, напротив, в состоянии что-то помнить и при частичной по- тере памяти. Сеть ART имеет ограниченную числом нейронов емкостьпамяти.1) 2) 3) 4) 5) С В С В С В Е С В С В ЕЕ Рис. 3.11. Пример обучения ART-сети",
      "debug": {
        "start_page": 30,
        "end_page": 42
      }
    },
    {
      "name": "Глава 4 . Эффективные нейронные сети",
      "content": "--- Страница 43 --- (продолжение)\nГлава 4 ЭФФЕКТИВНЫЕ НЕЙРОННЫЕ СЕТИ Наличие разных типов нейронных сетей, алгоритмов их обучения для различных типов задач не гарантирует автоматически, что создан-ная нейронная сеть будет решать эффективно ту или иную проблему.Рассмотрим несколько методик, помогающих в создании и тренировке эффективных нейронных сетей. 4.1. Обработка данных При использовании нейронных сетей на реальных данных предвари- тельная обработка бывает полезной практически всегда. В общем видепроцесс обработки состоит в этапе предобработки данных перед их по- дачей нейронной сети. Предобработка предполагает приведение данных в удобный для об- работки нейронной сетью вид. В некоторых случаях требуется этап по-следующей обработки выходов нейронной сети для восстановления первоначального вида данных (рис. 4.1). Самой простой и эффективной формойпредварительной обработки является снижениеразмерности входных данных. Нейронная сеть сменьшим числом входов требует более простую топологию и соответственно быстрее обучается и работает. Очевидно, что уменьшение размерностиведет к потере части информации, поэтому оченьважно выбрать стратегию предобработки так, что- бы оставить как можно больше важной информа- ции и отбросить только малосущественную. По-добный подход реализуется мощной методикойснижения размерности, носящей название анализ основных компонент.Постоб работка ДанныеПредобработка Рис. 4.1. Обработка данных\nГлава 4 ЭФФЕКТИВНЫЕ НЕЙРОННЫЕ СЕТИ Наличие разных типов нейронных сетей, алгоритмов их обучения для различных типов задач не гарантирует автоматически, что создан-ная нейронная сеть будет решать эффективно ту или иную проблему.Рассмотрим несколько методик, помогающих в создании и тренировке эффективных нейронных сетей. 4.1. Обработка данных При использовании нейронных сетей на реальных данных предвари- тельная обработка бывает полезной практически всегда. В общем видепроцесс обработки состоит в этапе предобработки данных перед их по- дачей нейронной сети. Предобработка предполагает приведение данных в удобный для об- работки нейронной сетью вид. В некоторых случаях требуется этап по-следующей обработки выходов нейронной сети для восстановления первоначального вида данных (рис. 4.1). Самой простой и эффективной формойпредварительной обработки является снижениеразмерности входных данных. Нейронная сеть сменьшим числом входов требует более простую топологию и соответственно быстрее обучается и работает. Очевидно, что уменьшение размерностиведет к потере части информации, поэтому оченьважно выбрать стратегию предобработки так, что- бы оставить как можно больше важной информа- ции и отбросить только малосущественную. По-добный подход реализуется мощной методикойснижения размерности, носящей название анализ основных компонент.Постоб работка ДанныеПредобработка Рис. 4.1. Обработка данных\n--- Страница 44 ---\n44 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Анализ основных компонент Рассмотрим линейное преобразование вектора x = (x1, , xd) из обу- чающей выборки { xn} в новый вектор z = (z1, , zM) В M-мерном про- странстве, причем M < d. Вектор x может быть представлен как линей- ная комбинация ортонормальных векторов ui: 1d ii ixz u ==∑ . (4.1) Оставим только M базисных векторов ui и соответственно M коэф- фициентов zi. Остальную часть коэффициентов мы заменим константа- ми bi: 11Md ii ii ii Mxz u b u == +=+∑∑ /tildenosp . (4.2) Рассмотрим весь обучающий набор { xn}, где n = 1, , N. Необходимо выбрать базисные векторы ui и коэффициенты bi так, чтобы приближе- ние (4.2) давало лучший результат для всего набора данных. Ошибка при этом будет выглядеть следующим образом: 22 11 111() ( )22NN d nn n ii nn i MEx x z b == = +=− = −∑∑ ∑ /tildenosp . (4.3) Можно показать, что минимум ошибки E в отношении базисных векторов ui будет найден при минимизации величины 11 2d i iME =+=λ∑ . (4.4) Здесь величины λ i являются собственными значениями базисных векторов ui, которые соответственно будут собственными векторами. Таким образом, минимум ошибки достигается отбрасыванием d – M са- мых малых собственных значений и соответствующих им базисов ui, т.е. собственных векторов. Практически метод анализа основных компонент реализуется сле- дующим образом. Вычисляется матрица ковариаций обучающей выбор- ки () ()() ()nn T nKx x x x=− −∑ – это среднее значение по всей обучаю- щей выборке. Затем находятся собственные векторы и собственные значения матрицы K. Согласно (4.4) оставляются собственные векторы,\n--- Страница 45 ---\nГлава 4. Эффективные нейронные сети 45 соответствующие наибольшим собственным значениям K. После этого проецируются векторы из { xn} на оставленные собственные векторы ui. x2 x1u1 u2 Рис. 4.2. Пример метода анализа основных компонент В результате получен трансформированный вектор zn меньшей раз- мерности в пространстве размерности M, минимально отличающийся от xn. В численном виде ошибку преобразования E можно найти по (4.4). На рис. 4.2 каждый из двумерных векторов х должен быть трансформи- рован в одномерный z проекцией на собственный вектор ui. Нормализация входных данных Одной из самых распространенных форм предварительной обработки входов нейронной сети является нормализация данных. Очень часто бы- вает, что параметрами обучающего вектора могут быть семантическиразличные значения, например, x – возраст, вес, температура, давление пациента. В зависимости от шкалы измерения численные значения этих величин могут различаться в несколько раз. Причем большие значения ни коим образом не являются более значимыми относительно малых. Применяя простое линейное преобразование, можно выровнять от- носительные значения для всех компонент входного вектора. Для этогодля каждой компоненты входного вектора независимо рассчитываются среднее значение ix и дисперсия 2 iσ: 11N n ii ixxN==∑ ; (4.5) 22 11()1N n ii i ixxN=σ= −−∑ . (4.6)\n--- Страница 46 ---\n46 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Здесь 1( , , )nnn N xxx= – вектор из обучающей выборки { xn}. Используя (4.5) и (4.6), получим новый входной вектор 1( , , )nnn N xxx=/tildenosp/tildenosp/tildenosp , где n n ii i ixxx−=σ/tildenosp . (4.7) Таким образом, новое преобразованное значение x/cohγdndnъtdT≈MaccMntn будет иметь ну- левое среднее значение и единичную дисперсию. 4.2. Оптимизация процесса обучения Одной из часто используемых методик улучшения результатов обу- чения нейронной сети является добавление к определению функцииошибки сети так называемого регулирующего параметра: EE=+ γ Ω/tildenosp . (4.8) Здесь E – среднеквадратическое отклонение, a Ω – регулирующий па- раметр, γ – коэффициент, определяющий степень его влияния на ошиб-ку Е/cohγdndnъtdT≈MaccMnt. Существует много способов задания регулирующего параметра, рассмотрим один из самых простых – метод понижения весов. В 1987 г.Г. Хинтон показал, что регулирующий параметр следующего вида су- щественно может улучшить работу сети [18]: 2 1 2ijw Ω=∑ . (4.9) Некоторое улучшение результата работы нейронной сети можно объяснить следующим образом. Для получения аппроксимации функ-ции с большим «стробированием» восстановленной функции требуютсябольшие значения весов. Параметр вида (4.9) поощряет веса сети при- нимать небольшие значения, и аппроксимация функции будет более гладкая. Ранняя остановка обучения Другим эффективным методом, улучшающим процедуру обучения, является так называемая ранняя остановка обучения [19]. Метод позволяет улучшить результаты работы нейронной сети за счет устранения эффекта ее переобучения. Этот эффект заключается в том, что на реальных задачах с присутствием шума в обучающей вы-\n--- Страница 47 ---\nГлава 4. Эффективные нейронные сети 47 борке нейронная сеть начинает настраиваться не только на определение необходимых закономерностей в данных, но и на шум и искажения, что сильно ухудшает результат ее работы. Суть метода заключается в том, что вся обучающая выборка { xn} разбивается на три набора: { xtrain} – обучающий, { xvalid} – проверочный и {xtest} – тестовый. При обучении нейронной сети для настройки весов используется обучающий набор {xtrain}. Ошибка обучения сети вычисляется по набору { xvalid}. Получается, что проверка эффек- тивности сети производится на независимых данных. В моментобучения, когда сеть начинает на-страиваться на шум, присутст-вующий только в выборке { x train}, ошибка, вычисляемая по совер- шенно другим данным, начинаетрасти (рис. 4.3). Именно в этотмомент t 0, когда сеть работает наилучшим образом и имеет смысл прекратить процесс обучения сети. Необязательный наборданных { x test} служит для окончательной проверки эффективности ее работы. Комитет сетей Обычной практикой пр и решении задачи нейронными сетями явля- ется построение и обучение нескольких различных нейронных сетей, из которых затем выбирается для использования лучшая по производи-тельности сеть. У такого подхода есть очевидные недостатки, а именно:все усилия, направленные на построение многих сетей, пропадаютвпустую. Второй, более серьезный недостаток – это отсутствие гаран- тий, что сеть, наилучшая по производительности на одних тестовых данных, будет также хороша на других. Как решение поставленной проблемы М. Перррон и Л. Купер в 1993 г. предложили использовать комитет сетей [20]. Можно формаль- но показать, что при использовании нескольких нейронных сетей ошибка существенно уменьшается.ОбучениеПроверка t0Ошибка Время обученияt Рис. 4.3.: Изменение ошибки\n--- Страница 48 ---\n48 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Предположим, имеется набор из L произвольных нейронных сетей. Задача – построить аппроксимацию функции y(x), тогда каждая сеть i должна будет реализовывать функцию h(x), такую, что () () ()iiyx h x x =+ ε . Среднеквадратическую ошибку отдельной сети можно записать как 22( ( () () )) ( ( () ))ii i iEe y xh x e x=−= ε , где e[·] – это среднее значение 22(( ( )) ) ( ( )) ( )iiex x p x d xε= ε ∫. Отсюда средняя ошибка по всем сетям в отдельности равна 2 cp 11(( ( )) )L i iEe xL==ε∑ . (4.10) Простейшей формой реализации комитета сетей будет усреднение значений выходов всех L сетей: 11()L i iy yxL==∑ . (4.11) Отсюда ошибка комитета сетей будет равна 22 1111[ ( () () )] [ ( )]LL ii iiEe y x h x eLL===− = ε∑∑ . (4.12) Если предположить, что ошибки ε i имеют нулевое среднее e(ε i) = 0 и не коррелируют e(ε iεj) = 0, то ошибку комитета сетей можно переписать как 2 cp 2 111()L i iEeEL L==ε =∑ . (4.13) Следовательно, ошибка комитета сетей в идеальном случае будет в L раз меньше ошибки отдельных нейронных сетей. Но в действительно- сти предположения о некоррреляции ε i и e(ε i) = 0 не выполняются, по- этому получаем просто E < Eср. В этой главе рассмотриваются известные методы и подходы, исполь- зуемые для построения нейронных сетей. В следующем разделе дается краткий обзор теоретических оценок эффективности их работы. В раз- деле, посвященном конструктивным алгоритмам, приведен довольнополный, на наш взгляд, ряд алгоритмов построения нейронных сетей.В заключение главы детально будут рассмотрены методики эволюци- онного подхода к их построению.\n--- Страница 49 ---\nГлава 4. Эффективные нейронные сети 49 4.3. Критерии эффективности нейронных сетей Наличие очевидной взаимной зависимости между сложностью мо- дели, размерами обучающей выборки и результирующей обобщающей способностью модели на независимых данных предполагает возмож-ность определения этой зависимости тем или иным способом [21].Иными словами, для нахождения метода построения эффективных ней- ронных сетей нам необходимо определиться с возможными способами оценки обобщающей способности сети ˆ.P Существующие оценки можно условно разделить на валидационные и алгебраические. Валидационные оценки основаны на дополнительном анализе данных. Алгебраический способ построения основан на выпол-нении некоторых предположений о распределении данных. Валидационные оценки Простейшим случаем валидационной оценки является разбиение всей доступной для обучения выборки данных на обучающую и прове- рочную выборки. Обучающая выборка используется в процедуре обу- чения сети, а проверочная – только для вычисления среднеквадратиче-ской ошибки модели. Тем не менее проверочная выборка является ча-стью процедуры настройки сети, поскольку она через значение ошибки будет определять выбор сети. Данный подход эффективен в случае, ес- ли имеется большое количество примеров, достаточное для образованиядвух наборов. Выбор примеров для проверочного набора всегда будет существен- ен. Метод кросс-валидации (KB) предполагает усреднение эффекта выбора определенного проверочного набора путем построения и по- переменного использования на всей доступной выборке данных не-скольких поднаборов [19]. Весь набор разбивается на Q поднаборов S j размера V каждый, так что N = QV. Обучается Q различных вариантов сети, реализующих оценки ˆ(, )jfxD на всей выборке, исключая пооче- редно Sj. Таким образом, оценка обобщающей способности сети при- мет вид 2 11ˆ ˆ() ( ( , ) ) jVN CV j k k jk SPxf x D yN=∈=−∑∑ . (4.14) Наиболее известен вариант KB без одного (leave-one-out) с V = 1:\n--- Страница 50 ---\n50 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ 2 LOO 11ˆ ˆ() ( ( , ) )N jk k kPxf x D yN==−∑ . (4.15) Использование (4.15) предполагает обучение каждой из N дополни- тельных нейронных сетей на N – 1 примерах. Фактически это означает расширение пространства поиска параметров сети в пределах выбран- ного класса топологий сети Ai ⊂ A. Алгебраические оценки В статистике известна зависимость между ожидаемой ошибкой на обучающей и тестовой выборках для линейных моделей [22] при сред-неквадратическом определении ошибки: 2 test train 2PEEN=+ σ . (4.16) Здесь P – число свободных параметров модели. В случае нейронных се- тей число свободных параметров определяется архитектурой (числом слоев L, количеством нейронов сети γ и набором весов связей между ней- ронами сети W. Предполагается, что данные выборки D зашумлены неза- висимым стационарным шумом с нулевым средним и дисперсией σ2. Как было показано Барроном в [22], (4.16) может быть применено к случаю нелинейных моделей при выполнении нескольких условий. Модель ˆˆ(, )fxDθ может рассматриваться как локально линейная в ок- рестности θ/tildenosp. Это предположение игнорирует гессиан и форму по- верхности модели в пространстве параметров высшего порядка. Мо- дель является полной, т.е. существует такой набор параметров θ/tildenosp, что ˆ(, ) ()fx D f xθ= /tildenosp . На основе этого предположения строится ряд асимптотических оце- нок обобщающей способности сети. Одним из первых был предложен информационный критерий Акаика (AIC) [23]. Похожим критерием яв- ляется предложенный Шварцем байесовский информационный крите-рий (BIC) [24]: 2 trainˆ ˆ () 2AICPPx EN=+ σ ; 2 BIC trainln( )ˆ ˆ () 2PNPx EN=+ σ . (4.17) Здесь σ ˆ2 – некоторая оценка дисперсии шума для данных выборки.\n--- Страница 51 ---\nГлава 4. Эффективные нейронные сети 51 Акаик предложил использовать оценку2 trainˆNENPσ=−. Результатом будет финальная ошибка предсказания (final prediction error, FPE) . Еще одним ее аналогом является обобщенная кросс-валидация (generalizedcross-validation, GCV). Хотя оценки GCV и FPE будут слегка различать- ся для малых выборок, они являются асимптотически эквивалентными для больших N: FPE trainˆ()NPPx ENP+=−; GCV train 21ˆ() (1 )Px EP N= −. (4.18) Ключевым моментом будет являться то, что в алгебраических оцен- ках учитывается сложность модели через число свободных параметров P. Очевидно, что в ряде случаев могут задействоваться не все доступ- ные параметры модели [25], поэтому рядом авторов было предложенооценивать эффективное число параметров модели. В серии работ [26 –28] Муди предложил критерий, названный обобщенной прогнозируе-мой ошибкой (generalized prediction error, GPE): GPE train2ˆ()PxE PNε =+ , 1()SC Pt r H H− ε= . (4.19) Здесь HS и HC – гессианы среднеквадратической ошибки и регули- рующего слагаемого соответственно, а Pε – эффективное число пара- метров модели. В случае широко используемого регулирующего сла-гаемого понижения весов число эффективных параметров P ε примет вид 1P k kk kPε =λ≈λ+ ξ∑ , где λ k – это собственные вектора HS, а ξ – коэффициент. В работах [21, 29] предложена аналогичная GPE оценка – сетевой информационный критерий (network information criterion, NIC). Сравнивая два различных подхода к оценке обобщающей способно- сти сети, можно сделать несколько общих выводов. Оценки, основан- ные на валидации, являются очень затратными с вычислительной точки\n--- Страница 52 ---\n52 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ зрения. Асимптотический характер алгебраических оценок предполага- ет наличие достаточно большого набора данных, но дает более точный результат. Использование приведенных оценок для выбора той или иной ней- ронной сети затруднено по причине отсутствия механизмов переборамоделей, однако для GPE в [24] были сделаны такие попытки. Важно отметить, что Мурата в работе [19] показал, что критерии, подобные AIC, NIC и GPE, эффективнее всего применять для сравнениямоделей иерархически вложенных друг в друга, т.е. когда одна нейрон- ная сеть вложена в другую как подсеть меньшей размерности. 4.4. Конструктивный подход к построению нейронных сетей Конструктивные методы предполагают последовательное построе- ние (редукцию) нейронной сети путем поэтапного добавления (удале-ния) элементов архитектуры сети (связей, нейронов, скрытых слоев).Алгоритм конструирования функционирует по определенному правилутаким образом, что любое изменение архитектуры на каждом этапе га- рантированно уменьшает значение ошибки сети. Правило подчиняется следующей схеме: локализация неверно решенных примеров и коррек-ция архитектуры сети только для них, не затрагивая, по возможности,корректных. 4.4.1. Бинарные алгоритмы Первые алгоритмы автоматического конструирования нейронных сетей были связаны с построением нейронных сетей, реализующихпроизвольные логические функции. Эти сети были несколько упроще- ны: входы и выходы таких нейронных сетей могли принимать только значения {0, 1}. Алгоритм черепичного построения Исторически одним из самых первых подобных алгоритмов был ал- горитм черепичного построения, предложенный в 1989 г. в работе [30]. Черепичный алгоритм строит нейронную сеть слой за слоем. Каждый последующий от входов слой имеет меньшее число нейронов. Постро-енная сеть может осуществлять некоторое бинарное отображение N inp\n--- Страница 53 ---\nГлава 4. Эффективные нейронные сети 53 входов в единственный выход. Обучающее множество состоит из NP бинарных векторов, так что NP ≤ 2Ninp. Идея алгоритма заключена в добавлении новых слоев нейронов та- ким образом, чтобы входные обучающие вектора, имеющие различныесоответствующие им выходные значения, имели в нем различное внут-реннее представление. Внутренним представлением входного вектора X P на слое L будут являться все выходные значения нейронов этого слоя. Для входного слоя ( L = 0) для каждого примера внутренним пред- ставлением будет он сам. Число классов для слоя L будет подчиняться соотношениям L PPNN≤ , 0 PPNN= . Более строгим определением клас- сов будут совпадающие классы. Класс для слоя L будет совпадающим, если все входные вектора, попадающие в этот класс, будут иметь оди- наковое желаемое выходное значение. Работа алгоритма будет заключаться в добавлении нового слоя и проверке, что все классы для него являются совпадающими. Если этоудается не для всех классов, то для корректно отображаемых классов веса замораживаются, а для некорректных строится новый слой нейро- нов. Новый слой строится из одного основного нейрона – M и несколь- ких дополнительных – A (рис. 4.4). Всякий раз основной нейрон будет создаваться так, чтобы число входных обучающих векторов NεL, для ко- торых неверно выполняется отображение, уменьшалось. В случаеN εL*= 0, L* – последний слой, а значение основного нейрона и есть же- лаемое выходное значение. Дополнительные нейроны обеспечивают условие совпадающих классов: если условие не выполнено, то это озна-чает, что существуют входные векторы из разных классов, дающие рав-ные выходные значения. Все такие векторы объединяются под допол- нительным нейроном, фактически образуя новый класс. Веса как основного, так и дополнительных нейронов настраиваются модификацией обычного алгоритма обучения персептрона – «карман-ным» алгоритмом. Алгоритм обучения персептрона находит решение только в случае линейной разделимости данных, в противном случае алгоритм не сходится [4], «карманный» алгоритм всегда останавливает-ся на варианте весов, наиболее близком к решению. Процедура работы черепичного алгоритма имеет простую геометри- ческую интерпретацию (рис. 4.5). Основной нейрон (сплошная линия) наилучшим образом разделяет два класса, а дополнительные нейроны\n--- Страница 54 ---\n54 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ (пунктирная линия) только создают корректирующие линейные разде- лители для неверно классифицированных входных векторов. Этот про- цесс повторяется на каждом слое, что позволяет создать нейронную сеть вида рис. 4.4. M M MA AA A AA M x2x1 { } ∈C1 { } C ∈ 2 Рис. 4.4. Вид сети, построенной черепичным алгоритмомРис. 4.5. Геометрическая интерпретация черепичного алгоритма Сходимость черепичного алгоритма доказывается следующей т е о - р е м о й: Пусть все классы слоя ( L – 1) – совпадающие, а число ошибок основного нейрона NεL–1 > 0. Тогда всегда существует набор весов w, со- единяющих слой ( L – 1) с основным нейроном нового слоя L, причем так, что NεL < NεL–1 – 1. Более того, такой набор весов всегда единстве- нен. Доказательство теоремы будет являться способом построения та-кого набора весов и подробно приведено в работе [31]. Башенный алгоритм Вариантом черепичного алгоритма будет являться так называемый. башенный алгоритм, предложенный в 1990 г. Галлантом. Отличием ба- шенного алгоритма будет то, что здесь для организации совпадающихклассов не используются дополнительные нейроны, а сразу строятся со-единения с входными нейронами. На каждом новом слое L создается основной нейрон и соединяется со всеми входами сети. В таком случае внутренним представлением входных векторов будет являться сам\n--- Страница 55 ---\nГлава 4. Эффективные нейронные сети 55 входной вектор вместе со значением основного нейрона слоя ( L – 1). В случае NP входных векторов всегда будет иметь место NP совпадающих классов с одним элементом в каждом. Обучение основных нейронов происходит, как и для черепичного алгоритма, «карманным» методом.В случае нескольких выходных значений для каждого выхода строитсяотдельная нейронная сеть с объединением впоследствии результата. Алгоритм быстрой надстройки Алгоритм быстрой надстройки [32] использует несколько иную стратегию для построения нейронной сети. Сеть не наращивается по- степенно от входов до момента схождения. Наоборот, новые нейроныдобавляются между выходным и выходным слоями. Роль этих нейроновзаключается в корректировании ошибок выходного нейрона. В общемслучае нейронная сеть, построенная алгоритмом быстрой надстройки, имеет форму бинарного дерева. Первоначально имеется сеть с P входами и одним выходным нейро- ном. Как и в черепичном алгоритме, сеть тренируется «карманным» ал-горитмом. В результате обучения для каждого примера X i = {x1, , xP} из обучающей выборки возможно четыре состояния выхода сети: - верно активный: Выход сети oP равен желаемому для Xj значению Ti и равен 1: oP = Ti = 1, P1 – соответствующий набор входных векторов; - верно неактивный: oP = Ti = 0, набор P2; - неверно активный: oP = 1, Tj = 0, набор P3; - неверно неактивный: oP = 0, Tj = 1, набор P4. Идея алгоритма заключается в добавлении двух нейронов-потомков к текущему нейрону (выходному в начале работы). Первый потомок предназначен для коррекции значений выходов, которые были неверно активны – P3, а второй соответственно для неверно неактивных – P4 (рис. 4.6). Для набора P3 ошибочные значения можно устранить, если первый потомок активен только для примеров: { }3 1 1241,()0, { , , }pPoppP P P∈=∈(4.20) Здесь o1(p) выходное значение первого потомка для примера p. Для этого первый потомок соединяется с выходной связью с отрицательнымзначением, абсолютное значение которого больше, чем максимальный размер ошибки для набора P 3. Следует заметить, что для набора P2\n--- Страница 56 ---\n56 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ (верно неактивный) влияние первого потомка будет только повторение правильного результата. Это значит, что набор Pi можно убрать из обу- чающего набора (4.20) для первого потомка: { }3 1 141,()0, { , }pPoppP P∈=∈. (4.21) Аналогично для второго потомка который соединяется с выходной связью уже с положительным значением, имеем обучающий набор { }4 2 231,()0, { , }pPoppP P∈=∈. (4.22) Отображения, заданные в (4.21) и (4.22), потенциально не будут ли- нейно разделимыми, поэтому нейроны-потомки обучаются при помощи «карманного» алгоритма. Это подразумевает, в свою очередь, создание рекурсивно корректирующих нейронов для каждого из потомков. Ре-зультатом будет нейронная сеть на рис. 4.7. ВходыВходы Рис. 4.6. Добавление нейронов- потомков к текущему нейронуРис. 4.7. Нейронная сеть в виде би- нарного дерева, построенная алго- ритмом быстрой надстройки С х о д и м о с т ь а л г о р и т м а г а р а н т и р у е т теорема , к о т о р а я г л а с и т : п о - томки будут всегда производить меньше ошибок, чем родитель. Доказа- тельство приведено в [32].\n--- Страница 57 ---\nГлава 4. Эффективные нейронные сети 57 Применение принципа построения нейронной сети, использованного в бинарных алгоритмах, возможно и для вещественных данных. В рабо- те [33] используется следующий оригинальный прием: данные выборки по определенным правилам преобразуются в бинарные значения, поним конструируется нейронная сеть, схематично решающая проблему,которая затем окончательно обучается по исходным данным. 4.4.2. Древовидные нейронные сети В начале 1990-х гг. независимо друг от друга двумя коллективами авторов были предложены схожие алгоритмы для генерации и обучениянейронных сетей специального вида. В 1991 г. в работе [34] были пред- ложены древовидные нейронные сети или NT-сети, которые представ- ляли собой древоподобную структуру с персептронами в узлах. В 1992 г.в серии работ [35 – 37] были заявлены самоформирующиеся древовид-ные нейронные сети (self-generating neural trees) или SGNT-сети. SGNT-сети отличаются от сетей, предложенных в [34], тем, что имеют в узлах нейроны специального вида. Схема работы сетей обеихтипов будет сильно схожа: обе сети предполагают использование обу-чающих данных без учителя; алгоритм настройки выполняет иерархи- ческую кластеризацию; обе сети предполагают некоторую избыточ- ность в структуре и методы ее устранения. Рассмотрим общую идеюработы на примере SGNT-сетей. Пусть e i = (ai1, , ain) – пример из обучающей выборки. Нейроном nj назовем пару вида ( Wj, Cj). Здесь Wj = (wj1, , Win) – вектор настраивае- мых весов нейрона, а Cj – это множество всех нейронов-сыновей. SGNT-сетью назовем дерево вида 〈{nj},{lk}〉, состоящее из множества нейронов { nj} созданного алгоритмом обучения на основе обучающей выборки и множества связей { lk}. Между нейронами ni и nj существует связь только если nj ∈ Ci. Нейрон nk ∈ {nj} будем называть победителем множества { nj} для примера ei, если выполняется условие для каждого j (, ) (, )ki jidn e dn e ≤ , где (, )j i dn e – евклидово расстояние между нейро- ном nj и примером: 2 1() (, )n kj k i k k jipa a dn en=− =∑ . (4.23) Здесь pk будут весами k-го атрибута. Из определения вытекает следую-\n--- Страница 58 ---\n58 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ щий иерархический алгоритм построения SGNT-сети. На вход подается обучающая выборка E = {ei}, 1 , ,iN= и задается пороговое расстоя- ние ξ ≥ 0. Алгоритм: сору(Корень, ei); for(i=1,j=1; i<N; i++) { МинимальнаяДистанция = d( e∞,Корень); Победитель = СтарыйПобедитель = Корень; МинимальнаяДистанция = test( ei,Корень); if(МинимальнаяДистанция > ξ) { if(leaf(Победитель)) { copy ( nj,Победитель); connect ( nj,Победитель); j = j + i; } copy( nj,ei); connect( nj,Победитель); j = j + 1; } update(Победитель, ei); } Функции: copy( n,e) – создает нейрон п и копирует значения элементов вектора е в веса нейрона п; d(n,e) – определяет дистанцию между n и е согласно (4.23); test ( e, ПодВетвь) – находит победителя в текущей ветви SGNT-сети с вершиной в ПодВетвь для примера е и возвращает расстояние между победителем и е; leaf ( n) – определяет, является ли нейрон п концевой вершиной дерева; connect ( n0, n1) – делает нейрон n1 потомком нейрона n0; update (ni, ek+1) – обновляет веса нейрона ni в соответствии с правилом (4.24). Веса каждого неконцевого нейрона SGNT-сети будут представлять собой среднее значение соответствующих примеров выборки E. Отсюда\n--- Страница 59 ---\nГлава 4. Эффективные нейронные сети 59 последовательное правило обновления весов, где wi jk – k-й элемент ней- рона nj после предъявления i примеров выборки: 11 1()1ii i i jk jk k jkww awi++=+ −+. (4.24) После того как SGNT-сеть построена, может потребоваться некото- рая оптимизация ее структуры. Предлагается балансировать дерево, ис- пользуя горизонтальный и вертикальный критерии. Кроме того, сетьповторно тренируется несколько раз, и «мертвые» ветви дерева, кото-рые не продолжают расти, отсекаются как излишние (подробнее см. [35]). Эффективность SGNT-сетей проверялась на задаче голосования в США и задаче классификации грибов [36]. При сравнимых значенияхпроизводительности время обучения-построения SGNT-сетей оказалосьв несколько раз меньше, чем у сетей, созданных методом «каскадной корреляции» (п. 4.4.5). NT-сети показали свою эффективность на задаче распознавания гласных звуков. Был проведен сравнительный анализ смногослойным персептроном и сетями радиально-базисных функций[34]. Основное отличие NT-сетей от SGNT-сетей заключается в том, что SGNT-сети фактически строят только разбиение множества обучающих примеров гиперплоскостями, а NT-сети с персептронами в узлах могуторганизовать более сложное разбиение. 4.4.3. Алгоритмы Monoplan, NetLines и NetSphere Алгоритмы Monoplan, NetLines и NetSphere – это алгоритмы автома- тического построения нейронных сетей, объединенные общей идеей,предложенные Торрес-Морено [38]. Рассмотрим кратко их принцип ра-боты. Требуется по обучающей выборке ( x 1, , xn)1( , , )n xx и целевым зна- чениям принадлежности двум классам { t10 = –1, t20 = +1} построить ней- ронную сеть. На начальном этапе имеется сеть, состоящая из входов и одного выходного нейрона. Сеть обучается с использованием любого изалгоритмов, пригодных для настройки персептрона, например, «кар-манным» алгоритмом. Если после обучения имеются ошибки, то добав-ляется и обучается нейрон h = 1 в единственный скрытый слой. Если ошибки классификации присутствуют, то модифицируются целевые значения такие, как t 11 = +1 для всех примеров правильно определенных нейроном и t11 = –1 для прочих, и снова добавляется нейрон h = 2 в\n--- Страница 60 ---\n60 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ скрытый слой. Всякий раз, когда нейрон добавлен и обучен, его веса ос- таются замороженными. При подобном способе построения каждый но- вый нейрон будет устранять хотя бы одну ошибку предыдущего нейро- на, что гарантирует сходимость. В алгоритмах NetLines и NetSphere появляются усовершенствования. Рассмотрим пример на рис. 4.8. Класс t1 определен серым цветом, ос- тальное – класс t2. Следуя процедуре построения, получаем сеть с двумя нейронами в скрытом слое, задающими две границы решения w1 и w2. Из рис. 4.8. видно, что разные классы все еще разделены неверно. Те-перь, всякий раз после добавления нейрона выходной нейрон ( w 3) будет переобучаться по схеме, аналогичной описанной. Таким образом, пра- вильное решение достигается гораздо быстрее. Алгоритм NetSphereимеет отличия только в форме пороговой функции 22 1() ( )N ii iga w x ==− − θ∑ , (4.25) где θ – радиус окружности, wi – веса нейрона, задающие центр окруж- ности. Таким образом, разделяющая поверхность будет иметь более сложный вид и существенно улучшает результаты. w2+-+ +-- +-++++ ++--+-w1w3 Рис. 4.8. Пример работы алгоритма NetLines Данное семейство алгоритмов предназначено для решения класси- фикационных задач в случае двух классов. Для расширения на произ- вольное число классов предложено использовать несколько нейронных\n--- Страница 61 ---\nГлава 4. Эффективные нейронные сети 61 сетей с объединением результатов работы. Работа алгоритма была ис- следована для задач диагностики рака и заболевания диабетом [38]. 4.4.4. Метод динамического добавления узлов Еще одним интересным методом построения нейронных сетей явля- ется метод динамического наращивания узлов [39]. В оригинальном виде процесс построения начинается с исходной нейронной сети с одним нейроном в единственном скрытом слое. Приобучении сети любым способом, например методом обратного распро-странения ошибки, вычисляется ошибка E результирующих выходов сети y n(x;W), где x – входной вектор, W – матрица весов связей. Можно считать, что сложность текущей сети не соответствует поставленной задаче, если ошибка E с числом эпох обучения уменьшается недоста- точно быстро, либо не уменьшается совсем. Уменьшение скорости ни-же заданного порога служит сигналом к тому, что для улучшения ре- зультатов при падении скорости ошибки в [39] предлагается добавлять по одному нейрону в скрытый слой. Формально скорость измененияошибки можно определить следующим образом: 0tt tEE E−δ−<Ω. (4.26) Здесь Et0 – значение ошибки в момент t0, t0 – момент добавления предыдущего нейрона, Ω – некоторая заданная минимальная скорость изменения ошибки, при превышении которой принимается решение обизменении сети, δ – количество эпох – циклов обучения сети перед про- изведением оценки скорости ошибки. Таким образом, скорость опреде- ляется изменением ошибки относительно первоначального замера зна-чения. Основным достоинством предлагаемого здесь подхода является то, что в нем не налагается никаких принципиальных ограничений на спо- соб построения нейронной сети, а производится только оценка измене-ния производительности. Упомянутую методику можно применять инаряду с другими способами построения нейронных сетей оптимальной топологии. Метод динамического добавления узлов был опробован на нескольких простых задачах, таких как сложение двоичных чисел. Былопоказано, что для подобных задач дополнительные вычисления возрас-тают не более чем на 40% по сравнению с тренировкой нейронной сети уже оптимальной структуры.\n--- Страница 62 ---\n62 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ 4.4.5. Каскадная корреляция и ее модификации Одним из самых известных и исследованных методов построения нейронных сетей является алгоритм каскадной корреляции, предложен-ный Фальманом в [40]. Метод содержит в себе две основные идеи: осо-бую каскадную структуру, вариант архитектуры сетей прямого распро- странения, в которой новые нейроны последовательно добавляются и уже не изменяются после настройки. Вторым важным моментом явля-ется способ автоматического выбора и настройки нового нейрона. Рас-смотрим алгоритм каскадной корреляции.ВходыВыходы Рис. 4.9. После добавления второго нейрона: – узлы заморожены, × – узлы настраиваются Алгоритм начинает работу с обычного персептрона – сети, состоя- щей только из входов и выходов, с одним полносвязным слоем настраи- ваемых весов. Алгоритм можно разбить на две части: обучение выход- ных нейронов и создание нового нейрона. На начальном этапе, в первой части, настраивается прямое соедине- ние входов-выходов с использованием любого алгоритма обучения од- нослойной сети. Алгоритм использует эффективный метод обучения quick-prop [41]. После того, как за заданное число циклов обучения непроисходит существенного уменьшения ошибки, принимается решение:если ошибка меньше желаемого значения, работа алгоритма завершена,в противном случае – инициируется создание нового нейрона. Во второй части алгоритма новый нейрон-«кандидат» добавляется к существующей сети, причем его входы соединяются со всеми внешни-\n--- Страница 63 ---\nГлава 4. Эффективные нейронные сети 63 ми входами сети и выходами уже добавленных прежде нейронов. Вход- ные веса «кандидата» настраиваются особым образом так, чтобы мак- симизировать влияние выхода этого нейрона на ошибку сети. После на- стройки его выход соединяется со всеми выходными нейронами сети,входные веса замораживаются, а все выходные настраиваются алгорит-мом quick-prop. На рис. 4.9. показана сеть с двумя добавленными ней- ронами. Рассмотрим способ настройки весов нейрона-«кандидата» для мак- симизации его влияния на ошибку. Целью обучения будет максимиза-ция функции S – суммы по всем выходным нейронам o степени корре- ляции между o h, новым значением «кандидата» и E внутренней ошиб- кой, полученной на выходном нейроне o. Отсюда S можно определить как , () ( )hh pp o o opSo o E E=− −∑∑ , (4.27) где Ep,o – ошибка на выходе о для входного примера p; oph – выходное значение нейрона-«кандидата» для входного примера p и величины ⎯Eo и ho – значения соответствующих величин, усредненные по всей выборке. Для максимизации S вычисляются частные производные S в отноше- нии каждого входного веса wih нового нейрона: / ,, ,()op o o p i p h po iSEE f I w∂=σ − ∂∑ . (4.28) Здесь σ 0 – знак корреляции между выходным значением «кандидата» oph и ошибкой Ep,o выхода o; f'p – производная активационной функции нейрона-«кандидата» для примера р; Ii,p – это входное значение, которое получает «кандидат» от i-го, добавленного на предыдущем этапе, ней- рона. Максимизация S происходит также с помощью алгоритма quick- prop. В результате его работы находятся желаемые значения входящих весов «кандидата» wih. Однажды вычисленные, они остаются без изме- нения до конца работы всего алгоритма. Для улучшений характеристик работы алгоритма используются не- сколько приемов: - используется пул – набор из нейронов «кандидатов» с активацион- ными функциями различного типа и случайной инициализацией весов.Выбирался тот, что показывал максимальное значение S;\n--- Страница 64 ---\n64 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ - так как веса нейронов, добавленных на предыдущем этапе, не из- меняются, их выходы, соответствующие различным обучающим при- мерам, могут быть сохранены для повторного использования. Алгоритм каскадной корреляции был протестирован на большом ко- личестве задач, как искусственного происхождения: N – четность, про- блема «двух спиралей» [40], так и классификационных задачах на ре- альных данных [42, 43]. Наиболее полное исследование алгоритма для 42 различных тестовых наборов было представлено в работе [44]. Дос-тупны также критические работы по сравнению каскадной архитектурыс методами статистического анализа [45] и анализ каскадной архитекту- ры нейронной сети [46]. В большинстве работ алгоритм показал результаты не хуже, а в ряде случаев лучше, чем обычный алгоритм обратного распространения.Кратко достоинства алгоритма каскадной корреляции можно сформу-лировать так: 1) автоматическое определение числа нейронов. Возможность ис- пользования различных активационных функций одновременно; 2) быстрое обучение сети, нет трудоемкого обратного распростране- ния ошибки. Алгоритм каскадной корреляции получил дальнейшее развитие. Ав- торами было предложено его обобщение для случая рекуррентных се-тей [47] и для задач с ограниченной точностью [48]. Существует несколько интересных модификаций алгоритма каскад- ной корреляции. Стандартный алгоритм конструировал нейронную сеть с большим количеством слоев с единственным нейроном. Необходи-мость в реализации нейронных сетей на аппаратном уровне породилаварианты каскадной корреляции с ограничением числа слоев [49, 50]. В этом случае нейроны добавлялись в слой до достижения некоторого за- данного числа, затем формировался следующий слой и т.д. В результатеполучались компактные нейронные сети с характеристиками не хуже,чем для сетей с каскадированием. Некоторый интерес представляет алгоритм, предложенный в [51]. Существенным отличием от оригинального алгоритма является отсут- ствие каскадирования и роста сети вверх. Новые нейроны не соединя-ются с входами прежде добавленных нейронов, иначе говоря, нейроныдобавляются всегда только в один скрытый слой. Отсюда название ал- горитма: алгоритм каскадной корреляции второго порядка, а ориги- нальный алгоритм считается алгоритмом высшего порядка.\n--- Страница 65 ---\nГлава 4. Эффективные нейронные сети 65 Еще один вариант каскадной корреляции, предложенный в работах [52, 53] под именем BINCOR , использует стратегию добавления сразу нескольких нейронов, по числу выходов сети (рис. 4.10). Это сущест- венно ускоряет процесс обучения, так как каждый нейрон корректируетошибку отдельного выхода, а не всех сразу, но и в то же время приво-дит к заметному увеличению размеров сети.ВходыВыходы Рис. 4.10. BINCOR – вариант алгоритма каскадной корреляции Сразу пять вариантов алгоритма каскадной корреляции разработано и исследовано Л. Пречелтом в [44]. Основной особенностью предло- женных модификаций является использование в различных сочетаниях минимизации ошибки сети вместо максимизации ковариации (4.27) вы-бор нейрона-«кандидата» сразу по двум наборам данных: обучающемуи проверочному. Лучшие результаты показал вариант, названный kogi9 , в котором используются соображения, аналогичные примененным в ал- горитме каскадной корреляции второго порядка: добавление нейронов водин слой. Только в kogi9 аналогично формируется сразу два скрытых слоя. Алгоритм КасПер Самые новейшие исследования и модификации алгоритма каскад- ной корреляции были проведены в серии работ Треадголдом и Гедео- ном [54 – 58]. Фактически ими был предложен новый алгоритм – Кас-\n--- Страница 66 ---\n66 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Пер. Авторы предложили отбросить использование оценки корреля- ции по (4.27) и принцип замораживания весов, так как исследования показали, что это приводит к созданию нейронной сети завышенного размера. Сеть будет требовать больше нейронов для коррекции оши-бок, допущенных предыдущими нейронами. Вместо этого в КасПерпосле добавления нейрона дообучается вся сеть с использованием особого алгоритма обучения. Алгоритм КасПер начинает построение нейронной сети подобно оригинальному методу каскадной корреляции: начинает с единствен-ного скрытого нейрона и последовательно добавляет по одному ней- рону. Всякий раз после добавления нейрона используется адаптиро- ванная версия добавления алгоритма обратного распространенияошибки RPROP. RPROP модифицирован таким образом, чтобы всякийраз после добавления нового нейрона скорости обучения для весов се-ти устанавливались в различные значения, в зависимости от положе- ния веса в сети. Все веса нейронной сети разбиваются на три группы: L 1, L2 и L3 (рис. 4.11). Набор L1 составляют все веса, соединяющие но- вый нейрон со входами сети и выходами прежде добавленных нейро-нов. Второй набор включает в себя веса, соединяющие выход нового нейрона со всеми выходами сети. Оставшиеся веса включаются в на- бор L 3. - набор весов L3- набор весов L2- набор весов L1 Новый нейрон ВходыВыходы Рис. 4.11. Архитектура КасПер в момент добавления второго нейрона\n--- Страница 67 ---\nГлава 4. Эффективные нейронные сети 67 Скорости обучения для групп L1, L2 и L3 принимают значения соот- ветственно L1 >> L2 > L3. Причина для подобного соотношения подобна причине, по которой оригинальный алгоритм каскадной корреляции ис- пользует оценку корреляции: большое значение L1 поощряет новый нейрон к устранению оставшихся ошибок сети. А при L2 > L3 уменьша- ется ошибка сети без сильного влияния на веса остальных нейронов. Заметим, что таким образом КасПер реализует преимущества замора- живания весов и техники корреляции оригинального метода каскаднойкорреляции и одновременно устраняет проблемы насыщения, обуслов-ленные специфической оценкой корреляции и неизменным состоянием плохо настроенных нейронов. Алгоритм КасПер также использует методику снижения весов для улучшения характеристик построенной сети. Градиент ошибки дляКасПера выглядит следующим образом: Epoch 0,01 2sign( ) 2N ij ij ij ijEEkw www− ∂∂=− ⋅ ⋅∂∂, (4.29) где NEpoch – число эпох обучения после добавления последнего нейрона, sign( wij) – знак операндов wij. Решение о добавлении нового нейрона принимается, если скорость уменьшения ошибки становится меньше 1% за время 15+α N. Здесь N – число уже установленных нейронов сети с поправочным коэффициентом. 4.4.6. Методы редукции Другим основным направлением, которое можно выделить среди конструктивных алгоритмов, являются методы редукции нейронной се- ти. Все подходы к редукции объединены следующей общей схемой ра- боты. Исходной является некоторая, уже обученная, нейронная сеть. Впредположении, что размеры этой сети можно уменьшить, отсекаютсяопределенные каким-либо образом избыточные нейроны или сущест- вующие связи между ними. Как правило, в качестве критерия выбира- ется степень воздействия нейрона (связи) на величину ошибки, а уби-раются соответственно слабо влияющие нейроны (веса). После упроще-ния нейронной сети повторяется процесс дообучения и повторного усе- чения до тех пор, пока величина ошибки не достигнет желаемой вели- чины или топология нейронной сети не достигнет необходимой просто-ты. Очевидными недостатками подхода усечения являются неопреде-\n--- Страница 68 ---\n68 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ ленность в принципах построения исходной нейронной сети и большой объем вычислений. Обычно метод усечения применяется для экономии ресурсов при необходимости аппаратной реализации уже найденной нейронной сети. Редукция нейронов Исторически, первой техникой упрощения сети была техника усече- ния, предложенная в 1989 г. Мозером и Смоленским [59]. Ими былавведена следующая оценка значимости j-го нейрона: S j = E(без нейрона j) – E(с нейроном j). (4.30) Вычисление (4.30) будет трудоемким, так как потребуется полный проход по всей обучающей выборке для каждого нейрона сети. Вместоточного значения s j предложено вычисление оценки ŝj. Для этого поро- говая функция g(a) каждого нейрона дополняется коэффициентом α j: ()j jj i i izgw z=α∑ ; (0) 0g=; () t h ()gaa= . (4.31) Таким образом, для α j = 0 нейрон будет отсутствовать, для α j = 1 присутствовать. Отсюда (4.30) можно переписать: (1 ) (0 )jj jsE E=α = −α = . (4.32) Для оценки sj авторами было предложено использовать производную ошибки по α j, которую можно автоматически вычислять с помощью модифицированного алгоритма обратного распространения ошибки: ˆ 1jj jEs∂=− α =∂α. (4.33) Контрастирование сетей Более исследованным подходом к упрощению нейронной сети явля- ется техника усечения весов, основанная на оценке значимости весов связей. В отечественной литературе техника усечения весов носит на-звание процедура контрастирования и впервые была предложена в ра- боте [60]. Пусть имеется нейронная сеть, правильно решающая все примеры обучающего набора. При работе метода обратного распространенияошибки сеть вычисляет градиент функции оценки (ошибки) H(w) по ве- сам связей w j – ∂E/∂wj. Пусть w0 – текущий набор весов сети, а ошибка\n--- Страница 69 ---\nГлава 4. Эффективные нейронные сети 69 сети для текущего примера равна E0. Тогда в линейном приближении можно записать функцию оценки в точке w как: 00() ( )pp i iEEw E w ww∂=+ −∂∑ . (4.34) Используя приближение (4.34), можно оценить изменение оценки при замене wp0 на wp* как 0*(,) ( )pp iEpq w ww∂ℵ=⋅ −∂. (4.35) Здесь q – индекс примера обучающего набора, для которого были вы- числены оценка и градиент. Величину ℵ(p,q) будем называть показате- лем чувствительности к замене wp0 на wp* для примера q. Далее необхо- димо вычислить показатель чувствительности, не зависящий от номера примера. Обычно используется равномерная норма максимума модуля: () m a x ((,) ) qpp qℵ= ℵ . Таким образом, процедура контрастирования заключается в сле- дующем. Каким-либо образом обучается нейронная сеть с требуемым малым значением ошибки Ê. На каждом шаге работы определяем ми- нимальный показатель чувствительности – ℵ(p*), заменяем соответст- вующие веса w0 p* на w* p* и исключаем его из сети. Вся процедура контра- стирования повторяется до тех пор, пока значение ошибки Ê не начина- ет увеличиваться. Из способа вычисления (4.34) и (4.35) видно, что необходимо знать вид оценочной функции и процедуры обучения для правильной работы процедуры контрастирования. Однако существует вариант контрасти-рования, который позволяет обойтись без этих сведений. Пусть дана только обученная нейронная сеть и обучающее множест- во, причем вид функции оценки и процедура обучения неизвестны. В этом случае контрастирование весов производится понейронно. В каж- дом нейроне стоит адаптивный сумматор, который суммирует входныесигналы нейрона, умноженные на соответствующие веса связей. Еслиобозначить входные сигналы нейрона при решении q-го примера как x pq, получим формулу для показателя чувствительности весов: *(,) ( )q pp p pq x w wℵ= − . (4.36)\n--- Страница 70 ---\n70 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Получаем показатель чувствительности *() m a x ( ) ( )q pppqpx w wℵ= − . В работе [58] в качестве основных целей контрастирования указаны: упрощение технической реализации нейронной сети и выявление зна- ний, полученных сетью в ходе обучения. Результаты экспериментов поконтрастированию нейронных сетей опубликованы в [61]. Optimal Brain Damage и Optimal Brain Surgeon Для направления упрощения сети методом усечения весов широко известными являются методы «оптимального повреждения мозга»(Optimal Brain Damage, OBD) [62] и его более позднее развитие – «луч- ший нейрохирург» (Optimal Brain Surgeon, OBS) [63]. Рассмотрим эти методы подробнее. Пусть изменение ошибки δ E в результате некоторых малых измене- ний значений весов w i на wi + δwi 3 1()2ii j i j i ii jEEw H w w O ww∂δ= δ + δδ + δ∂∑∑ ∑ . (4.37) Здесь 2 ij ijEHww∂=∂∂ – элементы матрицы вторых производных, гессиан. В предположении, что процесс обучения завершен и ошибки нет в (4.37) можно отбросить первое слагаемое. В работе [62] было предло- жено рассматривать приближенное значение гессиана по элементам егоглавной диагонали, отсюда следует, что 2 1 2ii i iEH wδ= δ∑ . (4.38) Тогда, если исходный вес wi сделать равным нулю, то увеличение ошибки можно оценить по (4.38) с δ wi = wi. Отсюда легко можно опре- делить значимость веса wi как 2() / 2ii iHw . В результате алгоритм OBD можно сформулировать в следующих этапах. Для уже имеющейся обученной нейронной сети с малым значе- нием ошибки Ê вычисляются вторые производные Hii для каждого веса\n--- Страница 71 ---\nГлава 4. Эффективные нейронные сети 71 wi; по (4.38) определяется значимость весов и отбрасывается часть с ма- лыми значениями до тех пор, пока не будет достигнут некоторый более общий критерий (размер сети, величина ошибки). Однако предположение, что гессиан можно аппроксимировать по его диагонали, не всегда дает хорошее решение. Более совершеннаяпроцедура усечения весов – OBS избегает подобных предположений [61]. Предположим, установлен вес w i = 0. Оставшиеся веса сети коррек- тируются так, чтобы минимизировать увеличение ошибки сети – δ w. Из (4.37), предполагая, что сеть была предварительно обучена, и отбрасы- вая малую величину θ(δ w3), получаем в векторной форме 1 2TEw H wδ=δ δ . (4.39) Изменения значений весов должны удовлетворять условию 11 0Tewwδ+ = . (4.40) Здесь e1 – единичный вектор в пространстве весов, параллельный оси wi. Изменение весов δ w, минимизирующее δ E, можно найти, используя множитель Лагранжа: 1 1 1[]i iiwwH e H− −δ= − . (4.41) Соответствующее увеличение ошибки 2 11 2[]i i iiwE H−δ= . (4.42) Заметим, что в случае, если гессиан является диагональным, выра- жения (4.41) фактически сводятся к предыдущему методу. Практический алгоритм OBS можно представить следующим обра- зом. Имеется уже обученная нейронная сеть с малым значением функ-ции ошибки. Затем вычисляется обратная матрица гессиана H –1, по (4.42) рассчитываются δ Ei для каждого веса i и выбирается i, дающий минимальное увеличение ошибки. При помощи (4.41) обновляются значения всех весов сети. Вся процедура повторяется, пока не будетдостигнут некоторый более общий критерий (размер сети, величинаошибки).\n--- Страница 72 ---\n72 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ На рис. 4.12. наглядно иллюстрируется сопоставление методов OBD и OBS для случая двух весов. Локальный минимум, найденный гради- ентным спуском, находится в точке w*. На рисунке также отражено простейшее соображение, когда усечение весов производится по эмпи-рическому правилу: меньшее значение соответствует менее значимомувесу. A CB w2OBDOBSОшибка По весуw2 w* 0 Рис. 4.12. Сравнение методов Optimal Brain Damage и Optimal Brain Surgeon для случая двух весов Такой способ отбросит меньший вес w2 и переместит вектор весов в точку А. Метод OBD, следуя правилу (4.38), отбросит вес w1 и перемес- тит вектор весов в В. Метод OBS кроме того, что отбросит вектор w1, в соответствии с (4.41) скорректирует вес w2. Таким образом, величина ошибки будет подчиняться неравенству (по весу) ( ) ( )E E OBD E OBS ≥≥ . Исследования показали, что действительно метод OBS гарантирует лучший результат, чем более ранний OBD [63, 64].\n--- Страница 73 ---\nГлава 4. Эффективные нейронные сети 73 4.5. Эволюционный способ создания нейронных сетей 4.5.1. Генетические алгоритмы Современным способом построения нейронных сетей является ис- пользование эволюционного подхода к поиску по пространству возмож-ных архитектур. Исторически сложились три методологии эволюционно-го поиска [65]: эволюционное программирование Фогела, 1966 г., эволю- ционные стратегии Peченберга, 1973 г. и генетические алгоритмы (ГА) Холланда, 1975 г. Идеи этих методов похожи, но они различаются в вы-боре представления индивидуумов, механизмах селекции, форме генети-ческих операторов и способах оценки приспособленности. Наиболее универсальным подходом являются ГА Холланда [66 – 68]. Идея ГА основана на принципе эволюции биологических существ:из всей совокупности особей данного вида выживают только наиболееприспособленные к сложившимся условиям окружающей среды инди- видуумы. Полезные свойства закрепляются и накапливаются на генети- ческом уровне в последующих поколениях путем обмена генами. Воз-вращаясь к нейронным сетям, отметим, что в данном случае набор всехвозможных нейронных сетей будет выступать в качестве популяции,архитектура сети (число слоев, нейронов, связей) будет генетическим кодом, критерием приспособленности будет наилучшая точность ре- зультатов и малый размер сети. Каждый индивидуум популяции будет представлять собой возмож- ное решение поставленной проблемы. Параметры решения кодируются в строку символов – генетическую строку. В оригинальном методе, предложенном Холландом, использовалась двоичная строка фиксиро-ванной длины. Если проблема имеет несколько параметров, генетиче-ская строка содержит набор подстрок, или генов, соответствующих ка- ждому параметру. Современные варианты ГА применяют генетическую строку переменной длины [69] различной структуры: векторы, графы,упорядоченные списки, Лисп-выражения [65]. Используя биологиче-скую терминологию, можно сказать, что информация, содержащаяся в строке, будет генотипом индивидуума популяции. Результат же – вос- становленное решение задачи, можно назвать фенотипом. Очевидно,что способ кодирования задачи в генетическую строку будет сущест-венно определять эффективность работы ГА. Кодирование решения за-дачи в генетическую строку производится пользователем ГА. Сами ГА в процессе функционирования не владеют информацией о значении за-\n--- Страница 74 ---\n74 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ кодированной строки. Основной процесс их работы заключается в ма- нипулировании наборами генетических строк – популяциями в репро- дукционном процессе с помощью генетических операций. В репродукционном процессе новые решения (индивидуумы) созда- ются при помощи выбора, рекомбинаций и изменений существующихрешений, основываясь на оценочной функции, часто называемой функ- цией приспособленности (фитнесс-функцией) . Функция приспособлен- ности оценивает приспособленность каждого индивидуума по отноше-нию к «окружающей среде». Так как каждый индивидуум являет собойвозможное решение задачи, то «окружающая среда» есть поставленная задача. Если поставлена задача найти оптимальную нейронную сеть при помощи ГА, то функцией приспособленности может быть величина,обратная ошибке сети или равная нулю в случае, если сеть не можетрешить проблему. Следует отметить, что ГА использует только значе-ние функции приспособленности, но не смысловую нагрузку. Это по- зволяет строить произвольно сложную оценочную функцию путем ком- бинирования нескольких факторов. Механизмы селекции Для выделения индивидуума из популяции и последующей обработ- ки генетическими операциями используются механизмы селекции [70].По аналогии с биологическими механизмами селекции индивидуумы с большим значением функции приспособленности (более приспособлен- ные) вероятнее всего выбираются для «размножения». Обычно выделя-ют два механизма выбора. Первый вариант считается классическим иносит название выбор по колесу рулетки, предложенное в [71]. Выбор по колесу рулетки предполагает выбор генетических строк с вероятно- стью, пропорциональной значениям их функции приспособленностиотносительно данной популяции. Данный подход прекрасно работает на начальных этапах ГА, но ближе к концу значения функции приспособленности отличаются друг от друга незначительно, что препятствует выбору действительно луч- ших индивидуумов. Другим недостатком приведенного способа выборабудет то, что в случае доминирования некоторого большого значенияфункции приспособленности над всей популяцией результатом может стать скатывание ГА к неоптимальному решению. В альтернативном подходе: выбор по рангу в популяции [72] шанс быть выбранным для индивидуума зависит от ранга или относительного\n--- Страница 75 ---\nГлава 4. Эффективные нейронные сети 75 положения в популяции. Вся популяция сортируется по значению функ- ции принадлежности. Ранг R индивидуума определяется по формуле 1 NiRM−+= ; 1N pMp ==∑ , (4.47) где i – позиция индивидуума в популяции, а N – их количество. Для по- пуляции из N = 10 индивидуумов, согласно (4.47), ранг первого индиви- дуума будет равен 1100,1855R=≈ , а последнего 1010, 0155R=≈ . Такой подход позволяет решить все поставленные выше проблемы и, кроме того, абстрагироваться от абсолютного значения функции приспособ- ленности. Операция скрещивания Операция скрещивания, или кроссинговер, моделирует воспроизвод- ство потомка от двух родителей. Скрещивание создает новых членовпопуляции путем комбинирования различных частей двух генетических строк родителей, как показано на рис. 4.13. Число и позиция точек пе- ресечения выбирается, как правило, случайным образом. Использова-ние операции скрещивания позволяет закреплять и усиливать в сле-дующих поколениях выработанные «полезные гены» – параметры ре- шения. ПотомокРодитель 2 Родитель 1 11 00100 1010010000 010 0001011 101 Рис. 4.13. Операция для двух точек сечения Операция мутации Операция мутации генерирует новых индивидуумов, случайным об- разом изменяя произвольное количество битов (символов, генов) гене- тической строки (рис. 4.14). Задача этой операции – производить слу- чайные скачки по пространству возможных решений и выводить ГА излокальных решений, полученных операцией кроссинговер. Для того\n--- Страница 76 ---\n76 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ чтобы ГА не сводились к алгоритмам простого случайного поиска, ве- роятность применения оператора мутации делается малой. 1101 0101011010010101 Рис. 4.14. Операция мутации. Изменяется четвертый бит Таким образом, принципиальная схема работы ГА будет выглядеть следующим образом (рис. 4.15). Выбор: 1) способа кодировки в генетическую строку; 2) функции приспособ- ленности; 3) параметров алгоритмаГенерация начальной популяцииВыбор и ранжиро- вание индиви- дуумовОценка индиви- дуумовприспособ- ленности ВыходСкрещи- вание МутацияНовое поколение Рис. 4.15. Схема работы генетических алгоритмов Попробуем теоретически обосновать, почему такой простой прин- цип работы и несложные генетические операции дают столь мощный поисковый механизм. Рассмотрим простой пример. Пусть задача за-ключается в том, чтобы при помощи ГА найти максимум функции f (x) = x 2 для x ∈ [0, 255]. В качестве генетической строки возьмем стро- ку из 8 элементов, являющую двоичное представление х. Если имеются только следующие строки (рис. 4.16) и неизвестен вид функции приспособленности F, лучшим способом для получения новых строк будет поиск совпадений между строками с максимальным при- способлением. Ген. строка Приспособленность 00001101 11000110 00111001 11110001169 39204 3249 57600 Рис. 4.16. Значения функции приспособленности для f (x) = x2\n--- Страница 77 ---\nГлава 4. Эффективные нейронные сети 77 Для рис. 4.16 можно сделать вывод, что 1 в старшем разряде влияет на высокое значение приспособления. Идея использования малых час- тей строк с высоким приспособлением для производства новых строк формально описывается концепцией генетических схем и строительных блоков. Назовем шаблоном структуру, описывающую подстроки с совпаде- ниями [66]. Шаблон [1 * 0001 * 0] (* – любой символ) соответствует любой из строк: {10000100,10000110, 11000110}. Оценим количествовсех возможных шаблонов S, заключенных в популяции из n строк дли- ны l. Если каждая генетическая строка строится алфавитом из k симво- лов ( k = 2 для двоичных строк), тогда S = (k + 1) l, так как кроме k симво- лов может быть и знак «*». Для примера на рис. 4.16 S = (2 + 1)8 = 6561, тогда как число всех возможных строк 28 = 256. Для популяции строк размера n число вовлеченных шаблонов будет лежать в диапазоне от 2l до n·2l. Таким образом, даже малая популяция может содержать огром- ное количество информации о важных совпадениях строк. Назовем длиной шаблона δ расстояние между первым и последним элементом не*. Операция скрещивания имеет тенденцию к разбиваниюшаблонов большой длины, когда точки разбиения выбираются равно- мерно случайно, например шаблон 1*****10 имеет больший шанс быть разбитым, чем * * * * *10* (6/7, 86% против 1/7, 14%). Нижнюю границу для вероятности выживания p в при скрещивании оценим как вс1/ ( 1 ) pp l≥− δ − . (4.48) Здесь pс – это вероятность применения скрещивания. Оценим эффект применения операции выбора. Когда имеется m(t) строк, заданных определенным шаблоном на момент времени t в попу- ляции, можно ожидать: в 1(1 ) ( ) .n i imt mtn f f =+= ⋅ ∑ (4.49) Здесь n – размер популяции, fв – среднее значение приспособленности для строк, представленных данным шаблоном. Если переписать форму- лу (4.49), используя среднее значение приспособленности для популя- ции avg 11n i iffn==∑ , получим вa v g (1 ) ( )mt mt f f+= ⋅ . (4.50)\n--- Страница 78 ---\n78 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Шаблон со значением приспособленности выше среднего имеет больше шансов быть перенесенным в следующем поколении и получает при этом увеличение числа попадающих в него строк. Общий эффект от операций скрещивания и выбора может быть оце- нен как комбинация (4.48) и (4.50): в c avg(1 ) ( ) 1 ( )1fmt mt p mtflδ ⎡ ⎤+≥ −⎢ ⎥ − ⎣ ⎦. (4.51) Каждый шаблон увеличивает свое присутствие в зависимости от двух факторов: отношения приспособленности шаблона к средней при-способленности популяции и длины шаблона. Таким образом, опреде- л я е т с я теорема шаблонов [ 70]: шаблоны с большим значением приспособленности и короткой длиной продвигаются экспоненциально через поколения. Такие шаблоны называются строительными шаблона- ми. Можно сказать, что механизмы ГА ведут к комбинированию луч- ших частичных решений в лучшее общее решение. Экспоненциальное продвижение шаблонов малой длины и высокой приспособленности производится параллельно в пределах одной попу- ляции из n генетических строк. В работе [71] было оценено более точно количество шаблонов, обработанных в каждом поколении: их количест- во оценено около n 3. Так как за одно поколение производится только n вычислений функции приспособленности, эта особенность ГА была на- звана неявным параллелизмом. 4.5.2. Эволюционные алгоритмы для нейронных сетей Идея комбинирования нейронных сетей и генетических алгоритмов представляется довольно естественной хотя бы вследствие того, что обе методики были позаимствованы из биологических систем [73]. Можно сказать, что таким объединением моделируется естественная ситуация, когда индивидуумы, выжившие в результате эволюционного процесса и приспособившиеся к создавшимся условиям окружающей среды (гене- тические алгоритмы), затем непосредственно обучаются решать болееконкретные задачи (нейронные сети). Можно выделить три основных подхода к объединению ГА и нейронных сетей [74]: 1) использование ГА как инструмента поиска параметров алгоритма обучения нейронной сети; 2) для непосредственного обучения сети (поиска по пространству весов); 3) для нахождения архитектуры нейронной сети.\n--- Страница 79 ---\nГлава 4. Эффективные нейронные сети 79 В нашем обзоре будет рассматривается только последний вариант. Общую схему поиска архитектуры нейронных сетей с помощью ГА можно представить следующим образом. Информация об архитектуре нейронной сети специальным образом кодируется в генетический код.Затем генерируются поколения, к которым применяются стандартныегенетические операции. Оценка каждого индивидуума производится следующим образом: нейронная сеть восстанавливается из генетиче- ской строки и производится ее тестирование. Значение функции при-способленности, как правило, будет зависеть от значения ошибки сетина проверочной выборке. Работа ГА завершается по достижению же- лаемого значения ошибки [75]. Для вычисления функции принадлежности применяется два подхо- да: с использованием процедуры обучения сети и без обучения. Эффек-тивность использования процедуры обучения нейронной сети для оцен-ки ее приспособленности основано на «эффекте Балдвина». В своей ра- боте «Новый фактор в эволюции» в 1896 г. Балдвин предположил, что в эволюционном процессе через поколения в генетическом коде переда-ется не сами полученные навыки, а только гены, несущие способность кобучению, что естественным образом повышает адаптационные воз- можности индивидуума. В работе [76] дается вывод о том, что при ис- пользовании «эффекта Балдвина» ГА дают значительно лучшие резуль-таты для нейронных сетей, правда, при этом время работы несколькоувеличивается. Описанная выше процедура поиска нейронных сетей при помощи ГА вполне очевидна и проста. Основной трудностью здесь будет удач-ный выбор способа кодирования информации об архитектуре сети в ге-нетический код. Эта проблема породила массу различных подходов и вариаций. Рассмотрим основные трудности, возникающие при кодиро- вании/декодировании нейронных сетей, и дадим краткий обзор наибо-лее интересных подходов. 4.5.3. Подходы к кодированию нейронных сетей Можно выделить два подхода к кодированию нейронных сетей: прямое и порождающее кодирование [77]. Прямое кодирование подра-зумевает хранение полной информации о структуре сети в генетиче-ском коде. В противоположность этому порождающее кодирование предполагает хранение правил порождения нейронной сети. Каждое из правил может порождать часть сети.\n--- Страница 80 ---\n80 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Основным достоинством прямого кодирования будет порождение малого количества «нежизнеспособных» нейронных сетей, т.е. сетей с архитектурой, делающей невозможным их функционирование. Главной причиной появления порождающего кодирования была невозможностьиспользования прямого кодирования для нейронных сетей произвольнобольшой размерности. Объем генетического кода для больших сетей возрастал пропорционально, что существенно отражалось на произво- дительности. Многие алгоритмы вынуждены были искусственно огра-ничивать размер сети и соответственно уменьшать пространство поис-ка. Для порождающего кодирования размеры сети не будут существен- ны, но применение генетических операций к правилам влечет за собой порождение некоторого количества «нежизнеспособных» архитектур,которые следовало распознавать и отбрасывать. Как было отмечено в[78], для порождающего кодирования время поиска несколько выше,чем для прямого. 4.5.4. Прямое кодирование Прямая кодировка по Миллеру Одной из самых простых является техника кодирования, предложен- ная Миллером в [79]. Все соединения сети из N нейронов кодируются матрицей связности нейронов размерностью N·(N+1). Все строки мат- рицы затем конкатенируются в генетическую строку (рис. 4.17). 5 43 21 5432112345 Генетическая строка00110001100000100001000000000000001000010011000110 Рис. 4.17. Прямая кодировка по Миллеру Все восстановленные сети, не попадающие под определение ней- ронных сетей прямого распространения, отбрасываются. Кодировкабыла исследована для задачи XOR на популяции из 50 индивидуумов\n--- Страница 81 ---\nГлава 4. Эффективные нейронные сети 81 [79]. Аналогичный принцип кодирования, с некоторыми модификация- ми, применяется и в работах других авторов [80 – 83]. GENITOR Одним из классических подходов к прямому кодированию является алгоритм GENITOR, предложенный в [72]. В данном способе кодирова- ния в генетический код заносится информация как об архитектуре, так ио весах связей. Индексный бит обозначает присутствие соединения, а8-битовое число представляет целое значение соответствующего веса в интервале [–127, +127] (рис. 4.18). GENITOR требует приблизительное (максимальное) определение архитектуры нейронной сети до началаработы. Вследствие того, что операция скрещивания может иметь точкисечения в произвольном месте битовой строки, потомки могут иметьзначения некоторых весов, существенно отличающиеся от родительско- го. В дальнейшем этот метод получил развитие для вещественных чи- сел. В работе [84] исследуется влияние размера генетической строки,построенной подобным образом, на конечный результат. значение веса соответствующей связи-102-1 40 65 -15 индексный бит 1 01000001 1 10001111 1 11100110 1 10000001 0 10110110 1 00101000 Рис. 4.18. Кодирование алгоритмом GENITOR BP-Generator Описанные выше методы, основанные на кодировании соединений, имеют один существенный недостаток: необходимо указывать размеры максимальной сети. Этого неудобства лишены методы, базирующиесяна стратегии кодирования нейронов. Одним из первых подобный спо-соб был предложен в работах [85 – 87] под именем BP-Generator . Строка генетического кода представляла собой набор пронумеро- ванных нейронов. Каждому нейрону предшествует список связанных с\n--- Страница 82 ---\n82 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ ним нейронов (рис. 4.19). Существует вариант, когда строка кода со- держит также информацию о весах связей. Родитель 2 Родитель 1 Потомок 5 43 21 2 41 5 421 13 215 4 3 21 6 543 21 6 2 41521 2 4 5 4332121 421 13 21 Рис. 4.19. Применение операции скрещивания для кодировки BP-Generator Алгоритм предлагает следующий оператор мутации: случайное уда- ление и внесение слабых соединений (малое значение веса) и простодобавление новых нейронов. Схема работы оператора скрещиванияпредставлена на (рис. 4.19). Точка сечения может быть только междунейронами и не разбивает списка связанных нейронов. Если в генетиче- ском коде после применения генетических операций появляются ней- роны без входных и выходных соединений, они исключаются из кода. Вработе [88] применяется аналогичный способ кодирования с использо-ванием распараллеленного эволюционного процесса. Древовидное представление В способе кодирования, предложенном Козой в работе [89], была применена кодировка нейронов и весов связей в виде дерева, записан- ного в виде выражения на Лиспе. Такое представление также позволяетне заботиться об определении предварительных размеров сети. На рис. 4.20. приведен пример такой записи. Вершины дерева – P представля- ют собой нейроны, а вершины W – веса связей. Для восстановления\n--- Страница 83 ---\nГлава 4. Эффективные нейронные сети 83 нейронной сети дерево разбирается снизу вверх от выходов сети. Каж- дый выход требует свое собственное дерево, поэтому фактически выхо- ды будут независимы друг от друга. Каждый нейрон P может иметь произвольное количество поддеревьев, начинающихся с W. Вес W имеет два поддерева: одно для значения веса, второе для обозначения нейрона источника соединения. Два входных нейрона обозначены как D0 и D1 . Значения весов может быть записано как вещественным чис- лом, так и выражением. Операция скрещивания заключается в обменеподдеревьями двух родителей, мутация произвольно изменяет поддере- вья и значения весов. В работе [90] аналогичная схема кодирования ис- пользована в модуле построения нейронных сетей в известной про-грамме NeuroForecaster. Нейронная сеть Представление в виде дерева 1.584*1.21.1+0.741 -1.387 1.191-.9891.66 D1D0 WPP W WW W D0 -0.989 D1 1.1911.584 1.2 D1 -1.387 D0 1.660.741 1.1* + WP5 43 21LISP S-выражение (P (W (+ 1.1 0.741) (P (W 1.66 D0) (W -1.387))) (W (* 1.2 1.584) (P (W 1.191 D1) (W -0.989 D0)))) Рис. 4.20. Древовидное представление архитектуры Послойное представление Схема прямого кодирования, основанная на представлении нейрон- ной сети в генетическом коде по слоям, была предложена в [91]. Основ-ной структурой будет список блоков с информацией о слоях и блок па- раметров сети (рис. 4.21). Значения весов не кодируются. В качестве параметров сети сохраняются параметры алгоритма обучения. Для каж-дого слоя в блоке описания сохраняется информация о размере сети,выходных и входных соединениях слоя с другими слоями. Выходныесоединения предполагают, что каждый нейрон слоя имеет связь со сле- дующим смежным слоем. Входные соединения приходят с предшест-\n--- Страница 84 ---\n84 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ вующего слоя сети, заданного в поле соединение. Поле радиус опреде- ляет распределение соединений по нейронам присоединенного слоя, поле плотность – число нейронов в этой области. Все три поля: соеди- нение, радиус и плотность задаются относительно числа нейронов соот-ветствующего слоя. Соединение ПлотностьРадиусСоединение ПлотностьРадиус ПлотностьРадиусВходные соединения Выходные соединения Размер слояСлой N Слой 1Параметры алгоритма обучения Рис. 4.21. Послойное представление Оператор мутации предполагает случайное изменение параметров сети или любого из параметров произвольно выбранного слоя. Точкиразбиения для операции скрещивания выбираются четко между слоямисети, обмен генетической информацией происходит послойно. Эта система кодирования применялась для задач выделения углов на изображении, распознавания символов алфавита и аппроксимации [91]. 4.5.5. Порождающее кодирование Для порождающего кодирования характерным является сохранение в генетическом коде не параметров нейронной сети, а правил, по кото- рым генерируются эти параметры. Мотивацией к применению правилявляется способ формирования биологических систем: для головногомозга количество составляющих нейронов будет на порядки превышатьчисло нуклеотидов в геноме. Эффективность биологического кода ос- нована также на модульной организации и определенной степени фрак- тальности. Большая часть систем с порождающим кодированием длязадания правил построена на принципах, взятых из формальных грам-матик.\n--- Страница 85 ---\nГлава 4. Эффективные нейронные сети 85 Грамматика графовой генерации Одним из первых порождающих способов кодирования нейронных сетей была предложенная Китано в [92] грамматика графовой генера-ции – разновидность контекстно-свободной грамматики. Продукцион- ные правила грамматики имеют вид: AB CDS⎡ ⎤→⎢ ⎥ ⎣ ⎦. (4.52) Левая часть правила – символ грамматики, правая часть представля- ет собой 2 × 2 матрицу символов из алфавита {A,B, ,Z,a,b, .р, 0,1}. Символы с А по Z являются нетерминальными символами, из которых и составляется генетическая строка. В строке обязательно должен при-сутствовать стартовый символ S, с которого будет производиться грам-матический разбор. Вершины алфавита {а, b, , р} являются псевдотер- минальными и определяют 16 фиксированных правил грамматики, ге- нерирующих все возможные 2 × 2-матрицы из нулей и единиц (рис. 4.22). 1 2 3 4 5 6 7 8Восстановленная нейронная сетьПроцесс порождения матр ицы связности для нейроновВосстановленные из генетический строки правила порождения:Генетическая строка aa aa eaa capc C a aaa aa aepc caC B AD BA BA DCS p e cD C A B aS b1000 1 11 1 11 00 110 0aa ba 00 00 48 3 12Saa baa aaaaa aepc ca BA DC00 00 10 10 00 00 00 1000 0000 0000 00 00 0011 1011 0010 00 00 0010 0010 0000 0000 00 Рис. 4.22. Грамматика графовой генерации, согласно Китано В конце разбора терминальные вершины {0,1} составляют финальную матрицу присутствия/отсутствия связи между соответствующими ней-ронами, подобно тому, как было приведено в [79] (п. 4.5.4). Топологиянейронной сети приводится к виду прямого распространения путем от-\n--- Страница 86 ---\n86 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ брасывания всех обратных связей. На рис. 4.22 приведен пример разбо- ра и порождения сети. В процессе восстановления нейронной сети воз- можно появление «мертвых» символов – без правил вывода. Такие сим- волы интерпретируются как нули по правилу 00000⎡ ⎤→⎢ ⎥ ⎣ ⎦. (4.53) Если в генетической строке имеются два правила с одинаковыми ле- выми частями, используется только первое встреченное. Грамматиче-ский разбор прекращается после некоторого предопределенного числа шагов. Все нетерминальные символы, присутствующие в матрице, за- меняются нулями. Китано использовал операцию выбора «по колесу рулетки», элитизм (сохранение в следующем поколении части лучших представителей по-пуляции), скрещивание с одной и несколькими точками сечения. Мута- ции производились путем случайного изменения символов в генетиче- ской строке на прочие, с вероятностью от 2 до 30%. Китано произвел сравнение своего метода с прямым кодированием, предложенным Миллером (п. 4.5.4) для задач 4 × 4 и 8 × 8 кодирования. Результаты предполагают несколько более быструю сходимость и луч- шее масштабирование для грамматики графовой генерации. Было ис-следовано влияние длины генетического кода на результаты для 5, 10,20 и 40 правил порождения в коде. Было отмечено, что большее число правил дает лучшие результаты в эволюционном процессе. Аксоновые деревья В модели, предложенной Нолфи [93 – 94], нейроны кодируются по- ложением точки на плоскости. Отображение информации о нейроне из генетической строки в фактический нейрон производится один к одно-му. Соединения нейронов в слои производится по специальным прави-лам, параметры которых сохранены в генетическом коде. Процесс построения связей между нейронами отображен на рис. 4.23. Нейроны, попавшие в крайнюю левую часть плоскости, рассматрива-ются как нейроны первого скрытого слоя, соответственно нейроны, по-павшие в правую часть, становятся выходными нейронами. Соединения между нейронами определяются путем построения специальных аксо- новых деревьев. Аксоновые деревья растут по правилу порождения: [] [] FF FF→− + . (4.54)\n--- Страница 87 ---\nГлава 4. Эффективные нейронные сети 87 Как видно, деревья представляют собой фрактальные структуры, ос- нованные на правилах L-системы (п. 4.5.5). Число итераций роста каж- дого дерева фиксируется (обычно 5) (рис. 4.23, б). Длина сегмента дере- ва и угол ветвления определяются из генетической информации(рис. 4.23, а). Соединение считается установленным всякий раз, когда ветвь дерева достигает другого нейрона (рис. 4.23, в) Генетическая строка в бay - координата на плоскости тип нейронапорогвес синаптической связидлина сегмента дереваугол ветвления дереваx - координата на плоскостипороговая функция нейрона Рис. 4.23. Построение сети, согласно Нольфи Индексное значение i входного или выходного нейрона сети ui опре- деляется по значению типа нейрона. Для входов индекс i1 = тип нейрона mod N1. Для выходов i0 = N – N0 + тип нейрона mod N0, где N1 – факти- ческое число входов, N0 – фактическое число выходов, N – общее число нейронов. Значения весов связей сохраняется в генетической строке, поэтому никакого дополнительного алгоритма обучения не было использовано.\n--- Страница 88 ---\n88 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Нолфи использовал нейронные сети для исследования и симуляции ис- кусственной жизни, заключающейся в поиске пищи и воды псевдосу- ществами [94, 95]. Также были исследованы возможности метода для построения автономных роботов [96]. Предложенный метод был в дальнейшем расширен и модифициро- ван в работе Канджелози [97]. Вместо прямого кодирования каждого нейрона точкой на плоскости в генетический код были добавлены пра- вила деления и миграции для некоторых «псевдоклеток», обеспечиваятем самым создание достаточного числа нейронов. Похожие правиладля генерации нейронов сети использованы Фуллмером в [98]. Правила роста нейронов сильно напоминают процесс построения в известной игре «Жизнь». Порождение начинается с «праклетки», кото-рая делится на дочерние клетки. После некоторого числа итерацийклетки «созревают» в нейроны. Во всем, что касается построения связеймежду нейронами, метод остался без изменений. L-системы Этот подход был разработан с использованием методологии так на- зываемых L-систем, предложенных Линденмайером в 1976 г. как способ моделирования морфогенеза растений. L-система представляет собой формальную грамматику, продукционные правила которой использу-ются для генерации строк символов, описывающих состояние некото- рой биологической системы. Например, если имеются начальная строка «F» и правило: FF FF F→−+ + − , (4.55) то после применения (4.55) на втором шаге строится строка FF FFFF FF FF FFFF FF−+ + −−−+ + −+ + −+ + −−−+ + − Полученная строка может иметь следующую интерпретацию: « F» – передвинуть рисующую след «черепашку» вперед, «–» – повернуть под углом налево, «+» – повернуть направо (рис. 4.24). Если ввести в алфавит два новых символа «[» и «]», можно работать с более натуралистичной моделью растения (рис. 4.25). Первый символ«[» сохраняет позицию и направление черепашки в стеке, второй «]» – восстанавливает верхнее состояние из стека. Перепишем правила: [[ [ ] ] [ ] ] XFXX F F XX→− + ++ − , (4.56) FF F→ .\n--- Страница 89 ---\nГлава 4. Эффективные нейронные сети 89 Рис. 4.24. Выполнение двух итераций по (4.55) Рис. 4.25. Грамматика (4.56) с углом 33° для пяти итераций Боер и Куипер предложили использовать для кодирования архитек- туры нейронной сети грамматику, являющуюся вариантом L-системы [99, 100]. В генетической строке, описывающей нейронную сеть, сохра- няются продукционные правила вида LPR S<>→ . (4.57) Правила вида (4.57) принадлежат к классу контекстно-зависимых L- систем. Символ языка P из предыдущей строки преобразуется в строку символов S в новой строке, если он имел символ L своим левым сосе- дом, а символ R своим правым. В один момент времени к текущему предложению последовательно применяются все возможные продукци- онные правила. В алгоритме широко используется принцип модульности, взятый из биологических систем. Модуль – это выделенная группа не связанных между собой нейронов сети, каждый из которых соединен с одним и тем же определенным набором узлов. Все четыре части правила (4.57) представляют собой либо нейроны, либо группы нейронов (модули), закодированные строкой определенно-го вида. Отдельные нейроны модуля обозначаются буквами алфавита {A, , Z} . Модуль нейронов выделяют скобками: [C,DE] и обрабатыва- ется в продукционных правилах как единственный символ. Каждый нейрон или модуль, по умолчанию, считается соединенным со смежными нейронами. Не соединенные друг с другом нейроны или модули разделяются запятой: С, D. Для обозначения связи несмежных\n--- Страница 90 ---\n90 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ модулей используется число N – скачок через N нейронов (модулей) вперед. Строка А1[С, DE]B означает, что нейрон А имеет соединения с нейроном В, но не имеет связи с модулем [С, DE]. Подобной нотации оказалось достаточно, чтобы задавать правила порождения нейронных сетей прямого распространения. 1)AB B B→ ; 2)B B [C,D]>→ ; 3)BC→ ; 4)CD C<→ ; 5)DD C 1>→ . В результате применения продукционных правил 1) –5) к аксиоме А, мы поэтапно получаем генетические строки. Итоговая строка [С, С1][С, С]С восстанавливается в сеть, изображенную на рис. 4.26. bc d g f eCC C DD DCC CC C C CC C C D D C D CB B B BBB A a Рис. 4.26. Восстановление нейронной сети с использованием правил 1 – 5. Результирующая строка [С, С1][С, С]С дает сеть (g) В генетической строке продукционные правила вида (4.57) кодиру- ются как трехбитовые двоичные числа. Эти числа переводятся в симво-лы в алфавите {А, , G, 1, , 5, [,],*}, где * – служебный символ разде- литель правил, с использованием специальной трансляционной табли- цы. Таким образом, генетическая строка представляет собой бинарную строку, к которой применимы классические операции мутации и скре-щивания. Для восстановления правил триплеты считываются из строкив три фазы и в обоих направлениях (по аналогии с процессом считыва- ния ДНК). Подобный способ способствует получению большого числа строк, которые не могут быть интерпретированы как правила, поэтомуиспользуется методика выявления и вычленения ошибок. Метод был применен для задачи «исключающего или» и к упрощен- ной задаче распознавания символов. Исследования данного подхода к кодированию нейронной сети показали, что метод не лишен недостат-ков [101]. Изначально провозглашенное свойство модульности претер-\n--- Страница 91 ---\nГлава 4. Эффективные нейронные сети 91 пело значительные упрощения при кодировании: модуль может быть связан только с одним или максимум с двумя другими модулями. Так как конструирование сети идет на уровне модулей, каждый из которых допускает произвольно сложную структуру, и нет механизма поощре-ния развития более простых особей, полученная в результате сеть неисключает перегруженность топологии. Клеточное кодирование Один из самых развитых методов кодирования нейронных сетей был предложен Ф. Груо в серии работ [102 – 108] под названием клеточное кодирование. Клеточное кодирование использует грамматическое дере-во для воспроизведения процесса развития «клеток» в нейронную сеть.Генетическая строка состоит из набора инструкций, применяющихся кпервоначальной «клетке», которая соединена со всеми входами и выхо- дами сети. В процессе выполнения инструкций восстанавливается ней- ронная сеть большего размера. Клетка начинает считывать инструкции с вершины дерева операций вида рис. 4.27, а. Узлы дерева представляют собой отдельные инструк- ции. Клетки – результат применения инструкций делятся и модифици- руются дальше, путем перемещения считывающей головки вниз по вет-вям дерева. Основными инструкциями можно считать: 1) последовательное деление (SEQ), дает в результате две клетки А и В, где А получает все входы исходной клетки и выход, соединенный с В, а клетка В соответственно соединена с выходами исходной клетки; 2) параллельное деление (PAR), для которого обе дочерние клетки наследуют входы и выходы исходной клетки; 3) завершение чтения (END), эта инструкция останавливает рост клетки, к которой была применена; 4) рекурсивный вызов (REC), производится рекурсивное повторение всех примененных перед этим операций к данной клетке. Рекурсивныевызовы генерируют модульную структуру сети и позволяют создавать крупные нейронные сети при небольшом количестве инструкций. Кроме перечисленных имеется более десятка других операций как для манипулирования числом и расположением клеток, так и для моди-фикации контекста клетки (весов связей, порога и т.п.). Обработка опе- рации и соответственно рост сети производится до момента, когда все клетки считают инструкцию «конец». На рис. 4.27, б приведен пример этапов построения нейронной сети.\n--- Страница 92 ---\n92 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ б авыходывыходы выходывыходывходы входы входы входышаг 3 шаг 2 шаг 1 шаг 0 Дерево операций SEQ PAR END REC END Рис. 4.27. Процесс «клеточного» построения сети Способы задания генетических операций, применяемых в методе клеточного кодирования, схожи с теми, которые использовал Коза в па-радигме генетического программирования (п. 4.5.4). Мутация реализо-вана путем изменения вершин дерева (команд) на команды с аналогич- ной арностью. Операция скрещивания выполняется путем обмена под- деревьями между родителями. Груо был сформулирован набор свойств генетического кода, способ- ного эффективно кодировать нейронные сети. Основные свойства сле- дующие: 1. Полнота – способ кодирования считается полным по отношению к классу архитектур, если любая возможная архитектура может быть импредставлена; 2. Компактность – способ кодирования А считается более компакт- ным, чем В, если для любой архитектуры сети длина кода А меньшедлины кода В; 3. Замкнутость – в отношении класса архитектуры нейронных сетей существует тогда, когда любая возможная строка кода восстанавливает- ся в архитектуру данного класса; 4. Модульность – способ кодирования является модульным, если он допускает повторное ссылочное использования одного участка кода внескольких местах генетической строки [106]. Клеточное кодирование дало толчок для создания большого количе- ства модификаций и вариаций на данную тему. В работах Фридриха и\n--- Страница 93 ---\nГлава 4. Эффективные нейронные сети 93 Морага [109, 110] было предложено использовать вещественные веса и расширенный набор команд. Публикации других авторов Кодйабахиана и Мейера [111, 112] предлагают геометрически ориентированное кле- точное кодирование, в котором добавлена парадигма выращивания свя-зей между нейронами. Люк и Спектор представили предварительныерезультаты для значительно модифицированного клеточного кодирова- ния [113] – кодирования по краям. Основным изменением является то, что для представления нейронной сети используется «лес» сетей, гдеотдельные сети имеют возможность ссылаться и вызывать рекурсивнопрочие сети. В варианте Талко [114] метод расширен правилами вывода активационных функций нейронов и параметров алгоритма обучения. В серии работ Хуссейна [115 – 123] был предложен способ кодирова- ния, основанный на грамматике, извлеченной из клеточного кодирова-ния, под именем атрибутная грамматика (NGAGE). В своем очень под-робном и обстоятельном обзоре [124] Хуссейн рассматривает сильные и слабые характеристики клеточного кодирования. Основным недостатком он находит его неоднозначность. В частности, идентичные поддеревьяразных деревьев не всегда будут восстанавливаться в одинаковые частинейронной сети, благодаря использованию некоторых команд. Клеточное кодирование было применено для решения ряда задач ро- бототехники [125, 126], а также для задач классификации [127]. До-вольно подробное исследование и сравнение результатов с конструк-тивными методами было проведено Фредриксоном [128] для целого ря- да практических задач. Проделанный обзор методов автоматического построения нейрон- ных сетей позволяет сделать следующие общие выводы. Конструктив-ные алгоритмы позволяют быстро создавать сети, но жестко заданные правила построения не позволяют совершать отклонения в пространст- ве архитектур от некоторой окрестности. Это позволяет говорить о кон-структивном подходе, как о подходе, реализующем локальный поиск, исильно подверженный попаданию в локальные минимумы. Особое место занимают алгоритмы редукции: они позволяют полу- чить неплохие результаты, но предполагают полноту имеющейся моде- ли до начала работы. Иными словами, до начала работы алгоритма ужепредполагается наличие некоторой нейронной сети, успешно решаю-щей проблему, для которой делается предположение о возможном ее упрощении. Это условие сильно ограничивает круг возможного приме- нения методов усечения.\n--- Страница 94 ---\n94 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Эволюционные алгоритмы несомненно реализуют охват всего про- странства архитектур и соответственно реализуют глобальный поиск, с большей вероятностью попадания в глобальный минимум. Но подоб- ный способ организации поиска занимает очень много времени. 4.6. Исследование подходов к построению нейронных сетей Все способы автоматического построения нейронных сетей можно условно разделить на два направления – конструктивные алгоритмы и методы глобального поиска [23]. И в том и в другом случаях универ- сальную процедуру для получения нейронной сети с желаемыми свой-ствами в общем виде можно определить следующим образом. В про-странстве всех возможных нейронных сетей по специальной процедуре производится перебор вариантов нейронных сетей, которые последова- тельно оцениваются. Попытаемся примерно оценить размерность про-странства архитектур нейронных сетей. 4.6.1. Размерность пространства поиска Все множество подклассов A, определяемых выбранной топологией сети, можно оценить по рекурсивной формуле, предложенной в [86]. Допустим, некоторым образом стало известно число скрытых слоев L и число нейронов γ, необходимых для решения некоторой проблемы. Возможное распределений нейронов по слоям можно определить ре- курсивной формулой T(L,γ). Для простейшего случая имеем (,) 1TL γ=, если 1L= или 1γ=. (4.58) Если число нейронов γ < L, то полагаем T(L,γ) = 0. В случае γ > L мы получим ( , ) ( 1, 1) ( , 1)TL TL TLγ=− γ − + γ − . (4.59) Формула (4.59) рекурсивно учитывает число возможных разбиений, опираясь на более простую архитектуру с γ – 1 нейронами. Первое сла- гаемое в (4.59) учитывает число возможных разбиений для случая, ко-гда новый дополнительный слой содержит единственный дополнитель-ный нейрон. Второй член предполагает, что дополнительный нейрон был добавлен к последнему скрытому слою более простой архитектуры, которая имеет L слоев.\n--- Страница 95 ---\nГлава 4. Эффективные нейронные сети 95 Кроме множества вариантов архитектур в пределах каждого класса Ai ⊂ A нейронные сети будут характеризоваться еще вектором взвешен- ных связей между нейронами сети W = {wi, i = 1, , p}. Попытаемся оценить размерность p вектора весов сети W для фиксированного зна- чения числа нейронов сети γ. В таком случае все возможные топологии будут варьироваться в пределах двух крайних классов архитектур: от ALγ =1 ⊂ A – сети только с одним скрытым слоем ( L = 1) до максимально вытянутой сети ALγ =γ ⊂ A по одному нейрону на скрытом слое ( L = γ). Будем предполагать полную связность всех нейронов сети, то есть каждый слой нейронов, включая входы сети, соединен со всеми после-дующими от него слоями, включая все выходы сети. Все нейронные се- ти с меньшим числом весов W могут быть получены из полносвязной сети путем установки соответствующих весов в нулевое значение. Та-ким образом, для первого варианта максимальное количество весовоценивается соотношением 1 ()Lpk o k o==γ + + ⋅ . (4.60) Здесь предполагается, что сеть имеет k входов и o выходов. В случае максимально вытянутой сети размерность увеличивается за счет взаимного соединения слоев: 2 ()2Lpk o k o=γγ+ γ=+ γ + + ⋅ . (4.61) Очевидно, что в реальных задачах граничные варианты архитектуры будут встречаться довольно редко. Вместо них будет использован неко-торый промежуточный вариант и реальное значение p будет лежать ме- жду p L=0 и pL=γ. Необходимо также подчеркнуть, что даже для известной архитекту- ры с известным количеством слоев и числом нейронов, с фиксирован-ным числом весов сети, собственно для получения нейронной сети,корректно решающей поставленную задачу, существует необходимость в определении конкретных значений весов сети или, иными словами, в обучении сети. Таким образом, получено представление о значительной размерно- сти пространства архитектур нейронных сетей, по которому предстоит осуществлять поиск. Исходя из этого, можно сделать вывод о том, что прямой перебор архитектур реализовывать нерационально. Отсюда сле-\n--- Страница 96 ---\n96 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ дует, что единственным разумным способом построения нейронной се- ти является использование некоторой эвристики перебора моделей. Именно такой способ автоматического построения нейронных сетей реализуют конструктивный и эволюционный подходы. 4.6.2. Анализ известных направлений Конструктивные методы, исторически более ранние, реализуют по- иск архитектуры путем итеративного изменения сложности выбранноймодели. Решение о способе модификации сети принимается в соответ-ствии с некоторым заданным алгоритмом построения. Алгоритм по-строения задается таким образом, чтобы добавление новых нейронов (связей) на каждом этапе гарантированно уменьшало (в случае усечения не увеличивало) значение ошибки работы сети. Общий принцип работы подобных алгоритмов носит характер лока- лизации неверно решаемых нейронной сетью примеров обучающей вы- борки D, фиксирование уже построенной сети для правильно решаемых примеров и добавление архитектуры нейронами (связями), корректи-рующими работу сети для неверно решаемых примеров. Нагляднымпримером могут являться бинарные алгоритмы (п. 4.4.1), где этот прин- цип выражен в явном виде. В одном из самых исследованных методов построения нейронных сетей – каскадной корреляции – также можнопроследить эту схему: новый нейрон-«кандидат» обучается изолиро-ванно от уже построенной сети, причем так, чтобы максимально влиятьна остаточное значение ошибки всей сети. Построение завершается, ко- гда ошибка уменьшается до желаемого значения. Приведенный выше принцип построения модели предполагает неко- торую избыточность в структуре готовой нейронной сети. Так, в рабо-тах [51, 56] отмечено, что техника «заморозки» весов каскадной корре- ляции приводит к построению нейронной сети несколько большего размера, чем необходимо. Избыточность неизбежно возникает при кон-структивном подходе, при необходимости обеспечения относительнойизоляции уже построенной сети и соответственно той части обучающей выборки, для которой верно находится решение, от воздействия нового нейрона (веса). В ряде современных конструктивных алгоритмов:Kogi9, КасПер (п. 4.4.5), делается попытка уйти от этой проблемы. Большая часть конструктивных алгоритмов жестко фиксирует спо- соб наращивания архитектуры, например добавлением нейронов в единственный скрытый слой сети, или же, наоборот, присоединением\n--- Страница 97 ---\nГлава 4. Эффективные нейронные сети 97 всякий раз нового скрытого слоя с единственным нейроном (п. 4.4.5). Очевидно, что тот или иной фиксированный способ будет обеспечивать уменьшение ошибки в равной степени не для всех типов задач. Так, в работе Хванга и др. [45] было показано, что каскадная корре- ляция на некоторых задачах классификации и регрессии будет созда-вать сети с неоптимальной обобщающей способностью. Хванг объясня- ет этот результат использованием в алгоритме механизмов каскадиро- вания и корреляции, что влечет за собой слишком зубчатые значениявыходов сети. Конструктивные алгоритмы строят нейронные сети путем итератив- ного изменения сложности модели. Подобная организация поиска за- трагивает, фактически, только смежные классы архитектур A i в про- странстве архитектур A. Иными словами, в таком случае можно только рассчитывать на попадание в ближайший минимум, как правило, ло-кальный. В итоге результаты будут значительно зависеть от выбора на- чальных параметров алгоритма. Основным преимуществом конструктивного подхода можно считать небольшое время работы. Это следует напрямую из алгоритма построе-ния, при котором всякий раз имеем дело с нейронной сетью, уже, хотя бы частично, решающей задачу. Каждое дальнейшее усложнение архи- тектуры только улучшает результат. Еще одним положительным ре-зультатом подобного подхода к построению сетей будет являться ие-рархическая организация архитектур созданных нейронных сетей. В ря- де случаев при использовании некоторых эффективных критериев оце- нок обобщающей способности нейронной сети, для корректного срав-нения различных построенных нейронных сетей свойство вложенностибудет важным (п. 4.3). Особое место среди конструктивных алгоритмов занимают алгорит- мы редукции сети: они позволяют получить неплохие результаты, нопредполагают полноту имеющейся модели до начала работы. Инымисловами, до начала работы алгоритма уже предполагается наличие не-которой нейронной сети, успешно решающей проблему, для которой делается предположение о возможном ее упрощении. Очевидно, что не для всех случаев предварительно может существует такая сеть. Это ус-ловие сильно ограничивает круг возможного применения методов ре-дукции. Существенным шагом вперед на пути разработки алгоритмов, строящих нейронные сети, является использования такого механизма\n--- Страница 98 ---\n98 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ глобального поиска, как эволюционные алгоритмы. Методы глобально- го поиска, как следует из названия, основаны на поиске в пределах всех возможных моделей в пространстве архитектур. Среди всех топологий нейронных сетей целенаправленно ищется единственная сеть, которая будет формировать наилучшую оценку ˆ()fx . Применительно к нейронным сетям генетические алгоритмы дают достаточно эффективный механизм выбора архитектуры. Эволюцион- ный подход обеспечивает глобальный охват пространства поиска и, очевидно, в процесс поиска будут вовлекаться как простые, так и слож-ные архитектуры. Немаловажным отличием эволюционного подхода отконструктивного можно считать обеспечение охвата всех классов архи- тектур A, а не только смежных. Поиск по всему пространству дает большие шансы для нахождения глобального минимума. Кроме этого,способ организации работы генетических алгоритмов дает возможностьнаходить решение при произвольно сложном виде оценочной функцииприспособленности. Слабым местом при организации построения нейронных сетей мето- дами эволюционного поиска будет являться весьма значительное времяполучения результата. Это обусловлено необходимостью на каждомэтапе эволюции производить сначала построение, а затем и перебор большого числа индивидуумов, при этом вычисляя значения функции приспособленности для каждого. Именно за счет поддержки популяциииндивидуумов – возможных решений поставленной задачи – реализует-ся больший охват пространства архитектур. В некоторых реализациях делаются попытки снижения времени ра- боты ГА, например у Китано (п. 4.5.5), в генетической строке ограничи-вается размер представимой нейронной сети, тем самым искусственносужается пространство поиска. Другим популярным способом является распараллеливание эволю- ционного процесса на мультипроцессорных архитектурах [128]. Гене-тические алгоритмы очень эффективно могут быть распараллелены:всю популяцию можно произвольным образом разбить на субпопуля-ции, и вычисление функции приспособленности может вестись незави- симо во всех субпопуляциях. Вычисление лучшего среди всей популя- ции индивидуума делается в момент синхронизации данных о победи-телях в каждой субпопуляции. Этот способ является очень эффектив-ным, но требует соответствующей технической базы.\n--- Страница 99 ---\nГлава 4. Эффективные нейронные сети 99 Проведенные исследования обоих направлений по построению ней- ронных сетей позволяют говорить о том, что однозначного лидера не существует. Оба приведенных выше подхода обладают и достоинства- ми и недостатками. Конструктивные алгоритмы за разумное время по-лучают нейронную сеть, слегка отличающуюся от оптимальной. С дру-гой стороны, эволюционный подход к построению обеспечивает, как правило, лучший результат, но при этом затрачивается значительное количество времени. Было бы интересным сделать попытку объединения лучших сторон обоих подходов: использовать принцип поэтапного построения нейрон- ной сети, присущий конструктивному направлению, одновременно с мощным поисковым механизмом, реализуемым генетическими алго-ритмами. В результате может получиться алгоритм с неплохими харак-теристиками. 4.7. Метод мониторинга динамики изменения ошибки В данном разделе рассматривается модификация метода динамиче- ского добавления нейрона путем усовершенствования мониторингаскорости изменения ошибки [129, 130]. Метод динамического наращи-вания узлов среди всех конструктивных алгоритмов можно выделить как наиболее интересный в плане общего определения схемы построе- ния (п. 4.4.4). Основным моментом, в котором была произведена моди-фикация, является способ определения скорости изменения ошибки. Оригинальный метод динамического наращивания предлагает до- вольно простую схему построения нейронной сети [39]. Процесс поиска архитектуры начинается с нейронной сети с одним нейроном в единст-венном скрытом слое. В [39] предполагается, что сложность сети не со-ответствует поставленной задаче, если ошибка сети с числом эпох обу-чения уменьшается недостаточно быстро. Значение скорости уменьше- ния ошибки вычисляется по формуле (4.26). При падении скорости уменьшения ошибки ниже заданного порога в скрытый слой добавляет-ся нейрон. Как видно из (4.26), при вычислении значения скорости ошибки принято рассматривать абсолютное изменение ошибки по отношению к некоторому первоначальному значению ошибки и не учитывать воз-можность ее кратковременного увеличения вследствие, например, эф-фекта переобучения.\n--- Страница 100 ---\n100 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ В варианте, предложенном авторами данной монографии, несколько изменился способ оценки скорости ошибки: принимается во внимание динамика изменения ошибки на всем интервале обучения δ: {}12 1ii tt iEE +δ −δ −δ =− <Ωδ∑ , 0 ( ), [1, ]i tt≥δ ∈ δ . (4.62) Здесь 1i tE +−δ и i tE−δ представляют собой значения ошибок на интерва- ле обучения δ, Ω – некоторая заданная минимально возможная скорость изменения ошибки, при превышении которой принимается решение обизменении сети, δ – количество эпох-циклов обучения сети перед про- изведением оценки скорости ошибки. Заметим, что при оценке скорости по (4.62) учитывается возмож- ность наличия периодов временного роста ошибки на интервале δ, при-чем за счет эффекта усреднения отсекаются локальные провалы произ- водительности на интервале обучения δ . В результате подобного подхо- да к определению скорости ошибки общая производительность методадинамического наращивания узлов улучшается. За счет оценки общейдинамики скорости на значительном интервале обучения δ отпадет не- обходимость усложнения топологии нейронной сети, вызванная вре- менным, не системным, замедлением скорости уменьшения ошибки. Более того, интервал δ можно теперь сделать большим по длительности. Помимо изменения способа определения скорости ошибки была предложена иная схема построения нейронной сети. Алгоритм работы метода мониторинга динамики изменения ошибки выглядит следую- щим образом. Исходной является нейронная сеть прямого распростра-нения с одним скрытым слоем. Обучение производится методом обрат-ного распространения ошибки с инерцией и адаптивной скоростью спуска. После δ эпох обучения производится оценка скорости ошибки по формуле (4.62). При скорости падения ошибки, меньшей некоторогодопустимого порога Ω, производится модификация нейронной сети: до-бавляется несколько нейронов в текущий скрытый слой – единственный на начальном этапе. При достижении некоторого первоначально задан- ного из эмпирических соображений числа γ L нейронов в слое создается новый скрытый слой и нейрон добавляется уже в него. При добавлениинового слоя число нейронов в предыдущем активном слое уменьшаетсяв T раз.\n--- Страница 101 ---\nГлава 4. Эффективные нейронные сети 101 Дополнительно было исследовано два варианта работы алгоритма: - найденные на предыдущем шаге значения весов сети и параметров нейронов «замораживались» и использовались после модификации ней- ронной сети. Добавившиеся веса связей и пороги нейронов иницииро-вались случайными значениями. Данный вариант применялся толькодля сети с одним скрытым слоем; - при добавлении нового скрытого слоя, когда число нейронов на предыдущем слое уменьшалось, веса всех нейронов сбрасывались, т.е.сеть полностью переобучалась. В качестве тестовой проблемы были взяты данные реальной задачи из области предсказания землетрясений [131]. В работе исследуется за- висимость между интенсивностью импульсов электромагнитных полейтектонических разломов и возможностью землетрясения. Непосредст-венно перед тем, как начинаются заключительные процессы подготовкиземлетрясения, происходит прекращение движения блоков относитель- но соседних с ним участков. Следствием этого становится уменьшение интенсивности регистрируемого сигнала, что должно говорить о подго-товке к тектоническому возмущению. Результаты практического при-менения предложенного алгоритма к данному кругу проблем опублико- ваны в [132]. В данном разделе ограничимся констатацией, которая позволяет сделать следующие выводы: 1) метод динамического добавления нейрона, используемый без при- менения к более информированной стратегии построения нейронной се- ти, можно считать эффективным только для малых задач, т.е. требующихпостроения небольших нейронных сетей. Для задач большей размерностидля получения результата, близкого к оптимуму, требуется специальный подбор параметров алгоритма (порог скорости, δ и т.д.); 2) достоинством предлагаемого подхода можно считать то, что он не накладывает никаких ограничений на топологию нейронной сети и спо-собы ее построения, поэтому его можно свободно комбинировать с дру-гими алгоритмами оптимизации нейронных сетей; 3) в рамках дальнейшего усовершенствования приведенного метода мониторинга динамики изменения ошибки возможно использованиеадаптивно меняющегося значения величины порога Ω в процессе рабо- ты алгоритма; 4) несмотря на то, что алгоритм мониторинга динамики изменения ошибки оказался достаточно эффективным, исследования в области со-\n--- Страница 102 ---\n102 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ четания эволюционного и конструктивного подходов следует вести не- сколько в ином направлении, чему и посвящены последующие разделы; 5) идея мониторинга динамики изменения ошибки оказалась доста- точно продуктивной и использовалась нами в качестве составного эле-мента развиваемого ниже более общего метода. 4.8. Эволюционное накопление признаков 4.8.1. Предлагаемая организация поиска архитектуры Рассматривая алгоритмы, реализующие конструктивный и эволюци- онный подходы, приходим к следующим заключениям. Конструктив-ный подход позволяет строить нейронные сети за очень малое время, но обеспечивает только локальный поиск по пространству архитектур, за- ведомо обрекая на попадание в ближайший локальный минимум. Схемапостроения большинства конструктивных алгоритмов жестко фиксиро-вана по причине необходимости уменьшения ошибки на каждой итера- ции. Тем самым рост архитектуры в пространстве реализуется только в одном направлении. Эволюционный подход обеспечивает глобальныйпоиск в пространстве архитектур и охват всех классов архитектур, а нетолько смежных. Поиск по всему пространству дает больше шансов длянахождения глобального минимума. Основным недостатком эволюци- онного поиска можно считать очень длительное время нахождения ре- шения. Практические исследования нейронных сетей показывают, что для многих задач, в условиях зашумленных наборов данных ограниченной размерности, совершенно различные нейронные сети обеспечивают практически одинаковые результаты [24]. Таким образом, при поискеархитектуры нейронной сети не обязательно охватывать все простран-ство целиком. Для нахождения приемлемого результата достаточно бу- дет реализовать стратегию перемещения по пространству небольшими скачками с подробным поиском в пределах некоторой области. Предлагается следующая стратегия организации поиска архитекту- ры. Основополагающим остается принцип, согласно которому слож- ность нейронной сети в процессе поиска итеративно увеличивается. Изменения, которые необходимо внести в первоначальную модель, оп-ределяются при помощи генетических алгоритмов. Для этого нами раз-работан специальный способ кодировки архитектуры нейронной сети вгенетическую строку, учитывающий наличие постоянной части сети.\n--- Страница 103 ---\nГлава 4. Эффективные нейронные сети 103 Формально эта процедура будет выглядеть следующим образом. Имеется некоторая базовая нейронная сеть с архитектурой A. На каж- дом этапе алгоритма будем получать новую нейронную сеть Ã путем добавления к сети некоторого количества нейронов (γ ∆ > 1). Способ до- бавления допускает образование как новых слоев L∆, так и дополни- тельных связей p∆. Фрагмент ∆ будем называть наращивающим блоком нейронной сети. Способ добавления достраивающего блока ∆ будем определять при помощи эволюционной процедуры AA=+ ∆/tildenosp . (4.63) С другой стороны, на данный способ организации поиска можно взглянуть, как на разновидность конструктивного алгоритма. Здесь фактически реализован аналогичный итеративный принцип построения сети, за единственным исключением: вместо жесткого способа построе-ния сети, например каскадированием или по слоям, способ надстройкина каждом этапе будет выбираться произвольно, в зависимости от по- требности задачи. Необходимость будет оцениваться специальной эво- люционной процедурой, реализующей целенаправленный перебор иоценку вариантов. Более того, условие возможного добавления сразунескольких нейронов допускает реализацию одновременно и каскадно-го и послойного наращивания. Следует заметить, что многие эффективные алгоритмы конструк- тивного направления для того, чтобы сгладить ограничения жесткогоспособа построения, в неявном виде реализуют переборную стратегиюв неявном виде. Так, во всех модификациях алгоритма каскадной кор- реляции для выбора нейронов-«кандидатов» используется так называе- мый пул из нескольких нейронов. Нейроны всего пула поочередно оце-ниваются, и из всего пула выбирается тот, что будет влиять на ошибкусети больше всех. Как было показано выше, эволюционная процедура, предлагаемая в данном подходе, имеет специальный способ кодировки архитектурынейронной сети в генетическую строку, учитывающий наличие посто-янной части сети. Фактически это будет обозначать разделение генети-ческой строки на постоянную и варьируемую части. В терминах теоре- мы шаблонов положительный эффект от подобной организации можно обосновать следующим образом. Как известно, по мере сходимости эволюционной процедурыалго- ритм формирует генетическую строку, отдельные участки которой из- меняются все реже и реже, по мере образования строительных шабло-\n--- Страница 104 ---\n104 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ нов. Такой эффект свидетельствует о близости конечного решения. По- стоянная часть генетической строки будет обусловлена конкретной ар- хитектурой нейронной сети, полученной на предыдущей итерации. Из способа построения предполагается, что сформированная подобнымобразом постоянная часть даст лучшие частичные решения для решае-мой нейронной сетью задачи. Можно считать, что постоянная часть на- шей генетической строки будет фактически являться строительным шаблоном, зафиксированным принудительно. Таким образом, времясходимости генетической процедуры значительно ускоряется. Необхо-димо добавить, что при этом также сокращается длина генетической строки, что дополнительно увеличивает скорость работы. Организация постоянной и переменной частей переложена на разра- ботанный нами метод кодирования нейронной сети в генетическуюстроку, подробно изложенный в следующем разделе. Заметим также,что постоянная часть не будет содержать в себе полное описание архи- тектуры базовой сети, а только тех ее элементов, которые существенны для переменной части. Выделяя постоянную часть в генетическом коде, тем самым реализу- ем следующую, более естественную концепцию эволюционного процес- са, когда основная часть генетической информации через поколения со- храняется неизменной. Классические генетические операции, такие, какскрещивание и мутация, могут создавать потомков, радикально отли-чающихся от своих родителей по всему набору признаков. Очевидно, что подобные революционные изменения в генотипе не характерны для жи- вой природы. Если на микроуровне быстрая смена генотипа микроорга-низмов еще возможна, то на макроуровне некий базовый набор призна-ков не изменяется или меняется медленнее. Вряд ли можно предполо- жить, что появление наземного животного мира было обусловлено фор- мированием легких у водоплавающих в пределах всего одного-двух по-колений. Таким образом, выделение постоянной и переменной частей вгенетическом коде можно считать более естественным способом для вос-создания эволюционного процесса получения решения. 4.8.2. Реализация кодирования путями В немалой степени эффективность генетического поиска будет зави- сеть от правильности выбора способа кодирования информации об ар- хитектуре нейронной сети в генетическую строку. Более того, органи- зация построения нейронной сети по предложенному в предыдущем\n--- Страница 105 ---\nГлава 4. Эффективные нейронные сети 105 пункте способу налагает дополнительные требования на кодирующую схему. Генетическая информация в строке должна содержать сведения о способе добавления достраивающего блока в основной сети. Исследования показывают, что порождающее кодирование для ряда задач оказывается значительно менее эффективным [133, 134] и тре-бующим больше времени для достижения результата [78]. Поэтому для предложенной схемы поиска эффективнее было бы использование пря- мой схемы кодирования. В данной работе было использовано развитие варианта прямого ко- дирования, основанного на «путях» передачи сигнала в нейронной сети, предложенного в [135]. Данный вариант прямого кодирования предпо- лагает сохранение в генетическом коде не информации о количествеслоев, нейронов или связях в нейронной сети, а условные «пути» пере-дачи сигнала в сети от входов к выходам. В виде, предложенном в [135], путь в сети будет определяться спи- ском нейронов, начиная только с любого из входных нейронов и закан- чиваясь только одним из выходных нейронов. Никаких ограничений напредставление и порядок нейронов внутри такого «пути» не налагается.Оригинальный метод имеет некоторые ограничения: он предполагает ограничение размеров сети сверху и может порождать нейронные сети произвольного вида, не ограничиваясь классом нейронных сетей прямо-го распространения. Метод кодировки «путями» авторы подвергли существенным изме- нениям применительно к поставленной задаче. Все требования к моди- фицированному построению нейронной сети путями можно изложитьследующим формальным образом. 1) До начала формирования генетической строки имеем некоторую базовую нейронную сеть прямого распространения, состоящую из ней- ронов Г = {h 1, , hγ} и имеющую k входов и o выходов. Зададим некото- рое количество строящих нейронов U = {hγ+1, , hγ+u}. В формировании «путей» будут участвовать все нейроны { Г,U}, то есть новые нейроны будут равноправно участвовать при построении «путей» наряду с ней- ронами базовой сети. 2) Архитектура нейронной сети задается набором множеств – «путей» продвижения сигнала { Pi}. Путь продвижения сигнала в нейронной сети задается упорядоченным списком нейронов из Г и U, тем самым модели- руя расположение нейронов последовательно по всем слоям, начиная от слоя расположения начального и заканчивая слоем расположения конеч-\n--- Страница 106 ---\n106 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ ного. Каждый путь P может начинаться только с нейрона из набора до- пустимых стартовых нейронов S и может заканчиваться одним нейроном из набора допустимых выходных нейронов E. Повторения нейронов в пределах одного «пути» не допускается. Никакого другого ограниченияна порядок нейронов внутри пути не накладывается. 3) Возможно несколько способов формирования наборов S и E. При практической реализации данного способа кодирования авторы остано- вились на следующем. Набор допустимых стартовых нейронов S фор- мируется из всех нейронов базовой нейронной сети Г, за исключением выходов сети и последнего скрытого слоя. Набор допустимых выход- ных нейронов E соответственно из всех нейронов базовой сети Г за ис- ключением входов сети и первого скрытого слоя. Естественно, что в обанабора не попадают строящие нейроны U. 4) Непосредственно в генетической строке нейроны представляются своими индексами. Каждый из путей отделяется друг от друга специ- альным служебным символом '#'. Предложенную схему может проиллюстрировать рис. 4.28. (Постоянная часть, в эволюции не участвует) 45 ##2 35 ##2 45 ##1 35 ##1 #2 5# #3 7 8 5# #1 6 4 5#Генетическая строка: (варьируемая часть)} { 2 5 }{ 3 7 8 5 }{ 1 6 4 5 }Возможные пути:Набор допустимых выходных нейронов = { 3 4 5} E Набор допустимы стартовых нейронов = { 1 2 3 4} S {U= Строящие нейроны 1 23 488 77 665 Рис. 4.28. Предложенный вариант кодировки путями Генетические операции В оригинальном методе к построенному пути применяются следую- щие генетические операции: оператор скрещивания с двумя точками разбиения и четыре варианта оператора мутации [135]. В модификации метода «путей» для работы с генетическими строками использовались\n--- Страница 107 ---\nГлава 4. Эффективные нейронные сети 107 следующие генетические операции: скрещивание с одной точкой раз- биения и два варианта операции мутации. Операция скрещивания, или пересечения, служит для закрепления полезных для решения постав- ленной задачи свойств генетической строки. Скрещивание выбираетслучайным образом точку между путями в генетической строке двухиндивидуумов. Результатом будут являться два потомка, составленные из путей, принадлежащих обоим родителям (рис. 4.29). 9#10 13 #3 9#10 13 #3 8#12 15 #1 8#12 15 #1 8##2 8##2 8#12 15 #1 9# 10 13 3 8# #2 # Измененные нейроны МутацииТочка разбиения Потомок № 2 Операция мутацииОбъединение с перестановкой Потомок №1 9#10 11 #3 9#10 11 #3 9#10 13 2 # 9#10 13 2 # 9#10 #2 9#10 #2 12 11 9# 11 #3 9# 10 #29# 10 #2 11 10 9# #3 9# 10 13 #2 9# 10 #2 9#10 11 #3 9# 10 13 #2 9# 10 #2 Операция к россингове ра Рис. 4.29. Генетические операции Как было уже сказано, используется два способа внесения мутаций в генетический код. Первый вариант оператора мутации производит слу-чайным образом замещение нейронов в произвольном числе «путей», составляющих генетическую строку индивидуума. Второй вариант опе- ратора мутации предусматривает создание случайным образом генети-ческой строки полностью. Внесение случайных изменений – мутаций –позволяет производить скачки в пространстве архитектур и выбираться из локальных экстремумов, куда нас может привести поиск оптималь- ной архитектуры. Необходимо отметить, что применение генетическихопераций может привести к появлению некорректных архитектур длянейронной сети прямого распространения. Распознавание и исправле- ние генетического кода подобных индивидуумов производится на этапе восстановления нейронной сети из генетического кода.\n--- Страница 108 ---\n108 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Новизна и эффективность приведенного подхода заключается в спо- собе организации эволюционного перебора. В отличие от общеприня- тых подходов к эволюционному построению нейронных сетей на каж- дом этапе генетический код, определяющий ∆, всегда будет иметь ма-лый размер и соответственно меньший размер пространства поиска.Общепринятые подходы имеют дело с индивидуумами, пытающимися решить сразу всю задачу. В короткой генетической строке гораздо быстрее образуются строи- тельные блоки, что ускоряет сходимость метода. 4.8.3. Функция приспособленности В генетических алгоритмах под «приспособленностью» отдельного индивидуума популяции к среде рассматривается его способность кэффективному решению некоторой заданной целевой функции – в на-шем случае построения эффективной нейронной сети. Существует множество вариантов задания функции приспособленности для по- строения нейронных сетей эволюционными методами. Далее предлага-ется оригинальный вариант определения функции приспособленности.По форме задание функции приспособленности, предложенное автора- ми, попадает под общий принцип, известный как принцип описания минимальной длины [136]. В исследованиях использовалась функцияприспособленности F следующего вида: train valid11FC SEE=++ + . (4.64) Здесь Etrain – среднеквадратическая ошибка сети, восстановленной из генетической строки, заданная на обучающей выборке, Evalid – средне- квадратическая ошибка сети на проверочной выборке, C – оценка слож- ности топологии нейронной сети, S – скорость уменьшения ошибки нейронной сети на проверочных данных. Величина C служит для определения сложности топологии сети и оценивается по следующей эмпирической формуле: 11 1111,0 1,0FcNC FcN⎧⎛ ⎞ ⎫+σ >⎜⎟⎪⎪ σ ⎝⎠=⎨⎬⎛⎞⎪⎪ +σ ≤⎜⎟⎩⎝ ⎠ ⎭, где 1 train1FE= . (4.65) Здесь через N обозначено общее число нейронов в сети,\n--- Страница 109 ---\nГлава 4. Эффективные нейронные сети 109 1 2 11()N ii innN=⎡ ⎤σ= −⎢ ⎥⎣ ⎦∑ представляет собой стандартное отклонение и характеризует степень разброса количества нейронов в скрытых слоях сети от некоторого среднего числа нейронов in по слоям, ni – число нейронов в скрытом слое i. Согласно (4.65), большую величину С будут иметь нейронные сети, имеющие малое число нейронов, расположенных максимально равномерно по скрытым слоям. Именно последнее слагаемое в (4.65) поощряет нейронные сети без резких отличий числа нейронов на скры-тых слоях. Скорость уменьшения ошибки сети на проверочных данных S опре- деляется в (4.66) и характеризует архитектуру нейронной сети: {}12 1ii tt iEE +δ −δ −δ =− <Ωδ∑ 0 () , [ 1 , ]i tt≥δ ∈ δ . (4.66) Здесь 1i tE +−δ и i tE−δ представляют собой значения ошибок на интерва- ле обучения δ, Ω – некоторая заданная минимально возможная скорость изменения ошибки, при превышении которой принимается решение обизменении сети, δ – количество эпох-циклов обучения сети перед про- изведением оценки скорости ошибки. Можно считать, что сложность сети не соответствует поставленной задаче, если ошибка с числом эпох обучения уменьшается недостаточнобыстро либо не уменьшается совсем. Способ оценки скорости взят изпредложенного метода мониторинга динамики изменения ошибки (п. 4.7). При данной оценке скорости ошибки принимается во внимание динамика изменения ошибки на некотором интервале обучения δ . Здесь i tE−δ представляют собой значения ошибки на интервале обучения δ в момент времени i. Заметим, что при оценке скорости по (4.66) учитыва- ется возможность наличия периодов временного роста ошибки на ин- тервале δ, причем за счет эффекта усреднения отсекаются локальные провалы производительности, вызванные временным замедлением ско-рости уменьшения ошибки.\n--- Страница 110 ---\n110 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ 4.9. Алгоритм эволюционного наращивания нейронной сети Соображения по поводу организации поиска нейронной сети, пред- лагаемого способа кодирования и реализации функции приспособлен- ности реализовываются в виде предлагаемого нового алгоритма для по- строения нейронных сетей, названного алгоритмом эволюционного на-ращивания нейронной сети [137 – 142]. Опишем формально схему ра-боты алгоритма. Основная идея алгоритма эволюционного наращивания нейронной сети заключается в том, чтобы строить нейронную сеть в процессе обу-чения. Сеть строится на основе исходной сети, состоящей в общем видетолько из входов и выходов. Сеть достраивается новыми нейронами и связями как между существующими, так и новыми нейронами. В каче- стве новых добавочных блоков нейронов используются подсети нейро-нов, полученные эволюционным способом. Подсеть выбирается эволю-ционным способом таким образом, чтобы уменьшить общую ошибкунейронной сети на предложенной обучающей выборке. Алгоритм мож- но условно разбить на два этапа. Основной этап работы алгоритма бу- дет заключаться в реализации механизма поиска наращивающих бло-ков. Вторым этапом алгоритма будет надстройка нейронной сети и про-верка эффективности ее работы. Рассмотрим алгоритм эволюционного наращивания нейронной сети по этапам. Поиск наращивающих блоков Предлагаемый в данном алгоритме способ осуществления эволюци- онного поиска представляет собой основу данного метода построениянейронной сети и выделяет его среди прочих методик поиска топологиинейронных сетей с помощью генетических алгоритмов. Его основнойотличительной чертой будет являться то, что в каждый момент в эво- люционном переборе будет участвовать лишь часть нейронной сети, в то время как в большинстве методик в генетический поиск вовлекаетсявся сеть. На этапе инициализации эволюционного поиска предполагается, что уже имеется некоторая нейронная сеть, которую необходимо улучшить. В начале работы алгоритма берется сеть, состоящая только из входов ивыходов – без скрытого слоя нейронов. Задается некоторое максималь-но возможное количество нейронов, которые разрешено добавить к сети\n--- Страница 111 ---\nГлава 4. Эффективные нейронные сети 111 в результате эволюционного поиска. На основе нейронов этой сети и полученной информации о дополнительных нейронах случайным обра- зом генерируется некоторая начальная популяция. Затем выполняется этап генетического поиска: для каждого индивидуума из текущей попу-ляции производится вычисление оценочной функции приспособленно-сти. Данная функция в нашем случае будет определять эффективность и точность работы нейронной сети, восстановленной из генетической строки, при решении поставленной задачи. Все индивидуумы ранжи-руются в соответствии со своим значением функции приспособленно-сти. Для получения нового поколения, генетически более приспособлен- ного к среде, к выбранным индивидуумам применяются генетическиеоперации мутации и скрещивания. После этого опять производится вы-числение функции приспособленности и выбор новых кандидатов в ро-дители. Весь процесс эволюционного порождения останавливается, ко- гда не происходит существенного увеличения величины лучшего зна- чения функции приспособленности для популяции. На этом эволюци-онный этап работы алгоритма заканчивается. Среди лучших представи-телей популяции выбирается тот, который даст способ наращивания нейронной сети. Пример генетического кода индивидуума победителя, давшего наибольшее улучшение работы нейронной сети, представленна рис. 4.30. #8 12 15 #1 9# 10 13 8##2 #3 9#10 11 #3 9# 10 13 #2 9# 10 #2 Рис. 4.30. Генетическая строка При извлечении информации из данной генетической строки полу- чаются два наращивающих блока (рис. 4.31, а). После нахождения строительных блоков для нейронной сети эволюционный этап считает- ся завершенным. Надстройка сети Надстройка нейронной сети является очевидным следующим шагом, логично завершающим этап эволюционного поиска. Строительные бло-ки, полученные на предыдущем шаге, добавляются к сети. На рис. 4.31, а показана фаза добавления полученных надстроечных блоков 1 и 2 к те- кущей нейронной сети. Светлым цветом выделены новые добавленные нейроны, пунктирной линией отмечены новые связи между нейронами.\n--- Страница 112 ---\n112 ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ Хотелось бы отметить, что в принципе добавление новых нейронов при наращивании сети необязательно. Нейронная сеть может развиваться только за счет добавления новых синаптических связей между сущест- вующими нейронами. Генетическая строка: 3 13 10 9# #1 15 12 8 # #2 8# # 1 10 9# #2 10 9# # 2 13 10 9# # 3 1 13 151210 112 234 5 6 789 11 а б Рис. 4.31. Схема алгоритма эволюционного накопления признаков На рис. 4.31, б показана получившаяся в результате модификации новая нейронная сеть. Полученная нейронная сеть проверяется на обу- чающей и на проверочной выборках. В случае, если значение ошибкиработы сети устраивает, работа всего алгоритма считается завершенной,а текущая сеть считается искомой нейронной сетью оптимальной архи- тектуры для данной задачи. В противном случае, полученная сеть рас- сматривается, как база для наращивания и управление передается об-ратно на эволюционный этап. Обсуждение алгоритма Предложенный алгоритм сочетает в себе лучшие качества конструк- тивного и эволюционного подходов. От конструктивного направления взят принцип, согласно которому сложность нейронной сети в процессе поиска итеративно увеличивается. Изменения, которые необходимовнести в первоначальную модель, определяются при помощи генетиче-ских алгоритмов. Для этого разработан специальный способ кодировкиархитектуры нейронной сети в генетическую строку, учитывающий на- личие постоянной части сети.\n--- Страница 113 ---\nГлава 4. Эффективные нейронные сети 113 В результате представленный алгоритм, за счет применения генети- ческих алгоритмов, позволяет реализовать более широкий охват про- странства архитектур по сравнению с конструктивным направлением. Результатом будет улучшенное качество решения. Итеративный прин-цип увеличения сложности позволяет значительно уменьшить времен-ные затраты сравнительно с большей частью методов построения ней- ронных сетей эволюционного направления. Нелинейные модели имеют овражистое пространство решений с большим числом локальных минимумов. Представленный алгоритм по-зволяет реализовать некоторую непрерывность в пространстве моделей, создавая последовательность вложенных моделей, когда одна нейрон- ная сеть вложена в другую как подсеть меньшей размерности. В рядеработ [21, 24] отмечена необходимость построения иерархически вло-женных друг в друга моделей для эффективного использования крите-риев оценки обобщающей способности модели. Как способ дальнейшего развития предложенного алгоритма эволю- ционного наращивания нейронной сети можно использовать распарал-леливание эволюционного этапа алгоритма на системах многопроцес-сорной обработки информации. Сделать это позволяет наличие не- скольких индивидуумов в популяции и независимая реализация вычис- ления фитнесс-функции каждого индивидуума. В дополнение к этому,можно применить к построенной нашим алгоритмом нейронной сетилюбой из эффективных методов усечения.\n--- Страница 114 ---\nЗАКЛЮЧЕНИЕ За свою менее чем полувековую историю существования теория ней- ронных сетей показала свою эффективность в решении многих реальных задач. Нейронные сети успешно применяются для анализа и прогнози- рования на финансовом рынке, построения систем медицинской диагно-стики. Очень широко нейронные сети в настоящее время применяются вробототехнике и в системах управления. Как яркий пример можно при- вести появление целой плеяды электронных домашних игрушек, кото- рые ведут себя, как настоящие живые существа, могут обучаться и дажеприобретать индивидуальные черты характера! Нейронные сети исполь-зуются везде, где невозможно построить четкий алгоритм решения зада- чи: при выделении отдельных элементов изображения, распознавании текста, предсказании погоды, сочинении музыки и других областях, гдераньше применение машины было немыслимо. Вычислительный принцип, который нейронные сети позаимствовали у своего биологического прототипа головного мозга, распределенная обработка информации независимыми простейшими вычислителяминесомненно является чрезвычайно эффективным. В романе фантастаС. Лема «Непобедимый» описана ситуация, когда мощный космическийкорабль оказался бессилен перед механистической формой жизни – примитивными электронными «мушками», которые, объединяясь в «облака» в огромных количествах, увеличивали свой интеллектуальныйпотенциал и могли противостоять любой опасности. Причем поврежде-ние любой части такого «облака» влекло за собой только наращивание числа «мушек» и соответственно общего интеллекта до количества, адекватного угрозе. Тот же принцип объединения простых вычислите-лей лежит в основе теории нейронных сетей, что делает нейронные сетиспособными создавать мощные вычислительные комплексы с высокой степенью параллелизма и надежности. В заключение можно сказать, что история исследований в области нейронных сетей еще довольно коротка и они еще не успели проявитьсебя в полную силу. Возможно, по мере того, как мы будем больше уз-навать об устройстве головного мозга, на основе нейротехнологий бу- дут развиваться новые, еще более необычные способы познания мира.\n--- Страница 115 ---\nЛитература 115 ЛИТЕРАТУРА 1. Calvin W.H. and Ojemann G.A. Conversations with Neil's Brain: The Neural Nature of Thought And Language. – Addison-Wesley, 1994. (http://faculty.washington.edu/wcalvin/). 2. Busis N.A. Neurology and Neurosurgery. Neuroscience Internet Guides. (http://www. neuroguide.com/neurogui/). 3. Розенблатт Ф. Принципы нейродинамики. Перцептрон и теория механизмов мозга. – М.: Мир, 1965. – 480 с. 4. Минский М., Пайперт С. Персептроны. – М.: Мир, 1971. 5. Rumelhart D.E., Hinton G.E., and Williams R.J. Learning internal representations by error propogation // Parallel distributed processing: Exploratians in the Micro- structures of Cognitron / D.E. Rumelhart, J.I. McCelland, (Eds.). – Cambridge: MIT Press, 1986. – V. 1. 6. Bishop C.M. Neural Network for Pattern Recognition. – Oxford: Oxford University Press, 1997. – 482 p. 7. Дуда Р., Харт П. Распознавание образов и анализ сцен. – М.: Мир, 1976. – 512 c. 8. Muller B., Reinhardt J., Strickland M.T. Neural Networks. – Springer-Verlag, 1995. – 242 p. 9. Горбань А.Н., Дунин-Барковский В.Л., Кирдин А.Н. Нейроинформатика. – Ново- сибирск: Наука, 1998. – 296 c. (http://www.neuropower.de/rus/). 10. Gorban A.N. and Wunsch D.C. The General Approximation Theorem // Proceedings of Intern. Joint Conf. on Neural Networks'98. – 1998. 11. Иванов В.В., Пурэвдорж Б., Пузырин И.В. Методы второго порядка для обуче- ния многослойного перцептрона // Математическое моделирование. – 1998. – Т. 10. – № 3. – С. 117 – 124. 12. Hertz J., Krogh A., Palmer R. Wstep do teorii obliczen neuronowych. Wyd. II. – Warszawa: WNT, 1995. 13. Уоссермен Ф. Нейрокомпъютерная техника: теория и практика. – 1992. – 184 c. (http://www.neuropower.de/rus/). 14. Короткий С. Нейронные сети: обучение без учителя. – 1998. (http://www.orc.ru/ stasson/n3.zip). 15. Терехов С.А. Лекции по теории и приложениям искусственных нейронных се- тей. – Снежинск: ВНИИТФ, 1994. (http://www.vniitf.ru/ nimfa/). 16. Короткий С. Нейронные сети Хопфилда и Хэмминга. – 1998. (http://www. orc.ru/stasson/n4.zip). 17. Camargo F.A. Learning Algorithms in Neural Networks // Tech. Rep. CUCS-062-90, Columbia Univer sity, NY, 10027, December 1990. (ftp://archive .cis. ohio-state.edu/ pub/neuroprose/Camargo.learning.ps).\n--- Страница 116 ---\n116 Литература 18. Sjoberg J. and Ljung L. Overtraining, Regularization, and Searching for Minimum in Neural Networks. // Tech.Rep., Department of Electrical Engineering, Linkoping University, 1992. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/sjoberg.overtrain- ing.ps. gz). 19. Amari S., Murata N. and Muller K.R. Asymptotic Statistical Theory of Overtraining and Cross-Validation // Tech.Rep. METR-95-06, Univ. of Tokyo, 1995. (ftp:// archive.cis.ohio-state. edu/pub/neur opr ose/amari. overtraining .ps. gz). 20. Monasson R. and Zecchina R. Learning and Generalization Theories of Large Com- mittee-Machines // Tech. Rep. I-10129, Politecnico di Torino, 1995. (ftp://archive.cis. ohio-state.edu/pub/neuroprose/zecchina.committee.ps.gz). 21. Murata N., Yoshizawa S., and Amari S. Network Information Criterion – Determining the number of hidden units for an Artificial Neural Network model // IEEE Transac- tions on Neural Networks. – November 1994. – V. 5. – Nо. 6. – P. 865 – 872. (http://www. islab.brain.riken.go.jp/ mura/paper/mura94nic.ps.gz). 22. Barron A. Predicted squared error: a criterion for automatic model selection. – NY, 1984. 23. Ripley B.D. Pattern Recognition and Neural Networks. – Cambridge: Cambridge University Press, 1997. – 403 p. 24. Moody J.E. and Utans J. Architecture selection strategies for neural networks: appli- cation to corporate bond rating prediction. – Wiley, 1995. – P. 277 – 300. (ftp://cse. ogi.edu/pub/tech-reports/1994/94-036.ps.gz). 25. Lawrence S., C. Lee Giles, and Ah Chung Tsoi. What Size Neural Network Gives Optimal Generalization? Convergence Properties of Backprop-agation // Tech. Rep. UMIACS-TR-96-22 and CS-TR-3617, Univ. of Maryland, 1996. 26. Utans J. and Moody J. Selecting Neural Network Architectures Via the Prediction Risk: Application to Corporate Bond Rating Prediction // Tech. Rep., Yale Univ., 1991. (ftp://archive.cis.ohio-state. edu/pub/neur opr ose/utans. bondrating.ps. gz). 27. Moody J.E. Note on generalization, regularization and architecture selection in non- linear learning systems // IEEE Computer Society Press, 1991. – P. 1 – 10. (ftp:// neural.cse.ogi.edu/pub/neural/papers/ moody91 .generalize.ps.Z). 28. Moody J.E. The effective number of parameters: an analysis of generalization and regularization in nonlinear learning systems. – 1992. – P. 847 – 854. (ftp://neural.cse. ogi.edu/pub/neural/papers/moody9 1.peffective.ps.Z). 29. Murata N., Yoshizawa S., and Amari S. Learning Curves, Model Selection and Com- plexity of Neural Networks // M.Kaufmann, San Mateo, CA, 1993. – V. 5. – P. 607 –614, (http://www.islab.brain.rik-en.go.jp/mura/paper/mura93nips92. ps.gz). 30. Mezard M., and Nadal J.P. Learning in feedforward layered networks: The Tiling al- gorithm // Journal of Physics. – 1989. – V. A22. – P. 2191 – 2203, 31. Bodenhausen U. Automatic Structuring of Neural Networks for Spatio-Temporal Real-World Applications // PhD thesis, der Fakultat fur Informatik der Universitat Karlsruhe, 1994. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/bodenhausen.thesis. ps.gz). 32. Frean M. The Upstart Algorithm: A Method for Constructing and Training Feed- Forward Neural Networks // Tech. Rep. 89/469, Edinburgh Univ., 1989. (ftp:// archive.cis.ohio-state.edu/pub/neuroprose/ frean.upstart.ps.gz).\n--- Страница 117 ---\nЛитература 117 33. Thimm G. and Fiesler E. Two Neural Network Construction Methods // Neural Proc- essing Letters. – 1997. – V. 6. – P. 25 – 31, (http://www.wkap.nl/issuetoc. htm/). 34. Sankar A. and Mammone R.J. Speaker Independent Vowel Recognition Using Neural Tree Networks. // Proceedings of Intern. Joint Conf. on Neural Networks, Seattle, 1991. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/sankar.ijcnn9 11.ps.gz). 35. Wen W.X., Jennings A.? and Liu H. Learning a Neural Tree. // Proceedings of Intern. Joint Conf. on Neural Networks'92, Beijing, China, 1992. (ftp://archive.cis.ohio-state. edu/pub/neuroprose/wen.sgnt-learn.ps.gz). 36. Wen W.X., Jennings A., Liu H.? and Pang V. Some Performance Comparisons for Self-Generating Neural Tree. // Proceedings of Intern. Joint Conf. on Neural Net- works'92, Beijing, China, 1992. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/wen. sgnt-learn.ps.gz). 37. Wen W.X., Pang V., and Jennings A. A Comparative Study Between SGNT and SONN. IIАГ92, Hobart, Australia, Nov 1992. (ftp://archive.cis.ohio-state.edu/pub/ neuroprose/wen.sgnt-learn .ps. gz). 38. Torres-Moreno J.M. Apprantissage et generalisation par des reseaux de neurones: etude de noveaux algorithmes constructifs // Ph.D.thesis, de l’Institut National Polytechique de Grenoble, 1997. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/ torres.thesis.ps.Z). 39. Ash T. Dynamic Node Creation in Back-Propagation Networks // Connection Sci- ence. – 1989. – V. 1. 40. Fahlman S.E. and Lebiere C. The cascade-correlation learning architecture // In Ad- vances in Neural Information Processing II. – 1990. – P. 524 – 532, (ftp://archive. cis.ohio-state.edu/pub/neuroprose/fahlman.cascor-tr. ps.gz). 41. Fahlman S.E. An Empirical Study of Learning Speed in Back-Propagation Networks // Tech. Rep. CMU-CS-88-162, School of Computer Science, Carnegie Mellon Uni- versity, 1988. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/fahlman.quickprop-tr. ps.gz). 42. Yang J. and Honavar V. Experiments with the Cascade-Correlation Algorithm // Tech. Rep. 91-16, Department of Computer Science, Iowa State University, 1991.(ftp://archive.cis.ohio-state.edu/pub/neuroprose/zecchina.committee.ps.gz). 43. Squires C.S., Jr. Jude, W. Shavlik. Experimental Analysis of Aspects of the Cascade- Correlation Learning Architecture // Neural Networks. – 1991. (ftp://archive. cis.ohio-state.edu/pub/neuroprose/squires.cascor.ps.gz). 44. Prechelt L. Investigation of the CasCor Family of Learning Algorithms // Neural Networks. – May 1997. – V. 10. – Nо. 5. – P. 885 – 896. (ftp://ftp.ira.uka.de/pub/ neuron/neurnetw97.ps.gz). 45. Jenq-Neng Hwang, Shih-Shien You, Shyh-Rong Lay, and I-Chang Jou. What's Wrong with A Cascaded Correlation Learning Network: A Projection Pursuit Learning Per- spective // IEEE Trans. Neural Networks. – 1996. – V. 7. – Nо. 2. – P. 278 – 289. (ftp://archive.cis.ohio-state. edu/pub/neur opr ose/hwang. cclppl. ps. gz). 46. Shultz T.R. and Elman J.L. Analyzing Cross Connected Networks // Advances in Neural Information Processing Systems. – 1990. – V. 6. – P. 1117 – 1124. (ftp:// archive.cis.ohio-state.edu/pub/neuroprose/shultz.cross.ps.gz).\n--- Страница 118 ---\n118 Литература 47. Fahlman S.E. The Recurrent Cascade-Correlation Architecture // Tech. Rep. CMU- CS-91-100, School of Computer Science, Carnegie Mellon University, 1991. (ftp://archive.cis.ohio-state. edu/pub/neuroprose/fahlman.rcc.ps.gz). 48. Hoehfeld M. and Fahlman S.E. Learning with Limited Numerical Precision Using the Cascade-Correlation Algorithm.II Tech.Rep. CMU-CS-91-130, School of Computer Science,Carnegie Mellon University, 1991. (ftp://archive.cis.ohio- state.edu/pub/ neuroprose/ hoehfeld.precision.ps. gz). 49. Klagges H. and Soegtrop M. Limited Fan-in Random Wired Cascade – Correlation, 1991. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/klagges.rndwired-cascor. ps.gz). 50. Phatak D.S. and Koren I. Connectivity and Performance Tradeoffs in the Cascade Correlation Learning Architecture // Tech. Rep. TR-92-CSE-27, ECE Dept., UMASS, Amherst, 1994. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/phatak. layered-cascor.ps.gz). 51. Sjogaard S. A Conceptual Approach to Generalization in Dynamic Neural Networks // PhD thesis, Computer Science Department, Aarhus University, 1991. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/sjogaard.concept.ps.gz). 52. Simon N., Kerckhoffs E., and Corporaal H. Variations on the Cascade-Correlation Learning Architecture for Fast Convergence // Neural Network World. – 1992. – V. 2. – P. 497 – 510. 53. Simon N. Constructive Supervised Learning Algorithms for Artificial Neural Net- works // PhD thesis, Delft University of Technology, Faculty of Electrical Engineer- ing, 1993. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/simon.thesis. ps.Z). 54. Treadgold N.K. and Gedeon T.D. Exploring Constructive Cascade Networks // IEEE Transactions on Neural Networks, 1999. (http://www.cse.unsw.edu.au/nickt/doc/ acasper.ps). 55. Treadgold N.K and Gedeon T.D. Exploring Architecture Variations in Constructive Cascade Networks // Proc. Int. Joint Conf. on Neural Networks, Anchorage, 1998. – P. 343 – 348. (http://www.cse.unsw. edu.au/nickt/doc/tower.ps). 56. Treadgold N.K. and Gedeon T.D. A Cascade Network Algorithm Employing Pro- gressive RPROP. // Int. Work Conf. on Artificial and Natural Neural Networks, Lan- zarote, 1997. – P. 733 – 742. (http://www.cse.unsw.edu.au/nickt/doc/casper.ps). 57. Treadgold N.K. and Gedeon T.D. Extending and Bench marking the CasPer Algo- rithm // Australian Conference on Artificial Intelligence, Perth, 1997. – P. 398 – 406. (http://www.cse.unsw.edu.au/nickt/doc/casperpclass.ps). 58. Treadgold N.K. and Gedeon T.D. Extending CasPer: A Regression Survey. // Int. Conf. on Neural Information Processing, Dunedin, 1997. – P. 310 – 313. (http:// www.cse.unsw.edu.au/nickt/doc/casperpreg.ps). 59. Mozer M.C., Smolensky P. Skeletonization: a technique for trimming the fat from a network via relevance assessment // Advances in Neural Information Processing Systems. – 1989. – V. 1. – P. 107 – 115. 60. Горбань А.Н. Обучение нейронных сетей. – М.: СП «ParaGraph», СССР – США, 1990. – 160 c. 61. Еремин Д.И. Контрастирование. II Нейропрограммы / Под ред. А.Н. Горбаня. – Красноярск: Изд-во КГТУ, 1994. – С. 88 – 108.\n--- Страница 119 ---\nЛитература 119 62. Le Cun Y., Denker J.S., and Solla S.A. Optimal Brain Damage // Advances in Neural Information Processing Systems II (Denver1989). – 1990. – P. 598 – 605. 63. Hassibi B. and Stork D.G. Second Order Derivatives for Network Pruning: Optimal Brain Surgeon // Neural Information Processing Systems. – 1992. (ftp://archive.cis. ohio-state.edu/pub/neuroprose/stork.obs.ps.gz). 64. Pedersen M.W., Hansen L.K., and Larsen J. Pruning with Generalization Based Weight Saliences: OBD, OBS., 1994. (ftp://archive.cis.ohio-state.edu/pub/neuroprose /pedersen.pruning.ps.gz). 65. Spears W.M., A. De Jong, Back T., et al. An Overview of Evolutionary Computation // Proceedings of the 1993 European Conference on Machine Learning, 1993. (http://www.aic.nrl.navy.mil/spears/papers/ecml93.ps). 66. Holland J.H. Adaptation in Natural and Artificial Systems. Univ. of Michigan Press., Second Ed. 1992, MA: The MIT Press edition, 1975. 67. Дмитрович А.И. Интеллектуальные информационные системы. Тетрасистемс. – Минск, 1997. – 367 с. 68. Hussain T.S. An Introduction to Evolutionary Computation // Tech. Rep., CITO Researcher Retreat, Ontario, 1998. (http ://www. cs. queensu. ca/home/hussain/). 69. Fjalldal J.B. Evolving Neural Network Controllers Using Genetic Algorithms with Variable Length Genotypes // Tech. Rep., School of Cognitive and Computing Sciences at the Univ. Of Sussex, March 1999. (http://www.cogs.susx.ac.uk/users/ johannf/report/report.html). 70. Koza J.R. Survey Of Genetic Algorithms And Genetic Programming. II Tech. Rep., Computer Science Department Margaret Jacks Hall Stanfo rd University, 1995. (http://smi-web.stanford.edu/people/koza/). 71. Goldberg D.E. Genetic algorithms in search, optimization and machine learning. – Addison-Wesley, Reading, 1989. 72. Whitley D. The GENITOR algorithm and selection pressure: why rank-based allocation of reproductive trials is best // Proceedings of the 3rd International Conference on Genetic Algorithms and their applications (ICGA). – 1989. – P. 116 – 121, 73. Hussain T.S. Methods of Combining Neural Networks and Genetic Algorithms. // Tech. Rep., ITRC/TRIO Researcher Retreat, Ontario, May 1997. (http://www.cs. queensu.ca/home/hussain/). 74. Xin Yao. A Review of Evolutionary Artificial Neural Networks // International Journal of Intelligent Systems, 1991. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/ yao.eann.ps.gz). 75. Branke J. Evolutionary Algorithms for Neural Network Design and Training // 1st Nordic Workshop on Genetic Algorithms and its Applications, 1995. (ftp://ftp.aifb. uni-karlsruhe.de/pub/jbr/Vaasa.ps.gz). 76. Boers E.J.W., Borst M.V., and Sprinkhuizen-Kuyper I.G. Evolving Artificial Neural Networks Using the «Baldwin Effect» // Tech. Rep. TR95-14, Computer Science Department, Leiden Univ., 1995. (ftp://ftp.wi.leidenuniv.nl/pub/CS/ MScTheses/tr95- 14.ps). 77. Koehn Ph. Combining Genetic Algorithms and Neural Networks: The Encoding Problem // PhD thesis, The Un iversity of Tennessee, Knoxvi lle, 1994. (ftp://archive. cis.ohio-state.edu/pub/neuroprose/koehn.encoding.ps.gz).\n--- Страница 120 ---\n120 Литература 78. Gruau F., Whitley D., and Pyeatt L. A comparison between cellular encoding and direct encoding for genetic neural networks // Proceedings of the First Genetic Programming Conference. – 1996. – P. 81 – 89. 79. Miller G., Todd P. and Hegde S. Designing neural networks using genetic algorithms // Proceedings of the 3rd International Conference on Genetic Algorithms and their applications (ICGA). – 1989. – P. 379 – 384. 80. Marti L. Genetically Generated Neural Networks I: Representational Effects // Tech. Rep. CAS/CNS-TR-92-014, Boston University, Center for Adaptive Systems, 1992. 81. Marti L. Genetically Generated Neural Networks II: Searching for an Optimal Repre- sentation // Tech. Rep. CAS/CNS-TR-92-015, Boston University, Center for Adaptive Systems, 1992. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/marti.ga2. ps.gz). 82. Belew R.K., McInerney J., and Schraudolph N.N. Evolving Networks: Using the Genetic Algorithm with Connectionist Learning // Tech. Rep. CS90-174, Computer Science Dept. Univ. California at San Diego, 1990. (ftp://archive.cis.ohio-state.edu/ pub/neuroprose/belew.evol-net.ps.gz). 83. Dasgupta D. Evolving N euro-Controllers for a Dynamic System Using Structured Genetic Algorithms // Applied Intelligence. – 1998. – V. 8. – P. 113 – 121. (http:// www.wkap.nl/issuetoc.htm/). 84. Korning P.G. Training Neural Networks by means of Genetic Algorithms Working on Very Long Chromosomes // PhD thesis, Aarhus University Ny Munkegade, 1997. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/korning.nnga. ps.gz). 85. Schiffmann W., Joost M., and Werner R. Performance Evaluation of Evolutionarily Created Neural Network Topologies // Proc. of Parallel Problem Solving from Nature, Lect. Notes in Computer Science, 1991, Schwefel, H.P. and Maenner, R., Springer. – P. 274 – 283, 1991. (http://www.uni-koblenz.de/evol/mertenpublications. html). 86. Schiffmann W., Joost M., and Werner R. Synthesis and Performance Analysis of Multilayer Neural Network Architectures // Tech. Rep. 16/1992, University of Koblenz, Institute fur Physics, 1992. (ftp://archive.cis.ohio-state.edu/pub/neuroprose /schiff. gann .ps. gz). 87. Schiffmann W., Joost M., and Werner R. Application of Genetic Algorithms to the Construction of Topologies for Multilayer Perceptrons // Proc. of Artificial Neural Networks and Genetic Algorithms, Innsbruck 1993, Albrecht et al. – Springer Verlag, 1993. – P. 675 – 682. (http://www.uni-koblenz.de/evol/mertenpublications.html). 88. Figueira Pujol J.C. and Poli R. Evolving the Topology and the Weights of Neural Networks Using a Dual Representation // Applied Intelligence. – 1998. – V. 8. – P. 73 – 84. (http://www.wkap.nl/issuetoc.htm/). 89. Koza J.R. and Rice J.P. Genetic Generation of Both the Weight and Architecture for a Neural Network // Proceedings of the International Joint Conference on Neural Networks. – 1991. – V. II. – P. 397 – 404. 90. Wong F. Genetically Optimized Neural Networks // NIBS Technical Report, TR- 940216, 1994.(ftp://archive.cis.ohio-state.edu/pub/neuroprose/wong.nnga.ps.gz). 91. Mandischer M. Representation and Evolution of Neural Networks // Proceedings of the International Joint Conference on Neural Networks and Genetic Algorithms. – 1993. – P. 643 – 649.\n--- Страница 121 ---\nЛитература 121 92. Kitano H. Designing neural network using genetic algorithm with graph generation system // Complex Systems. – 1990. – V. 4. – P. 461 – 476. 93. Nolfi S. and Parisi D. Growing Neural Network // Artificial life. – June 1992. – V. III. – P. 16. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/nolfi.growing. ps.gz). 94. Nolfi S. and Parisi D. Self-selection of input stimuli for improving performance. // Tech. Rep., Institute of Psychology, CNR, Italy, 1992. (ftp://archive.cis.ohio-state. edu/pub/neuroprose/nolfi.self-sel.ps.gz). 95. Nolfi S., Miglino O., and Parisi D. Phenotypic Plasticity in Evolving Neural Networks // Tech. Rep. PCIA-94-05, Dpt. of Cognitive Processes and Artificial Intelligence, Italy, May 1994. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/nolfi.plastic.ps.gz). 96. Nolfi S., Floreano D., Miglino O., and M ondada F. How to evolve autonomous robots: different approaches in evolutionary robotics // Tech. Rep. PCIA-94-03, Dpt. of Cognitive Processes and Artificial Intelligence, Italy, May 1994. (ftp://archive.cis. ohio-state.edu/pub/ neuroprose/nolfi.erobot.ps.gz). 97. Cangelosi A., Parisi D., and Nolfi S. Cell division and migration in a genotype for neural networks // Network: Computation in Neural Systems. – 1993. – V. 5. – P. 497 – 515. 98. Fullmer B. and Miikkulainen R. Using Marker-Based Genetic Encoding Of Neural Networks To Evolve Finite-State Behaviour // Proceedings of the First European Conference on Artificial Life (ECAL-91), Paris, 1991. 99. Boers E. and Kuiper H. Biological Metaphors and the Design of Modular Artificial Neural Networks // PhD thesis, Departments of Computer Science and Experimental and Theoretical Psychology at Leiden University, the Netherlands, 1992. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/boers.biological-metaphors. ps.gz). 100. Boers E., Kuiper H., Happel B., and Sprinkhuizen-Kuyper I. Designing Modular Artificial Neural Networks // Tech. Rep. 93-24, Department of Computer Science Leiden University, The Netherlands, 1993. (ftp://ftp.wi.leidenuniv.nl:21/pub/CS/MScTheses/ tr93-24.ps.gz). 101. Тютерев В.В., Шевелев О.Г. Использование L-грамматик для определения ней- ронной сети оптимальной топологии методом генетических алгоритмов // IIIV межвузовская конференция студентов, аспирантов и молодых ученых «Наука и образование»: Тез. докл. – Томск: ТГПУ, 2000. 102. Gruau F. Cellular Encoding of Genetic Neural Networks // Tech. Rep. 92-21, Laboratoire de l'Informatique du Parallelisme, Ecole Normale Superieure de Lyon, 1992. (http://www.cwi.nl/gruau/gruau/RR92-21.ps.Z). 103. Gruau F. and Whitley D. Adding Learning to the Cellular Developmental Process: A Comparative Study // Tech. Rep. RR93-04, Laboratoire de l'Informatique du Parallelisme, Ecole Normale Superieure de Lyon, 1993. (http://www.cwi.nl/gruau/ gruau/RR93-04.ps.Z). 104. Gruau F. Automatic definition of sub-neural networks // Tech. Rep. RR94-28, Laboratoire de l'Informatique du Parallelisme, Ecole Normale Superieure de Lyon, 1994. (http://www.cwi.nl/gruau/gruau/RR94-28.ps.Z). 105. Gruau F. Neural Network Synthesis Using Cellular Encoding and the Genetic Algorithm // PhD thesis, Ecole Normale Superieure de Lyon, 1994. (ftp://lip.ens- lyon.fr/pub/LIP/Rapports/PhD/PhD94-01-E.ps.Z).\n--- Страница 122 ---\n122 Литература 106. Hussain T.S. Modularity Within Neural Networks // PhD thesis, Department of Computing and Information Sciences, 1995. (http://www.cs.queensu.ca/home/ hussain/). 107. Gruau F. Automatic Definition of Modular Neural Networks // Adaptive Behavior. – 1995. – V. 3. – P. 151 – 183. (http:/ /www.cwi.nl/gruau/g ruau/AB. ps.Z). 108. Gruau F. Genetic Programming of Neural Networks: Theory and Practice // Intelligen Hybrid Systems. – 1995. – P. 245 – 271. (http://www.cwi.nl/gruau/gruau/ aigp94.ps.Z). 109. Friedrich C.M. and Moraga C. Using Genetic Engineering to Find Modular Structures and Activation Functions for Architectures of Artificial Neural Networks // Computational Intelligence, Theory and Applications. – 1997. – P. 150 – 161. (ftp://archive.cis.ohio-state.edu/pub/neuroprose/friedrich.nnarch-activation.ps.gz). 110. Friedrich C.M. and Moraga С. An Evolutionary Method to Find Good Building-Blocks for Architectures of Artificial Neural Networks // Proceedings of the Sixth International Conference on Information Processing and Management of Uncertainty in Knowledge- Based Systems (IPMU '96); Granada, Spain (1996). – 1996. – P. 951 – 956. 111. Kodjabachian J. and Meyer J.A. Evolution and deveopment of control architectures in animats // Robotics and Autonomous Systems. – 1995. – V. 16. – P. 161 – 182. (www.biologie.ens.fr/perso/meyer/ publications.html). 112. Kodjabachian J. and Meyer J.A. Evolution and development of modular control architectures for 1-D locomotion in six-legged animats // (неопубликована). – 1997. (www.biologie.ens.fr/perso/meyer/publications. html). 113. Luke S. and Spector L. Evolving graphs and networks with edge encoding: Preliminary report // Late-Breaking Papers of the Genetic Programming'96 Conference, 1997. (www.cs.umd.edu/seanl/papers/graph-paper.ps). 114. Talko B. A Rule-Based Approach for Constructing Neural Networks Using Genetic Programming // PhD thesis, Univ. of Melbourne, Australia, 1999. (http://www.cs.mu. oz.au/research/vislab/). 115. Browse R.A. Hussain T.S., and Smillie M.B. Using Attribute Grammars for the Genetic Selection of Backpropagation Networks for Character Recognition // Proceedings of Applications of Artificial Neural Networks in Image processing IV (January 25 – 28, San Jose, CA), 1999. (http://www.cs.queensu.ca/home/ hussain/). 116. Hussain T.S. and Browse R.A. Genetic Encoding of Neural Network Processing and Control // Accepted for publication in Graduate Student Workshop of GECCO99: Genetic and Evolutionary Computation Conference (July 13 – 17, Orlando, FL),1999. (http://www.cs.queensu.ca/home/hussain/). 117. Hussain T.S. and Browse R.A. Genetic Operators with Dynamic Biases That Operate on Attribute Grammar Representations of Neural Networks // Birds-of-a-Feather Workshop on Advanced Grammar Techniques within Genetic Programming and Evolutionary Computation, held at GECCO99, Orlando, FL, 1999. (http://www.cs. queensu.ca/home/hussain/). 118. Hussain T.S. and Browse R.A. Network Generating Attribute Grammar Encoding // Proceedings of the 1998 IEEE International Joint Conference on Neural Networks (May 4 – 9, Anchorage, Alaska). – 1998. – V. 1. – P. 431 – 436. (http://www.cs. queensu.ca/home/hussain/).\n--- Страница 123 ---\nЛитература 123 119. Hussain T.S. and Browse R.A. Genetic Encoding of Neural Networks Using Attribute Grammars // Tech.Rep., CITO Researcher Retreat, Ontario, 1998. (http://www.cs. queensu.ca/home/hussain/). 120. Hussain T.S. and Browse R.A. Using Attribute Grammars for Genetic Encoding of Neural Networks and Syntactic Constraint of Genetic Programming // Twelfth Canadian Conference on Artificial Intelligence: Workshop on Evolutionary Computation, V. June 17, Vancouver, BC, 1998. (http://www.cs.queensu.ca/home/ hussain/). 121. Hussain T.S. and Browse R.A. Basic Properties of Attribute Grammar Encoding. // Late Breaking Papers at the Genetic Programming 1998 Conference (July 22 – 25, Madison, Wisconsin). – 1998. – P. 256. (http://www.cs.queensu. ca/home/hussain/). 122. Hussain T.S. and Browse R.A. Including Control Architecture in Attribute Grammar Specifications of Feedforward Neural Networks // Proceedings of the 1998 Joint Conference on Information Sciences: Second International Workshop on Frontiers in Evolutionary Algorithms (October 23 – 28, Research Triangle Park, North Carolina). – 1998. – V. 2. – P. 432 – 436. (http://www.cs.queensu.ca/home/hussain/). 123. Hussain T.S. Network Generating Attribute Grammar Encoding // Tech. Rep., Queen's University, Ontario, Canada, March 1998. (http://www.cs.queensu.ca/home/ hussain/). 124. Hussain T.S. Cellular Encoding: Review and Critique // Tech. Rep., Queen's University, July 19, 1997. (http://www.cs.queensu.ca/home/hussain/). 125. Gruau F. Cellular Encoding for interactive Robotics // Tech. Rep. 425, Sussex University, School of Cognitive and Computing Science, 1996. (ftp://ftp.cogs.susx. ac.uk/pub/reports/csrp/csrp425.ps.Z). 126. Gruau F. and Quatramaran. Cellular encoding for interactive evolutionary robotics // ECAL97, 1997. (http://www.cwi.nl/gruau/gruau/e.ps.gz). 127. Dellaert F. and Vandewalle J. Automatic Design of Cellular Neural Networks by Means of Genetic Algorithms: Finding a Feature Detector // Tech.Rep., Dept. of Computer Engineering and Science Case Western Reserve University, Cleveland, Dept. of Electrical Engineering Katholieke Universiteit, Belgium, 1996. 128. Soegtrop M. and Klagges H. A Massively Parallel Neurocomputer // Tech. Rep., IBM Research Division Physics Group Munich, 1997. (ftp://archive.cis.ohio-state. edu/pub/neuroprose/klagges.massively-parallel.ps.gz). 129. Тютерев В.В. Методы оптимизации нейронных сетей со сложной топологиче- ской структурой // Труды регион, науч.-практ. конф. «Сибирская школа моло-дого ученого». – Томск: Изд-во ТГПУ, 1999. – Т. 4. – C. 28 – 30. 130. Тютерев В.В. Подход к моделированию эффективных по размеру нейронных сетей // Материалы XXXVII Международной научной студенческой конферен- ции «Студент и Научно-технический прогресс»: Математика. – Новосибирск: Изд-во Новосиб. ун-та, 1999. – С. 141 – 142. 131. Малышков Ю.П. и др. Предсказание землетрясений методом измерения лито- сферных импульсов // Вулканология и сейсмология. – 1998. – № 1. – C. 92. 132. Тютерев В.В. Определение эффективного размера нейронной сети в процессе обучения методом динамического наращивания узлов // Сборник трудов VI\n--- Страница 124 ---\n124 Литература Всероссийского семинара «Нейрокомпьютеры и их применение». – М., 2000. – С. 549 – 551. 133. Тютерев В.В. Построение нейронных сетей эффективного размера методом ге- нетических алгоритмов // IV Сибирский конгресс по прикладной и индустри- альной математике (ИНПРИМ-2000): Тез. докл. – Новосибирск: Изд-во Ин-та математики, 2000. – Т. 2. – С. 126 – 127. 134. Siddiqi A.A., Lucas S.M. A comparison of matrix rewriting versus direct encoding for evolving neural networks // Proceedings of Intern. Joint Conf. on Neural Networks'98, Anchorage, Alaska, 1998. 135. Jacob W., Rehder M. Evolution of neural net architectures by a hierarchical grammar-based genetic system // Proc. of the International Joint Conference on Neural Networks and Genetic Algorithms. – 1993. – P. 72 – 79. 136. Hinton E., Drew van Camp. Keeping Neural Networks Simple by Minimizing the Description Length of the Weights, 1993. (http://www.cs.utoronto.ca/drew/colt93.ps). 137. Тютерев В.В. Применение генетических алгоритмов для определения опти- мальной топологии нейронных сетей // Нейроинформатика и ее приложения: Материалы VIII Всероссийского семинара / Под общ. ред. А.Н.Горбаня. – Красноярск: ИПЦ КГТУ, 2000. – С. 171. 138. Иваненко Б.П., Парфенов А.Н., Тютерев В.В. Исследование генетически по- строенных нейронных сетей на примере моделирования системы взаимодейст- вующих нефтяных скважин // Моделирование неравновесных систем – 2000: Материалы III Всероссийского семинара / Под общ. ред. А.Н.Горбаня. – Крас- ноярск: ИПЦ КГТУ, 2000. – C. 99. 139. Тютерев В.В. Применение генетических алгоритмов для автоматического по- строения нейронных сетей // Материалы междунар. науч.-практ. конф. «Ком- пьютерные технологии в науке, производстве, социальных и экономических процессах». – Новочеркасск: Юж.-Рос. гос. техн. ун-т., НАБЛА, 2000. – Т. 2. – С. 33 – 34. 140. Тютерев В.В. Алгоритм эволюционного наращивания нейронной сети // Сбор- ник трудов III Всероссийской научн.-технич. конференции «Нейроинформати- ка-2001». – М.: МИФИ, 2001. – Т. 1. – С. 213 – 218. 141. Тютерев В.В., Новосельцев В.Б. Автоматическое построение нейронных сетей методом эволюционного наращивания. – Томск: Том. ун-т, 2001. – 22 с. Деп в ВИНИТИ 11.09.2001, 1944-÷2001, 2001. 142. Тютерев В.В., Новосельцев В.Б. Исследования алгоритма автоматического по- строения нейронной сети // Исследования по анализу и алгебре. – Томск: Изд- во Том. ун-та, 2001. – Т. 3. – С. 269 – 281.\n--- Страница 125 ---\nОГЛАВЛЕНИЕ Глава 1 . Предыстория вопроса 3 1.1. Биологический прототип 3 1.2. История 41.3. Формальный нейрон 5 1.4. Возможности многослойного персептрона 9 1.5. Классификация нейронных сетей 101.6. Пороговые функции 111.7. Обучение нейронных сетей 13 Глава 2 . Сети прямого распространения 14 2.1. Теорема Колмогорова 14 2.2. Алгоритм обратного распространения ошибки 152.3. Сети радиально-базисных функций 202.4. Обучение без учителя. Правило Хебба 23 2.5. Сети Кохонена. SOM 26 Глава 3 . Рекуррентные нейронные сети 30 3.1. Сети Хопфилда 30 3.2. Ассоциативная память. ДАП 33 3.5. Теория адаптивного резонанса 37 Глава 4 . Эффективные нейронные сети 43 4.1. Обработка данных 43 4.2. Оптимизация процесса обучения 464.3. Критерии эффективности нейронных сетей 49 4.4. Конструктивный подход к построению нейронных сетей 524.4.1. Бинарные алгоритмы 524.4.2. Древовидные нейронные сети 57 4.4.3. Алгоритмы Monoplan, NetLines и NetSphere 59 4.4.4. Метод динамического добавления узлов 614.4.5. Каскадная корреляция и ее модификации 624.4.6. Методы редукции 67\n--- Страница 126 ---\n126 Оглавление 4.5. Эволюционный способ создания нейронных сетей 73 4.5.1. Генетические алгоритмы 73 4.5.2. Эволюционные алгоритмы для нейронных сетей 78 4.5.3. Подходы к кодированию нейронных сетей 794.5.4. Прямое кодирование 80 4.5.5. Порождающее кодирование 84 4.6. Исследование подходов к построению нейронных сетей 944.6.1. Размерность пространства поиска 94 4.6.2. Анализ известных направлений 96 4.7. Метод мониторинга динамики изменения ошибки 994.8. Эволюционное накопление признаков 102 4.8.1. Предлагаемая организация поиска архитек- туры 102 4.8.2. Реализация кодирования путями 104 4.8.3. Функция приспособленности 108 4.9. Алгоритм эволюционного наращивания нейрон- ной сети 110 ЗАКЛЮЧЕНИЕ 114 ЛИТЕР АТУР А 115\n--- Страница 127 ---\nОглавление 127 ДЛЯ ЗАМЕТОК\n--- Страница 128 ---\n128 Оглавление Сергей Владимирович Аксенов Виталий Борисович Новосельцев ОРГАНИЗАЦИЯ И ИСПОЛЬЗОВАНИЕ НЕЙРОННЫХ СЕТЕЙ (методы и технологии) Редактор Т.С. Портнова Верстка Д.В. Фортес К-ОКП ОК-005-93, код продукции 953380 Изд. лиц. ИД № 04000 от 12.02.2001. Подписано к печати 12.04.2006. Формат 60 × 84 1/16. Бумага офсетная. Печать офсетная. Гарнитура «Таймс». Усл. п. л. 7,44. Уч.-изд. л. 8,33. Тираж 500 экз. Заказ № 8. ООО «Издательство научно-технической литературы» 634050, Томск, пр. Ленина, 34а, тел. (382-2) 53-33-35 Отпечатано в типографии ЗАО «М-Принт», г. Томск, ул. Пролетарская, 38/1",
      "debug": {
        "start_page": 43,
        "end_page": 128
      }
    }
  ]
}
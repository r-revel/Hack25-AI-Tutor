{
  "title": "Работа с таблицами Delta Lake",
  "chapters": [
    {
      "name": "Работа с таблицами Delta Lake",
      "content": "Практические руководства Evolution    \n\n # Работа с таблицами Delta Lake   Эта статья полезна?          \nС помощью этого руководства вы научитесь использовать сервис Managed Spark для обработки таблиц формата Delta Lake.\nВы построите витрину данных, отражающую полную информацию о клиентах и их пути, сохраните результат в формате Delta Lake и выгрузите историю изменений таблицы в логи.\nВы будете использовать следующие сервисы:\n- Managed Spark — сервис, который позволяет развернуть кластерное вычислительное решение на основе Apache Spark для распределенной обработки данных.\n- Object Storage — сервис для хранения данных любого типа и объема. Будет использоваться в качестве хранилища для скриптов.\nШаги:\n1. Подготовьте файл CSV.\n2. Подготовьте скрипт задачи.\n3. Создайте задачу Managed Spark.\n4. Наблюдайте за ходом выполнения задачи.\n\n## Перед началом работы\n1. Зарегистрируйтесь в личном кабинете Cloud.ru. Если вы уже зарегистрированы, войдите под своей учетной записью.\n2. Создайте бакет Object Storage, в котором будут храниться необходимые файлы и логи.\n3. Настройте DNS-сервер и подсеть.\n4. Создайте кластер Data Platform, в котором будет размещен инстанс.\n5. Скачайте и установите root-сертификат на устройство.\n6. Создайте пароль и добавьте его в Secret Manager. Этот секрет станет паролем для доступа к интерфейсу Managed Spark.\n7. Создайте инстанс Managed Spark.\n8. Создайте публичный SNAT-шлюз для доступа инстанса к внешним источникам.\n9. Сверьте совместимость версий Spark и Delta Lake.\n\n## 1. Подготовьте файл CSV\nНа этом шаге вы загрузите в хранилище Object Storage файлы с данными для обработки.\n1. Скачайте CSV-таблицу delta-table.csv: нажмите Скачать в правом верхнем углу.\n2. В ранее созданном бакете Object Storage создайте папку input.\n3. Загрузите CSV-таблицу в папку input.\n\n## 2. Подготовьте скрипт задачи\nНа этом шаге вы загрузите в хранилище Object Storage файл, содержащий скрипт для обработки данных из CSV-таблицы.\n1. Скопируйте скрипт и назовите файл delta-script.py.\n\n```\nimport time\nfrom pyspark.sql import SparkSessionfrom pyspark.sql.types import DoubleType, FloatType, LongType, StructType,StructField, StringTypefrom delta import *\nspark = (SparkSession.builder        .appName('Delta test')        .enableHiveSupport()        .getOrCreate()        )\nSCHEMA = StructType([    StructField(\"vendor_id\", LongType(), True),    StructField(\"trip_id\", LongType(), True),    StructField(\"trip_distance\", FloatType(), True),    StructField(\"fare_amount\", DoubleType(), True),    StructField(\"store_and_fwd_flag\", StringType(), True)])\nTABLE_TIME = time.strftime('%Y_%m_%d__%H_%M_%S')TABLE_NAME = \"delta_lab\" + TABLE_TIMEROOT_PATH = \"s3a://your-bucket-name/\"CSV_PATH = ROOT_PATH + \"input/delta-table.csv\"FULL_PATH_DELTA_TABLE = ROOT_PATH + \"warehouse_delta/\" + TABLE_NAME\ndef read_csv_to_table():    _csv_df = (        spark        .read        .option(\"delimiter\", \";\")        .option(\"header\", True)        .csv(CSV_PATH, schema=SCHEMA)    )    _csv_df.show()    return _csv_df\ndef insert_data_to_table(df):    df.write.format(\"delta\").save(FULL_PATH_DELTA_TABLE)\ndef read_data_from_table():    df = spark.read.format(\"delta\").load(FULL_PATH_DELTA_TABLE)    df.show()\ndef update_delta_table():    delta_table = DeltaTable.forPath(spark, FULL_PATH_DELTA_TABLE)\n    delta_table.update(        condition=\"vendor_id % 2 = 0\",        set={            \"trip_distance\": \"trip_distance + 2\"        }    )\ndef show_history_delta():    delta_table = DeltaTable.forPath(spark, FULL_PATH_DELTA_TABLE)    history = delta_table.history()    history.show()\ndef read_specific_version_delta(version: int):    df = spark.read.format(\"delta\").option(\"versionAsOf\", version).load(FULL_PATH_DELTA_TABLE)    df.show()\nif __name__ == \"__main__\":    csv_df = read_csv_to_table()    insert_data_to_table(df=csv_df)    read_data_from_table()\n    update_delta_table()    read_data_from_table()\n    update_delta_table()    read_data_from_table()\n    show_history_delta()\n    read_specific_version_delta(version=1)\n    spark.stop()\n```\n2. В строке ROOT_PATH = \"s3a://your-bucket-name/\" скрипта замените your-bucket-name на название бакета Object Storage.\n3. В ранее созданном бакете Object Storage создайте папку jobs.\n4. Загрузите скрипт в папку jobs.\nВ результате получится следующая структура бакета с файлами:\n- <bucket>\n- input\n- delta-table.csv\n- jobs\n- delta-script.py\n \n Описание работы Python-скрипта \n \n\n## 3. Создайте задачу Managed Spark\nНа этом шаге вы создадите задачу Managed Spark с использованием подготовленного скрипта.\nДля продолжения работы убедитесь, что статус инстанса Managed Spark изменился на «Готов».\n1. Перейдите в сервис Managed Spark.\n2. Откройте созданный ранее инстанс.\n3. Перейдите на вкладку Задачи.\n4. Нажмите Создать задачу.\n5. В блоке Общие параметры введите название задачи, например delta.\n6. В блоке Скрипт приложения:\n\n- В поле Тип запускаемой задачи выберите Python.\n- В поле Путь к запускаемому файлу укажите путь к файлу delta-script.py.\nВ данном случае путь s3a://{bucket_name}/jobs/delta-script.py, где {bucket_name} — название созданного бакета Object Storage.\n7. В блоке Настройки активируйте опцию Добавить Spark-конфигурацию (–conf). Добавьте следующие параметры и их значения:\n Параметр Значениеspark.jars.packagesio.delta:delta-spark_2.12:3.2.0spark.sql.extensionsio.delta.sql.DeltaSparkSessionExtensionspark.sql.catalog.spark_catalogorg.apache.spark.sql.delta.catalog.DeltaCatalogspark.log.levelERROR\n8. Нажмите Создать.\nЗадача Managed Spark начнет выполняться и отобразится на странице инстанса во вкладке Задачи.\n\n## 4. Наблюдайте за ходом выполнения задачи\nНа этом шаге вы будете наблюдать за ходом выполнения задачи, просматривая информацию, поступающую в логи.\nВы можете посмотреть логи задачи, когда задача находится в статусах «Выполняется» и «Готово», то есть как в процессе выполнения, так и по завершению задачи.\n\n### Перейдите к логам\n1. В строке задачи нажмите  и выберите Перейти к логам.\n2. Используйте фильтр, чтобы найти логи, например, за определенное время.\n\n### Перейдите в Spark UI\n1. Откройте инстанс Managed Spark.\n2. Во вкладке Задачи нажмите Spark UI. В соседней вкладке откроется интерфейс Spark UI.\n3. Вернитесь в инстанс и откройте вкладку Информация.\n4. Скопируйте данные из блока Настройки доступа.\n5. Введите данные инстанса:\n- Username — значение поля Пользователь.\n- Password — значение секрета в поле Пароль.\nВ интерфейсе Spark UI вы найдете информацию о ходе выполнения задачи.\n\n## Результат\nКогда задача перейдет в статус «Выполнено», откройте бакет Object Storage.\nВ бакете появится новая папка с названием формата delta-lab_<TIME_STAMP>.\nВ этой папке хранятся:\n- версии таблицы «delta-table.csv»;\n- папка _delta_log с логами задачи.\nЧтобы посмотреть историю изменений таблицы с помощью метода history():\n1. Откройте сервис Managed Spark.\n2. Перейдите на вкладку Задачи.\n3. Скопируйте ID задачи.\n4. Нажмите  и выберите Перейти к логам.\n5. В поле Запрос введите labels.spark_job_id=\"ID\", где ID — идентификатор задачи, скопированный ранее.\n6. Нажмите Скачать журнал логов.\n7. Выберите формат файла.\n8. Нажмите Скачать.\n9. Откройте скачанный файл.\nИстория изменений отображается в нескольких сообщениях.\nВы обработали таблицу формата Delta Lake с помощью сервиса Managed Spark и просмотрели информацию об изменениях в таблице.\n\n  [© 2025 Cloud.ru](https://cloud.ru)",
      "debug": {
        "start_page": 1,
        "end_page": 1
      }
    }
  ]
}
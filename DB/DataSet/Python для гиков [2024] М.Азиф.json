{
  "title": "Python для гиков [2024] М.Азиф",
  "chapters": [
    {
      "name": "Глава 1. Оптимальный жизненный цикл разработки на Python 23",
      "content": "--- Страница 23 ---\n24 Раздел 1. Python помимо основ уделяет особое внимание написанию кода и придерживается своей уникальной фи­ лософии разработки. Сегодня Python используется в самых разных отраслях -от образования до медицины, и в каждом проекте ощущается его характерная культура. Разработчики считают, что код должен быть максимально простым. Если задачу можно решить несколькими способами, следует выбрать вариант, наиболее соот­ ветствующий соглашениям и философии Python. Настоящие фанаты даже создают артефакты, максимально соответствующие правилам языка, а нарушителей этих правил считают плохими программистами. В этой книге мы постараемся не отсту­ пать от философии Python. Существует даже официальный документ, написанный Тимом Питерсом, -<<Дзен Python» (The Zen of Python), который призывает поддерживать принципы языка. В нем сказано, что Python должен быть максимально аккуратным, понятным и хоро­ шо задокументированным. Прочесть его можно, выполнив в консоли команду import this (рис. 1.1 ): ( 1 ) illlport: t:his thia С• The Zen of Python, Ьу Tilll Pet:ers BeAut:iful is Ьett:er t:han u9ly. Explicit is better th.m illlplicit. Silllple is Ьett:er t:han complex. Complex is Ьetter thAn complicated. Flat is Ьet:ter than nested. Sparse is Ьet:ter than dense. ReadaЬilit:y counts. SpeciAl сАаев Aren't special enou9h t:o break t:he rules. Alt:hou9h prActicalit:y beat:s purit:y. Errors should never pass silent:ly. Unlesa explicitly silenced. In t:he face of amЬiguity, refuse t:he t:empt:ation to guезз. There should Ье one--and preferaЬly only one --obvious way t:o do it:. Althou9h thAt way may not Ье obvious at first unless you're Dutch. Now is better than never. Althou9h never is oft:en Ьetter t:han *ri9ht* now. If t:he implementation is hard t:o explain, it's а Ьаd idea. If t:he implementation is easy to explain, it may Ье а 9ood idea. Namespaces are one honkin9 9reat idea --let's do more of thoвel <module 'this' from '/usr/liЬ/pythonЗ.6/this.py'> Рис. 1.1. Дзен Python Красивое лучше, чем уродливое. Явное лучше, чем неяшюе. Простое лучше, чем сложное. Многосоставное лучше, чем запутанное. Последовательное лучше, чем вложенное.\n--- Страница 24 ---\nГлава 1. Оптимальный жизненный цикл разработки на Python У меренное лучше, чем нагроможденное. Читаемость имеет значение. Особые случаи не настолько особые, чтобы нарушать правила. При этом практичность важнее безупречности. Ошибки никогда не должны замалчиваться. Если они не замалчиваются явно. Встретив двусмысленность, не пытайся угадать. Должен существовать, предпочтительно, только один очевидный способ решить задачу. Хотя он поначалу может быть и не очевиден. Сейчас лучше, чем никогда. Хотя никогда зачастую лучше, чем *прямо* сейчас. Если реализацию сложно объяснить, то идея плохая. Если реализацию легко объяснить, то идея, возможно, хорошая. Пространство имен -отличная штука! Будем делать его больше!25 Текст выглядит загадочно, будто выбит на стене в гробнице фараона, но это сдела­ но намеренно. При этом каждая строчка имеет глубокое значение. Мы будем ссы­ латься на <<дзен Python» на протяжении всей книги, поэтому обсудим несколько отрывков: ♦«Красивое лучше, чем уродливое»: важно, чтобы код был хорошо написан, чита­ ем и не требовал разъяснений; он должен не только отлично работать, но икра­ сиво выглядеть; не следует использовать кратчайший путь при написании кода, это может затруднить его чтение. ♦«Простое лучше, чем сложное»: не нужно усложнять; если есть выбор, следует использовать более простой вариант; лишних сложностей нужно избегать; если для большего удобства необходимо удлинить код, лучше предпочесть этот ва­ риант. ♦«Должен существовать, предпочтительно, только один очевидный способ ре­ шить задачу»: у проблемы должно быть только одно наилучшее решение, кото­ рое нужно постараться найти; работая над структурой кода, стоит стремиться к такому решению. ♦«Сейчас лучше, чем никогда»: не стоит ждать идеальных условий; решать про­ блему следует сейчас, опираясь на имеющуюся информацию, предположения, навыки, инструменты и инфраструктуру; после уже можно улучшать решение; не нужно ждать подходящего момента, он может никогда не наступить. ♦«Явное лучше, чем неявное»: код не должен требовать пояснений; к выбору имен переменных, классов и структуры функций, а также к созданию общей архитек-\n--- Страница 25 ---\n26 Раздел 1. Python помимо основ туры следует подходить разумно; здесь следует проявить чрезмерную бдитель­ ность и сделать код максимально понятным. ♦«Последователыюе лучше, чем вложеююе»: вложенная структура занимает меньше места, но затрудняет чтение; по возможности используйте последова­ тельные структуры. Этапы проекта Python Для начала определим основные этапы проекта. Каждый из них включает несколь­ ко схожих действий, как показано на схеме (рис. 1.2): Рис. 1.2. Этапы проекта Python Стандартный проект Python состоит из следующих этапов: 1.Анализ требований: на этом этапе мы общаемся с ключевыми заинтересован­ ными лицами и анализируем их требования. Важно понять, что нужно делать, прежде чем решить, как это делать. Такими лицами могут быть пользователи или руководители компании. Необходимо наиболее подробно записать все тре­ бования, изучить их и обсудить с конечными пользователями перед следующим этапом; анализ требований не входит в цикл проектирования, разработки и тестирования, его нужно завершить до этих этапов. Важны как функциональ­ ные, так и нефункциональные требования. Первые следует разделить на модули, которые должны максимально соответствовать будущим модулям кода. 2.Проектирование: это техническое решение в ответ на требования из предыду­ щего этапа. Здесь идет обдумывание, как мы будем создавать наше решение. Это творческий процесс, в ходе которого используются опыт и навыки для соз­ дания подходящего набора и структуры модулей и оптимальных связей между\n--- Страница 26 ---\nГлава 1. Оптимальный жизненный цикл разработки на Python 27 ними. На этом этапе особенно важно не допускать ошибок. Любые неверные решения на этом этапе обходятся дороже, чем на более поздних этапах. На ис­ правление ошибок потребуется в 20 раз больше усилий, чем на устранение оши­ бок аналогичного масштаба на этапе н,аписан.ия кода (разработка). Например, последствия при неверном подборе подходящих данных или неправильно рас­ считанном масштабе проекта будут более серьезными, по сравнению с ошибка­ ми при реализации функции. Проектирование часто лежит в более абстрактной плоскости идей. Ошибки здесь не так очевидны, их невозможно обнаружить в ходе тестирования, но можно легко перехватывать с помощью хорошо проду­ манной системы обработки исключений. 3.На этапе проектирован.ия мы: •создаем структуру кода и определяем его модули; •выбираем фундаментальный подход ( функциональное программирование, объектно-ориентированное программирование или их гибрид); •определяем классы и функции и выбираем имена для этих высокоуровневых компонентов; 4.Разработка (Написание кода): здесь мы воплощаем проект на Python, начиная с высокоуровневых абстракций, компонентов и модулей, а затем переходя к деталям. 5.Тестирование: на этом этапе выполняется проверка, правильно ли работает код. 6.Развертывание: после тщательного тестирования мы предоставляем наше ре­ шение конечному пользователю, которому неважно, чем мы занимались на пре­ дыдущих этапах. Он просто решает свои задачи, описанные в требованиях. До­ пустим, мы работаем над проектом по машинному обучению, цель которого - прогнозировать осадки в Оттаве, тогда в ходе развертывания мы пытаемся опре­ делить, как предоставить пользователю удобное решение. Теперь вы понимаете, из каких этапов состоит проект, и мы можем перейти к стра­ тегическим аспектам. Стратегия процесса разработки Стратегическое планирование разработки охватывает каждый этап и переходы ме­ жду ними. Для составления плана мы должны ответить на следующие вопросы: 1.Собираемся ли мы использовать подход с минимальным проектированием и сразу перейти к написанию кода? 2.Выберем ли мы разработку через тестирован.ие (Test-Driven Development, TDD), где сначала пишутся тесты на основе требований, а затем создается код? 3.Хотим ли мы сначала создать мин.имальн.о жизн.еспособн.ый продукт (Minimum ViaЫe Product, MVP), а затем постепенно развивать его?\n--- Страница 27 ---\n28 Раздел 1. Python помимо основ 4.Какой стратегии мы будем придерживаться при проверке таких нефункциональ­ ных требований, как безопасность и производительность? 5.Будем ли мы разрабатывать под конкретные устройства или же развернем целый кластер или облако? 6.Какими будут объем, скорость и типы входных и выходных данных? Будем ис­ пользовать распределенную файловую систему Hadoop (Hadoop distributed Ш.е system, HDFS) или файловую структуру Simple Storage Service (S3) от Amazon? Базу данных будем использовать SQL или NoSQL? Данные будут храниться локально или в облаке? 7.Работаем ли мы над такими специализированными вариантами использования, как машинное обучение (Machine Learning) с особыми требованиями к созда­ нию конвейеров данных и моделей для тестирования, развертывания и обслужи­ вания? Ответы на эти вопросы помогут определить этапы процесса разработки. В послед­ нее время предпочтительно использовать процессы итеративного подхода в том или ином виде. Также на старте популярна концепция MVP. Обо всем этом мы по­ говорим в следующих подразделах. Итерация по этапам Современный подход к разработке предусматривает короткие циклы проектирова­ ния, разработки и тестирования. Традиционная каскадная модель уже давно мертва. Выбор правильных детализации, акцента и частоты этапов зависит от характера проекта и избранной стратегии разработки. Если мы хотим выбрать стратегию с минимальной разработкой и сразу перейти к написанию кода; этап проектирования будет коротким. Но даже в этом случае необходимо обдумать структуру будущих модулей. Независимо от выбора стратегии этапы связаны между собой. Мы начинаем с про­ ектирования, реализуем план в коде, затем тестируем его. Отметив все недостатки, мы возвращаемся на этап проектирования. Стремление к MVP в первую очередь Рекомендуется отбирать только самые важные требования для создания минималь­ но жизнеспособного продукта и постепенного его улучшения. Циклы проектирова­ ния, написания кода и тестирования повторяются снова и снова, пока не будет го­ тов итоговый продукт, который можно развернуть и использовать. Рассмотрим, как реализовать решение некоторых специализированных предметных областей на Python.\n--- Страница 28 ---\nГлава 1. Оптимальный жизненный цикл разработки на Python Стратегия разработки для специализированных предметных областей 29 Python используется в самых разных сценариях. Рассмотрим пять важных областей, где он применяется , и узнаем, как составлять стратегию разработки в зависимости от специфики каждого из них: ♦машинное обучение; ♦облачные и кластерные вычисления; ♦системное программирование; ♦сетевое программирование; ♦бессерверные вычисления. Рассмотрим каждый вариант поподробнее. Машинное обучение На сегодняшний день Python -самый популярный язык для написания алгоритмов машинного обучения , где требуется хорошо структурированная среда. Язык имеет обширную коллекцию высококачественных библиотек для реализации ML. В стандартном проекте обычно используется методология CRISP-DM (Cross­ lndustry Standard Process for Data Mining) -стандарт, описывающий общие процессы и подходы к аналитике данных. Схема жизненного цикла CRISP-DM вы­ глядит следующим образом (рис. 1.3): Рис. 1.3. Жизненный цикл CRISP-DM\n--- Страница 29 ---\n30 Раздел 1. Python помимо основ В проектах ML проектирование и реализация конвейеров обработки данных зани­ мают почти 70% процесса разработки, поэтому нужно стремиться к следующему: ♦Их можно легко масштабировать. ♦Их можно использовать многократно. ♦Они поддерживают потоковую и пакетную обработку в соответствии со стан­ дартами Apache Beam. ♦Они сочетают в себе функции fit() и transform() (подробнее об этом в главе 6 «Расширенные советы и приемы Python» ). Оценка модели машинного обучения -важная часть этапа тестирования. Важно выбрать подходящие метрики производительности для верной оценки модели в со­ ответствии с задачей, характером данных и типом реализуемого алгоритма. Что нам важнее: правильность, точность, полнота, показатель Fl-score или комбинация этих метрик? Помимо стандартных тестов обязательно проводится оценка модели. Облачные и кластерные вычисления Облака и кластеры усложняют инфраструктуру тем, что требуют подключения специализированных библиотек. Архитектура языка позволяет начать с минималь­ ного количества базовых пакетов, а затем импортировать дополнительные, что имеет решающее значение и отлично подходит для облачных и кластерных вычис­ лений. Python -предпочтительный язык для Amazon Web Services (AWS), Microsoft Azure и Google Cloud Platform (GCP). Проекты в этой области имеют отдельные среды разработки, тестирования и произ­ водства. Важно синхронизировать их между собой. При использовании модели инфраструктура как услуга (infrastructure-as-a­ service, laaS) контейнеры Docker могут сильно помочь -код можно запускать где угодно, и он везде будет иметь одинаковые среду выполнения и зависимости. Системное программирование В Python есть интерфейсы для служб операционной системы (ОС). Основные биб­ лиотеки имеют привязки к PortaЫe Operating System Interface (POSIX), что по­ зволяет разработчикам создавать так называемые инструменты оболочки и исполь­ зовать их для системного администрирования и различных утилит. Такие инстру­ менты, написанные на Python, совместимы на разных платформах. Один и тот же инструмент можно использовать на Linux, Windows и macOS без изменений. Например, инструмент оболочки, который копирует полный каталог, разработан­ ный и протестированный под Linux, будет без изменений работать в Windows. Под­ держка Python для системного программирования включает следующее: ♦Определение переменных среды. ♦Поддержка файлов, сокетов, каналов, процессов и многопоточности.\n--- Страница 30 ---\nГлава 1. Оптимальный жизненный цикл разработки на Python 31 ♦Возможность использовать регулярные выражения (Regular Expression, regex) для сопоставления по шаблону. ♦Возможность предоставлять аргументы командной строки. ♦Поддержка стандартных потоковых интерфейсов, запуска shеll-команд и расши- рений файлов. ♦Возможность создавать ZIР-архивы для файловых утилит. ♦Поддержка парсинга файлов XML и JSON. Когда для системного программирования используется Python, этап развертывания будет минимальным. Код просто упаковывается в исполняемый файл. Кстати, язык не подходит для разработки системных драйверов или библиотек операционной системы. Сетевое программирование В эпоху автоматизации систем информационных технологий самым узким местом считаются сети. Проблема кроется в использовании собственных сетевых операци­ онных систем от разных производителей и недостаточная их открытость. Предпо­ сылки цифровой трансформации меняют эту тенденцию. Ведутся масштабные работы по изменению сети с целью сделать ее легко программируемой и потреб­ ляемой как услуга (network-as-a-service, NaaS). Главный вопрос: можно ли исполь­ зовать Python для сетевого программирования? Разумеется, ДА. Фактически это один из самых популярных языков для автоматизации сетей. Поддержка Python для сетевого программирования включает следующее: ♦Программирование сокетов, включая ТСР и UDP. ♦Поддержка связи «клиент-сервер». ♦Поддержка прослушивания портов и обработки данных. ♦Выполнение команд в системе Secure Shell (SSH). ♦Загрузка и выгрузка файлов по протоколам SCP и FTP. ♦Поддержка библиотек для работы с SNMP. ♦Поддержка протоколов RESTCONF и NETCONF для извлечения и обновления конфигурации сетевых устройств. Бессерверные вычисления Это облачная модель выполнения приложений, в которой поставщики облачных сервисов (Cloud Service Provider, CSP) предоставляют вычислительные ресурсы и серверы приложений. Таким образом, разработчики могут развертывать и выпол­ нять код без проблем, связанных с управлением этими вычислительными ресурса­ ми и серверами. Все основные поставщики (Microsoft Azure Serverless Functions, А WS Lambda и Google Cloud Platform) поддерживают бессерверные вычисления для Python.\n--- Страница 31 ---\n32 Раздел 1. Python помимо основ Важно понимать, в таких вычислениях все же есть серверы, но управляют ими CSP. Разработчики не занимаются установкой и обслуживанием серверов и напрямую не отвечают за их масштабируемость и производительность. Для Python существуют следующие популярные бессерверные библиотеки . и фреймворки: ♦Serverless: фреймворк с открытым кодом для бессерверных функций или серви­ сов А WS Lambda, написанный на Node.js; это первый фреймворк, разработан­ ный для создания приложений на А WS Lambda. ♦Chalice: бессерверный микрофреймфорк для Python, разработанный А WS; луч­ ший вариант для всех, кто хочет через А WS Lambda быстро развертывать при­ ложения, автоматически меняющие масштаб по необходимости; имеет ключе­ вую особенность -утилиту для локального моделирования приложения до от­ правки его в облако. ♦Zappa: это инструмент развертывания, встроенный в Python, который упрощает развертывание WSGI-приложений. Далее рассмотрим эффективные способы разработки кода Python. Эффективное документирование кода Python Найти эффективный способ документирования кода всегда важно. Задача состоит в составлении всеобъемлющего, но простого способа разработки кода. Сначала рас­ смотрим комментарии в Python, а потом и строки документации. Комментарии Python В отличие от строк документации, комментарии не видны компилятору. Они нуж­ ны для пояснения кода. Комментарий в Python начинается с символа «il», как пока­ зано на снимке (рис. 1.4): Docstring # Простой пример для демонстрации комментариев. print (\"This is Chapter 1\") This is Chapter 1 Рис. 1.4. Пример комментария в Python Основной способ документирования кода -многострочный блок комментариев, именуемый Docstring. Особенность языка в том, что строки документации связаны с объектом и доступны для проверки. Рекомендации по docstring описаны в Пред­ ложениях по улучшению Python (Python Enhancement Proposal, РЕР) 257, и их\n--- Страница 32 ---\nГлава 1. Оптимальный жизненный цикл разработки на Python 33 цель -дать читателю понимание, что они должны быть краткими, но достаточно подробными. Строки docstring обозначаются тремя двойными кавычками (\"\"\"). Некоторые рекомендации по созданию docstring: ♦Размещать его следует сразу после определения функции или класса. ♦Сначала лучше написать однострочное пояснение, а затем более подробное опи­ сание. ♦Пустые места следует умело использовать для организации комментариев, а пустые строки для организации кода, но не следует ими злоупотреблять. В следующих разделах подробнее поговорим о docstring. Стили docstring В Python есть несколько стилей оформления: ♦Google. ♦NumPy/SciPy. ♦ Epytext. ♦Restructured. Типы docstring При разработке кода мы создаем разные типы документации, в том числе: ♦построчные комментарии; ♦документация по функциям или классам; ♦описание алгоритмов. Рассмотрим каждый вариант подробнее. Построчные комментарии Один из простых вариантов использования docstring -написание комментария в несколько строк (рис. 1.5): Это комментар ий налисаннЫЙ, более чем в одну строку print (\"Chapter 2 wПl Ье aЬout Python Мodules\") с➔ Chapter 2 will Ье about Python Modules Рис 1.5. Пример построчного комментария\n--- Страница 33 ---\n34 Раздел 1. Python помимо основ Документация на уровне функций или классов Если docstring поместить сразу после определения функции или класса, Python свяжет документирование именно с ними. Комментарий будет храниться в атрибу­ те doc этой функции или класса, и его можно просмотреть непосредственно во вре­ мя выполнения программы с помощью атрибута doc или функции help, как показано в примере (рис. 1.6): [5} def douЫe(n): '' 'Takes in а numЬer n, returns the douЫe of its value''' return 'n••2 [б} print(douЫe._doc_) С-+ Takes in а numЬer n, returns the douЫe of its value [8} help(douЬle) С-+ Help on function douЫe in module _main_: douЬle(n) Takes in а numЬer n returns the douЫe of its value Рис. 1.6. Пример функции help О c1ass ComplexNumber Это к.пасс для r 1атематических операuий над комплексными числами Атрибуты: real (тип .int): вещественная часть комплексного числа. imag (тип int): мнимая часть вещественного числа. def_iпit._ (self, real, imag): The constructor for ComplexNumЬer class. Параметры: real (тип iпt): вещt.�с•1•менная 1,1ас:ть комплексноРо числа. irnag (тип int): мнима с� часть вещестееннсго 1.:1и<.\":ла. def add(self, num): Функция добавлен ия дsух комплексных чисел. Параметры: nLнr, (!<Ласе CornplexNL1li'.ЬerJ: добавлР.емое ко�плек.сное число. Возвращае'!': CcmplexNumher: комплексное число , КО'!'Орое содержит сумму. re = self.real + num.real im = self.imag + num.imag return ComplexNumЬer {re, im} Рис. 1.7. Пример docstring для класса\n--- Страница 34 ---\nГлава 1. Оптимальный жизненный цикл разработки на Python 35 Для документирования классов рекомендуется придерживаться следующей струк­ туры: ♦Краткое описание, желательно в одну строку. ♦Первая пустая строка. ♦Подробное описание. ♦Вторая пустая строка. Пример показан на рис. 1.7. Детали алгоритмов В Python все чаще используется описательная или прогностическая аналитика и прочая сложная логика. Детали используемого алгоритма должны быть четко ука­ заны со всеми предположениями. Если алгоритм реализован как функция, его ло­ гику лучше описать до сигнатуры этой функции. Разработка эффективной схемы именования Если разработать и реализовать правильную логику в коде -это наука, то сделать ее красивой и читаемой -это искусство. Разработчики уделяют особое внимание схеме наименования и соблюдению философии Дзен Python. Это один из немногих языков, для которых существуют подробные рекомендации в этой области, напи­ санные самим Гвидо ван Россумом. Они описаны в документе РЕР 8, где есть це­ лый раздел про соглашения об именовании, за которым следует множество баз кода. Об этом можно узнать подробнее по ссылке https://www.Python.org/dev/peps/pep-0008/. Общие принципы именования, описанные в РЕР 8: ♦Имена модулей записываются в нижнем _регистре. ♦Имена классов и исключений записываются в ВерблюжьемРегистре. ♦Глобальные и локальные переменные записываются в нижнем _регистре. ♦Имена функций и методов записываются в нижнем _регистре. ♦Константы записываются в ВЕРХНЕМ РЕГИСТРЕ. Некоторые рекомендации по структуре кода из РЕР 8: ♦Отступы важны. используйте для них четыре пробела вместо табуляции (ТАВ). ♦Вложение должно быть не глубже четырех уровней. ♦Строка не должна превышать 79 символов. длинные строки можно разделять символом «\\». ♦Для лучшей читаемости кода функции следует разделять двумя пустыми стро­ ками. ♦Между логическими частями кода лучше вставлять пустую строку.\n--- Страница 35 ---\n36 Раздел 1. Python помимо основ Данные рекомендации -это просто предложения, которые можно адаптировать. При этом схема именования все же должна опираться на принципы РЕР 8 в качест­ ве основного документа. Рассмотрим подробнее различные схемы именования в контексте языковых струк­ тур Python. Методы В именах методов следует использовать символы в нижнем регистре. Имя должно состоять из одного или нескольких слов, разделенных символом подчеркивания. Пример: calculate sum Для простоты читаемости кода выбирать для имени метода следует глагол, который указывает на действие, выполняемое методом. Если это метод с доступом, отличным от puhlic, его имя начинается с символа под­ черкивания. Пример: _my_calculate_sum Dunder (DouЫe Under) или магические методы (Magic Methods) начинаются и заканчиваются символом подчеркивания. Пример: ♦_init_; ♦add Два символа подчеркивания не рекомендуется указывать в начале или в конце име­ ни метода. Такая схема именования предназначена для встроенных методов Python. Переменные Имена переменных записываются в нижнем регистре и состоят из одного или не­ скольких слов, разделенных символом подчеркивания. Это должно быть существи­ тельное, соответствующее сущности, которую оно представляет. Пример: ♦х; ♦my_var. Имена переменных с доступом private начинаются с символа подчеркивания. Пример: _my_secret_variaЬle Булевы переменные Булевы переменные, которые начинаются со слова is или has, проще воспринима­ ются. Пример: Class Patient: Is admitted = False Has heartbeat = False\n--- Страница 36 ---\nГлава 1. Оптимальный жизненный цикл разработки на Python 37 Коллекции Коллекции (или наборы) содержат несколько переменных, поэтому лучше давать им имя во множественном числе. Пример: Class Patient: Admitted_patients = ['John', 'Peter'] Словари Имя словаря должно быть максимально явным. Если есть словарь людей, привя­ занных к городам, в этом случае его лучше назвать следующим образом: persons_cities = {'Imran': 'Ottawa', 'Steven': 'Los Angeles'} Константы В Python нет неизменяемых переменных. Например, в С++ ключевое слово const указывает, что переменная представляет собой константу. В Python переменные указываются как константы по соглашению об именовании. При обработке компи­ лятор не выдаст ошибку. Для констант рекомендуется использовать слова в верхнем регистре, разделенные символом подчеркивания. Пример: CONVERSION FACTOR Классы Классы обозначаются в стиле ВерблюжьегоРегистра (CamelCase). Имя начинается с заглавной буквы. Если название содержит больше одного слова, их следует пи­ сать слитно и каждое нужно начинать с заглавной буквы. В имени следует использовать существительные. Оно должно наилучшим образом представлять сущность, которой класс соответствует. Одна из рекомендаций - добавлять дополнительные слова для уточнения типа или характера сущности. Пример: ♦HadoopEngine; ♦ParquetType; ♦TextЬoxWidget. О чем еще нужно помнить: ♦Классы исключений для обработки ошибок всегда заканчиваются словом Error, например: FileNotFoundError ♦Некоторые встроенные классы Python не следуют этим рекомендациям.\n--- Страница 37 ---\n38 Раздел 1. Python помимо основ ♦Для удобства восприятия следует указывать префиксы ваsе и AЬstract для базо­ вых и абстрактных классов соответственно, например: AЬstractCar BaseClass Пакеты В имени пакета не рекомендуется указывать символ подчеркивания. Название должно быть коротким и в нижнем регистре. Если используется больше одного слова, они также записываются строчными буквами. Пример: Mypackage Модули Имя модуля должно быть коротким и понятным. Оно записывается в нижнем реги­ стре, слова разделяются символом подчеркивания. Пример: main_module.py Соглашения об импорте С годами сообщество Python выработало соглашение для псевдонимов, которые используются для популярных пакетов. Пример: import numpy as np import pandas as pd import seaЬorn as sns import statsmodels as sm import matplotlib.pyplot as plt Аргументы Имена аргументов функций рекомендуется называть, как и переменные, поскольку они по своей сути и есть временные переменные. Полезные инструменты Есть пара инструментов, которыми можно проверить, насколько код соответствует рекомендациям РЕР 8. Рассмотрим их. Pylint Устанавливается следующей командой: pip: $ pip install pylint\n--- Страница 38 ---\nГлава 1. Оптимальный жизненный цикл разработки на Python 39 Это анализатор исходного кода, который проверяет соблюдение рекомендаций РЕР 8 при наименованиях и выводит отчет. Его можно настроить и под другие соглашения. РЕР8 Устанавливается следующей командой: pip: $ pip install рерВ Утилита проверяет код на соблюдение правил РЕР 8. Далее поговорим о системах контроля версий для Python. Системы контроля версий Сначала кратко познакомимся с историей систем контроля. Их эволюция прошла несколько этапов развития: ♦Этап 1: исходный код изначально запускался локальными системами, хранив­ шимися на жестком диске; такая коллекция кода называлась локальным репози­ торием. ♦Этап 2: в больших командах невозможно бьшо использовать локальную систему контроля версий; разработчики постепенно перешли на централизованные репо­ зитории, хранившиеся на сервере, к которому был доступ у всей команды; это решило трудности совместного использования кода, но создало дополнительную проблему блокировки файлов в многопользовательской среде. ♦Этап 3: эти проблемы решены в современных репозиториях, наподобие Git; члены команды имеют полную копию, и им необходимо подключаться к репо­ зиторию, только когда есть необходимость поделиться кодом. Что не стоит хранить в репозитории системы контроля версий Во-первых, ничего, кроме файла исходного кода, регистрировать в системе не нуж­ но. Файлы, сгенерированные компьютером, сохранять не стоит. Предположим, есть исходный файл main.py. Если его скомпилировать, сгенерированный код не принад­ лежит репозиторию. Этот код является производным файлом и не должен регист­ рироваться системой контроля версий. На это есть три причины: ♦Любой член команды может создать производный файл после получения исход­ ного кода. ♦Во многих случаях скомпилированный код больше исходного, его добавление сделает репозиторий медленным; если в команде много разработчиков, каждый из них получит копию сгенерированного файла, что очень замедлит работу всей системы.\n--- Страница 39 ---\n40 Раздел 1. Python nомимо основ ♦Системы контроля версий предназначены для хранения дельты (разницы) или изменений, внесенных разработчиками в исходные файлы с момента последнего коммита (commit); файлы, отличные от исходных, обычно имеют двоичный формат, и в системе вряд ли есть инструмент для их сравнения (diff tool), так что ей придется сохранять файл целиком; это также замедлит работу всей сис­ темы. Во-вторых, в системе нельзя хранить конфиденциальную информацию, в том числе ключи API и пароли. В качестве исходного репозитория сообщество Python предпочитают GitHub. Там находится большая часть известных пакетов Python. Если команд разработчиков несколько, необходимо разработать и поддерживать правильный протокол и про­ цедуры взаимодействия. Понимание стратегий развертывания кода Для крупномасштабных проектов, где имеется вполне определенные DEV-или РRОD-среды, развертывание кода и разработка стратегии становятся важными. Python является наиболее предпочтительным языком для облачных и кластерных вычислений. При развертывании кода может возникнуть несколько сложностей: ♦В точности одинаковые преобразования должны происходить в DEV-, TEST- и PROD-cpeдax. ♦Если код постоянно обновляется в DEV-cpeдe, возникает сложность, как син­ хронизировать эти изменения с РRОD-средой. ♦Какой тип тестирования планируется проводить в средах DEV и PROD? Рассмотрим две главные стратегии развертывания кода. Пакетная разработка Эта традиционный процесс разработки. Мы пишем код, компилируем его, а затем тестируем. Процесс повторяется, пока все требования не будут выполнены. Затем происходит развертывание кода. Непрерывная интеграция и непрерывная доставка Непрерывная интеграция и непрерывная доставка (Continuous integration/ Continuous delivery, CI/CD) в контексте Python относится к непрерывной интегра­ ции и развертыванию, а не к их пакетному процессу. Это помогает создать среду DevOps (Development-Operations), устраняя разрыв между разработкой и эксплуа­ тацией программного обеспечения.\n--- Страница 40 ---\nГлава 1. Оптимальный жизненный цикл разработки на Python 41 CI -это непрерывная интеграция, сборка и тестирование разных модулей кода по мере их обновления. Это означает, что код, разработанный индивидуально каждым членом команды, интегрируется, создается и тестируется, как правило, много раз в день. После этого репозиторий в системе контроля версий обновляется. Главное преимущество CI -проблемы и ошибки обнаруживаются в самом начале, обычно в тот же день. Их можно поправить сразу, а не через несколько дней, не­ дель или даже месяцев, когда они просочились в другие модули, а те, в свою оче­ редь, создали множество зависимостей. В отличие от Java и С++, Python является интерпретируемым языком, а значит, код выполняется интерпретатором на любой целевой машине. Для сравнения, компили­ руемый код обычно пишется для одного типа целевых машин и может разрабаты­ ваться разными членами команды. Когда будет понятно, какие действия выполня­ ются при внесении изменения, их можно автоматизировать. Код Python зависит от внешних пакетов, поэтому отслеживание их имен и версий является частью автоматизации процесса сборки. Хорошей практикой будет указы­ вать эти пакеты в файле requirements. txt. Имя может быть любым, но сообщество разработчиков называет его так. Пакеты устанавливаются следующей командой: $pip install -r requirements.txt Создание файла requirements. txt выполняется следующей командой: $pip freeze > requirements.txt Цель непрерьmной интеграции -обнаружить ошибки на ранних стадиях, но она может сделать процесс разработки нестабильным. Допустим, один из программи­ стов внес серьезную ошибку, остальной команде приходится ждать, пока ошибка будет устранена. Надежное самотестирование и выбор правильной частоты инте­ граций помогут решить проблему. Тестировать следует каждое внесенное измене­ ние, причем этот процесс в конечном итоге должен быть автоматизирован. При на­ личии в коде ошибки сборка завершится сбоем, а член команды получит уведомле­ ние. Он сможет сначала предоставить быстрое решение, прежде чем тратить время на устранение и полное тестирование проблемы, и его коллеги не будут заблокиро­ ваны. Когда новый код создан и протестирован, его можно обновить с помощью непре­ рывной доставки -CD. В полном конвейере CVCD каждое изменение собирается и тестируется, а после отображается в развернутом коде. При правильном подходе конечное решение будет постоянно улучшаться. В некоторых случаях каждый цикл CVCD может быть постоянным переходом от МVР к полному решению. Таким об­ разом, решение адаптируется к меняющимся условиям с учетом новой информа­ ции. Хороший пример- пандемия COVID-19, когда новые данные поступают очень быстро. Если продукт зависит от подобных изменений, конвейер CI/CD при­ несет только пользу, поскольку разработчики смогут постоянно обновлять решение при поступлении новой информации. Далее обсудим популярные среды разработки для Python.\n--- Страница 41 ---\nРаздел 1. Python помимо основ Среды разработки Python Для маленьких проектов достаточно использовать любой текстовый редактор. Для средних и крупных -лучшим выбором будет ин.тегрирован.н.ая среда разработки (lntegrated Development Environment, IDE). Она помогает при написании кода, отладке и устранении ошибок, а также совместима с системой контроля версий для упрощения развертывания. На рынке существует множество решений, в основном бесплатных. Обратите внимание, что мы не будем составлять их рейтинг или ран­ жировать в каком-либо порядке, а просто опишем их преимущества. Таким обра­ зом, читатель сможет сделать лучший для себя выбор, основываясь на своем про­ шлом опыте, проектных требованиях и сложности проекта. IDLE Integrated Development and Learning Environment (IDLE) -стандартный редак­ тор Python, доступный для Windows, macOS и Linux. Он поставляется бесплатно и хорошо подходит новичкам в целях обучения. Для сложных проектов не рекомен­ дуется. SuЫime Text Это популярный многоязычный редактор, который подходит для основных плат­ форм (Windows, macOS, и Linux). Бесплатно предоставляется только ознакомительная версия. Python поддерживается на базовом уровне. Но, благодаря возможности устанавливать различные плагины, можно создать полноценную среду разработки, правда, это требует времени и на­ выков. С помощью плагинов среду можно интегрировать с системой контроля вер­ сий, например, Git или Subversion (SVN), но не все функции могут быть доступны. Atom Также популярный редактор, похожий на SuЫime Text. Предоставляется бесплатно. PyCharm Это одна из лучших интегрированных сред разработки для Python. Доступна в Windows, macOS и Linux. Это полноценная IDE с поддержкой автозавершения ко­ да, отладки, рефакторинга, умного поиска, доступа к популярным серверам баз данных, интеграции с системами контроля версий и многих других возможностей. Базовые функции можно дополнять с помощью плагинов. PyCharm поставляется в следующих форматах: ♦Community Edition -бесплатная версия для разработки на Python.\n--- Страница 42 ---\nГлава 1. Оптимальный жизненный цикл разработки на Python 43 ♦Professional Edition -платная версия с поддержкой веб-разработки (HTML, JavaScript и SQL). Visual Studio Code VS Code -среда с открытым исходным кодом от Microsoft. Лучшая IDE для раз­ работки на Python в Windows. По умолчанию поставляется без поддержки этого языка, но можно установить расширения. Поставляется бесплатно и доступна также для macOS и Linux. Это легкая среда с множеством удобных функций: автозавершение кода, отладка, рефакторинг, поиск, доступ к серверам баз данных, интеграция с системами контроля версий и т.д. PyDev Если вы знакомы со средой Eclipse, на PyDev стоит обратить внимание. Он пред­ ставляет собой плагин в виде редактора .для Eclipse, который также можно исполь­ зовать для Jython и IronPython. Предоставляется бесплатно и доступен для всех ос­ новных платформ. В нем есть все преимущества Eclipse, а также удобная интегра­ ция с Django, модульное тестирование и Google Арр Engine (GAE). Spyder Если вы планируете использовать язык для data science и машинного обучения, Spyder хорошо подойдёт в качестве IDE. Среда написана на Python и предлагает инструменты для редактирования, отладки, интерактивного выполнения, глубокой проверки и расширенной визуализации. Она также поддерживает интеграцию с Matplotlib, SciPy, NumPy, Pandas, Cython, IPython и SymPy. Профессиональным разработчикам можно порекомендовать PyCharm или PyDev. Но если вы увлекаетесь data science и машинным обучением, Spyder, безусловно, стоит изучить. Заключение В этой главе мы заложили фундамент, на котором будут строиться последующие главы книги. Начали с духа и философии языка, затем обсудили этапы стандартно­ го проекта и способы оптимизировать работу над ними в зависимости от варианта применения. Для такого лаконичного языка, как Python, особенно важна качествен­ ная документация, упрощающая понимание кода. Поэтому мы также рассмотрели практики ее ведения. Поговорили о схемах именования, рассмотрели системы кон­ троля версий и узнали о способах развертывания кода. В заключение сравнили не­ сколько сред разработки с целью помочь вам сделать выбор в зависимости от тре­ бований.\n--- Страница 43 ---\n44 Раздел 1. Python помимо основ Эта глава будет полезна всем, кто начинает работу над проектом на Python. Она поможет разработать эффективную стратегию и принять взвешенные решения на этапе проектирования. В следующей главе мы поговорим, как разделить код в про­ екте на модули. Вопросы 1.Что такое Дзен Python? 2.Какой тип документирования доступен в Python во время вьmолнения программы? 3.Как устроен жизненный цикл CRISP-DM? Дополнительные ресурсы ♦· Modern Python Cookbook, второе издание, автор: Стивен Ф. Лотт (Steven F. Lott). ♦Python Programming Blueprints, автор: Дэниел Фуртадо (Daniel Furtado). ♦Secret Recipes ofthe Python Ninja, автор: Коди Джексон (Cody Jackson). Ответы 1.Это рекомендации по разработке проектов на Python, написанных Тимом Питерсом. 2.В отличие от обычных комментариев, docstring доступен компилятору во время выполнения. 3.CRISP-DM расшифровывается как Cross-Industry Standard Process for Data Mining. Он применяется к жизненному циклу проектов Python в сфере машин­ ного обучения и определяет различные этапы проекта.\n--- Страница 44 ---\n2 Использование модулей для сложных проектов Начинающие Руthоn-программисты часто предпочитают помещать весь код в один файл, поскольку так не возникает проблем с определением функций и классов, они находятся вместе с основной программой. Такой способ привлекает новичков из-за простоты выполнения кода и возможности избежать управления несколькими фай­ лами. Эrот вариант плохо масштабируется и не подходит для средних и крупных проектов. С увеличением в программе функций и классов становится сложнее их отслеживать. Решить проблему можно с помощью разделения на модули. Это ключевой инстру­ мент для снижения сложности проекта. Модульность способствует повышению эффективности программирования, простой отладке, совместной работе и повтор­ ному использованию. В этой главе мы расскажем, как создавать и использовать мо­ дули и пакеты в Python. Темы этой главы: ♦Знакомство с модулями и пакетами. ♦Импорт модулей. ♦Загрузка и инициализация модуля. ♦Написание многоразовых модулей. ♦Сборка пакетов. ♦Доступ к пакетам из любого расположения. ♦Общий доступ к пакету.https://t.me/it_boooks/2\n--- Страница 45 ---\n46 Раздел 1. Python помимо основ Технические требования В этой главе вам понадобится: ♦Python 3.7 или более поздней версии. ♦Аккаунт Test PyPI и токен API в этом аккаунте. Пример кода для этой главы можно найти по ссылке: https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter02. Знакомство с модулями и пакетами Модули представляют собой файлы Python с разрешением .ру. Это способ органи­ зовать функции, классы и переменные с помощью одного или нескольких файлов для простоты управления, повторного использования и дополнения по мере услож­ нения проrраммы. Пакет -это следующий уровень модульного проrраммирования. Он похож на папку для организации множества модулей или подпакетов, что имеет основопола­ гающее значение для обмена пакетами с целью многократного использования. Исходными файлами Python, в которых используются стандартные библиотеки, можно легко делиться через электронную почту, GitHub и общие диски. Но делать это можно с единственным предостережением -версии языка должны быть со­ вместимы. Такой подход не работает в проектах с большим количеством файлов и зависимостями от сторонних библиотек, тем более если код написан для опреде­ ленной версии языка. Для выхода из такой ситуации создание и совместное исполь­ зование пакетов являются обязательными условиями для эффективного обмена ко­ дом и многократного его использования. Далее рассмотрим, как импортировать модули и какие способы импорта поддержи­ ваются в Python. Импорт модулей Код в одном модуле может получить доступ к коду в другом модуле с помощью процесса, называемого импортом модулей. Для наглядности создадим два модуля и один скрипт, который будет их использо­ вать. Модули будут обновляться и повторно использоваться на протяжении всей главы. Для этого создадим файл с расширением .ру и именем модуля. В нашем примере это будет mycalculator. ру с двумя функциями: add () И suЬtract (). Функция add () вы­ числяет сумму двух чисел, переданных ей в качестве аргументов, и возвращает ре­ зультат. Функция suЬtract () вычисляет разность двух чисел, переданных ей в каче­ стве аргументов, и возвращает результат.",
      "debug": {
        "start_page": 23,
        "end_page": 45
      }
    },
    {
      "name": "Глава 2. Использование модулей для сложных проектов 45",
      "content": "--- Страница 46 --- (продолжение)\nГлава 2. Использование модулей для сложных проектов Фрагмент кода mycalculator .ру: # mycalculator.py с функциями add и subtract · def add(x, у): \"\"\"Эта функция складывает 2 числа\"\"\" return х + у def suЬtract(x, у): \"\"\"Эта функция вычисляет разность 2 чисел\"\"\" return х - у Имя модуля совпадает с именем файла. 47 Затем создадим второй модуль, добавив файл с именем myrandom .py. В нем будут две функции: random _ ld () и random _ 2d (). Функция random _ ld () генерирует случайное число от 1 до 9, а random_2d() -от 10 до 99. Обратите внимание, этот модуль использует библиотеку random, которая является встроенным модулем Python. Фрагмент кода из файла myrandom.py: # myrandan.py с пользовательской и встроенной функциями random import raпdom def random_ld(): \"\"\"Эта функция генерирует случайное число от О до 9\"\"\" return random.randint (0,9) def random_2d(): \"\"\" Эта функция генерирует случайное число от 10 до 99\"\"\" return random.randint (10,99) Далее создадим главный скрипт calcmainl .ру, который импортирует модули, опи­ санные выше, и использует их для реализации функций калькулятора. Наиболее распространенный способ импортировать модули -оператор import. Фрагмент кода calcmainl . ру: # calanai.nl.py с функцией main() import mycalculator import myrandom def my_ main ( ) : \"\"\" Это главная функция, которая создает 2 случайных числа\\ и применяет к ним функции калькулятора \"\"\" х = myrandom.random_2d( ) у= myrandom.random_ld( ) sum = mycalculator.add(x, у) diff = mycalculator.subtract(x, у) Importing modules 35 priпt (\"х = {}, у = {}\". format (х, у)) print (\"sшn is {}\". format (sum)) print(\"diff is {}\".format(diff))\nГлава 2. Использование модулей для сложных проектов Фрагмент кода mycalculator .ру: # mycalculator.py с функциями add и subtract · def add(x, у): \"\"\"Эта функция складывает 2 числа\"\"\" return х + у def suЬtract(x, у): \"\"\"Эта функция вычисляет разность 2 чисел\"\"\" return х - у Имя модуля совпадает с именем файла. 47 Затем создадим второй модуль, добавив файл с именем myrandom .py. В нем будут две функции: random _ ld () и random _ 2d (). Функция random _ ld () генерирует случайное число от 1 до 9, а random_2d() -от 10 до 99. Обратите внимание, этот модуль использует библиотеку random, которая является встроенным модулем Python. Фрагмент кода из файла myrandom.py: # myrandan.py с пользовательской и встроенной функциями random import raпdom def random_ld(): \"\"\"Эта функция генерирует случайное число от О до 9\"\"\" return random.randint (0,9) def random_2d(): \"\"\" Эта функция генерирует случайное число от 10 до 99\"\"\" return random.randint (10,99) Далее создадим главный скрипт calcmainl .ру, который импортирует модули, опи­ санные выше, и использует их для реализации функций калькулятора. Наиболее распространенный способ импортировать модули -оператор import. Фрагмент кода calcmainl . ру: # calanai.nl.py с функцией main() import mycalculator import myrandom def my_ main ( ) : \"\"\" Это главная функция, которая создает 2 случайных числа\\ и применяет к ним функции калькулятора \"\"\" х = myrandom.random_2d( ) у= myrandom.random_ld( ) sum = mycalculator.add(x, у) diff = mycalculator.subtract(x, у) Importing modules 35 priпt (\"х = {}, у = {}\". format (х, у)) print (\"sшn is {}\". format (sum)) print(\"diff is {}\".format(diff))\n--- Страница 47 ---\n48 Раздел 1. Python nомимо основ Этот фраrмеи'1' ВilПопняется тот.хо в спучае, если специапън&Я переиенная\\ I nallle I УС'1'АИО8Jlеи& 8 JСАЧеС'l'Ве I'JIUKOЙ 111111 if name my_main () \" main \": С помощью оператора import мы импортируем оба модуля в главный скрипт. В ка­ честве основной определена функция my_main(}, которая будет выполняться при ус­ ловии, что модуль calcmainl выполняется в качестве главной программы. Детали выполнения основной функции из тела главного скрипта будут рассмотрены в раз­ деле «Установка параметров для специальных переменных». В функции my_main() мы создаем два случайных числа с помощью модуля myrandom, а затем вычисляем сумму и разность этих чисел с помощью модуля mycalculator. В конце выводим ре­ зультаты на консоль с помощью оператора print. ВАЖНОЕ ПРИМЕЧАНИЕ Модуль подгружается только один раз. Если он импортирован другим модулем или главным скриптом, то будет инициализирован в процессе выполнения кода. Если дру­ гой модуль в программе повторно импортирует тот же модуль, он не будет загружен дважды. Это означает, что если внутри модуля есть локальные переменные, они бу­ дут действовать как одиночки (Singleton), то есть инициализироваться однократно. Импортировать модуль можно и другими способами, например, с помощью importlib. import _ module () и встроенной функции _ import _ () . Рассмотрим, как рабо­ тает import и альтернативные варианты. Оператор import Как упоминалось ранее, import -распространенный способ подключить модуль. Пример использования import math Оператор отвечает за два действия: во-первых, ищет модуль, указанный после клю­ чевого слова import, во-вторых, привязывает результаты этого поиска к имени пе­ ременной (которое совпадает с именем модуля) в локальной области выполнения. В следующих двух подразделах мы поговорим, как работает этот оператор и как под­ гружать определенные элементы из модуля или пакета. Как работает оператор import Все глобальные переменные и функции добавляются в глобальное пространство имен в начале выполнения. Для примера напишем небольшую программу, которая выделяет содержимое пространства имен globals: # gloЬalmain.py с функцией globals() def print_globals(): print (globals())\n--- Страница 48 ---\nГлава 2·. Использование модулей для сложных проектов def hello(): print (11Hello11) if name == 11 main 11 • print _globals () 49 Программа содержит две функции: print_globals о и hello (). Первая выводит со­ держимое глобального пространства имен. А вторая выполняться не будет, она приводится здесь с целью показать ссылку на нее в консольном выводе. После вы­ полнения кода вывод консоли будет следующим: { name 11 : \" main \" doc \": \"None\", \" , \"__package _\": \"None11, 11 loader \":\"< frozen_importlib_external .\\ SourceFileLoader object at Oxl01670208>11, _spec_11: \"None\", \" annotations \":{ ), \" builtins \": \"<module 'builtins' (built-in) >11, Importing modules 37 11 file \":11/ PythonForGeeks/source_code/chapter2/\\ modules/globalmain .py11, 1 1_cached_\": 11None11, 11print_globals11:11<function print_globals at \\ Ох101бс4378>11, 11hello11 :11<function hello at Ох101бс4400>11 Ключевые моменты кода: ♦Переменной _ name _ присваивается значение _ main _; подробнее об этом мы поговорим в подразделе «Загрузка и инициализация модуля» . . ♦ Переменной _file_ задается путь к файлу главного модуля. ♦Ссылки на каждую функцию добавляются в конце. Если добавить print (globals ()) в скрипт calcmainl .ру, вывод консоли будет выгля­ деть следующим образом: { name 11 : 11 main11 , \" doc \": \"None\", 11 __package_11:11None11, 11 loader 11:11< frozen_importlib_exter nal.\\ SourceFileLoader object at Ox100del208>11 , _spec_11: 11None11, 11 annotations 11 :{), 11 builtins 11: 11<module 'builtins' (built-in) >1 1,\n--- Страница 49 ---\n50 \" file \":\"/PythonForGeeks/source_code/chapter2/modulel/ main.py\", 11 cached 11:11None11, 11mycalculator11:\"<module 'mycalculator' from \\ '/PythonForGeeks/source_code/chapter2/modules/\\ mycalculator.py'>\", Раздел 1. Python помимо основ 11myrandom\":11<module 'myrandom' from '/PythonForGeeks/source_ code/chapter2/modules/myrandom.py'>11, 11my_main\":11<function my_main at Ох100е351е0>\" Обратите внимание на две дополнительные переменные (mycalculator и myrandom), добавленные в глобальное пространство имен. При каждом подключении библио­ теки создается переменная с тем же именем, которая содержит ссылку на модуль, прямо как переменная для глобальных функций (в нашем примере это my_main). Можно заметить, что в других подходах к импорту модулей есть возможность явно определять переменные для каждого модуля. Оператор import делает все это авто­ матически за нас. Импорт отдельных частей модуля Мы можем подгрузить не весь модуль, а только его часть (например, переменную, функцию или класс). Для этого используется оператор from: from math import pi Еще одной рекомендуемой практикой является использование другого имени для подключаемого модуля. Это делается для удобства или в случае, когда использу­ ются одинаковые имена для разных ресурсов в двух разных библиотеках. Для де­ монстрации внесем изменения в файл calcmainl.py (обновленной программой будет calcmain2 .ру), используя сокращенные обозначения calc и rand для модулей mycalculator и myrandom соответственно. Это изменение значительно упростит ис­ пользование модулей в главном скрипте: # calaoain2.py с сокращенными обозначениями дпя модулей import mycalculator as calc import myrandom as rand def my_main(): 111111 Это главная функция, которая создает 2 случайных числа\\ и применяет к ним функции калькулятора х = rand.random_2d() у= rand.random_ld() sum = calc.add(x,y) diff = calc.suЬtract(x,y) print (11х = {}, у = {} 11 • format (х, у)) print(\"s\\ШI is {}11.format(sum)) print(11diff is {}11.format(diff))\n--- Страница 50 ---\nГлава 2. Использование модулей для сложных проектов Importing modules 39 Этот фрагмеН'1' выпаnияется тоm.11:0 в случае, еспи специа.пьиая nеремеиная\\ name ' установлена в nчестве главной\"\"\" if name == \" main \"· my_main () 51 На следующем шаге объединим две концепции, о которых говорилось ранее, в но­ вой версии программы (calcmainЗ.py). В этом обновлении мы используем оператор from с именами модулей, а затем импортируем отдельные функции из каждого мо­ дуля. В случае функций add () и suЬtract () применим оператор as для установки дру­ гого локального определения ресурса модуля. Фрагмент кода из файла calcmainЗ. ру: # calc:mainЗ.py с оператором from и сокращениями from mycalculator import add as my_add from mycalculator import suЬtract as my_suЬtract from myrandom import random_2d, random ld def my_main(): Это главная функция, которая создает 2 случайных числа\\ и применяет к ним функции калькулятора х= random _ 2d () у= random _ ld () sum = my_add(x,y). diff = my_subtract(x,y) print (\"х = {}, у = {}\". format (х, у)) print(\"sum is {}\".format(sum)) print (\"diff is {}\". format (diff)) priпt(globals()) Этот фрагмеН'1' выпаnияется тот.11:0 в случае, еспи специа.пьиая nеремеиная\\ name ' установлена в nчестве главной\"\"\" if name my_main () 11 main \"· Поскольку мы использовали print (globals ()), вывод программы на консоль пока­ жет, что переменные, соответствующие каждой функции, созданы согласно обо­ значениям: name \"·\" main 11 , \" doc \": \"None\", \"__package_\" : \"None\", \" loader \":\"< frozen_importlib_external.\\ SourceFileLoader object at Ox1095f1208>\", _spec_\" : \"None\", \" annotations \": { } , \" builtins \": \"<module 'builtins' (built-in) >\", \" file_\":\"/PythonForGeeks/source_code/chapter2/modulel/\n--- Страница 51 ---\n52 main_2.py11 , 11 cached 11 : 11None 11 , 11 my_add11 :11<function add at Ох10964540 0>11 , 1 1my_suЬtract11 :11<function suЬtract at Ох109645598>11, 11randan_2d11:11<function random_2d at Ох10967а840>11 , 11randan_ld11:11<function random_ld at Ох1096456а8>11 , 11my_main11 :1 1<function my_main at Ох109645378>11 Раздел 1. Python помимо основ Обратите внимание, переменные, вьщеленные жирным шрифтом, соответствуют изменениям оператора import в файле calcmainЗ. ру. Оператор _import_ Оператор _import_ представляет собой низкоуровневую функцию, которая прини­ мает строку в качестве входных данных и запускает операцию импорта. Такие функции являются частью ядра Python и предназначены для разработки библиотек или доступа к ресурсам операционной системы. Для создания приложений они обычно не используются. Это ключевое слово можно использовать для подключе­ ния библиотеки random в модуль myrandom.py: # импорт random random = _ import _ ( 'random' ) Остальной код в myrandom.py можно оставить без изменений. Этот метод приведен только для демонстрации, но вы можете самостоятельно изу­ чить все подробности. Такой способ не рекомендуется использовать для пользова­ тельских приложений, он разработан для интерпретаторов. Если нужен расширенный функционал, который не дает обычная инструкция import, существует оператор importlib. import _ module. Инструмент importlib.import_module Библиотека importlib дает возможность импортировать любые модули. Она предла­ гает разнообразие функций (включая _import_), связанных с более гибким под­ ключением модулей. Простой пример, как импортировать модуль random в myrandom.py с помощью importlib: import importlib random = importlib.import_module('random') Остальной код в myrandom. ру можно оставить без изменений. Модуль importlib хорошо известен благодаря динамическому импорту модулей. Он очень полезен, когда имя модуля заранее не известно и модули нужно подключать\n--- Страница 52 ---\nГлава 2. Использование модулей для сложных проектов 53 во время выполнения. Это распространенное требование при разработке плагинов и расширений. Наиболее популярны несколько следующих функций: ♦ _ import _: реализация функции _ import -У о которой мы уже говорили; ♦import_module: используется чаще всего для динамического подключения моду­ лей и позволяет указывать абсолютный или относительный путь к ним; является оберткой для importlib._import_; обратите внимание, что первая функция воз­ вращает пакет или модуль (например, packageA.modulel), указанный в функции, а вторая -пакет или модуль верхнего уровня (например, packageA); ♦importlib.util.find_spec: заменяет метод find_loader, устаревший с версии Python 3.4; используется для проверки, существует ли модуль и является ли он дейст­ вующим; ♦invalidate caches: используется для очистки внутреннего кэша искателей (finder), хранящихся в sys.meta_path; внутренний кэш позволяет быстрее загру­ жать модули без повторного запуска искателей; но если мы импортируем мо­ дуль динамически, особенно если он создан после запуска интерпретатора, хо­ рошей практикой будет применить метод invalidate_caches; эта функция удалит из кэша все модули или библиотеки, и можно быть уверенным, что запрошен­ ный модуль загружен из системного пути методом import; ♦reloact: повторно загружает ранее подключенный модуль; в качестве входного параметра этой функции необходимо предоставить объект модуля; это означает, что import должен быть выполнен успешно; функция очень полезна в ситуациях, когда исходный код модуля был изменен и нужно загрузить новую версию без .перезапуска программы. Абсолютный и относительный импорт Абсолютный и относительный импорт особенно важен, когда есть необходимость подключения настраиваемых и проектно-специфичных модулей. Для демонстрации этих двух концептов возьмем проект с различными пакетами, подпакетами и моду­ лями: project 1-pkql 1 1-modulel. ру 1 L-module2 . ру ( содержит фунJЩИI) funcl () ) L- pkq2 1-_init_.py 1-moduleЗ. ру L-suЬ_ykql L-moduleб.py (содержит фунJЩИI) func2 ())\n--- Страница 53 ---\n54 f-pkqЗ 1 f-module4. ру 1 f-module5. ру L- suЬykq2 L-module7. руРаздел 1. Python помимо основ Используя эту структуру, обсудим, как использовать абсолютный и относительный импорт. Абсолютный импорт Абсолютный путь можно указать, начав с пакета верхнего уровня и двигаясь вниз по структуре. Несколько примеров того, как можно импортировать модули: from pkgl import modulel from pkgl.module2 import funcl from pkg2 import moduleЗ from pkg2.suЬ_pkgl.module6 import func2 from pkgЗ import module4, moduleS from pkg3.suЬ_pkg2 import module7 При абсолютном импорте необходимо указать подробный путь к каждому пакету или файлу, начиная с пакета верхнего уровня. Это аналогично пути к файлу. Такой способ наиболее предпочтителен, поскольку он легко читается и помогает отслеживать точное местоположение подключаемых ресурсов. На абсолютный им­ порт меньше всего влияют совместное использование проекта и изменения в теку­ щем расположении операторов import. Фактически РЕР 8 рекомендует использовать именно этот тип подключения. Однако иногда абсолютный импорт представляет собой довольно длинные опера­ торы, зависящие от размера структуры папок в проекте. Относительный импорт При относительном импорте указывается подключаемый ресурс относительно те­ кущего расположения, а именно, расположения файла с кодом, где находится import. Возвращаясь к примерам, рассмотрим несколько сценариев относительного импор­ та. Эквивалентные операторы выглядят следующим образом: ♦Сценарий 1: импорт funcl в modulel.py: from .module2 import funcl ♦Мы поставили одну точку(.) только потому, что module2.py находится в той же папке, что и modulel.py. ♦Сценарий 2: импорт module4 в modulel .ру: from pkgЗ import module4\n--- Страница 54 ---\nГлава 2. Использование модулей для сложных проектов 55 ♦ В этом случае мы поставили две точки ( ), поскольку module4. ру находится в родственной с modulel. ру папке. ♦Сц�нарий 3: импорт func2 в modulel .ру: from pkg2.sub_pkg_l.module6 import func2 Здесь мы поставили две точки( ), поскольку целевой модуль (moduleб.py) нахо­ дится внутри папки, которая расположена в родственном каталоге для modulel. ру; мы использовали одну точку для доступа к пакету sub _pkgl, а дру­ гую -для доступа к moduleб. Одним из преимуществ относительного импорта является значительное сокращение объемов длинных строк кода. Но эти операторы могут быть запутанными и сложны­ ми в обслуживании при совместном использовании проектов между командами и организациями. Относительный импорт нелегко читать, и им сложно управлять. Загрузка и инициализация модуля Всякий раз при взаимодействии интерпретатора Python с import или эквивалентны­ ми инструкциями он выполняет три операции, описанные далее. Загрузка модуля На этом шаге интерпретатор ищет указанный модуль в sys .path (подробнее - в подразделе <<Доступ к пакетам из любого расположения») и загружает исходный код. Об этом мы уже говорили в подразделе «Как работает оператор import». Установка параметров для специальных переменных На этом шаге интерпретатор определяет несколько специальных переменных, на­ пример, _ name _) которая определяет пространство имен для модуля Python. Это одна из самых важных переменных. В цримере с модулями calcmainl .ру, mycalculator .ру и myrandom.py переменная _ name _ будет задана для каждого модуля следующи� образом: Таблица 2.1. Значение атрибута _nаmе_для разных модулей Имя модуля Значение -- main.py -mаш - myrandom.py myrandom mycalculator.py mycalculator Существуют два способа задания переменной _name _, которые описаны далее.\n--- Страница 55 ---\n56 Раздел 1. Python помимо основ Случай А: модуль является главной программой Если модуль запускается в качестве основной программы, переменной _ name _ бу­ дет установлено значение _main _ независимо от имени файла или модуля Python. Например, при выполнении calcmainl .ру интерпретатор назначает жестко заданную строку _ main _ переменной _ name _. Если в качестве основной программы запус­ каются myrandom.py или mycalculator.py, переменная _name_ автоматически получит значение main Поэтому добавим строку if _ name _ == '_main _' во все главные скрипты для про­ верки, является ли программа основной для вьшолнения. Случай Б: модуль импортируется другим модулем В этом случае модуль не является основной программой, а импортируется другим модулем. В примере myrandom и mycalculator импортируются в calcmainl .ру. Как толь­ ко интерпретатор Python находит файлы myrandom.py и mycalculator .ру, он назначает имена myrandom и mycalculator из оператора import переменной _name_ для каждо­ го модуля. Назначение происходит до выполнения кода внутри этих модулей. Это показано в таблице 2.1. Есть еще несколько специальных переменных, которые заслуживают внимания: ♦_file_: содержит путь к импортируемому в данный момент модулю; ♦ _ doc _: выводит docstring, добавляемую в класс или метод; как уже упоминалось в предыдущей главе, docstring -это строка комментария, добавляемая сразу после определения класса или метода; ♦_package _: используется для указания, является ли модуль пакетом или нет; в качестве значения могут быть имя пакета, пустая строка или попе; ♦_dict_: возвращает все атрибуты экземпляра класса в виде словаря; ♦dir: возвращает все связанные методы или атрибуты в виде списка; ♦locals и globals: используются как методы для отображения локальных и гло­ бальных переменных в виде записей словаря. Выполнение кода После установки специальных переменных интерпретатор Python выполняет. код в файле построчно. Важно понимать, что функции (и код в классах) не выполняются, пока они не вызываются другими строками кода. Приведем краткий анализ с точки зрения выполнения при запуске calcmainl .ру: ♦mycalculator .ру: после установки специальных переменных в этом М?дуле нет кода, который должен выполняться при инициализации; ♦myrandom.py: после установки специальных переменных и оператора import в этом модуле отсутствует дальнейшее выполнение кода во время инициализации;\n--- Страница 56 ---\nГлава 2. Использование модулей для сложных проекrов 57 ♦calcmainl . ру: после установки специальных переменных и выполнения операто­ ров import выполняется следующий оператор: if name == \"_main_\"; это выраже­ ние вернет значение true, поскольку мы запустили файл calcmainl . ру; внутри if будет вызвана функция my _ main о, которая затем вызовет методы из модулей myrandom.py И mycalculator .ру. Строку if _name_ == \"_main_\" можно добавить в любой модуль, независимо от того, является ли он основной программой или нет. Преимущество такого подхода заключается в возможности использовать модуль, в том числе и как основную про­ грамму. Существует также другое применение этого способа, которое заключается в добавлении модульных тестов в модуль. Стандартные модули Python имеет встроенную библиотеку с более 200 стандартных модулей. Точное количество варьируется от дистрибутива к дистрибутиву. Все их можно импорти­ ровать в программу . Список модулей очень обширен, но в качестве примера рас­ смотрим наиболее часто используемые: ♦math: предоставляет математические функции для арифметических операций; ♦random: полезен для генерации псевдослучайных чисел с использованием различ­ ных типов распределений; ♦statistics: предлагает статистические функции, например, среднее значение (mean), медиана (median) и дисперсия (variance); ♦Ьаsеб4: предоставляет функции для кодирования и деко-дирования данных; ♦calendar: здесь расположены функции, связанные с календарем, что очень удоб­ но при использовании календарных расчетов; ♦collections: содержит специализированные типы данных для контейнеров, от­ личные от встроенных (dict, list, или set); такие типы данных включают в себя deque, Counter И ChainМap; ♦csv: служит для чтения из СSV-файлов и записи в них; ♦datetime: предлагает универсальные функции для работы с датой и временем; ♦decimal: предлагает функции с десятичными арифметическими операциями; ♦ logging: упрощает регистрацию и вход в приложение; ♦os и os.path: дают доступ к системным функциям; ♦socket: содержит низкоуровневые функции для сетевых коммуникаций через со­ кеты; ♦sys: предоставляет доступ к интерпретатору Python для низкоуровневых пере­ менных и функций; ♦time: предоставляет функции для расчетов со временем, например, для преобра­ зования одних единиц времени в другие.\n--- Страница 57 ---\n58 Раздел 1. Python помимо основ Написание многоразовых модулей Модуль можно объявить многоразовым, когда он обладает следующими характери­ стиками: ♦Независимая функциональность. ♦Универсальная функциональность. ♦Традиционный стиль программирования. ♦Четко определенная документация. Если модуль или пакет не обладает этими характеристиками, использовать его в других программах будет сложно или даже невозможно. Рассмотрим каждую ха­ рактеристику отдельно. Независимая функциональность Функции в модуле должны работать независимо от других модулей, а также ло­ кальных или глобальных переменных. Чем более независимы функции, тем больше возможностей повторного использования модулей. Если есть зависимость от дру­ гих модулей, она должна быть минимальной. Например, в модуле mycalculator. ру обе функции независимы друг от друга и могут использоваться в других программах (рис. 2.1): mycalculator.py -EiHl·M Рис. 2.1. Модуль mycalculator с функциями Add и Subtract В модуле myrandom.py используется системная библиотека random для генерации слу­ чайных чисел. Этот модуль можно легко использовать снова, поскольку библиотека является встроенной в Python (рис. 2.2): myrandom.py Рис. 2.2. Модуль myrandom с зависимостью от библиотеки random\n--- Страница 58 ---\nГлава 2. Использование модулей для сложных проектов 59 Модуль со сторонними библиотеками будет работать в нашем окружении, но при работе с другими модулями могут возникнуть проблемы, если в их окружении не подключены те же библиотеки, что и в нашем. Убедимся в этом на примере. Создадим новый модуль mypandas.py, который будет использовать базовый функционал библиотеки pandas. Для простоты добавим в него только одну функцию, которая выводит DataFrame в соответствии со словарем, предоставленным в качестве входной переменной для этой функции. Фрагмент кода mypandas. ру: lmypandas.py import pandas def print_dataframe(dict): \"\"\" Эта функция выводит словарь в виде фрейма данных \" \"\" brics = pandas.DataFrame(dict) print(brics) Модуль mypandas.py будет использовать библиотеку pandas для создания объекта dataframe из словаря. Эта зависимость также показана на следующей схеме (рис. 2.3): mypandas.py Рис. 2.3. Модуль mypandas с зависимостью от сторонней библиотеки pandas Обратите внимание, библиотека pandas не является встроенной или системной. Если попробовать поделиться нашим модулем с другими без определения явной зависи­ мости от сторонней библиотеки (в нашем случае pandas), программа при попытке использовать этот модуль выдаст следующее сообщение об ошибке: ImportError: No module named pandas' Вот почему важно сделать модуль максимально независимым. Если приходится использовать сторонние библиотеки, необходимо определить четкие зависимости и использовать соответсrвующий подход к упаковке их в пакет. Об этом мы погово­ рим в подразделе «Общий доступ к пакету». Генерализация функционала В идеале многоразовый модуль должен фокусироваться на решении общей про­ блемы, а не какой-то конкретной. Допустим, мы хотим переводить дюймы в санти-\n--- Страница 59 ---\n60 Раздел 1. Python помимо основ метры. Для этого можно написать функцию на основе формулы преобразования. А как насчет функции, которая переводит любые значения из имперской системы в метрическую? У нас может быть одна функция для всех преобразований (дюймы в сантиметры, футы в метры, мили в километры) или разные функции для каждого типа преобразования. А что насчет обратных преобразований (сантиметры в футы)? Возможно, сейчас в этом нет необходимости, но это может понадобиться позже нам или кому-то, кто также использует этот модуль. Такая генерализация сделает функционал модуля не только исчерпывающим, но и подходящим для повторного использования без расширения его функционала. Для демонстрации принципам изменим модуль myrandom и сделаем его более общим и, следовательно, более пригодным для повторного использования. Сейчас у нас определены разные функции для однозначных и двузначных чисел. Что если мы захотим сгенерировать трехзначное случайное число или число от 20 до 30? Для генерализации этого требования введем в модуль новую функцию get_random, кото­ рая принимает пользовательский ввод для верхнего и нижнего пределов для слу­ чайных чисел. Эта функция будет обобщением для двух других, которые мы опре­ делили ранее. Теперь их можно удалить или оставить в модуле для удобства ис­ пользования. Мы создали новую функцию только для сравнения обобщенной функции (get_random ) с отдельными функциями (random_ld и random_2d ). Обновленная версия модуля myrandom.py (myrandomv2 .ру) выглядит так: # myrandanv2.py со стандартной и пользовательско й функциями random import random def random _ ld () : \"\"\"Эта функция генерирует случайное число между О and 9\"\"\" return random.randint(0,9) def random_2d(): \"\"\"Эта функция генерирует случайное число между 10 and 99'\"\"' return random.randint(l0,99) def get_random(lower, upper): \"\"\"Эта функция генерирует случайное число между нижним\\ и верхним пределами return random.randint(lower,upper) Традиционный стиль программирования В первую очередь это касается того, как мы пишем имена функций, переменных и модулей. В Python существует система написания кода и соглашения об именова­ нии, которые обсуждались в предыдущей главе. Важно следовать этим соглашени­ ям, особенно при создании многоразовых модулей и пакетов. В противном случае, их будет очень неудобно использовать снова.\n--- Страница 60 ---\nГлава 2. Использование модулей для сложных проектов 61 Для наглядности рассмотрим следующий фрагмент кода, где для имен функций и параметров используется верблюжий регистр: def addNumЬers(nurnParaml, nurnPararn2) #код функции пропущен def featureCount(moduleName) # код функции пропущен Если вы привыкли работать с Java, этот стиль написания вам подойдет. Но в Python это считается плохой практикой. Использование стиля, не принятого в Python, за­ трудняет повторное использование таких модулей. Ниже приведен еще один фрагмент с уже верным стилем написания имен функций: def add_numЬers(nurn_paraml, nurn_pararn2) # код функции пропущен Def feature_count(module_name) # код функции пропущен Другой пример хорошего многоразового стиля программирования приводится на скриншоте из PyChaпn IDE для библиотеки pandas (рис. 2.4): 8 test ( extl'a_ar'gs ) • tseries, ( pand.,s.) .ar_t,1 end�. p�r�O'd­pandas.utit._tester �rors, da�fi:rst, yeJ 8 to_nuмric( ears, e11/.'ot's, do'wncast) _p,aildas-.core. index:es. tinieclettas pandas. core. too ts·. dateti11es pandas.core.toots.nu11el'it 8 to_pickte(obj, fi.•t�path_or_buffer, 8 to_;tiмdetta(lar-s, uni't, errors) _tstib tOtlJ)ress'i'on, -pandas.io.pickt• pan�as.core.toots.timedettas pandas 8 read_sq't_taЫe(taЫe_neм, coil, sch1м, index_c ,ot, _ pandas.io.sqt 8 read_taЫe pandas. io. parsers РП1!88. л. t9,'choose the ••�� l« flrst) q_g•tlon and·(!'lert, а dot afte11Nard1 Next Тlр Рис. 2.4. Библиотека pandas в PyChaпn IDE Имена функций и переменных легко понять даже без чтения какой-либо докумен­ тации. Соблюдение стандартного стиля программирования делает код более удоб­ ным для многократного использования. Четко определенная документация Четко определенная и понятная документация так же важна, как и написание уни­ версального и независимого модуля с соблюдением всех рекомендаций. Без про­ зрачной документации модуль не вызовет интерес разработчиков к его повторному использованию. Как программисты, мы уделяем больше внимания написанию кода.\n--- Страница 61 ---\n62 Раздел 1. Python nомимо основ Но написание всего нескольких строк документации может сделать 100 строк на­ шего кода более удобными в использовании и сопровождении. Рассмотрим несколько хороших примеров документирования модуля на примере mycalcula tor. ру: \"\"\" mycalculator.py Этот модуль предоставляет функции для сложения и вычитания 2 чисел\"\"\" def add (х, у) : \"\"\" Эта функция складывает 2 числа. пример: add (3, 4) return х + у def suЬtract(x, у): \"\"\" Эта функция находит разность 2 чисел пример: subtract (17, 8) \"\"\" return х - у В Python важно помнить следующее: ♦Можно использовать тройные кавычки для многострочных комментариев. ♦Строки в тройных кавычках указываются в начале модуля, а затем используются в качестве документации. ♦Если функция начинается с комментария в тройных кавычках, этот комментарий используется как документация для этой функции. Можно написать сколько угодно модулей с сотнями строк кода. Но требуется нечто большее, чем простое программирование, для возможности многократно их ис­ пользовать. Например, генерализация, стиль написания кода и, самое главное, до­ кументация. Сборка пакетов Существует ряд методов и инструментов для создания и распространения пакетов. Python не может похвастаться богатой историей стандартизации по их созданию. В первое десятилетие XXI века было много проектов по оптимизации этого процесса, но большого успеха они не принесли. За последние годы удалось добиться опреде­ ленного успеха благодаря усилиям рабочей группы Python Packaging Authority (РуРА). В этом подразделе мы рассмотрим методы создания пакетов, доступ к ним в про­ грамме, а также публикацию и совместное использование в соответствии с реко­ мендациями РуРА. Начнем с именования пакетов, рассмотрим файл инициализации, а затем попробу­ ем собрать пакет.\n--- Страница 62 ---\nГлава 2. Использование модулей для сложных проектов 63 Именование Имена пакетов должны следовать тому же правилу, что и модули, -нижний ре­ гистр и без подчеркиваний. Пакеты действуют как структурированные модули. Файл инициализации пакета Пакет может иметь опциональный исходный файл с именем _init_.py (или просто файл init). Его наличие (даже пустого) рекомендуется для пометки папок как паке­ тов. Начиная с версии Python 3.3 использовать файл необязательно (РЕР 420: неяв­ ные пакеты пространства и.мен). Файл инициализации имеет многоцелевое назна­ чение. До сих пор нет единого мнения, что можно указывать в нем, а что -нет. Вот несколько примеров использования: ♦Пустой _init_.py: разработчикам придется использовать явный импорт и управлять пространствами имен по своему усмотрению; они будут вынуждены импортировать отдельные модули, что может быть неудобно для большого па­ кета. ♦. Полный импорт в _init_.py: в этом случае можно импортировать весь пакет, а потом ссылаться на модули напрямую в коде по имени или сокращенному обо­ значению пакета; это удобнее, но лишь за счет поддержки всех импортов в фай­ ле init . ♦Ограниченный импорт: при таком подходе разработчики могут импортировать в ini t только ключевые функции из разных модулей и управлять ими в про­ странстве имен пакета; это дает дополнительное преимущество, заключающееся в предоставлении оболочки вокруг функциональных возможностей базового модуля; если по какой-то причине придется проводить рефакторинг базовых мо­ дулей, есть возможность сохранить пространство имен прежним, особенно для потребителей API; единственным недостатком такого подхода являются допол­ нительные усилия по управлению и поддержке такими файлами. Иногда разработчики добавляют в файл инициализации код, который выполняется при импорте модуля из пакета. Примером этого служит создание сеансов для уда­ ленных систем, таких, как базы данных или удаленный SSH-cepвep. Сборка пакета Рассмотрим, как собрать пакет, на простом примере. Создадим пакет masifutil, ис­ пользуя следующие модули и подпакет: ♦Модуль mycalculator .ру: его мы уже создали в подразделе «Импорт модулей». ♦Модуль myrandom.py: его мы также уже создали в подразделе «Импорт модулей». ♦Подпакет advcalc: будет содержать один модуль advcalculator .ру; для этого под­ пакета определим файл init, но оставим его пустым.\n--- Страница 63 ---\n64 Раздел 1. Python помимо основ Модуль advcalculator .ру предоставляет дополнительные функции для вычисления квадратного корня и логарифма по основаниям 1 О и 2. Исходный код модуля сле­ дующий: # advcalculator.py с функциями sqrt, log и ln import math def sqrt (х) : \"\"\"Эта функция вычисляет квадратный корень числа\"\"\" return math.sqrt(x) def log(x): \"\"\"Эта функция возвращает логарифм по основанию 10\"\"\" return math.log(x,10) def ln (х): \"\"\" Эта функция возвращает логарифм по основанию 2\"\"\" return math.log(x,2) Файловая структура пакета masifutil с файлами инициализации выглядит следую­ щим образом (рис. 2.5): mypackages masifutil advcalc � _init_.py � advcalculator.py Gi _init_.py � mycalculator.py G1 myrandom.py � pkgmainO.py � pkgmain1.py � pkgmain2.py Рис. 2.5. Структура папок в пакете masifutil с модулями и подпакетами Затем создадим новый скрипт (pkgmainl .ру), который будет использовать модули из пакета или из подпапки masifutil. В этом скрипте мы импортируем модули из ос­ новного пакета и подпакета, используя структуру папок. А затем с помощью функ­ ций модуля возьмем два случайных числа, вычислим их сумму и разность, а также квадратный корень и логарифмы первого случайного числа. Исходный код pkgmainl . ру выглядит так: f pkgmainO.py с прямым импортом import masifutil.mycalculator as calc import masifutil.myrandan as rand import masifutil.advcalc.advcalculator as acalc def my _ main () : \"\"\" Это основная функция, которая созда�т два случайных\\ числа и затем применяет к ним функции калькулятора \"\"\" х = rand.random_2d() у = rand.random_ld()\n--- Страница 64 ---\nГлава 2. Использование модулей для сложных проектов surn = calc.add(x,y) diff = calc.suЬtract(x,y) sroot = acalc.sqrt(x) loglOx = acalc.log(x) log2x = acalc.ln(x) print (\"х = {}, у = {}\". format (х, у)) print(\"sl.ШI is {}\".format(surn)) print (\"diff is {}\". format (diff)) print(\"square root is {}\".format(sroot)) print (\"loq Ьаsе of 10 is {}\". format (loglOx)) pririt (\"loq Ьаsе of 2 is {}\". format (log2x)) ВЫпо.11JОUt1rся тот.хо, еспи nереиеиная к.ах основкая \"\"\" if name my_main () \" ma.in \"· --уС'l'аио:вnеиа 65 Здесь мы используем имена пакета и модуля для импорта. Это неудобно, особенно если мы импортируем также подпакеты. Можно получить те же результаты, ис­ пользуя следующие инструкции: t mypkgmainl.py с операторами from from masifutil import mycalculator as calc from masifutil import myrandom as rand from masifutil.advcalc import advcalculator as acalc #остальной код такой же, как в mypkgmainl.py Как уже говорилось, создавать пустой файл _init_.py необязательно. Но мы доба­ вили его для наглядности. Далее рассмотрим, как добавить несколько операторов import в файл инициализа­ ции. Начнем с импорта модулей. В файле ini t верхнего ранга импортируем все функции, как показано ниже: #файл init для пакета 'masifutil' from .mycalculator import add, suЬtract from .myrandom import random_ld, random_2d frdm .advcalc.advcalculator import sqrt, log, ln Обратите внимание на использование точки (.) перед именем модуля. В Python это обязательно для относительного импорта. Благодаря этим трем строкам в файле init, основной скрипт будет выглядеть про­ ще: # pkgmain2.py с основной функцией import masifutil\n--- Страница 65 ---\n66 Раздел 1. Python помимо основ def my_main(): '\"\"' Это основная функция, которая создает два случайных\\ числа и затем применяет к ним функции калькулятора х = masifutil.random_2d() у= masifutil.random_ld() sum = masifutil.add(x,y) diff = masifutil.subtract(x,y) sroot = masifutil.sqrt(x) loglOx = masifutil.log(x) log2x = masifutil.ln(x) print(\"x ={},у= {}\".format(x, у)) print(\"sum is {}\".format(sum)) print (\"diff is {}\" .format (diff)) print (\"square root is {}\". format (sroot)) print(\"log Ьаsе of 10 is {}\".format(loglOx)) print(\"log base of 2 is {}\".format(log2x)) ВЬmОJIНЯется только, если переменная ка.х основная 111111 if name my_main () \" main \"· name установлена Функции из двух главных модулей и модуля в подпакете доступны на уровне глав­ ного пакета. Разработчикам не придется разбираться в иерархии и структуре моду­ лей. Все это благодаря инструкциям import в файле ini t. Пакет собирается таким образом, что исходный код пакета находится в той же пап­ ке, что и главная программа или скрипт. Это работает только для совместного ис­ пользования модулей в рамках одного проекта. Но что если необходимо использо­ вать пакет в других проектах или программах? Доступ к пакетам из любого расположения Пакет, созданный ранее, доступен только в случае, если программа, вызывающая модули, находится на том же уровне, что и расположение пакета. Такое условие нецелесообразно для повторного использования кода. В этом подразделе мы обсудим, как сделать пакеты доступными и пригодными для использования в любом месте системы. Добавление в sys.path Это полезная опция для динамической настройки sys .path. Обратите внимание, sys.path -это список каталогов, в которых интерпретатор Python делает поиск ка-\n--- Страница 66 ---\nГлава 2. Использование модулей для сложных проектов 67 ждый раз, когда выполняется оператор import в исходной программе. С помощью этого подхода мы присоединяем (добавляем) пути каталогов или папок с нашими пакетами к sys. path. Для пакета masifutil мы создадим новую программу pkgrnainЗ .ру, которая является копией pkgrnain2 .ру (будет обновлена позже), но хранится вне папки с пакетом masifutil . Программа pkgrnainЗ.py разместится в любой папке, кроме mypackages. Структура папок с новым основным скриптом (pkgrnainЗ.p y) и пакетом masifutil (рис. 2.6): v mypackage_mains � pkgmainЗ.py т ra mypackages , ;;i masifutil .,.,, ;.:;; advcalc � _init_.py t,;j advcalculator.py � _init_.py t;i mycalculator.py ta myrandom.py Рис. 2.6. Структура папок пакета masifutil и новый основной скрипт pkgmainЗ.py При выполнении pkgrnainЗ. ру программа выдаст ошибку: ModuleNotFounc!Error: No module named 'masifutil'. Это ожидаемо, поскольку путь к пакету masifutil не добав­ лен в sys. path. Для добавления папки пакета в sys. path отредактируем основную программу. Назовем ее pkgrnain4.py и дополним инструкциями по добавлению в sys.path: # pkgmain4.py с кодом добавления в sys.path import sys sys.path .append('/Users/muasif/Google Drive/PythonForGeeks/ source _ code/ chapter2 /mypackaqe s ' ) import masifutil def my_main(): \"\"\" Это основная функция, которая создает два случайных\\ числа и затем применяет к ним функции калькулятора х = masifutil.random_2d() у= masifutil.random_ld() sum = masifutil.add(x,y) diff = masifutil.subtract(x,y) sroot = masifutil.sqrt(x} loglOx = masifutil.log(x) log2x = masifutil.ln(x)\n--- Страница 67 ---\n68 11 1111 print (\"х = {}, у = {}\". format (х, у)) print(\"sum is {}\".format(sum)) print(\"diff is {}\".format(diff)) print(\"square root is {}\".format(sroot)) print (\"log Ьаsе of 10 is {}\". format (loglOx)) print (\"log Ьаsе of 2 is {}\". format (log2x)) Выпо.пияется только, еспи переиеиная' name хах основная ,, ,, ,, if name my_main () \" main \"· Раздел 1. Python помимо основ 'YC'l'&ИOIIJieиA После добавления строк основной скрипт выполнился без ошибок и с ожидаемым консольным выводом. Это связано с тем, что пакет masifutil теперь доступен по пути, по которому интерпретатор Python может загрузить его при импорте в основ­ ной скрипт. Альтернативой sys.path может служить функция site.addsitedir из модуля site. Единственное преимущество этого подхода в том, что функция также ищет файлы . pth в указанных папках. Это полезно для добавлении дополнительных директорий (например, подпакетов). Фрагмент основного скрипта (pktpaminS.py) с функцией addsi tedir: # pkgmainS. ру import site site.addsitedir('/Users/muasif/Google Drive/PythonForGeeks/ source_code/chapter2/mypackages') import masifutil #остальной код без изменений как в pkymain4.py Обратите внимание, при таком подходе каталоги доступны только при выполнении программы. Для использования sys.path на постоянной основе (на уровне сеанса или системы) есть другие более полезные способы. Переменная среды PYTHONPA ТН Это удобный способ добавить папку с пакетом в sys.path, который интерпретатор Python будет использовать для поиска пакетов и модулей, если они отсутствуют во встроенной библиотеке. Посмотрим, как можно определить эти переменные в зави­ симости от операционной системы. В Windows эту переменную среды можно определить следующими способами: ♦Командная строка: РУТНОNРАТН = \"C:\\pythonpathl;C:\\pythonpath2\"; это хороший способ для активного сеанса. ♦Графический пользовательский интерфейс: выполняем переход по меню Мой компьютер > Свойства системы > Дополнительные параметры систе­ мы > Переменные среды; это постоянная настройка.\n--- Страница 68 ---\nГлава ·2. Использование модулей для сложных проектов 69 В Linux и macOS это можно сделать с помощью команды export РУТНОNРАТН= '/ some/path/'. Если используется Bash или аналогичный термин.ал, переменная сре­ ды будет эффективна только для терминального сеанса. Для перманентной уста­ новки переменной среды рекомендуется добавить ее в конец файла профиля: ~/bash _profile. Если попробовать выполнить pkgmainЗ. ру без установки PYTHONPATH, интерпретатор вернет ошибку ModuleNotFoundError: No module named 'masifutil '. Это также ожидае­ мо, поскольку путь к пакету masifutil не добавлен в PYTHONPATH. На следующем шаге добавим путь к папке с пакетом masifutil в переменную РУТНОNРАтн и перезапустим pkgmainЗ .py. Теперь программа работает без ошибок и с ожидаемым консольным выводом. Использование .рth-файла в пакете site Это еще один удобный способ добавления пакетов в sys. path. Достигается опреде­ лением файла .pth в пакетах site. Файл может содержать все папки, которые надо добавить в sys.path. Рис. 2.7. Файловая структура с файлом my.pth\n--- Страница 69 ---\n70 Раздел 1. Python помимо основ Для демонстрации мы создали файл my.pth в каталоге venv/liЬ/PythonЗ. 7/site­ packages. Как видно на рис. 2.7, мы добавили папку, которая содержит пакет masifutil. С этим простым файлом .pth основной скрипт pkymainЗ.py отлично рабо­ тает без ошибок и с ожидаемым консольным выводом. Рассмотренные методы доступа к пользовательским пакетам эффективны для по­ вторного использования в одной системе с любой программой. Далее мы рассмот­ рим, как делиться пакетами с другими разработчиками и сообществами. Общий доступ к пакету Существует много инструментов для общего доступа к пакетам и проектам Python. Сосредоточимся на тех, которые рекомендованы группой РуРА. Инструменты, о которых следует упомянуть: ♦distutils: поставляется с базовой версией Python, но плохо масштабируется для сложных и нестандартных пакетов; ♦setuputils: сторонний инструмент и расширение для distutils; рекомендуется для сборки пакетов; ♦wheel: формат распространения пакетов Python, который делает установку бы­ стрее и проще по сравнению с двумя предыдущими; ♦pip: менеджер пакетов и модулей Python, который поставляется, начиная с вер­ сии 3.4; легко позволяет устанавливать модули командой: pip install <имя_ модуля>; ♦Python Package Index (PyPI): репозиторий программного обеспечения (ПО) для языка Python; используется для поиска и установки ПО, которое разрабатывает­ ся и распространяется сообществом Python; ♦Twine: утилита для публикации пакетов Python в PyPI. Далее добавим в пакет masifutil дополнительные компоненты в соответствии с ре­ комендациями РуР А. Затем установим обновленный пакет masifutil для всей сис­ темы с помощью pip. В конце опубликуем его в Test PyPI, а затем установим его из Test PyPI. Создание пакета в соответствии с рекомендациями РуРА РуРА рекомендует использовать пример проекта для сборки многоразовых пакетов, который доступен по ссылке https://github.com/pypa/sampleproject.\n--- Страница 70 ---\nГлава 2. Использование модулей для сложных проектов 71 Фрагмент кода из примера проекта на GitHub (рис. 2.8): Рис. 2.8. Пример проекта от РуРА на GitHub Для начала рассмотрим ключевые файлы и папки, прежде чем использовать их для обновления пакета masifutil: ♦setup.py: самый важный файл, который должен быть в корне проекта или пакета; это скрипт для сборки и установки пакета, который содержит глобальную функ­ цию setup () ;файл также предоставляет интерфейс командной строки (Com­ mand Line Interface, CLI) для выполнения различных команд; ♦setup.cfg: это ini-файл, который может использоваться файлом setup.py для определения значений по умолчанию; ♦ключевые аргументы, которые можно передать в функцию setup (): •Name (имя); •Version (версия); •Description (описание); • URL (URL-aдpec ); •Author (автор); •License (лицензия).\n--- Страница 71 ---\n72 Раздел 1. Python помимо основ ♦READМE.rst/READМE.md: этот файл (в формате reStructured или Markdown) может содержать информацию о пакете или проекте; ♦license. txt: файл содержит подробные условия распространения и должен быть включен в каждый пакет; файл лицензии особенно важен в странах, где распро­ странять пакеты без соответствующей лицензии незаконно; ♦МANIFEST. in: с помощью него можно указать список дополнительных файлов, ко­ торые должны быть включены в пакет; этот список не включает файлы с исход­ ным кодом (они добавляются автоматически); ♦ <имя_ пакета>: это пакет верхнего уровня, который включает в себя все моду- ли и подпакеты; его использование не обязательно, но рекомендовано; ♦data: здесь можно добавить файлы с данными, если необходимо; ♦tests: заглушки для будущих модульных тестов. На этом шаге обновим предыдущий пакет masifutil в соответствии с рекоменда­ циями РуРА. Новая структура папок и файлов обновленного пакета masifutilv2 вы­ глядит так (рис. 2.9): � masifutilv2 r data i;,-dist masifutil advcalc t';:i _jnit_.py � mycalculator.py � myrandom.py р masifutilv2.egg-info >tests 111 LICENSE.txt 11 MANIFEST.in 11 README.md 11 setup.cfg � setup.py Рис. 2.9. Обновленная файловая структура masifutilv2 Мы добавили каталоги data и tests, но они пока пустые. О модульных тестах мы поговорим в одной из следующих глав. Содержимое большинства дополнительных файлов описывается в примере проекта, поэтому здесь их обсуждение мы пропустим, за исключением файла setup. ру. В setup.py указаны основные аргументы для проекта. Остальные аргументы дос­ тупны в примере файла setup.py, который предоставлен группой РуРА вместе с примером проекта. Фрагмент кода из setup.py представлен ниже: from setuptools import setup setup ( name='masifutilv2',\n--- Страница 72 ---\nГлава 2. Использование модулей для сложных проектов version='0.1.0', author= 'Мuhaпlnad Asif' , author_email ='ma@exaпple.can', packages= [ 'masifutil' , 'masifutil/advcalc' ], python_requires ='>=З.5, <4', url='http://pypi.python.org/pypi/P acka.qeName/', license='LICENSE.txt', descripti on='A sanple packa.qe for illustration purposes', long_description =open('READМE.md') .read(), install_requires =[ ], 73 С помощью setup.py теперь можно делиться пакетом masifutilv2 как локально, так и удаленно. Об этом мы поговорим в следующих подразделах. Установка из локального исходного кода с помощью pip Мы добавили в пакет новые файлы и теперь можем установить его с помощью pip. Самый простой способ установки -выполнить следующую команду с указанием пути к папке masifutilv2: >pip install <путь к masifutilv2> Ниже приведен консольный вывод команды при запуске без установки пакета wheel: Processing ./masifutilv2 Using legacy 'setup.py install' for masifutilv2, since package 'wheel' is not installed. Installing collected packages: masifutilv2 Running setup.py install for masifutilv2 done Successfully installed masifutilv2-0.l.O Утилита установила пакет, но с использованием ЕGG-формата, поскольку пакет wheel не был установлен. Как выглядит наше виртуальное окружение после уста­ новки показано на рис. 2.10. После установки пакета в виртуальное окружение мы протестировали его с помо­ щью программы pkgmainЗ .ру, которая отработала, как и ожидалось. СОВЕТ Удалить пакет можно командой pip uninstall masifutilv2.\n--- Страница 73 ---\n74 Раздел 1. Python помимо основ Рис. 2.10. Виртуальное окружение после установки masifutilv2 с помощью pip Далее установим пакет wheel, а затем снова переустановим его: >pip install <путь к masifutilv2> Консольный вывод будет следующим: Processing ./masifutilv2 Building wheels for collected packages: masifutilv2 Building wheel for masifutilv2 (setup.py) done Created wheel for masifutilv2: filename=masi futilv2-0.l.0-py3-none-any.whl size=3497 sha256=038712975b7d7eЫf3fefa799da9e294b34 e79caea24abb444dd8lf4cc44b36e Stored in folder: /private/var/folders/xp/g88fvmgs0k90w0rc_ qq4xkzxpsxllv/T/pip-ephem-wheel-cache-12eyp_wq/wheels/ de/14/12/7 lb4d69630lfdl052adf28719lfdd054ccl7efбc9b59066277 Successfully built masifutilv2 Installing collected packages: masifutilv2 Successfully installed masifutilv2-0.l.O На этот раз пакет успешно установлен с помощью wheel. Также видно, что он ото­ бражается в нашем виртуальном окружении, как показано на рис. 2.11.\n--- Страница 74 ---\nГлава 2. Использование модулей для сложных проектов .,, Ь venv ►bЬin ►binclude ,,, blib • Ь pythonЗ.7 ,, 1З site-packages ►• _pycache_ ►111 _distutils_hack ► 8_pytest ► 1111 attr ►8 attrs-20.3.0.dist-info ► Ь Ыеасh ►8 Ыeach-3.2.1.dist-info ► • dateutil ►Ь docutils ► 8 docutils-0.16.dist-info ►•idna ► 111 idna-2.10.dist-info ► 8 keyring ► • keyring-21.7.0.dist-info ►Ь masifutil т Ь masifutilv2-0.1.0.dist-info • INSTALLER 8' LICENSE.txt • МЕТАDАТА • RECORD • REQUESTED 1111 top_level.txtа WHEEL Рис. 2.11. Виртуальное окружение после установки masifutilv2 с пакетом wheel, используя pip 75 Мы узнали, как установить пакет с помощью утилиты pip из локального исходного кода. Далее опубликуем пакет в централизованном репозитории (Test PyPI). Публикация пакета в Test PyPI В качестве следующего шага добавим пример пакета в репозиторий PyPI. Перед этим нужно создать аккаунт Test PyPI. Обратите внимание, Test PyPI -это отдель­ ный экземпляр индекса пакета, предназначенный специально для тестирования. Помимо создания аккаунта также необходимо добавить АРI-токен. Детали созда­ ния аккаунта и добавления токена мы оставим на веб-сайте Test PyPI (https:/1 test.pypi. orgl). Для отправки пакета понадобится утилита Twine. Предположим, что она уже уста­ новлена с помощью pip. Для отправки masifutilv2 выполним следующие действия: 1.Создадим дистрибутив следующей командой. Утилита sdist создаст с помощью архиватора Т AR ZIР-файл ниже папки dist: >python setup.py sdist\n--- Страница 75 ---\n76 Раздел 1. Python помимо основ 2.Отправим файл дистрибутива в Test PyPI. При запросе имени пользователя и па­ роля укажем _token_ и АРI-токен соответственно. >twine upload -repository testpypi dist/masifutilv2-0.1.0.tar.gz 3.Эта команда отправит ZIР-файл пакета в репозиторий Test PyPI. Вывод консоли будет следующим: Uploading distriЬutions to https://test.pypi.org/leqacy/ Enter your username: _ token _ Enter your password: Uploading masifutilv2-0.l.0.tar.gz 100%1······••1 5.15k/5.15k [00:02<00:ОО, 2.2lkВ/s] После успешной загрузки можно просмотреть файл по адресу: https://test.pypi.org/project/ masifutilv2/0. 1. 0/. Установка пакета из PyPI Установка пакета из Test PyPI аналогична установке из обычного репозитория, за исключением, что нужно указать URL-aдpec репозитория с помощью аргументов index-url. Команда выглядит следующим образом: >pip install --index-url https://test.pypi.org/simple/ --nodeps masifutilv2 Консольный вывод будет следующим: Looking in indexes: https://test.pypi.org/simple/ Collecting masifutilv2 Downloading https://test-files.pythonhosted.org/ packages/b7/e9/7afe390b4ecle5842e8e62a6084505cЬcбЬ9 fбadf0e37ac695cd23156844/masifutilv2-0.l.O.tar.gz (2.3 lсВ) Вuilding wheels for collected packages: masifutilv2 Вuilding wheel for masifutilv2 (setup.py) done Created wheel for masifutilv2: filename=masifutilv2- 0.l.0-py3-none-any.whl size=3497 sha256=a3dЬ8f04Ы18e16ae291Ьad9642483874 f5c9f447dЬee57c0961Ь5f8fЬf99501 Stored in folder: /Users/rmJasif/LiЬrary/caches/pip/ wheels/lc/47/29/95b9edfe28f02a605757cl f1735660a6f79807ece430f5b836 Successfully built masifutilv2 Installing collected packages: masifutilv2 Successfully installed masifutilv2-0.l.O Из вывода видно, что pip ищет модуль в Test PyPI. Как только он находит пакет с именем masifнti.lv2, загружает его, а затем устанавливает в виртуальном окружении.\n--- Страница 76 ---\nГлава 2. Использование модулей для сложных проектов 77 Проще говоря, после создания пакета с использованием рекомендуемого формата и стиля публикация и доступ к пакету -лишь вопрос выбора утилит Python и следо­ вания стандартным действиям. Заключение В этой· главе мы рассмотрели концепцию модулей и пакетов в Python. Узнали, как создавать многоразовые модули и как импортировать их другими модулями и про­ граммами. Мы также обсудили загрузку и инициализацию модулей, когда они включаются другими программами в процессе импорта. В последней части главы мы поговорили о создании простых и сложных пакетов, предоставили множество примеров кода для доступа к пакетам, а также установили и опубликовали пакет для повторного использования. Это важные навыки, если вы работаете над проектом в команде организации или создаете библиотеки Python для более крупного сообщества. В следующей главе мы поговорим о новом уровне модульности с использованием объектно-ориентированного программирования в Python. Охватим такие понятия, как инкапсуляция, полиморфизм и абстракция, которые являются ключевыми инст­ рументами при создании и управлении проектами в реальном мире. Вопросы 1.В чем разница между модулем и пакетом? 2.Что такое абсолютный и относительный импорт в Python? 3.Что такое РуРА? 4.Что такое Test PyPI и зачем он нужен? 5.Является ли init-фaйл обязательным условием сборки пакета? Дополнительные ресурсы ♦Modular Programming with Python, автор: Эрик Вестра (Erik Westra). ♦Expert Python Programming, авторы: Михал Яворски (Michal Jaworski) и Тарек Зиаде (Tarek Ziade). ♦Руководство пользователя по пакетам Python (https://packaging.python.orgl). ♦РЕР 420: неявные пакеты пространства имен (https://www.python.org/dev/ peps/pep-04 201).\n--- Страница 77 ---\n78 Раздел 1. Python помимо основ Ответы 1.Модуль предназначен для организации функций, переменных и классов в от­ дельные файлы кода Py1hon. Пакет Py1hon похож на папку для организации не­ скольких модулей или подпакетов. 2.Абсолютный импорт требует указания абсолютного пути к пакету, начиная с верхнего уровня, тогда как относительный импорт основан на относительном пути пакета в соответствии с текущим расположением программы, где исполь­ зуется оператор import. 3.Python Packaging Authority (РуРА)- это рабочая группа, которая поддерживает основной набор программных проектов, используемых в создании пакетов в Python. 4.Test PyPI-это репозиторий программного обеспечения в Py1hon. Он предна­ значен для тестирования. 5.Начиная с Python 3.3 init-фaйл является опциональным.\n--- Страница 78 ---\n3 Расширенное объектно-ориентированное программирование на Python Python можно использовать в качестве декларативного модульного языка програм­ мирования, такого, как С, а также для императивного или полноценного объектно­ ориентированного программирования (ООП) вместе с другими языками, такими, как Java. Декларативное программирование -это парадигма, которая сосредото­ чена на том, что мы хотим реализовать, императивное же фиксируется на конкрет­ ных шагах на пути к этой реализации. Python хорошо вписывается в обе парадиг­ мы. Объектно-ориентированное программирование -это форма императивного программирования, в которой свойства и поведение реальных объектов включены в программы. ООП также рассматривает отношения между разными типами объек­ тов из реального мира. В этой главе мы рассмотрим, как принципы ООП могут быть реализованы с помо­ щью Python. Предположим, что вы уже знакомы с такими основными понятиями, как классы, объекты и экземпляры, а также имеете представление о наследовании между объектами. Темы этой главы: ♦Знакомство с классами и объектами. ♦Принципы ООП. ♦Полиморфизм. ♦Композиция как альтернативный подход к проектированию. ♦Утиная типизация в Python. ♦Когда не стоит использовать ООП в Python.https://t.me/it_boooks/2\n--- Страница 79 ---\n80 Технические требования В этой главе вам понадобится: ♦Python 3.7 или более поздней версии. Пример кода для этой главы: Раздел 1. Python помимо основ https ://github. com/PacktPublishing/Python-for-Geeks/tree/master/ChapterOЗ. Знакомство с классами и объектами Класс -это схема того, как объект должен быть определен. Сама по себе она не содержит никаких данных, это просто шаблон. Объект класса -это экземпляр, созданный из.класса, поэтому его так же называ­ ют экземпляром класса. В этой книге мы будем использовать термины объект и экземпляр как синонимы. Объекты в ООП могут быть представлены как физиче­ ские предметы -столы, стулья или книги, но в большинстве случаев они пред­ ставляют собой абстрактные (не физические) сущности, например, счета, имена, адреса и платежи. Рассмотрим классы и объекты на примерах. Различия между атрибутами класса и атрибутами экземпляра Атрибуты определены как часть описания класса, их значения не меняются во всех экземплярах, созданных из этого класса. Обратиться к атрибутам можно по имени класса или имени экземпляра, но первый вариант предпочтительнее (для чтения или обновления). Состояние или данные объекта представлены в атрибутах эк­ земпляра. В Python класс определяется ключевым словом class. Как мы уже говорили в пер­ вой главе, имя класса указывается в ВерблюжьемРегистре. Следующий фрагмент кода создает класс car (автомобиль): #carexamplel.py class Car: pass Этот класс не имеет атрибутов и методов. Он пустой, и можно подумать, что без дополнительных компонентов он совершенно бесполезен, но это не так. В Python можно добавлять атрибуты н.а лету без определения в классе. В следующем фраг­ менте мы добавим атрибуты в экземпляр во время выполнения: #carexamplel.py class Car: pass",
      "debug": {
        "start_page": 46,
        "end_page": 79
      }
    },
    {
      "name": "Глава 3. Расширенное объектно-ориентированное программирование на Python 79",
      "content": "--- Страница 80 --- (продолжение)\nГлава 3. Расширенное объектно-ориентированное программирование на Python if name ==\" main \"· car = Car {} car.color = \"Ыuе\" car.miles = 1000 print (car.color) print (car.miles) 81 Здесь мы создали экземпляр (car) нашего класса Car, а затем добавили два атрибута этого экземпляра: color (цвет) и miles (мили, пробег). Обратите внимания, в приме­ ре мы добавляем атрибуты именно экземпляра. Затем добавим атрибуты класса и атрибуты экземпляра с помощью метода конст­ руктора L ini t _), который загружается при создании объекта. Фрагмент кода с двумя атрибутами экземпляра (color и miles) и методом init выглядит следующим образом: lt�le2.py class Car: c_mileage_units = \"Mi\" def init (self, color, miles): self.i color = color self.i_mileage = miles ·if name == \" main \"· carl = Car (\"Ьlue\", 1000) print (car.i_color) print (car.i_mileage) print (car.c_mileage_units) print (Car.c_mileage_units) В этой программе мы сделали следующее: 1.Создали класс Car с атрибутом c_mileage_units (единицы пробега) и двумя пере­ менными экземпляра: i_ color и i _mileage. 2.Создали экземпляр car класса car. 3.Отправили на вывод атрибуты экземпляра, используя переменную экземпляра car. 4.Отправили на вывод атрибуты класса, используя переменную экземпляра car и имя класса Car. Консольный вывод будет одинаковым в обоих случаях. ВАЖНОЕ ПРИМЕЧАНИЕ self -это ссылка на создаваемый экземпляр. Часто используется в Python для дос­ тупа к атрибутам и методам экземпляра в пределах метода экземпляра, включая ме­ тод init. Не является ключевым словом, и его использование необязательно. Вместо него может быть использовано любое другое слово, например, this или Ыаh, кроме случаев, когда он должен быть первым параметром методов экземпляра, но соглаше­ ние об использовании self как аргумента слишком строгое.\nГлава 3. Расширенное объектно-ориентированное программирование на Python if name ==\" main \"· car = Car {} car.color = \"Ыuе\" car.miles = 1000 print (car.color) print (car.miles) 81 Здесь мы создали экземпляр (car) нашего класса Car, а затем добавили два атрибута этого экземпляра: color (цвет) и miles (мили, пробег). Обратите внимания, в приме­ ре мы добавляем атрибуты именно экземпляра. Затем добавим атрибуты класса и атрибуты экземпляра с помощью метода конст­ руктора L ini t _), который загружается при создании объекта. Фрагмент кода с двумя атрибутами экземпляра (color и miles) и методом init выглядит следующим образом: lt�le2.py class Car: c_mileage_units = \"Mi\" def init (self, color, miles): self.i color = color self.i_mileage = miles ·if name == \" main \"· carl = Car (\"Ьlue\", 1000) print (car.i_color) print (car.i_mileage) print (car.c_mileage_units) print (Car.c_mileage_units) В этой программе мы сделали следующее: 1.Создали класс Car с атрибутом c_mileage_units (единицы пробега) и двумя пере­ менными экземпляра: i_ color и i _mileage. 2.Создали экземпляр car класса car. 3.Отправили на вывод атрибуты экземпляра, используя переменную экземпляра car. 4.Отправили на вывод атрибуты класса, используя переменную экземпляра car и имя класса Car. Консольный вывод будет одинаковым в обоих случаях. ВАЖНОЕ ПРИМЕЧАНИЕ self -это ссылка на создаваемый экземпляр. Часто используется в Python для дос­ тупа к атрибутам и методам экземпляра в пределах метода экземпляра, включая ме­ тод init. Не является ключевым словом, и его использование необязательно. Вместо него может быть использовано любое другое слово, например, this или Ыаh, кроме случаев, когда он должен быть первым параметром методов экземпляра, но соглаше­ ние об использовании self как аргумента слишком строгое.\n--- Страница 81 ---\n82 Раздел 1. Python помимо основ Можно обновлять атрибуты класса, используя переменную экземпляра или имя класса, но результат может быть разным. Когда обновление происходит через имя класса, изменение распространяется на все экземпляры этого класса. Когда обнов­ ление осуществляется с помощью переменной экземпляра, изменение касается только этого экземпляра. Убедимся на примере следующего фрагмента кода с клас­ сом Car: JlcarexupleЗ .py Jlопределение класса Car такое же как в carexample2.py if пате == 11 main 11: carl = Car (11Ыuе11 , 1000) car2 = Car(11red11, 2000) print(11using carl: 11 +carl.c_mileage_uпits) print(11using car2: 11 +car2.c_mileage_units) print(1 1using Class: 11 +Car.c_mileage_uпits) carl.c_mileage_units = 11lan11 print(11using carl: 11 + carl.c_mileage_units) print(11using car2: 11 + car2.c_mileage_units) print(11using Class: 11 + Car.c_mileage_units) C&r. с_ mileage _ uni ts = 11NP11 print(11using carl: 11 + carl.c_mileage_units) print(11using car2: 11 + car2.c_mileage_units) print ( 11using Class: 11 + Car. с_ mileage _ uni ts) Консольный вывод программы можно проанализировать следующим образом: 1.Первый набор операторов print выводит значение атрибута класса по умолча­ нию -мi. 2. После выполнения выражения carl.c_mileage_units = 11km11 значение атрибута класса останется прежним (мi) для экземпляра car2 и атрибута на уровне класса. 3.После выполнения выражения car.c_mileage_units = 11NP11 значение атрибута класса изменится на NP и для car2, и на уровне класса, но останется прежним ( km) для carl, потому что мы задали его явно. ВАЖНОЕ ПРИМЕЧАНИЕ Имена атрибутов начинаются с букв «с» и «i» с целью показать, что они принадлежат к классу (Qlass) или экземпляру (instance), а не являются обычными локальными или глобальными переменными. Имена атрибутов экземпляра, не относящихся к типу pub­ lic, но относящихся к типам private или protected, должны начинаться с одинарного или двойного подчеркивания. Мы поговорим об этом позже в этой главе.\n--- Страница 82 ---\nГлава 3. Расширенное объектно-ориентированное программирование на Python 83 Конструкторы и деструкторы классов Python, как и другие языки ООП, использует конструкторы и деструкторы, но со своими особенностями именования. Целью конструкторов является инициализация - или присвоение значений атрибутам на уровне класса или уровне экземпляра (в основном, второй вариант) при создании экземпляра класса. Метод _ ini t _ извес­ тен как конструктор и всегда выполняется при создании нового экземпляра. В Python поддерживаются три типа конструкторов: ♦Конструктор по умолчанию: если конструктор (метод _init_) не включен в класс или не объявлен, класс использует пустой конструктор по умолчанию; он не делает ничего, кроме, инициализации экземпляра класса; ♦Конструктор без параметров: этот тип конструктора не принимает никаких аргументов, кроме ссылки на создаваемый экземпляр; в следующем примере продемонстрирован конструктор без параметров для класса Name: class Name: #конструктор без параметров def init (self): print(\"A new instance of Name class is \\ created\") ♦Поскольку с этим конструктором не передаются никакие аргументы, его функ­ ционал ограничен. Например, в нашем примере мы отправили на консоль сооб­ щение, что для класса Name создан новый экземпляр; ♦Конструктор с параметрами: такой тип конструктора может принимать один или несколько аргументов, а состояние экземпляра можно задать на основе входных аргументов, предоставленных методом конструктора; добавим в класс Name конструктор: class Name: #конструктор с параметрами def init (self, first, last): self.i first = first self.i last = last Деструкторы -это противоположность конструкторов. Они выполняются при удалении или уничтожении экземпляра. В Python деструкторы почти не использу­ ются, поскольку удалением неиспользуемых экземпляров занимается сборщик му­ сора. Если необходимо добавить логику внутри деструктора, можно использовать метод _ del _. Он вызывается автоматически при удалении всех ссылок на экземп­ ляр. Синтаксис определения деструктора в Python следующий: def del (self): print(\"Object is deleted.\")\n--- Страница 83 ---\n84 Раздел 1. Python помимо основ Различия между методами класса и методами экземпляра В Python можно определить три типа методов: ♦Методы экземпляра: связаны с экземпляром и требуют создания экземпляра перед их выполнением; принимают первый атрибут как ссылку на экземпляр {self) и могут считывать и обновлять состояние экземпляра; примером такого метода является ini t . ♦Методы класса: объявляются с помощью декоратора @classmethoct; для выпол­ нения не нужен экземпляр класса; для этого метода ссылка на класс (по согла­ шению обозначается cls) будет автоматически отправлена в качестве первого аргумента. ♦Статические методы: объявляются с помощью декоратора @staticmethoct; мето­ ды не имеют доступа к объектам cls или self; они похожи на функции полезно­ сти, которые принимают определенные аргументы и предоставляют выходные данные на основе их значений; например, если нужно оценить конкретные вход­ ные данные или сделать парсинг данных для обработки, то для этих целей можно написать статические методы; они работают как обычные функции, которые оп­ ределяются в модулях, но доступны в контексте пространства имен класса. Рассмотрим пример, как эти методы могут быть определены и использованы: #methodsexaпplel.py class Car: c_mileage_units = \"Mi\" def init (self, color, miles): self.i color = color self.i_mileage = miles def print_color (self): print (f\"Color of the car is {self.i_color)\") @classmethod def print_units(cls): print (f\"mileage unit are {cls.c_mileage_unit}\") print(f\"class name is {cls._name_}\") @statianethod def print _ hello О : print (\"Hello from а static method\") if name ==\" main \": car = Car (\"Ыuе\", 1000) car.print_color() car.print_units() car.print_hello()\n--- Страница 84 ---\nГлава 3. Расширенное объектно-ориентированное программирование на Python Car.print_color(car); Car.print_units(); Car.print_hello() В этой программе мы сделали следующее: 85 1.Создали класс car с атрибутом класса (c_mileage_units), методом класса (print_units), статическим методом (print_hello), атрибутами экземпляра (i_color и i_mileage), методом экземпляра (print_color) и методом конструктора Linit_). 2.Создали экземпляр car класса Car, используя его конструктор. 3.Используя переменную экземпляра car, вызвали метод экземпляра, метод класса и статический метод. 4.Используя имя класса car, снова вызвали метод экземпляра, метод класса и ста­ тический метод. Обратите внимание, метод экземпляра можно вызвать по имени класса, но для этого нужно передать переменную экземпляра в качестве первого аргумента (это также объясняет, зачем нужен аргумент self дЛЯ каждого метода экземпляра). Консольный вывод этой программы будет следующий: Color of the car is Ыuе mileaqe unit are Мi class name is Car Hello fran а static method Color of the car is Ыuе mileaqe unit are Мi class name is Car Hello fran а static method Специальные методы Когда мы определяем класс в Python и пытаемся вывести на консоль один из его экземпляров оператором print, мы получаем строку, содержащую имя класса и ссылку на экземпляр объекта, то есть адрес объекта-в памяти. Не существует стан­ дартной реализации функционала to string, доступной дЛЯ экземпляра или объекта. Фрагмент кода, демонстрирующий это поведение: #саrехапр14.ру class Car: def init (self, color, miles): self.i color = color self.i_mileage = miles if name ==\" main \": car = Car (\"Ыuе\", 1000) print (car)\n--- Страница 85 ---\n86 Раздел 1. Python помимо основ Вывод консоли будет аналогичным выводу ниже, что не соответствует ожидаемому выводу от оператора print: < main .Car object at Ох100саае80> Для какого-либо осмысленного вывода из оператора print нужно реализовать спе­ циальный метод _ str _, который вернет строку с информацией об экземпляре и который можно настроить по мере необходимости. Фрагмент кода из файла carexample4 .рус методом str #carexample4.py class Car: c_mileage_units = \"Mi\" def init (self, color, miles): self.i color = color self.i_mileage = miles def str (self): return f\"car with color {self.i_color} and \\ mileage {self.i_mileage}\" if name ==\" main \": car = Car (\"Ыuе\", 1000) print (car) Теперь консольный вывод оператора print будет таким: car with color Ыuе and mileage 1000 При правильной реализации _str_ можно использовать print без специальных функций, наподобие to _ string (). Такой способ управления преобразованием строк в духе Python. Другим популярным методом, используемым по схожим причи­ нам, является _ repr _, с помощью которого интерпретатор Python проверяет объ­ ект. Такой способ больше подходит для отладки. Эти и некоторые другие методы называются специальными или дандерами (Dunder), так как они всегда начинаются и заканчиваются двойным символом под­ черкивания (DouЫe under). Обычные методы не должны использовать подобное именование. В некоторой литературе они иногда называются магическими. Суще­ ствует несколько десятков специальных методов, доступных для реализации с классом. Полный их список доступен в официальной документации Python 3 по ад­ ресу https ://docs.python. org/3/reference/datamodel. html#specialnames . В этом подразделе мы рассмотрели классы и объекты. В следующем мы поговорим о различных принципах объектно-ориентированного программирования в Python. Принципы ООП Объектно-ориентированное программирование -это способ объединения свойств и поведения в единую сущность, которая называется объектом. Для дос-\n--- Страница 86 ---\nГлава 3. Расширенное объектно-ориентированное программирование на Python 87 тижения наибольшей эффективности и модульности в Python доступно несколько принципов: ♦Инкапсуляция данных. ♦Наследование. ♦Полимфоризм. ♦Абстракция. В следующих подразделах мы подробно рассмотрим каждый из этих принципов. Инкапсуляция данных Это фундаментальная концепция ООП, иногда также упоминается как абстракция. Это понятие включает в себя объединение данных и связанных с ними действий. На самом деле это нечто большее, чем просто объединение или абстракция. Здесь можно перечислить три ключевые задачи инкапсуляции: ♦Объединение данных и действий, связанных с ними. ♦Сокрытие внутренней структуры и деталей реализации объекта. ♦-Ограничение доступа к определенным компонентам (атрибутам или методам) объекта. Инкапсуляция упрощает использование объектов с помощью сокрытия внутренних деталей ее реализации, а также помогает контролировать обновление состояний объекта. Далее мы подробно рассмотрим эти задачи. Объединение данных и действий Для объединения данных и действий в одну инициализацию мы определяем атри­ буты и методы в классе. Класс в Python может иметь следующие типы элементов: ♦Конструктор и деструктор. ♦Методы и атрибуты класса. ♦Методы и атрибуты экземпляра. ♦Вложенные классы В предыдущем разделе уже говорилось обо всех элементах класса, за исключением вложенных или внутренних классов. Мы рассмотрели примеры реализации конст­ рукторов и деструкторов. Использовали атрибуты экземпляра для инкапсуляции данных в наших экземплярах или объектах. Мы также обсудили методы класса, статические методы и атрибуты класса с примерами кода в предыдущем разделе. В завершение темы обсудим следующий фрагмент кода с вложенным классом. Возьмем класс Car и вложенный внутри него класс Engine.\n--- Страница 87 ---\n88 Раздел 1. Python помимо основ Каждому автомобилю нужен двигатель, поэтому имеет смысл сделать его внут­ ренним: #carwithinnerexamplel.py class Car: \"\"\"внешний класс\" 1111 c_mileage_units = \"Mi\" def init (self, color, miles, eng_size): self.i color = color self.i_mileage = miles self.i_enqine = self.Enqine(enq_size) def str (self): return f\"car with color {self.i_color}, mileage \\ {self.i_mileage} and engine of {self.i_engine}\" class Enqine: \"\"\"внутренний класс\"\"\" def init (self, size): self.i size = size def str (self) : return self.i size if name ==\" main \": car = Car (\"Ьlue\", 1000, \"2.51\") print (car) print(car.i_enqine.i_size) В примере определен внутренний класс Engine внутри обычного класса car. Класс Engine имеет только один атрибут i_size, метод конструктора Linit_) и метод _ str _. По сравнению с предыдущими примерами в класс car мы внесли следую­ щие изменения: ♦Метод _ init _ включает новый атрибут для объема двигателя; также бьша до­ бавлена строка для создания нового экземпляра Engine, связанного с экземпля­ ром Car. ♦Метод _str_клacca car включает в себя атрибут внутреннего класса (i_size). Основная программа имеет оператор print для экземпляра Car, а также имеет строку для вывода значения атрибута i _ size класса Engine. Консольный вывод будет сле­ дующим: car with color Ыuе, mileaqe 1000 and enqine of 2.SL 2.51 Вывод показывает, что у нас есть доступ к внутреннему классу изнутри реализации класса и мы можем обратиться к атрибутам внутреннего класса снаружи.\n--- Страница 88 ---\nГлава З. Расширенное объектно-ориентированное программирование на Python 89 Далее поговорим, как можно скрыть некоторые атрибуты и методы, и они не будут доступны и видимы за пределами класса. Сокрытие информации Из предыдущих примеров видно, что доступ ко всем атрибутам на уровне класса и экземпляра не имеет ограничений. Такой подход приводит к плоскому дизайну, а класс становится просто оберткой вокруг переменных и методов. Лучший подход в ООП состоит в сокрытии определенных атрибутов экземпляра и предоставлении доступа извне только к тем из них, которые необходимы. Для понимания, как это реализуется в Python, рассмотрим два термина: частный (private) и защищенный (protected). Частные переменные и методы Частная переменная или атрибут могут быть определены двойным символом под­ черкивания перед именем переменной. В Python нет ключевого слова наподобие private, как в других языках программирования. Переменные класса и экземпляра могут быть лишь помечены как частные. Такой метод может быть определен двойным символом подчеркивания перед име­ нем метода. Он может быть вызван только внутри класса и недоступен за его пре­ делами. Каждый раз при определении атрибута или метода как частного интерпретатор Python не разрешает доступ к такому атрибуту или методу за пределами определе­ ния класса. Ограничение также распространяется на подклассы. Поэтому доступ к частным атрибутам и методам есть только у кода внутри класса. Защищенные переменные и методы Защищенные переменные или методы помечаются одинарным символом подчерки­ вания в начале имени. Они должны быть доступны коду внутри определения клас­ сов и подклассов. Например, если надо преобразовать атрибут i _ color из риЫiс в protected, достаточно просто поменять его имя на_ i _ color. Интерпретатор Python не навязывает такое использование защищенных элеме_нтов внутри класса или под­ класса. Речь, скорее, о соблюдении соглашений об именовании и использовании защищенных атрибутов и методов в соответствии с их определением. Используя частные или защищенные переменные и методы, можно скрывать неко­ торые детали реализации объекта. Это полезно, поскольку позволяет иметь ком­ пактный и чистый код внутри большого класса, не открывая все содержимое для внешнего мира. Еще одна причина скрывать атрибуты -контроль обновления и доступа к ним. В заключение рассмотрим новый вариант класса car с частными и защищенными переменными и частным методом: #carexaпple5.py class Car:\n--- Страница 89 ---\n90 c_mileage_units = \"Mi\" _ max _ speed = 200 def init (self, color, miles, model): self.i color = color self.i_mileage = miles self. no doors = 4 self. model = model def str (self): return f\"car with color {self.i_color}, mileage {self.i_mileage}, model {self._model} and doors {self._doors() }\" def _doors (self): return self. no doors if name ==\" main \": car = Car (\"Ьlue\", 1000, \"Camry\") print ·(car) Мы добавили в класс car следующие элементы: Раздел 1. Python помимо основ ♦частная переменная _max_speed (максимальная скорость) со значением по умол­ чанию; ♦частная переменная _no_doors (нет дверей) со значением по умолчанию внутри метода конструктора _ ini t _; ♦защищенная переменная _model (модель), добавленная только для демонстрации; ♦частный метод _ doors () в экземпляре для хранения информации о количестве дверей; ♦метод _str_ дополнен частным методом _doors () для получении информа- ции о количестве дверей. Консольный вывод будет предсказуемым. Но если попробовать обратиться к част­ ным методам или переменным из основной программы, они будут недоступны, а интерпретатор выдаст ошибку. Такое поведение ожидаемо, поскольку цель частных переменных и методов -быть доступными только внутри класса. ВАЖНОЕ ПРИМЕЧАНИЕ Python не делает переменные и методы закрытыми, а только создает видимость это­ го. Он фактически искажает имена переменных с именем класса так, что они не видны в классе, который их содержит. В примере с классом car можно обратиться к частным переменным и методам. Python предоставляет доступ к ним за пределами определения класса с другим име­ нем атрибута, которое состоит из символа подчеркивания вначале, имени класса и\n--- Страница 90 ---\nГлава 3. Расширенное объектно-ориентированное программирование на Python 91 имени частного атрибута. Таким же образом можно получить доступ к частным методам. Строки кода ниже допустимы, но не поощряются и противоречат определениям private и protected: print (Car._Car_max_speed) print (car._Car_doors()) print (car._model) Как видно, _ car добавлена перед фактическим именем частной переменной. Это сделано для минимизации конфликтов с переменными и во внутренних классах. Защита данных В предыдущих примерах понятно, что можно без ограничений получать доступ к атрибутам экземпляра. И также нет ограничений на использование реализованных методов. Поэтому мы маскируем их под частные или защищенные для сокрытия от влияния извне. Но в реальных задачах нужно предоставить доступ к переменным так, что этот процесс можно бьmо легко контролировать и обслуживать. Во многих ООП-языках это достигается с помощью модификаторов доступа -геттеров и сеттеров: ♦Геттеры (от англ. get-получать) используются для доступа к частным атрибу­ там класса или его экземпляра. ♦Сеттеры (от англ. set-задавать) используются для задания частных атрибутов класса или его экземпляра. С помощью этих модификаторов можно реализовать дополнительную логику для доступа и установки атрибутов, которую к тому же удобно хранить в одном месте. Есть два способа реализовать модификаторы: традици01тый и с помощью декора­ торов. Традиционный подход к использованию геттеров и сеттеров В традиционном подходе мы пишем методы экземпляров с префиксами get и set, за которыми следует символ подчеркивания и имя переменной. Изменим наш класс car для использования модификаторов доступа к атрибутам экземпляра следующим образом: #саrехапрlеб.ру class Car: _mileage_un its = \"Mi\" def init (self, col, mil): self. color = col self._mileage = mil\n--- Страница 91 ---\n92 def str (self): return f\"car with color {self.get_color()} and \\ mileage { self. get _ mileage () } \" def get_color(self): return self. color def get_ mileage (self) : return self._mileage def set_mileage (self, new_mil): self._mileage = new_mil if name ==\" main \": car = Car (\"Ыuе\", 1000) print (car) print (car.get_color()) print(car.get_mileage()) car.set_mileage(2000) print (car.get_color()) print(car.get_mileage()) Мы внесли в класс car следующие изменения: Раздел 1. Python помимо основ ♦Атрибуты экземпляра color и mileage добавлены как частные переменные. ♦Добавлены геттеры для атрибутов экземпляра color и mileage. ♦Добавлен сеттер только для атрибута mileage, поскольку color задается один раз при создании объекта. ♦ В основной программе получили данные для нового экземпляра класса, исполь­ зуя геттеры; затем обновили пробег, используя сеттер; и снова получили данные для атрибутов color и mileage. На консоли видно вполне ожидаемые выходные данные. Мы не стали определять сеттеры для всех атрибутов, а сделали их только там, где это требуется и имеет смысл. В ООП использование геттеров и сеттеров является лучшей практикой, но в Python они не очень популярны. Культура разработчиков по-прежнему заключается в доступе к атрибутам напрямую. Использование декоратора property Использование декораторов для определения геттеров и сеттеров -это современ­ ный подход, который позволяет реализовать код в духе Python.\n--- Страница 92 ---\nГлава 3. Расширенное объектно-ориентированное программирование на Python 93 Существует декоратор @property, с которым код выглядит проще и аккуратнее. До­ полним им класс Car: #carexaпple7.py class Car: _mileage_units = \"Mi\" def init (self, col, mil): self. color = col self._mileage = mil def str (self) : return f\"car with color {self.color} and mileage \\ { self. mileage} \" @property def color (self) : return self. color @property def mileage (self): return self._mileage @mileage.setter def mileage (self, new_mil): self._mileage = new_mil if name ==\" main \": -- car = Car (\"Ыuе\", 1000) print (car) print (car.color) print{car.mileage) ·car .mileage = 2000 print {car.color) print{car.mileage) Мы внесли следующие изменения: ♦Сделали атрибуты экземпляра частными переменными. ♦Добавили геттер-методы для color и mileage, используя декоратор @property и имя атрибута в качестве имени метода. ♦Добавили сеттер-методы для mileage с помощью декоратора @mileage. setter, при- своив методу то же имя, что и имя атрибута. В основном скрипте мы обращаемся к атрибутам color и mileage по имени экземп­ ляра, за которым следует точка и имя атрибута (в стиле Python). Это делает синтак­ сис кода лаконичным и читабельным. Использование декораторов также упрощает имена методов.\n--- Страница 93 ---\n94 Раздел 1. Python помимо ОСНОВ Итак, мы обсудили все аспекты инкапсуляции в Python и использование классов для объединения данных и действий. Также мы рассмотрели, как скрывать ненуж­ ную информацию от внешнего мира и защищать данные в классе с помощью гетте­ ров, сеттеров и декоратора property. Далее поговорим, как реализуется наследование. Расширение классов с помощью наследования Концепция похожа на наследование в реальном мире, где дети получают некоторые характеристики своих родителей в дополнение к собственным характеристикам. Точно также класс может быть дополнен элементами (атрибуты и методы) другого класса. Класс, от которого мы наследуем, называется родительским, базовым или суперклассом. Класс, который мы наследуем, называется производным, дочерним или подклассом. Ниже приводится простая схема отношений между родительским и дочерним классом (рис. 3.1): Родителыкии ,ласе Расширяет Дочерний класс Рис. 3.1. Отношения между родительским и дочерним классом В Python дочерний класс обычно наследует все элементы родительского, но этим поведением можно управлять, используя соглашение об именовании (например, использовать двойное подчеркивание) и модификаторы доступа. Наследование бывает двух типов: простое и множественное. Рассмотрим каждый вариант поподробнее. Простое наследование В данном случае класс является производным от одного родителя. Это распростра­ ненная форма наследования в ООП, которая больше похожа на генеалогическое дерево людей. Синтаксис родительского и дочернего класса при простом наследо­ вании: class BaseClass: <атрибуты и методы базового класса> class ChildClass (BaseClass): <атрибуты и методы дочернего класса> Изменим наш пример класса car, сделав его производным от родительского класса Vehicle (транспортное средство). Также добавим еще один дочерний класс Truck (грузовик) для демонстрации подробностей концепции наследования: #inheritancel.py class Vehicle:\n--- Страница 94 ---\nГлава 3. Расширенное объектно-ориентированное программирование на Python def init {self, color): self.i color = color def print_vehicle_info{self): print{f\"This is vehicle and I know my color is \\ {self.i_color}\") class Car (Vehicle): def init {self, color, seats): self.i color = color self.i seats = seats def print_me{self): print{ f\"Car with color {self.i_color} and no of \\ seats {self.i_seats)\") class Truck (Vehicle): def init {self, color, capacity): self.i color = color self.i_capacity = capacity def print_me{self): print{ f\"Truck with color {self.i_color} and \\ loading capacity {self.i_capacity) tons\") if name ==\" main \": car = Car {\"Ьlue\", 5) car . print _ vehicle _ info () car .print_ me О truck = Truck{\"white\", 1000) truck. print _ vehicle _ info () truck.print_ me О 95 В этом примере мы создали родительский класс Vehicle с одним атрибутом i _ color и одним методом print_vehicle_in fo. Оба элемента имеют возможность наследова­ ния. Затем мы создали два дочерних класса: car и тruck. Каждый из них имеет по одному дополнительному атрибуту (i_seats и i_capacity, соответственно) и один дополнительный метод (print_me). В методах print_me в каждом дочернем классе мы получаем доступ к атрибуту экземпляра базового класса и к атрибутам экземпляра подкласса. Такой дизайн является преднамеренным для уточнения идеи наследования элемен­ тов от суперкласса и добавления отдельных элементов индивидуально в подкласс. Использование двух дочерних классов показывает роль наследования в отношении возможности повторного использования.\n--- Страница 95 ---\n96 Раздел 1. Python помимо основ В основной программе мы создали экземпляры Car и тruck и попытались обратиться к родительскому методу, а также к методу экземпляра. Ожидаемый консольный вывод показан ниже: Тhis is vehicle and I know пrу color is Ыuе car with color Ыuе and no of seats 5 Тhis is vehicle and I know пrу color is whi te Truck with color white and loadinq capacity 1000 tons Множественное наследование При множественном наследовании дочерний класс может быть производным от нескольких родителей. Эта концепция применяется в сложных объектно­ ориентированных проектах, где объекты связаны с множеством других объектов. Но нужно быть осторожным при наследовании от нескольких классов, особенно, если они наследуются от общего суперкласса. Может возникнуть проблема ромбо­ видного наследования (diamond inheritance). Это ситуация, когда мы создаем класс Х, наследуя его от двух классов У и Z, которые, в свою очередь, наследуются от общего класса А. Тогда для класса Х вознцкает неопределенность относительно кода в классе А, который он наследует через классы У и Z. Множественное насле­ дование не поощряется из-за возможных проблем, которые оно вызывает. Для демонстрации изменим классы Vehicle и car, а также добавим родительский класс Engine. Код с множественным наследованием будет следующий: #inheritance2.py class Vehicle: def init (self, color): self.i color = color def print_vehicle_info(self): print( f\"This is vehicle and I know my color is \\ {self. i_color} \") class Engine: def init (self, size): self.i size = size def print_enqine_info(self): print(f\"This is Engine and I know my size is \\ {self.i_size}\") class Car (Vehicle, Engine): def init (self, color, size, seat): self.i color = color self.i size = size self.i seat = seat\n--- Страница 96 ---\nГлава 3. Расширенное объектно-ориентированное программирование на Python def print_car_info(self): print(f\"This car of color {self.i_color} with \\ seats {self.i_seat} with engine of size \\ {self.i_size}\") if narne ==\" main \": car = Car (\"Ыuе\", \"2.51\", 5 ) car. print_ vehicle _ info () car.print_enqine_info() car. print _ car _ info () 97 В этом примере мы создали два родительских класса: Vehicle и Engine. Класс Vehicle такой же, как в предыдущем примере. Класс Engine имеет один атрибут i_size и один метод print_engine_info. Класс Car наследуется от обоих классов Vehicle и Engine. Он имеет один дополнительный атрибут (i_seats) и один дополнительный метод (print_car_info). В методе экземпляра у нас есть доступ к атрибутам экземп­ ляра обоих родительских классов. В основной программе мы создали экземпляр класса car. С помощью него мы мо­ жем получить доступ к методам экземпляра родительских и дочерних классов. На консоли мы увидим ожидаемые выходные данные: Тhis is vehicle and I know П!'j color is Ыuе car with color Ыuе and no of seats 5 Тhis is vehicle and I know П!'j color is whi te Truck with color white and loadinq apacity 1000 tons В этом разделе мы рассмотрели два типа наследования -простое и множествен­ ное. Теперь поговорим о полиморфизме в Python. Полиморфизм В буквальном смысле полиморфизм означает «множество форм». В ООП поли­ морфизм -это способность экземпляра вести себя по-разному и использовать один и тот же метод с одними и теми же именем и аргументами для выбора пове­ дения в соответствии с классом, к которому он принадлежит. Эту концепцию можно реализовать двумя способами: перегрузкой метода и пере­ определением метода. Рассмотрим каждый вариант поподробнее. Перегрузка метода Это способ достичь полиморфизма за счет наличия нескольких методов с одинако­ выми именами, но разным типом или количеством аргументов. В Python нет про­ стого способа реализовать такой подход, поскольку два метода не могут иметь одно имя. Все является объектом, включая классы и методы. При написании методов для\n--- Страница 97 ---\n98 Раздел 1. Python помимо основ класса с точки зрения пространства имен они являются его атрибутами, следова­ тельно, одинаковых имен быть не может. Если создать два метода с одним именем, синтаксической ошибки не будет, второй просто заменит первый. Внутри класса метод можно перегрузить, задав значение по умолчанию аргумен­ там. Это не лучший способ реализации, но он работает. Ниже приведен пример пе­ регрузки метода внутри класса в Python: #methodoverloadingl.py class Car: def init (self, color, seats): self.i color = color self.i seat = seats def print_me(self, i='Ьasic'): if(i =='basic'): print(f1 1This car is of color {self.i_color}11 ) else: print(f11This car is of color {self.i_color} \\ with seats { self. i _ seat} 11) if name == 11 main 11• car = Car (11Ыuе11, 5 ) car .print_ me О car .print_ me ( 'Ьlah') car .print_ me ( 'detail') В этом примере добавлен метод print _ те с аргументом, имеющим значение по умолчанию, которое будет использоваться, если в метод не передаются никакие параметры. Если print_me не получит параметры, на консоли мы увидим только цвет экземпляра Car. Когда методу передается аргумент (независимо от его значе­ ния), поведение меняется, и теперь метод предоставляет цвет и количество мест для экземпляра car. Вот что будет на консоли: Тhis car is of color Ыuе Тhis car is of color Ыuе with seats 5 Тhis car is of color Ыuе with seats 5 ВАЖНОЕ ПРИМЕЧАНИЕ В Python можно использовать сторонние библиотеки (например, overload) для более аккуратной реализации перегрузки методов. Переопределение метода Наличие метода с одинаковым именем в родительском и дочернем классах называ­ ется переопределением метода. Предполагается, что реализация методов на разных уровнях классов отличается. При вызове переопределяющего метода для экземпля-\n--- Страница 98 ---\nГлава 3. Расширенное объектно-ориентированное программирование на Python 99 ра дочернего класса интерпретатор ищет этот метод в области определения на со­ ответствующем уровне. Метод выполняется на уровне дочернего класса. Если ин­ терпретатор не находит метод на уровне дочернего экземпляра, он ищет его на уровне родительского. Если метод переопределен в дочернем классе с помощью экземпляра его же уровня, но мы хотим выполнить его в родительском, можно ис­ пользовать метод super () для доступа к методу на уровне родительского класса. В Python это более популярный тип полиморфизма, который используется наряду с наследованием и позволяет эффективно его использовать. Для демонстрации переопределения метода изменим код в inhertancel .ру, переиме­ новав имя метода print_vehicle_info в print_me. Как мы знаем, методы print_me уже присутствуют в двух дочерних классах с разными реализациями. Ниже приведен обновленный код с выделенными изменениями: #methodoverridinql.py class Vehicle: def init (self, color): self.i color = color def print_me(self): print(f\"This is vehicle and I know my color is \\ {self. i _color} \") class Car (Vehicle): def init (self, color, seats): self.i color = color self.i seats = seats def print_me(self): print( f\"Car with color {self.i_color} and no of \\ seats {self.i_seats}\") class Truck (Vehicle): def init (self, color, capacity): self.i color = color self.i_capacity = capacity def print_me(self): print( f\"Truck with color {self.i_color} and \\ loading capacity {self.i_capacity} tons\") if name ==\" main \": vehicle = Vehicle(\"red\") vehicle. print _ me () car = Car (\"Ыuе\", 5) car .print_me () truck = Truck(\"white\", 1000) truclt .print_ me ()\n--- Страница 99 ---\n100 Раздел 1. Python помимо основ В этом примере мы переопределяем метод print _ me в дочерних классах. Когда мы создаем три разных экземпляра классов Vehicle, car и тruck и выполняем один и тот же метод, мы получаем разное поведение. Вот что будет в консольном выводе: Тhis is vehicle and I know my color is red car with color Ыuе and no of seats 5 Truck with color white and loading capacity 1000 tons Переопределение метода имеет много практических применений. Например, можно наследовать встроенный класс list и переопределить его методы для расширения функционала. Выборочная сортировка является примером переопределения метода для объекта list. В следующих главах мы рассмотрим несколько примеров переоп­ ределения метода. Абстракция Это еще одна мощная возможность ООП, которая позволяет скрыть детали реали­ зации и показать только необходимые или высокоуровневые функции объекта. Примером из реальной жизни служит автомобиль, который мы имеем, с его основ­ ным функционалом для нас как водителя, но без реальных знаний того, как все уст­ роено. Эта концепция связана с инкапсуляцией и наследованием вместе, именно поэтому мы оставили ее напоследок. Еще одна причина вьщелить абстракцию в отдельную тему -подчеркнуть использование абстрактных классов в Python. Абстрактные классы в Python Абстрактный класс выступает шаблоном для других классов. С его помощью можно создать набор абстрактных методов (пустых), которые будут реализованы дочерним классом. Простыми словами, абстрактным назьmается класс, который содержит один или несколько абстрактных методов. У таких методов есть объявле­ ние, но нет реализации. В таком классе могут быть методы, которые уже реализованы и могут использо­ ваться дочерним классом (как есть) с помощью наследования. Данная концепция помогает реализовать общие программные интерфейсы приложения (Application Programming lnterface, API), а также определять в одном месте общую базу кода, которую можно повторно использовать в дочерних классах. ПРИМЕЧАНИЕ Экземпляры абстрактных классов создавать нельзя. Абстрактный класс можно реализовать с помощью встроенного модуля АВС (Abstract Base Classes) из пакета аЬс. Этот пакет также включает модуль abstractmethod, который использует декораторы для объявления абстрактных методов.\n--- Страница 100 ---\nГлава 3. Расширенное объектно-ориентированное программирование на Python 101 Ниже показан простой пример с использование модуля АБС и декоратора aЬstractrnethod: taЬstractionl.py fran аЬс iq:юrt АВС, aЬstractmethod class Vehicle (АВС) : def hello (self) : print (f\"Hello from aЬstract class\") @aЬstractmethod def print_me(self): pass class Car (Vehicle): def init (self, color, seats): self.i color = color self.i seats = seats \"\"\"Необходимо реализовать этот метод\"\"\" def print_1118(self): print( f\"Car with color {self.i_color} and no of \\ seats {self.i_seats}\") if name == \" main \": # vehicle = Vehicle() # vehicle.hello() car = Car (\"Ьlue\", 5) car. print _ 1118 О car. hello () #это невозможно В этом примере мы сделали следующее: ♦Сделали класс Vehicle абстрактным, унаследовав его от класса АБС, а также объ­ явили один из методов (print_me) абстрактным с помощью декоратора @aЬstractrnethod. ♦Обновили зна�омый класс car, реализовав в нем метод print_me, и сохранив весь остальной код как в предыдущем примере. ♦ В основной части программы попытались создать экземпляр класса vehicle (код прокомментирован на иллюстрации); создали экземпляр класса car и вьmолни­ ли методы print _ me и hello. При попытке создать экземпляр класса Vehicle вылезет ошибка, что невозможно создать экземпляр абстракrного класса Vehicle с абстрактным методом print_me: Can't instantiate aЬstract class Vehicle with aЬstract methods print_lll8 Также при попытке не реализовывать метод print_me в дочернем классе car мы по­ лучим ошибку. Для экземпляра класса car мы имеем ожидаемый консольный вывод из методов print _ me и hello.\n--- Страница 101 ---\n102 Раздел 1. Python помимо основ Композиция как альтернативный подход к проектированию Это еще одна популярная концепция, которая также имеет отношение к инкапсуля­ ции. Простыми словами, композиция означает включение одного или нескольких объектов в другой объект для образования объекта из реального мира. Класс, кото­ рый включает другие объекты класса, называется составным (или композитным). А классы, объекты которых входят в составной класс, называются компонентами. На скриншоте приведен пример составного класса с тремя компонентами -А, В и С (рис. 3.2): (.;cJL 1 авнт1 �-;г:асс Рис. 3.2. Отношения между составным классом и классами-компонентами Композиция считается альтернативным подходом к наследованию. Обе концепции устанавливают отношения между объектами. В случае наследования -объекты тесно связаны, так как изменения в родительских классах могут нарушить код в дочерних. При композиции -объекты связаны слабо, что облегчает внесение из­ менений в один класс без нарушений кода в другом. Благодаря своей гибкости композиционный подход довольно популярен, но это не означает, что он подходит для решения всех задач. Возникает вопрос, какой подход лучше использовать для конкретной проблемы? Для этого есть правило. Когда есть отношения между объ­ ектами, правильным выбором будет наследование. Например, автомобиль являет­ ся транспортным средством, а кот является животным. В этом случае дочерний класс является расширением родительского с дополнительным функционалом и возможностью повторного использования функций родительского класса. Если от­ ношение между объектами такое, что один объект содержит другой, лучше ис­ пользовать композицию. Например, автомобиль содержит аккумулятор. В предыдущем примере с множественным наследием мы реализовали класс car как потомок класса Engine, что является не лучшим вариантом использования наследо­ вания. Поэтому попробуем использовать композицию и реализовать класс car с объектом Engine внутри. У нас может быть еще один класс Seat, который может входить в класс car.\n--- Страница 102 ---\nГлава 3. Расширенное объектно-ориентированное программирование на Python 103 Рассмотрим эту концепцию в следующем примере, где мы создаем класс car, кото­ рый содержит Engine и Seat внутри себя: #c�sitionl.py class Seat: def init (self, type): self.i_type = type def str (self): return f1 1Seat type: {self.i_type}11 class Engine: def init (self, size): self.i size = size def str (self): return f11Engine: {self.i_size}11 class Car: def init (self, color, eng_size, seat_type): self.i color = color self.engine = Engine(eng_size) self.seat = Seat(seat_type) def print_me(self): print(f11This car of color {self.i_color} with \\ { self. engine} and { self. seat} 11) if name == 11 main 11• car = Car (11Ыuе11, 112.5111, 11leather11) car .print_me () print(car.engine) print (car. seat) print(car.i_color) print(car.engine.i_size) print(car.seat.i_type) Проанализируем этот фрагмент кода: 1.Мы определили классы Engine и seat и указали в каждом по одному атрибуту: i_size и i_type соответственно. 2.Затем определили класс Car, добавив атрибут i _ color, экземпляр Engine и экземп­ ляр seat. Экземпляры Engine и Seat были созданы одновременно с экземпляром Car.\n--- Страница 103 ---\n104 Раздел 1. Python помимо основ 3.В основной программе мы создали экземпляр car и реализовали следующие дей­ ствия: •car.print_me: обращается к методу print_me экземпляра Car; •print (car .engine): выполняет метод str класса Engine; •print (car. seat): выполняет метод str класса Seat; •print (car .i_color): обращается к атрибуту i_color экземпляра Car; •print (car .engine. i_size): обращается к атрибуту i size экземпляра Engine внутри экземпляра car; •print(car.seat.i_t ype): обращается к атрибуту i_type экземпляра Seat внутри экземпляра car. Консольный вывод получится следующим: Тhis car of color Ыuе with Enqine: 2.SL and Seat type: leather Enqine: 2.SL Seat type: leather Ыuе 2.SL leather Далее обсудим утиную типизацию, которая является альтернативой полиморфизму. Утиная типизация в Python Утиная типизация, в основном, используется в языках, которые поддерживают ди­ намическую типизацию, например, Python и JavaScript. Название заимствовано из следующей цитаты: «Если это выглядит как утка, плавает как утка и крякает как утка, то, вероятно, это и есть утка». Смысл цитаты в том, что объект можно идентифицировать по его поведению. Если птица ведет себя как утка, то она, скорее всего, утка. Это и есть основной принцип утиной типизации. В данном случае тип класса объекта менее важен, чем метод (поведение), который им определен. Тип объекта не проверяется, но выполняется метод, который ожида­ ется. Для демонстрации рассмотрим простой пример с тремя классами car, Cycle и Horse (машина, велосипед и лошадь, соответственно). И попытаемся реализовать для каждого из них метод start. В классе нorse вместо start мы назовем метод push. Ни­ же приведен фрагмент кода с тремя классами и основной программой в конце: #ducttypel.py class Car:\n--- Страница 104 ---\nГлава 3. Расширенное объектно-ориентированное программирование на Python def start(self): print (\"start engine Ьу ignition /battery\") class Cycle: def start·(self) : print (\"start Ьу pushing paddles\") class Horse: def push (self) : print (\"start Ьу pulling/releasing the reins\") if name ==\" main \": for obj in Car(), Cycle(), Horse(): оЬj .start() 105 В основной программе мы пытаемся динамически перебирать экземпляры классов и вызывать метод start. Как и ожидалось, строка obj . start () не сработала для объ­ екта нorse, поскольку в классе нет такого метода. В этом примере видно, что можно помещать разные типы классов или экземпляров в один оператор и выполнять ме­ тоды для них. Если изменить метод с именем push на start в классе нorse, основная программа бу­ дет выполняться без ошибок. Утиная типизация применяется во многих сценариях для упрощения решений. Например, при использовании итераторов и метода len ко многим объектам. Мы подробно остановимся на итераторах в следующей главе. До этого момента мы рассматривали понятия и принципы ООП, а также их пре­ имущества. Далее речь пойдет о ситуациях, когда их использование не очень вы­ годно. Когда не стоит использовать ООП в Python Python является языком, который обладает гибкостью и может быть использован как для объектно-ориентированного программирования (Java), так и для деклара­ тивного программирования (С). ООП всегда привлекает разработчиков, поскольку обладает такими мощными инструментами, как инкапсуляция, абстракция, насле­ дование и полиморфизм. Но они подходят не для всех задач. Лучше всего исполь­ зовать их для больших и сложных приложений, особенно если у них есть пользо­ вательский интерфейс. Если программа больше похожа на скрипт для выполнения определенных задач и нет необходимости хранить состояния объектов, использование ООП излишне. На­ пример, приложения для Data Science и ин,тен,сивн,ой обработки дан,н,ых не требу­ ют использования принципов ООП. Для них гораздо важнее определить, как вы­ полнять задачи в определенном порядке для достижения целей. Примером из ре­ альной жизни служит написание клиентских программ для выполнения задач по\n--- Страница 105 ---\n106 Раздел 1. Python помимо основ обработке больших объемов данных в кластере узлов вроде Apache Spark для па­ раллельной обработки. Мы рассмотрим эти типы приложений в следующих главах. Несколько дополнительных сценариев, где использование ООП не обязательно: ♦Чтение файла, применение логики и запись в новый файл -тип программы, которую проще реализовать с помощью функций в модуле, чем с помощьюооп. ♦Настройка устройств с использованием Python очень популярна и может быть решена с помощью обычных функций. ♦Анализ и преобразование данных из одного формата в другой также можно вы­ полнить с помощью декларативного программирования вместо ООП. ♦Перенос старой кодовой базы на новую с помощью ООП -также не самая лучшая идея; если старый код написан без использования шаблонов ООП, на выходе получатся функции, не связанные с ООП, заключенные в классы и объ­ екты, которые трудно обслуживать и расширять. Важно сначала проанализировать постановку задачи и требования, прежде чем ре­ шить, использовать ООП или нет. Это также зависит от того, какие сторонние биб­ лиотеки будут использованы в программе. Если необходимо расширять классы из сторонней библиотеки, без ООП не обойтись. Заключение В этой главе мы изучили концепцию классов, а также обсудили, как создавать их и использовать для создания объектов и экземпляров. Затем мы рассмотрели четыре столпа ООП: инкапсуляцию, наследование, полиморфизм и абстракцию. Мы также рассмотрели простые и понятные примеры кода для лучшего понимания этих кон­ цепций, которые являются фундаментальными для ООП в Python. Узнали про утиную типизацию, которая демонстрирует отсутствие зависимости от классов, а затем поговорили о задачах, где не следует использовать ООП. В этой главе вы не только освежили свои знания об основных понятиях, но и узна­ ли, как реализовать их в синтаксисе Python. В следующей главе мы рассмотрим не­ сколько библиотек для продвинутого программирования. Вопросы 1.Что такое класс и объект? 2.Что такое dunder? 3.Поддерживает ли Python наследование от нескольких классов? 4.Можно ли создать экземпляр абстрактного класса? 5.Правда ли, что в утиной типизации важен тип класса?\n--- Страница 106 ---\nГлава 3. Расширенное объектно-ориентированное программирование на Python 107 Дополнительные ресурсы ♦Modular Programming with Python, автор: Эрик Вестра (Erik Westra). ♦Python 3 Object-Oriented Programming, автор: Дасти Филлипс (Dusty Phillips). ♦Learning Object-Oriented Programming, автор: Гастон С. Хиллар (Gaston С. Hillar). ♦Python for Everyone -Third edition, авторы: Кэй Хорстманн (Сау Horstrnann) и Ране Некэз (Rance Necaise ). Ответы 1.Класс -это схема для указания интерпретатору, как что-то должно быть опре­ делено. Объект -это экземпляр, созданный из класса на основе того, что в нем определено. 2.Dunder -это специальный метод, который всегда начинается и заканчивается двойным подчеркиванием. Для каждого класса доступно несколько десятков специальных методов. 3.Да, Python поддерживает наследование от нескольких классов. 4.Нет, мы не можем создать экземпляр абстрактного класса. 5.Неправда, в утиной типизации методы важнее классов.\n--- Страница 107 ---\nРаздел 2 Расширенные концепции программирования В этом разделе мы продолжим наше путешествие и изучим передовые концепции Python. Коснемся таких понятий, как итераторы, генераторы, обработка ошибок и исключений. Это поможет вам выйти на новый уровень программирования. Поми­ мо написания кода мы также изучим создание и автоматизацию модульных и инте­ грационных тестов с помощью таких фреймворков, как unittest и pytest. В послед­ ней главе раздела рассмотрим расширенные функции для преобразования данных и создания декораторов в Python, а также способы использования структур данных, включая pandas DataFrames, для аналитических приложений. Этот раздел содержит следующие главы: ♦«Глава 4: Библиотеки Python для решения сложных задач». ♦«Глава 5: Тестирование и автоматизация с помощью Python». ♦«Глава 6: Расширенные советы и приемы в Python».",
      "debug": {
        "start_page": 80,
        "end_page": 107
      }
    },
    {
      "name": "Глава 4. Библиотеки Python для продвинутого программирования 111",
      "content": "--- Страница 108 ---\n4 Библиотеки Python для продвинутого программирования В предыдущих главах мы рассмотрели разные подходы к созданию модульных и многоразовых программ. В этой главе мы рассмотрим несколько понятий, таких, как итераторы, генераторы, ведение журналов (логирование) и обработка ошибок. Это позволит писать эффективный код, пригодный для повторного использования. Предполагается, что вы уже знакомы с синтаксисом Python и умеете писать управ­ ляющие структуры. Мы изучим работу циклов, обработку файлов, как лучше всего их открывать и по­ лучать доступ к ним. А также, как обрабатывать ошибки, которые могут быть как ожидаемыми, так и неожиданными. Мы также поговорим о логировании и о спосо­ бах, как настроить системы ведения журналов и регистрации событий. Эта глава научит вас использовать библиотеки для создания сложных проектов. Темы этой главы: ♦Введение в контейнеры данных Python. ♦Итераторы и генераторы для обработки данных. ♦Обработка файлов в Python. ♦Обработка ошибок и исключений. ♦Модуль logging в Python. Технические требования В этой главе вам понадобится: ♦Python 3.7 или более поздней версии. Примеры кода для этой главы: https:/lgithub.com/PacktPuЬlishing/Python-for-Geeks/tree/master/Chapter04.\n--- Страница 109 ---\n112 Раздел 2. Расширенные концепции программирования Начнем с изучения контейнеров данных, которые пригодятся в последующих темах этой главы. Введение в контейнеры данных Python Python поддерживает несколько типов данных, как числовых, так и коллекционных (наборы). Определение числовых типов основано на присвоении переменной зна­ чения, Это значение и будет определять числовой тип данных, например, с пла­ вающей точкой или целое. Специальный конструктор (например, int о или float о) также может использоваться для создания переменной определенного типа. Кон­ тейнерные типы данных тоже можно определить, либо присвоив значение в соот­ ветствующем формате, либо с помощью специального конструктора для каждого типа в коллекции. В этом разделе мы рассмотрим пять различных контейнерных типов данных: строки, списки, кортежи, словари и множества. Строки Строка (String), по сути, не является набором (далее слова коллекция и набор мож­ но считать синонимами), но мы рассмотрим ее, поскольку она широко используется в Python и реализуется с помощью неизменной последовательности кодовых точек Юникода. Так как строка использует последовательность (коллекцию), мы изучим ее в этом разделе. Строки в Python являются неизменными. Благодаря этому они обеспечивают безо­ пасное выполнение конкурентных программ, где множество функций могут обра­ щаться к одному и тому же строковому объекту и давать одинаковый результат. С изменяемыми объектами такой подход невозможен. Поскольку строки неизменяе­ мы, их часто используют как ключи для словарей или как элементы множеств. Не­ достаток заключается в том, что приходится создавать новый экземпляр, даже если нужно внести небольшое изменение. ПРИМЕЧАНИЕ Изменяемые объекты можно изменить после их создания, неизменяемые -нельзя. Строковые литералы могут быть заключены одинарными кавычками ('Ыаh'), двой­ ными кавычками (\"Ыаh Ыаh\") или тремя двойными или одинарными кавычками (\"\"\"none\"\"\" или '' 'none' ''). Также стоит отметить, строковые объекты обрабатыва­ ются в Python 3 и в Python 2 по-разному. В Python 3 они могут содержать только текстовые последовательности в виде точек данных Юникода, тогда как в Python 2 они могут содержать и текст, и байтовые данные. В Python 3 байтовые данные хра­ нятся в типе bytes. Разделение текста и байтов в Python 3 делает его чистым и эффективным, но в ущерб переносимости данных. Текст Юникода в строках нельзя сохранить на диск или отправить в удаленное расположение в сети без преобразования в двоичный\n--- Страница 110 ---\nГлава 4. Библиотеки Python для продвинутого программирования 113 формат. Для этого необходимо закодировать строковые данные в последователь­ ность байтов одним из следующих способов: ♦Метод str.encode (encoding, errors): используется со строковыми объектами и принимает два аргумента; пользователь может указать тип кодека (по умолча­ нию UTF-8) и способ обработки ошибок. ♦Преобразование в байтовый тип данных: строковый объект можно преобра­ зовать в тип Bytes, передав экземпляр строки конструктору вместе со схемой ко­ дирования и схемой обработки ошибок. Подробную информацию о методах и атрибутах, доступных для строкового объек­ та, можно найти в официальной документации к Python соответствующей версии. Списки Список (List) -один из базовых типов коллекций в Python, который используется для хранения нескольких объектов с помощью одной переменной. Списки являют­ ся динамичными и изменяемыми, то есть объекты в нем могут быть изменены, а сам список увеличен или уменьшен. Объекты списка реализованы не с использованием связных списков, а с помощью массива переменной длины, который содержит ссылки на хранимые им объекты. Указатель на массив и его длина хранятся в заголовке списка, который обновляется по мере добавления и удаления объектов в нем. Поведение такого массива похоже на список, но на самом деле это не так, поэтому некоторые операции со списками в Python не оптимизированы. Например, вставка и удаление объектов имеет слож­ ность: n. Для исправления ситуации язык предоставляет тип данных deque (двусторонняя очередь) во встроенном модуле collections. Такой тип предоставляет функционал стеков и очередей, что является удобной альтернативой в случаях, когда для реше­ ния задачи требуется поведение связанного списка. Списки можно создавать пустыми или с начальным значением, используя квадрат­ ные скобки. Рассмотрим пример, который демонстрирует создание пустого или не­ пустого списка, используя только квадратные скобки или конструктор объекта списка: el = [] #nустой список е2 = list () #пустой список с использованием конструктора gl = ( 'а' , 'Ь' ] #список с 2 элементами g2 = list(['a', 'Ь']) #список с 2 элементами с использованием конструктора gЗ = list (gl) #список, созданный из списка Подробности об операциях над объектами списка (add, iпsert, append и delete) можно посмотреть в официальной документации Python. Далее рассмотрим кортежи.\n--- Страница 111 ---\n114 Раздел 2. Расширенные концепции программирования Кортежи Кортеж (Tuple) -это неизменяемый список, соответственно, его нельзя изме­ нить после создания. Обычно используются для малого количества элементов, а также, когда важны их положение и последовательность в коллекции. Для сохра­ нения последовательности элементов кортежи созданы неизменяемыми, и это их отличает от списков. Операции с ними, как правило, выполняются быстрее, чем со списками. Когда значения в коллекции должны быть постоянными и в опреде­ ленном порядке, предпочтительнее использовать кортежи из-за их превосходной производительности. Из-за неизменности кортежи обычно инициализируются значениями. Простой кор­ теж можно создать с помощью круглых скобок. Ниже показаны несколько спосо­ бов создания экземпляров кортежей: w = () #пустой кортеж х = (2, 3) у = (\"Hello World\") z = (\"Hello World\",) #кортеж с 2 элементами #не кортеж, для разделения элементов нужна запятая #запятая делает это кортежем В этом фрагменте создан пустой кортеж (w), кортеж с числами (х) и кортеж с тек­ стом «Hello World» (z). Переменная у не является кортежем, поскольку для кортежа из 1 элемента требуется запятая в конце для указания на перечисление объектов. Теперь познакомимся со словарями. Словари Словарь (Dictionary) -один из наиболее популярных и универсальных типов дан­ ных в Python. Он представляет собой набор для хранения данных в формате «ключ:значение». Это изменяемый и неупорядоченный тип данных. В других язы­ ках они называются ассоциативными массивами и хеш-таблицами. Словарь может быть задан списком, в котором данные идут парами (ключ:значение) и заключены в фигурные скобки. Ключ отделяется от значения двоеточием, а пары между собой разделяются запятой. Ниже представлен код с определением словаря: mydict = { \"braпd\": \"ВМW\", \"model\": \"330i\", \"color\": \"Blue\" Ключи в словаре не могут повторяться. Они должны быть представлены неизмен­ ным объектом, например, строкой, кортежем или числом. Значения в словаре могут иметь любой тип данных, включая списки, множества, пользовательские объекты и даже другие словари.\n--- Страница 112 ---\nГлава 4. Библиотеки Python для продвинутого программирования 115 При работе со словарями важны три объекта или списка: ♦Ключи: используются для перебора элементов словаря; список ключей можно получить с помощью метода keys () ; dict_object.keys() ♦Значения: это объекты, хранящиеся в паре с ключами; список значений можно получить с помощью метода values (): dict_object.values() ♦Элементы: это пары «ключ-значение», хранящиеся в словаре; список элементов можно получить с помощью метода i tems (} : dict_object.items() Далее рассмотрим множества, которые являются ключевой структурой данных в Python. Множества Множество (Set) -набор уникальных объектов. Это изменяемая и неупорядочен­ ная коллекция. Объекты множества не повторяются. Python использует структуру хеш-таблицы при реализации уникальности для множества, как и для ключей в словаре. В Python множества ведут себя почти как в математике. Этот тип данных применяется в ситуациях, когда важен не порядок объектов, а их уникальность. С его помощью можно проверить, входит ли определенный объект в коллекцию. ПОДСКАЗКА Если возникает потребность в неизменяемом множестве, Python имеет подобный ва­ риант реализации, называемый frozenset. Создать новое множество можно с помощью фигурных скобок или конструктора множества set (). В следующем фрагменте приведены несколько примеров создания множества: sl = set () s2 = {} s3 = set ( ['а', s3 = (1,2} s4 = {1, 2, 1} 'Ь']) # пустое множество # пустое множество с фигурными скобками # множество, созданное из списка, используя конструктор # множество, созданное с помощью фигурных скобок # множество будет создано только с элементами 1 и 2, # повторя!СЩИЙся элемент будет проигнорирован Обращение к заданным объектам невозможно с использованием индексации. Для доступа необходимо извлечь один объект из множества как список или перебрать множество для извлечения объектов по-одному. Как и математические множества, в Python они поддерживают такие операции, как объединение, пересечение и раз­ ность. В этом разделе мы рассмотрели ключевые понятия строк и коллекций в Python 3, которые важны для понимания следующей темы -итераторы и генераторы.\n--- Страница 113 ---\n116 Раздел 2. Расширенные концепции программирования Итераторы и генераторы для обработки данных Итерация (Iteration) -один из ключевых инструментов обработки и преобразо­ вания данных, особенно при работе с большими наборами, а также когда размес­ тить весь набор в памяти невозможно или нецелесообразно. Итераторы позволяют вводить данные в память по одному элементу за раз. Итераторы можно создавать, определяя их в отдельном классе и реализуя специ­ альные методы вроде _ iter _ и _ next _. Существует также новый способ с помо­ щью оператора yield, известного также под названием генератор. Далее мы под­ робнее поговорим об этом. Итераторы Итераторы -это объекты, используемые для перебора других объектов. Переби­ раемый объект называется итерируемым (iteraЬle). Теоретически, итератор и пере­ бираемый объект -это два разных объекта, но можно реализовать итератор внут­ ри класса объектов типа iteraЫe. Так делать не рекомендуется, хоть и технически возможно. Позже мы увидим на примере, почему это считается плохим подходом. В следующем фрагменте приводится несколько примеров использования цикла for для итерации в Python: #iteratorl.py #пример 1: итерация по списку for х in [1,2,3]: print (х) #пример 2: итерация по строке for х in \"Python for Geeks\": print(x, end=\"\") print (' ') #пример 3: итерация ПО CJIOвaP8) week_days = {1: 'Mon', 2: 'Tue', 3: 'Wed' , 4 : 'Thu' , 5: 'Fri', 6: 'Sat', 7: 'Sun'} for k in week_days: print(k, week_days[k]) #пример 4: итерация по файлу for row in open('aЬc.txt'): print(row, end=\"\") В примере были использованы разные циклы for для прохода по списку, строке, словарю и файлу. Все эти типы данных являются итерируемыми. Далее рассмотрим, какие характеристики делают объект итерируемым и что такое протокол итерации.\n--- Страница 114 ---\nГлава 4. Библиотеки Python для продвинутого программирования 117 ВАЖНОЕ ПРИМЕЧАНИЕ Каждый набор в Python является итерируемым по умолчанию. В Python итератор должен реализовывать два специальных метода: i ter и _ next _. Доступность объекта для перебора обусловлена, как минимум, наличием метода _iter_. Иными словами, когда объект реализует метод _iter_, его можно назвать итерируемым. Рассмотрим эти методы: ♦ _ i ter _: возвращает объект итератора; вызывается в начале цикла для получе­ ния объекта итератора; ♦ _ next _: вызывается при каждой итерации цикла и возвращает следующий эле- мент в итерируемом объекте. Для создания пользовательского объекта, который можно перебирать, реализуем пример с классом week, который хранит номера и названия всех дней недели в сло­ варе. По умолчанию он не будет итерируемым. Сделать его доступным для перебо­ ра можно, добавив метод _ i ter _. Для упрощения примера добавим в тот же класс и метод _ next _. Ниже приведен фрагмент кода с классом week и основной про­ граммой, которая выполняет перебор элементов с целью получить названия дней недели: Jiterator2.py class Week: def _init_(self): self.days = {1: 'Monday', 2: \"Tuesday\", 3: \"Wednesday\", 4: \"Thursday\", 5:\"Friday\", 6:\"Saturday\", 7:\"Sunday\"} self. index = 1 def _iter_(self): self. index = 1 return self def _next_(self) : if self. index < 1 1 self._index > 7 raise Stopiteration else: ret_value = self.clays[self._inc:lex] self. index +=1 return ret value if{_name == \"_main_\"): wk = Week{} for clay in wk: print{day)\n--- Страница 115 ---\n118 Раздел 2. Расширенные концепции программирования Этот пример призван продемонстрировать, как методы _ i ter _ и _ next _ можно реализовать в одном классе объекта. Этот способ часто встречается в Интернете, но он считается плохим подходом к реализации итераторов. Когда мы используем его в цикле for, мы возвращаем главный объект в качестве итератора, поскольку реали­ зовали _iter_ и _next_ в одном классе. Результаты могут быть непредсказуемы­ ми. Для демонстрации выполним следующий код для'Гого же класса Week: #iteratorЗ.py class Week: #определение класса как в предыдущем примере if(_name_ == \" main \"): wk = Week() iterl = iter (wk) iter2 = iter (wk) print (iterl._next_() ) print{iter2 . next ()) print(next{iterl)) print(next(iter2)) Здесь перебирается один объект двумя разными итераторами. Результат новой про­ граммы не соответствуют ожидаемому выводу. Это происходит из-за общего атри­ бута _index, который используется двумя итераторами. Консольный вывод будет следующим: Мonday ТUesday Wednesday Тhursday Обратите внимание, мы намеренно не использовали цикл for. Мы создали два ите­ ратора для одного объекта класса Week с помощью функции iter, которая является стандартной в Python и вызывает метод _ i ter _. Для получения следующего эле­ мента в наборе мы напрямую использовали метод _ next _) а также функцию next, которая является стандартной, как и iter. Использование итерируемого объекта в качестве итератора не считается потокобезопасным. Лучшим подходом всегда будет использование отдельного класса итератора и соз­ дание нового его экземпляра с помощью метода _ i ter _. Каждый экземпляр дол­ жен управлять собственным · внутренним состоянием. Далее приведена исправлен­ ная версия того же примера с классом week и отдельным классом итератора: #iterator4.py class Week: def init {self):\n--- Страница 116 ---\nГлава 4. Библиотеки Python для продвинутого программирования self.days = (1: 'Monday', 2: \"Tuesday\", 3: \"Wednesday\", 4: \"Thursday\", 5: \"Friday\", 6: \"Saturday\", 7: \"Sunday\"} def iter (self) : return Weeklterator(self.days) class Weekiterator: def init (self, dayss): self.days_ref = dayss self. index = 1 def next (self) : if self. index < 1 1 self._index > 8: raise Stoplteration else: ret_value = self.days_ref[self._index] self. index +=1 return ret value if ( name \"_main_\"): wk = Week () i terl = iter (wk) iter2 = iter (wk) print(iterl. next ()) print(iter2. next ()) print(next(iterl)) print(next(iter2)) 119 В примере есть отдельный класс с методом _ next _, который имеет собственный атрибут _ index для управления состоянием итератора. У экземпляра итератора бу­ дет ссылка на объект-контейнер (словарь). Результат консольного вывода будет ожидаемым: каждый итератор выполняется отдельно для одного и того же экземп­ ляра класса week. Вывод консоли будет такой: Мonday Мonday ТUesday ТUesday Простыми словами, для создания итератора нужно реализовать методы _ i ter _ и _ next _, управлять внутренним состоянием и вызывать исключение Stoprteration, когда нет доступных значений. Далее мы изучим генераторы, которые упрощают способ возврата итераторов.\n--- Страница 117 ---\n120 Раздел 2. Расширенные концепции программирования Генераторы Генератор (Generator) -простой способ вернуть экземпляр итератора, который можно использовать для перебора. Достигается это путем реализации только функ­ ции генератора. Она аналогична обычной функции, но с оператором yield вместо return. Оператор return допускается в функции генератора, но не будет использо­ ваться для возврата следующего элемента в итерируемом объекте. Функция является генератором, если она имеет хотя бы один оператор yield. Ос­ новное отличие его в том, что он приостанавливает выполнение функции и сохра­ няет ее внутреннее состояние. При следующем вызове выполнение начинается со строки, на которой закончил интерпретатор. Такой подход делает функциональ­ ность итераторов простой и эффективной. Методы _ i ter _ и _ next _ реализуются автоматически, как и исключение stopiteration, которое тоже вызывается автоматически. Локальные атрибуты и их значения сохраняются между последовательными вызовами без написания допол­ нительной логики. Интерпретатор Python предоставляет все эти возможности вся­ кий раз, когда определяет функцию-генератор (функцию с оператором yielct внутри). Для понимания работы генератора начнем с простого примера, который использу­ ется при создании последовательности из первых трех букв алфавита: fgeneratorsl.py def my_gen (): yield 'А' yield 'В' yield 'С' if ( name \" main \"): iterl = my_gen() print(iterl._next_()) print(next(iterl)) print(iterl._next_()) Здесь реализована простая функция-генератор с тремя операторами yield и без опе­ ратора return. В основной части программы мы сделали следующее: 1.Вызвали функцию-генератор, которая возвращает экземпляр итератора. На этом этапе ни одна строка внутри my_gen () не выполняется. 2.Используя экземпляр итератора, вызвали метод _next_, который начинает вы­ полнение функции my _gen () , приостанавливается после выполнения первой инст­ рукции yield и возвращает букву А. 3.Затем вызываем функцию next () в экземпляре итератора. Результат будет таким же, как при использовании метода _next_. Но в этом случае функция my_gen() начинает выполнение со строки, идущей следом за той, на которой она остано­ вилась в последний раз из-за оператора yield. На следующей строке находится другой оператор yield, поэтому после вьmода буквы в происходит еще одна пауза.\n--- Страница 118 ---\nГлава 4. Библиотеки Python для продвинутого программирования 121 4.Следующий метод _next_ приведет к выполнению очередного оператора yield, который вернет букву с. Далее мы вернемся к нашему классу week и используем генератор вместо класса итератора. Пример кода: #qenerator2.py class Week: def init (self): self.days = {1: 'Monday', 2: \"Tuesday\", 3: \"Wednesday\", 4: \"Thursday\", 5:\"Friday\", 6:\"Saturday\", 7:\"Sunday\"} def week_qen(self): for х in self.days: yield self.days[x] if(_name_ == \"_main_\"): wk = Week() iterl = wk.week_qen() iter2 = iter(wk.week_qen()) print(iterl . next ()) print(ite r2. next ()) print(next(iterl)) print(next(iter2}} В сравнении с iterator4 .ру реализация класса week с генератором выглядит гораздо аккуратнее и дает те же результаты. В этом заключается сила и популярность гене­ раторов в Python. Прежде чем закончить тему, важно выделить несколько дополни­ тельных ключевых особенностей: ♦Генератор-выражения: используются для создания простых генераторов (из­ вестных также как анонимные функции) «на лету» без написания специальных методов; синтаксис похож на списковое включение (List Comprebension), но вместо квадратных скобок используются круглые; следующий фрагмент кода показывает, как можно использовать генератор-выражение для создания и ис­ пользования генератора; сравним его со списКОJ\\1 включения: #qeneratorЗ.py L = [1, 2, 3, 4, 5, 6, 7, 8, 9, О] f1 [x+l for х in L] gl = (x+l for х in L) print(gl. next () ) print(gl. next () ) ♦Бесконечные потоки: с помощью генераторов можно реализовать бесконеч­ ные потоки данных; информацию проблематично записывать в память, но ге-\n--- Страница 119 ---\n122 Раздел 2. Расширенные концепции программирования нераторы легко решают задачу, поскольку возвращают только один элемент данных за раз. ♦Конвейерная обработка генераторами: при решении сложных задач несколько генераторов могут быть использованы в качестве конвейера; концепцию конвей­ ерной обработки множеством генераторов можно объяснить на примере, где нужно вычислить сумму квадратов простых чисел; можно решить это с помо­ щью традиционных циклов for, но мы попытаемся использовать два генератора: prime_gen для генерации простых чисел и x2_gen для возведения в квадрат про­ стых чисел, полученных от prime _gen; выходные данные из двух генераторов пе­ редаются в функцию sum для получения их суммы; далее приведен фрагмент ко­ да для решения этой задачи: #qenerator4. py def prime_qen(num): for cand in range(2, num+l): for i in range (2, cand): if (cand % i) == О: break else: yield cand def x2_qen(list2): for num in list2: yield num*num print(sum(x2_qen(prime_qen(5)))) Генераторы работают по запросу, поэтому не только эффективно потребляют па­ мять, но и позволяют создавать значения по необходимости. Это помогает избе­ жать ненужной генерации данных, которые могут вообще не использоваться. Гене­ раторы хорошо подходят для обработки больших объемов данных, для передачи данных из одной функции в другие, а также для имитации конкурентности. В следующем подразделе поговорим об обработке файлов в Python. Обработка файлов в Python Чтение и запись файлов -одни из основных инструментов в любом языке про­ граммирования. Python также поддерживает эти операции. Мы рассмотрим основ­ ные действия с файлами, доступные в стандартной библиотеке языка, а именно, как открывать и закрывать файлы, читать и записывать данные, управлять файлами с помощью менеджеров контекста и открывать не<;колько файлов с помощью одно­ го дескриптора. Рассмотрим подробнее возможные действия с файлами.\n--- Страница 120 ---\nГлава 4. Библиотеки Python для продвинутого программирования 123 Операции с файлами Действия над файлами обычно начинаются с их открытия и последующего чтения или обновления содержимого. Открытие и закрытие файла Для применения любой операции к файлу сначала нужно получить указатель или ссылку на него. Это можно сделать, открыв файл встроенной функцией open, кото­ рая возвращает указатель на объект file (в некоторой литературе называется дес­ криптором файла). В функцию open требуется передать один обязательный пара­ метр -имя файла с абсолютным или относительным путем. Опционально можно также указать режим доступа: чтение, запись, добавление и пр. Полный список ре­ жимов доступа: ♦r: файл открывается только для чтения; применяется по умолчанию, если не ука­ зано иное: f = open ( ' аЬс . txt ' ) ♦а: файл открывается с добавлением новой строки в конец файла: f = open ( ' аЬс . txt ' , ' а ' ) ♦w: файл открывается для записи; если файл не существует, он будет создан; если существует, он будет переопределен, а его текущее содержимое уничтожено: f = open ( 'аЬс. txt ' , 'w' ) ♦х: файл открывается для монополыюй записи; если он уже существует, возник­ нет ошибка: f = open ( 'аЬс. txt' , 'х' ) ♦t: файл открывается в текстовом режиме; применяется по умолчанию; ♦ь: файл открывается в двоичном режиме; ♦+: файл открывается для чтения и записи: f = open ( 'аЬс. txt ' , ' r+ ' ) Режимы можно комбинировать для получения нужного результата. Помимо имени и режима доступа можно также передать тип кодировки, что особенно полезно для текстовых файлов. Ниже приведен пример открытия файла в кодировке UTF-8: f= open (\"аЬс. txt\", mode = 'r', encoding = 'utf-8') После окончания работы необходимо закрыть файл и освободить ре�урсы для дру­ гих процессов. Сделать это можно методом close для экземпляра файла или дес­ криптора. Фрагмент кода с методом close: file = open (\"аЬс. txt\", 'r+w') #операции с файлом file. close ()\n--- Страница 121 ---\n124 Раздел 2. Расширенные концепции программирования После закрытия файла ресурсы, связанные с его экземпляром и блокировками ( если они есть), будут освобождены операционной системой. Это является лучшей прак­ тикой в любом языке программирования. Чтение и запись Файл можно прочесть, открыв его в режиме доступа r, а затем используя один из методов чтения: ♦read (n) : считывает из файла n символов; ♦readline () : возвращает из файла одну строку; ♦readlines () : возвращает из файла все строки в виде списка. Точно так же можно добавить или записать данные в файл, когда он открыт в соот­ ветствующем режиме: ♦write (х): записывает в файл строку или последовательность байтов и возвраща- ет число символов, добавленных в файл; ♦writelines (lines): записывает в файл список строк. В следующем примере мы создадим новый файл, добавим в него несколько тексто­ вых строк, а затем считаем данные с помощью операций чтения: #writereadfile. py: запись в файл и Ч'МИИе из неrо fl = open(\"myfile. txt\", 'w') fl.write(\"Тhis is а saпple file\\n\") lines =[\"This is а test data\\n\", \"in two lines\\n\"] fl.writelines(lines) fl. close () f2 = open(\" myfile.txt\" , 'r') print(f2.read(4)) print(f2.readline()) print(f2.readline()) f2. seek (0) for line in f2.readlines(): print(line) f2 .close () В примере мы сначала записали в файл три строки. Используя операции чтения, мы считали сначала четыре символа, а затем две строки методом readline. В конце мы переместили указатель обратно в начало файла методом seek и считали все строки методом readlines. Далее рассмотрим, как менеджер контекста упрощает работу с файлами.\n--- Страница 122 ---\nГлава 4. Библиотеки Python для продвинутого программирования 125 Менеджер контекста Разумное и справедливое потребление ресурсов имеет решающее значение для лю­ бого языка программирования. Файловый обработчик или соединение с базой дан­ ных -это несколько примеров, когда общепринятой практикой является своевре­ менное освобождение ресурсов. Если они не будут освобождены, это приведет к утечке памяти, которая может повлиять на производительность системы или при­ вести к сбою. Для решения проблемы и своевременного освобождения ресурсов в Python исполь­ зуются контекстные менеджеры. Они резервируют и освобождают ресурсы точно в соответствии с необходимостью. Когда менеджер используется с ключевым сло­ вом with, ожидается, что оператор после with должен вернуть объект, который реа­ лизует протокол управления контекстом. Этот протокол требует от возвращаемого объекта реализации двух методов: ♦. _ enter _ () : вызывается с ключевым словом wi th и используется для резервиро­ вания ресурсов, необходимых для оператора после ключевого слова wi th; ♦. _ exi t _ () : вызывается после выполнения блока wi th и используется для осво- бождения ресурсов, зарезервированных методом . _ enter _ (). Когда файл открывается, используя блок wi th, нет необходимости закрывать его обратно. Оператор open вернет объект дескриптора файла, в котором уже реализо­ ван протокол управления контекстом, а файл будет закрыт автоматически, после того как выполнение блока with завершится. Измененная версия примера с записью и чтением файла выглядит следующим образом: #contextmgrl.py with open(\"myfile.txt\", 'w') as fl: f1. write (\"This is а sample file\\n\") lines = [\"This is а test data\\n\", \"in two lines\\n\"] fl.writelines(lines) with open(\"myfile.txt\", 'r') as f2: for line in f2.readlines(): print (line) Код с менеджером контекста прост и легко читаем. Его использование является ре­ комендуемым подходом для работы с файлами. Работа с множеством файлов Python поддерживает работу с несколькими файлами одновременно. Можно откры­ вать их в разных режимах и работать с ними. Количество файлов не ограничено. Рассмотрим пример, где откроем два файла в режиме чтения и будем обращаться к ним в любом порядке: 1.txt This is а sample file 1\n--- Страница 123 ---\n126 This is а test data 1 2.txt This is а sample file 2 This is а test data 2 #multifilesreadl.py Раздел 2. Расширенные концепции программирования with open{\"l.txt\") as filel, open(\"2.txt\") as file2: print(file2.readline()) print(filel.readline()) Мы также можем читать из одного файла и записывать в другой. Пример кода вы­ глядит следующим образом: #multifilesread2.py with open(\"l.txt\", 'r') as filel, open(\"З.txt\", 'w') as file2: for line in filel.readlines(): file2.write(line) Python также имеет более элегантное решение для работы с множеством файлов - модуль fileinput. Он может принимать список из нескольких файлов и обрабаты­ вать их все как единый ввод. Пример кода с двумя входными файлами (1. txt и 2. txt) и модулем fileinput показан ниже: #multifilesreadl.py import fileinput with fileinput.input(files = (\"l.txt\",'2.txt')) as f: for line in f: print(f.filename()) print(line) При таком подходе мы получаем один дескриптор, который работает с нескольки­ ми файлами последовательно. Далее обсудим обработку ошибок и исключений в Python. Обработка ошибок и исключений В Python существует множество типов ошибок. Наиболее распространенные из них связаны с синтаксисом в коде и называются синтаксическими. Также нередко они возникают прямо во время выполнения программы и называются, соответственно, ошибками выполнения. Те из них, которые можно обработать внутри программы, называются исключениями. В этом разделе мы покажем, как обрабатывать исклю­ чения и ошибки выполнения. Прежде чем перейти дальше, рассмотрим наиболее распространенные: ♦IndexError: программа обращается к элементу по недопустимому индексу ( адресу в памяти).\n--- Страница 124 ---\nГлава 4. Библиотеки Python для продвинутого программирования 127 ♦ModuleNotFoundError: указанный модуль не найден по системному пути. ♦ZeroDivisionError: программа пытается разделить число на ноль. ♦KeyError: программа пытается получить значение из словаря по недопустимому ключу. ♦Stoprteration: возникает, когда метод next не находит следующий элемент итерации в контейнере. ♦TypeError: программа пытается выполнить операцию с объектом неподходящего типа. Полный список ошибок доступен в официальной документации Python. Далее рас­ смотрим непосредственно обработку ошибок и исключений соответствующими инструментами. Работа с исключениями в Python При возникновении ошибок во время выполнения программа может нанести ущерб системным ресурсам, например, повредив файлы и таблицы базы данных. Вот по­ чему обработка исключений является одним из ключевых компонентов написания надежных программ. Идея состоит в том, что нам необходимо предвидеть потенци­ альные ошибки и продумать реакцию программы на них. Как и многие другие языки, Python использует ключевые слова try и except. За ними следуют два отдельных блока кода, которые нужно исполнить. Блок try содержит обычный набор операторов, для которых мы предполагаем, что может возникнуть ошибка. Блок except будет выполнен, в случае если блок try содержит ошибку. Синтаксис кода с блоками try и except: try: #набор операторов except: #операторы, которые будут выполнены, если в блоке try есть ошибка Если мы ожидаем один или несколько определенных типов ошибок, можно опре­ делить блок except, используя имя ошибки, и добавить столько блоков, сколько нужно. Такие именованные блоки except выполняются, когда в блоке try возникает исключение с соответствующим именем. К блоку except можно добавить оператор as для хранения объекта исключения в качестве переменной. Ниже приведен при­ мер кода с множеством возможных ошибок в блоке try и множеством блоков except: #exceptionl.py try: print (х) х = 5 у = о z = х /у print( 'х'+ у)\n--- Страница 125 ---\n128 except NameError азе: print(e) except ZeroDivisionError:Раздел 2. Расширенные концепции программирования print(\"Division Ьу О is not allowed\") except Exception азе: print (\"An error occurred\") print(e) Рассмотрим блоки except из примера подробнее: ♦Блок NameError: будет выполняться, когда оператор в блоке try попытается обратиться к неопределенной переменной; в нашем случае -при попытке ин­ терпретатора исполнить строку print (х); кроме того, мы присвоили объект ис­ ключения переменной с именем е и использовали ее в операторе print. ♦Блок ZeroDivisionError: будет выполняться, когда мы попытаемся вычислить выражение z = х/у при у = о; для выполнения этого блока сначала нужно испра­ вить ошибку в блоке NameError. ♦Блок except по умолчанию: это универсальный блок except, который будет вы­ полняться, если не найдено совпадений с предыдущими двумя блоками except; строка print ( 'х' + у) также вызовет ошибку типа TypeError и будет обработана здесь; поскольку в этом блоке мы не получаем никакого конкретного типа ис­ ключения, можно использовать ключевое слово Exception для сохранения объек­ та исключения в переменной. Обратите внимание, как только при любом операторе в блоке try возникает ошиб­ ка, остальные операторы игнорируются, а управление переходит к одному из бло­ ков except. В нашем примере нужно сначала исправить ошибку NameError, тогда можно будет увидеть следующее исключение, и так далее. Мы добавили в пример три разных типа ошибки для демонстрации определения нескольких блоков except для одного блока try. Порядок блоков except важен. Сначала стоит определить кон­ кретные именованные блоки, а блок по умолчанию лучше ставить в конце. На схеме (рис. 4.1) показаны все возможные блоки обработки исключения. Как видно из схемы, помимо try и except Python также поддерживает блоки else и finally для более эффективной обработки ошибок. Блок else выполняется, если в блоке try нет ошибок. Код здесь будет выполняться в обычном режиме, и в случае возникновения ошибки исключение не будет выдано. При необходимости в блок else можно добавить вложенные блоки try и except. Обратите внимание, блок else является необязательным. Блок finally выполняется независимо от наличия или отсутствия ошибок в блоке try. Он обычно используется для освобождения ресурсов через закрытие соедине­ ний и открытых файлов. Хотя этот блок необязательный, настоятельно рекоменду­ ется его реализовать.\n--- Страница 126 ---\nГлава 4. Библиотеки Python для продвинутого программирования except else finally { \"\"'\"'\"\" , , { Запустить этот код, если возникло исключение { Если нет исключений в блоке try, запустить этот код { Всеrда запускать в конце этот код Рис. 4.1. Блоки обработки исключений в Python 129 Далее рассмотрим использование этих блоков на примере. В блоке try откроем но­ вый файл для записи. Если при открытии файла возникнет ошибка, будет выдано исключение, а сведения об ошибке будут выведены на консоль с помощью опера­ тора print в блоке except. Если ошибок не возникнет, выполнится код в блоке else, который записывает в файл некоторый текст. В обоих случаях (возникнет ошибка или нет) файл будет закрыт в блоке finally. Пример кода: #exception2.py try: f = open (\"аЬс. txt\", \"w\") except Exception as е: print(\"Error:\" + е) else: f. write ( \"Hello World\") f.write(\"End\") finally: f. close () Мы подробно рассмотрели обработку исключений в Python. Далее обсудим, как принудительно вызывать исключения в коде. Вызов исключений Исключения вызываются интерпретатором Python, когда во время выполнения воз­ никает ошибка. Мы можем вызывать ошибки или исключения сами, когда понима­ ем, что продолжение выполнения приведет к неверному выводу или сбою работы кода. Вызов ошибки или исключения обеспечит корректный выход из программы.\n--- Страница 127 ---\n130 Раздел 2. Расширенные концепции программирования Объект исключения можно передать вызывающей стороне, используя ключевое слово raise. Существуют разные типы исключений: ♦Встроенное исключение. ♦Пользовательское исключение. ♦Универсальный объект Exception. В следующем примере вызовем простую функцию для вычисления квадратного корня и реализуем ее для создания исключения, когда входной параметр не являет­ ся допустимым положительным числом: #exceptionЗ.py import math def sqrt(num): if not isinstance (num, (int, float)) : raise ТypeError(\"only numЬers are allowed\") if num < О: raise Exception (\"Negative numЬer not supported\") return math.sqrt(num) if name ==\" main \"· try: print (sqrt (9)) print(sqrt( 'а')) print (sqrt(-9)) except Exception as е: print(e) В коде мы вызываем встроенное исключение путем создания нового экземпляра класса TypeError, поскольку переданный в функцию sqrt () параметр не является числом. Мы также вызываем общее исключение, когда передаем в функцию число меньше О. В обоих случаях мы передаем в конструктор пользовательский текст. Далее мы рассмотрим, как определить собственное пользовательское исключение и передавать его вызывающему коду. Определение пользовательских исключений В Python можнq определить собственное исключение с помощью создания нового класса, производного от встроенного класса Exception или его подкласса. Для де­ монстрации изменим предыдущий пример и определим два пользовательских клас­ са исключений, заменив ими встроенные типы ошибок TypeError и Exception. Новые пользовательские классы исключений будут производными от TypeError и Exception. Пример кода с пользовательскими исключениями: #exception4.py import math\n--- Страница 128 ---\nГлава 4. Библиотеки Python для продвинутого программирования class NumТypeError(ТypeError): pass class NegativeNumError (Exception) : def init (self): super(). init (\"Negative nuinьer not supported\") def sqrt (num) : if not isinstance (num, (int, float)) raise NumТypeError ( \"onl у nшnЬers are allowed\") if num < О: raise NegativeNumError return math.sqrt(num) if name == \" main \"· try: print (sqrt (9)) print(sqrt( 'а')) print (sqrt(-9)) except NumТypeError азе: print(e) except NegativeNumError азе: print(e) 131 В этом примере класс NumTypeError является производным от класса TypeError, и мы ничего в него не добавляем. Класс NegativeNumError наследуется от класса Exception, мы переопределяем его конструктор, добавляя пользовательское сообщение для этого исключения как часть конструктора. Когда мы вызываем эти пользователь­ ские исключения в функции sqrt () , мы не передаем никакого текста с классом ис­ ключений NegativeNumError. Когда мы выполняем основную программу, получаем сообщение с оператором print (е), поскольку он задан как часть определения класса. В этом подразделе мы рассмотрели, как обрабатывать встроенные типы ошибок с помощью блоков try и except, как определять по,льзовательские исключения и вы­ зывать их декларативно. Далее обсудим ведение логов в Python. Модуль logging в Python Логирование (Logging) является фундаментальным требованием для любых мас­ штабных приложений и представляет собой ведение журнала событий внутри про­ граммы. Это помогает не только при отладке, но также дает представление о внут­ ренних процессах и проблемах приложения.\n--- Страница 129 ---\n132 Раздел 2. Расширенные концепции программирования Преимущества логирования: ♦Отладка кода, ес�и необходимо проверить, когда и почему в приложении про­ изошел сбой. ♦Диагностика необычного поведения приложения. ♦Предоставление данных для проверки соблюдения законов и нормативно­ правовых норм. ♦Оценка поведения пользователей и выявление попыток несанкционированного доступа. Прежде чем перейти к практическим примерам, рассмотрим основные компоненты системы логирования в Python. Основные компоненты системы логирования Для ведения журнала приложения необходимы следующие компоненты: ♦Логгер (Logger, он же регистратор событий). ♦Уровн.и логирован.ия (Logging levels, они же приоритеты событий). ♦Форматтер (Logging formatter, он же форматировщик). ♦Обработчик (Logging handler, он же обработчик сообщений в системе логиро­ вания). Общая схема системы логирования в Python (рис. 4.2): Обработчик Рис. 4.2. Компоненты системы логирования в Python Каждый из компонентов подробно обсудим ниже. Логгер Выходные объекты ., ' ' , • Логгер -это интерфейс для системы ведения логов, с которым взаимодействует разработчик. Класс Logger в Python предоставляет несколько методов регистрации\n--- Страница 130 ---\nГлава 4. Библиотеки Python для продвинутого программирования 133 сообщений с разными приоритетами (уровнями). Чуть позже рассмотрим методы класса Logger с примерами. Приложение взаимодействует с экземпляром Logger, который устанавливается с ис­ пользованием конфигурации и уровней логирования. При получении событий эк­ земпляр Logger выбирает один или несколько подходящих обработчиков журнала и делегирует им эти события. Каждый обработчик, как правило, предназначен для конкретного выходного объекта. Он отправляет сообщения после применения фильтра и форматирования к предполагаемым выходным объектам. Уровни логирования События и сообщения в системе могут иметь разный приоритет. Например, ошибки являются более важными, чем предупреждения. Уровни логирования -это способ задать приоритеты для разных событий. В Python определено шесть уровней, каж­ дый из которых выражен целым числом, указывающим на степень серьезности: NOTSET, DEBUG, INFO, WARNING, ERROR и CRITICAL. Их описание приводится на рис. 4.3: • -• • --CRITICf1L ERROR 1'VARNING DEBUG NOTSET Предоставляет информацию о проблемах самого высокого уровня, вплоть до уровня, на котором приложение больше не может выполняться должным образом Предоставляет информацию о серьезной проблеме и указывает на то, что функция приложения не может работать должным образом Предоставляет информацию о каком-то неожиданном событии или указывает на возможную проблему Предоставляет меньше деталей, предназначенных для отслеживания поведения приложения Предоставляет подробную информацию, предназначенную для устранения неполадок и исправления ошибок Логирование не установлено Рис. 4.3. Уровни логирования в Python Формапер Форматтер (или форматировщик) помогает улучшить форматирование сообще­ ний, что важно для согласованности и удобочитаемости, как для человека, так и для компьютера. Он та�е добавляет в сообщения дополнительный контекст, напри­ мер, время, имя модуля, номер строки, потоки и процессы, что очень помогает при отладке. Пример выражения форматировщика: \"%(asctime)s -%(name)s -%(levelname)s -%(funcName) s:%(lineno)d -%(message)s\"\n--- Страница 131 ---\n134 Раздел 2. Расширенные концепции программирования Когда используется такое выражение, сообщение hello Geeks уровня INFO на консо­ ли будет выглядеть примерно следующим образом: 2021-06-10 19:20:10,864 - а.Ь.с - INFO - <module name>:10 - hello Geeks Обработчик Роль обработчика заключается в записи данных журнала в указанное место назна­ чения, это могут быть консоль, файл или даже электронная почта. В Python доступ­ но множество встроенных обработчиков, несколько популярных из них представ­ лены ниже: ♦StreamНandler: для вывода на консоль; ♦FileHandler: для записи в файл; ♦SМТPHandler: для отправки по электронной почте; ♦socketHandler : для отправки в сетевой сокет; ♦syslogHandler: для отправки на локальный или удаленный сервер системных логов Unix; ♦HTTPHandler: для отправки на веб-сервер с использованием методов GET или Роsт. Обработчик использует форматтер для добавления к логам дополнительной контек­ стной информации. Он также использует уровни логирования для их фильтрации. Работа с модулем logging На этом этапе мы рассмотрим, как использовать модуль logging на примерах. Нач­ нем с базовых параметров и постепенно доведем их до продвинутого уровня. Логгер по умолчанию Если не создавать экземпляр класса logger, будет доступен логгер Python по умол­ чанию, также известный как корневой логгер. Его можно реализовать, импортируя модуль logging и используя его методы для отправки событий. В следующем при­ мере показано, как использовать корневой логгер для регистрации событий журнала: #loggingl.py import logging logging.deЬug(\"This is а debug message\") logging.warning(\"This is а warning message\") logging.info(\"This is an info message\") Методы debug, warning и info используются для отправки событий логгеру в соответ­ ствии с уровнем серьезности. Для данного логгера приоритет события по умолча­ нию имеет значение WARNING, а вывод по умолчанию установлен stderr, а, значит, все\n--- Страница 132 ---\nГлава 4. Библиотеки Python для продвинутого программирования 135 сообщения будут выводиться только на консоль или терминал. Этот параметр бло­ кирует вывод сообщений уровня DEBUG и INFO на консоль. Вывод выглядит следую­ щим образом: WARNING:root:Тhis is а warning message Приоритет корневого логгера может быть изменен добавлением следующей строки после оператора import: logging.basicConfig(level=logging.D EBUG) После изменения уровня на DEBUG консольный вывод показывает все сообщения: DEВUG:root:Тhis is а deЬug message WARNING:root:Тhis is а warning message INFO:root:Тhis is an info message Впрочем, использовать логгер по умолчанию не рекомендуется, кроме как для са­ мых базовых задач. Для лучшей практики следует создавать новый именованный логгер, который мы рассмотрим далее. Именованный логгер Мы можем создать отдельный логгер со своими именем, приоритетом, обработчи­ ками и форматтерами. В следующем примере создадим такой логгер и укажем для него пользовательское имя и приоритет: #logging2.py import logging loggerl = logging.getLogger(\"my_logger\") logging.ЬasicConfi g() loggerl.setLevel(logging.INFO) loggerl. warning ( \"This is а warning message\") loggerl.info(\"This is а info message\") loggerl.deЬu g(\"This is а debug message\") logging.info(\"This is an info message\") Когда мы создаем экземпляр логгера, используя метод getLogger со строковым име­ нем или с помощью имени модуля (используя глобальную переменную _name_J, для одного имени может существовать только один экземпляр. Это значит, что, ес­ ли мы попытаемся использовать метод getLogger с таким же именем в любой части приложения, интерпретатор Python проверит, создан ли уже экземпляр для этого имени. Если создан, интерпретатор вернет тот же экземпляр. Создав экземпляр, мы должны сделать вызов к корневому логгеру (basicConfig ()) для предоставления обработчика и форматтера нашему логгеру. Без настройки об­ работчика, в крайнем случае, будет использоваться внутренний обработчик, кото­ рый просто выведет сообщения без какого-либо форматирования. А уровень сооб­ щения будет WARNING, независимо от приоритета, который установлен в пользова­ тельском логгере.\n--- Страница 133 ---\n136 Раздел 2. Расширенные концепции программирования На консоли можно увидеть ожидаемые выходные данные: WARNING:my_logqer:Тhis is а warning message INFO:my_logqer:Тhis is а info message Также важно отметить следующее: ♦Мы задали для логгера приоритет INFO; мы смогли зарегистрировать сообщения warning и info, но не debug. ♦Когда мы использовали корневой логгер ( с помощью экземпляра logging), мы не смогли отправить сообщение info; это вызвано тем, что корневой логгер все еще использовал уровень по умолчанию, то есть WARNING. Логгер со встроенным обработчиком и пользовательским форматтером Можно создать объект логгера со встроенным обработчиком, но пользовательским форматировщиком. В этом случае объект обработчика может использовать объект пользовательского форматтера, также объект обработчика может быть добавлен к объекту логгера в качестве его обработчика, прежде чем мы начнем использовать логгер для каких-либо событий журнала. Следующий пример демонстрирует соз­ дание обработчика и форматтера, а также добавление первого в логгер: #loggingЗ.py import logging logger = logging.getLogger('my_logger') my_handler = logging.StreamНandler() my_formatter = loggi.ng.Fo:z:matter('%(asctime)s -'\\ '%(name)s -%(levelname)s -%(message)s') my_handler.setFo:z:matter(my_formatter) logger.addНandler(my_handler) logger.setLevel(logging.INFO) logger. warning (\"This is а warning message\") logger.info(\"This is an info message\") logger.debug(\"This is а debug message\") Можно создать логгер с теми же настройками, используя метод basicConfig с соот­ ветствующими аргументами. Ниже приведена исправленная версия файла loggingЗ .рус настройками basicConfig: #loggingЗA.py import logging logger = logging. getLogger ( 'my _ logger') logging.ЬasicConfig(handlers =[logging.StreamНandler()], foпnat =\"%(asctime)s -%(name)s - \" \"%(levelname)s -%(message)s\", level=logging.INFO)\n--- Страница 134 ---\nГлава 4. Библиотеки Python для продвинутого программирования logger.warning{\"This is а warning message\") logger.info{\"This is an info message\") logger.debug{\"This is а debug message\") 137 До сих пор мы использовали встроенные классы и объекты для настройки логгеров. Далее рассмотрим логгер с пользовательскими обработчиками и форматтерами. Логгер с файловым обработчиком Обработчик отправляет лог-сообщения в место назначения. По умолчанию каждый регистратор (логгер) настроен на отправку сообщений на консоль или терминал. Это можно изменить, настроив его с новым обработчиком, который отправляет со­ общения в другое место назначения. Файловый обработчик можно создать, исполь­ зуя один из двух подходов, рассмотренных ранее. В этом разделе использован тре­ тий подход для автоматического создания файлового обработчика, используя метод basicconfig с помощью указания имени файла в качестве атрибута. Пример кода по­ казан ниже: #logginq4.py import logging logging.ЬasicConfiq(filename='loqs/logginq4.loq' , level=logginq. DEВUG) logger = logging .getLogger { 'my_logger' ) logger.setLevel{logging.INFO) logger.warning{\"This is а warning message\") logger.info{\"This is а info message\") logger.debug{\"This is а debug message\") Теперь сообщения будут генерироваться в файл, указанный с помощью метода basicConfig, и в соответствии с приоритетом события, для которого установлено значение INFO. Логгер с множеством обработчиков Создать логгер с несколькими обработчиками довольно просто. Это может быть достигнуто либо с помощью метода basicConfig, ·либo путем присоединения обра­ ботчиков у логгеру вручную. Для демонстрации изменим пример loggingЗ.py сле­ дующим образом: 1.Создадим два обработчика ( один для вывода на консоль и один для вывода в файл), которые являются экземплярами классов streamНandler и fileHandler. 2.Создадим два отдельных форматтера по одному для каждого обработчика. Мы не будем включать информацию о времени для форматтера обработчика консоли. 3.Зададим для двух обработчиков разные приоритеты сообщений. Важно пони­ мать, что приоритет на уровне обработчика не может переопределить обработ­ чик корневого уровня.\n--- Страница 135 ---\n138 Полный пример кода: #loggingS.py import logging Раздел 2. Расширенные концепции программирования logger = logging.getLogger('my_logger') logger. setLevel (logging .DEВUG) console_handler = logging.StreamНandler() file_handler = logging.FileНandler(\"logs/loggingS.log\") #установка приоритетов на уровне обработчика console_handler. setLevel(logging.DEB UG) file_handler.setLevel(logging.INFO) #создание отдельных форматтеров для двух обработчиков console_formatter = logging.Formatter( '%(name)s -%(levelname)s -%(message)s') file formatter = logging.Formatter('%(asctime)s -' '%(name)s -%(levelname)s -%(message)s') #добавление форматтеров в обработчик console_handler.setFormatter(console_formatter) file_handler.setFormatter(file_formatter) #добавление обработчиков в логrер logger.addНandler(console_handler) logger.addHandler(file_handler) logger.error(\"This is an error message\") logger.warning(\"This is а warning message\") logger. info ( \"This is an info message\") logger. debug ( \"This is а debug message\") Несмотря на то что мы установили для обработчиков разные приоритеты (шrо и DEBUG), они будут эффективны только в случае, если уровни логгера имеют более низкий приоритет (по умолчанию WARNING). Поэтому нужно в начале программы для логгера установить уровень DEBUG. Тогда приоритету на уровне обработчика может быть присвоено значение DEBUG или более высокое значение. Это важно учитывать при разработке стратегии логирования для приложения. В примерах этого раздела мы настраивали логгер программно (непосредственно в коде программы). Далее попробуем настроить его с помощью файла конфигурации. Настройка логгера с множеством обработчиков с помощью файла конфигурации Программная настройка логгера привлекательна, но непрактична для среды раз­ вертывания (также известна как производственная среда -production environ­ ment; не путайте со средой разработки -development environment). В средах\n--- Страница 136 ---\nГлава 4. Библиотеки Python для продвинутого программирования 139 развертывания есть необходимость настраивать конфигурацию логгера иначе, чем в средах разработки, и иногда приходится повышать уровень логирования для уст­ ранения неполадок, с которыми мы сталкиваемся только в реальных средах. По­ этому есть возможность предоставить конфигурацию логгера через файл, который легко изменить в соответствии с требуемой средой. Конфигурация может быть ли­ бо в виде файлов JSON (JavaScript Object Notation) или У АМL (Yet Another Markup Language), либо в виде списка пар «ключ:зн.ачен.ие» в файле .conf. Для примера покажем конфигурацию логгера с помощью файла У АМL, который не от­ личается от программной реализации в предыдущем примере. Полный файл У AML и код Python показаны далее. Файл конфигурации У АМL: version: 1 formatters: console formatter: format: '%(name)s -%(1evelname)s -%(message)s' file formatter: format: '%(asctime)s -%(name)s -%(1evelname)s - %(message)s' handlers: console handler: class: logging.StreamНandler level: DEBUG formatter: console formatter stream: ext://sys .stdout file handler: class: logging.FileHandler level: INFO formatter: file formatter filename: logs/loggingб.log loggers: my_logger: level: DEBUG handlers: [console_handler, file_handler] propagate: no root: level: ERROR handlers: [console_handler] Код программы Python, которая использует файл У АМL для настройки логгера: #logginqб . ру import logging import logging.config import yaml\n--- Страница 137 ---\n140 Раздел 2. Расширенные концепции программирования with open('loggingб.conf.yaml', 'r') as f: config = yaml.safe_load(f.read()) logging.config.dictconfig(config) logger = logging.getLogger('my_logger') logger.error(\"This is an error message\") logger.warning(\"This is а warning message\") logger.info(\"This is а info message\") logger.debug(\"This is а debug message\") Для загрузки конфигурации из файла мы использовали метод dictConfig вместо basicConfig. Результат получился аналогичным, как при настройке логгера про­ граммно. Для полнофункционального регистратора доступны и другие дополни­ тельные параметры конфигурации. Далее обсудим, какие события следует регистрировать, а какие-нет. Что стоит и не стоит записывать в журнал Однозначного ответа на этот вопрос нет, но в качестве рекомендации для логиро­ вания важна следующая информация: ♦Приложение должно регистрировать все ошибки и исключения; наиболее под­ ходящий для этого способ -ведение журнала событий в исходном мо�ле. ♦Исключения, которые обрабатываются альтернативным потоком кода, могут регистрироваться как предупреждения. ♦ В целях отладки полезной информацией являются входные и выходные данные функций. ♦Также полезно заносить в журнал точки принятия решений, они могут быть по­ лезны для устранения неполадок. ♦Активности и действия пользователей, особенно связанные с доступом к опре­ деленным ресурсам и функциям в приложении, важно регистрировать в целях безопасности и аудита. При регистрации сообщений также важна контекстная информация -время, имя логгера, имя модуля, имя функции, номер строки, уровень логирования и т. д. Эта информация имеет важное значение для анализа причин неполадок. Не следует регистрировать в журнал конфиденциальную информацию, например, идентификаторы пользователей, адреса электронной почты, пароли, а также любые личные и секретные данные. Кроме того, необходимо избегать регистрации слу­ жебной и персональной информации вроде медицинских записей, государственных документов и сведений об организациях.\n--- Страница 138 ---\nГлава 4. Библиотеки Python для продвинутого программирования 141 Заключение В этой главе мы рассмотрели различные темы, связанные с использованием расши­ ренных модулей и библиотек Python. Мы узнали, как создавать и использовать ите­ раторы для перебираемых объектов. Затем рассмотрели генераторы, которые мож­ но использовать и создавать более эффективно, чем итераторы. Кроме того, узнали, как открывать файлы, выполнять чтение и запись данных, а также научились ис­ пользовать контекстный менеджер. В дополнение мы поговорили об основе любого приличного приложения Python, а именно, об обработке ошибок и исключений, вы­ зове исключений в коде и определении пользовательских исключений. В конце мы рассмотрели, как настроить структуру системы логирования, используя различные параметры обработчиков и форматтеров. В следующей главе мы сосредоточимся на создании и автоматизации модульных и интеграционных тестов. Вопросы 1.В чем разница между списком и кортежем? 2.Какой оператор Python всегда используется при работе с менеджером контекста? 3.Зачем нужен оператор else в блоке try-except? 4.Почему лучше использовать генераторы вместо итераторов? 5.В чем польза использования нескольких обработчиков для логирования? Дополнительные ресурсы ♦<<Python. К вершинам мастерства», автор: Лучано Рамалъо («Fluent Python», Luciano Ramalho ). ♦«Advanced Guide to Python 3 Programming>>, автор: Джон Хант (John Hunt). ♦«Стандартная библиотека Python 3. Справочник с примерами», автор: Даг Хеллман («The Python 3 Standard Library Ьу Example», Doug Hellmann). ♦Документация по Python 3. 7.10 (https://docs.python.org/3. 71). ♦Официальная документация Python, ( https://docs.python. org/3/library/logging. config. htm[). Ответы 1.Список является изменяемым объектом, тогда как кортеж -неизменным. Об­ новить список можно после его создания, что недоступно для кортежей.\n--- Страница 139 ---\n142 Раздел 2. Расширенные концепции программирования 2.С менеджером контекста используется оператор with. 3.Блок else выполняется только в случае, если код в блоке try завершается без ошибок. В блоке else можно написать дальнейшие действия, которые необхо­ димо выполнить после завершения работы основного функционала в блоке try без каких-либо проблем. 4.Генераторы эффективны в использовании памяти и более просты в создании, чем итераторы. Генератор автоматически предоставляет экземпляр iterator и реализацию функции next «из коробки». 5.Один обработчик фокусируется на одном месте вывода сообщений, поэтому рас­ пространено использование нескольких обработчиков. Если нужно отправить события журнала на несколько выводов и с разными уровнями приоритета, нам потребуется несколько обработчиков. Также, если необходимо записывать со­ общения в несколько файлов с разными уровнями логирования, можно создать разные обработчики для работы с разными файлами.\n--- Страница 140 ---\n5 Тестирование и автоматизация с помощью Python Тестирование программного обеспечения (ПО) -это проверка приложения или программы на соответствие требованиям пользователя или желаемым специфика­ циям, а также оценка масштабируемости и оптимизации кода. Тестирование реаль­ ными пользователями требует много времени и не является эффективным исполь­ зованием ресурсов. Более того, тесты выполняются не один-два раза, а являются непрерывным процессом. Автоматизация тестирования -набор программ, кото­ рые проверяют поведение приложения с помощью различных сценариев в качестве входных данных для этих программ. Профессиональные среды разработки ПО тре­ буют выполнения автоматизированных тестов после каждого коммита ( фиксации изменений исходного кода) в центральном репозитории. В этой главе мы изучим различные подходы к автоматизации тестирования, а также поговорим о фреймворках и библиотеках для этих целей. Затем остановимся на мо­ дульном тестировании и способах его реализации в Python. Далее обсудим пре­ имущества разработки через тестирования (Test-driven Development, TDD) и правильный способ ее исполнения. В конце мы рассмотрим автоматизированную непрерывную интеграцию (Continuous Integration, CI) и сложности, связанные с ее надежной и эффективной реализацией. Эта глава поможет понять концепции авто­ матизированного тестирования в Python на разных·уровнях. Темы этой главы: ♦Понимание различных уровней тестирования. ♦Работа с тестовыми фреймворками Python. ♦Разработка через тестирование. ♦Автоматизированная непрерывная интеграция. К концу главы вы будете понимать не только различные типы автоматизации тес­ тирования, но также научитесь писать модульные тесты, используя один из двух популярных фреймворков.\n--- Страница 141 ---\n144 Раздел 2. Расширенные концепции программирования Технические требования Для этой главы понадобится: ♦Python 3.7 или более поздней версии. ♦Аккаунт Test PyPI и токен API для этого аккаунта. Примеры кода для этой главы можно найти по адресу: https:/ lgithub. com/PacktPublishing/Python-for-Geeksltreelmaster/Chapter05. Понимание различных уровней тестирования Тестирование выполняется на различных уровнях в зависимости от типа приложе­ ния, уровня его сложности и роли команды, работающей над приложением. К раз­ ным уровням тестирования относятся: ♦модульное тестирование; ♦интеграционное тестирование; ♦системное тестирование; ♦ приемочное тестирование. Эти уровни применяются в следующем порядке (рис. 5.1): Рис. 5.1. Уровни тестирования при разработке ПО Рассмотрим эти уровни подробнее.",
      "debug": {
        "start_page": 108,
        "end_page": 141
      }
    },
    {
      "name": "Глава 5. Тестирование и автоматиза ция с помощью Python 143",
      "content": "--- Страница 142 --- (продолжение)\nГлава 5. Тестирование и автоматизация с помощью Python 145 Модульное тестирование Это тип тестирования, ориентированный на наименьший возможный блок кода (модуль). Им может быть функция в модуле, метод в классе или модуль в приложе­ нии. Такой тест изолированно выполняет один блок кода и проверяет, что тот рабо­ тает должным образом. Это помогает обнаруживать ошибки на ранней стадии раз­ работки и исправлять их. В Python модульные тесты обычно нацелены на отдель­ ные классы или модули без использования зависимостей. Тесты пишутся непосредственно разработчиками и могут выполняться в любое время. Это своего рода тестирование методом белого ящика (White-box Testing). Для этого в Python есть несколько библиотек и инструментов: pyunit (unittest), pytest, doctest, nose и некоторые другие. Интеграционное тестирование Это совместное тестирование отдельных модулей программы в группе на взаимо­ действие между собой и корректный обмен данными. Интеграционное тестирование обычно проводится тестировщиками, а не разработ­ чиками и выполняется после модульного тестирования. Основное внимание уделя­ ется выявлению проблем интеграции, когда различные модули и функции исполь­ зуются вместе. Иногда для тестирования требуются внешние ресурсы или данные, которые недоступны в среде разработки. Тогда на помощь приходит фиктивное тестирование (Mock Testing), которое обеспечивает замену внешним и внутрен­ ним зависимостям. Фиктивные объекты (или тосk-объекты) имитируют поведение реальных зависимостей. Примерами служат отправка электронного письма или оп­ лата кредитной картой. Это своего рода тестирование методом черного ящика (Black-box Testing). Ис­ пользуемые библиотеки и инструменты почти такие же, как в модульном тестиро­ вании, с той лишь разницей, что границы тестов расширяются и включают не­ сколько модулей в один тест. Системное тестирование Границы системного тестирования расширяются до уровня системы. Это может быть полноценный модуль или приложение. Здесь проверяется функциональность с точки зрения сквозного тестирования (End-to-End, Е2Е), то есть от начала и до конца. Системные тесты также разрабатываются тестировщиками, но уже после заверше­ ния процесса интеграционного тестирования. Можно сказать, что инtеграционное тестирование является необходимым условием системного тестирования, иначе придется делать много лишних действий. На данном этапе можно выявить потен­ циальные проблемы, но не наверняка определить их местоположение. Точная при-\nГлава 5. Тестирование и автоматизация с помощью Python 145 Модульное тестирование Это тип тестирования, ориентированный на наименьший возможный блок кода (модуль). Им может быть функция в модуле, метод в классе или модуль в приложе­ нии. Такой тест изолированно выполняет один блок кода и проверяет, что тот рабо­ тает должным образом. Это помогает обнаруживать ошибки на ранней стадии раз­ работки и исправлять их. В Python модульные тесты обычно нацелены на отдель­ ные классы или модули без использования зависимостей. Тесты пишутся непосредственно разработчиками и могут выполняться в любое время. Это своего рода тестирование методом белого ящика (White-box Testing). Для этого в Python есть несколько библиотек и инструментов: pyunit (unittest), pytest, doctest, nose и некоторые другие. Интеграционное тестирование Это совместное тестирование отдельных модулей программы в группе на взаимо­ действие между собой и корректный обмен данными. Интеграционное тестирование обычно проводится тестировщиками, а не разработ­ чиками и выполняется после модульного тестирования. Основное внимание уделя­ ется выявлению проблем интеграции, когда различные модули и функции исполь­ зуются вместе. Иногда для тестирования требуются внешние ресурсы или данные, которые недоступны в среде разработки. Тогда на помощь приходит фиктивное тестирование (Mock Testing), которое обеспечивает замену внешним и внутрен­ ним зависимостям. Фиктивные объекты (или тосk-объекты) имитируют поведение реальных зависимостей. Примерами служат отправка электронного письма или оп­ лата кредитной картой. Это своего рода тестирование методом черного ящика (Black-box Testing). Ис­ пользуемые библиотеки и инструменты почти такие же, как в модульном тестиро­ вании, с той лишь разницей, что границы тестов расширяются и включают не­ сколько модулей в один тест. Системное тестирование Границы системного тестирования расширяются до уровня системы. Это может быть полноценный модуль или приложение. Здесь проверяется функциональность с точки зрения сквозного тестирования (End-to-End, Е2Е), то есть от начала и до конца. Системные тесты также разрабатываются тестировщиками, но уже после заверше­ ния процесса интеграционного тестирования. Можно сказать, что инtеграционное тестирование является необходимым условием системного тестирования, иначе придется делать много лишних действий. На данном этапе можно выявить потен­ циальные проблемы, но не наверняка определить их местоположение. Точная при-\n--- Страница 143 ---\n146 Раздел 2. Расширенные концепции программирования чина обычно определяется интеграционным тестированием или даже добавлением дополнительных модульных тестов. Системное тестирование также относится к типу тестирования методом черного ящика и может использовать те же библиотеки, что и интеграционное. Приемочное тестирование Приемочное тестирование выполняется реальным пользователем перед приемкой программы для повседневного использования Оно также известно, как ИАТ­ тестирование (User Acceptance Testing). Обычно для таких тестов не применяют автоматизацию, но ее стоит использовать в ситуациях, когда пользователям прихо­ дится взаимодействовать с продуктом через API. Этот тип тестирования легко спу­ тать с системным, но он отличается тем, что обеспечивает удобство использования приложения с точки зрения реальных пользователей. Существуют т�кже два до­ полнительных типа приемочного тестирования: заводские приемочные испытания (Factory Acceptance Testing, FAT) и эксплуатационное приемочное тестирование (Operational Acceptance Testing, ОАТ). Первый более популярен с точки зрения аппаратного обеспечения, а второй выполняется рабочими командами, отвечаю­ щими за использование продукта в производственных средах. Вы также могли слышать про альфа-и бета-тестирование, которые выполняются на уровне пользователей и не автоматизируются. Альфа-тестирование выполняется разработчиками и другим персоналом для имитации реального поведения пользо­ вателей. Бета-тестирование выполняется клиентами или настоящими пользовате­ лями для получения обратной связи о продукте, прежде чем он будет выпущен в общий доступ. Кроме того, существует регрессионное тестирование. Оно выполняется всякий раз, когда вносятся изменения в исходный код или любые внешние и внутренние зави­ симости. Эта практика гарантирует, что продукт работает так же, как и до внесения изменений. Поскольку регрессионное тестирование повторяется многократно, его автоматизация необходима. Далее рассмотрим, как создавать тест-кейсы (test case -тестовый случай, тесто­ вый сценарий, тест) с использованием тестовых фреймворков. Работа с тестовыми фреймворками Python Для Python есть стандартные и сторонние библиотеки для автоматизации тестиро­ вания. Самые популярные из них: ♦pytest; ♦unittest; ♦doctest; ♦nose.\n--- Страница 144 ---\nГлава 5. Тестирование и автоматизация с помощью Python 147 Эти фреймворки подходят как для модульного, так и для интеграционного и сис­ темного тестирования. Мы рассмотрим два из них: unittest, который является ча­ стью стандартной библиотеку Python, и pytest, который доступен как сторонняя библиотека. Основное внимание в главе будет уделено тестовым примерам (по большей части модульным) с использованием этих двух библиотек. Но помните, что интеграционное и системное тестирование также могут быть построены с ис­ пользованием этих библиотек и шаблонов проектирования. Прежде чем начать писать тесты, сначала важно понять, что это такое. В контексте этой главы и книги в целом мы будем определять тест-кейс, как способ проверки результатов определенного поведения кода в соответствии с ожидаемыми резуль­ татами. Разработку тестов можно разделить на четыре этапа: 1.Подготовка: на этом этапе мы подготавливаем среду для тестов. Здесь не вы­ полняется никаких действий или проверок. В сообществе этот этап известен как подготовка тестовых фикстур (Test Fixture) -приспособлений для проверки. 2.Действие: здесь мы запускаем тестируемую систему. Этап действия приводит к изменениям системы. А измененное состояние -это именно то, что мы хотим оценить в рамках тестирования. Обратите внимание, здесь мы по-прежнему ни­ чего не проверяем. 3.Проверка: здесь мы оцениваем результаты предыдущего этапа и сравниваем их с ожидаемыми результатами. На основе этого тест-кейс помечается как успеш­ ный или неудачный. В большинстве инструментов проверка достигается с по­ мощью встроенных функций или операторов assert. 4.Очистка: на этом этапе среда очищается. Нужно убедиться, что внесенные из- менения на этапе действия не влияют на другие тесты. Ключевые этапы -действие и проверка. Подготовка и очистка являются необя­ зательными, но настоятельно рекомендуются. Они обеспечивают подготовку фик­ стур -оборудования, устройств или ПО, которые предоставляют среду для после­ довательного тестирования продукта. Термин «тестовая фикстура» используется в том же контексте для модульного и интеграционного тестирования. Фреймворки или библиотеки предоставляют вспомогательные методы или опера­ торы для удобной реализации этих этапов. Далее мы будем оценивать фреймворки / . . unittest и pytest по следующим аспектам: ♦Как создавать базовые тест-кейсы для этапов действия и проверки. ♦Как создавать тест-кейсы с помощью фикстур. ♦Как создавать тест-кейсы для проверки исключений и ошибок. ♦Как запускать тест-кейсы в большом количестве. ♦Как включать и исключать тест-кейсы во время выполнения. Эти темы затрагивают не только создание разных тестовых сценариев, но и спосо­ бы их выполнения. Начнем знакомство с unittest.\n--- Страница 145 ---\n148 Раздел 2. Расширенные концепции программирования Работа с фреймворком unittest Перед обсуждением практических примеров с unittest рассмотрим некоторые тер­ мины и методы, связанные с модульным тестированием и с самой библиотекой в частности. Терминология ниже используется почти во всех библиотеках: ♦Тест-кейс: тест, тестовый сценарий или тестовый случай -набор инструкций кода, основанных на сравнении текущего состояния единицы кода с состоянием после ее выполнения. ♦Тестовый набор: это набор тестовых сценариев, которые могут иметь общие предварительные условия, этапы инициализации и очистки; это способствует повторному использованию кода автоматизации и сокращает время выполнения. ♦Исполнитель тестов: приложение Python, которое выполняет тесты (модуль­ ные), выполняет все проверки, определенные в коде, и возвращает результат как успешный или неудачный. ♦setUp: специальный метод, который. выполняется перед каждым тестом. ♦setupclass: специальный метод, который выполняется однократно перед началом выполнения тестов из набора. ♦teardown: специальный метод, который выполняется после завершения каждого теста независимо от его результата. ♦teardownClass: специальный метод, который выполняется один раз после завер- шения всех тестов в наборе. Для написания тестовых сценариев необходимо реализовать их как методы экземп­ ляра класса, который должен наследоваться от базового класса тestcase. Класс тestcase имеет несколько методов, которые упрощают написание и выполнение тес­ тов. Эти методы сгруппированы в три категории: ♦Методы, связанные с выполнением: setUp, tearDown, setupClass, teardownClass, run, skipTest, skipTestif, subTest и debug; они задействуются исполнителем для вы­ полнения кода до или после тест-кейсов, запуска теста, пропуска теста или вы­ полнения блока кода как подтеста; в классе реализации теста эти методы можно переопределять; подробнее можно узнать в официальной документации по адре­ су https://docs.python.org/ЗЛibrary/unittest.html. ♦Методы проверки: используются для реализации тестов, которые проверяют условия успешного или неудачного исхода и возвращают результат автоматиче­ ски; имена таких методов обычно начинаются с префикса assert; список таких методов очень длинный, на рис. 5.2 приведены самые часто используемые. ♦Методы и атрибуты, связанные с дополнительной информацией: предостав­ ляют дополнительную информацию о тестовых сценариях, которые должны быть или уже выполнены; ниже приведены несколько ключевых методов и ат­ рибутов: •Атрибут--failureException: предоставляет исключение, вызванное тестовым методом, которое можно использовать как суперкласс для определения поль­ зовательского исключения с дополнительной информацией.\n--- Страница 146 ---\nГлава 5. Тестирование и автоматизация с помощью Python 149 •Атрибут longMessage: определяет, что делать с пользовательским сообщени­ ем, которое передается как аргумент с методом assert; если значение аргу­ мента тrue, сообщение добавляется к стандартному сообщению об ошибке; если значение аргумента false, пользовательское сообщение заменяет стан­ дартное. •Метод countTestcases о: возвращает количество тестов, прикрепленных к тес­ товому объекту. •Метод shortDescription (): возвращает описание тестового сценария, если та­ ковое добавлено, используя docstring. Имя метода Проверка условий assertEqual (х, у) Делает проверку на равенство х и у ., assertT гuе (х) Делает проверку на равенство х булевому значению true assertFalse (х) Делает проверку на равенство х булевому значению false assertNotEqual (х) Делает проверку на неравенство х и у assert (а, с) Делает проверку на наличие элемента а в коллекции с assertNotln (а, с) Делает проверку на отсутствие элемента а в коллекции с assert!s (х, у) Делает проверку на соответствие объекта х объекту у assert!sNot (х, у) Делает проверку на несоответствие объекта х объекту у assert!sNone (х) Делает проверку на_равенство объекта значению None Рис. 5.2. Примеры методов assert ·класса TestCase В этом разделе мы рассмотрели основные методы класса тestcase. Далее остано­ вимся на создании модульных тестов с помощью unittest. Создание тестов с помощью базового класса TestCase Библиотека unittest -это стандартный фреймворк тестирования Python, разработ­ чики которого были вдохновлены JUnit (популярный фреймворк в Java). Модуль­ ные тесты пишутся в отдельных файлах, и их следует подключать к основному про­ екту. В подразделе «Сборка пакетов» в главе 2 («Использование модулей для слож-\n--- Страница 147 ---\n150 Раздел 2. Расширенные концепции программирования ных проектов») уже упоминалось, что сообщество РуРА рекомендует иметь от­ дельную папку для тестов при создании пакетов в проекте. В примерах кода для этого раздела мы будем следовать следующей структуре Project-name 1--src 1 --_init_.py 1 --myadd/myadd.py 1--tests 1 --_init_.py 1 --tests_myadd/test_myaddl.py 1 --tests_myadd/test_myadd2.py 1--READМE.md В первом примере создадим набор тестов для функции add в модуле myadd. ру: #myadd.py со сложением двух чисел def add(x, у): \"'\"'Эта функция складывает 2 числа\"\"\" return х + у Важно понимать, для одного фрагмента кода (в нашем случае для функции add) может быть несколько тестовых сценариев. Для этой функции мы реализовали че­ тыре теста, меняя значения входных параметров, как показано в примере: #test_myaddl.py набор тестов для функции myadd import unittest from myunittest.src.myadd.myadd import add class МyAddTestsuite(unittest.Testcase): def test_addl (self) : \"\"\"тест для проверки сложения двух положительных чисел\"\"\" self.assertEqual(15, add(lO , 5), \"should Ье 15\") def test_add2(self): \"\"\"тест для проверки сложения положительного и отрицательного чисел\"\"\" self.assertEqua1(5, add(lO, -5), \"should Ье 5\") def test_addЗ(self): \"\"\"тест для проверки сложения отрицательного и положительного чисел\"\"\" self.assertEqual(-5, add(-10, 5), \"should Ье -5\") def test_add4(self): \"\"\"тест для проверки сложения двух отрицательных чисел\"\"\" self.assertEqual(-15, add(-10, -5), \"should Ье -15\") if name ==' main unittest .main ()\n--- Страница 148 ---\nГлава 5. Тестирование и автоматизация с помощью Python 151 Ключевые моменты кода: 1.Для реализации модульных тестов через unittest нужно импортировать стан­ дартную библиотеку с тем же именем. 2.Модули, которые нужно протестировать в наборе, также необходимо импорти­ ровать. В нашем случае мы импортировали функцию add из модуля myadd.py с помощью относителыюго импорта (подробности в подразделе «Импорт моду­ лей» в главе 2 «Использование модулей для сложных проектов»). 3.Мы реализуем класс тестовых наборов, который наследуется от базового класса unittest. тestcase. Сценарии реализуются в подклассе MyAddTestSuite. Конструктор класса unittest. Testcase может принимать имя метода в качестве входных дан­ ных, которые можно использовать для выполнения тест-кейсов. По умолчанию уже реализован метод runTest, с помощью которого исполнитель запускает тес­ ты. в большинстве случаев не нужно предоставлять собственный метод или по� вторно реализовывать метод runтest. 4.Для реализации тестового сценария нужно написать метод, который начинается с префикса test и символа подчеркивания. Это помогает исполнителю находить нужные сценарии для выполнения. Руководствуясь соглашением об именова­ нии, мы добавили в набор четыре метода. 5.В каждом тестируемом методе мы использовали специальный метод assertEqual, доступный из базового класса, который представляет этап проверки и позволяет определить, результат теста будет успешным или неудачным. Первый параметр этого метода -ожидаемый результат теста. Второй параметр -значение, по­ лучаемое после выполнения тестируемого кода. Третий параметр (необязатель­ ный) -сообщение, которое будет предоставлено в отчете в случае неудачи теста. 6.В конце набора мы добавили метод unittest.main для запуска исполнителя тес­ тов. Тот, в свою очередь, необходим для запуска метода runтest, который упро­ щает выполнение без использования консольных команд. Этот mаin-метод (с классом TestProgram внутри) сначала обнаружит все тесты, которые нужно вы­ полнить, а затем выполнит их. ВАЖНОЕ ПРИМЕЧАНИЕ Модульные тесты можно выполнять с помощью команды Python -m unittest <тесто­ вый набор или модуль>, но примеры кода в этой главе предполагают, что мы выполня­ ем сценарии, используя интегрированную среду разработки PyCharm. Далее рассмотрим следующий уровень сценариев с использованием тестовых фик­ стур. Создание тестов с помощью тестовых фикстур Мы рассмотрели методы setUp и tearDown, которые запускаются автоматически ис­ полнителями до и после выполнения сценариев. Эти методы (наряду с setUpClass и tearDownClass) предоставляют тестовые фикстуры и полезны для эффективной реа­ лизации модульных тестов.\n--- Страница 149 ---\n152 Раздел 2. Расширенные концепции программирования Для начала пересмотрим реализацию функции add и сделаем этот фрагмент кода частью класса MyAdd. Мы также реализуем исключение TypeError, если входные ар­ гументы будут недопустимыми. Ниже приведен полный фрагмент кода с новым методом add: #myadd2.py -класс с методом сложения двух чисел class МyAdd: def add(self, х, у): \"\"\"Эта функция складывает 2 числа\"\"\" if (not isinstance (х, (int, float))) 1 \\ (not isinstance (у, (int, float))) raise ТypeError(\"only numЬers are allowed\") return х + у Ранее мы создали тест-кейсы, используя только этапы действия и проверки. Теперь изменим предыдущий пример, добавив методы setUp и tearDown. Ниже приведен на­ бор тестов для класса myAdd: #test_myadd2.py набор тестов для метода класса myadd2 import unittest from myunittest.src.myadd.myadd2 import MyAdd class МyAddТestsuite(unittest.TestCase): def setUp(self): self. myadd = MyAdd () def tearDown(self): del (self .myadd) def test_addl (self) : \"\"\"тест для проверки сложения двух положительных чисел\"\"\" self.assertEqual(15, self.myadd.add(lO, 5), \"should Ье 15\") def test_add2 (self): \"\"\"тест для проверки сложения положительного и отрицательного чисел\"\"\" self.assertEqual(5, self.myadd.add(lO , -5), \"should Ье 5\") #test addЗ и test add4 пропущены, они совпадают с test addl и test add2 В этом наборе тестов мы добавили или изменили следующие элементы : 1.Мы добавили метод setup, в котором создали новый экземпляр класса MyAdd и со­ хранили указатель на него в качестве атрибута экземпляра. Это означает, что мы будем создавать новый экземпляр класса MyAdd до выполнения тестового случая. Возможно, это не идеальное решение для этого сценария, и было бы лучше ис­ пользовать метод setUpClass и создать один экземпляр класса MyAdd для всего набора тестов, но мы создали эту реализацию в целях демонстрации.\n--- Страница 150 ---\nГлава 5. Тестирование и автоматизация с помощью Python 153 2.Мы также добавили метод tearDown. Для демонстрации, как его реализовать, мы просто вызвали деструктор (используя функцию del) экземпляра MyAdd, который мы создали в методе setup. В отличие от метода setup, метод tearDown выполняет­ ся после каждого тестового случая. Если бы мы использовали метод setupClass, для него существует tearDownClass, эквивалент tearDown. Далее мы рассмотрим примеры кода, в которых создадим тестовые случаи для об­ работки исключения TypeError. Создание тестов с обработкой ошибок В предыдущих примерах мы просто сравнивали результаты тестов с ожидаемыми выводами. Мы не рассматривали поведение программы в сценариях с обработкой исключений, когда входные аргументы для функции add могут быть недопустимого типа. Модульные тесты должны учитывать такие аспекты. Поэтому создадим такие тест-кейсы для обработки ожидаемых ошибок или исклю­ чений. В примере будем использовать ту же функцию add, которая выдает исклю­ чение TypeError, если аргумент не является числом. Тестовые случаи будут созданы передачей нечисловых аргументов в функцию add. Ниже приведен фрагмент кода: #test_myadd.3.py набор тестов для метода класса rnyadd2 для проверки ошибок irnport unittest frorn rnyunittest.src.rnyadd.rnyadd2 irnport MyAdd class MyAddTestSuite(unitte st.TestCase): def setup (self) : self.rnyadd = MyAdd() def test_typeerrorl (self): \"\"\"тест для проверки, можем ли мы обработать нечисловые вводные\\ данные\"\"\" self.assertRaises(ТypeError, self.myadd.add, \\ 'а' , -5) def test_typeerror2(self ): \"\"\"тест для проверки, можем ли мы обработать нечисловые вводные\\ данные\"\"\" self.assertRaises(ТypeError, self.myadd.add, \\ 'а' , 'Ь') Мы добавили два дополнительных тестовых сценария в модуль test_addЗ.py. Они используют метод assertRaises для проверки, выдается исключение определенного типа или нет. В качестве аргументов для сценариев мы использовали одну букву (а) и две буквы (а и ь). В обоих случаях ожидается предполагаемое исключение TypeError. Обратите внимание на аргументы метода assertRaises. В качестве второго аргумента этот метод ожидает только имя метода или функции, параметры которых должны передаваться отдельно в качестве аргументов функции assertRaises.\n--- Страница 151 ---\n154 Раздел 2. Расширенные концепции программирования До сих пор мы выполняли несколько тестов в одном наборе. Теперь обсудим, как можно запускать несколько тестовых наборов одновременно как программно, так и с помощью командной строки. Выполнение нескольких наборов тестов По мере написания сценариев для каждой единицы кода число модульных тестов быстро растет. Использование тестовых наборов помогает привнести в разработку тестов модульность. Это также упрощает поддержку и расширение продукта в свя­ зи с расширением функциональности приложения. Инструменты непрерывной ин­ теграции, такие, как Jenkins, прямо «из коробки» предоставляют возможность за­ пускать множество тестовых наборов в основном скрипте. Фреймворки unittest, nose или pytest также обладают подобным функционалом. для· примера создадим простое приложение калькулятора (класс MyCalc) с методами add, subtract, multiply и divide. Позже добавим по одному тестовому набору для каж­ дого метода в этом классе. Таким образом, получится четыре набора тестов для приложения. При создании наборов и сценариев в них важно соблюдать структуру каталогов. В нашем приложении она будет следующей (рис. 5.3): v • myunittest ••sгс ► • myadd т • mycalc r':, mycak:.py \" • tests ► • tests_myadd \" • tests_mycalc r':, mycalctestrunner.py r':, test_mycalc_add.py r':, test_mycalc_divide.py r':, test_mycalc_multiply.py i':, test_mycalc_suЫract.py Рис. 5.3. Структура каталогов для приложения mycalc и наборов тестов Код Python содержится в модуле mycalc.py, а файлы с наборами тестов (test mycalc* .ру) представлены ниже. Обратите внимание, что в следующих приме­ рах iаглядно приводится только один тестовый сценарий из каждого набора. В ре­ альности же их будет несколько. Начнем с функций калькулятора в файле mycalc.py: #mycalc.py с функциями add, subtract, multiply и divide class MyCalc: def add(self, х, у): \"'\"'Эта функция выполняет сложение двух чисел\"\"\" return х + у\n--- Страница 152 ---\nГлава 5. Тестирование и автоматизация с помощью Python def suЬtract(self, х, у): \"\"\" Эта функция выполняет вычитание двух чисел\"\"\" return х -у def multiply(self, х, у): \"\"\" Эта функция выполняет умножение двух чисел\"\"\" return х * у def divide(self, х, у): \"\"\" Эта функция выполняет деление двух чисел\"\"\" return х / у Далее идет набор тестов для функции add в файле test_mycalc_add.py: #test_mycalc_add.py набор тестов для метода add import unittest from myunittest.src.mycalc.mycalc import MyCalc class МyCalcAddTestsuite(unittest.TestCase): def setUp (self) : self.calc = MyCalc() def test_add(self): \"\"\"тест для проверки двух положительных чисел'\"\"' self.assertEqual(l5, self.calc.add(lO, 5), \"should Ье 15\") Потом идет набор тестов для функции suЬstract в файле test_mycalc_suЬstract.py: #test_mycalc_suЬtract.py набор тестов для метода subtract import unittest from myunittest.src.mycalc.mycalc import MyCalc class МyCalcSuЬtractTestsuite(unittest.TestCase): def setUp (self) : self.calc = MyCalc() def test_suЬtract(self): \"\"\"тест для проверки двух положительных чисел\"\"\" self.assertEqual(5, self.calc.suЬtract(l0,5), \"should Ье 5\") Затем идет набор тестов для функции multiply в файле test_mycalc_multiply.py: #test_mycalc_multiply.py набор тестов для метода multiply import unittest from myunittest.src.mycalc.mycalc import MyCalc 155\n--- Страница 153 ---\n156 Раздел 2. Расширенные концепции программирования class МyCalcМultiplyТestSuite(unittest.TestCase): def setUp (self) : self.calc = MyCalc() def test_multiply(self): \"\"\"тест дпя проверки двух положительных чисел\"\"\" self.assertEqual(50, self.calc.multiply(lO, 5), \"should Ье 50\") И, наконец, набор тестов для функции divide в файле test_mycalc_divide. ру: #test_mycalc_divide.py набор тестов дпя метода divide import unittest from myunittest.src.mycalc.mycalc import MyCalc class МyCalcDivideTestSuite(unittest.TestCase): def setUp(self): self.calc = MyCalc() def test_divide (self): \"\"\"тест дпя проверки двух положительных чисел\"\"\" self.assertEqual(2, self.calc.divide(lO , 5), \"should Ье 2\") Теперь у нас есть код приложения из всех четырех наборов тестов. Следующий шаг: как запустить все тесты за один раз. Один из простых способов -с помощью интерфейса командной строки (Command-Line Interface, CLI) с ключевым сло­ вом discover. В нашем примере мы выполним команду ниже для нахождения и вы­ полнения всех сценариев во всех наборах тестов в каталоге tests _ mycalc: python -m unittest discover myunittest/tests/tests_mycalc Эта команда будет выполняться рекурсивно и сможет находить тесты в подкатало­ гах. Другие необязательные параметры, приведенные ниже, позволяют выбирать тестовые наборы для выполнения: ♦-v: делает вывод подробным; ♦-s: задает начальный каталог для поиска тестов; ♦-р: шаблон для поиска тестовых файлов; по умолчанию используется test* .ру, но данный параметр позволяет это значение изменить; ♦-t: указывает на каталог проекта верхнего уровня; если ничего не указано, на- чальный каталог считается каталогом верхнего уровня. Возможность запускать несколько наборов из командной строки очень проста и эффективна, но иногда нужно контролировать процесс запуска опреде.Jiенных тес­ тов из разных наборов, которые к тому же могут быть в разных директориях. Это удобно делать именно с помощью Python. Ниже приведен код, как можно загру-\n--- Страница 154 ---\nГлава 5. Тестирование и автоматизация с помощью Python 157 жать наборы тестов от имени класса, находить тест-кейсы в каждом из наборов и запускать их с помощью исполнителя unittest: import unittest from test_mycalc_add import MyCalcAddTestSuite from test_mycalc_subtract import MyCalcSubtractTestSuite from test_mycalc_multiply import MyCalcMultiplyTestSuite from test_mycalc_divide import MyCalcDivideTestSuite def run_mytests(): test classes = [MyCalcAddTestSuite, MyCalcSuЬtractTestSuite,\\ MyCalcMultiplyTestSuite,MyCalcDivideTestSuite loader = unittest.TestLoacler() test _ suites = [] for t class in test classes: suite = loader.loadTestsFranТestcase(t_class) test_suites.append(suite) final_suite = unittest.TestSuite(test_suites) runner = unittest.TextTestRunner() results = runner.run(final_suite) if name ==' main run _ mytests () Мы познакомились с библиотекой unittest. Теперь перейдем к созданию тестовых сценариев с помощью библиотеки pytest. Фреймворк тестирования pytest Тестовые сценарии, написанные с помощью unittest, легче читаются и управляют­ ся, особенно если вы уже работали с Юnit или аналогичными платформами. Но для крупномасштабных приложений Python библиотека pytest предпочтительнее из-за простоты реализации и способности адаптироваться под сложные требования тес­ тирования. В случае с pytest нет необходимости расширять класс модульного теста из любого базового класса. Можно писать тест-кейсы без реализации каких-либо классов. Данный фреймворк имеет открытый исходный код. Он может автоматически нахо­ дить тесты для выполнения, подобно unittest, если имя файла имеет префикс test. И такое поведение можно настраивать. Фреймворк pytest предлагает функционал, подобный unittest, поэтому сосредоточимся на функциях, которые отличаются или ДОПОЛНЯЮТ платформу.\n--- Страница 155 ---\n158 Раздел 2. Расширенные концепции программирования Создание тестов без базового класса Для демонстрации пересмотрим модуль myadd2 .ру, реализуя функцию add без класса. Эта новая функция сложит два числа и выдаст исключение, если в качестве аргу­ мента получит нечисловое значение. Ниже приведен пример использования pytest: # myaddЗ.py -класс с методом сложения двух чисел def add(self, х, у): \"\"\"Эта функция складывает 2 числа\"\"\" if (not isinstance (х, (int, float))) 1 not isinstance (у, (int, float))) : raise ТypeError ( \"only numЬers are allowed\") return х +_у А также модуль тестовых сценариев : #test_myaddЗ.py тестовый набор для функции myadd import pytest from mypytest.src .myaddЗ import add def test_addl (): \"\"\"тест для проверки двух положительных чисел\"\"\" assert add(lO, 5) = 15\" def test_add2 (): 111111 тест для проверки двух положительных чисел assert add(lO, -5) = 5, \"should Ье 5\" Мы привели только два теста для test_myaddЗ.py, поскольку другие сценарии будут аналогичными. Их можно найти в каталоге исходного кода к этой главе на GitHub. Несколько ключевых отличий в реализации тестового сценария: ♦Нет необходимости создавать тесты в классе; можно реализовать их как методы класса без наследования от базового класса; это ключевое отличие от библиоте­ ки unittest. ♦Операторы assert доступны в качестве ключевого слова для проверки любых условий с целью определения результата теста (успех или неудача); отделение ключевого слова assert от условных операторов делает проверки в тестовых сценариях гибкими и настраиваемыми. Также следует отметить , фреймворк pytest делает консольный вывод и отчеты эффективнее. Пример консольного вывода после выполнения тестов в модуле test _ myaddЗ. ру: test_myaddЗ.py: :test_addl PASSED test_myaddЗ.py: :test_add2 PASSED [25%] [50%]\n--- Страница 156 ---\nГлава 5. Тестирование и автоматизация с помощью Python 159 test_ myaddЗ .ру:: test_addЗ PASSED [75%] test_myaddЗ.py: :test_add4 PASSED [100%] ========= 4 passed in 0.03s ========== Далее рассмотрим, как с помощью pytest проверять ожидаемые ошибки. Создание тестов с обработкой ошибок Создание тест-кейсов для проверки ожидаемого исключения в pytest и unittest от­ личаются. Фреймворк pytest использует менеджер контекста для проверки исклю­ чений. В модуле test_myaddЗ.py мы уже добавили два тестовых примера для провер­ ки исключений. Фрагмент кода приведен ниже: def test_ typeerrorl () : \"\"\"тест для проверки, можем ли мы обработать нечисловое значение\"\"\" with pytest.raises(TypeError): add( 'а', 5) def test_typeerror2(): \"\"\"тест для проверки, можем ли мы обработать нечисловое значение\"\"\" with pytest.raises(TypeError, match=\"only numЬers are allowed\"): add ( ' а ' , ' Ь ' ) Для проверки исключения мы используем функцию raises с целью указать, какое исключение ожидается при запуске определенного фрагмента кода (в нашем пер­ вом тестовом примере add ('а', s) ). Во втором сценарии мы использовали аргумент match для проверки сообщения, которое устанавливается при возникновении ис­ ключения. Далее обсудим использование маркеров с фреймворком pytest. Создание тестов с маркерами pytest Фреймворк pytest оснащен маркерами, которые позволяют прикреплять метадан­ ные или определять категории для наших тестов. Метаданные могут использовать­ ся для многих целей, например, для включения или исключения определенных тес­ товых сценариев. Маркеры реализуются с помощью декоратора @pytest.mark. В pytest есть несколько встроенных маркеров, самые популярные из них представ­ лены ниже: ♦skip: при использовании этого маркера исполнитель тестов пропустит сценарий при любых условиях; ♦skipif: тест может быть пропущен в зависимости от условного выражения, кото­ рое передается этому маркеру как аргумент; ♦xfail: ожидаемое неудачное завершение тестового случая будет проигнорирова­ но; используется с определенным условием;\n--- Страница 157 ---\n160 Раздел 2. Расширенные концепции программирования ♦parametrize: используется для выполнения нескольких вызовов тестового сцена- рия с разными значениями аргументов. Для демонстрации работы первых трех маркеров добавим их в модуль test_add3.py и перепишем пример. Новый тестовый модуль (test_add4 .ру) будет таким: @pytest.mark.skip def test _addl () : \"\"\"тест для проверки двух положительных чисел\"\"\" assert add(lO, 5) == 15 @pytest.mark .skipif(sys.version_info > (3,6), \\ reason=\" skipped for release > than Python 3.6\") def test_add2 (): \"\"\"тест для проверки двух положительных чисел\"\"\" assert add(lO, -5) == 5, \"should Ье 5\" @pytest.mark.xfail(sys.platform == \"win32\", \\ reason =\"ignore exception for windows\") def test_add3 (): \"\"\"тест для проверки двух положительных чисел\"\"\" assert add(-10, 5) == -5 raise Exception() Первый тест будет проигнорирован, так как мы использовали для него маркер skip без указания дополнительных условий. Для второго теста мы использовали маркер skipif с условием, что версия Python должна быть выше 3.6. Для последнего теста мы намеренно вызвали исключение и использовали маркер xfail. Этот тип исклю­ чения будет проигнорирован, если операционной системой является Windows. Дан­ ный маркер полезен для игнорирования ошибок, если они ожидаются для опреде­ ленного условия, например, для операционной системы. Консольный вывод будет следующий: test_myadd4.py: :test_addl SКIPPED (unconditional skip) Skipped: unconditional skip test_myadd4.py::test_add2 SКIPPED ( skipped for release > than [33%] Pytho ) [66%] Skipped: skipped for release > than Python 3.6 test_myadd4.py::test_add3 XFAIL (ignore exception for mac) @pytest.mark.xfail(sys.platform = \"win32\", [100%] reason=\"ignore exception for mac\") ===== = 2 skipped, 1 xfailed in 0.06s ======= Теперь рассмотрим использование маркера parametrize из библиотеки pytest.\n--- Страница 158 ---\nГлава 5. Тестирование и автоматизация с помощью Python 161 Создание тестов с параметризацией В предыдущих примерах мы создавали тестовые сценарии без передачи им каких­ либо параметров, но часто возникает необходимость запустить один и тот же тест, изменив входные данные. В классическом подходе мы вьmолняем несколько сце­ нариев, которые отличаются только входными данными. Именно так мы делали в test_myaddЗ.py. Рекомендуемым подходом в таких ситуациях является тестирова­ ние на основе данных (Data-driven Testing, DDT), при котором тестовые данные предоставляются через словари или таблицы (tаЫе или spreadsheet). Такой подход также носит название табличное тестирование или параметризированное тести­ рование. Данные, предоставленные через таблицу или словарь, используются для выполнения тестов с помощью стандартной реализации исходного кода теста. Та­ кой подход удобен в сценариях, когда нужно протестировать функционал, исполь­ зуя перестановку входных параметров. Вместо создания тестов для каждой пере­ становки мы можем предоставить их в формате таблицы или словаря и использо­ вать в качестве входных данных для нашего единственного тестового примера. Фреймворки, вроде pytest, будут выполнять тест столько раз, сколько перестановок содержится в таблице или словаре. Примером может служить поведение функции авторизации, в которую подставляется множество допустимых и недопустимых учетных данных пользователей. В pytest тестирование на основе данных можно реализовать, используя параметри­ зацию с помощью маркера. Маркером parametrize можно определить, какой вход­ ной аргумент надо передать, а также набор тестовых данных, который нужно ис­ пользовать. Фреймворк pytest автоматически выполнит тест множество раз в соот­ ветствии с количеством записей в тестовых данных, предоставленных маркером parametri ze. Для демонстрации изменим файл myadd4. ру, в котором будет только один тестовый пример, но данные для входных параметров будут разными: # test_myaddS.py набор тестов с маркером parameterize import sys import pytest from mypytest.src.myaddЗ import add @pytest.mark.parametrize (\"х,у ,ans\", [ (10, 5, 15), (10,-5, 5), (-10,5,-5), (-10,-5,-15)], ids=[\"pos-pos\" ,\"pos-neg\" , \"neg-pos\" , \"neg-neg\"] ) def test_add (x, у, ans}: \"\"\"тест для проверки двух положительных чисел\"\"\" assert add(x, у} == ans\n--- Страница 159 ---\n162 Раздел 2. Расширенные концепции программирования Для маркера parametrize мы указали три параметра, которые можно описать сле­ дующим образом: ♦Аргументы для тест-кейса: мы предоставили список аргументов, которые должны быть переданы в тестовую функцию в порядке, определенном в ней; тестовые данные, которые необходимо указать в следующем аргументе, также должны следовать в этом порядке. ♦Данные: тестовые данные представлены в виде списка различных наборов входных аргументов; количество записей в данных будет определять, сколько раз выполнится тест. ♦ids: это необязательный параметр, прикрепляющий дружественный тег к тесто­ вым наборам, которые мы указали в предыдущем аргументе; эти теги­ иден,тификаторы (ID) будут использоваться в выходном отчете для идентифи­ кации различных исполнений тестового сценария. Консольный вывод будет следующим: test_myaddS.py: :test_add[pos-pos] PASSED test_ myaddS .ру:: test_add[pos-neq] PASSED test_myaddS.py: :test_add[neq-pos] PASSED test_myaddS.py: :test_add[neq-neq] PASSED ====== 4 passed in О. 04s =======[ 25%] [ 50%] [ 75%] [100%] Вывод показывает, сколько раз выполняется тест и с какими данными. Тестовые сценарии, написанные с помощью маркеров pytest, лаконичны и просты в реализа­ ции. Мы экономим много времени и можем за короткое время сделать больше тес­ тов, лишь меняя входные данные. Далее обсудим еще одно важное свойство -фикстуры. Создание тестов с фикстурами pytest Фикстуры можно реализовать с помощью декоратора @pytest. fixture. В pytest они реализованы эффективнее, чем в других фреймворках, и тому есть несколько причин: ♦Высокая масштабируемость: мы можем определить общую настройку или фикстуры, которые можно повторно использовать через функции, классы, моду­ ли и пакеты. ♦Модульная реализация: в тесте могут быть использованы одна или несколько фикстур; они, в свою очередь, могут использовать другие фикстуры (аналогично вызову одной функции из тела другой). ♦Гибкость: каждый сценарий в наборе может использовать один и тот же или другой набор фикстур. ♦Мы можем создавать фикстуры с заданной областью действия; по умолчанию -это function, что означает, фикстура будет выполняться перед каждой тесто­ вой функцией (сценарием); можно определить следующие области: •Function: фикстура уничтожается после выполнения теста.\n--- Страница 160 ---\nГлава 5. Тестирование и автоматизация с помощью Python 163 •Module: фикстура уничтожается после выполнения последнего теста в модуле. •Class: фикстура уничтожается после выполнения последнего теста в· классе. •Package: фикстура уничтожается после выполнения последнего теста в пакете. •session: фикстура уничтожается после выполнения последнего теста в сеансе. Фреймворк имеет несколько полезных встроенных фикстур, которые можно ис­ пользовать «из коробки». Например, capfd для захвата вывода в дескрипторы фай­ лов, capsys для вывода данных в stdout и stderr, request для предоставления инфор­ мации о запрашиваемой тестовой функции и testdir для предоставления временно­ го каталога для выполнения тестов. Фикстуры в pytest также можно использовать для очистки в конце тестового сцена­ рия. Об этом мы поговорим чуть позже. В следующем примере мы создадим тесты для класса MyCalc, используя пользова­ тельские фикстуры. Фрагмент кода для MyCalc мы уже публиковали выше в подраз­ деле «Выполнение нескольких наборов тестов». Реализация фикстур и тестов при­ ведена ниже: #test_mycalcl.py тест функций калькулятора с помощью фикстур irnport sys irnport pytest frorn rnypytest.src.rnyaddЗ irnport add frorn rnypytest.src.rnycalc irnport MyCalc @pytest.fixture(scope=\"module\") def my_calc(): return MyCalc () @pytest.fixture def test_data (): return { 'х' :10, 'у' :5} def test_add(my_calc, test_data): \"\"\"тест дпя сложения двух чисел\"\"\" assert rny_calc.add(test_data.get('x'),test_data.get('y')) 15 def test_suЬtract(rny_calc, test_data): \"\"\"тест дпя разности двух чисел\"\"\" assert rny_calc.subtract(test_data.get('x'), test_data.get('y'})== 5 На что следует обратить внимание: 1.Мы создали две фикстуры: rny_calc и test_data. Для фикстуры rny_calc задана об­ ласть rnodule, поскольку мы хотим реализовать ее выполнение один раз для пре-\n--- Страница 161 ---\n164 Раздел 2. Расширенные концепции программирования доставления экземпляра класса MyCalc. Фикстура test_data использует область по умолчанию (function), то есть будет выполняться перед каждым методом. 2.Для тестов (test_add и test_suЬtract) мы использовали фикстуры в качестве входных аргументов. Имя аргумента должно совпадать с именем функции фик­ стуры. Фреймворк pytest автоматически ищет фикстуру с именем, указанным в аргументе. В нашем примере мы используем фикстуру в целях подготовки тестовых функций. Что касается очистки после выполнения тестов, существуют два подхода для реа­ лизации данного функционала, и мы обсудим их далее. Использование оператора yield вместо return При таком подходе мы пишем некоторый код для этапа подготовки, используем оператор yield вместо return, а затем пишем код для этапа очистки после инструк­ ции yield. Если у нас есть набор тестов или модуль с множеством фикстур, испол­ нитель будет выполнять каждую фикстуру (в установленном порядке), пока не встретит оператор yield. Как только выполнение тест-кейса завершено, исполни­ тель запускает выполнение всех полученных фикстур и выполняет код после инст­ рукции yield. Данный подход является рекомендуемым, поскольку мы получаем код, который легко поддерживать. Добавление финализатора с помощью фикстуры request В этом подходе мы должны рассмотреть три шага для создания метода очистки: ♦Мы должны использовать в фикстурах объект request; он может быть предо­ ставлен с помощью встроенной фикстуры с тем же именем. ♦Мы определим метод teardown отдельно или как часть реализации фикстуры. ♦Мы предоставим teardown как вызываемый метод для объекта request, используя метода addfinalizer. Для демонстрации обоих подходов изменим предыдущую реализацию фикстур. В обновленном коде реализуем фикстуру my_calc, использующую уiеld-подход, и фикстуру data_set, использующую addfinalizer. Исправленный пример кода: lttest_mycalc2.py тест функций калькулятора с помощью фикстур <операторы import > @pytest.fixture(scope=\"module\") def my_calc(): my_calc = MyCalc() yield my_calc del my_calc\n--- Страница 162 ---\nГлава 5. Тестирование и автоматизация с помощью Python @pytest.fixture def data_set(request): dict = { 'х' :10, 'у' :5) def delete_dict(oЬj): del obj request.addfin&lizer(lamЬda: delete_dict(dict)) return dict <остальные тестовые случаи> 165 Обратите внимание, для этих примеров нет реальной необходимости в реализации функционала, мы добавили их в целях демонстрации. ПОДСКАЗКА Использование nose и doctest для автоматизации тестов аналогично использованию фреймворков unittest и pytest. Далее мы поговорим о разработке через тестирование. Разработка через тестирование Разработка через тестирование (Test-driven development, TDD) -хорошо из­ вестная практика в разработке ПО, при которой мы сначала пишем тесты, а затем код для необходимой функции в приложении. Подход имеет три простых правила: ♦Не стоит писать какой-либо функциональный код, пока модульные тесты для него завершаются неудачей. ♦Не стоит в том же тесте писать кода больше, чем необходимо для неудачного результата. ♦Не стоит писать функционального кода больше, чем необходимо для прохожде- ния неудачного теста. Эти правила заставляют нас следовать известному трёхэтапному подходу к разра­ ботке ПО, который носит название «Красный, Зеленый, Рефакторинг» (Red, Green, Refactor). Эти этапы повторяются непрерывно. Они изображены на рис. 5.4 и опи­ саны далее. Красный На этом этапе мы пишем тест без какого-либо кода, который нужно проверить. Очевидно, что в этом случае тест завершится неудачей. Мы не будем пытаться пи­ сать полный тестовый сценарий, а только достаточное количество кода для провала теста.\n--- Страница 163 ---\n166 Раздел 2. Расширенные концепции программирования Зеленый На этом этапе мы пишем код, пока не сможем пройти тест. Опять же, количество кода не должно превышать необходимый минимум для прохождения теста. После мы запускаем все тесты с целью убедиться, что ранее написанные кейсы тоже за­ вершаются успехом. Рефакторинг На этом этапе мы повышаем качество кода, а именно, облегчаем его чтение и оп­ тимизируем использование, например, весь жесткий код (Hardcode) должен быть удален. Также рекомендуется запускать тесты после каждого цикла рефакторинга, результатом которого является чистый код. Можно повторять этот цикл, добавляя все больше тестовых сценариев и нового кода, пока не будет разработан весь функ­ ционал. Важно понимать, что TDD не является ни подходом к тестированию, ни подходом к проектированию. Это подход к разработке ПО в соответствии со спецификациями, которые определены написанием тестов в первую очередь. На следующей схеме представлены три этапа TDD (рис. 5.4): Совершенствуем код Тест провален Написание кода, пока тест не будет пройден Рис. 5.47 TDD: красный, зеленый, рефакторинг Далее мы расскажем о роли автоматизации тестов в процессе непрерывной инте­ грации.\n--- Страница 164 ---\nГлава 5. Тестирование и автоматизация с помощью Python 167 Автоматизированная непрерывная интеграция Непрерывная интеграция ( continuous integration, CI) -это процесс, который со­ четает преимущества автоматизированного тестирования и систем контроля версий для обеспечения полностью автоматизированной среды интеграции. Этот подход подразумевает внедрение кода в общий репозиторий. При каждом добавлении кода в репозиторий ожидается, что будут запущены следующие два процесса: ♦Автоматизированный процесс сборки для подтверждения, что новый код ничего не нарушает с точки зрения компиляции или синтаксиса. ♦Автоматизированное выполнение тестов для проверки, что действующий и но- вый функционал соответствуют определенным тестам. Различные шаги и этапы CI показаны на схеме ниже. Несмотря на наличие на схеме фазы сборки, для проектов Python она необязательна, поскольку можно выполнять интеграционные тесты без скомпилированного кода (рис. 5.5): ·Отправка кода для слияния Сборка ·Сборка релиза Тестирование · Автоматизированные тесты Отчет ·Подробный отчет Рис. 5.5. Фазы тестирования в рамках непрерывной интеграции Для создания СI-системы требуется стабильный распределенный контроль версий и инструмент, с помощью которого можно реализовать рабочий поток для тестиро­ вания целого приложения через серию наборов тестов. Существует несколько ком­ мерческих и свободных решений для непрерывной интеграции и непрерывной дос­ тавки (CD). Эти инструменты предназначены для простой интеграции с системой контроля версий и фреймворком автоматизации тестирования. Популярные инст­ рументы для CI: Jenkins, ВатЬоо, Buildbot, GitLab CI, CircleCI и Buddy. Подробная информация о них приведена ниже в подразделе «Дополнительные ресурсы».\n--- Страница 165 ---\n168 Раздел 2. Расширенные концепции программирования Очевидные преимущества автоматизированного CI заключаются в быстром обна­ ружении ошибок и в их удобном и максимально быстром исправлении. Важно по­ нимать, что CI предназначена не для исправления ошибок, но определенно помога­ ет их легко выявлять и оперативно исправлять. Заключение В этой главе мы рассмотрели различные уровни тестирования приложений и два фреймворка {unittest и pytest) для автоматизации тестов на Python. Узнали, как создавать сценарии базового и расширенного уровня с помощью них. Затем мы по­ знакомились с подходом TDD (разработка через тестирование) и его очевидными преимуществами. В конце мы затронули основы непрерывной интеграции (CI), ко­ торая является ключевым шагом в разработке ПО с использованием моделей Agile и Development-Operations (DevOps). Эта глава будет полезна всем, кто хочет научиться писать модульные тесты для приложений Python. Приведенные примеры кода станут хорошей отправной точкой для написания тестовых сценариев с помощью любого фреймворка. В следующей главе поговорим о различных расширенных приемах и советах по разработке приложений на Python. Вопросы 1.Модульное тестирование выполняется методом белого или черного ящика? 2.Когда следует использовать фиктивные объекты (mосk-объекты)? 3.Какие методы используются для реализации тестовых фикстур с фреймворком unittest? 4.Чем разработка через тестирование (TDD) отличается от непрерывной интегра­ ции (CI)? 5.Когда следует использовать тестирование на основе данных (DDT)? Дополнительные ресурсы ♦«Learning Python Testing», автор: Дэниел Арбакл (Daniel Arbuckle). ♦«Python. Разработка на основе тестирования» (Test-Driven Development with Py­ thon), автор: Гарри Персиваль (Harry J.W. Percival). ♦«Python. Лучшие практики и инструменты» (Expert Python Programming) авторы: Михал Яворски (Michal Jaworski) и Тарек Зиаде (Tarek Ziade). ♦Сведения о фреймворке unittest в документации Python: https ://docs.python. org/3/library/unittest. html.\n--- Страница 166 ---\nГлава 5. Тестирование и автоматизация с помощью Python 169 Ответы 1.Метод белого ящика. 2.Фиктивные объекты имитируют поведение внешних или внутренних зависимо­ стей. С их помощью можно сосредоточиться на создании тестов для проверки функционального поведения. setUp, tearDown, setUpClass, tearDownClass. 3.TDD -это подход к разработке ПО, при котором сначала пишутся тесты. CI - это процесс, при котором все тесты выполняются при каждой сборке нового ре­ лиза. Прямой связи между TDD и CI нет. 4.DDT используется, когда необходимо провести функциональное тестирование с несколькими перестановками входных параметров. Например, если нам нужно протестировать конечную точку API с разными комбинациями входных аргу­ ментов.\n--- Страница 167 ---\n6 Дополнительные советы и приемы Python В этой главе мы рассмотрим дополнительные рекомендации по решению сложных задач в Python. Мы познакомимся с вложенными функциями и лямбда-функциями, а также поговорим о создании декораторов. Изучим преобразования данных с по­ мощью методов filter, map и reduce. Узнаем, как работать со сложными структурами данных вроде вложенных словарей и включений для разных типов коллекций. В конце познакомимся с функционалом библиотеки pandas для объектов DataFrame. Все эти приемы не только помогут в решении сложных задач с наименьшим коли­ чеством кода, но и научат писать программы быстрее и эффективнее. Темы этой главы: ♦Расширенные приемы использования функций. ♦Расширенные концепции структур данных. ♦Введение в pandas DataFrame. К концу главы вы научитесь использовать функции Python для выполнения таких сложных задач, как преобразование данных и создание декораторов. Также вы уз­ наете, как использовать структуры данных, включая pandas DataFrame, в аналити­ ческих приложениях. Технические требования Для этой главы понадобится: ♦Python 3.7 или более поздней версии. ♦Аккаунт Test PyPI и токен API в этом аккаунте.",
      "debug": {
        "start_page": 142,
        "end_page": 167
      }
    },
    {
      "name": "Глава 6. Дополнительные советы и приемы Python 170",
      "content": "--- Страница 168 --- (продолжение)\nГлава 6. Дополнительные советы и приемы Python Примеры кода для этой главы можно найти по адресу: https://github.com/PacktPuЬlishing/Python-for-Geeks/tree/master/ChapterOб. Начнем с приемов по использованию функций в Python. Расширенные приемы использования функций в Python 171 Реализация функций в Python и других языках программирования является ключом к модульности и многократному использованию кода. В современных языках роль функций расширена и включает в себя создание простого, лаконичного кода без использования сложных циклов и условных операторов. Начнем с функций counter, zip и itertools. Функции counter, itertools и zip для итерационных задач Для задач по обработке данных широко используются итераторы, которые мы под­ робно рассматривали в главе 4 («Библиотеки Python для продвинутого программи­ рования»). В этом разделе мы подробнее остановимся на функциях для удобной работы с итераторами и итерируемыми объектами. К ним относятся counter, zip и itertools, которые мы рассмотрим поподробнее. Counter Counter (счетчик) -это тип контейнера, который отслеживает количество каждого элемента, представленного в нем. Он полезен для подсчета частоты встречаемых данных для последующего анализа. Посмотрим на простом примере, как работает эта функция: #counter.py from collections import Counter tприменение счетчика к объекту строки print(Counter(\"people\")) tприменение счетчика к объекту списка my_counter = Counter([l,2,1,2,3,4,1,3]) print(my_counter. most_common(l)) print(list(my_counter.e lements())) *применение счетчика к объекту словаря print(Counter({'A': 2, 'В': 2, 'С': 2, 'С': 3}}} Мы создали несколько экземпляров Counter с объектами строки, списка и словаря. Класс Counter имеет методы most _ common и elements. Мы использовали метод most common со значением 1, что позволяет получить элемент, который чаще всего\nГлава 6. Дополнительные советы и приемы Python Примеры кода для этой главы можно найти по адресу: https://github.com/PacktPuЬlishing/Python-for-Geeks/tree/master/ChapterOб. Начнем с приемов по использованию функций в Python. Расширенные приемы использования функций в Python 171 Реализация функций в Python и других языках программирования является ключом к модульности и многократному использованию кода. В современных языках роль функций расширена и включает в себя создание простого, лаконичного кода без использования сложных циклов и условных операторов. Начнем с функций counter, zip и itertools. Функции counter, itertools и zip для итерационных задач Для задач по обработке данных широко используются итераторы, которые мы под­ робно рассматривали в главе 4 («Библиотеки Python для продвинутого программи­ рования»). В этом разделе мы подробнее остановимся на функциях для удобной работы с итераторами и итерируемыми объектами. К ним относятся counter, zip и itertools, которые мы рассмотрим поподробнее. Counter Counter (счетчик) -это тип контейнера, который отслеживает количество каждого элемента, представленного в нем. Он полезен для подсчета частоты встречаемых данных для последующего анализа. Посмотрим на простом примере, как работает эта функция: #counter.py from collections import Counter tприменение счетчика к объекту строки print(Counter(\"people\")) tприменение счетчика к объекту списка my_counter = Counter([l,2,1,2,3,4,1,3]) print(my_counter. most_common(l)) print(list(my_counter.e lements())) *применение счетчика к объекту словаря print(Counter({'A': 2, 'В': 2, 'С': 2, 'С': 3}}} Мы создали несколько экземпляров Counter с объектами строки, списка и словаря. Класс Counter имеет методы most _ common и elements. Мы использовали метод most common со значением 1, что позволяет получить элемент, который чаще всего\n--- Страница 169 ---\n172 Раздел 2. Расширенные концепции программирования встречается в контейнере my-counter. Метод elements использован для возврата ис­ ходного списка из экземпляра Counter. Вывод консоли будет следующим: Counter ( { 'р' : 2 , 'е' : 2 , ' о' : 1, ' l ' : 1}) [ (1, 3)] (1, 1, 1, 2, 2, 3, 3, 4] Counter({ 'С': 4, 'А': 2, 'В': 2}) Важно отметить, что с объектом словаря мы намеренно использовали повторяю­ щийся ключ, но в экземпляре Counter мы получаем только одну пару «ключ­ значение», которая встречается последней. Кроме того, элементы в экземпляре counter упорядочены на основе значений каждого элемента. Также обратите внима­ ние, класс counter преобразует словарь в хеш-таблицу. zip Функция zip используется для создания объединенного итератора из двух и более отдельных итераторов. Это удобно, если надо выполнить несколько итераций па­ раллельно. Например, можно использовать функцию zip для математических алго­ ритмов, связанных с интерполяцией или распознаванием образов. А также полезно при обработке цифровых сигналов, когда надо объединить несколько сигналов (ис­ точников данных) в один. Пример использования функции: #zip.py num list = [1, 2, 3, 4, 5] lett_list = ['alpha', 'bravo', 'charlie'] zipped_iter = zip(num_list,lett_list) print(next(zipped_iter)) print(next(zipped_iter)) print(list(zipped_iter)) Мы объединили два списка с помощью функции zip. Обратите внимание, один спи­ сок больше другого по количеству элементов. Вывод консоли будет следующим: (1, 1 alpha 1) (2, 'bravo') ((3, 'charlie'), (4, 'delta')] Как и ожидалось, первые два кортежа мы получаем с помощью функции next, кото­ рые представляют собой комбинацию соответствующих элементов из каждого спи­ ска. Мы использовали конструктор list для перебора оставшихся кортежей из ите­ ратора zip. В итоге мы получили оставшиеся кортежи в формате списка.\n--- Страница 170 ---\nГлава 6. Дополнительные советы и приемы Python 173 itertools При работе с большим объемом данных без итераторов не обойтись. Модуль itertools может предложить дополнительные функции, которые будут полезны. Рассмотрим несколько из них: ♦count: используется при создании итератора для подсчета чисел; можно указать начальное число (по умолчанию О) и по желанию задать шаг приращения; фраг­ мент кода ниже возвращает итератор, который предоставляет числа 10, 12 и 14: #itertools_count.py import itertools iter = itertools.count(lO, 2) print(next(iter)) print(next(iter)) ♦cycle: позволяет бесконечно перебирать итератор; фрагмент ниже демонстриру­ ет использование этой функции для списка букв алфавита: letters = {'А', 'В', 'С') for letter in itertools.cycle(letters): print(letter) ♦repeat: предоставляет итератор, который снова и снова возвращает объект, если для него не задан аргумент times; фрагмент кода ниже будет повторять строко­ вый объект пять раз: for х in itertools.repeat('Python', times=S): print (х) ♦accumulate: возвращает итератор, который предоставляет общую сумму или дру­ гие накопленные результаты на основе функции-агрегатора, которая была пере­ дана в accumulate в качестве аргумента; легче понять ее работу на примере: #itertools_accumulate.py import itertools, operator listl = (1, 3, 5) res = itertools.accumulate(listl) print(\"default:\") for х in res: print (х) res = itertools.accumulate(listl, operator.mul) print(\"Multiply:\" •for х in res: print (х) ♦Сначала мы использовали функцию accumulate, не предоставив агрегатор для по­ лучения накопленных результатов; по умолчанию accumulate выполнит сложение двух чисел (1 из) из исходного списка; этот процесс повторится для всех чисел,\n--- Страница 171 ---\n174 Раздел 2. Расширенные концепции программирования а результат сохранится внутри итерируемого объекта (в нашем случае это res); во второй части кода мы предоставили функцию mul (умножение) из модуля operator, и на этот раз будет произведено умножение вместо сложения. ♦chain: комбинирует два и более итерируемых объекта и возвращает объединен­ ный объект; ниже приведен пример с двумя итерируемыми объектами ( списка­ ми) и функцией chain: listl ['А','В','С'] list2 = ['W','X','Y','Z'] chained_iter = itertools.chain(listl, list2) for х in chained iter: print (х) ♦Обратите внимание, функция объединяет объекты последовательно, то есть сна­ чала будут доступны элементы из listl, а затем из list2. ♦compress: используется для фильтрации элементов из одного набора на основе другого; в примере продемонстрирована выборка букв алфавита из списка на основе итерируемого объекта selector: letters = ['А', 'В', 'С'] selector = [True, О, 1) for х in itertools.canpress(letters, selector): print (х) ♦Для объекта selector можно использовать значения True/False или 1/0; на выходе будут буквы А и с. ♦groupby: определяет ключи для каждого элемента в объекте и группирует их на основе этих ключей; для работы функции требуется другая функция (key_func), которая идентифицирует ключ в каждом элементе итерируемого объекта; сле­ дующий пример демонстрирует использование обеих функций: #itertools _groupby.py import itertools mylist = [(\"А\", 100), (\"А\", 200), (\"В\", 30), \"В\", 10)) def get_key(group): return group[0] for key, grp in itertools.groupЬy(mylist, get_key): print(key + \"-->\", list(grp)) ♦tee: используется для дублирования итераторов из единого объекта; ниже при­ веден пример дублирования двух итераторов из единого списка: letters = ['А', 'В', 'С'] iterl, iter2 = itertools.tee(letters)\n--- Страница 172 ---\nГлава 6. Дополнительные советы и приемы Python for х in iterl: print (х) for х in iter2: print(x) 175 Далее мы рассмотрим еще одну категорию функций, которые широко используют­ ся для преобразования данных. Использование методов filter, map и reduce для преобразования данных Методы map, fil ter и reduce -это три функции Python, которые помогают писать простой и лаконичный код. Они применяются ко всему набору сразу, без перебора или использования итерационных операторов. Методы map и filter доступны как встроенные и используются для преобразования и фильтрации данных. Метод reduce необходимо импортировать из модуля functtools, и он полезен при анализе для получения осмысленных результатов из большого набора данных. Далее мы рассмотрим каждую функцию с примерами. map Функция map определяется с помощью следующего синтаксиса: map(func, iter, ) Аргумент func -это имя функции, которая будет применяться к каждому элементу объекта iter. Три точки указывают, что можно передать несколько ,итерируемых объектов. Однако важно понимать, что количество аргументов функции (func) должно совпадать с числом итерируемых объектов. Результатом функции map будет объект map (или mар-объект), который является генератором. Возвращаемое значение можно преобразовать в список, передав объект map конструктору list. ВАЖНОЕ ПРИМЕЧАНИЕ В Python 2 функция map возвращает список. В Python 3 это поведение было изменено. Перед обсуждением работы map реализуем простую функцию преобразования, ко­ торая конвертирует список чисел в список квадратов их значений. Пример кода: #mapl.py вычисление квадрата каждого элемента в списке mylist = [1, 2, 3, 4, 5) new_list = [] for item in mylist: square = item*item new_list.append(square) print (new _ list)\n--- Страница 173 ---\n176 Раздел 2. Расширенные концепции программирования Здесь используется цикл for для перебора списка, вычисления квадрата каждого элемента, а затем добавления в новый список. Такой подход распространен, но не соответствует философии Python. Вывод консоли будет следующим: [1, 4, 9, 16, 25) С помощью map этот код можно упростить: # map2.py вычисление квадрата каждого элемента в списке def square (num) : return num * num mylist = [1, 2, 3, 4, 5] new_list = list(map(square, mylist)) print(new_list) Используя map, мы предоставили имя функции (square) и ссылку на список (mylist). Возвращаемый объект будет преобразован в список с помощью конструктора list. Консольный вывод будет аналогичен предыдущему примеру. В следующем примере мы предоставим два списка в качестве входных данных для функции map: #mарЗ.ру вычисление квадрата каждого элемента в списке def product(numl, num2): return numl * num2 mylistl = [1, 2, 3, 4, 5] mylist2 = [6, 7, 8, 9] new_list = list(map(product, mylistl, mylist2)) print (new_list) На этот раз функция map будет использовать функцию product, которая берет соот­ ветствующие элементы из двух списков и перемножает их, прежде чем вернуть об­ ратно в функцию map. На консоли будет следующее: [6, 14, 24, 36) Анализ вывода говорит, что только первые четыре элемента из каждого списка ис­ пользуются функцией map. Она автоматически останавливает выполнение, когда в любом из итерируемых объектов заканчиваются элементы. Это означает, что, даже если мы предоставим объекты разных размеров, функция map не вызовет никаких исключений, а просто будет работать с тем количеством элементов, которое можно сопоставить между наборами. Поэтому в нашем примере в итоговом списке new _ list всего четыре элемента.\n--- Страница 174 ---\nГлава 6. Дополнительные советы и приемы Python 177 filter Функция filter также работает с наборами, но только с одним за раз. Она обеспе­ чивает возможность фильтрации итерируемого объекта по критериям, которые предоставляются через определение функции. Синтаксис функции следующий: filter (func, iter) Функция func предост�вляет критерии фильтрации и должна возвращать значения True или False. Поскольку filter работает только с одним объектом, для func допус­ кается только один аргумент. В следующем примере filter используется для выбо­ ра элементов, значения которых являются четными числами. Для задания критери­ ев выбора реализована функция is _ even, которая оценивает, является число четным или нет. Пример кода: #filterl.py получение четных чисел из списка def is_even(num): return (num % 2 == О) mylist = [l, 2, 3, 4, 5, 6, 7, 8, 9) new_list = list(filter(is_even, mylist)) print(new_list) На консоли будет следующий вывод: [2, 4, б, 8] reduce Функция reduce используется для применения кумулятивной обработки (Cumula­ tive Processing) к каждому элементу последовательности, которая передается ей в качестве аргумента. Функция кумулятивной обработки не предназначена для пре­ образования или фильтрации. Как следует из названия, она нужна для получения в конце единого результата на основе всех элементов последовательности. Синтаксис функции следующий: reduce (func, iter[,initial]) Функция func используется для применения кумулятивной обработки к каждому элементу итерируемого объекта. Кроме того, initial -это опциональное значение, которое можно передать в func в качестве начального значения для обработки. Важно понимать, что в func всегда нужно будет передавать два аргумента в случае использования reduce: первым аргументом будет начальное значение (если оно ука­ зано) или первый элемент последовательности, а вторым аргументом будет сле­ дующий элемент последовательности.\n--- Страница 175 ---\n178 Раздел 2. Расширенные концепции программирования В примере ниже использован простой список из пяти чисел. Мы реализуем собст­ венный метод для сложения двух чисел, а затем используем reduce для суммирова­ ния всех элементов списка. Пример кода: #reducel.py вычисление суммы чисел из списка from functools import reduce def Se<t,_Sum(numl, num2) : return numl+num2 mylist = [1, 2, 3, 4, 5) result = reduce(seq_sum, mylist) print(result) В выводе будет 15, что является суммой всех элементов списка mylist. Если мы предоставим функции reduce начальное значение, оно будет включено в результат. Например, выводом той же программы с оператором ниже будет 25: result = reduce(seq_sum, mylist, 10) Как уже упоминалось, reduce выдает только одно значение, которое соответствует функции func. В нашем примере это будет целое число. Мы рассмотрели методы map, filter и reduce, доступные в Python. Они широко ис­ пользуются в data science для преобразования и обработки данных. Одна из про­ блем использования map и filter заключается в том, что они возвращают объекты типа map и filter, которые нужно явно преобразовывать в list для дальнейшей об­ работки. Включения и генераторы не имеют таких ограничений, при этом предо­ ставляют аналогичную функциональность. Использовать их относительно проще. Поэтому они пользуются большей популярностью, чем map, filter и reduce. Вклю­ чения и генераторы мы обсудим в подразделе «Расширенные концепции структур данных», а сейчас поговорим о лямбда-функциях. Создание лямбда-функций Лямбда-функции (Lambda function) -это анонимные функции, основанные на однострочном выражении. В случае с регулярными функциями мы определяем их ключевым словом def, тогда как анонимные функции определяются словом lamЬda. Они ограничены одной строкой, поэтому не могут содержать несколько операторов и не могут использовать оператор return. Значение возвращается автоматически после вычисления однострочного выражения. Лямбда-функции можно использовать везде, где используются обычные функции. Самый простой и удобный способ -map, reduce и filter. Такие функции помогают сделать код аккуратным.\n--- Страница 176 ---\nГлава 6. Дополнительные советы и приемы Python 179 Для демонстрации повторно используем предыдущие примеры с map и filter, но изменим код, заменив func на лямбда-функцию: #lamЬdal.py вычисление квадрата каждого элемента в списке mylist = [1, 2, 3, 4, 5) new_list = list(map(lamЬda х: х*х, mylist)) print(new_list) #lamЬda2.py получение четных чисел из списка mylist = [1, 2, 3, 4, 5, 6, 7, 8, 9) new_list = list(filter(lamЬda х: х % 2 == О, mylist)) print (new _ list) #lamЬdaЗ.py :вычисление произведения ЭJieмewroв в двух спис:�сах mylistl = [1, 2, 3, 4, 5] mylist2 = [б, 7, 8, 9] new_list = list(map(lamЬda х,у: х*у, mylistl, mylist2)) print (new _ list) Хотя код стал проще, использовать лямбда-функции следует с осторожностью, по­ скольку их нельзя использовать повторно и сложно поддерживать. Нужно об этом помнить, прежде чем вводить их в эксплуатацию. Добавлять любые изменения и новый функционал будет непросто. Главное правило состоит в том, что использо­ вать их стоит только для простых выражений, когда написание отдельной функции было бы накладно. Внедрение одной функции в другую Когда мы добавляем одну функцию внутри другой, она носит название внутренней или вложенной. Преимущества таких функций заключается в прямом доступе к переменным, которые определены или доступны в области видимости внешней функции. Создать внутреннюю функцию можно аналогично обычной функ­ ции, используя ключевое слово def, но с соответствующим отступом. Внутренние функции не могут выполняться или вызываться внешней программой. Однако если внешняя функция возвращает ссылку на внутреннюю, та может быть использована вызывающей стороной. Рассмотрим примеры возвращения ссылок позже. А сейчас обсудим несколько преимуществ и способов применения данных функций. Инкапсуляция Распространенный вариант использовани,я вложенных функций -возможность скрыть их функционал от внешнего мира. Они доступны только в пределах области действия внешней функции и скрыты от области действия глобальной функции.\n--- Страница 177 ---\n180 Раздел 2. Расширенные концепции программирования Следующий пример демонстрирует одну внешнюю функцию, которая скрывает внутреннюю: #innerl.py def outer_hello(): print (\"Hello from outer function\") def inner _ hello () : print(\"Hello from inner function\") inner _ hello () outer _ hello () В теле основной программы можно вызывать только внешнюю функцию. Внутрен­ нюю функцию можно вызывать только из тела внешней. Вспомогательные функции В некоторых случаях можно обнаружить, что часть кода внутри функции доступна для повторного использования, поэтому есть возможность вынести его в отдельную функцию. Но если код допустимо использовать только внутри функции, тогда речь идет о создании именно внутренней функции. Она также называется вспомога­ тельной. Пример такой функции представлен ниже: def outer_fn(x, у): def qet__prefix(s): return s [: 2] х2 = qet__prefix (х) у2 = qet__prefix(y) #.цат.иейшая обраб0'1'1СА х2 and у2 Мы определили внутреннюю функцию get_prefix (вспомогательная функция) внут­ ри внешней функции фильтрации первых двух букв значения аргумента. Поскольку фильтрация будет повторяться для всех аргументов, мы добавили вспомогательную функцию, которую можно будет использовать повторно внутри внешней функции. Замыкания и фабричные функции Это вариант использования, где внутренние функции проявляют себя лучше всего. Замыкание (Closure) -это вложенная функция вместе с ее окружающей средой. Оно создается динамически и может возвращаться другой функцией. Главное пре­ имущество состоит в том, что возвращаемая функция имеет полный доступ к пере­ менным и пространствам имен, в которых она была создана, даже если замыкаю­ щая (в нашем случае внешняя) функция завершила выполнение.\n--- Страница 178 ---\nГлава 6. Дополнительные советы и приемы Python 181 Данную концепцию можно продемонстрировать на примере. В коде ниже мы реа­ лизовали фабрику замыканий (Closure factory) для создания функции, которая вы­ числяет степень основания, а замыкание будет сохранять основание: #inner2.py def power_calc_factory(base): def power_calc(exponent): return base**exponent return power_calc power_calc_2 = power_gen_factory(2) power_calc_З = power_gen_factory(З) print(power_calc_2(2)) print(power_calc_2{3)) print(power_calc_3{2)) print{power_calc_3{4)) В примере внешняя функция (power_calc_factory) выступает как фабрика замыканий, поскольку создает новое замыкание при каждом вызове, а затем возвращает его вы­ зывающему объекту. Кроме того, power_calc -это внутренняя функция, которая принимает одну переменную (ьаsе) из пространства имен замыкания, а затем вто­ рую переменную (exponent), которая передается в качестве аргумента. Обратите внимание, наиболее важным оператором здесь является return power _ calc. Он воз­ вращает внутреннюю функцию как объект с его замыканием. Когда мы вызываем power _ calc : factory в первый раз вместе с аргументом base, соз­ дается замыкание с его пространством имен, включая переданный аргумент, и за­ тем возвращается вызывающему объекту. Когда мы снова вызываем ту же функ­ цию, мы получаем новое замыкание с объектом внутренней функции. В этом при­ мере мы создали два замыкания: одно со значением base, равным 2, и второе со значением base, равным 3. Когда мы вызываем внутреннюю функцию, передавая разные значения для переменной exponent, код в ней (в нашем случае power_calc) также будет иметь доступ к значению base, которое уже было передано внешней функции. Традиционно вложенные функции используются для сокрытия или инкапсуляции функционала внутри функции. Но наиболее мощное их применение проявляется, когда они используются вместе с внешними функциями, выступающими в качестве фабрики для создания динамических функций. Внутренние функции также исполь­ зуются для реализации декораторов. Более подробно мы обсудим это в следующем подразделе. Изменение поведения функции с помощью декораторов Концепция декораторов в Python основана на шаблоне структурного проектирова­ ния Decorator. Он позволяет добавлять к объектам новое поведение, ничего не ме-\n--- Страница 179 ---\n182 Раздел 2. Расширенные концепции программирования няя в самой их реализации. Это поведение добавляется в специальные объекты­ оболочки. В Python декораторы -это специальные функции высшего порядка, которые по­ зволяют разработчикам добавлять новый функционал в существующую функцию (или метод), ничего не изменяя внутри нее. Обычно декораторы добавляются перед определением функции. Они используются для реализации многих возможностей приложения, но особенно популярны при проверке данных, логировании, кэширо­ вании, отладке, шифровании и управлении транзакциями. Для создания декоратора мы должны определить вызываемый объект ( функцию, метод или класс), который принимает функцию в качестве аргумента. Вызываемый объект вернет другой объект-функцию с поведением, которое определил декоратор. Функция, отмеченная декоратором (будем называть ее декорирован.н.ая фун.кция), передается в качестве аргумента функции, которая реализует декоратор (будем на­ зывать ее фун.кция-декоратор). Функция-декоратор выполняет переданную ей функцию в дополнение к поведению, добавленному как часть функции-декоратора. Ниже приведен простой пример, в котором мы определяем декоратор для добавле­ ния метки времени до и после выполнения функции: #decoratorl.py from datetime import datetime def add_timestamps(myfunc): def _add_timestamps(): print(datetime.now()) myfunc () print(datetime.now()) return _add_timestamps @add_timestamps def hello_world(): print (\"hello world\") hello _ wor ld () Мы определили функцию-декоратор add_timestam ps, которая принимает любую функцию в качестве аргумента. Во внутренней функции (_actd_timest amps) мы фик­ сируем время до и после выполнения функции и передаем его в качестве аргумента. Функция-декоратор возвращает объект внутренней функции с замыканием. Как мы уже обсуждали, декораторы не делают ничего, кроме разумного использования внутренних функций. Использование символа @ для декорирования функции экви­ валентно следующим строкам кода: hello = add_timestamps(hello_world) hello ()\n--- Страница 180 ---\nГлава 6. Дополнительные советы и приемы Python 183 В данном случае мы явно вызываем функцию-декоратор, передавая имя функции как параметр. Другими словами, декорированная функция равна внутренней функ­ ции, которая определяется внутри функции-декоратора. Именно так Python интер­ претирует и вызывает функцию-декоратор, когда видит символ @ перед определе­ нием функции. Однако возникает проблема, когда необходимо получить больше информации о вызове функций, например, для отладки. Когда мы используем встроенный метод help с функцией hello_world, мы получаем справку только для внутренней функции. То же самое произойдёт при использовании docstring, которая сработает для внут­ ренней функции, но не для декорированной. Кроме того, сериализация кода будет трудной задачей для декорированных функций. В Python есть простое решение: использовать декоратор wraps из библиотеки functools. Изменим пример кода, доба­ вив декоратор wraps: #decorator2.py from datetime import datetime from functools import wraps def add_timestamps(myfunc): @wraps (myfunc) def _add_timestamps(): print(datetime.now()) myfunc () print(datetime.now()) return _add_timestamps @add_timestamps def hello_world(): print (\"hello world\") hello _ wor ld () help(hello_world) print(hello_world) Использование wraps предоставит дополнительные сведения о выполнении вложен­ ных функций, которые мы можем увидеть в консольном выводе при выполнении данного фрагмента кода. До этого момента мы рассматривали простые примеры декораторов для понимания концепции. Дальше узнаем, как передавать аргументы с функцией в декоратор, как возвращать значение из декоратора и как объединять несколько декораторов в це­ почку. Начнем с передачи атрибутов и возвращения значений с помощью декора­ торов.\n--- Страница 181 ---\n184 Раздел 2. Расширенные концепции программирования Использование декорированной функции с возвращаемым значением и арrументом Когда декорированная функция принимает аргументы, добавление декораторов требует дополнительных трюков. Например, можно использовать *args и **kwargs во внутренней функции-обертке. Это заставит ее принимать произвольное количество позиционных и ключевых аргументов. Ниже приведен простой пример декориро­ ванной функции с аргументами и возвращаемым значением: #decoratorЗ.py from functools import wraps def power(func}: @wraps(func} def inner_calc(* args, **kwargs): print ( \"Decorating power func\") n = func(*args, **kwargs) return n return inner calc @power def power_Ьase2 (n): return 2**n print(power_base2(3}) Внутренняя функция inner_calc принимает общие параметры *args и **kwargs. Для возврата значения из inner_calc мы можем удержать его из функции (в нашем слу­ чае это func или power _ base2 (n} ), которая выполняется внутри нашей внутренней функции. А затем вернуть итоговое значение из inner _ calc. Создание декоратора с собственными арrументами В предыдущих примерах мы использовали так называемые стандартные декора­ торы. Это функции, которые принимают имя декорированной функции в качестве аргумента и возвращают внутреннюю функцию, работающую как функция­ декоратор. Если декоратор имеет собственные аргументы, все работает немного иначе. А именно, он создается поверх стандартного. Проще говоря, декоратор с ар­ гументами -это еще одна функция, которая на самом деле возвращает стандарт­ ный декоратор (а не внутреннюю функцию внутри декоратора). Эту концепцию лучше понять на измененном примере decoratorЗ .ру. В новом варианте мы вычисля­ ем степень значения base, которое передается декоратору в качестве аргумента. Полный код с вложенными функциями-декораторами представлен ниже: #decorator4.py from functools import wraps\n--- Страница 182 ---\nГлава 6. Дополнительные советы и приемы Python def power_calc(base): def inner_decorator(func): @wraps(func) def inner_calc(*args, **kwargs): exponent = func(*args, **kwargs) return base**exponent return inner calc return inner decorator @power _ calc (Ьаsе=З) def power_n(n): return n print(power_n(2)) print(power_n(4)) Этот код работает следующим образом: 185 ♦ Функция-декоратор power_calc принимает один аргумент base и возвращает функцию inner _ decorator, которая является стандартной реализацией декоратора. ♦Функция inner_decorator принимает функцию в качестве аргумента и возвращает функцию inner _ calc для фактического вычисления. ♦Функция inner _ calc вызывает декорированную функцию для получения атрибута exponent, а затем использует атрибут base, который передается внешней функции­ декоратору в качестве аргумента; как и ожидалось, замыкание вокруг внутрен­ ней функции делает значение атрибута base доступным для функции inner _ calc. Далее рассмотрим, как использовать более одного декоратора с функцией или ме­ тодом. Использование нескольких декораторов Существует возможность использовать несколько декораторов с функцией. Это дос­ тигается объединением их в цепочку. Декораторы могут быть одинаковыми или раз­ ными. Реализовать это можно, размещая их один -за другим перед определением функции. Когда с функцией используется несколько декораторов, выполнить ее можно только один раз. Для демонстрации рассмотрим пример, в котором мы зано­ сим лог в журнал целевой системы, используя метку времени. Она добавляется через отдельный декоратор, а целевая система выбирается на основе другого декоратора. В следующем примере показаны определения трех декораторов (add_time_stamp, file и console): #decorator5 .ру (part 1) from datetime import datetime from functools import wraps\n--- Страница 183 ---\n186 def add_timestarrq:i(func): @wraps(func) def inner_func(*args, **kwargs): Раздел 2. Расширенные концепции программирования res = \" {}: {} \\n\". forrnat (datetime. now (), func (*args, \\ **kwargs)) return res return inner func def file(func): @wraps(func) def inner_func(*args, **kwargs): res = func(*args, **kwargs) with open(\"log.txt\", 'а') as file: file.write(res) return res return inner func def console(func): @wraps(func) def inner_func(*args, **kwargs): res = func(*args, **kwargs) print (res) return res return inner func В этом примере мы реализовали три функции-декоратора: ♦file: добавляет в предопределенный текстовый файл сообщение, предоставлен­ ное декорированной функцией; ♦console: выводит сообщение от декорированной функции на консоль; ♦add_timestamp: добавляет временную метку перед сообщением; выполнение должно происходить перед декораторами file и console, а, значит, стоять в це­ почке он должен последним. В следующем фрагменте кода можно использовать эти декораторы для различных функций внутри основной программы: #decoratorS.py (part 2) @file @add_timestarrq:i def log (msg) : return msg @file @console @add_timestarrq:i def logl (msg) : return msg\n--- Страница 184 ---\nГлава 6. Дополнительные советы и приемы Python @console @add_timestamp def log2 (msg) : return msg log(\"This is а test message for file only\") logl(\"This is а test message for both file and console\") log2(\"This message is for console only\") 187 Мы использовали три функции-декоратора, определенные ранее, в разных комби­ нациях для демонстрации вариантов поведения одной функции. В первой комбина­ ции мы выводим сообщение в файл только после добавления метки времени. Во второй -выводим сообщение и в файл, и на консоль. Наконец, в последней ком­ бинации мы выводим сообщение только на консоль. Это показывает гибкость, ко­ торую предоставляют декораторы без необходимости изменения функций. Стоит отметить, что они полезны для упрощения кода и добавления поведения в лаконич­ ной форме, но влекут за собой дополнительные затраты. Использовать их необхо­ димо только в ситуациях, когда преимущества перевешивают издержки. На этом можно закончить обсуждение концепций, связанных с расширенными функциями, и перейти к структурам данных. Расширенные концепции структур данных Python предлагает обширную поддержку для структур данных, в том числе ключе­ вые инструменты для хранения, обработки и извлечения данных, а также доступа к ним. В главе 4 ( «Библиотеки Python для продвинутого программирования») мы уже рассматривали структуры данных, доступные в Python. Здесь мы затронем допол­ нительные концепции, например, вложенные словари и использование включений со структурами данных. Начнем со словарей. Внедрение словаря в словарь Python позволяет вкладывать один словарь в другой. Вложенный словарь имеет множество применений, например, в обработке и преобразовании данных из одного формата в другой. На рис. 6.1 показан вложенный словарь. Корневой словарь имеет два словаря для ключа 1 и ключа 3. Словарь для ключа 1 содержит в себе дополнительные словари. Словарь для ключа 3 -обычный словарь с парами «ключ-значение». Подобный корневой словарь можно записать следующим образом: root_dict = {'1': {'А': {dictA}, 'B':{dictв}}, '2': [list2], '3': {'Х': vall,'Y':val2,'Z': val3}\n--- Страница 185 ---\n188 Раздел 2. Расширенные концепции программирования - - -tll{ \"' ;;; Рис. 6.1. Пример словаря в словаре Мы создали корневой словарь с набором объектов словаря и списка. Создание или определение вложенного словаря Вложенный словарь можно определить или создать, поместив разделенные запя­ тыми словари в фигурные скобки. Для демонстрации создадим словарь со студен­ тами. Каждая запись о студенте будет иметь имя (name) и возраст (age) в качестве элементов, которые сопоставляются с номером студента: #dictionaryl.py dictl = { 100: { 'name' : 'John' , 'age' : 24}, 101: { 'name': 'Mike', 'age' :22}, 102: { 'name': 'Jim', 'age' :21} } print (dictl) print(dictl.get(l00)) Далее рассмотрим, как динамически создавать словари, а также добавлять или об­ новлять вложенные элементы. Добавление во вложенный словарь Динамически создать вложенный словарь или добавить в него элементы можно не­ сколькими способами. В следующем примере рассмотрим три разных подхода. Они аналогичны способам в модуле dictionaryl .ру: ♦ В первом случае создаем внутренний словарь (studentl0l) через прямое присвое­ ние элементов «ключ-значение», а затем назначаем его ключу в корневом слова­ ре; такой подход предпочтителен, где это возможно, потому что код легче чи­ тать и обслуживать. ♦Во втором случае создаем пустой внутренний словарь (student102) и задаем зна­ чения ключам с помощью операторов присваивания; такой подход рекомендо­ ван, когда значения доступны через другие структуры данных.\n--- Страница 186 ---\nГлава 6. Дополнительные советы и приемы Python 189 ♦ В третьем случае напрямую инициируем пустой каталог для третьего ключа корневого словаря; после этого присваиваем значения, используя двойную ин­ дексацию (то есть два ключа): первый ключ для корневого словаря, а второй­ для внутреннего; такой подход делает код кратким, но не является предпочти­ тельным, если читабельность кода важна для дальнейшего обслуживания. Полный пример кода для всех трех подходов выглядит следующим образом: #dictionary2.py #определение :вкутреинеrо спо:варя 1 studentl00 = { 'name': 'John', 'age': 24} #определение :вкутреине:rо спо:варя 2 studentl0l = {} studentl0l['name'] = 'Mike' studentl0l['age'] = '22' #назначение :виутреиних спо:варей 1 и 2 хорие:вону спо:вар.� dictl = {} dictl[l00] studentl00 dictl[l0l] studentl0l #создание :вкутреинеrо спо:варя Иапряиу1О :внутри хорие:во:rо спо:варя dictl[l02] = {} dictl[l02] ['narne'] = 'Jim' dictl[l02] ['age'] = '21' print(dictl) print(dictl.get(l02}} Далее обсудим, как получить доступ к различным элементам вложенного словаря. Доступ к элементам вложенного словаря Для добавления значений и словарей внутри словаря можно использовать двойную индексацию. В качестве альтернативы можно использовать метод get объекта сло­ варя. Тот же подход применим для доступа к различным элементам внутреннего словаря. Ниже приведен пример использования методов get и двойной индексации: #dictionaryЗ.py dictl = {100:{'name':'John', 'age':24}, 101: { 'narne': 'Mike', 'age': 22}, 102: { 'narne' : 'Jim', 'age' : 21} print(dictl.get(l00}) print(dictl.get(l00} .get('narne'}} print(dictl[l0l]) print(dictl[l0l] ['age']) Рассмотрим теперь, как удалить внутренний словарь или элементы из него.\n--- Страница 187 ---\n190 Раздел 2. Расширенные концепции программирования Удаление из вложенного словаря Для удаления словаря или элемента можно использовать общую функцию del или метод объекта словаря -рор. В следующем примере показаны оба этих варианта: #dictionary4.py dictl = {100:{'name':'John', 'age':24}, 101: { 'name': 'Mike', 'age' :22}, 102: { 'name': 'Jim', 'age' :21) } del (dictl[l0l]['age']) print (dictl) dictl[l02] .pop('age') print (dictl) Далее обсудим, как включения помогают обрабатывать данные из разных типов структур данных. Использование включений / Включение (Comprehension) -это быстрый способ создания новых последова­ тельностей (списков, наборов, словарей) из уже существующих. Python поддержи­ вает четыре типа включений: ♦Списковое включение. ♦Словарное включение. ♦Включение множеств. ♦Генераторное включение. Кратко рассмотрим каждый тип с примерами. Списковое включение Списковое включение (List comprehension) подразумевает создание динамического списка с помощью цикла и условного оператора, если это необходимо. Примеры помогут лучше понять концепцию. В первом фрагменте кода (listl .ру) мы создадим новый список из исходного, добавив 1 к каждому элементу исходного списка: #listl.py listl = [1, 2, 3, 4, 5, 6, 7, 8, 9, О] list2 = [x+l for х in listl] print(list2)\n--- Страница 188 ---\nГлава 6. Дополнительные советы и приемы Python 191 В этом случае новый список будет создан с помощью выражения x+l, где х -эле­ мент исходного списка. Это эквивалентно следующему традиционному коду: list2 = [] for х in listl: list2.append(x+l) Используя списковое включение, можно заменить три строки кода одной. Во втором примере (list2.py) мы создадим новый список из исходного списка чи­ сел от 1 до 10, но будем включать только четные. Для этого можно просто добавить условие к предыдущему фрагменту кода: #list2.py listl = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] list2 = [х for х in listl if х % 2 = О] print (list2) Как видите, условие добавляется в конец выражения включения. Теперь попробуем создать словарь с помощью включения. Словарное включение Словарное включение (Dictionary comprehension) похоже на списковое и позволяет создавать словарь из другого словаря таким образом, что элементы исходного сло­ варя выбираются или преобразовываются условно. Ниже приведен пример созда­ ния словаря из элементов существующего словаря. Элементы выбираются по усло­ вию (меньше или равны 200), а затем делятся на 2. Обратите внимание, значения также преобразуются обратно в целые числа как часть выражения включения: #dictcarpl.py dictl = {'а': 100, 'Ь': 200, 'с': 300} dict2 = {x:int(y/2) for (х, у) in dictl.items() if у <=200} print(dict2) Данный код эквивалентен следующему фрагменту из традиционного программиро­ вания: Dict2 = {} for х,у in dictl.items(): if у<= 200: dict2[x] = int(y/2) Как видите, включение значительно сокращает код.\n--- Страница 189 ---\n192 Раздел 2. Расширенные концепции программирования Включение множеств Множества также можно создавать, используя включение множеств (Set compre­ hension). Синтаксис похож на создание спискового включения, за исключением фигурных скобок вместо квадратных. Следующий фрагмент демонстрирует это: #setcoopl.py listl = (1, 2, 6, 4, 5, 6, 7, 8, 9, 10, 8] setl = {х for х in listl if х % 2 == О} print ( setl) Данный код эквивалентен следующему фрагменту из традиционного программиро­ вания: Setl = set () for х in listl: if х % 2 == О: setl. add (х) Как и ожидалось, повторяющиеся записи в наборе не будут учитываться. Мы рассмотрели доступные типы включений для разных структур данных, далее обсудим параметры фильтрации для них. Введение в Pandas DataFrame Pandas -библиотека Python с открытым исходным кодом, которая предоставляет инструменты для высокопроизводительной обработки данных и упрощает анализ информации. Обычно используется для изменения формы, сортировки, создания срезов, агрегирования и объединения данных. Библиотека построена поверх библиотеки NumPy -еще одной библиотеки Python для работы с массивами. NumPy работает значительно быстрее, чем традиционные списки Python, поскольку данные хранятся в одном непрерывном месте памяти. Pandas работает с тремя ключевыми структурами данных: ♦series: одномерный массив, который содержит массив данных и массив меток данных; массив меток называется index и может быть указан автоматически с помощью целых чисел от О до п-1, если он явно не задан пользователем. ♦DataFrame: представление табличных данных, вроде электронной таблицы со спи­ ском столбцов; объект DataFrame помогает хранить и обрабатывать табличные данные в строках и столбцах; следует отметить, что DataFrame имеет индекс, как для столбцов, так и для строк. ♦Panel: трехмерный контейнер данных.\n--- Страница 190 ---\nГлава 6. Дополнительные советы и приемы Python 193 DataFrame -это ключевая структура, которая используется для анализа данных. В оставшейся части этого подраздела мы будем широко использовать объект DataFrame в наших примерах. Прежде чем обсуждать его расширенные возможно­ сти, кратко рассмотрим основные операции, доступные для объектов DataFrame. Операции с объектом DataFrame Существует несколько способов создания объекта DataFrame, например, из слова­ ря, СSV-файла, листа Excel или массива NumPy. Один из самых простых -ис­ пользовать данные из словаря в качестве ввода. Рассмотрим на примере, как можно создать объект DataFrame на основе еженедельных данных о погоде, хранящихся в словаре: tpandasl.py import pandas as pd weekly_data = {'day': ['Monday', 'Tuesday', 'Wednesday', \\ 'Thursday', 'Friday', 'Saturday', 'Sunday'], 'temperature': (40, 33, 42, 31, 41, 40, 30], 'condition' :['Sunny', 'Cloudy', 'Sunny', 'Rain' , 'Sunny', 'Cloudy', 'Rain'] df = pd.DatAFrame(weeltly_data) print (df) На консоли отобразится содержимое DataFrame (рис. 6.2): day temp condition 0 Monday 40 Sunny 1 Tuesday 33 Cloudy 2 Wednesday 42 Sunny 3 Thursday 31 Rain 4 Friday 41 ·Sunny 5 Saturday 40 Cloudy б Sunday 30 Rain Рис. 6.2. Содержимое DataFrame Библиотека Pandas богата методами и атрибутами, однако здесь мы рассмотрим только самые часто используемые: ♦index: предоставляет список индексов (или меток) объекта DataFrame; ♦columns: предоставляет список столбцов в объекте DataFrame;\n--- Страница 191 ---\n194 Раздел 2. Расширенные концепции программирования ♦size: возвращает размер объекта DataFrame в виде количества строк, умножен­ ного на количество столбцов; ♦shape: предоставляет кортеж, представляющий измерение объекта DataFrame; ♦axes: возвращает список, который представляет оси объекта DataFrame; просты­ ми словами, включает в себя строки и столбцы; ♦describe: генерирует статистические данные; например, количество, среднее зна­ чение, стандартное отклонение, минимальное и максимальное значения; ♦head: возвращает п (по умолчанию 5) строк из объекта DataFrame, аналогично команде head для файлов; ♦tail: возвращает последние п (по умолчанию 5) строк из объекта DataFrame; ♦drop_duplicates: удаляет дубликаты строк на основе всех столбцов в DataFrame; ♦dropna: передавая соответствующие аргументы этому методу, можно удалить отсутствующие значения (например, строки и столбцы); можно также указать, строки и столбцы будут удаляться на основе одного пропущенного значения или при отсутствии всех значений в строке или столбце; ♦sort _ values: можно использовать для сортировки строк на основе одного или не­ скольких столбцов. Дальше рассмотрим базовые операции с объектами DataFrame. Настройка пользовательского индекса Метки столбцов (индексы) обычно добавляются в соответствии с данными, предос­ тавленными словарем или любым другим потоком данных. Можно изменить ин­ декс DataFrame следующими способами: 1.Задать один из столбцов в качестве индекса (например, day из предыдущего примера), используя следующий простой оператор: df_new = df.set_index('day') 2.DataFrame начнет использовать столбец day в качестве индекса следующим об­ разом (рис. 6.3): temp condition day Monday 40 Sunny Tuesday 33 Cloudy Wednesday 42 Sunny Thursday 31 Rain Friday 41 Sunny Saturday 40 Cloudy Sunday 30 Rain Рис. 6.3. Содержимое DataFrame после применения столбца day в качестве индекса\n--- Страница 192 ---\nГлава 6. Дополнительные советы и приемы Python 3.Задать индекс вручную, передав его через список, как в следующем примере: #pandas2.py weekly_data = <данные как в предыдущем примере> df = pd.DataFrame(weekly_data) df.index = [ 'КJN', 'ТUЕ', 'WED', 'ТНU', 'FRI', 'SAT', 'SUN'] print (df)195 4.В этом случае DataFrame начнет использовать индекс, предоставленный через список. Содержимое DataFrame изменится следующим образом (рис. 6.4): day temp condition MON Мonday 40 Sunny TUE Tuesday 33 Cloudy WED Wednesday 42 Sunny THU Thursday 31 Rain FRI Friday 41 Sunny SAT Saturday 40 Cloudy SUN Sunday 30 Rain Рис. 6.4. Содержимое DataFrame после создания столбца индекса вручную Далее узнаем, как перемещаться по DataFrame. Навигация внутри DataFrame Существует несколько десятков способов получить строку данных или определен­ ное расположение из объекта. Распространенными являются методы loc и iloc. Рас­ смотрим несколько способов навигации по DataFrame, используя набор данных из предыдущих примеров: #pandas3.py import pandas as pd weekly_data = <хах в примере pandasl.py> df = pd.DataFrame(weekly_data) df. index = [ 'MON' , 'TUE' , 'WED' , 'THU' , 'FRI ' , 'SAT' , 'SUN' ] Существует несколько способов выбрать строку или расположение в объекте: ♦Мы можем выбрать одну или несколько строк, используя метки индекса с мето­ дом loc; метка предоставляется как отдельный элемент или список; ниже приве­ ден фрагмент с двумя примерами, как можно выбрать одну или несколько строк: print (df. loc[ 'ТUЕ']) print (df.loc[ ,[ 'ТUЕ', 'WED']])\n--- Страница 193 ---\n196 Раздел 2. Расширенные концепции программирования ♦Мы можем выбрать значение из расположения в объекте DataFrame по метке строки и метке столбца: print (df .loc[ 'FRI', 'tenp']) ♦Мы можем выбрать строку по значению индекса без указания меток: #предоставляем строку с индексом 2 print(df.iloc[2]) ♦Мы можем выбрать значение из расположения, используя индексы строки и столбца, путем обработки объекта DataFrame как двумерного массива; в сле­ дующем фрагменте кода мы получаем значение из расположения с индексом строки, равным 2, и индексом столбца, равным 2: print(df.iloc[2,2]) Далее обсудим способы, как можно добавить строку или столбец в объект DataFrame. Добавление строки или столбца в DataFrame Самый простой способ добавить строку в DataFrame -присвоить список значений расположению индекса или его метке. Например, мы можем добавить новую стро­ ку с меткой тsт для предыдущего примера (pandasЗ.py) с помощью следующей инст­ рукции: df.loc['TST'] = ['Test day 1', 50, 'NA'] Важно отметить, что если такая метка строки уже существует в объекте, этот же код может обновить новые значения в строке. Если мы используем не метку индекса, а индекс по умолчанию, можно использо­ вать номер индекса для обновления существующей строки или добавления новой с помощью следующего кода: df.loc[B] = ['Test day 2', 40, 'NA'] Для наглядности покажем полный пример кода: #pandas4.py import pandas as pd weekly_data = <JCёUC в примере pandasl.py> df = pd.DataFrame(weekly_data) df. index = [ 'МОN' , 'TUE' , 'WED' , 'THU' , 'FRI ' , 'SAT' , 'SUN' ] df.loc['TSTl'] = ['Test day 1', 50, 'NA'] df.loc[7] = ['Testday2', 40, 'NA'] print (df) В библиотеке Pandas доступны разные способы добавления нового столбца в DataFrame. Рассмотрим три из них: ♦Добавление списка значений после метки столбца: при таком подходе новый столбец добавляется после существующих столбцов; если указать существую­ щую метку столбца, это позволит заменить или обновить значения в нем.\n--- Страница 194 ---\nГлава 6. Дополнительные советы и приемы Python 197 ♦Метод insert: принимает метку и список значений в качестве аргументов; это особенно удобно, когда мы хотим вставить столбец в любое место; обратите внимание, метод не позволяет вставить столбец, если он уже существует с такой же меткой; это означает, что метод нельзя использовать для обновления. ♦Метод assign: позволяет добавлять несколько столбцов одновременно; если ука- зать существующую метку, значения в столбце будут заменены или обновлены. В следующем примере кода мы используем все три подхода для вставки нового столбца в объект DataFrame: lpandaaS.'P'f iuport pandaa \" pd weekly_data = <как в примере pandasl.py > df = pd.DataFrame(weekly_data) #Добавляем столбец и обновляем его df['Humidityl'] [60, 70, 65,62,56,25, ''] df['Humidityl'] = [60, 70, 65,62,56,251, ''] #Вставляем столбец с индексом 2, используя insert df.inaert(2, \"Humidity2\", [60, 70, 65,62,56,25, \"]) #Добавляем 2 столбца методом assign dfl = df.uaign(HumidityЗ = [60, 70, 65,62,56,25, ''], Humidity4 = [60, 70, 65,62,56,25, ' ']) print(dfl) Далее рассмотрим, как удалять строки и столбцы из объекта DataFrame. Удаление индекса, строки или столбца из DataFrame Удалить индекс достаточно просто. Это можно сделать методом reset_index. Одна­ ко он добавляет индексы по умолчанию и сохраняет столбец с пользовательскими индексами в качестве столбца данных. Для полного удаления столбца вместе с ме­ тодом необходимо использовать аргумент drop. Продемонстрируем на примере: lpandaaб.'P'f import pandas as pd weekly_data = <хах а DрН118р8 pandaal.'P'f > df = pd. DataFraшe (weekly_ dat&) df.index = ['МОN', 'TUE', 'WED', 'THU', 'FRI', 'SAT','SAT'] print(df) print (df.reиt_indu(drop=Тrue) )\n--- Страница 195 ---\n198 Раздел 2. Расширенные концепции программирования Для удаления повторяющейся строки из DataFrame можно использовать метод drop _ duplicate. Для удаления определенной строки или столбца можно использовать метод drop. В следующем примере удалим все строки с метками SАТ и SUN, а также столбцы с меткой condition: #pandas7.py import pandas as pd weekly_data = <хах в примере pandasl.py> df = pd.DataFrame(weekly_data) df.index = ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN'] print (df) dfl= df.drop(index=['SUN', 'SAT']) df2= dfl.drop(column s=['condition']) print (df2) Теперь рассмотрим, как переименовать индекс или столбец. П�реименование индексов и столбцов в DataFrame Для переименования меток индекса или столбца используется метод rename. Пример кода выглядит следующим образом: #pandas8.py import pandas as pd weekly_data = <хах в примере pandasl.py> df = pd.DataFrame(weekly_data) df. index = [ 'MON' , 'TUE' , 'WED' , 'THU' , 'FRI ' , ' SAT' , ' SUN' ] dfl=df .rename (index={ 'SUN': 'SU', 'SAT': 'SA'}) df2=dfl.rename(columns=('condition':'cond'}) print (df2) Обратите внимание, текущая и новая метки для индекса и столбца предоставляются в виде словаря. Далее изучим некоторые дополнительные приемы использования объектов DataFrame . Сложные случаи использования DataFrame На данном этапе мы рассмотрели основные операции, которые можно выполнять с объектом DataFrame. Теперь перейдем к более сложным операциям для вычисле­ ния и преобразования данных.\n--- Страница 196 ---\nГлава 6. Дополнительные советы и приемы Python 199 Замена данных Часто возникает необходимость заменить числовые или строковые данные другим набором значений. Библиотека Pandas имеет много инструментов для этого. Наи­ более популярным является метод at. Он обеспечивает простой способ доступа или обновления данных в любой ячейке DataFrame. Для массовой замены значений дос­ тупен метод replace, который можно использовать разными способами. Например, для замены одного числа на другое или одной строки на другую, а также для заме­ ны целого фрагмента, заданного регулярным выражением. Кроме того, можно ис­ пользовать метод для замены любых записей, предоставленных через список или словарь. В следующем примере (pandastrickl .ру) рассмотрим эти варианты, исполь­ зуя объект DataFrame из предыдущих примеров: #pandastrickl.py import pandas as pd weekly_data = <как в примере pandasl.py> df = pd.DataFrame(weekly_data) Последовательно применим несколько операций замены в этом примере: ♦Заменим все вхождения числового значения 40 на 39, используя следующий опе­ ратор: df.replace(40,39, inplace=True) ♦Заменим все вхождения строки sunny на sun с помощью следующей строки: df.replace(\"Sunny\",\"Sun\",inplace=True) ♦Заменим все вхождения строки на основе регулярного выражения (а именно, Cloudy на Cloud) с помощью следующего фрагмента: df.replace(to_replace=\"лCl.*\",value=\"Cloud\", inplace=True,regex=True) iили также можно применить к определенному столбцу df[\"condition\"] .replace(to_replace =\"лCl.*\",value =\"Cloud\", inplace=True,regex=True) ♦Обратите внимание, что использование меток to _ replace и value опционально. ♦Заменим все вхождения нескольких строк, представленных списком, другим списком строк: df. replace ( [\"Monday\", \"Tuesday\"] , [\"Mon\", \"Tue\"], inplace=True) ♦Здесь мы заменили Monday и Tuesday на Mon и Tue. ♦Заменим все вхождения нескольких строк, используя пары «ключ-зн.ачен.ие» в словаре: df.replace({\"Wednesday\":\"Wed\",\"Thursday\":\"Thu\"}, inplace=True)\n--- Страница 197 ---\n200 Раздел 2. Расширенные концепции программирования ♦ В этом случае ключи словаря (wednesday и Thursday) будут заменены соответст­ вующими значениями (wed и Thu). ♦Заменим все вхождения строки для определенного столбца, используя несколько словарей; это можно сделать, используя имя столбца в качестве ключа и сле­ дующий оператор: df. replace { { \"day\": \"Friday\"}, { \"day\": \"Fri\"}, inplace =True) ♦ В этом сценарии первый словарь указывает на имя столбца и заменяемое значе­ ние; второй словарь указывает на то же имя столбца, но с новым значением, ко­ торое заменит исходное; в нашем случае мы заменим все вхождения Friday в столбце day на Fri. ♦Заменим все вхождения нескольких строк с помощью вложенного словаря: df. replace { { \"day\": { \"Saturday\" : \"Sat\", \"Sunday\": \"Sun\"}, \"condition\":{\"Rainy\":\"Rain\"}}, inplace =True) ♦Внешний словарь (с ключами day и condition) используется для определения столбцов для этой операции; внутренний словарь хранит заменяемые данные вместе с замещающими значениями; с помощью этого подхода мы заменили Saturday и Sunday на Sat и Sun внутри столбца day, а также Rainy на Rain внутри столбца condi tion. Полный код с этими операциями доступен в файле pandastrickl. ру в репозитории к этой главе. Обратите внимание, что можно запустить операцию замены во всем объекте DataFrame или только в определенном столбце или строке. ВАЖНОЕ ПРИМЕЧАНИЕ Аргумент inplace =True задействован во всех вызовах метода replace. Он использует­ ся для настройки вывода метода replace внутри того же объекта DataFrame. Пара­ метр по умолчанию вернет новый объект, не меняя исходный. Для удобства этот ар­ гумент доступен со многими методами DataFrame. Применение функций к столбцу или строке объекта DataFrame Иногда нужно очистить, скорректировать или преобразовать данные перед их ана­ лизом. Простой способ применить функцию к DataFrame -использовать методы apply, applymap или map. Метод apply применим к столбцам или строкам, а метод applymap работает поэлементно для всего DataFrame. Метод map также работает по­ элементно, но для одного ряда. Рассмотрим пару примеров кода для демонстрации работы методов. Часто импортированные в DataFrame данные нуждаются в очистке. Например, они могут иметь пробелы в конце или в начале, символы новой строки и другие неже­ лательные элементы. Их можно легко удалить, используя метод map и лямбда­ функцию в рядах столбцов. Лямбда-функция применяется к каждому элементу\n--- Страница 198 ---\nГлава 6. Дополнительные советы и приемы Python 201 столбца. В нашем примере сначала удалим конечный пробел, точку и запятую. За­ тем удалим начальный пробел, подчеркивание и тире в столбце condition . После очистки данных в столбце condition мы создадим столбец temp_F из значений столбца temp, а затем преобразуем эти значения из градусов Цельсия в градусы Фа­ ренгейта. Обратите внимание, мы будем использовать другую лямбда-функцию для этого преобразования и метод apply. Когда мы получим результат от метода apply, сохраним его внутри новой метки столбца (temp_F) для создания нового столбца. Пример кода следующий: #pandastrick2.py import pandas as pd weekly_data { 'day': [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], 'temp': [40, 33, 42, 31, 41, 40, 30), 'condition' : ['Sunny, ', '_Cloudy ', 'Sunny', 'Rainy', '--Sunny. ', 'Cloudy. ', 'Rainy'] df = pd.DataFrame{weekly_data) print{df) df[\"condition\"] = df[\"condition\"] .map{ lamЬda х: х. lstrip ( '_-' ) . rstrip ( ' , . ' ) ) df[\"temp_F\"] = df[\"temp\"] .apply{lamЬda х: 9/5*х+З2 ) print {df) Обратите внимание, в этом коде мы предоставили те же входные данные, что и в прошлых примерах, за исключением начальных и конечных символов в столбце condition. Запрос строк в объекте DataFrame Запросить строки на основе значений в определенном столбце можно одним из распространенных подходов -применив фильтр с использованием логического «И/ИЛИ». Однако это быстро становится запутанным для простых задач вроде по­ иска строки со значением в определенном диапазоне. Библиотека pandas предлагает более аккуратный способ: метод between, который похож на ключевое слово between вSQL. В примере ниже используется тот же объект weekly_data, что и раньше. Сначала мы покажем использование традиционного фильтра, а затем продемонстрируем ис­ пользование метода between для запроса строк со значениями температуры от 30 до 40 включительно: #pandastrickЗ.py import pandas as pd weekly_data = <xaJC з примере pandasl.py> df = pd.DataFrame{weekly_data)\n--- Страница 199 ---\n202 Раздел 2. Расширенные концепции программирования print (df [ (df. terrp >= 30) & (df. tenp<:=40) ]) print(df[df.temp.Ьetween(З0,40 )]) \"в обоих случаях мы получим одинаковый консольный вывод. Однако использовать меtод between удобнее, чем фильтры с условиями. \"'· Запрос строк на основе текстовых данных также хорошо поддерживается библио- текой Pandas. Этого можно добиться методом доступа str для столбцов строкового типа в объекте DataFrame. А именно, если мы хотим найти строки в объекте weekly_data на основе состояния дня (например, Rainy или Sunny), мы можем написать традиционный фильтр или применить str к столбцу вместе с методом contains. Следующий пример демонстрирует оба варианта получения строк со значениями Rainy ИЛИ Sunny в столбце condi tion: #pandastrick4.py import pandas as pd weekly_data = <ках в примере pandasl.py> df = pd.DataFrame(weekly_data) print(df[(df.condition='Rainy') 1 (df.condition='Sunny')]) print (df [df [ 'condition' ] .str .contains ( 'Rainyl Sunny')]) Оба подхода дадут одинаковый результат. Получение статистики по данным объекта DataFrame Получить статистические данные, например, центральную тенденцию, стандартное . отклонение и форму можно, используя метод describe. Вывод метода для числовых столбцов включает следующее: ♦count (количество); ♦mean ( среднее значение); ♦standard deviation (стандартное отклонение); ♦min (минимальное значение); ♦max (максимальное значение); ♦25th percentile (25-й перцентиль), 50th percentile (50-й перцентиль), 75th percentile (75-й перцентиль). Разделение перцентилей по умолчанию можно изменить с помощью аргумента percentiles. Если метод describe используется для нечисловых данных вроде строк, мы можем получить от них count, unique, top и freq. Где top -наиболее распространенное зна­ чение, а freq -частота наиболее распространенного значения. По умолчанию только числовые столбцы оцениваются методом describe, если мы не предоставим аргумент include с соответствующим значением.\n--- Страница 200 ---\nГлава 6. Дополнительные советы и приемы Python 203 В примере ниже мы вычислим следующую статистику для объекта weekly_date: ♦Использование метода describe с аргументом include или без него. ♦Использование аргумента percentiles с методом describe. ♦Использование метода groupby для группировки данных по столбцам, а затем применение метода describe в завершение. Полный пример получится следующим : #pandastrickS. py import pandas as pd import numpy as np pd.set_option('display.max_columns', None) weekly_data = <:ках в примере pandasl.py> df = pd.DataFrame(weekly_data) print(df.describe()) print(df.describe(include =\"all\")) print(df.describe(percentiles=np.arange(0, 1, 0.1))) print(df.groupby('condition') .describe(percentiles =np.arange(0, 1, 0.1))) Обратите внимание, что мы изменили параметры max _ columns в самом начале для отображения всех столбцов, которые ожидаются в выводе. В противном случае не­ которые столбцы будут усечены для консольного вывода метода groupby. На этом можно завершить изучение расширенных приемов работы с объектом DataFrame. С их помощью любой может начать использовать библиотеку Pandas для анализа данных. Для изучения дополнительных приемов можно обратиться к официальной документации по библиотеке. Заключение В этой главе мы рассмотрели некоторые продвинутые приемы и хитрости для на­ писания эффективных и лаконичных программ на Python. Мы начали с расширен­ ных функций, таких, как map, reduce и filter. Затем· изучили несколько концепций: внутренние функции, лямбда-функции и декораторы. Также обсудили использова­ ние структур данных, включая вложенные словари и включения. В конце мы рас­ смотрели основные операции с объектом DataFrame и коснулись некоторых реаль­ ных случаев их использования. Эта глава, в основном, сосредоточена на практических знаниях и опыте использо­ вания передовых концепций Python. Это важно всем, кто хочет разрабатывать при­ ложения на Python, особенно связанные с анализом данных. Примеры кода, пред­ ставленные в главе, наглядно демонстрируют работу расширенных приемов, дос­ тупных для функций, структур данных и библиотеки Pandas.\n--- Страница 201 ---\n204 Раздел 2. Расширенные концепции программирования Следующая глав.а посвящена многопроцессорной обработке и многопоточности в Python. Вопросы 1.Какие из функций map, filter и reduce являются встроенными в Python? 2.Какие декораторы являются стандартными? 3.Что предпочтительнее . выбрать для большого набора данных -списковое включение или генераторное включение? 4.Что такое DataFrame в контексте библиотеки pandas? 5.Зачем нужен аргумент inplace в методах библиотеки pandas? Дополнительные ресурсы ♦«Mastering Python Design Patterns», автор: Сакис Касампалис (Sakis Kasampalis). ♦«Python и анализ данных» (Python for Data Analysis), автор: Уэс Маккини (Wes McКinney). ♦«Hands-On Data Analysis with Pandas», второе издание, автор: Стефани Молин (Stefanie Molin). ♦Официальная документация Pandas: https://pandas. pydata.org/docs/. Ответы 1.Встроенными являются функции map и filter. 2.Декораторы называются стандартными, когда не имеют аргументов. 3.В этом случае предпочтительнее генераторное включение. Оно эффективно ис­ пользует память, поскольку значения генерируются по-одному. 4.DataFrame -это представление табличных данных, которое широко использу­ ется для анализа данных с помощью библиотеки pandas. 5.Когда для аргумента inplace в методах pandas установлено значение тrue, резуль­ тат операции сохраняется в том же объекте DataFrame, к которому применяется операция.\n--- Страница 202 ---\nРаздел 3 Масштабирование за пределы одного потока Этот раздел книги посвящен программированию масштабируемых приложений. Обычно интерпретатор Python использует один поток, работающий в одном про­ цессе. В этой части книги мы обсудим, как масштабировать приложение за пределы одного потока. Для этого мы затронем такие понятия, как многопоточность, много­ процессорная обработка и асинхронное программирование на одной машине. Затем исследуем, как можно использовать несколько машин и выполнять приложения в кластерах с помощью Apache Spark. В конце рассмотрим среды облачных вычисле­ ний, которые позволят сосредоточиться на приложении и оставить управление ин­ фраструктурой поставщикам облачных услуг. В этот раздел входят следующие главы: ♦«",
      "debug": {
        "start_page": 168,
        "end_page": 202
      }
    },
    {
      "name": "Глава 7. Многопроцессорная обработка, многопоточность и асинхронное программирование 207",
      "content": "--- Страница 202 --- (продолжение)\nГлава 7: Многопроцессорная обработка, многопоточность и асинхронное про­ граммирование». ♦«Глава 8: Масштабирование приложений Python с помощью кластеров». ♦«Глава 9: Программирование на Python для облака».\nГлава 7: Многопроцессорная обработка, многопоточность и асинхронное про­ граммирование». ♦«Глава 8: Масштабирование приложений Python с помощью кластеров». ♦«Глава 9: Программирование на Python для облака».\n--- Страница 203 ---\n7 Многопроцессорная обработка, многопоточность и асинхронное программирование Мы можем написать эффективный код и оптимизировать его для быстрого выпол­ нения, но ресурсы, доступные процессам, всегда ограничены. Для ускорения рабо­ ты некоторые задачи можно запускать параллельно на одном или нескольких ком­ пьютерах. Эта глава посвящена параллельной обработке для приложений }?ython, выполняющихся на одной машине. О выполнении на нескольких машинах мы по­ говорим в следующей главе. А сейчас сосредоточимся на встроенной поддержке для реализации параллельных вычислений. Начнем с многопоточности, а затем пе­ рейдем к многопроцессорности. После узнаем, как проектировать быстрые системы с помощью асинхронного программирования. Для каждого из подходов мы рас­ смотрим полноценный практический пример реализации конкурентного приложе­ ния для загрузки файлов с Google Диска. Темы этой главы: ♦Многопоточность в Python и ее ограничения. ♦Многопроцессорная обработка. ♦Асинхронное программирование для адаптивных систем. Вы узнаете о разных вариантах создания многопоточных и многопроцессорных приложений с использованием встроенных библиотек Python. Эти знания помогут вам создавать не только более эффективные приложения, но и масштабные проек­ ты для большого количества пользователей.\n--- Страница 204 ---\n208 Раздел 3. Масштабирование за пределы одного потока Технические требования Для этой главы понадобится: ♦Python 3.7 или более поздней версии. ♦Аккаунт Google Диска. ♦Ключ API для аккаунта Google Диска. Примеры кода к этой главе можно найти по адресу: https://github.com/PacktPuЬlishing/Python-for-Geeks/tree/master/Chapter07. Многопоточность в Python и ее ограничения Поток (Thread) -это базовая единица выполнения в операционной системе, со­ стоящая из счетчика команд, стека и набора регистров. Процессы приложения мо­ гут быть построены с использованием нескольких потоков, которые могут работать одновременно и совместно использовать одну память. При многопоточности все потоки процесса используют общий код и другие ресур­ сы вроде данных и системных файлов. Для каждого потока вся связанная с ним ин­ формация хранится как структура данных внутри ядра операционной системы и называется Блок Управления Потоком (Thread Control Block, ТСВ). ТСВ состоит из нескольких основных компонентов: ♦Счетчик команд (Program Counter, РС): отслеживает ход выполнения про- граммы. ♦Системные регистры (System Registers, REG): хранят переменные данные. ♦Стек (Stack): массив регистров, который управляет историей выполнения. На рис. 7.1 приведена схема с тремя потоками. Каждый из них имеет собственный счетчик команд, стек и системный регистр, но использует общий код и прочие ре­ сурсы с другими потоками. Блок управления также содержит идентификатор потока, состояние потока (выпол­ няется, ожидает или остановлен) и указатель на процесс, которому он принадлежит. Мн.огопоточн.ость -это концепция операционной системы, которая предлагается системным ядром. ОС обеспечивает параллельное выполнение нескольких потоков в контексте одного процесса, позволяя потокам совместно использовать память. Это означает, что полный контроль над управлением потоков имеет ОС, а не при­ ложение. Этот момент важно подчеркнуть для последующего обсуждения различ­ ных вариантов параллелизма. При выполнении потоков на компьютере с одним цен.тральным процессором (ЦП) операционная система переключает его с одного потока на другой, поэтому кажет­ ся, что потоки выполняются параллельно. Дает ли многопоточность с одним ЦП преимущества, зависит от характера приложения. Если оно использует только ло-\n--- Страница 205 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное 209 кальную память, преимуществ, скорее всего, нет. Более того, производительность может упасть из-за издержек, связанных с переключением потоков. Если приложе­ ние зависит от других ресурсов, выполнение может ускориться благодаря более эффективному использованию ЦП. Пока один поток ждет ресурс, другой может использовать процессор. Системный регистр Системный регистр Системный регистр Стэк Стэк Стэк Счетчик команд Счетчик команд ' Счетчик команд Поток 1 Поток 2 Поток З Рис. 7.1. Несколько потоков в процессе При наличии нескольких процессоров или ядер ЦП потоки можно выполнять па­ раллельно. Рассмотрим ограничения многопоточного программирования. Слепое пятно Python С точки зрения программирования многопоточнщ:ть -это подход, при котором мы выполняем разные части приложения одновременно. Python использует множе­ ство потоков ядра для выполнения пользовательских потоков. Реализация Python (CPython) позволяет потокам обращаться к объектам через глобальную блокировку интерпретатора (Global Interpreter Lock, GIL). В двух словах, GIL -это мью­ текс (mutex, механизм взаимного исключения), который позволяет использовать интерпретатор только одному потоку за раз и блокирует остальные потоки. Это не­ обходимо для защиты счетчика ссылок от сборки мусора. Без такой защиты счет­ чик может быть поврежден, если он обновляется несколькими потоками одновре­ менно. Причиной такого ограничения является защита внутренних структур дан­ ных интерпретатора и стороннего кода С, который не является потокобезопасным.\n--- Страница 206 ---\n210 Раздел 3. Масштабирование за пределы одного потока ВАЖНОЕ ПРИМЕЧАНИЕ Это ограничение глобальной блокировки интерпретатора отсутствует в других реали­ зациях Python -Jython и lronPython. Создается впечатление, что написание многопоточных программ на Python не име­ ет преимуществ, но это не так. Мы по-прежнему можем писать код, который будет выполняться одновременно или параллельно, и мы можем увидеть это на примере. Многопоточность полезна в следующих случаях: 1.Задачи ввода-вывода: при наличии нескольких операций ввода-вывода можно повысить производительность, используя несколько потоков. Пока один поток ждет ответа от ресурса, он освобождает GIL и позволяет работать другим пото­ кам. Исходный поток возобновится после ответа от ресурса ввода-вывода. 2.Адаптивные GUI-приложения: в них требуется отображать ход выполнения задач, запущенных в фоновом режиме (например, загрузка файла), а также да­ вать пользователю взаимодействовать с другими компонентами интерфейса. Это возможно благодаря отдельным потокам для действий, инициированных поль­ зователем в графическом интерфейсе. 3.Многопользовательские приложения: здесь также необходима многопоточ­ ность, например, для веб-сервера или файлового сервера. В таких приложениях при поступлении нового запроса в основном потоке создается новый поток, ко­ торый будет обслуживать этот запрос, пока основной поток ожидает новых за­ просов. Прежде чем рассмотреть концепцию на примере, познакомимся с ключевыми ком­ понентами многопоточного программирования на Python. Ключевые компоненты многопоточного программирования на Python Многопоточность в Python позволяет выполнять разные компоненты программы одновременно. Для создания нескольких потоков приложения используется модуль threading, основные компоненты которого рассмотрим далее. Модуль threading Модуль threading является стандартным и предоставляет простые методы для соз­ дания нескольких потоков программы. Внутри он использует низкоуровневый мо­ дуль _ thread, который часто использовался для реализации много поточности в бо­ лее ранних версиях Python. Для нового потока создадим объект класса Thread, который может принимать имя функции в качестве атрибута target и аргументы в качестве атрибута args для пере­ дачи в функцию. Потоку можно при создании задать имя с помощью аргумента name в конструкторе.\n--- Страница 207 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное 211 После создания объекта нужно запустить поток методом start. Реализовать, что основная программа или поток будут ждать завершения созданных потоковых объ­ ектов, можно с помощью метода join. Этот метод гарантирует, что главный поток (вызывающий поток) ожидает, пока поток, для которого вызван метод join, не за­ вершит выполнение. Для демонстрации этих процессов создадим простую программу с тремя потоками. Полный пример кода приведен ниже: #threadl.py создание простых потоков с помощью функции from threading import current_thread, Thread as Thread from time import sleep def print_hello(): sleep(2) print (\"{}: Hello\" .format (current_thread() .name)) def print_messaqe(msg): sleep (1) print (\" {}: {}\". format (current_ thread () .name, msg)) #создаем потоки tl = Тhread(tarqet=print_hello, name=\"Тh 1\") t2 = Thread(target=print_hello, name=\"Th 2\") tЗ = Тhread(tarqet=print_messaqe, arqs=[\"Good morninq\"],name=\"Тh 3\") #запускаем потоки tl.start() t2. start () tЗ.start () #ждем, пока все завершатся tl.join() t2. join () tЗ. join () В этой программе мы сделали следующее: ♦Создали две простые функции, print_hello и print_message, которые будут ис­ пользоваться потоками; задействовали функцию sleep из модуля time в обеих функциях с целью убедиться, что они завершают выполнение в разное время. ♦Создали три объекта Thread, два из которых будут выполнять одну функцию (print_hello) для демонстрации совместного использования кода потоками, а третий будет использовать вторую функцию (print_message), которая также при­ нимает один аргумент.\n--- Страница 208 ---\n212 Раздел 3. Масштабирование за пределы одного потока ♦Запустили все три потока один за другим, используя метод start. ♦Дождались завершения каждого потока с помощью метода join. Объекты Thread можно хранить списком для упрощения операций start и join, ис­ пользуя цикл for. Вывод консоли будет следующим: Тh 3: Good moming Тh 2: Hello Тh 1: Hello Потоки 1 и 2 имеют большее время ожидания, чем поток 3, поэтому он всегда бу­ дет завершаться первым. А потоки 1 и 2 могут завершаться в любом порядке, в за­ висимости от того кто первый получит процессор. ВАЖНОЕ ПРИМЕЧАНИЕ По умолчанию метод join блокирует вызывающий поток на неопределенное время. Но можно указать время ожидания (в секундах) в качестве аргумента для него. Это приведет к блокировке лишь на время ожидания. Рассмотрим еще несколько понятий, прежде чем перейти к более сложному примеру. Потоки-демоны В приложении главная программа неявным образом ждет завершения всех осталь­ ных потоков. Но иногда необходимо запустить некоторые из них в фоновом режи­ ме, не блокируя основную программу. Такие потоки назьmаются потоками­ демонами (Daemon Thread). Они остаются активными, пока выполняется главная программа (с обычными потоками). А когда основные потоки завершатся, нор­ мальной практикой считается завершить демоны тоже. Потоки-демоны часто ис­ пользуют в ситуациях, когда не будет ошибкой, если поток внезапно завершится посреди выполнения без потери или повреждения данных. Поток может быть объявлен демоном, используя один из следующих подходов: ♦Передать атрибут daemon со значением True конструктору (ctaemon = тrue). ♦Задать для атрибута daemon значение True в экземпляре потока (thread.daemon = True). Если поток установлен как демон, можно запустить его и забыть. Он завершится автоматически после окончания программы, которая его вызвала. В следующем фрагменте кода приводится использование демона и обычных потоков: #thread2.py создание обычных потоков и демонов from threading import current_thread, Thread as Thread from time import sleep de f daean _ func () : #print(threading.current_thread() .isDaemon())\n--- Страница 209 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное sleep (3) print(\"{}: Hello from daemon\".format(current_thread() .name)) def nondaean_func(): #print(threading.current_thread() .isDaemon()) sleep (1) print (\" {}: Hello from non-daemon\" . format (current_thread () .name)) #создаем потоки tl = Thread(target=daeom_func, name=\"Daemon Thread\", daemon=Тrue) t2 = Thread(target=nondaeom_func, name=\"Non-Daemon Thread\") #запускаем потоки tl. start () t2. start () print(\"Exiting the main program\") 213 В примере мы создали один поток-демон и один обычный поток. Поток-демон (daeom_func) выполняет функцию со временем ожидания 3 секунды, а обычный по­ ток выполняет функцию (nondaeom_func) со временем ожидания 1 секунда. Время ожидания устанавливается таким образом, что обычный поток завершит свое вы­ полнение первым. Консольный вывод будет следующим: Exitinq the main program Non-Dumon Тhread: Вello fran non-daemon Поскольку мы не использовали метод join ни в одном потоке, сначала завершится основной поток, а затем немного позже завершится обычный поток, выводя рrint­ сообщение. От потока-демона никаких сообщений не будет, так как он завершится сразу после обычного потока. Если изменить время ожидания функции nondaeom _ func на 5 секунд, получим следующие выходные данные: Exi tinq the main program Dualon Тhread: Вello fran dulllon Non-Daemon Тhread: Вello fran non-daemon Задерживая выполнение обычного потока, мы гарантируем, что поток-демон за­ вершит свое выполнение и не будет внезапно прерван. ВАЖНОЕ ПРИМЕЧАНИЕ Если применить метод j oin к демону, главному потоку придется ждать его завершения. Теперь рассмотрим, как синхронизировать потоки в Python.\n--- Страница 210 ---\n214 Раздел 3. Масштабирование за пределы одного потока Синхронизация потоков Синхронизация потоков -это механизм, который не позволяет двум или более потокам выполнять один блок кода одновременно. Блок кода, который обращается к общим данным или ресурсам, называется критической секцией (Critical Section). Наглядная схема показана ниже (рис. 7.2): Рис. 7.2. Два потока обращаются к критической секции программы Критическая секция Когда несколько потоков, обращающихся к критической секции, одновременно пы­ таются получить доступ к данным или изменить их, результаты могут быть непред­ сказуемыми. Такая ситуация называется состоянием гонки (Race condition). Для демонстрации реализуем простую программу с двумя потоками, где каждый из них выполняет операцию инкремента общей переменной миллион раз. Большое число выбрано специально для наблюдения за состоянием гонки. Его также можно наблюдать, используя меньшее значение переменной на медленном ЦП. Оба потока будут использовать одну функцию (inc) в качестве целевой. Код для доступа к об­ щей переменной и увеличения ее на 1 находится в критической секции, и оба пото­ ка обращаются к ней без какой-либо защиты: #threadЗa.py без синхронизации потоков from threading import Thread as Thread def inc (): global х for in range(lOOOOOO): x+=l tглобальная переменная х = о iсоздаем потоки tl = Thread(target=inc, name=\"Th 1\") t2 = Thread(target=inc, name=\"Th 2\")\n--- Страница 211 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное #запускаем потоки tl. start () t2.start() #ожидаем завершения потоков tl. join () t2. join () print(\"final value of х :\", х) 215 Ожидаемый вывод должен иметь значение 2 ООО ООО, но этого не происходит. При каждом выполнении программы мы получаем разное значение х, которое намного меньше 2 ООО ООО. Причина кроется в состоянии гонки между потоками. Рассмот­ рим сценарий, при котором потоки Th 1 и Th 2 выполняют ;критическую секцию (x+=l) одновременно. Оба потока запрашивают текущее значение х. Предположим, оно равно 100. Потоки одновременно считают значение как 100 и увеличат его до 101.В память будет записано новое значение 101. Происходит однократное увели­ чение, хотя оба потока должны увеличивать значение переменной независимо. Ко­ нечным значением х должно быть 102. Достичь этого можно с помощью синхронизации потоков, которая использует класс Lock из модуля threading. Замок (Lock) для блокировки потоков реализуется с помощью объекта семафора, предоставленного операционной системой. Семафор (Semaphore) -это объект синхронизации на уровне ОС для управления доступом к ресурсам и данным для нескольких процессоров и потоков. Класс Lock предостав­ ляет два метода, acquire и release: 1.Метод acquire используется для получения замка, который имеет два состояния - заблокирован (по умолчанию) или разблокирован. Когда замок заблокирован, выполнение запрашивающего потока приостанавливается, пока замок не будет освобожден текущим потоком. Как только замок освобождается, он передается запрашивающему потоку. При запросе незаблокированного замка выполнение потока не блокируется. Если замок доступен (имеет состояние unlocked), запра­ шивающий поток получает его ( состояние становится locked). В противном слу­ чае запрашивающий поток получает False в качестве ответа на запрос замка. 2.Метод release используется для освобождения замка, то есть состояние прину­ дительно переходит в unlocked. Если какой-то пЬток заблокирован и ждет полу­ чения замка, данный метод позволит начать выполнение. Изменим пример threadЗa. ру, используя замок для оператора инкремента общей пе­ ременной х. Создадим замок на уровне главного потока, а затем передадим его функции inc для реализации блокировки вокруг общей переменно й: #threadЗb.py с использованием синхронизации потоков frorn threading irnport Lock, Thread as Thread def inc with lock (lock): global х\n--- Страница 212 ---\n216 Раздел 3. Масштабирование за пределы одного потока for in range(lOOOOOO): lock. acquire () x+=l lock. release () х = о my lock = Lock () #создаем потоки tl = Thread(target= inc_with_lock, args=(mylock,), name=\"Th 1\") t2 = Thread(target= inc_with_lock, args=(mylock,), name=\"Th 2\") #запускаем потоки tl. start () t2. start () #ожидаем завершения потоков tl. join () t2. join () print(\"final value of х :\", х) При использовании объекта Lock значение х всегда будет равно 2000000, поскольку теперь только один поток за раз может увеличивать значение общей переменной. Преимущество синхронизации потоков заключается в более производительном ис­ пользовании системных ресурсов и предсказуемых результатах. Однако, использовать замки следует с осторожностью, поскольку неправильная их реализация ведет к ситуации взаимной блокировки. Предположим, поток получает замок для ресурса А и ждет получения замка для ресурса В. Но другой поток уже имеет замок для ресурса В и ждет замок для ресурса А. Оба потока будут ждать освобождения замков, но этого никогда не произойдет. Во избежание взаимобло­ кировок библиотеки многопоточной и многопроцессорной обработки поставляются с такими механизмами, как времени ожидания, в течение которого ресурс может удерживать блокировку, или использовать менеджер контекста для получения замков. Использование синхронизированной очереди Модуль Queue в Python реализует очереди с множественными производителями и множественными потребителями. Очереди очень полезны в многопоточных при­ ложениях, когда необходимо безопасно обмениваться информацией между потока­ ми. Прелесть синхронизированной очереди в том, что она поставляется со всеми нужными механизмами блокировки, и нет необходимости использовать дополни­ тельную семантику. Модуль Queue имеет три типа очередей: ♦FIFO (First in Fir,st out): задача, добавленная первой, извлекается первой; ♦LIFO (Last in First out): задача, добавленная последней, извлекается первой;\n--- Страница 213 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное 217 ♦Приоритетная очередь (Priority queue): записи сортируются, и первой извле- кается запись с наименьшим приоритетом. Эти очереди используют замки для защиты доступа к записям от конкурирующих потоков. Рассмотрим на примере, в котором создадим очередь FIFO с фиктивными задачами. Для обработки задач из очереди реализуем пользовательский потоковый класс, который наследуется от класса Thread. Это еще один способ реализации по­ тока. Для создания пользовательского класса нужно переопределить методы ini t и run. В методе init требуется вызвать метод init суперкласса (класса Thread). Метод run является исполнительной частью потокового класса. Результат получится следующим: #threadS.py с очередью и пользовательским классом Thread from queue import Queue from threading import Thread as Thread from time import sleep class МyWorker (Тhread): def init (self, name, q): threading.Тhread._init_(self) self.name = name self.queue = q def run (self) : while True: item = self.queue.get() sleep (1) try: print (\" {}: {}\". format (self. name, item)) finally: self.queue.task_done() #заполняем очередь myqueue = Queue () for i in range (10): myqueue.put(\"Task {}\". format (i +1)) #создаем потоки for i in range (5): worker = MyWorker(\"Th {}\".format(i+l), myqueue) worker.daemon = True worker. start () myqueue. j oin () В этом примере мы создали пять рабочих потоков с помощью пользовательского класса MyThread. Потоки обращаются к очереди для получения элемента задачи. По-\n--- Страница 214 ---\n218 Раздел 3. Масштабирование за пределы одного потока еле этого ждут 1 секунду и выводят имя потока и имя задачи. При каждом вызове get для элемента очереди последующий вызов task_done () указывает на завершение обработки задачи. Важно отметить, что мы применили метод join к объекту myqueue, а не к потокам. Таким образом, он блокирует главный поток, пока все элементы в очереди не будут обработаны и завершены (для них вызывается task_done ). Это рекомендуемый спо­ соб блокировать главный поток, когда объект очереди используется при хранении задач для потоков. Далее реализуем приложение для загрузки файлов с Google Диска, используя класс Thread, класс Queue и пару сторонних библиотек. Практический пример: многопоточное приложение для загрузки файлов с Google Диска Ранее обсуждалось, что многопоточные приложения Python отлично подходят для задач ввода-вывода. Поэтому для реализации мы выбрали именно такое приложе­ ние, которое загружает файлы из общего каталога на Google Диске. Нам понадо­ бится следующее: ♦Google Диск: аккаунт на Google Диске (подойдет базовый бесплатный) с одним общим каталогом. ♦Ключ API: необходим для доступа к Google API; ключ должен быть включен, тогда Google API можно будет использовать для Google Диска; включить его можно, следуя инструкциям на сайте для разработчиков Google (https://developers.google.com/drivelapilvЗ/enaЬ/e-drive-api). ♦getfilelistpy: сторонняя библиотека, которая получает список файлов из общего каталога Google Диска; для установки библиотеки можно использовать инстру­ мент pip. ♦gdown: сторонняя библиотека, которая загружает файлы с Google Диска; для установки тоже можно использовать pip; существуют и другие библиотеки с тем же функционалом, мы выбрали gdown из-за ее простоты. Для работы модуля getfilelistpy необходимо создать структуру данных ресурса. Она будет включать в себя идентификатор папки id (в нашем случае это идентифи­ катор папки на Google Диске), ключ безопасности API (api_key) для доступа к ката­ логу Google Диска и список атрибутов файла (fields), которые будут извлечены при получении списка файлов. Создадим структуру данных ресурса следующим образом: resource = { \"api_key\": \"AizaSyDYКmm85kebxddКrGns4z0\", \"id\": \"0B8TxHW2CiбdЬckVwTRtT13RUU\", \"fields\": \"files(name, id, webContentLink)\",\n--- Страница 215 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное 219 '''Ключ и идентификатор API, используемые в примерах, не являются оригинальными, поэтому их следует заменить в соответствии с вашей учетной записью и идентификатором общего каталога''' Атрибуты файлов представлены только идентификатором, именем и веб-ссылкой (file id, name и web link, соответственно). Далее необходимо добавить каждый файл в очередь как задачу для потоков. Очередь будет использоваться несколькими ра­ бочими потоками для параллельной загрузки файлов. Для гибкости приложения с точки зрения количества рабочих потоков, которые можно использовать, создадим пул рабочих потоков. Размер пула контролируется глобальной переменной, заданной в начале программы. Мы создаем рабочие пото­ ки в соответствии с размером пула. Каждый поток в пуле имеет доступ к очереди со списком файлов. Как и в предыдущем примере, каждый рабочий поток берет один элемент файла из очереди за раз, загружает его и помечает задачу как выпол­ ненную с помощью метода task _ done. Пример кода для определения структуры дан­ ных ресурса и определения класса для рабочего потока будет следующим: #threads_casestudy.py from queue import Queue from threading import Thread import time from getfilelistpy import getfilelist import gdown THREAD POOL SIZE = 1 resource = { \"api_key\": \"AizaSyDYКmm85kea2bxddКrGns4z0\", \"id\": \"0B8TxHW2CiбdЬckVweTRtTl3RUU \", \"fields\": \"files(name,id,webContentLink)\", class DownlaodWorker(Thread): def init (self, name, queue): Thread. init (self) self.name = name self.queue = queue def run (self) : while True: #получаем id и имя файла из очереди iteml = self.queue.get() try: qdown.download( iteml['webContentLink'], './files/{}' .format(iteml['name']), quiet=False) finally: self.queue.task_done()\n--- Страница 216 ---\n220 Раздел 3. Масштабирование за пределы одного потока Мы получаем метаданные файлов из каталога Google Диска, используя структуру данных ресурса, следующим образом: def get_files(resource): #глобальный список файлов files_list res = getfilelist.GetFileList(resource) files_list = res['fileList'] [О] return files list В функции main мы создаем объект Queue для вставки метаданных файлов в очередь. Объект Queue передается пулу рабочих потоков. Они, в свою очередь, загружают файлы, как уже обсуждалось ранее. Мы используем класс time для измерения вре­ мени, необходимого для загрузки всех файлов из каталога. Код функции main сле­ дующий: def main(): start_time = time.monotonic() files = qet_files(resource) #добавляем инфрмацию о файлах в очередь queue = Queue ( ) for item in files['files']: queue.put (item) for i in range (TНREAD_POOL_SIZE): worker = DownlaodWorker(\"Thread {}\".format(i+l), queue) worker.daemon = True worker. start () queue. j oin () end_time = time.monotonic() print('Time taken to download: {} seconds'.format( end time -start_time)) main () В каталоге Google Диска имеется 1 О файлов, размером от 500 КБ до 3 МБ. Мы за­ пускали приложение с 1, 5 и 1 О рабочими потоками. Общее время загрузки 1 О фай­ лов с 1 потоком составило примерно 20 секунд. Это эквивалентно коду без исполь­ зования потоков (он доступен в репозитории для этой книги в качестве примера). Время работы кода без потоков для загрузки 1 О файлов составило 19 секунд. При загрузке файлов с 5 потоками на MacBook (Intel Core i5 с 16 ГБ ОЗУ) время значительно сократилось (до 6 секунд). На других машинах время может отличать­ ся, но все равно оно будет уменьшаться при увеличении количества рабочих пото­ ков. При использовании 1 О потоков время сократилось до 4 секунд. Это показыва­ ет, что время выполнения задач ввода-вывода улучшается за счет многопоточности, независимо от ограничений глобальной блокировки интерпретатора.\n--- Страница 217 ---\nГnа.ва 7. Многопроцессорная обработка, многопоточность и асинхронное 221 Мы рассмотрели реализацию потоков в Python, а также использование механизмов блокировки с помощью классов Lock и Queue. Далее поговорим о многопроцессор­ ном программировании на Python. Многопроцессорная обработка Мы увидели все сложности и ограничения многопоточного программирования, а также оценили, стоит ли его использование затраченных усилий. Для задач ввода­ вывода оно может быть полезно, но в других случаях лучше выбрать альтернатив­ ный подход -многопроцессорную обработку, поскольку глобальная блокировка интерпретатора не ограничивает отдельные процессы и выполнение может проис­ ходить параллельно. Это особенно эффективно, когда приложения выполняются на многоядерных процессорах и интенсивно их используют. Во встроенных библиоте­ ках Python многопроцессорная обработка -единственный вариант использования многоядерных процессоров. Графические процессоры (Graphics Processing Units, GPU) имеют большее коли­ чество ядер, чем обычные процессоры, и лучше подходят для задач параллельной обработки. Единственная сложность заключается в необходимости переноса дан­ ных из основной памяти в память графического процессора. Эти дополнительные действия окупятся при обработке большого объема данных, но если объем неболь­ шой, преимуществ будет мало или не будет совсем. Сегодня использование GPU для обработки больших данных, особенно для моделей машинного обучения, ста­ новится популярным. NVIDIA представила графический процессор CUDA для па­ раллельной обработки, который хорошо поддерживается сторонними библиотека­ ми Python. На уровне ОС каждый процесс имеет структуру данных, которая называется блок управления процессом (Process Control Block, РСВ). Как и блок управления задачей (Task Control Block, ТСВ), РСВ использует идентификатор процесса (Process ID, PID), счетчик команд, регистры ЦП, информацию о планировании ресурсов ЦП и многие другие атрибуты, а также хранит состояние процесса (выполняется или ожидает). Когда задействуются несколько процессов ЦП, изначально совместного использо­ вания памяти нет, а, значит, шанс повреждения данных ниже. Если двум процессам нужен совместным доступ к данным,.им необходим какой-либо механизм взаимо­ действия между собой. Python поддерживает такое взаимодействие через примити­ вы (Primitive). Далее рассмотрим основы создания процессов в Python, а также об­ судим, как достичь взаимодействия между ними. Создание нескольких процессов Для многопроцессорного программирования Python предоставляет пакет rnultiprocessing (аналогичный пакету rnultithreading). Он включает два подхода к\n--- Страница 218 ---\n222 Раздел 3. Масштабирование за пределы одного потока реализации многопроцессорной обработки, которые используют объект Process и объект Pool. Рассмотрим каждый из них. Использование объекта Process Процессы могут быть реализованы с помощью создания объекта Process, а затем использования его метода start, аналогичного методу start для запуска объекта Thread. Фактически, объект Process предлагает тот же API, что и объект Thread. Про­ стой пример создания нескольких дочерних процессов показан ниже: #processl.py создание простых процессов с помощью функции import os from multiprocessing import Process, current_process as ер from time import sleep def print_hello(): sleep (2) print (\" { }-{}: Hello\". format (os. getpid (), ер() . name)) def print_messaqe(msg): sleep (1) print (\" { }-{}: {}\". format (os. getpid (), ер() . name, msg)) def main(): processes = [] #создание процессов processes.append(Process(target=print_hello, name=\"Process 1\")) processes.append(Process(target=print_hello, name=\"Process 2\")) processes.append(Process(target=print_message, args=[\"Good morning\"], name=\"Process 3\")) #запуск процессов for р in processes: p.start() #ждем завершения всех процессов for р in processes: p.join() print(\"Exiting the main process\") if name main() main '·\n--- Страница 219 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное 223 Как уже упоминалось, методы объекта Process очень похожи на аналогичные мето­ ды объекта Thread. Поэтому объяснение данного кода аналогично объяснениям кода в примерах многопоточности. Использование объекта Pool Объект Pool предлагает удобный способ (с помощью метода map) создания процес­ сов, назначения им функций и распределения входных параметров по процессам. Для примера мы взяли размер пула, равный 3, но предоставили входные параметры для 5 процессов. Это нужно с целью убедиться, что одновременно активны не бо­ лее трех дочерних процессов, независимо от количества параметров, переданных методом map нашего объекта Pool. Дополнительные параметры будут переданы тем же дочерним процессам, когда они закончат свое текущее выполнение. Пример ко­ да представлен ниже: #process2.py создание процессов с помощью pool import os from multiprocessing import Process, Pool, current_process as ер from time import sleep def print_ messaqe (msg) : sleep (1) print (\" { }-{}: {}\". format (os. getpid (), ер() . name, msg)) def main (): #создаем процесс из пула with Рооl(З) as proc: proc.map(print_message, [\"Orange\", \"Apple\", \"Banana\", \"Grapes\", \"Pears\"]) print(\"Exiting the main process\") if name main main () Распределение входных параметров функции, которая привязана к набору процес­ сов пула, осуществляется методом map. Он ожидает завершения выполнения всех функций, поэтому нет необходимости использовать join, если процессы создаются с помощью объекта Pool. Некоторые различия между объектами Process и Pool представлены в таблице ниже: Таблица 7.1. Сравнение объектов Pool и Process Использование объекта Pool Использование объекта Process В памяти остаются только активные процессы В памяти остаются все созданные процессы\n--- Страница 220 ---\n224 Раздел 3. Масштабирование за пределы одного потока Таблица 7.1 (окончание) Использование объекта Pool Использование объекта Process Лучше работает с большими наборами данных Лучше работает с малыми наборами и повторяющимися задачами данных Процессы при вводе-выводе блокируются, пока Процессы при вводе-выводе не будет предоставлен ресурс ввода-вывода не блокируются Далее обсудим, как обмениваться данными между процессами. Обмен данными между процессами Пакет multiprocessing предлагает два способа обмена данными между процессами: общая память и серверный процесс. Рассмотрим каждый из них. Использование общих объектов ctype {общая память) В этом случае создается блок общей памяти, и процессы имеют к нему доступ. Блок создается при инициализации одного из типов данных ctype, доступных в па­ кете multiprocessi ng. Ими являются Array (массив ctype) и Value (универсальный объ­ ект ctype). Оба типа выделяются из общей памяти. Создать массив ctype можно сле­ дующим оператором: mylist = multiprocessing.Array('i', 5) Он создаст массив с типом данных integer и размером 5. Литерал i -это код типа, в данном случае обозначает integer (целое число). Для типа данных float использу­ ется код d. Также можно инициализировать массив, передав последовательность в качестве второго аргумента вместо размера: mylist = multiprocessing.Array('i', [l,2,3,4,5)) Создать объект Value можно следующим способом: obj = multiprocessing. Value ( 'i') Будет создан объект integer, поскольку указан код i. Значение объекта можно за­ дать с помощью атрибута value. Оба объекта ctype имеют опциональный аргумент Lock со значением тrue по умол­ чанию. При значении тrue этот аргумент используется для создания нового рекур­ сивного объекта блокировки, который предоставляет синхронизированный доступ к значениям объектов. При значении False защита отключена, и процесс будет не­ безопасным. Если процесс обращается к общей памяти только для чтения, парамет­ ру Lock можно установить False. В дальнейших примерах кода мы оставим для ар­ гумента Lock значение по умолчанию True. Для демонстрации использования объектов ctype из общей памяти создадим список с 3 числовыми значениями, массив ctype размера з для хранения увеличенных зна­ чений исходного массива и объект ctype для хранения суммы увеличенного масси-\n--- Страница 221 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное 225 ва. Эти объекты будут созданы родительским процессом в общей памяти, а затем обновлены дочерним процессом. Взаимодействие родительского и дочернего про­ цессов с общей памятью показано на схеме (рис. 7.3): Родительскии процесс ------------ Доцерн�•и процесс ------------ Рис. 7.3. Использование общей памяти родительским и дочерним процессами Далее показан полный пример кода использования общей памяти: #processЗ.py использование общей памяти объектами ctype import multiprocessing from multiprocessing import Process, Pool, current_process as ер def inc_sum_list(list, inc_list, sum): sum.value = О for index, num in enumerate(list}: inc_list[index] = num + 1 sum.value = sum.value + inc_list[index] def main(}: mylist = [2, 5, 7] inc list = multiprocessinq.Array('i', 3) sum = mul tiprocessinq. Value ( ' i ' ) р = Process(target=inc_sum_list, args=(mylist, inc_list, sum)) p.start() р. join () print(\"incremented list: \", list(inc list}) print(\"sum of inc list: \", sum.value) print(\"Exiting the main process\") if name main(} main '·\n--- Страница 222 ---\n226 Раздел 3. Масштабирование за пределы одного потока К общим типам данных (в нашем случае inc_list и sum) обращаются как родитель­ ский, так и дочерний процессы. Важно отметить, что использовать общую память не рекомендуется, поскольку для этого требуются механизмы синхронизации и блокировки (аналогично механизмам в случае многопоточности ), когда к одним и тем же объектам общей памяти обращаются несколько процессов, а аргумент Lock имеет значение False. Далее рассмотрим использование серверного процесса. Использование серверного процесса В этом случае серверный процесс запускается сразу после старта программь1 Python. Он используется для создания и управления новыми дочерними процесса­ ми, запрошенными родительским процессом. Серверный процесс может содержать объекты Python, к которым другие процессы могут обращаться через прокси. Для реализации серверного процесса и совместного использования объектов пакет multiprocessing предоставляет объект Manager. Он поддерживает разные типы дан- ных, включая: ♦списки; ♦словари; ♦Lock; ♦Rlock;♦очереди; ♦Value; ♦массивы. Рис. 7.4. Использование серверного процесса для обмена данными между процессами Пример кода, который мы выбрали для демонстрации серверного процесса, создает объект dictionary с помощью объекта мanager, а затем передает словарь дочерним процессам для вставки дополнительных данных и вывода содержимого. Для наше­ го примера мы создадим три дочерних процесса: два для вставки данных в объект словаря и один для получения содержимого словаря для вывода на консоль. На рис. 7.4\n--- Страница 223 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное 227 приведена схема взаимодействия между родительским, серверным и тремя дочер­ ними процессами. Родительский создает серверный, как только выполняется запрос нового процесса с помощью контекста мanager. Дочерние процессы создаются и управляются серверным процессом. Общие данные доступны в серверном процес­ се, а также во всех остальных, включая родительский. Полный пример кода показан ниже: #process4.py использование общей памяти с помощью серверного процесса import multiprocessing from multiprocessing import Process, Manager def insert_data (dictl, code, suЬject): dictl[code] = subject def output(dictl): print(\"Dictionary data: \", dictl) def main(): with multiprocessing.Мanaqer() as mgr: #создаем словарь в серверном процессе mydict = mgr.dict({l00: \"Maths\", 200: \"Science\"}) pl Process(target=insert_data, args=(mydict, 300, \"English\")) р2 Process(target=insert_data, args=(mydict, 400, \"French\")) рЗ Process(target=output, args=(mydict,)) pl. start () р2. start () pl. join () р2. join () рЗ. start () рЗ. join () print (\"Exiting the main process\") if name main(} main Серверный процесс обеспечивает больше гибкости, чем использование общей па­ мяти, поскольку он поддерживает огромное разнообразие типов объектов. Однако этот способ имеет низкую производительность по сравнению с общей памятью.\n--- Страница 224 ---\n228 Раздел 3. Масwтабиро1;Jание за пределы одного потока Об�ен объектами между процессами В предыдущем подразделе мы узнали, как процессы могут обмениваться данными через внешний блок памяти или новый процесс. Здесь мы исследуем обмен данны­ ми между процессами с использованием объектов Python. Модуль mul tiprocessing предоставляет для этого два способа: объект Queue и объект Pipe. Использование объекта Queue Объект Queue доступен в пакете multiprocessing и аналогичен объекту синхронизи­ рованной очереди (queue.Queue), который мы использовали для многопоточности. Объект Queue безопасен для процессов и не требует дополнительной защиты. Ниже показан пример работы данного объекта: lproceaaS.py использование очереди для обмена данными import multiprocessing from multiprocessing import Process, Queue def copy_data (list, myqueue) : for num in list: myqueue.put(num) def output(myqueue): while not myqueue.empty(): print(myqueue.get()) def main (): mylist = [2, 5, 7] myqueue = Queue() pl = Process(target=copy_data, args=(mylist, myqueue)) р2 = Process(target=output, args=(myqueue,)) pl. start () pl. join () р2. start () р2. join () print ( \"Queue is empty: \", myqueue. empty () ) print(\"Exiting the main process\") if name main() ' main '· Мы создали стандартный объект list и многопроцессорный объект Queue. Она оба передаются новому процессу, который связан с функцией copy_data. Она копирует\n--- Страница 225 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное 229 данные из list в Queue. Инициируется новый процесс для вывода содержимого объ­ екта Queue. Обратите внимание, данные в Queue задаются предыдущим процессом и доступны новому процессу. Это удобный способ обмена данными без лишней сложности общей памяти или серверного процесса. Использование объекта Pipe Объект Pipe можно сравнить с каналом между двумя процессами для обмена дан­ ными. Он особенно удобен, если нужна двусторонняя связь. Когда мы создаем Pipe, он предоставляет два объекта соединения, находящихся по обе стороны канала. Каждый из них предоставляет методы send и recv для отправки и получения данных соответственно. Для демонстрации создадим две функции, которые будут прикреплены к двум от­ дельным процессам: ♦Первая будет отправлять сообщение через соединение объекта Pipe; мы отпра­ вим несколько сообщений и завершим взаимодействие сообщением БУЕ. ♦Вторая функция будет получать сообщение через другое соединение объекта Pipe; она будет выполняться бесконечным циклом, пока не получит сообщение БУЕ. Обе функции (или процесса) обеспечиваются двумя объектами соединения. Пол­ ный код выглядит следующим образом: #processб.py обмен данными через Pipe from multiprocessing import Process, Pipe def mysender (s_conn): s_conn.send( { 100, \"Мaths\"} ) s_conn. send( {200, \"Science\"}) s _conn. send (\"БУЕ\") def myreceiver(r_conn): while True: msg = r _ conn. recv () if msg == \"БУЕ\": break print(\"Received message \", msg) def main(): sender_conn, receiver_conn = Pipe() pl = Process(target=mysender, args=(sender_conn, )) р2 = Process(target=myreceiver, args=(receiver_conn,)) pl. start () р2. start ()\n--- Страница 226 ---\n230 pl. join () р2. join () Раздел 3. Масштабирование за пределы одного потока print(\"Exiting the main process\") if name main() ' main '· Стоит отметить, что данные в объекте Pipe легко могут повредиться, когда два про­ цесса попытаются одновременно выполнить чтение или запись, используя один объект соединения. Поэтому многопроцессорные очереди являются лучшим вари­ антом, они обеспечивают надлежащую синхронизацию между процессами. Синхронизация процессов Синхронизация процессов гарантирует, что два или более процесса не будут обра­ щаться к одному ресурсу или программному коду (критической секции) одновре­ менно. Эта ситуация может привести к состоянию гонки и повреждению данных. Вероятность возникновения этого не очень высока, но все же возможна. Этих си­ туаций можно избежать, используя либо подходящие объекты со встроенной син­ хронизацией, либо объект Lock, как в случае многопоточности. Мы рассмотрели пример использования типов данных queues и ctype с включенной блокировкой (Lock = тrue), что делает процесс безопасным. В следующем примере мы продемонстрируем использование Lock с целью убедиться, что один процесс получает доступ к консольному выводу за раз. Для этого мы создали процессы, ис­ пользуя объект Pool. Для передачи одного и того же объекта Lock всем процессам мы использовали его из объекта мanager, а не из пакета multiprocessing. Мы также использовали функцию partial для привязки Lock к каждому процессу вместе со списком, который передан каждой функции процесса. Полный пример кода пред-· ставлен ниже: llprocess7.py демонстрация синхронизации и блоокировки from functools import partial from multiprocessing import Pool, мanager def printme (lock, msg): lock. acquire () try: print(msg) finally: lock. release () def main (): with Рооl(З) as proc:\n--- Страница 227 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное lock = Manager() .Lock() func = partial(printme,lock) proc.map (func, [\"Orange\", \"Apple\", \"Banana\", \"Grapes\", \"Pears\"]) print(\"Exiting the main process\") if name main() main '· 231 Без использования объекта Lock выходные данные от разных процессов могут быть перепутаны. Практический пример: многопроцессорное приложение для загрузки файлов с Google Диска Здесь мы реализуем пример, аналогичный предыдущему, но вместо многопоточно­ сти используем многопроцессорность. Требования и задачи будут такие же, как для многопоточного приложения. Мы будем использовать тот же код, что и в случае многопоточности, за исключе­ нием использования множества процессов вместо потоков. Еще одно отличие - задействование объекта JoinaЫeQueue из модуля multiprocessing, который обеспечи­ вает ту же функциональность, что и объект Queue. Код для определения структуры данных ресурса и для функции загрузки файлов с Google Диска показан далее: #processes_casestudy.py import time from multiprocessing import Process, JoinaЬleQueue from getfilelistpy import getfilelist import gdown PROCESSES POOL SIZE = 5 resource = { \"api_key\": \"AizaSyDYКmm85keqnk4bF1Da2bxdc!КrGns4z0\", \"id\": \"0B8TxHW2CiбdЬckVwetTlVЗRUU\", \"fields\": \"files(name,id,webContentLink)\", def mydownloader ( queue) : while True: fполучаем id и имя файла из очереди iteml = queue.get() try: qdown.download(iteml['webContentLink'], './files/{} ' .format(iteml['name']) , quiet=False) finally: queue.task_done()\n--- Страница 228 ---\n232 Раздел 3. Масштабирование за пределы одного потока Мы получаем метаданные файлов, такие, как имя, и НТТР-ссылку из каталога, ис­ пользуя структуру данных ресурса, как показано ниже: def get_files(resource): res = getfilelist.GetFileList(resource) files_list = res['fileList'] [О] return files list В функции main мы создаем объект JoinaЫeQueue и вставляем метаданные файлов в очередь. Очередь передается пулу процессов, которые загрузят файлы. Мы задей­ ствуем класс time для измерения времени, необходимого для загрузки всех файлов из Google Диска. Код функции main выглядит так: def main (): files = get_files(resource) Jдобавляем информацию о файлах в очередь myqueue = JoinaЬleQueue () for item in files['files']: myqueue.put(item) processes = [] for id in range(PROCESSES_POOL_SIZE): р = Process(target=mydownloader, args=(myqueue,)) p.daemon = True р. start () start_time = time.monotonic() myqueue. join () total_exec_time = time.monotonic() -start time print(f'Time taken to download: {total_exe c_time:.2f} seconds') if name main main () Мы запустили приложение с разным числом процессов: 3, 5, 7 и 10. Время загрузки одних и тех же файлов немного быстрее, чем в многопоточном приложении. Время выполнения зависит от компьютера. В нашем случае (MacBook Pro: Intel Core i5 с 16 ГБ ОЗУ) оно заняло около 5 секунд при 5 процессах и 3 секунды при 1 О процес­ сах, работающих параллельно. Это на 1 секунду быстрее, чем в многопоточном приложении, как и ожидалось, поскольку многопроцессорная обработка обеспечи­ вает настоящий параллелизм.\n--- Страница 229 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное Асинхронное программирование для адаптивных систем 233 Многопроцессорность и многопоточность относятся к синхронному программиро­ ванию, где отправляется запрос и ожидается ответ, прежде чем будет выполнен следующий блок кода. И если какое-либо переключение контекста и применяется, то это обеспечивается операционной системой. Асинхронное программирование в Python отличается двумя аспектами: ♦Задачи должны быть созданы для асинхронного выполнения; это означает, что родительскому вызывающему объекту не нужно ждать ответа от другого про­ цесса; тот ответит, как только закончит выполнение. ♦Операционная система больше не управляет переключением контекста между процессами и потоками; асинхронная программа получит только один поток в процессе, но его возможности будут разнообразны; при таком стиле выполнения каждый процесс или задача добровольно передают контроль в случае простоя или ожидания другого ресурса; таким образом, другие задачи тоже могут вы­ полняться; эта концепция называется кооперативной многозадачностью. Кооперативная многозадачность (Cooperative Multitasking) -это эффективный инструмент для достижения параллелизма на уровне приложения. При таком под­ ходе создаются не процессы или потоки, а задачи, которые включают тасклеты (Tasklet), корутины (Coroutine) и зеленые потоки (Green Thread). Они координи­ руются одной функцией, которая называется циклом событий (Event Loop ). Он регистрирует задачи и обрабатывает поток управления между ними. Прелесть заключается в том, что цикл событий в Python реализуется с помощью генераторов. А, значит, они могут выполнять функцию и приостанавливать ее в определенной точке ( с помощью yield), сохраняя при этом контроль над стеком объектов до во­ зобновления работы. Для систем, основанных на кооперативной многозадачности, всегда стоит BOr;Ipoc, когда возвращать управление планировщику или циклу событий. Наиболее попу­ лярная логика заключается в использовании операций ввода-вывода в качестве со­ бытий для освобождения управления, поскольку для этих операций всегда требует­ ся время ожидания. Возникает вопрос, разве не эту же логику мы использовали для многопоточности? Она безусловно повышает производительность приложения при работе с опера­ циями ввода-вывода, но есть разница. В случае многопоточности ОС управляет пе­ реключением контекста между потоками и может вытеснить любой запущенный поток по любой причине и передать управление другому. Но при асинхронном про­ граммировании или кооперативной многозадачности операционной системе не видны задачи и корутины. Фактически они не могут быть вытеснены главным цик­ лом событий. Но это не означает, что ОС не может вытеснить весь процесс Python. Он по-прежнему конкурирует за ресурсы с другими приложениями и процессами на уровне ОС.\n--- Страница 230 ---\n2._34 Раздел 3. Масштабирование за пределы одного потока Далее изучим некоторые понятия асинхронного программирования, которое пре­ доставляется модулем asyncio, а затем завершим подробным анализом конкретного примера. Модуль asyncio Модуль asyncio доступен, начиная с Python 3.5 и более поздних версиях. Он предна­ значен для написания конкурентных программ с помощью синтаксиса async/await . Но для сложных приложений рекомендуется использовать Python, начиная с вер­ сии 3.7. Библиотека богата множеством возможностей, поддерживает создание и выполнение корутинов, выполнение сетевых операций ввода-вывода, распределе­ ние задач по очередям и синхронизацию параллельного кода. Для начала рассмотрим, как писать и выполнять корутины и задачи. Корутины и задачи Корутины -это функции, которые должны выполняться асинхронно. Простой пример вывода строки на консоль с помощью них выглядит так: #asynciol.py создание простого корутина import asyncio import time async def say(delay, msg): await asyncio.sleep(delay) print(msg) print(\"Started at \", time.strftime(\"%X\")) asyncio.run(say(l,\"Good\")) asyncio.run(say(2, \"Morning\")) print(\"Stopped at \", time.strftime(\"%X\")) Ключевые моменты кода: ♦Корутин принимает аргументы delay и msg; delay позволяет добавить задержку перед отправкой строки msg на консоль. ♦Здесь используется функция asyncio. sleep вместо традиционной time. sleep, по­ скольку последняя не возвращает контроль циклу событий; вот почему важно использовать совместную функцию asyncio. sleep. ♦Корутин выполняется дважды с двумя разными значениями аргумента delay с помощью метода run, который не будет выполнять корутины совместно.\n--- Страница 231 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное 235 Консольный вывод показывает, что корутины выполняются один за другим, по­ скольку общая задержка составляет 3 секунды: Started at 15:59:55 Good Мominq Stopped at 15:59:58 Параллельное выполнение корутинов можно запустить функцией create_task из мо­ дуля asyncio. Она создает задачу, с помощью которой можно планировать совмест­ ное выполнение корутинов. В следующем примере изменим asynciol .py и обернем корутин в задачу, используя функцию create_task. А если точнее, создадим две задачи, которые служат обертка­ ми для say. Дождемся выполнения обеих задач с помощью await: #asyncio2.py создание и параллельное выполнение корутинов import asyncio import time async def say(delay, msg): await asyncio.sleep(delay ) print (msg) async def main (): taskl = asyncio.create_task( say(l, 'Good')) task2 = asyncio.create_task( say(l, 'Morning')) print(\"Started at \", time.strftime(\"%X\")) await taskl await task2 print(\"Stopped at time. strftime (\"%Х\")) asyncio.run(main()) Консольный вывод будет следующим: Started at 16:04:40 Good Мominq Stopped at 16:04:41 Вывод показывает, что обе задачи выполнились за 1 секунду. Это доказывает, что они выполнялись параллельно.\n--- Страница 232 ---\n236 Раздел 3. Масштабирование за пределы одного потока Объекты, ожидающие результатов Объект является ожидающим (awaitaЬle), если можно применить к нему оператор await. Множество функций и модулей внутри asyncio предназначено для работы с ожидающими результатов объектами. Но большинство объектов Python и сторон­ них библиотек не создано для асинхронного программирования. При разработке таких приложений важно выбирать совместимые библиотеки, которые могут пре­ доставить ожидающие результатов объекты. Такие объекты делятся на три типа: корутины, задачи и фьючеры (Future). С кору­ тинами и задачами мы уже знакомы. Фьючер -это низкоуровневый объект, анало­ гичный механизму колбэк (Callback), который используется для обработки резуль­ тата, поступающего от async/await. Объекты Future обычно не предоставляются для программирования на уровне пользователей. Одновременное выполнение задач Если нужно запустить несколько задач параллельно, можно использовать awai t, как . в предыдущем примере. Но есть способ лучше -функция gather. Она запустит ожидающие объекты в указанной последовательности. Если какой-то из них явля­ ется корутином, он будет запланирован как задача. Использование функции gather мы увидим в следующем подразделе. Распределение задач с помощью очередей Объект Queue в пакете asyncio похож на модуль Queue, но не является потокобезопас­ ным. Модуль asyncio предоставляет различные реализации очередей, например FIFO, LIFO и приоритетные очереди. Очереди из модуля asyncio могут быть ис­ пользованы для распределения рабочих нагрузок по задачам. Для демонстрации напишем небольшую программу, которая будет имитировать выполнение реальной функции, засыпая на случайный промежуток времени. Неоп­ ределенное время ожидания рассчитывается для 1 О выполнений и добавляется к объекту Queue основным процессом в качестве рабочих элементов. Объект Queue пе­ редается в пул из трех задач. Каждая из них выполняет назначенный корутин, кото­ рый потребляет время выполнения в соответствии с доступной ему записью в оче­ реди. Полный пример кода: laayncioЗ.py распределение рабочих нагрузок через очередь import asyncio import random import time async def executer (name, queue) : while True:\n--- Страница 233 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное exec_time = await queue.get() await asyncio.sleep(exec_time) queue.task_done() iprint(f'{name} has taken {exec_tim e:.2f} seconds') async def main (): myqueue = aayncio. Queue () calc exuection time = О -- for _ in range(lO): sleep_for = random.uniform(0 .4, 0.8) calc_exuection_time += sleep_for myqueue.put_nowait(sleep_for) tasks = [] for id in range(З): task = aayncio.create_taak(executer(f'Task-{id+l} ', myqueue)) tasks.append(task) start_time = time.monotonic() await myqueue.join() total_exec_time = time.monotonic() -start time for task in tasks: task.cancel () await asyncio.gather(*tasks, return_exceptions=True) print(f\"Calculated execution time {calc_exuection_time:0.2f}\") print(f\"Actual execution time {total_exec_time:0.2f}\") asyncio.run(main()) 237 Мы использовали функцию put _ no _ wait объекта Que�e, поскольку она является не­ блокирующей операцией. Консольный вывод будет следующим: C&lculated eucution tillle 5.58 Actual execution tillle 2.05 Вывод указывает, что задачи выполняются параллельно. Это в 3 раза быстрее, чем при последовательном выполнении. Пока что мы рассмотрели базовые концепции пакета asyncio. Прежде чем завер­ шить тему, вернемся к уже знакомому примеру из подраздела про многопоточность и реализуем его с помощью asyncio.\n--- Страница 234 ---\n238 Раздел 3. Масштабирование за пределы одного потока Практический пример: асинхронное приложение для загрузки файлов с Google Диска Мы реализуем уже знакомое приложение, но на этот раз будем использовать мо­ дуль asyncio с функциями async, await и async queue. Требования остаются такие же, за исключением использования библиотек aiohttp и aiofiles вместо gdown. Причина заключается в том, что gdown не создана как асинхронный модуль и не подходит для такого программирования. Это важно учитывать при выборе библиотек для работы с асинхронными приложениями. Для этого приложения мы создали корутин mydownloader, который загружает файлы с Google Диска, используя модули aiohttp и aiofiles. Полный пример кода с выде­ ленными отличиями показан ниже: #asyncio_casestudy.py import asyncio import time import aiofiles, aiohttp from getfilelistpy import getfilelist TASK POOL SIZE = 5 resource = { \"api_key\": \"AizaSyDYКmm85keqnk4bF1DpYa2dКrGns4z0\", \"id\": \"0B8TxHW2CiбdЬckVwetTlVЗRUU\", \"fields\": \"files(name, id, webContentLink)\", async def mydownloader (name, queue) : while True: #получаем id и имя файла из очереди item = await queue.get() try: async with aiohttp.ClientSession() as sess: async with sess.get(item['webContentLink']) as resp: if resp.status == 200: f = await aiofiles.open('./files/{}' . format (item[ 'name']), mode='wb') await f.write(await resp.read()) awai t f. close () finally: print ( f\" { name} : Download completed for \", i tem [ 'name' ] ) queue.task_done()\n--- Страница 235 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное 239 Процесс загрузки списка файлов из общей папки аналогичен предыдущим приме­ рам с многопоточностью и многопроцессорной обработкой. Здесь мы создали пул задач (настраиваемый) на основе корутина mydownloader. Затем эти задачи планиру­ ются для совместного выполнения, и наш родительский процесс ждет завершения всех задач. Код выглядит следующим образом: def get_files(resource): res = getfilelist.GetFileList(resource) files_list = res['fileList'] [О] return files list async def main (): files = get_files(resource) fдобавляем информацию о файлах в очередь myqueue = asyncio.Queue() for item in files['files']: myqueue.put_nowait(item) tasks = [] for id in range(TASK_POOL_SIZE): task = asyncio.create_task(myclownloacler(f'Task-{id+l}', myqueue)) tasks.append(task) start_time = time.monotonic() awai t myqueue . join О total_exec_time = time.monotonic() -start time for task in tasks: task. cancel () await asyncio.qather(*tasks, return_exceptions=True) print(f'Time taken to download: {total_exec_time:.2f} seconds') asyncio.run(main()) Мы запустили приложение, варьируя количество задач (3, 5, 7 и 1 О). Время загруз­ ки файлов с помощью asyncio требует меньше времени, чем с помощью многопо­ точности или многопроцессорности. Время выполнения меняется в зависимости от компьютера, но в нашем случае (MacBook Pro Intel Core i5 с 16 ГБ ОЗУ) потребовалось около 4 секунд при 5 зада­ чах и 2 секунды при 1 О задачах. Это ощутимо быстрее, чем в двух предыдущих примерах. Такой результат ожидаем, поскольку asyncio обеспечивает лучшую реа­ лизацию параллелизма для задач ввода-вывода, когда их нужно осуществить с ис­ пользованием подходящего набора объектов программирования.\n--- Страница 236 ---\n240 Раздел 3. Масштабирование за пределы одного потока На этом мы завершаем изучение асинхронного программирования. Мы представи­ ли все ключевые компоненты для создания асинхронного приложения с помощью пакета asyncio. Заключение В этой главе мы рассмотрели различные варианты параллельного программирова­ ния на Python с использованием стандартных библиотек. Мы начали с многопоточ­ ности и основных принципов конкурентного выполнения. Затем узнали о такой проблеме многопоточности, как глобальная блокировка интерпретатора, которая разрешает доступ к объектам Python только одному потоку за раз. На практических примерах изучили концепции блокировки и синхронизации. Мы также обсудили типы задач, для которых многопоточное программирование наиболее эффективно. Мы узнали, как достичь параллелизма с помощью нескольких процессов в Python. Увидели, как обмениваться данными между процессами, используя общую память и серверный процесс, а также как безопасно обмениваться объектами с помощью Queue и Pipe. Наконец, написали пример с несколькими процессами, аналогичный многопоточному приложению. Затем познакомились с совершенно другим подхо­ дом к параллелизму с помощью асинхронного программирования. Мы начали с ба­ зовых концепций и ключевых слов async и await. Увидели, как создавать задачи или корутины с помощью пакета asyncio. И завершили главу примером с использовани­ ем принципов асинхронного программирования. В этой главе представлено много практических примеров реали;зации параллельных приложений. Эти знания пригодятся всем, кто хочет создавать многопоточные или асинхронные приложения с помощью стандартных библиотек, доступных в Python. В следующей главе мы поговорим про использование сторонних библиотек для создания параллельных приложений. Вопросы 1.Что координирует потоки Python? Это интерпретатор Python? 2.Что такое GIL в Python? 3.Когда следует использовать потоки-демоны? 4.Что лучше использовать в системе с ограниченной памятью для создания про- цессов: объект Process или объект Pool? 5.Что такое Futures в пакете asyncio? 6.Что такое цикл событий в асинхронном программировании? 7.Как написать асинхронный корутин или функцию на Python?\n--- Страница 237 ---\nГлава 7. Многопроцессорная обработка, многопоточность и асинхронное 241 Дополнительные ресурсы ♦«Learning Concurrency in Python», автор: Эллиот Форбс (Elliot Forbes). ♦«Python. Лучшие практики и ин.струмен.ты» (Expert Python Programming ) авто- ры: Михал Яворски (Michal Jaworski) и Тарек Зиаде (Tarek Ziade). ♦«Python 3 Object-Oriented Programming», автор: Дасти Филлипс (Dusty Phillips). ♦«Mastering Concurrency in Python», автор: Гуан Нгуен (Quan Nguyen). ♦«Python Concurrency with asyncio», автор: Мэтью Фаулер (Mathew Fowler). Ответы 1.Потоки и процессы координируются ядром операционной системы. 2.GIL (Глобальная блокировка интерпретатора) в Python -это механизм блоки­ ровки для одновременного выполнения только одного потока за раз. 3.Потоки-демоны используются, когда завершение потока не является проблемой после завершения основного потока. 4.Объект Pool является лучшим выбором, поскольку хранит в памяти только один активный процесс. 5.Futures похожи на механизм callback, который используется для обработки ре­ зультатов от вызовов async/await. 6.Объект цикл событий отслеживает задачи и контролирует поток управления между ними. 7.Асинхронный корутин можно начать писать с async def.\n--- Страница 238 ---\n8 Масштабирование Python с помощью кластеров В предыдущей главе мы обсуждали параллельную обработку на одном компьютере с использованием потоков и процессов. Здесь мы поговорим о параллельной обра­ ботке на нескольких машинах в кластере. Кластер (Cluster) -это группа вычис­ лительных устройств, работающих вместе для выполнения ресурсоемких задач вроде обработки данных. В частности, мы рассмотрим возможности Python в об­ ласти интенсивных вычислений, которые обычно используют кластеры для парал­ лельной обработки больших объемов информации. Для таких задач доступно мно­ жество инструментов, но мы сосредоточимся на Apache Spark (как на механизме обработки данных) и на PySpark (как на библиотеке Python для создания таких приложений). Если Apache Spark правильно настроен и реализован, можно существенно повысить производительность приложений и значительно превзойти платформы конкурен­ тов, наподобие Hadoop MapReduce. Мы также рассмотрим, как распределенные наборы данных используются в кластерной среде. Эта глава поможет понять, как использовать платформы кластерных вычислений для обработки больших объемов данных, а также как реализовать подобные приложения на Python. Для демонстра­ ции практического применения языка в области кластеров рассмотрим два учебных примера: первый -расчет значения числа п, а второй -генерирование облака слов из файла данных. Темы этой главы: ♦Возможности кластеров для параллельной обработки. ♦Устойчивые распределенные наборы данных. ♦PySpark для параллельной обработки. ♦Практические примеры использования Apache Spark и PySpark.",
      "debug": {
        "start_page": 202,
        "end_page": 238
      }
    },
    {
      "name": "Глава 8. Масштабирование Python с помощью кластеров 242",
      "content": "--- Страница 239 --- (продолжение)\nГлава 8. Масштабирование Python с помощ·ью кластеров 243 К концу главы вы узнаете, как писать приложения для обработки данных, которые можно выполнять на рабочих узлах в кластере Apache Spark. Технические требования В этой главе понадобится: ♦Python 3.7 или более поздней версии. ♦Кластер Apache Spark с одним узлом. ♦PySpark поверх Python для разработки программ-драйверов. ПРИМЕЧАНИЕ Версия, используемая для Apache Spark, должна совпадать с версией Python для за­ пуска драйверов. Пример кода для этой главы можно найти по адресу: https:/lgithub.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter08. Начнем с рассмотрения опций кластера, доступных для параллельной обработки. Возможности кластеров для параллельной обработки При обработке большого объема информации нам может не хватить одного компью­ тера с несколькими ядрами. Особенно, если речь идет о потоковой обработке в ре­ жиме реального времени. В таких ситуациях есть необходимость использования нескольких систем, способных обрабатывать информацию распределенным обра­ зом и выполнять задачи параллельно. Использование нескольких компьютеров для подобных ресурсоемких задач называется кластерными вычисления.ми (Cluster computing). Есть несколько фреймворков распределенной обработки больших дан­ ных, доступных для координации выполнения заданий в кластере. Самые попу­ лярные из них: Hadoop MapReduce и Apache Spark. Оба фреймворка имеют от­ крытый исходный код и поставляются Apache. Существуют разные варианты (на­ пример, Databricks) их исполнения с дополнительными возможностями; а также поддержкой обслуживания, но основные принципы остаются прежними. Если взглянуть на рынок, количество развертываний Hadoop MapReduce может быть больше, но рост популярности Apache Spark в конечном итоге меняет ситуа­ цию. Поскольку Hadoop MapReduce по-прежнему очень актуален, важно понять, что он собой представляет и почему Apache Spark является лучшим выбором. Крат­ ко рассмотрим их далее.\nГлава 8. Масштабирование Python с помощ·ью кластеров 243 К концу главы вы узнаете, как писать приложения для обработки данных, которые можно выполнять на рабочих узлах в кластере Apache Spark. Технические требования В этой главе понадобится: ♦Python 3.7 или более поздней версии. ♦Кластер Apache Spark с одним узлом. ♦PySpark поверх Python для разработки программ-драйверов. ПРИМЕЧАНИЕ Версия, используемая для Apache Spark, должна совпадать с версией Python для за­ пуска драйверов. Пример кода для этой главы можно найти по адресу: https:/lgithub.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter08. Начнем с рассмотрения опций кластера, доступных для параллельной обработки. Возможности кластеров для параллельной обработки При обработке большого объема информации нам может не хватить одного компью­ тера с несколькими ядрами. Особенно, если речь идет о потоковой обработке в ре­ жиме реального времени. В таких ситуациях есть необходимость использования нескольких систем, способных обрабатывать информацию распределенным обра­ зом и выполнять задачи параллельно. Использование нескольких компьютеров для подобных ресурсоемких задач называется кластерными вычисления.ми (Cluster computing). Есть несколько фреймворков распределенной обработки больших дан­ ных, доступных для координации выполнения заданий в кластере. Самые попу­ лярные из них: Hadoop MapReduce и Apache Spark. Оба фреймворка имеют от­ крытый исходный код и поставляются Apache. Существуют разные варианты (на­ пример, Databricks) их исполнения с дополнительными возможностями; а также поддержкой обслуживания, но основные принципы остаются прежними. Если взглянуть на рынок, количество развертываний Hadoop MapReduce может быть больше, но рост популярности Apache Spark в конечном итоге меняет ситуа­ цию. Поскольку Hadoop MapReduce по-прежнему очень актуален, важно понять, что он собой представляет и почему Apache Spark является лучшим выбором. Крат­ ко рассмотрим их далее.\n--- Страница 240 ---\n244 Раздел 3. Масштабирование за пределы одного потока Hadoop MapReduce Hadoop -платформа распределенной обработки общего назначения, которая вы­ полняет задания по обработке больших объемов данных на сотнях или тысячах вы­ числительных узлов в кластере Hadoop. На рис. 8.1 описаны три ключевых компо­ нента: MapRe,juce Внедрение данных Apacr,e Hadoop YARN Распределенная фаиповая систеr,,а Hadoop 1HDFS1 Рис. 8.1. Экосистема Apache Hadoop MapReduce Рассмотрим подробнее: ♦Распределенная файловая система Hadoop (Hadoop Distributed File System, HDFS) -нативная файловая система для хранения файлов таким образом, что их можно распараллелить в кластере. ♦YARN (Yet Another Resource Negotiator) -система, которая обрабатывает хранящиеся в HDFS данные и планирует выполнение заданий системой; может использоваться для обработки графов, потоков или пакетов. ♦MapReduce -фреймворк, который позволяет обрабатывать большие наборы данных, распределяя их на несколько более мелких наборов; он обрабатывает данные с помощью двух функций: map и reduce; их роли аналогичны тем, что мы рассматривали в главе 6 ( «Расширенные советы и приемы Python» ); ключевое отличие заключается в том, что мы используем множество функций map и reduce параллельно для обработки нескольких наборов данных одновременно. ♦Разделив большой набор данных на несколько маленьких, можно передать их в качестве входных множеству функций map для обработки на разных узлах кла­ стера. Каждая функция map принимает на входе один набор данных, обрабатыва­ ет его в соответствии с требованиями и выдает результат в виде пар «ключ­ значение». Как только будут доступны выходные данные от всех функций map, одна или несколько функций reduce объединят результаты в соответствии с тре­ бованиями.\n--- Страница 241 ---\nГлава 8. Масштабирование Python с помощью кластеров 245 ♦Для более подробного объяснения возьмем пример подсчета определенных слов, вроде «attack» или «weapon», в большом наборе текста. Текстовую информацию можно разделить на небольшие наборы данных (например, восемь). Таким обра­ зом, мы будем иметь восемь функций map для подсчета двух слов в предостав­ ленном наборе. Каждая функция map на выходе предоставляет количество слов «attack» и «weapon» в своем наборе. На следующем этапе выходные данные всех функций map передаются двум функциям reduce, по одной для каждого слова. Каждая функция reduce агрегирует результаты функций map для своего слова и предоставляет совокупный результат в качестве вывода. Схема работы фрейм­ ворка MapReduce для этого примера представлена на рис. 8.2. Обратите внимание, что в Python есть одноименные функции map и reduce: ·,. Входные данные·-•• ._ ,, .,_ ., .,_ ', Шаг предварительной обработки входных данных (Мар-шаг, Маппер) Рис. 8.2. Работа фреймворка MapReduce -• Остальные компоненты мы пропустим, поскольку они не имеют отношения к об­ суждению в этой главе. Hadoop, в основном, построен на Java, но писать компонен­ ты map и reduce для модуля MapReduce можно на любом языке программирования, включая Python. Hadoop MapReduce хорошо подходит для обработки больших фрагментов данных путем их разделения на небольшие блоки. Узлы кластера обрабатывают их по от­ дельности, а перед отправкой запрашивающему объекту результаты объединяются. Hadoop MapReduce обрабатывает данные из файловой системы, поэтому не очень эффективен с точки зрения производительности. Однако он работает очень хорошо, когда скорость не является критичным требованием, например, если обработка вы­ полняется ночью.\n--- Страница 242 ---\n246 Раздел 3. Масштабирование за пределы одного потока Apache Spark Apache Spark -фреймворк кластерных вычислений с открытым исходным кодом для потоковой и пакетной обработки данных в режиме реального времени. Главная его особенность заключается в обработке информации прямо в оперативной памя­ ти, что разгружает CPU или GPU, обеспечивает низкую задержку и подходит для многих реальных сценариев благодаря следующим дополнительным факторам: ♦Он обеспечивает быстрое получение результатов для критически важных и чув­ ствительных ко времени приложений (например, для обработки в режиме реаль­ ного времени). ♦Он хорошо подходит для многократного выполнения задач благодаря обработке непосредственно в оперативной памяти. ♦Он позволяет использовать алгоритмы машинного обучения прямо «из коробки» (без дополнительной настройки). ♦ В нем доступна поддержка дополнительных языков программирования, таких, как Java, Python, Scala и R. Apache Spark охватывает широкий спектр рабочих нагрузок, включая пакетные и потоковьtе данные и итеративную обработку. Прелесть заключается в его возмож­ ности использовать Hadoop (через YARN) в качестве кластера развертывания, но он имеет и собственный менеджер кластеров. На высоком уровне основные компоненты Apache Spark можно разделить на три уровня, как показано на следующей схеме (рис. 8.3). r-----., r-----, r-----,., r-----., Java Python Scala R L. _____ , L-----·-' _ _____ ., Подцерживаемые языки •• Компоненты ядра Управление кластером Рис. 8.3. Экосистема Apache Spark Рассмотрим уровни подробнее.\n--- Страница 243 ---\nГлава 8. Масштабирование Python с помощью кластеров 247 Поддержка языков Scala -нативный язык Apache Spark, поэтому он широко применяется для разра­ ботки. Фреймворк также предоставляет высокоуровневые API для Java, Python и R. Многоязычная поддержка обеспечивается с помощью интерфейса удаленного вызо­ ва процедур (Remote Procedure Call, RPC). Для каждого языка есть RРС-адаптер, написанный на Scala. Он преобразует клиентские запросы, написанные на другом языке, в нативные запросы на Scala. Что облегчает принятие языка сообществом разработчиков. Основные компоненты Кратко рассмотрим основные компоненты Apache Spark: ♦Spark Core и RDD: Spark Core -это движок, который отвечает за обеспечение абстракции для RDD, планирование и распределение заданий по кластеру, взаи­ модействует с системами хранения, такими, как HDFS, Amazon S3 или RDBMS, а также управляет памятью и восстановлением после сбоя; RDD (Resilient Dis­ tributed Dataset) -это устойчивый распределенный набор данных, который представляет собой неизменяемую коллекцию; RDD разделены на сегменты для выполнения на разных узлах кластера; в следующем подразделе мы подробно на них остановимся. ♦Spark SQL: модуль для запроса данных (хранящихся как в RDD, так и во внеш­ них источниках) с помощью абстрактных интерфейсов, использование которых позволяет разработчикам сочетать команды SQL с инструментами аналитики. ♦Spark Streaming: модуль для обработки данных в режиме реального времени, что очень важно для анализа потоков с низкой задержкой. ♦MLliЬ-(Machine Learning Library): используется для применения алгоритмов машинного обучения в Apache Spark. ♦GraphX: этот модуль предоставляет API для параллельных вычислений на ос­ нове графов; он поставляется с различными алгоритмами построения графов; Граф -это математическая концепция, основанная на вершинах и ребрах, ко­ торая показывает, как объекты связаны или зависят друг от друга; объекты представлены вершинами, а их связи -ребрами. Управление кластером Apache Spark поддерживает несколько менеджеров кластера (Cluster Manager), таких, как Standalone, Mesos, У ARN и Kubernetes. Их ключевой функцией являет­ ся планирование и выполнение заданий на узлах кластера, а также управление ре­ сурсами. Для взаимодействия с одним или несколькими менеджерами в основной программе или в программе-драйвере используется специальный объект SparkSession. До версии 2.0 точкой входа считался объект SparkContext, но сейчас его\n--- Страница 244 ---\n248 Раздел 3. Масштабирование за пределы одного потока АРI-интерфейс является частью объекта SparkSession. На следующей схеме (рис. 8.4) показано взаимодействие между менеджером кластера, SparkSession (sparkContext) и рабочими узлами (Worker node) в кластере: SparkSessюn ,---------------\"\"\"1� \" Рабочмйуэел I / / / 11 I I SrHчliЫI Рабочий узел -Менеджер кластера \\ 11jllj•111111\\1111 Программа-драйвер \\ \\ ' \\ Рабочий узел ---------� ifMш\\liiш:11 Рис. 8.4. Экосистема Apache Spark Объект SparkSession может подключаться к различным типам менеджеров кластера. После подключения узлы кластера через менеджеров получают исполнителей. Ис­ полнители -это процессы Spark, которые выполняют задания и хранят результаты работ. Менеджер кластера на главном узле отвечает за отправку кода приложения в процессы-исполнители на рабочем узле. Когда код и данные приложения переме­ щаются на рабочие узлы, объект SparkSession в программе-драйвере напрямую взаимодействует с процессами-исполнителями для выполнения задач. В Apache Spark 3.1 поддерживаются следующие менеджеры кластеров: 1.Standalone: простой менеджер, входящий в состав Spark Core. Основан на глав­ ных (мастеры) и рабочих (подчиненные) процессах. Мастер является, по сути, менеджером кластера, а подчиненные размещают в себе исполнителей. Главные и рабочие процессы могут размещаться на одной машине, но в реальности так никто не делает. Для наилучшего результата рабочие процессы рекомендуется распределять по разным машинам. Standalone прост в настройке и предоставляет большинство необходимых кластеру возможностей. 2.Apache Mesos: еще один менеджер общего назначения с поддержкой Hadoop MapReduce. Он предпочтителен в крупных кластерных средах. Идея Apache Mesos заключается в объединении множества физических ресурсов � один вир­ туальный, который действует как кластер и обеспечивает абстракцию на уровне узла. Задуман как распределенный менеджер. 3.Hadoop YARN: менеджер кластера для Hadoop. По своей природе также являет­ ся распределенным.\n--- Страница 245 ---\nГлава 8. Масштабирование Python с помощью кластеров 249 4.Kubernetes: менеджер находится на экспериментальной стадии. Его назначени­ ем является развертывание и масштабирования контейнерных приложений. По­ следний выпуск Apache Spark включает планировщик Kubemetes. Перед завершением темы также стоит упомянуть фреймворк Dask, который пред­ ставляет собой библиотеку с открытым исходным кодом, написанную на Python для параллельных вычислений. Dask работает напрямую с распределенными аппа­ ратными платформами вроде Hadoop. Фреймворк использует проверенные библио­ теки и проекты Python, такие, как NumPy, Pandas и scikit-learn. Dask меньше и легче, и может работать с кластерами малого и среднего размера. Тогда как Apache Spark поддерживает множество языков и рекомендуется для крупных кластеров. Мы рассмотрели возможности кластеров для параллельных вычислений и можем перейти к главной структуре данных Apache Spark -устойчивым распределенным наборам данных. Устойчивые распределенные наборы данных (RDD) Устойчивые распределенные наборы данных (Resilient Distributed Datasets, RDD) -это основная структура данных в Apache Spark. Представляет собой не только распределенную коллекцию объектов, но и разделен таким образом, что каждый набор может обрабатываться на разных узлах кластера. Это делает RDD ключевым элементом распределенной обработки. Более того, он очень гибок с точки зрения отказоустойчивости и способен восстановить данные после сбоя. При создании объекта RDD главный узел реплицирует его на несколько исполнителей или рабо­ чих узлов. Если исполнитель или рабочий узел выходит из строя, главный узел об­ наруживает сбой и передает выполнение исполнителю на другом узле. Новый узел уже будет иметь копию объекта RDD и сможет немедленно начать выполнение. Данные, обработанные исходным узлом до сбоя, будут потеряны и заново обрабо­ таны новым исполнителем. Далее рассмотрим две главные операции с RDD и узнаем, как создавать объект RDD из разных источников данных. Операции с RDD RDD является неизменяемым объектом, а, значит, после создания его нельзя изме­ нить. Но с данными в нем можно выполнять два типа операций: преобразования (Transformation) и действия (Action). Преобразования Эти операции применяются к RDD и приводят к созданию нового объекта. Они принимают распределенный набор в качестве входных данных и создают один или\n--- Страница 246 ---\n250 Раздел 3. Масштабирование за пределы одного потока несколько наборов на выходе. Важно помнить, преобразования ленивы по своей природе. Это означает, что они будут выполняться, только когда над ними совер­ шается действие. Для лучшего понимания концепции ленивых вычислений пред­ ставим, что нам нужно преобразовать числовые данные в RDD, вычитая единицу из каждого элемента, а затем арифметически складывая ( операция действие) все эле­ менты в выходном RDD из этапа преобразования. Из-за ленивых вычислений опе­ рация преобразования не выполнится, пока не будет вызвана операция действия. В Apache Spark доступно несколько встроенных функций преобразования. Самые распространенные из них: ♦map: проходит по каждому элементу или строке объекта RDD и применяет задан­ ную функцию к каждому из них; ♦filter:. фильтрует данные из исходного RDD и предоставляет новый RDD с от­ фильтрованными результатами; ♦union: применяется к двум RDD, если они одного типа, и создает новый RDD, который является объединением входных RDD. Действия Действия -это вычислительные операции, применяемые к RDD. Их результаты должны быть возвращены в программу-драйвер (например, SparkSession). В Apache Spark доступно несколько встроенных функций-действий. Самые распространен­ ные из них: ♦count: возвращает количество элементов в RDD; ♦collect: возвращает в программу-драйвер весь RDD; ♦reduce: свертывает элементы в RDD; простой пример -операция сложения в наборе данных. С полным списком преобразований и действий можно ознакомиться в официальной документации Apache Spark. Далее узнаем, как создать RDD. Создание RDD Существует три подхода к созданию RDD. Распараллеливание коллекции Это один из самых простых подходов к созданию RDD. В этом случае коллекция создается или загружается в программу, а затем передается методу parallelize объ­ екта SparkContext. Этот вариант не используется за пределами разработки и тестиро­ вания, поскольку он требует наличия всего набора данных на одной машине, что очень неудобно при большом объеме информации.\n--- Страница 247 ---\nГлава 8. Масштабирование Python с помощью кластеров 251 Внешние наборы данных Apache Spark поддерживает распределенные наборы данных из локальной файло­ вой системы, HDFS, НВаsе или даже Amazon SЗ. При таком подходе данные загру­ жаются напрямую из внешнего источника. Для объекта SparkContext доступны удобные методы загрузки всех типов данных в RDD. Например, метод textFile можно использовать для загрузки текстовых данных из локальных или удаленных источников с указанием соответствующего URL-aдpeca (например, file://, hdfs:// или s3n: / /). Загрузка из существующих RDD RDD можно создавать с помощью операций преобразования. Это одно из главных отличий платформы от Hadoop MapReduce. Входной набор изменить нельзя, но можно создавать новые RDD из существующих. Несколько примеров такого созда­ ния с помощью функций map и filter мы уже видели. Далее рассмотрим дополнительные детали с прuмерами кода, используя библиоте­ ку PySpark. PySpark для параллельной обработки данных Как уже упоминалось, Apache Spark написан на Scala, что означает отсутствие на­ тивной поддержки Python. Существует также большое сообщество аналитиков и специалистов по data science, которые предпочитают использовать Python благода­ ря большому выбору библиотек. Бьшо бы неудобно переходить на другой язык только для распределенной обработки данных. Таким образом, интеграция Python с Apache Spark полезна не только сообществу data science, но и многим другим раз­ работчикам, которые хотели бы внедрить Apache Spark без изучения нового языка программирования. Сообщество Apache Spark разработало библиотеку PySpark для облегчения работы с платформой. Для работы кода Python с Apache Spark (написанным на Scala и Java), была разработана Jаvа-библиотека -Py4J. Она идет в комплекте PySpark и позволяет коду Python взаимодействовать с о.бъектами JVM (Java Virtual Machine). По этой причине перед установкой PySpark нужно установить JVМ. PySpark предлагает почти те же возможности и преимущества, что и Apache Spark. К ним относятся вычисления в оперативной памяти, возможность распараллелить рабочие нагрузки, использование ленивых вычислений и поддержка нескольких менеджеров кластера, например, Spark, У ARN и Mesos. Установка PySpark (и Apache Spark) выходит за рамки этой книги. В этой главе ос­ новное внимание будет уделяться использованию PySpark для раскрытия силы Apache Spark, а не их установке. Однако все же стоит упомянуть некоторые вари­ анты установки и зависимости.\n--- Страница 248 ---\n252 Раздел 3. Масштабирование за пределы одного потока В Интернете доступно множество руководств по установке для любой версии Apache Spark/PySpark и различных целевых платформ (Linux, macOS и Windows). PySpark включен в официальный релиз Apache Spark, который можно загрузить с веб-сайта (https://spark.apache.orgl). PySpark также доступен через утилиту pip от PyPI, которую можно использовать для локальной установки или для подключения к удаленному кластеру. Другой вариант установки PySpark-Anaconda, еще одна популярная система управления пакетами и средами. Если PySpark устанавливается вместе с Apache Spark, на целевой машине требуются следующие компоненты: ♦NM; ♦Scala; ♦Apache Spark. Для примеров кода, которые будут обсуждаться позже, мы установили Apache Spark версии 3.1.1 на macOS с включенным PySpark. PySpark поставляется вместе с командной оболочкой PySpark shell -интерфейсом командной строки для PySpark API. При запуске PySpark shell автоматически инициализирует объекты SparkSession и SparkContext, которые можно использовать для взаимодействия с ос­ новным движком Apache Spark. На рис. 8.5 показана инициализация PySpark shell: Рис. 8.5. PySpark shell На этапах инициализации PySpark shell можно наблюдать следующее: ♦Объект sparkcontext уже создан, его экземпляр доступен в командной оболочке как sc. ♦Также создан объект SparkSession, его экземпляр доступен как spark; теперь sparkSession является точкой входа в PySpark для динамического создания объ­ ектов RDD и DataFrame; объект SparkSession также может быть создан про­ граммно, и позже мы рассмотрим это на примере. ♦Apache Spark имеет пользовательский веб-интерфейс и веб-сервер для его раз­ мещения, и запускается по адресу http://192.16B.1.110:4040 для локальной уста­ новки; обратите внимание, что IР-адрес в этом URL является частным и принад­ лежит нашему компьютеру; порт 4040 выбран как порт по умолчанию; если он занят, будет выбран следующий доступный порт, например, 4041 или 4042.\n--- Страница 249 ---\nГлава 8. Масштабирование Python с помощью кластеров 253 В следующих подразделах мы узнаем, как создавать объекты SparkSession, изучим использование PySpark для операций с RDD и познакомимся с PySpark DataFrames и PySpark SQL. Для начала создадим сеанс Spark, используя Python. Создание программ SparkSession и SparkContext До выпуска Spark 2.0 sparkContext использовался как точка входа в PySpark. Начи­ ная с версии 2.0 в качестве точки входа был введен объект sparksession (для работы с RDD и DataFrame). SparkSession также включает все API, доступные в SparkContext, SQLContext, StreamingContext и HiveContext. Теперь SparkSession также может быть соз­ дан с помощью класса SparkSession, используя метод builder, как показано в сле­ дующем примере: import pyspark from pyspark.sql import SparkSession sparkl = SparkSession.builder .master (\"local [2] \") . appName ( 'New Арр' ) . qetOrCreate () Когда этот код выполняется в PySpark shell, в которой уже есть объект SparkSession по умолчанию (созданный как spark), он вернет тот же сеанс в качестве вывода ме­ тода builder. Вьmод консоли показывает расположение двух объектов SparkSession (spark и sparkl), это подтверждает, что они указывают на один объект SparkSession: »> spark <pyspark.sql.session.SparkSession oЬject at Ох1091019е8> »> sparkl <pyspark.sql.session.SparkSession oЬject at Ох1091019е8> Несколько ключевых понятий касательно метода builder: ♦getorcreate: метод гарантирует, что мы получим уже созданный сеанс в случае PySpark shell; если нового сеанса еще не существует, метод его создаст; в про­ тивном случае он вернет уже существующий сеанс; ♦master: если нужно создать сеанс, подключенный к кластеру, следует указать имя главного процесса (master), которым может быть имя экземпляра менеджеров Spark, У ARN или Mesos; если используется локально развернутый Apache Spark, можно использовать local [n], где n -целое число больше нуля; параметр n будет определять количество разделов, которые будут созданы для RDD и DataFrame; для локальной установки n может быть числом ядер ЦП; если уста­ новить для него значение local [ * J, что является распространенной практикой, это создаст столько рабочих потоков, сколько логических ядер есть в системе. Если нужно создать новый объект SparkSession, можно использовать метод newSession, доступный на уровне экземпляра существующего объекта SparkSession.\n--- Страница 250 ---\n254 Раздел 3. Масштабирование за пределы одного потока Пример кода для создания нового объекта SparkSession: import pyspark from pyspark.sql import SparkSession spark2 = spark.newSession() Консольный вывод для объекта spark2 подтверждает, что это сеанс, отличный от ранее созданных объектов SparkSession: >>> spark2 <pyspark.sql.session.SparkSession oЬject at Ox10910df98> Объект Sparkcontext также можно создать программно. Самый простой способ по­ лучить объект SparkContext из экземпляра SparkSession -использовать атрибут sparkContext. В библиотеке PySpark также есть класс SparkContext, который можно использовать для создания объекта SparkContext напрямую. Такой подход был рас­ пространен до Spark версии 2.0. ПРИМЕЧАНИЕ У нас может быть несколько объектов SparkSession, но только один объект SparkContext для каждой JVM. Класс sparkSession предлагает еще несколько полезных методов и атрибутов: ♦getActiveSession: метод возвращает активный SparkSession в текущем потоке Spark; ♦createDataFrame: метод создает объект DataFrame из RDD, списка объектов или объекта pandas DataFrame; ♦conf: атрибут возвращает интерфейс конфигурации для сеанса Spark; ♦catalog: атрибут предоставляет интерфейс для создания, обновления или запроса связанных баз данных, функций и таблиц. Полный список методов и атрибутов приводится в документации PySpark для клас­ са SparkSession по адресу: https :/ /spark. apache. org/docs/latest/api/python/reference/api/. PySpark для операций с RDD В подразделе «Устойчивые распределенные наборы данных (RDD)» мы уже рас­ смотрели ключевые функции и операции с RDD. Далее мы расширим эти знания в контексте PySpark с помощью примеров. Создание RDD из коллекций Python или внешнего файла Некоторые способы создания RDD мы уже рассматривали. Теперь исследуем, как создавать их из коллекций Python в памяти или из внешнего файла.\n--- Страница 251 ---\nГлава 8. Масштабирование Python с помощью кластеров 255 Оба подхода описаны ниже: 1.Создать RDD из коллекции данных можно методом parallelize, доступным в эк­ земпляре sparkContext. Он распределяет коллекцию для формирования объекта RDD. Метод принимает коллекцию в качестве параметра. Второй параметр (не­ обязательный) задает количество создаваемых разделов. По умолчанию этот ме­ тод создает разделы по количеству ядер, доступных на локальном компьютере или заданньiх при создании объекта SparkSession. 2.Создать RDD из внешнего файла можно методом textFile, доступным в экземп­ ляре sparkcontext. Он может загрузить файл как RDD из HDFS или из локальной файловой системы (таким образом, он будет доступен на всех узлах кластера). Для локального развертывания на основе системы можно указать абсолютный и/или относительный путь. С помощью этого метода можно задать минимальное количество разделов, которые будут созданы для RDD. Далее приведен пример (rddcreate .py) для демонстрации синтаксиса операторов PySpark, которые используются для создания нового RDD: da ta = [ 5, 4, 6, 3, 2, 8, 9, 2, 8, 7, 8, 4, 4, 8, 2, 7, 8, 9, 6, 9] rddl = spark.sparkContext .parallelize(data) print(rddl.getNumPartitions()) rdd2 = spark.sparkContext .textFile('sample.txt') print(rdd2.getNumPartitions()) Обратите внимание, файл sample. txt содержит произвольные текстовые данные, поскольку его содержимое неважно для нашего примера. Операции преобразования RDD с использованием PySpark PySpark предлагает несколько встроенных операций преобразования. Для демонст­ рации, как реализовать такую операцию, как map, возьмем текстовый файл в качест­ ве входных данных и используем функцию map, доступную с RDD, с целью преоб­ разовать его в другой RDD. Пример кода rddtranforml.py показан ниже: rddl = spark.sparkContext .textFile('sample.txt') rdd2 = rddl.map(lamЬda lines: lines.lower()) rddЗ = rddl.map(lamЬda lines: lines.upper()) print(rdd2.collect()) print(rddЗ.collect()) В этом примере мы применили две лямбда-функции с операцией map для преобразо­ вания текста в объекте RDD в нижний и верхний регистры. В конце мы использо­ вали операцию collect для получения содержимого объектов RDD. Другой популярной операцией преобразования является filter, которую можно ис­ пользовать для фильтрации некоторых записей.\n--- Страница 252 ---\n256 Раздел 3. Масштабирование за пределы одного потока Ниже приведен пример (rddtranform2.py), который отфильтровывает все четные числа из RDD: data = [5, 4, 6, 3, 2, 8, 9, 2, 8, 7, 8, 4, 4, 8, 2, 7, 8, 9, 6, 9) rddl = spark.sparkContext.parallelize(data) rdd2 = rddl.filter(lamЬda х: х % 2 !=О) print(rdd 2.collect()) При выполнении этого кода на консоль выводятся элементы 3, 5, 7 и 9. Далее рас­ смотрим несколько примеров с операциями действия с PySpark. Операции действия с RDD с использованием PySpark Для демонстрации операции используем RDD, созданный из коллекции, а затем применим _несколько встроенных операций действия из библиотеки PySpark. При­ мер кода rddactionl.py: data = [5, 4, 6, 3, 2, 8, 9, 2, 8, 7, 8, 4, 4, 8, 2, 7, 8, 9, 6, 9] rddl = spark.sparkContext.parallelize(data) print(\"RDD contents with partitions:\" + str(rddl.qlan() .collect())) print (\"Count Ьу values: \" +str (rddl .countвyValue ())) print(\"reduce function: \" + str(rddl.qlan() .collect())) print(\"Sum of RDD contents:\"+str(rddl.sum())) print (\"top: \" + str (rddl. top (5))) print(\"count: \"+ str(rddl.count())) print(\"max: \"+ str(rddl.max())) print(\"min\" + str(rddl.lllin())) time.sleep(60) Некоторые операции очевидны и не требуют подробностей ( count, max, min и sum). Остальные действия, которые стоит пояснить, рассмотрим ниже: ♦glom: приводит к созданию RDD путем объединения в список всех записей из каждого раздела; ♦collect: метод возвращает все элементы RDD в виде списка; ♦reduce: сокращает число элементов в RDD; в нашем случае мы использовали лямбда-функцию для объединения двух элементов в один и так далее; это при­ ведет к добавлению всех элементов в RDD; ♦top (х): возвращает первые х элементов массива, если они упорядочены. Мы рассмотрели, как создаgать RDD с помощью PySpark и реализовывать опера­ ции преобразования и действия с RDD. В следующем подразделе поговорим о PySpark DataFrame, еще одной популярной структуре данных, которая использу­ ется для аналитики.\n--- Страница 253 ---\nГлава 8. Масштабирование Python с помощью кластеров 257 PySpark DataFrames PySpark DataFrame -это табличная структура данных, состоящая из строк и столбцов, подобно таблицам в реляционной базе данных и pandas DataFrame, кото­ рые мы рассматривали в главе 6 ( «Расширен,н,ые советы и приемы Python» ). Основ­ ное отличие от pandas DataFrame в том, что объекты PySpark DataFrames распреде­ лены в кластере, то есть хранятся на разных его узлах. Использование DataFrame в основном предназначено для распределенной обработки большого набора структу­ рированных и неструктурированных данных, объем которых может измеряться пе­ табайтами. Как и RDD, объекты PySpark DataFrame являются неизменяемыми и основаны на ленивых вычислениях (будут отложены, пока не понадобятся). В DataFrame можно хранить как числовые, так и строковые данные. Столбцы не могут быть пустыми. Они должны содержать один тип данных и быть одинаковой длины. Строки могут содержать данные разных типов. Имена строк должны быть уникальными. Далее узнаем, как создать DataFrame, и рассмотрим некоторые ключевые операции, используя PySpark. Создание объекта DataFrame PySpark DataFrame можно создать с помощью одного из следующих источников данных: ♦Коллекции (списки, кортежи и словари). ♦Файлы (CSV, XML, JSON, Parquet и т. д.). ♦RDD с использованием методов toDF или createDataFrame. ♦Потоковые сообщения Apache Kafka можно преобразовать в PySpark DataFrame с помощью метода readStream объекта SparkSession. ♦Таблицы базы данных (например, Hive и HBase) можно запрашивать с помощью традиционных команд SQL, и выходные данные будут преобразованы в PySpark DataFrame. Для начала создадим DataFrame из коллекции, что является самым простым и удобным для демонстрации подходом. Следующий фрагмент показывает, как соз­ дать PySpark DataFrame из набора данных о сотрудниках: data = [ ('James', '', 'Bylsma', 'HR', 'М' ,40000), ( 'Kamal', 'Rahim', '', 'HR', 'М', 41000), ( 'Robert', ' ', 'Zaine' , 'Finance', 'м', 35000) , ('Sophia', 'Anne', 'Richer', 'Finance', 'F',47000), ('John', 'Will', 'Brown', 'Engineering', 'F',65000)\n--- Страница 254 ---\n258 Раздел 3. Масштабирование за пределы одного потока colurnns [\"firstname\",\"mi ddlename\",\"lastname\",\"department\",\"gender\",\"salary\"] df = spark.createDataFrame(data =data, schema = colurnns) print(df.printschema()) print (df. show ()) Сначала мы создали строку данных в виде списка сотрудников, а затем создали схему с именами столбцов. Когда схема представляет собой список имен столбцов, тип каждого их них определяется данными в нем. Каждый столбец по умолчанию помечается как допускающий пустое значение. Для определения схемы DataFrame вручную можно использовать более сложный API (structType или StructField), кото­ рый включает установку типа данных и пометку столбца как допускающего или не допускающего пустое значение. В консольном выводе будет сначала показана схе­ ма, а затем содержимое DataFrame в виде таблицы: root 1--firstname: strinq (nullaЬle = true) 1--middlename: strinq (nullaЬle = true) 1--lastname: strinq (nullaЬle = true) 1--department: strinq (nullaЬle = true) 1--qender: strinq (nullaЬle = true) 1--salary: lonq (nullaЬle = true) +---------+----------+--------+-----------+------+-------+ lfirstnamelmiddlenamellastnamel departmentlqenderlsalaryl +---------+----------+--------+-----------+------+-------+ Jamesl Bylsmal НRI MI 400001 Кamal I Rahiml I НRI MI 410001 RoЬert 1 1 Zaine I Finance I М 1 35000 1 Sophial Annel Richerl Financel FI 470001 Johnl Willl BrownlEnqineerinql FI 650001 +---------+----------+--------+-----------+------+-------+ В следующем примере создадим DataFrame из СSV-файла, в котором будут те же записи, что и в предыдущем примере. В dfcreate2 .ру мы также определили схему вручную, используя объекты StructType и StructField: schemas = StructType([ \\ ]) StructField ( \"firstname\", StringType (), True), \\ StructField ( \"middlename\", StringType (), True), \\ StructField(\"lastname\",StringType() ,True), \\ StructField (\"department\", StringType (), True), \\ StructField(\"gender\", StringType(), True), \\ StructField (\"salary\", IntegerType (), True) \\ df = spark.read.csv('df2.csv', header=Тrue, schema=schemas) print(df.printSchema()) print (df. show ())\n--- Страница 255 ---\nГлава 8. Масштабирование Python с помощью кластеров 259 Консольный вывод будет аналогичным выводу из предыдущего примера. Импорт JSON, ХМL и текстовых файлов в DataFrame поддерживается методом read с по­ мощью схожего синтаксиса. Информа цию о поддержке других источников, таких, как RDD и базы данных, вы можете изучить самостоятельно. Работа с PySpark DataFrame После создания DataFrame из некоторых данных, независимо от их источника, мы готовы проанализировать его, преобразовать и выполнить некоторые действия для получения требуемого результата. Большинство операций, поддерживаемых PySpark DataFrame, аналогичны RDD и pandas DataFrame. В целях демонстрации загрузим в объект DataFrame те же данные, что и в предыдущем примере, а затем выполним следующие операции: 1.Выберем один или несколько столбцов, используя метод select. 2.Заменим значения в столбце, используя словарь и метод replace. В библиотеке PySpark доступны и другие варианты замены данных в столбце. 3.Добавим новый столбец со значениями на основе данных имеющегося столбца. Полный пример кода (dfoperations .py) показан ниже: data = [ ('James', '', 'Bylsma', 'HR', 'М' ,40000), ( 'Kamal ' , 'Rahim' , ' ' , 'HR' , 'М' , 41 ООО) , ( 'Robert' , ' ', 'Zaine', 'Finance', 'М', 35000) , ('Sophia', 'Anne', 'Richer', 'Finance', 'F',47000), ('John', 'Will', 'Brown', 'Engineering', 'F',65000) columns = [\"firstname\",\"middlename\",\"lastname\", \"department\",\"gender\",\"salary\"] df = spark.createDataFrame(data=data, schema = columns) #отображаем 2 столбца print(df .select([df.firstname, df.salary]) .show()) заменяем значения в столбце myDict = { 'F': 'Female', 'М': 'Male'} df2 = df.replace(myDict, subset=['gender']) #добавляем новый столбец Рау Level на основе име1С1Цегося столбца values dfЗ = df2.withCol\\ПDil(\"Pay Level\", when((df2 .salary < 40000), lit(\"lO\")) \\ .when((df.salary >= 40000) & (df.salary <= 50000), lit(\"ll\")) \\ .otherwise (lit (\"12\")) \\ print(dfЗ.show())\n--- Страница 256 ---\n260 Раздел 3. Масштабирование за пределы одного потока Консольный вывод будет следующим (рис. 8.6): +---------+------+ lfirstnamelsalaryl +---------+------+ Jamesl 400001 Kamall 410001 Robertl 350001 Sophial 470001 Johnl 650001 +---------+------+ +---------+----------+--------+-----------+------+------+---------+ lfirstnamelmid dlenameJlastnamel departmcntlgenderlsalarylPay Lcvell +---------+----------+--------+-----------+------+------+---------+ Jamesl Bylsmal HRI HaleJ 400001 11! Kamall Rahiml HRI �lalel 410001 11[ Robertl ZaineJ Financel HaleJ 350001 101 Sophial Annel Richerl FinancelFernalel 470001 111 Johnl Willl BrownlEngi neeringlFemalel 650001 121 +---------+----------+--------+-----------+------+------+---------+ Рис. 8.6. Консольный вывод программы dfoperations.py В первой таблице показан результат операции select. В следующей таблице показан результат операции replace для столбца gender и нового столбца Рау Level. Для работы с PySpark DataFrame доступно множество встроенных операций. Мно­ гие из них аналогичны тем, которые мы обсуждали в pandas DataFrame. Более под­ робную информацию можно найти в официальной документации Apache Spark. На этом этапе каждый может задать закономерный вопрос: зачем использовать PySpark DataFrame, когда уже есть pandas DataFrame, предлагающий такие же операции? Ответ прост: PySpark предлагает распределенные объекты DataFrame. И операции с такими объектами предназначены для параллельного выполнения в кластере узлов. Это делает производительность выше, чем у pandas DataFrame. До этого момента мы наблюдали, что нам, как разработчикам, не нужно ничего программировать касательно делегирования распределенных объектов RDD и DataFrame различным исполнителям в автономном или распределенном кластере. Наше внимание сосредоточено только на аспекте написания кода для обработки данных. Координация и связь с локальным или удаленным кластером узлов автома­ тически обеспечивается SparkSession и SparkContext. В этом прелесть Apache Spark и PySpark: они позволяют нам сосредоточиться на решении реальных проблем и не беспокоиться о том, как будут выполняться рабочие нагрузки.\n--- Страница 257 ---\nГлава 8. Масштабирование Python с помощью кластеров 261 PySpark SQL Spark SQL -это один из ключевых модулей Apache Spark, который используется для обработки структурированных данных и действует как механизм распределен­ ных SQL-запросов. Spark SQL обладает хорошей масштабируемостью, поскольку является механизмом распределенной обработки. Обычно источником для Spark SQL выступает база данных, но SQL-запросы также можно выполнять к временным представлениям, которые можно создать из RDD и DataFrame. Для демонстрации работы PySpark со Spark SQL будем использовать тот же DataFrame, что и в предыдущем примере, используя данные о сотрудниках для соз­ дания экземпляра тempView для SQL-запросов. В примере кода сделаем следующее: 1.Создадим PySpark DataFrame для данных о сотрудниках из коллекции Python, как в предыдущем примере. 2.Создадим экземпляр тempView из PySpark DataFrame, используя метод createOrReplaceTempView. 3.Используя метод sql объекта SparkSession, выполним стандартные SQL-запросы к экземпляру тempView вроде записей обо всех сотрудниках, сотрудниках с зар­ платой более 45 ООО, количестве сотрудников по половому типу, а также ис­ пользование SQL-команды group Ьу для столбца gender. Полный пример кода (sqll .py) выглядит следующим образом: data = [ {'James', '', 'Bylsma', 'HR', 'М' ,40000), { 'Kamal', 'Rahim', '', 'НR', 'М', 41000), { 'Robert', ' ' , 'Zaine', 'Finance', 'М', 35000) , {'Sophia', 'Anne', 'Richer', 'Finance', 'F',47000), {'John', 'Will', 'Brown', 'Engineering', 'F',65000) columns = [\"firstname\",\"middlename\",\"lastname\", \"department\",\" gender\",\"salary\"] df = spark.createDataFrame{data =data, schema = columns) df. createOrReplaceTempView { \"ЕМР _DATA\") df2 = spark.sql{\"SELECТ * Е'RСМ ЕМР_DАТА\") print{df2.show{)) dfЗ = spark.sql{\"SELECТ firstname,middlename,lastname, salary Е'RСМ ЕМР _DATA WНERE SALARY > 45000\") print{dfЗ.show{)) df4 = spark.sql{{\"SELECТ gender, count(*) fran ЕМР DATA group Ьу gender\") ) print{df4.show{))\n--- Страница 258 ---\n262 Раздел 3. Масштабирование за пределы одного потока Консольный вывод всех трех SQL-запросов следующий: +---------+----------+--------+-----------+------+------+ lfirstnamelmiddlenamellastnamel departmentlgenderlsalaryl +---------+----------+--------+-----------+------+------+ Jamesl Bylsmal НRI MI 400001 Кamall Rahiml 1 НRI MI 410001 RoЬertl 1 Zainel Financel MI 350001 Sophial Annel Richerl Financel FI 470001 Johnl Willl BrownlEngineeringl FI 650001 +---------+----------+--------+-----------+------+------+ +---------+----------+--------+------+ lfirstnamelmiddlenamellastnamelsalaryl +---------+----------+--------+------+ Sophia I Anne I Richer 1 4 7000 1 John I Will l Brown 1 65000 1 +---------+----------+--------+------+ +------+--------+ lgenderlcount(l) 1 +------+--------+ FI 21 MI 31 +------+--------+ Spark SQL-это обширная тема. Мы рассмотрели в общих чертах только введение в нее с целью показать возможности использования команд SQL поверх структур данных Spark без знания источника данных. На этом мы завершим обсуждение ис­ пользования PySpark для обработки и анализа данных. В следующем подразделе рассмотрим несколько учебных примеров реальных приложений. Практические примеры использования Apache Spark и PySpark В предыдущем подразделе мы познакомились с основными концепциями и архи­ тектурой Apache Spark и PySpark. Здесь мы рассмотрим два примера реализации приложений. Пример 1: калькулятор числа п в Apache Spark Мы будем рассчитывать число л с помощью кластера Apache Spark, который запу­ щен на локальной машине. Значение л равно значению площади круга, если его радиус равен 1. Перед обсуждением алгоритма и программы-драйвера важно обго­ ворить настройку кластера для этого примера.\n--- Страница 259 ---\nГлава 8. Масштабирование Python с помощью кластеров 263 Настройка кластера Apache Spark До этого момента мы использовали PySpark, установленный локально на компью­ тере без кластера. Для примера настроим кластер с использованием нескольких виртуальных машин. Существует множество инструментов виртуализации (напри­ мер, VirtualВox), любой из которых подойдет для настройки. Мы использовали Ubuntu Multipass (https:l/multipass.runl) для создания виртуаль­ ных машин поверх macOS. Multipass работает как в Linux, так и в Windows. Это облегченный менеджер виртуализации, созданный специально для разработчиков, и позволяющий создавать виртуальные машины одной командой. Менеджер имеет мало команд, что упрощает его использование. Если вы решите его использовать, рекомендуем ознакомиться с официальной документацией по установке и конфи­ гурации. С помощью Multipass в нашем примере были созданы следующие вирту­ альные машины (рис. 8.7): Name State 1Pv4 lmage vm1 Runn,ng 192 168 64.2 Ubuntu 20.04 LTS vm2 Runn111g 192.168 64.3 Ubuntu 20.04 LTS vmЗ Running 192 168 64 4 Ubuntu 20.04 LTS Рис. 8.7. Виртуальные машины, созданные для кластера Apache Spark Мы установили Apache Spark 3.1.1 на каждой виртуальной машине, используя ути­ литу apt-get. Запустили Apache Spark как главный процесс на виртуальной машине vml, а затем запустили Apache Spark как рабочий процесс на vm2 и vmЗ, указав URI­ aдpec главного процесса. В нашем случае это spark: //192 .168. 64. 2. 7077. Полная на­ стройка кластера выглядит следующим образом (рис. 8.8): Node Name Role WebUI 192.168.64.2 Master (Spark:// 192.168.64.2: 7077) http:/ / l 92.168.64.2:8080/ 192.168.64.3 \\,\\1orker http:/ /l 92.168.64.3:808 1/ 192. 168.64.4 Worker http:/ /l 92.168.64.4:808 1/ Рис. 8.8. Кластер Apache Spark Пользовательский веб-интерфейс главного узла представлен на рис. 8.9. Краткое описание веб-интерфейса главного узла будет следующим: ♦Веб-интерфейс предоставляет имя узла с URL-aдpecoм; в нашем случае мы ис­ пользовали IР-адрес в качестве имени хоста, поэтому IР-адрес виден в URL. ♦Есть 2 рабочих узла, каждый из которых использует 1 ядро ЦП и 1 ГБ памяти. ♦Веб-интерфейс также предоставляет информацию о запущенных и завершенных приложениях. Веб-интерфейс рабочих узлов показан на рис. 8.10.\n--- Страница 260 ---\n264 Раздел 3. Масштабирование за пределы одного потока 5poNe 3_1_1 Spark Master at spark://192.168.64.2:7077 URL: spark://192.168.64.2:7077 Allve Workers: 2 Cores ln use: 2 Total, о Used Memory in use: 2.0 GiB Total, О.О В Used Resources in use: Applicatlons: О Running, О Completed Dtivers: О Runnlng, О Completed Status: ALIVE • Workers (2) Worker 1d Address State Cores мemory worker-20210529145544-192.168 .64.З-43027 192.168.64.З:43027 ALIVE 1 (О Used) 1024.О MIB (О.О В Used) worker-20210529150201-192.168.64.4-34453 192.168.64.4:34453 ALIVE 1 (О Used) 1024.0 MiB (О.О В Used) ► Running Applications (О) ► Completed Applications {О) Рис. 8.9. Пользовательский веб-интерфейс главного узла в кластере Apache Spark Spёi� з.,., Spark Worker at 192.168.64.3:43027 1D: worker-20210529145544-192.168.64.З-43027 Master URL: spark:/1192.168.64.2:7077 Cores: 1 (О Used) Memory: 1024.0 MIB (О.О В Used) Resources: Back to Master Running Executors {О) ExecutorlD State Cores мemory Resources Job Details Logs ►Finished Executors {13) Рис. 8.1 О. Пользовательский веб-интерфейс рабочих узлов в кластере Apache Spark Кратко подытожим про веб-интерфейс для рабочих узлов: Resources ♦Веб-интерфейс предоставляет идентифи каторы рабочих процессов (ID), а также имена узлов и порты, на которых рабочие процессы прослушивают запросы. ♦ URL главного узла также предоставляется в веб-интерфейсе. ♦Доступна информация о ядре ЦП и памяти, выделенной рабочим узлам. ♦Веб-интерфейс предоставляет сведения о выполняемых (Running Executor) и уже завершенных заданиях (Finished Executor).\n--- Страница 261 ---\nГлава 8. Масштабирование Python с помощью кластеров 265 Написание программы-драйвера для расчета числа п Для вычисления числа 1t мы применим популярный алгоритм (Монте-Карло), ко­ торый предполагает, что квадрат имеет площадь 4, и в него вписан круг с радиусом 1.Идея состоит в генерации огромного количества случайных чисел в области квадрата. Внутри квадрата находится круг с тем же диаметром, что и длина сторо­ ны квадрата. Это означает, что круг вписан в квадрат. Значение 1t рассчитывается как соотношение числа точек внутри круга к общему числу точек. Ниже приведен полный пример кода для программы-драйвера. Мы используем два раздела, поскольку мы имеем два рабочих узла. Для каждого из них зададим 1 О ООО ООО точек. Также важно отметить, URL главного узла Spark указан в качестве атри­ бута master при создании сеанса: #casestudyl.py: калькулятор Pi from operator import add from random import random from pyspark.sql import SparkSession spark = SparkSession.builder.master(\" sp&rk://192.168.64.2:7077\") \\ .appName(\"Pi claculator арр\") \\ . getOrCreate () partitions = 2 n = 10000000 * partitions def func (_) : х = random() * 2 - 1 у= random() * 2 - 1 return 1 if х ** 2 +у** 2 <= 1 else О count = spark.sparkContext.parallelize(range(l, n + 1), partitions) .map(func) .reduce(add) print(\"Pi is roughly %f\" % (4.0 * count / n)) Консольный вывод будет следующим: Pi\" is roughly 3.141479 Веб-интерфейс Spark предоставит статус приложения во время выполнения и после его завершения. На скриншоте, представленном на рис. 8.11, видно, что в выполне­ нии задания участвовало два рабочих узла. Можно кликнуть на имя приложения для перехода к следующему уровню детали­ зации, как показано на рис. 8.12. На скриншоте видно, какие рабочие узлы участ­ вуют в выполнении задач и какие ресурсы используются ( если выполнение еще не завершено).\n--- Страница 262 ---\n266 Раздел 3. Масштабирование за пределы одного потока �� t-r .,, ,f� З.1.1 Spark Master at spark://192.168.64.2:7077 URL: spark://192.168.64.2:7077 Aliveworlcщ: Co\"'s ln use: 2 Total, � �е<! М8mory in u : 2,0 GiB TotaL 2.0 GtB USed Resour<:e$ ln use: Appliclltlona: 1 Run11in9, о Compteted Drtvers: О Runnlng, О Completed Status: ALIVE • Workers (2) Worl(erld Addreaa State Corea мemory workar-20210529145544-192.168.64.3-43027 192.168.64.3:43027 ALIVE 1 (1 Used) 1024.0 MIB (1024.0 MIB Used) worker-2021052.91502.01-192.168.64.4•34453 192.168.64.4:34453 ЛUVE 1 (1 Used) 1024.0 MiB (1024.0 MiB Used) • Running Applications (1) Memory per Resourcos Per R8&0Ute8S Appllcatlon 1D Name Cores Executor Executor Submit1ed Tlme User State Duratlon врр-20210529191451·0000 PI cl&et1la!Or 2 1024.0 MIB 2021/05/29 19,14:51 muaslf RUNNING 9 s (kill) ,1рр Рис. 8.11. Статус калькулятора числа 1r в пользовательском веб-интерфейсе Spark Spёi� з.,., Application: Pi claculator арр 1D: арр-2021052919 1451·0000 Name: Pi claculator арр User: muaslf Cores: Unlimlted (2 granted) Executor Limit: Unllmited (2 granted) Executor Memory: 1024.О MiB Executor Resources: Submlt Date: 2021/05/29 19:14:51 State: FINISHED •Executor Summary (2) ExecutorlD Worker Cores у Removed Executors (2) ExecutorlD Worker Memory о \\'IOГker-202105 29150201-192.168.64.4-34453 \\�orker-20210529145544-192.168.64.3-43027 Resources State Cores Memory Resources State 1024 1024 КILLED КILLED Рис. 8.12. Информация о калькуляторе 1r на уровне исполнителейLogs Logs stdout stderr stdout stderr Мы рассмотр ели, как возможно настроить кластер Apache Spark в целях тестирова­ ния и экспериментов, а также как создать программу-драйвер на Python с помощью библиотеки PySpark для подключения к Apache Spark и отправки задания на два разных узла кластера. В следующем примере создадим облако слов с помощью библиотеки PySpark.\n--- Страница 263 ---\nГлава 8. Масштабирование Python с помощью кластеров 267 Пример 2: создание облака слов с помощью PySpark Облако слов -это визуальное представление частоты слов, встречающихся в оп­ ределенных текстовых данных. Проще говоря, чем чаще слово встречается в тексте, тем более крупным и жирным шрифтом оно выделяется в облаке слов. Такая ви­ зуализация также известна как облако тегов или текстовое облако. Это очень по­ лезный инструмент, котор·ый помогает определить наиболее важные темы в тексте. Например, облако слов используется для анализа контента в социальных сетях в целях маркетинга, бизнес-аналитики и безопасности. Для демонстрации напишем приложение для создания облака слов, которое читает текстовый файл из локальной файловой системы. Файл импортируется в объект RDD, который затем обрабатывается для подсчета количества повторений каждого слова. Затем идет обработка этих данных с целью отфильтровать слова, которые встречаются меньше двух раз, а также длина которых меньше четырех букв. Дан­ ные о частоте слов передаются объекту библиотеки WordCloud. Для отображения об­ лака слов используется библиотека matplotlib. Полный пример кода показан ниже: #casestudy2.py: приложения для подсчета слов import matplotlib.pyplot as plt from pyspark.sql import SparkSession from wordcloud import WordCloud spark = SparkSession.builder.master(\"local[*]\")\\ .appName(\"word cloud арр\")\\ . getOrCrea te () wc threshold = 1 wl threshold = 3 textRDD = spark.sparkContext.textFile('wordcloud.txt',3) flatRDD = textRDD.flatмap(lamЬda х: x.split(' ')) wcRDD = flatRDD.map(lamЬda word: (word, 1)) .\\ reduceByKey(lamЬda vl, v2: vl + v2) #отфильтровываем слова с числом вхождений меньше порогового значения filteredRDD = wcRDD.filter(lamЬda pair: pair[l] >= wc_threshold) filteredRDD2 = filteredRDD.filter(lamЬda pair: len(pair[0]) > wl_threshold) word_freq = dict(filteredRDD2.collect()) #создаем объект wordcloud wordcloud = WordCloud(width =480, height=480, margin=0) .\\ generate_fran_frequencies(word_freq) #выводи получившееся изображения облака слов plt.imshow(wordcloud, interpolation ='bilinear') plt.axis(\"off\") plt.margins{x =0, у=О) plt.show()\n--- Страница 264 ---\n268 Раздел 3. Масштабирование за пределы одного потока Вывод программы отображается как оконное приложение и выглядит следующим образом (на основе текста из файла wordcloud. txt) (рис. 8.13): Рис. 8.13. Облако слов, созданное с помощью PySpark RDD Обратите внимание, что в примере была использована не очень большая выборка текста. В реальном мире объем данных может быть огромным, поэтому использо­ вание кластера Apache Spark для обработки является оправданным. Оба практических примера дают навыки использования Apache Spark для масштаб­ ной обработки данных. Они будут особенно полезны разработчикам, которые инте­ ресуются обработкой естественного языка (N atural Language Processing, NLP), анализом текста и анализом тональности (эмоционального окраса текста). Эти на­ выки важны для специалистов data science и всех, кто занимается обработкой дан­ ных для аналитики и построением алгоритмов для NLP. Заключение В этой главе мы рассмотрели параллельную обработку огромных объемов данных в кластере узлов. Сначала мы изучили различные возможности кластеров, доступные для обработки. Провели сравнительный анализ Hadoop MapReduce и Apache Spark. Это показало, что Apache Spark поддерживает больше языков и систем управления кластерами, а также превосходит Hadoop MapReduce при работе в режиме реально­ го времени за счет обработки в оперативной памяти.\n--- Страница 265 ---\nГлава 8. Масштабирование Python с помощью кластеров 269 Рассмотрели главную структуру данных Apache Spark -RDD. Узнали, как созда­ вать RDD из разных источников данных, и познакомились с двумя типами опера­ ций: преобразования и действия. Основная часть главы была посвящена созданию RDD с помощью PySpark и управ­ лению ими с помощью Python. Затем мы разобрали несколько примеров с опера­ циями преобразования и действия. Познакомились с PySpark DataFrame для рас­ пределенной обработки данных. И завершили тему знакомством с PySpark SQL, используя несколько примеров. В конце мы рассмотрели два реальных примера: калькулятор числа 7t и приложение для создания облака слов из текстовых данных. Эта глава дает большой опыт в настройке Apache Spark как локально, так и с ис­ пользованием виртуализации. Здесь также представлено множество примеров, ко­ торые помогут улучшить ваши практические навыки. Эти знания понадобятся всем, кто хочет решать задачи по обработке больших данных, используя кластеры для повышения эффективности и масштабирования. В следующей главе мы рассмотрим варианты использования таких фреймворков, как Apache Beam, а также поговорим об использовании публичных облаков для обработки данных. Вопросы 1.Чем Apache Spark отличается от Hadoop MapReduce? 2.Чем преобразования о!личаются от действий в Apache Spark? 3.Что такое ленивые вычисления в Apache Spark? 4.Что такое SparkSession? 5.Чем PySpark DataFrame отличается от pandas DataFrame? Дополнительные ресурсы ♦«Spark in Action», второе издание, автор: Жан-Жорж Перрин (Jean-Georges Perrin). ♦«Learning PySpark», авторы: Томаш Драбас (Tomasz Drabas), Денни Ли (Denny Lee). ♦«PySpark Recipes», автор: Раджу Кумар Мишра (Raju Kumar Mishra). ♦Документация по Apache Spark для вашего релиза ( https ://spark. apache. org/docs/rel#). ♦Документация по Multipass (https://multipass.run/docs).\n--- Страница 266 ---\n270 Раздел 3. Масштабирование за пределы одного потока Ответы 1.Apache Spark обрабатывает данные в памяти, тогда как Hadoop MapReduce вы­ полняет их чтение и запись в файловой системе. 2.Преобразование переводит данные из одной формы в другую, а результаты ос­ таются в кластере. Действия -это функции, которые применяются к данным для получения результатов, возвращаемых программе-драйверу. 3.Ленивые вычисления применяются, в основном, к операциям преобразования. Это означает, что они не выполняются, пока не будет инициировано действие с · объектом данных. 4.SparkSession -это точка входа в приложение Spark для подключения к одному или нескольким менеджерам кластера и работы с исполнителями для выполне­ ния задач. 5.PySpark DataFrame является распределенным объектом и должен быть доступен на нескольких узлах кластера Apache Spark для параллельной обработки\n--- Страница 267 ---\n9 Программирование на Python для облака Облачные вычисления (Cloud Computing) -это обширное понятие, которое охва­ тывает различные области применения, включая физические и виртуальные вычис­ лительные платформы, инструменты разработки программного обеспечения, плат­ формы обработки больших данных, хранилища, сетевые функции, программные сервисы и т. д. В этой главе мы рассмотрим использование Python для облачных вычислений в двух взаимосвязанных аспектах. Во-первых, изучим варианты ис­ пользования Python при разработке приложения для облачных сред выполнения. Во-вторых, продолжим обсуждение обработки больших объемов данных, начатое в главе 8 ( «Масштабирование Python с помощью кластеров»), используя облака вме­ сто кластеров. Основное внимание будет уделено трем общедоступным облачным платформам: Google Cloud Platform (GCP), Amazon Web Services (AWS) и Microsoft Azure. Темы этой главы: ♦Знакомство с облачными возможностями для приложений Python. ♦Создание веб-сервисов Python для облачного развертывания. ♦Использование Google Cloud Platfonn для обработки данных. К концу главы вы научитесь разрабатывать и развертывать приложения на облач­ ной платформе, а также использовать Apache Beam в целом и для Google Cloud Platfonn в частности. Технические требования Для этой главы понадобится: ♦Python 3. 7 или более поздней версии. ♦Аккаунт Google Cloud Platfonn (подойдет бесплатная версия).\n--- Страница 268 ---\n272 Раздел 3. Масштабирование за пределы одного потока ♦Google Cloud SDK, установленный на вашем компьютере. ♦Apache Beam, установленный на вашем компьютере. Примеры кода для этой главы можно найти по адресу: https ://github. com/PacktPuЬlishing/Python-for-Geeks/tree/master/Chapter09. Начнем с изучения возможностей облачных платформ, доступных при разработке приложений для облачных развертываний. Знакомство с облачными возможностями для приложений Python Сегодня облачные вычисления являются крайним рубежом программистов. В этом подразделе мы изучим, как создавать приложения, используя облачные среды раз­ работки или специальный набор средств разработки для облачного развертывания -SDK (Software Development Кit). Также увидим, как вьшолнять код в облачной среде. В дополнение, рассмотрим различные опции в задачах с большими объема­ ми данных (Apache Spark в облаке). Начнем с облачных сред разработки. Среды разработки Python для облака Для подготовки сред разработки в случае большой тройки общедоступных облаков, перечисленных выше, доступны две модели: ♦Полностью облачная ин,тегрирован,н,ая среда разработки (Integrated Develop­ ment Environment, IDE). ♦Локально установленная IDE с возможностью интеграции с облаком. Познакомимся с каждым вариантом поподробнее. Облачная IDE Рассмотрим наборы инструментов, которые могут предложить А WS, GCP и Azure. Существуют и другие облачные среды разработки. К ним относятся PythonAnyWhere, Repl.it, Trinket и Codeanywhere. Большинство из них предлага­ ет как платные, так и бесплатные лицензии. Но мы остановимся на большой тройке: 1. А WS предлагает сложную облачную ШЕ А WS Cloud9, которая доступна через браузер. Она дает разработчикам широкие возможности и поддерживает не­ сколько языков программирования, включая Python. Важно отметить, что Cloud9 поставляется как приложение, размещенное на виртуальной машине Amazon ЕС2. Сама IDE предоставляется бесплатно, но за использование базового экзем­ пляра Amazon ЕС2 и хранилища может взиматься плата, причем весьма услов­ ная при ограниченном использовании. Платформа также предлагает инструмен-",
      "debug": {
        "start_page": 239,
        "end_page": 268
      }
    },
    {
      "name": "Глава 9. Программирование на Python для облака 271",
      "content": "--- Страница 269 --- (продолжение)\nГлава 9. Программирование на Python для облака 273 ты для создания и тестирования кода при н.епрерывн.ой ин.теграции (CI) и н.епре­ рывн.ой доставке (CD). 2.А WS CodeBuild -еще один доступный сервис, который компилирует исходный код, выполняет тесты и собирает пакеты программного обеспечения для развер­ тывания. Это сервер сборки, похожий на Bamboo. Вместе с А WS Cloud9 часто используется А WS CodeStar, который предлагает платформу на основе проектов для разработки, создания и развертывания ПО. CodeStar предлагает предопреде­ ленные шаблоны проектов для определения всей цепочки инструментов непре­ рывной поставки вплоть до выпуска кода. 3.Microsoft Azure предлагает встроенную среду Visual Studio, которая доступна в облаке, если вы используете платформу Azure DevOps. Онлайн-доступ к Visual Studio основан по платной подписке. Эта IDE известна своим богатым функцио­ налом для совместной работы. Microsoft Azure предлагает конвейеры Azure (Azure Pipelines) для создания, тестирования и развертывания кода на любой платформе вроде Azure, А WS и GCP. Azure Pipelines подцерживают множество языков, включая Node.js, Python, Java, РНР, Ruby, С/С++, .NET и даже инстру­ менты мобильной разработки. 4.Google предлагает Cloud Code для написания, тестирования и развертывания ко­ да, который может быть написан как в вашем браузере (например, через А WS Cloud9), так и в любой локальной IDE. Cloud Code поставляется с плагинами для самых популярных IDE, например, IntelliJ IDE, Visual Studio Code и JetBrains PyCharm. Google Cloud Code доступен бесплатно и предназначен для сред за­ пуска контейнеров. Как А WS CodeBuild и Azure Pipelines, Google также предла­ гает эквивалентный сервис, известный как Cloud Build, для непрерывного созда­ ния, тестирования и развертывания ПО в различных средах, включая виртуаль­ ные машины и контейнеры. Google также предлагает Colaboratory (или Google Colab), с удаленным использованием Jupyter Notebooks. Он популярен у специа­ листов data science. 5.Помимо прочего, Google Cloud предлагает сервисы Tekton и Jenkins для по- строения моделей разработки и поставки CI/CD. В дополнение ко всем перечисленным инструментам и сервисам эти облачные платформы предлагают среды-оболочки (Shell environment), как онлайн, так и ус­ тановленные локально. Это быстрый способ управления кодом, пусть и в условиях ограниченных возможностей. Далее рассмотрим варианты локальных IDE для использования Python в облаке. Локальная IDE для облачной разработки Облачная среда разработки является отличным инструментом для нативных вари­ антов интеграции с остальной частью облачной экосистемы. Это позволяет удобно создавать экземпляры ресурсов по запросу и развертывать их, а также не требует никаких токенов аутентификации. Однако есть нюансы. Например, инструменты\nГлава 9. Программирование на Python для облака 273 ты для создания и тестирования кода при н.епрерывн.ой ин.теграции (CI) и н.епре­ рывн.ой доставке (CD). 2.А WS CodeBuild -еще один доступный сервис, который компилирует исходный код, выполняет тесты и собирает пакеты программного обеспечения для развер­ тывания. Это сервер сборки, похожий на Bamboo. Вместе с А WS Cloud9 часто используется А WS CodeStar, который предлагает платформу на основе проектов для разработки, создания и развертывания ПО. CodeStar предлагает предопреде­ ленные шаблоны проектов для определения всей цепочки инструментов непре­ рывной поставки вплоть до выпуска кода. 3.Microsoft Azure предлагает встроенную среду Visual Studio, которая доступна в облаке, если вы используете платформу Azure DevOps. Онлайн-доступ к Visual Studio основан по платной подписке. Эта IDE известна своим богатым функцио­ налом для совместной работы. Microsoft Azure предлагает конвейеры Azure (Azure Pipelines) для создания, тестирования и развертывания кода на любой платформе вроде Azure, А WS и GCP. Azure Pipelines подцерживают множество языков, включая Node.js, Python, Java, РНР, Ruby, С/С++, .NET и даже инстру­ менты мобильной разработки. 4.Google предлагает Cloud Code для написания, тестирования и развертывания ко­ да, который может быть написан как в вашем браузере (например, через А WS Cloud9), так и в любой локальной IDE. Cloud Code поставляется с плагинами для самых популярных IDE, например, IntelliJ IDE, Visual Studio Code и JetBrains PyCharm. Google Cloud Code доступен бесплатно и предназначен для сред за­ пуска контейнеров. Как А WS CodeBuild и Azure Pipelines, Google также предла­ гает эквивалентный сервис, известный как Cloud Build, для непрерывного созда­ ния, тестирования и развертывания ПО в различных средах, включая виртуаль­ ные машины и контейнеры. Google также предлагает Colaboratory (или Google Colab), с удаленным использованием Jupyter Notebooks. Он популярен у специа­ листов data science. 5.Помимо прочего, Google Cloud предлагает сервисы Tekton и Jenkins для по- строения моделей разработки и поставки CI/CD. В дополнение ко всем перечисленным инструментам и сервисам эти облачные платформы предлагают среды-оболочки (Shell environment), как онлайн, так и ус­ тановленные локально. Это быстрый способ управления кодом, пусть и в условиях ограниченных возможностей. Далее рассмотрим варианты локальных IDE для использования Python в облаке. Локальная IDE для облачной разработки Облачная среда разработки является отличным инструментом для нативных вари­ антов интеграции с остальной частью облачной экосистемы. Это позволяет удобно создавать экземпляры ресурсов по запросу и развертывать их, а также не требует никаких токенов аутентификации. Однако есть нюансы. Например, инструменты\n--- Страница 270 ---\n274 Раздел 3. Масштабирование за пределы одного потока могут быть бесплатными, но используемые ими ресурсы -нет. Кроме того, их доступность в офлайн-режиме не является идеальной. Разработчики любят писать код без привязки к онлайн, таким образом, они имеют возможность работать в по­ езде или парке. Из-за подобных недостатков разработчики предпочитают сначала задействовать локальные редакторы или IDE, а затем уже используют дополнительные инстру­ менты развертывания на облачных платформах. Среды Microsoft Azure (Visual Studio и Visual Studio Code) доступны на локальных машинах. Платформы А WS и Google предлагают свои собст�енные SDK и плагины для интеграции с любой IDE. Рассмотрим эти модели позже в этой главе. Облачные среды выполнения для Python Среда выполнения (Runtime environment) -это исполнительная платформа, на которой выполняется код. Самый простой способ сделать такую среду для Python -создать виртуальную машину Linux или контейнер с установленным Python. Версия языка может быть установлена по нашему выбору. Для интенсивной обра­ ботки больших объемов данных кластер Apache Spark можно настроить на вычис­ лительных узлах в облаке. Но это налагает ответственность :щ задачи, связанные с обслуживанием платформы, на случай если что-то пойдет не так. Почти все пуб­ личные облачные платформы предлагают более элегантные решения, которые по­ зволяют упростить работу разработчикам и ИТ -администраторам. Они дают уже готовые среды в зависимости от типов приложений. Мы обсудим несколько сред выполнения, предлагаемых Amazon А WS, GCP и Microsoft Azure. Среды выполнения от Amazon AWS Amazon А WS предлагает следующие варианты: ♦А WS Beanstalk: предоставляется в виде «платформа как услуга» (Platform-as­ a-Service, PaaS) и может использоваться для развертывания веб-приложений, разработанных на Java, .NET, РНР, Node.js, Python и многих других языках; этот сервис также предлагает возможность использования Apache, Nginx, Passenger или IIS в качестве веб-сервера; сервис обеспечивает гибкое управление базовой инфраструктурой, которая требуется для развертывания сложных приложений. ♦AWS Арр Runner: используется для запуска контейнерных веб-приложений и микросервисов с API; это полностью управляемый сервис; таким образом, у вас нет административных обязанностей, а также нет доступа к базовой инфра­ структуре. ♦А WS Lambda: это бессерверная вычислительная среда, которая позволяет за­ пускать код, не беспокоясь об управлении какими-либо базовыми серверами; сервис поддерживает множество языков, включая Python; несмотря на то что Lambda-кoд можно выполнять непосредственно из приложения, этот вариант\n--- Страница 271 ---\nГлава 9. Программирование на Python для облака 275 хорошо подходит для ситуаций, если необходимо выполнить определенный фрагмент .кода, в случае когда другие сервисы А WS инициируют событие. ♦А WS Batch: этот вариант используется для запуска вычислительных задач в больших объемах в виде пакетов; это альтернатива кластерам Apache Spark и Hadoop MapReduce. ♦А WS Юnesis: также используется для обработки данных, но в потоковом виде и в режиме реального времени. Среды выполнения от GCP Ниже приведены варианты сред выполнения, предлагаемые GCP: ♦Арр Engine: это вариант PaaS от GCP для разработки и размещения веб­ приложений в большом масштабе; приложения развертываются как контейнеры, но исходный код упакован в них инструментом развертывания (эта сложность скрыта от разработчиков). ♦CloudRun: используется для размещения любого кода, созданного в виде кон­ тейнера; контейнер-приложения должны иметь конечные точки НТТР для раз­ вертывания в CloudRun; в отличие от Арр Engine за упаковку приложений в кон­ тейнер отвечает сам разработчик. ♦Cloud Function: это бессерверное и одноцелевое решение для размещения об­ легченного кода Python; код обычно запускается через прослушивание событий в других службах GCP или через прямые НТТР-запросы; похож на сервис А WS Lambda. ♦Dataflow: еще один бессерверный вариант, но в основном предназначен для об­ работки данных с минимальной задержкой; упрощает жизнь специалистам data science, устраняя сложность базовой платформы обработки и предоставляя кон­ вейер данных на основе Apache Beam. ♦Dataproc: этот сервис предлагает вычислительную платформу на базе Apache Spark, Apache Flink, Presto и других инструментов; подходит для -задач обработ­ ки данных, зависящих от экосистем Spark или Hadoop; сервис требует ручной подготовки кластеров. Среды выполнения от Microsoft Azure Microsoft Azure предлагает следующие варианты сред выполнения: ♦Арр Service: используется для создания и развертывания веб-приложений в большом масшта�е; веб-приложение можно развернуть как контейнер или за­ пустить в Windows или Linux. ♦Azure Functions: бессерверная среда выполнения, которой управляют события; используется для выполнения кода на основе определенного события или пря­ мого запроса, как А WS Lambda и GCP CloudRun.\n--- Страница 272 ---\n276 Раздел 3. Масштабирование за пределы одного потока ♦Batch: используется для выполнения облачных задач, которым требуются сотни или тысячи виртуальных машин. ♦Azure Databricks: Microsoft совместно с Databricks предлагает эту платформу на базе Apache Spark для обработки больших объемов данных. ♦Azure Data Factory: бессерверный сервис для обработки потоковых данных и преобразования их в осмысленные результаты. Как можем видеть, большая тройка облачных поставщиков предлагает различные среды выполнения в зависимости от доступных приложений и рабочих нагрузок. На облачных платформах можно развернуть следующие варианты использования: ♦Разработка веб-сервисов и веб-приложений. ♦Обработка данных с помощью облачной среды выполнения. ♦Приложения на базе микросервисов (контейнеров) с использованием Python. ♦Бессерверные функции или приложения для облака. В следующих подразделах мы рассмотрим первые два варианта. Остальные обсу­ дим в последующих главах, поскольку они требуют более детального обсуждения. Далее мы начнем создавать веб-сервис, используя Python, а также узнаем, как раз­ вернуть его в среде выполнения GCP Арр Engine. Создание веб-сервисов Python для облачного развертывания Разработка приложений для облака отличается от разработки для локальной среды. Существуют три ключевых требования, которые необходимо учитывать, при соз­ дании и развертывании облачного приложения: 1.Веб-интерфейс: для большинства облачных развертываний подходят приложе­ ния с графическим пользовательским ин,терфейсом (Graphical User Interface, GUI) или программн,ым ин,терфейсом приложен,ия (Application Programming Interface, API). Приложения с командной строкой не могут использовать облач­ ную среду, если они не развернуты на выделенном экземпляре виртуальной ма­ шины, тогда мы можем запускать их с помощью SSH или Telnet. Поэтому в ка­ честве примера мы выбрали приложение с веб-интерфейсом. 2.Настройка среды: все публичные облака поддерживают разные языки про­ граммирования и разные их версии. Например, GCP Арр Engine поддерживает Python 3.7, 3.8 и 3.9 (по состоянию на июнь 2021 года). Иногда облачные серви­ сы позволяют использовать свою версию для развертывания. Для веб­ приложений также важно задать точку входа для доступа к коду и настройкам на уровне проекта. Обычно они определяются в одном файле (У АМL в случае GCP Арр Engine ).\n--- Страница 273 ---\nГлава 9. Программирование на Python для облака 277 3.Управление зависимостями: главная сложность, связанная с портативностью приложений, заключается в зависимостях от сторонних библиотек. Для прило­ жений GCP Арр Engine мы документируем все зависимости в текстовом файле (requirements . txt) вручную или с помощью команды PIP freeze. Существуют и другие элегантные способы решить эту проблему. Например, упаковать все сто­ ронние библиотеки с приложениями в один файл для облачного развертывания вроде веб-архива Java (файл .war). Другой подход заключается в объединении всех зависимостей, содержащих код приложения и целевую платформу, в кон­ тейнер с последующим его развертыванием непосредственно на хостинг-плат­ форме. Развертывание на основе контейнеров мы рассмотрим в главе 11 ( «Python для разработки микросервисов» ). Существует, как минимум, три варианта развернуть приложение веб-сервиса Python в GCP Арр Engine: ♦Используя Google Cloud SDK через интерфейс CLI (командная строка). ♦Используя веб-консоли GCP (портал) вместе с Cloud Shell (интерфейс CLI). ♦Используя стороннюю IDE, например, PyChaпn. Подробно остановимся на первом варианте и кратко опишем два остальных. ВАЖНОЕ ПРИМЕЧАНИЕ Общие процессуальные действия для развертывания приложений Python в AWS и Az.­ ure являются одинаковыми, но детали будут различаться в зависимости от поддержки SDK и API, доступных у каждого облачного поставщика. Использование Google Cloud SDK Здесь мы обсудим, как использовать Google Cloud SDK для создания и развертыва­ ния примера. Приложение будет развернуто на платформе Google Арр Engine (GAE). Это платформа PaaS, которая лучше всего подходит для развертывания веб­ приложений с использованием разных языков программирования, включая Python. Для использования Google Cloud SDK необходимо выполнить предварительную подготовку на локальном компьютере: ♦ У становим и инициализируем Cloud SDK; после установки можно получить к нему доступ через CLI и проверить версию с помощью команды ниже. Обратите внимание, что почти все команды Cloud SDK начинаются с gcloud: gcloud -version ♦ У становим компоненты Cloud SDK для добавления расширения Арр Engine для Python 3; это можно выполнить следующей командой: gcloud components install app-engine-python ♦GCP CloudBuild API должен быть включен для облачного проекта GCP.\n--- Страница 274 ---\n278 Раздел 3. Масштабирование за пределы одного потока ♦Также должен быть включен биллинг и привязан к вашему аккаунту для выстав­ ления платежных счетов, даже если используется бесплатный пробный аккаунт. ♦Права пользователя GCP для настройки нового приложения Арр Engine и вклю­ чения сервисов API должны быть предоставлены на уровне.владельца (owner). Далее опишем, как настроить облачный проект GCP, создать веб-сервис и развер­ нуть его в GAE. Создание облачного проекта GCP Концепция облачного проекта GCP аналогична большинству IDE. Он состоит из набора параметров, которые управляют взаимодействием кода с сервисами GCP и отслеживают используемые ресурсы. Проект должен быть связан с платежным ак­ каунтом (биллинг-аккаунтом). Это необходимое условие для отслеживания, сколь­ ко ресурсов и сервисов потребляется каждым проектом. Далее рассмотрим, как создать проект с помощью Cloud SDK: 1.Авторизуемся в Cloud SDK, используя команду ниже. Если вход еще не выпол­ нен, это перебросит нас в веб-браузер: gcloud init 2.Создаем новый проект с именем time-wsproj. Название проекта должно быть ко­ ротким и содержать только буквы и цифры. Для лучшей читаемости допускается использование дефиса ( -): gcloud projects create time-wsproj 3.Переключаем область Cloud SDK по умолчанию на новый проект следующей командой (если это еще не сделано): gcloud config set project time-wsproj 4.Это позволит Cloud SDK использовать данный проект по умолчанию для любой команды, которую мы отправляем через CLI. 5.Создаем экземпляр Арр Engine в проекте по умолчанию или в любом другом, используя атрибут proj ect с помощью одной из следующих команд: gcloud арр create #для проекта по умолчанию gcloud арр create --project=time-wsproj #для конкретного проекта 6.Обратите внимание, что эта команда зарезервирует облачные ресурсы (в основ­ ном, вычислительные и хранилище) и предложит выбрать регион и зону для размещения ресурсов. Можно выбрать ближайшие к вам или наиболее подхо­ дящие с точки зрения вашей аудитории. 7.Включаем сервис Cloud Build API для текущего проекта. Как уже упоминалось, сервис Google Cloud Build используется для сборки приложения перед его раз­ вертыванием в среде выполнения Google вроде Арр Engine. Сервис Cloud Build API легче всего включить через веб-консоль GCP, поскольку это требует всего несколько кликов мышью. Для включения его с помощью Cloud SDK, нужно знать точное название сервиса. Список доступных сервисов можно получить с помощью команды gcloud services list.\n--- Страница 275 ---\nГлава 9. Программирование на Python для облака 279 8.Команда выдаст длинный список, в котором можно найти нужный сервис, свя­ занный с Cloud Build. Можно также использовать атрибут format с любой ко­ мандой для получения читаемого вывода Cloud SDK. Можно сделать еще удоб­ нее, используя утилиту grep (для Linux или macOS) вместе с командой для фильтрации результатов, а затем включив сервис командой еnаЫе: gcloud services list --availaЫe I grep cloudЬuild #Вывод будет таким -NАМЕ: cloudЬuild.googleapis.com #Команда Cloud SDK для включения сервиса gcloud services еnаЫе cloudЬuild.googleapis.com 9.Для включения Cloud Billing API в проект сначала нужно связать проект с бил­ линг-аккаунтом. Поддержка таких аккаунтов в Cloud SDK пока невозможна для общедоступной версии (General Availabllity, GA) в релизе Cloud SDK 343.0.0. Привязать биллинг-аккаунт к проекту можно через веб-консоль GCP. Но суще­ ствует также бета-версия команд Cloud SDK, дублирующая те же функции. Сначала нужно узнать идентификатор аккаунта, который будет использоваться. Информацию о биллинг-аккаунтах, связанных с авторизованным пользовате­ лем, можно получить с помощью команды beta: gcloud Ьеtа billing accounts list #Вывод будет включать следук:щую информацию #billingAccounts/0140E8-51Gl44-2AВ62E #Включение биллинга для текущего проекта gcloud beta billing projects link time-wsproj --billing­ account 0140Е8-51G144-2АВ62Е 10.Обратите внимание, если вы впервые используете команды beta, вам будет предложено установить Ьеtа-компонент. Необходимо дать согласие на уста­ новку. Если вы уже используете версию Cloud SDK с биллинг-компонентом, включенным для GA, можно пропустить использование beta или использовать соответствующие команды в соответствии с документацией для вашего релиза Cloud SDК. 11.Включаем сервис Cloud Billing API для текущего проекта, выполнив те же дей­ ствия, что и для Cloud Build API. Сначала нужно найти имя сервиса, а затем включить его следующим набором команд: gcloud services list --availaЫe I grep cloudЬilling #Вывод будет следующим -NАМЕ: cloudЬilling.googleapis.com #команда для включения сервиса gcloud services еnаЫе cloudЬilling.googleapis.com Шаги по настройке проекта просты для опытного пользователя облачных сервисов и займут всего несколько минут. После создания можно узнать информацию о кон­ фигурации следующей командой: gcloud projects describe time-wsproj\n--- Страница 276 ---\n280 Раздел 3. Масштабирование за пределы одного потока В консольном выводе будет статус жизненного цикла, имя, идентификатор и номер проекта. Ниже приведен пример вывода: createTime: '2021-06-05Т12:ОЗ:31.039Z' lifecycleState: ACТIVE name: time-wsproj projectid: time-wsproj projectNumЬer: '539807460484' Теперь, когда проект создан и настроен, можем приступать к разработке веб­ приложения. Создание приложения Python Для облачных развертываний можно создать приложение с помощью IDE или сис­ темного редактора, а затем локально эмулировать среду выполнения Арр Engine с помощью Cloud SDK и компонента app-engine-python, который мы предварительно установили. В качестве примера создадим приложение на базе веб-сервиса, которое будет предоставлять время и дату через REST API. Приложение можно запускать через АРI-клиент или веб-браузер. Для упрощения примера мы не будем включать аутентификацию. Создадим виртуальную среду с помощью пакета venv. Она будет использоваться в качестве обертки для интерпретатора, ядра, сторонних библиотек и скриптов, отде­ ляя их как от системного, так и других виртуальных Руthоn-окружений. Данный функционал поддерживается в Python, начиная с версии 3.3. Для создания вирту­ альных окружений существуют и другие инструменты, например, virtualenv и pipenv. РуРА рекомендует использовать venv, поэтому мы выбрали его для боль­ шинства примеров в этой книге. Для начала создадим каталог проекта с именем time-wsproj, который содержит сле­ дующие файлы: ♦app.yaml; ♦main.py; ♦requirements. txt. Для удобства мы назвали каталог тем же именем, что и облачный проект, но это не обязательное действие. Рассмотрим файлы подробнее. Файл app.yaml содержит параметры развертывания и среды выполнения, например, номер версии среды. В Python 3 файл app.yaml должен иметь, как минимум, пара­ метр среды выполнения (runtime-пapaмeтp): runtime: python38 Каждый сервис веб-приложения может иметь собственный У АМL-файл, но для простоты мы будем использовать только один. В нашем случае он будет содержать только атрибут среды выполнения (runtime-aтpибyт). Мы добавили еще несколько атрибутов в файл У AML для иллюстрации.\n--- Страница 277 ---\nГлава 9. Программирование на Python для облака 281 Модуль main. ру является точкой входа в приложение. Для примера мы выбрали библиотеку Flask. Она используется для веб-разработки и популярна, в основном, благодаря своей простоте и мощному функционалу. Мы рассмотрим ее более под­ робно в следующей главе. Полный код представлен ниже: from flask import Flask from datetime import date, datetime # Если точка входа не определена в app.yaml, Арр Engine будет искать # переменную арр, как в нашем УАМL-файле арр = Flask (_ name _) @app.route (' /') def welcome(): return 'Welcome Python Geek! Use appropriate URI for date and time' @app.route (' /date') def today () : today = date.today() return \"{date:\" + today.strftime(\"%B %d, %У\") + '}' @app.route ( '/time') def time () : now = datetime.now() return \"{time:\" + now.strftime(\"%H:%M:%S\"} + '}' if name == ' main # Для локального тестирования app.run(host='l27.0.0.l', port=8080, debug=True) Основные возможности этого модуля: ♦Здесь определена точка входа по умолчанию, которая имеет имя арр; она ис­ пользуется для перенаправления запросов, отправляемых этому модулю. ♦С помощью аннотации Flask мы определили обработчики для трех URL-aдpecoв: •Корневой URL (/) вызывает функцию welcome, которая возвращает приветст­ венное сообщение в виде строки. •URL /date вызывает функцию today, которая возвращает текущую дату в формате JSON. •URL /time вызывает функцию time, которая возвращает текущее время в фор­ мате JSON. ♦В конце модуля мы добавили функцию _ main _ для запуска локального веб- сервера,, который поставляется с Flask в целях тестирования. Файл requirements. txt содержит список зависимостей проекта от сторонних библио­ тек. С помощью его содержимого Арр Engine сделает необходимые библиотеки доступными приложению. В нашем случае потребуется библиотека Flask. Содер­ жимое этого файла будет следующим: Flask==2.0.l\n--- Страница 278 ---\n282 Раздел 3. Масштабирование за пределы одного потока Подготовив каталог проекта и описанные выше файлы, мы должны создать вирту­ альную среду внутри или вне каталога проекта и активировать его командой source: python -m venv myenv source myenv/bin/activate После активации мы должны установить необходимые зависимости в соответствии с файлом requirements. txt. Используем утилиту pip из того же каталога, где нахо­ дится requirements. txt: pip install -r requirements.txt После установки Flask и зависимостей структура каталога в PyCharm IDE будет выглядеть следующим образом (рис. 9.1): т time-wsproj т .myenv ► Ьin ► llinclude т .,iЬ т • pythonЗ.8 •site-packages ► ll_pycache_ ► llclick ► click-8.0.1.dist-info ► flask ► • Rask-2.0.1.dist-info ► itsdangerous ► itsdangerous-2.0.1.dist-info ► Jinja2 ► Jinja2-3.0.1.dist-info ► markupsafe ► 111 MarkupSafe-2.0.1. dist-info ► plp ► pip-20.2.3.dlst-info ► 11 pkgJesources ► setuptools ► setuptoois-49.2.1.dist-info ► .werkzeug ► Werkzeug-2.0.1.dist-info � easyjnstall.py L, pyvenv.cfg ::__ app.yaml main.py _ requirements.txt Рис. 9.1. Структура каталога для примера веб-приложения После подготовки файла проекта и зависимостей локально запускаем веб-сервер следующей командой: python main.py\n--- Страница 279 ---\nГлава 9. Программирование на Python для облака 283 Сервер запустится с отладочными сообщениями, из которых ясно, что сервер пред­ назначен только для тестирования и не подходит в качестве рабочей среды: *Serving Flask арр 'main' (lazy loading) *Environment: production WARNING: Тhis is а development server. Do not use it in а production deployment. Use а production WSGI server instead. *DeЬug mode: on *Running on http://127.0.0.l:8080/ (Press CТRL+c to quit) *Restarting with stat *DeЬuqqer is active! *DeЬuqqer PIN: 668-656-035 Приложение веб-сервиса доступно по следующим URI: ♦http://localhost:8080/; ♦http://localhost:8080/date; ♦http://localhost:8080/time. Ответ от веб-серверов для этих URI будет следующий (рис. 9.2): D locallost:8080 х f--;, 0 (D localhost:8080 Wek:ome Ру1Ьоо Geek! Usc approprialc URI fOC' dare апd lime �--•w•• D localhost:8080/date � ➔ОQ)localhost:8080/date {date:August 09, 2021} �---,, 8 •• Е) D localhost:8080/time Q)localhost:8080/time {time:22:56:04} Рис. 9.2. Ответ от примера веб-сервиса в браузере Веб-сервер будет остановлен прежде, чем мы перейдем к следующему этапу, а именно, к развертыванию приложения в Google Арр Engine. Развертывание в Google Арр Engine Развернуть приложение в GAE можно, выполнив следующую команду из каталога проекта: gcloud арр deploy\n--- Страница 280 ---\n284 Раздел 3. Масштабирование за пределы одного потока Cloud SDK прочитает файл арр. yaml, который предоставляет входные данные для создания экземпляра Арр Engine. Во время развертывания создается образ контей­ нера с помощью Cloud Build, который затем загружается в хранилище GCP. После успешного развертывания можно получить доступ к веб-сервису следующей ко­ мандой: gcloud арр Ьrowse Команда откроет приложение, используя браузер по умолчанию на вашем компью­ тере. URL-aдpec будет отличаться в зависимости от региона и зоны, выбранных при создании проекта. Важно понимать, что при каждом выполнении команды deploy создается новая вер­ сия приложения в Арр Engine. Это означает, что будет потребляться больше ресур­ сов. Мы можем проверить версии, которые были установлены для веб-приложения, следующей командой: gcloud арр versions list Старые версии приложения по-прежнему могут быть в состоянии обслуживания (URL-aдpeca будут немного изменены). Их можно остановить, запустить и удалить с помощью команды gcloud арр versions в Cloud SDK и идентификатора версии. Приложение можно останавливать и запускать командами stop и start, как показано ниже: gcloud арр versions stop <version id> gcloud арр versions start <version id> gcloud арр versions delete <version id> Идентификатор версии доступен, когда мы запускаем команду gcloud арр versions list. На этом можно завершить обсуждение, как создавать и развертывать веб­ приложение Python в Google Cloud. Далее мы обсудим, как развернуть то же при­ ложение с помощью консоли GCP. Использование веб-консоли GCP Консоль GCP представляет собой простой в использовании веб-портал для доступа к проектам GCP и управления ими, а также онлайн-версию Google Cloud Shell. Консоль к тому же предлагает настраиваемые панели инструментов, отслеживание облачных ресурсов, потребляемых проектами, сведения о биллинге, логирование активности и многие другие возможности. Некоторые задачи можно выполнять не­ посредственно в веб-интерфейсе, но для большинства действий потребуется Cloud Shell. Это тот же Cloud SDK, но доступный онлайн в любом браузере. Тем не менее Cloud Shell дает больше возможностей, чем Cloud SDK, по несколь­ ким причинам: ♦Он дает доступ к таким CLI, как gcloud и kubectl; последний используется для управления ресурсами в движке GCP Kubemetes.\n--- Страница 281 ---\nГлава 9. Программирование на Python для облака 285 ♦Он позволяет разрабатывать, делать отладку, создавать и развертывать прило­ жения с использованием редактора Cloud Shell (Cloud Shell Editor). ♦Cloud Shell также предлагает онлайн-сервер разработки для тестирования при­ ложения перед его развертыванием в Арр Engine. ♦Он обладает инструментами для обмена файлами между платформой Cloud She\\l и локальным компьютером. ♦Предусмотрена возможность предварительного просмотра веб-приложения че- рез порт 8080 или любой другой по вашему выбору. В Cloud Shell для создания нового проекта, сборки приложения и его. развертыва­ ния в Арр Engine используются те же команды, что и в Cloud SDК. Поэтому это останется вам для самостоятельного изучения. Необходимо будет просто повторить инструкции из предыдущего подраздела. Обратите внимание, проект можно соз­ дать с помощью консоли GCP. Интерфейс Cloud Shell можно включить, нажав зна­ чок Cloud Shell в верхнем меню справа. После включения в нижней части веб­ страницы консоли откроется интерфейс командной строки, как показано на скрин­ шоте (рис. 9.3): ·@· AppEnglne ••• - J. - WJ Vmlons •- ·-Taslt- <!) Cronjollo 1Б Reeп•№tea (1 CLOUD SHElL Termlnal Get started Resources .__ Python - ··--·------- - Standord •i АмdАрр EлgmPythonStondord ___ e. vislt О!!!!!!!! е for Python Stilndonl Envlronment oode N11\"4)1eo. Deploy wRh Google Cloud SDK DOWNI OAD ТНЕ Cl.OUO SOK $ gcl<Xld init Оер1оу to Арр Eng,,e $ gcloud 1рр deploy (time•WIP\"1P х + · ,L Open edltor !1!1 ф 111 Рис. 9.3. Консоль GCP с Cloud Shell. Как уже говорилось, Cloud Shell имеет инструмент редактирования, который мож­ но запустить с помощью кнопки Open editor. На следующем скриншоте, приве­ денном на рис. 9.4, показан файл Python, открытый в редакторе Cloud Shell.\n--- Страница 282 ---\n286 Раздел 3. Масштабирование за пределы одного потока ·@· ДppEngine ,., - J,, -· � -•-· ·-, _ <!) C.OO)al>o R•---�н <1 DashЬoard - to210611119112$ (100$) .-,_ 111 121, , и <ld 14 '\"\" IOd Summary eour.,, ,, r'\\est .t.,.rt f\\41•� ff'Cl8 dlttt111 1-ort oate 1 datttilt IIESEТZOOМ Version 202106111191125 ln sемсе default - 19:12 .1' __ щ __ ,., -.�Cll:IМa-11 , - lt:11; ,._App(ngk,e-­-·,_.,.·\"\"\"'-• • °\"\"\" tenninli вв i - �-- n ::t::::: �- ----- #te� ·- • Jf ·entrypoiAt' ts мt МН№ 1n -,,.у \\ . ,Р Щ11't wl\\\\. t.oot for •• � - 11 • а\\\\«!: ·арр• in fHe '1111.in,py'. 'rMs is the аи in Olilr r-,t tit• llfllP•f\\Atl(:(� 11 ttPcl',rovtt('/'J 12 tftf w.t(Ollill;(): 13 muni •tкt.«1К 1')thon Ge«k! tlи approprU!tt! UA.t for аи W tf.Jlle' ,. 1> 16 ean.rout,ef'/cl1te') 17 !Нf \\·ОС1�()1 18 \\od,fy • dtt•-t9Ну() Рис. 9.4. Консоль GCP с редактором Cloud She\\l > , >е-· Создавать и развертывать веб-приложения можно, используя также сторонние IDE с подключаемыми плагинами Google Арр Engine. Как показывает практика, плаги­ ны для популярных IDE, вроде PyCharm и Eclipse, в основном, созданы для Python 2 и устаревши х библиотек. Прямая интеграция IDE с GCP требует дополнительной работы и развития. На момент написания книги лучшим вариантом было использо­ вание Cloud SDK или Cloud Shell напрямую совместно с редактором или с IDE по вашему выбору. Мы рассмотрели разработку веб-приложений с помощью Python и их развертывание на платформе GCP Арр Engine. Amazon для этого предлагает сервис А WS Beanstalk, в котором процесс развертьmания почти совпадает с действиями для GCP Арр Engine, за исключением того, что Beanstalk не требует предварительной настройки проекта. Поэтому в решении от Amazon можно быстрее развертывать приложения. Для развертывания нашего приложения в А WS Beanstalk мы должны предоставить следующую информацию, используя консоль А WS или А WS CLI: ♦Имя приложения. ♦Платформа (в нашем случае Python 3.8). ♦Версия исходного кода. ♦Исходный код вместе с файлом requirernents. txt.\n--- Страница 283 ---\nГлава 9. Программирование на Python для облака 287 Мы рекомендуем использовать А WS CLI для веб-приложений, которые зависят от сторонних библиотек. Можно загрузить исходный код в виде ZIР-файла или веб­ архива ( файл WAR) с локального компьютера или скопировать его из хранилища Amazon S3. Детальные инструкции по развертыванию в А WS Beanstalk доступны по адресу: https://docs.aws.amazon.com/elasticbeanstalk/latestldglcreate-deploy-python-jlask.html. Azure для сборки и развертывания веб-приложений предлагает Арр Service. Инструкции по созданию и развертыванию в Azure доступны по адресу: https://docs.microsoft.com/en-us/azure/app-service/quickstart-python. Далее мы рассмотрим создание программ-драйверов для обработки данных с по­ мощью облачных платформ. Использование Google Cloud Platform для обработки данных Google Cloud Platfoпn предлагает Cloud Dataflow для пакетной и потоковой обра­ ботки данных в режиме реального времени. Сервис позволяет создавать конвейеры обработки для data science и аналитики. Cloud Dataflow использует Apache Beam, который был разработан Google, но сейчас развивается как проект Apache с откры­ тым исходным кодом. Он предлагает модель программирования для создания кон­ вейеров обработки данных. Такие конвейеры можно создавать с помощью Apache Beam, а затем выполнять с помощью сервиса Cloud Dataflow. Сервис Google Cloud Dataflow похож на Amazon Kinesis, Apache Stoпn, Apache Spark и Facebook Flux. Прежде чем перейти к обсуждению, как использовать Google Dataflow с Python, познакомимся с Apache Beam и его концепцией конвейера. Введение в основы Apache Beam Сегодня данные являются ценным активом для многих организаций. Приложения, устройства и люди генерируют огромные объемы информации, которые необходи­ мо обрабатывать, прежде чем их можно будет использовать. В номенклатуре Apache Beam действия по обработке данных обычно называются конвейерами. Иными словами, это ряд действий, которые выполняются над необработанными данными из различных источников, после чего они поступают в место назначения для дальнейшего использования в аналитике и других бизнес-приложениях. Apache Beam используется для разделения задачи на небольшие пакеты, которые можно обрабатывать параллельно. Один из основных вариантов применения серви­ са заключается в операциях извлечения, преобразования и загрузки (Extract, Trans­ f orm, Load -ETL). Три шага ETL лежат в основе конвейера, когда необходимо преобразовать необработанные данные в полезную информацию.\n--- Страница 284 ---\n288 Раздел 3. Масштабирование за пределы одного потока Ключевые концепции и компоненты описаны ниже: ♦Конвейер -это схема преобразования данных из одной формы в другую в рамках процесса обработки. ♦PColJection (Parallel Collection) -аналог RDD из Apache Spark; это распреде­ ленный набор данных, который содержит неизменяемую и неупорядоченную группу элементов; размер набора может быть фиксированным (ограниченным), подобно пакетной обработке, когда мы знаем, сколько заданий следует обраба­ тывать в одном пакете; размер также может быть гибким (неограниченным) в зависимости от постоянно меняющегося потокового источника данных. ♦PTransform -операции по преобразованию данных, определенные в конвейе­ ре; они выполняются над объектами Pcollection. ♦SDK-набор инструментов разработки ПО для конкретного языка (доступен для Java, Python и Go), с помощью которого мы создаем конвейеры и отправля­ ем их исполнителю (Runner) для выполнения. ♦Runner (исполнитель) -платформа выполнения для конвейеров Apache Beam; исполнитель обязан реализовать единственный метод -run (Pipeline), который по умолчанию является асинхронным; несколько доступных исполнителей - Apache Flink, Apache Spark и Google Cloud Dataflow. ♦Пользовательские функции (user-defined functions, UDF): Apache Beam пред­ лагает несколько типов; наиболее часто используемой является DoFn, которая ра­ ботает поэлементно; предоставленная реализация DoFn обернута в объект ParDo, предназначенный для параллельного выполнения. Простой конвейер выглядит следующим образом (рис. 9.5): 8--• P-Collection• P-Collection Рис. 9.5. Конвейер с тремя операциями PTransform При проектировании конвейера обычно необходимо учитывать три элемента: 1.Сначала нужно понять источник данных: файл, база данных или поток. В зави­ симости от ответа мы определим, какой тип операции Чтение преобразования (Read transform) нужно реализовать. Нам также необходимо понимать формат и структуру данных: как часть операции чтения или как отдельную операцию. 2.Следующий шаг -определить и спроектировать, что делать с данными. Это наша основная операция Преобразования (Transform). Их может быть несколь­ ко последовательно или параллельно с одними и теми же данными. Apache Beam SDK предоставляет несколько предварительно созданных преобразований, ко­ торые можно использовать. Это также позволяет нам писать свои преобразова­ ния с использованием функций ParDo/DoFn.\n--- Страница 285 ---\nГлава 9. Программирование на Python для облака 289 3.Наконец, нужно определить, каким будет вывод конвейера и где хранить его ре­ зультаты. На схеме выше этот шаг показан как операция Запись преобразования (Write transform). Мы рассмотрели структуру простого конвейера для разъяснения концепций, свя­ занных с Apache Beam. На практике конвейеры могут быть относительно сложны­ ми и включать несколько источников данных и выводов. Операции PTransform мо­ гут привести к созданию нескольких объектов PCollection, тогда возникнет необхо­ димость выполнять их параллельно. Далее мы изучим, как создать новый конвейер и выполнить его с помощью испол- нителей Apache Beam или Cloud Dataflow. Конвейеры Apache Beam Как мы уже обсуждали, конвейер -это набор действий или операций для дости­ жения определенных целей по обработке информации. Для этого требуется источ­ ник входных данных, например, память, локальные/облачные файлы или потоко­ вые данные. Псевдокод типичного конвейера будет выглядеть следующим образом: [Final PColletcion] = ( [Initial Input PCollection] 1 [First PTransform] 1 [Second PTransform] 1 [Third PTransform]) Initial Input PCollection используется как входные данные для операций PTransform. Выходная PCollection операции First PTransform будет использоваться как входные данные для Second PTransform и т. д. Итоговый вывод PCollection из по­ следней операции PTransform будет записан как Final PCollection и использован для экспорта результатов в место назначения. Для демонстрации создадим несколько конвейеров разного уровня сложности. Эти примеры предназначены показать роли различных компонентов и библиотек Apache, которые используются при создании и выполнении конвейера. В конце мы создадим конвейер приложения по подсчету слов, которое также упоминается в документации по Apache Beam и GCP Dataflow. Важно отметить, что мы должны установить библиотеку Python apache-beam с помощью утилиты pip для всех приме­ ров кода в этой главе. Пример 1: Создание конвейера со строковыми ·данными в оперативной памяти В этом примере мы создадим входной объект PCollection из коллекции строк в опе­ ративной памяти, применим пару операций преобразования, а затем выведем ре­ зультаты на консоль. Ниже приведен полный код примера: #pipelinel.py: отдельные строки из PCollection import apache_beam as beam with Ьeam.Pipeline() as pipeline: subjects = (\n--- Страница 286 ---\n290 Раздел 3. Масштабирование за пределы одного потока pipeline 'Subjects' >> Ьeam.Create(['English мaths Science French Arts',]) 'Split suЬjects' >> Ьeam.Flatм&p(str.split) Ьеаm.мар (print)) На что нужно обратить внимание: ♦Мы использовали символ « 1 » для записи различных операций PTransform в кон­ вейере; это перегруженный оператор, который больше похож на применение PTransform к PCollection для создания друrого объекта Pcollection. ♦Мы использовали оператор «»» для именования всех PTransform в целях логи­ рования и отслеживания; строка между « 1 » и «»» используется для отображе­ ния и логирования. ♦Мы использовали три операции преобразования; все они являются частью биб­ лиотеки Apache Beam: •Первая создает объект PCollection, который представляет собой строку, со­ держащую пять названий учебных предметов. •Вторая разделяет строковые данные и создает новую PCollection с помощью встроенного метода split объекта String. •Третья выводит каждую запись PCollection на консоль. Вывод консоли будет содержать список предметов, по одному в строке. Пример 2: Создание и обработка конвейера с кортежем данных в оперативной памяти В этом примере создадим PCollection из кортежей. Каждый кортеж будет содержать название учебного предмета и связанную с ним оценку. Основная операция PTransform в этом конвейере заключается в отделении предмета и оценки от дан­ ных. Пример кода: #pipeline2.py: отделение предметов с оценками от PCollection import apache_beam as beam def my_format(sub, marks): yield '{}\\t{}'.format(suЬ,marks) with beam.Pipeline() as pipeline: plants = ( pipeline 1 'Subjects' >> Ьeam.Create([ ('English', 'А'), ('Maths', 'В+'), ('Science', 'А-'), ( 'French' , 'А' ) ;\n--- Страница 287 ---\nГлава 9. Программирование на Python для облака ( 'Arts ' , 'А+' ) , ]) 'Format suЬjects with marks' » Ьeam.FlatмapТUple (my_format) 1 Ьеаm.Мар (print)) 291 В отличие от первого примера здесь использована операция преобразования FlatMapTuple с пользовательской функцией для форматирования данных кортежа. Вывод консоли покажет название каждого предмета с оценкой в отдельной строке. Пример 3: Создание конвейера из данных текстового файла В первых двух примерах мы реализовали простой конвейер для анализа строковых данных из большой строки и кортежей из PCollection. На практике же мы чаще ра­ ботаем с большим объемом информации, которая либо загружается из файла или хранилища, либо поступает в потоковом режиме. В этом примере будем использо­ вать данные из локального текстового файла в исходном объекте PCollection, а за­ тем выведем результаты в выходной файл. Полный пример будет выглядеть сле­ дующим образом: #pipelineЗ.py: чтение данных из файла и запись результатов в другой файл import apache_Ьeam as beam from apache_beam.io import WriteToText, ReadFromText with beam.Pipeline() as pipeline: lines = pipeline I ReadFranТext('samplel.txt') suЬjects = ( lines 1 'SuЬjects' » Ьeam.Flatмap(str.split)) suЬjects I WriteToText(fileyathyrefix='subjects', file _ name _ suffix=' . txt' , shard_name_template=' ') Мы применили операцию PTransform для чтения текстовых данных из файла перед запуском любых PTransform, связанных с непосредственной обработкой. В конце применили PTransform для записи данных в выходной файл. В примере мы исполь­ зовали две новые функции -ReadFromText и WriteToText: 1.ReadFromText: входит в модуль Apache Beam I/O и используется для чтения из тек­ стовых файлов в PCollection, состоящую из строк. Путь к файлу или шаблон файла могут быть предоставлены в качестве входного аргумента для чтения по локальному пути. Также можно использовать gs: / / для доступа к любому файлу в хранилище GCS. 2.Writeтoтext: записывает PCollection в текстовый файл. Для этого, как минимум, требуется указать аргумент f ile _ра th _prefix. Также можно указать аргумент file_path_suffix и, таким образом, задать расширение файла. Для\n--- Страница 288 ---\n292 Раздел 3. Масштабирование за пределы одного потока shard _ narne _ template задано пустое значение, это позволит создать файл с именем, используя аргументы префикса и суффикса. Apache Beam поддерживает шаблон имени шарда (shard) для определения имени файла на основе шаблона. Когда конвейер выполняется локально, создается файл subjects. txt с названиями учебных предметов в соответствии с операцией PTransfoпn. Пример 4: Создание конвейера для исполнителя Apache Beam с арrументами Итак, мы научились создавать простой конвейер, формировать объект PCollection из текстового файла и записывать результаты в новый файл. В дополнение к этим основным шагам нам нужно сделать еще несколько действий. После этого можно будет убедиться, что программа-драйвер готова отправить задание исполнителю GCP Dataflow или любому другому облачному исполнителю: ♦ В предыдущем примере мы указали имя входного и шаблон выходного файлов, заданные в программе-драйвере; на практике такие параметры передаются через аргументы командной строки; для парсинга (Parsing) и контроля аргументов командной строки мы будем использовать библиотеку argparse. ♦Добавим дополнительные аргументы, например, для установки исполнителя; он будет устанавливать целевой исполнитель конвейера с помощью исполнителей DirectRunner или GCP Dataflow; обратите внимание, DirectRunner -это конвей­ ерная среда выполнения для локального компьютера; она гарантирует, что кон­ вейеры максимально соответствуют модели Apache Beam. ♦Мы также реализуем функцию ParDo, которая будет использовать пользователь­ скую функцию для парсинга строк из текстовых данных; этого можно добиться, используя функции string, но здесь они были добавлены для демонстрации, как использовать ParDo и DoFn с PTransfoпn. Наши шаги будут следующими: 1.Сначала создадим парсер (Parser) аргументов и определим, какие из них мы ожидаем от командной строки. У становим для них значения по умолчанию и за­ дадим дополнительный текст справки, которая будет отображаться по ключево­ му слову help. Атрибут dest важен, поскольку он используется для идентифика­ ции аргументов, которые будут использоваться в операторах. Также определим функцию ParDo, которая будет использоваться для выполнения конвейера. При­ мер кода представлен ниже: #pipeline4.py(чacть 1): использование аргумента для конвейера import re, argparse, apache_bearn as beam from apache_bearn.io import WriteToText, ReadFromText from apache_bearn.options.pipeline_options import PipelineOptions class WordParsingDoFn(Ьeam.DoFn): def process(self, element): return re.findall (r' [\\w\\' ]+', element,_ re.UNICODE)\n--- Страница 289 ---\nГлава 9. Программирование на Python для облака def run(argv=None): parser = argparse ,Arguшentparser () parser.add_argument( '--input'' dest=' input' , default='sarnplel.txt', help=' Input file to process. ') parser.add_argument( '--output' , dest= 'output', default='suЬjects', help='Output file to write results to.') parser.add_argument( '--extension' , dest= 'ext' , default=' .txt', help='Output file extension to use.') known_args, pipeline_args = parser.parse_known_args(argv) 293 2.Теперь зададим DirectRunner в качестве среды выполнения конвейера и обозна­ чим задание, которое нужно вьmолнить. Пример кода для этого шага выглядит так: lpipeline4 . 'РУ (part 2) : с lеl'ОДОМ run pipeline_args.extend([ '--runner=DirectRunner' , '--joЬ_nalllFd8Do-local-joЬ', ]) pipeline_options = PipelineOptions(pipeline_args) 3.Наконец, реализуем конвейер с помощью объекта pipeline _ options, который мы создали на предыдущем шаге. Конвейер будет считывать данные из входного текстового файла, преобразовывать их в соответствии с функцией ParDo, а затем сохранять результаты в качестве вывода: lpipeline4. 'РУ (part З) : с юдом run with bearn.Pipeline(options=pipeline_options) as pipeline: lines = pipeline ReadFranТext(known_args.input) suЬjects = ( lines 1 'SuЬjects' >> bearn.ParDo(WordParsingDoFn()). with_output_types(str)) subjects I WriteToText(known_args.output, known_args.ext)\n--- Страница 290 ---\n294 Раздел 3. Масштабирование за пределы одного потока 4.Когда мы выполняем эту программу напрямую через IDE, используя значения по умолчанию для аргумента, или инициируем ее из интерфейса командной строки, используя следующую команду, мы получим тот же результат: python pipeline4.py --input sarrplel.txt --output myoutput --extension .txt 5.Здесь происходит парсинг слов из входного текстового файла (samplel. txt). За- тем они записываются по одному в каждую строку выходного файла. Apache Beam -обширная тема, поэтому невозможно охватить все его возможно­ сти в пределах одной главы. Тем не менее мы рассмотрели основы с примерами, позволяющие начать писать простые конвейеры, которые можно развернуть в GCP Cloud Dataflow. Это станет темой следующего подраздела. Создание конвейеров для Cloud Dataflow Примеры, которые обсуждалимь до сих пор, были сосредоточены на создании про­ стых конвейеров и их выполнении с помощью DirectRunner. В этом подразделе мы создадим программу-драйвер и развернем конвейер для подсчета слов в Google Cloud Dataflow. Эта программа важна, поскольку мы задаем все параметры, связан­ ные с облаком, внутри нее. Благодаря этому не придется использовать Cloud SDK или Cloud Shell для выполнения дополнительных команд. Конвейер подсчета слов будет представлять собой расширенную версию примера pipeline4 .ру. Дополнительные компоненты и шаги для развертывания конвейера приведены далее: 1. Создадим новый облачный проект GCP, используя шаги, аналогичные тем, что мы выполняли в приложении веб-сервиса для развертывания Арр Engine. Для этой задачи можно использовать Cloud SDK, Cloud Shell или консоль GCP. 2.Включим Dataflow Engine API для нового проекта. 3.Затем создадим сегмент для хранения входного и выходного файлов, а также предоставим временные и промежуточные каталоги для Cloud Dataflow. Для этой задачи тоже можно использовать консоль GCP, Cloud Shell или Cloud SDK. В Cloud Shell или Cloud SDK сегмент можно создать следующей командой: gsutil mЬ gs://<bucket name> 4.Может потребоваться связать аккаунт сервиса с созданным сегментом, если он не находится в том же проекте, что и задача конвейера. В этом случае также нужно будет выбрать роль администратора объекта хранилища для контроля доступа. 5.Нужно установить Apache Beam с необходимыми библиотеками gcp с помощью утилиты pip: pip install apache-beam[gcp] 6.Необходимо создать ключ аутентификации для аккаунта сервиса, который ис­ пользуется для облачного проекта GCP. Этого не потребуется, если мы будем выполнять программу-драйвер на платформе GCP, например, Cloud Shell. Ключ\n--- Страница 291 ---\nГлава 9. Программирование на Python для облака 295 аккаунта должен быть загружен на локальной машине. Сделать ключ доступным для Apache Beam SDK можно через указание пути к файлу ключа ( файл JSON) в переменной окружения GOOGLE -APPLICATION -CREDENTIALS. Перед обсуждением, как выполнить конвейер в Cloud Dataflow, кратко рассмотрим пример программы-драйвера для подсчета слов. В этой программе мы определим аргументы командной строки, похожие на те, что мы использовали в предыдущем примере (pipeline4. ру ), но с некоторыми отличиями: ♦Зададим переменную окружения GOOGLE_APPLICATION_CREDENTIALS не через операци­ онную систему, а через программу-драйвер (для упрощения). ♦Загрузим файл sample. txt в хранилище Google, в нашем случае это каталог gs//muasif/input; этот путь будет использоваться в качестве значения по умолча­ нию для аргумента input. Полный пример кода выглядит так: #wordcount.py(чacть 1): подсчет слов в текстовом файле import argparse, os, re, apache_beam as beam from apache_beam.io import ReadFromText, WriteToText from apache_beam.options.pi peline_o ptions import PipelineOptions from apache_beam.o ptions. pipeline_o ptions import SetupOptions def run(ar gv=None, save_main_session =True): os. environ [ \"GOOGLE _ APPLICATION _ CREDENTIALS\"] parser = argparse.Argumentparser() parser.add_argument( '--input', dest=' input' , default ='gs://muasif/in put/sam ple.txt', help='Input file to process. ') parser.add_argument( '--output', dest=' output', default=' qs://muasif/input/result', help='Output file to write results to. ') \"some folder/key.json\" known_ar gs, pipeline_ar gs = parser. parse_known_ar gs(argv) Далее зададим расширенные аргументы для параметров конвейера, таким образом, он будет работать в среде выполнения Cloud Dataflow. Эти аргументы приведены далее: ♦Платформа среды выполнения (исполнитель) для работы конвейера (в данном случае DataflowRunner). ♦. Идентификатор GCP Cloud Project. ♦Регион GCP.\n--- Страница 292 ---\n296 Раздел 3. Масштабирование за пределы одного потока ♦Путь к сегмеmам хранилища Google для хранения входного, выходного и вре- менных файлов. ♦Имя задания для отслеживания. На основе этих аргументов создадим объект параметров конвейера, который будет использоваться при выполнении. Фрагмент кода для этих задач будет выглядеть следующим образом: #wordcount.py (часть 2): с методом run pipeline_args.extend([ '--runner=DataflowRunner', '--project=word-count-316612', '--region=us-centrall', '--staging_location=gs://muasif/staqinq', '--temp_location=gs://mu&sif/teпp', '--job_name=my-wordcount-job', ] ) pipeline_options = PipelineOptions(pipeline_args) pipeline_options.view_as(SetupOptions) .save_main_session save main session Наконец, реализуем конвейер с уже определенными параметрами и добавим наши операции PTransform. В этом примере мы реализовали дополнительную операцию PTransform для добавления в пару каждому слову единицы (1). В еще одной опера­ ции PTransf orm мы сгруппируем пары и применим операцию sum для подсчета их частоты. Так мы узнаем количество каждого слова в текстовом файле: #wordcount.py (часть 3): с методом run with beam.Pipeline(options=pipeline_options) as р: lines = р I ReadFranТext(known_args.input) #Считаем количеств вхождений каждого слова counts = ( lines 'Split words' >> ( beam.Flatмap( lamЬda х: re.findall(r'[A-Za-z\\']+', х)). with_output_types(str)) 'Pair with 1' » beam.мap(lamЬda х: (х, 1)) 'Group & Sum' » beam.CanЬinePerI<ey(sum)) def foDDat_result(word_count): (word, count) = word_count return '%s: %s' % (word, count) output counts 1 'Format' >> beam.М&p(format_result) output WriteToText(known_args.output)\n--- Страница 293 ---\nГлава 9. Программирование на Python для облака 297 Мы задаем значения по умолчанию для каждого аргумента в программе-драйвере. Это означает, что можно вьmолнить программу напрямую командой python wordcount.py или с помощью другой команды для передачи аргументов через CLI: python wordcount.py \\ --project word-count-316612 \\ --region us-centrall \\ --input qs://muasif/input/sёlJli>le.txt \\ --output qs://muasif/output/results \\ --runner DataflowRunner \\ --tenp_location qs://muasif/tsmp \\ --staqinq_location qs://muasif/staqinq Выходной файл содержит результаты и количество каждого слова. GCP Cloud Dataflow предоставляет дополнительные инструменты для мониторинга хода вы­ полнения заданий, а также для оценки потребления ресурсов. На следующем скриншоте консоли GCP отображен список заданий, отправленных в Cloud Dataflow. В сводке отображаются их статусы и несколько ключевых метрик (рис. 9.6): JоЬа a-.--1ВIF\\A1E llcмAТI .--IQI. •INAIU� C-»t ·- '--\"8Alnllno ;;;- Fll!«joЬo • • ·- 1)11о .,._ ,._ ----- 1D ----- -.,.,, - 1-4М2021, 5-14 1-4М2021. - 2,30.0 2021-06-1<L1о ;ц.27- - 21:39-.43 - 21:34:29 17633507493-411!16047 c,mn,11 - 811W-·- 14М1.о11. $_,_ 1◄Jw>2021. - 1.30.О 2021-\\)6•1<1 10.д.◄t- � 21� 21:2$.01 927632$1 $2\"433$464 \"\"11!о11 •·- ,_ 1◄Jw>21121, 4-,U 14 Jw> 2021, ,_ 1 30.0 2021-06-14_�_51). _ ,. 13:5&'24 - 13:51:51 735$1091587,19375121 -1 Рис. 9.6. Сводка по заданиям Cloud Dataflow Можно перейти к подробной информации о любом задании (кликнув на его имя), как показано на следующем скриншоте. С правой стороны показаны сведения о задании и окружении, а также ход выполнения различных операций PTransf orm, которые мы определили для нашего конвейера. Пока задание выполняется, статус каждой операции PTransform обновляется в реальном времени, как показано на рис. 9.7. Важно отметить, что операции PTransform называются в соответствии со строками, которые использованы с оператором «»». Это помогает визуально выделить опе­ рации. На этом можно завершить обсуждение, как создавать и развертывать конвейер для Google Dataflow. По сравнению с Apache Spark решение Apache Beam отличается большей гибкостью. Благодаря возможностям облачной обработки можно полно­ стью сосредоточиться на моделировании конвейеров и не думать об их выполнении. Как уже упоминалось, Amazon предлагает аналогичный сервис (AWS Кinesis) для развертывания и вьmолнения конвейеров, но он больше ориентирован на потоковые\n--- Страница 294 ---\n298 Раздел 3. Масштабирование за пределы одного потока данные в реальном времени. Как и А WS Beanstalk, А WS Кinesis не требует предва­ рительной подготовки проекта. Руководство пользователя по обработке данных с помощью А WS Кщ.esis доступны по адресу: https://docs.aws.amazon.com/kinesis/. �my-wordcount1ob с,-) SНARE МАХ. TIME v Job info >1 JOBGRAPH EXECUТION DEТAILS JOBMEТRICS JoЬnome my-wordcountiob CL.EAR SELECTION JoЬIO 2021-()6-14_ 10_34 27-о Rмc!FtomText V 17633507493411316047 Succeeded 1 IIC JoЬtype Batch 1 of 1 stagt auocteded JоЬ stltut О Succeeded SDКveralon Apache Веаm Python 3.8 SDK2.30.0 • Splitworda JoЬrlillOn • us-central1 Succeeded Worker loc1tlon 8 us-central1-c 0sec Cunent workera 8 о 1 of 1 stage auoceecled Latиt work« llltut Wшer pool stopped. Swttlme 14 June 2021 at 21:34:29 GМТ+4 о P1lrwtth 1 Elapмd tlme 5min 15sec Succeeded Encryptlon type Google-managed key О sec 1 of 1 stagt auocteded Resource metrics л • Group&Sum V Curтent vCPUs 8 Succeeded Total VCPU tlme 8 0.06vCPVlv 0sec Cunent memo,y 8 3.75GB 2 of 2 stages suocteded Total merno,y tlm• О 0.224GBhr Currlnl HDD РО 8 25GB о Format TolllHDDPDtlm18 1.496 GB hr Succeeded Curтent SSO РО 8 08 О sec 1 ol 1 stage suoceeded Total SSD РО tlme 8 OGBhr Total Shum. cllta 15.981(8 PRIC81Нd 8 о Вi111Ые Shufllt cllta 4КВ WrhtToTtxt V pRIC8sstd. Succeeded 6sec 5 of 5 stages &uoceeded Рис. 9.7. Подробная информация о задании Cloud Dataflow со схемой и метриками Заключение В этой главе мы обсудили роль Python в разработке приложений для облачного развертывания в целом, а также использование Apache Beam для развертывания конвейеров обработки данных в Google Cloud Dataflow в частности. Мы начали со\n--- Страница 295 ---\nГлава 9. Программирование на Python для облака 299 знакомства с тремя крупными поставщиками облачных услуг и с продуктами раз­ работки, которые. они предлагают. Также познакомились с доступными вариантами сред выполнения каждого из них и сравнили их. Узнали, что существуют отдель­ ные среды выполнения для классических веб-приложений, контейнерных приложе­ ний и бессерверных функций. На практике попытались изучить эффективность Python для облачных веб-приложений, создав пример и развернув его в Google Арр Engine с помощью Cloud SDК. В конце главы мы расширили свое понимание про­ цесса обработки данных, начатое в предыдущей главе. Познакомились с конвейе­ рами Apache Beam. На примерах научились создавать их для Cloud Dataflow. Примеры кода в этой главе помогут вам приступить к созданию облачных проектов и написанию кода для Apache Beam. Эти знания понадобятся всем, кто хочет ре­ шать задачи обработки больших данных с помощью облачных сервисов. В следующей главе мы рассмотрим возможности Python для разработки веб­ приложений с использованием фреймворков Flask и Django. Вопросы 1.Чем А WS Beanstalk отличается от А WS Арр Runner? 2.Что делает сервис GCP Cloud Function? 3.Какие сервисы предлагает GCP для обработки данных? 4.Что такое конвейер Apache Beam? 5.Зачем нужен объект PCollection в конвейере обработки данных? Дополнительные ресурсы ♦«Разработка веб-приложений с использованием Flask на языке Python» (Flask Web Development), автор: Мигель Гринберг (Miguel Grinberg). ♦«Advanced Guide to Python 3 Programming», автор: Джон Хант (John Hunt). ♦«Apache Веат: А Complete Guide», автор: Герардус Блокдик (Gerardus Вlokdyk). ♦«Google Cloud Platform for Developers», авторы: Тэд Хантер (Ted Hunter) и Сти­ вен Портер (Steven Porter). ♦Документация по Google Cloud Dataflow: https:l/cloud.google. com/datajlow/docs. ♦Документация по AWS Elastic Beanstalk: https://docs.aws. amazon.com/elastic­ beanstalk. ♦Документация по Службе приложений Azure: https://docs.microsoft. com/en­ us/azure/app-service/. ♦Документация по А WS Kinesis: https://docs.aws.amazon.com/ kinesis/.\n--- Страница 296 ---\n300 Раздел 3. Масштабирование за пределы одного потока Ответы 1.А WS Beanstalk -это универсальное решение PaaS для развертывания веб­ приложений, а А WS Арр Runner -полностью управляемый сервис для развер­ тывания веб-приложений на основе контейнеров. 2.GCP Cloud Function -это бессерверный сервис, управляемый событиями, для выполнения программы. Событие также может поступать от другого сервиса GCP или НТТР-запроса. 3.Cloud Dataflow и Cloud Dataproc -два популярных сервиса обработки данных отGСР. 4.Конвейер Apache Beam -это набор действий, определенных для загрузки, пре­ образования и записи данных. 5.PCollection похож на RDD в Apache Spark и содержит элементы данных. При обработке данных в конвейере операция PTransform принимает один или не­ сколько объектов PCollection в качестве входных данных и выдает результаты в виде одного или нескольких объектов PCollection.\n--- Страница 297 ---\nРаздел 4 Python для веб-разработки, облака и сети Еще рано заканчивать наше путешествие, пока мы на практике не применили полу­ ченные знания. Это главная часть книги, где мы бросим вызов нашему прогрессу в обучении, решая реальные задачи. Во-первых, рассмотрим, как создавать веб­ приложения и интерфейсы REST API с использованием фреймворка Flask. Во­ вторых, подробно изучим реализацию бессерверных облачных приложений, ис­ пользуя архитектуру микросервисов и бессерверных функций. Коснемся как об­ лачных, так и локальных вариантов развертывания. В конце рассмотрим использо­ вание Python для создания моделей машиююго обучения (Machine Learning) и раз­ вертывания их в облаке. И завершим обсуждение, исследуя роль Python в автоматизации сети с реальными примерами. Раздел состоит из следующих глав: ♦«",
      "debug": {
        "start_page": 269,
        "end_page": 297
      }
    },
    {
      "name": "Глава 10. Использование Python для разработки веб-приложений и REST API 303",
      "content": "--- Страница 297 --- (продолжение)\nГлава 10: Использование Python для разработки веб-приложений и REST API». ♦«Глава 11: Использование Python для разработки микросервисов». ♦«Глава 12: Создание бессерверных функций с помощью Python». ♦«Глава 13: Python и машинное обучение». ♦«Глава 14: Использование Python для автоматизации сетей».\nГлава 10: Использование Python для разработки веб-приложений и REST API». ♦«Глава 11: Использование Python для разработки микросервисов». ♦«Глава 12: Создание бессерверных функций с помощью Python». ♦«Глава 13: Python и машинное обучение». ♦«Глава 14: Использование Python для автоматизации сетей».\n--- Страница 298 ---\n10 Использование Python для разработки веб-приложений и REST API Веб-приложение -это программа, которая размещается и выполняется на сервере в локальной сети или Интернете и доступна через браузер на устройстве пользова­ теля. Это позволяет получить доступ к приложению из любого места без установки дополнительного программного обеспечения на локальный компьютер. Благодаря такой простоте веб-приложения популярны уже более двух десятилетий. Они име­ ют широкий диапазон применения, будь то загрузка статического/динамического контента, интернет-магазины, онлайн-игры, социальные сети, обучение, поставка мультимедийного контента, опросы, блоги и даже сложные приложения для плани­ рования ресурсов предприятия. Веб-приложения по своей природе являются многоуровневыми (в основном трех­ уровневыми): пользовательский интерфейс, бизнес-логика и доступ к базе данных. Таким образом, они взаимодействует с веб-серверами (пользовательский интер­ фейс), сервером приложений (бизнес-логика) и системами управления базами дан­ ных (доступ к БД). С развитием мобильных технологий интерфейс стал чаще пред­ ставлять собой мобильное приложение, которому требуется доступ к уровню биз­ нес-логики через REST API, наличие которого (или других интерфейсов) считается фундаментальным требованием для веб-приложений. В этой главе мы обсудим, как использовать Python для написания многоуровневых веб-приложений. Для этого существует несколько фреймворков, но в рамках этой главы мы выбрали Flask за его функциональность и легкость. Важно понимать, что не стоит путать веб-приложения с мобильными приложениями для небольших устройств. Темы этой главы: ♦Требования к веб-разработке. ♦ Знакомство с фреймворком Flask. ♦Взаимодействие с базами данных с помощью Python.\n--- Страница 299 ---\n304 Раздел 4. Python для веб-разработки, облака и сети ♦Создание REST API с помощью Python. ♦Пример создания веб-приложения с помощью REST API. К концу главы вы научитесь использовать Flask для разработки веб-приложений, взаимодействия с БД и создания REST API и веб-сервисов. Технические требования Для этой главы понадобится: ♦Python 3.7 или более поздней версии. ♦Библиотека Python Flask 2.х с расширениями. Пример кода к этой главе можно найти по адресу: https ://github. com/PacktPuЬ/ishing/Python-for-Geeks/tree/master/Chapter 1 О. Начнем обсуждение с ключевых требований к разработке веб-приложений и REST API. Требования к веб-разработке Разработка веб-приложения включает в себя создание пользовательского интер­ фейса (User Interface, UI), маршрутизацию пользовательских запросов или дейст­ вий к конечным точкам приложения, преобразование входных данных, написание бизнес-логики для запросов, взаимодействие с уровнем данных для чтения/записи информации и предоставление результатов пользователю. Для реализации всех этих компонентов могут потребоваться разные платформы и даже разные языки программирования. В этом подразделе мы рассмотрим, какие компоненты и инст­ рументы требуются для веб-разработки, начиная с веб-фреймворков. Веб-фреймворки Разработка с нуля утомительна. Сделать ее удобной для программистов позволили фреймворки, существовавшие уже на ранних этапах зарождения веб-разработки. Они предоставляют набор библиотек, структуры каталогов, многоразовые компо­ ненты и инструменты развертывания. Веб-фреймворки обычно следуют архитекту­ ре, которая позволяет разработчикам создать сложное приложение за меньшее вре­ мя и оптимальным способом. Для Python существует несколько веб-фреймворков, из которых наиболее популяр­ ны Flask и Django. Оба предоставляются бесплатно и с открытым исходным кодом. Flask -легкий фреймворк со стандартными возможностями для создания веб-· приложений, также имеет возможность использования дополнительных библиотек и расширений. Django -это фреймворк с полным стеком, который уже содержит\n--- Страница 300 ---\nГлава 1 О. Использование Python для разработки веб-приложений и REST API 305 все необходимое «из коробки» и не требует дополнительных библиотек. Оба вари­ анта имеют свои плюсы и минусы, но в конечном итоге приложение может быть полностью разработано с использованием любого из них. Flask является лучшим выбором, когда мы хотим полностью контролировать при­ ложение с возможностью использования сторонних библиотек по мере необходи­ мости. Также это хороший вариант, если требования проекта часто меняются. Django полезен в ситуациях, когда нужны сразу все инструменты и библиотеки, и вы хотите сосредоточиться только на реализации бизнес-логики. Он хорошо подхо­ дит для масштабных проектов, но для небольших он может оказаться излишним. Использование Django требует предварительного изучения и опыта в веб­ разработке. Если вы только начинаете в этой области, присмотритесь к Flask. А по­ сле того как освоитесь с ним, можете легко перейти на Django для создания более сложных проектов. При создании веб-или любых других приложений с пользовательским интерфей­ сом мы часто сталкиваемся с понятием «модель-представление-контроллер» (Model View Controller, MVC). Это шаблон проектирования, который разделяет приложение на три уровня: ♦Модель: этот уровень представляет собой данные, которые обычно хранятся вБД. ♦Представление: этот уровень представляет собой пользовательский интерфейс, с которым взаимодействует пользователь. ♦Контроллер: предназначен для обеспечения логики взаимодействия пользова­ теля с приложением через пользовательский интерфейс; например, может воз­ никнуть необходимость создать новый объект или обновить уже существую­ щий; на уровне контроллера реализуется логика того, какой интерфейс (пред­ ставление) дать пользователю для создания или обновления объекта с моделью (данными) или без нее. Flask не обеспечивает прямой подцержки шаблонов проектирования MVC, но их можно реализовать с помощью кода. Django обеспечивает достаточно близкую реа­ лизацию MVC, но не полностью. Контроллер в Django управляется самим Django и недоступен для написания в нем собственного кода. Он и многие другие веб­ фреймворки Python следуют шаблону проектирования «модель-представле11ие­ шаблон» (Model View Template, МVТ). Он похож на МVС, за исключением уровня шаблона, который предоставляет специально отформатированные паттерны для создания ожидаемого пользовательского интерфейса с возможностью вставки ди­ намического содержимого в HTML. Пользовательский интерфейс UI -это уровень представления приложения, который иногда включается как часть веб-фреймворков. Мы уделяем этому отдельное внимание с целью выделить\n--- Страница 301 ---\n306 Раздел 4. Python для веб-разработки, облака и сети ключевые технологии и варианты, доступные для этого уровня. Прежде всего, пользователь взаимодействует через браузер, используя язык гипертекстовой раз­ метки (HyperText Markup Language, HTML) и каскадную таблицу стилей (Cas­ cading Style Sheets, CSS). Можно создавать интерфейсы напрямую на НТМL и CSS, но это довольно утомительно и не подходит для быстрой доставки динамиче­ ского контента. Существует несколько технологий, упрощающих создание UI: 1.UI-фреймворки: в основном это библиотеки HTML и CSS, которые предостав­ ляют различные классы (стили) для создания пользовательского интерфейса. В них по-прежнему есть необходимость писать или генерировать ключевые НТМL-части интерфейса, но без необходимости беспокоиться о внешнем виде страниц. Популярным примером является Bootstrap, построенный поверх CSS. Он был создан Twitter для собственного использования, но позже его исходный код был открыт для всех желающих. Еще один популярный вариант -ReactJS от Facebook, но это скорее библиотека, чем фреймворк. 2.Шаблонизатор (Template engine): еще один популярный механизм для дина­ мического создания контента. Шаблон похож на определение желаемого резуль­ тата, который содержит как статические данные, так и заглушки для динамиче­ ского контента, которые представляют собой маркированные строки, заменяе­ мые реальными значениями во время выполнения. Выходные данные могут иметь любой формат: HTML, XML, JSON или PDF. Один из самых популярных шаблонизаторов Python -Jinja2. Он также входит в состав фреймворка Flask. Django имеет собственный шаблонизатор. 3.Скрипты на стороне клиента: это программы, которые загружаются с веб­ сервера и выполняются браузером. JavaScript -самый популярный язык для создания клиентских скриптов. Для него существует множество библиотек, уп­ рощающих жизнь программистам. Для разработки веб-интерфейсов можно использовать несколько технологий. В стандартном проекте все три варианта используются в разной степени. Веб-сервер/сервер приложений Веб-сервер -это программное обеспечение, которое прослушивает клиентские запросы через НТТР и предоставляет контент (веб-страницы, скрипты, изображе­ ния) в соответствии с типом запроса. Основная задача его -обслуживать только статические ресурсы. Он не может выполнять код. Сервер приложений больше подходит для языка программирования. Его главная задача -предоставить доступ к реализации бизнес-логики, написанной с помощью языка программирования. В большинстве производственных сред веб-сервер и сер­ вер приложений объединены в одно ПО для простоты развертывания. Flask имеет собственный веб-сервер Werkzeug для этапа проектирования, но в производствен­ ной среде его не рекомендуется использовать. Для этого есть более подходящие варианты: Gunicom, uWSGI и движки от GCP.\n--- Страница 302 ---\nГлава 10. Использование Python для разработки веб-приложений и REST API 307 База данных Это не обязательный компонент, но практически необходимый для любого инте­ рактивного веб-приложения. Python предлагает несколько библиотек для доступа к распространенным системам управления базами данных (СУБД), например, MySQL, MariaDB, PostgreSQL и Oracle. Язык также оснащен облегченным серве­ ром БД -SQLite. Безопасность Безопасность имеет основополагающее значение, главным образом потому, что це­ левой аудиторией обычно являются пользователи Интернета, а конфиденциаль­ ность данных -главное требование в такой области. Протоколы SSL (Secure Sockets Layer) и недавно представленный TLS (Transport Layer Security) являют­ ся минимальными стандартами безопасности для защиты передачи данных между клиентами и сервером. Требования безопасности на транспортном уровне обычно выполняются веб-сервером, а иногда и прокси-сервером. Безопасность на уровне пользователей -следующее фундаментальное требование безопасности, мини­ мальной необходимостью которого являются имя пользователя и пароль. Безопас­ ность реализуется на уровне приложения, и ответственность за ее реализацию не­ сут разработчики. АР/ Уровень бизнес-логики в веб-приложениях может использоваться дополнительны­ ми клиентами. Например, мобильное приложение может использовать одну и ту же бизнес-логику для похожего набора возможностей. В сфере Business to Business (В2В) удаленное приложение может напрямую отправлять запросы на уровень биз­ нес-логики. Все это возможно, если мы предоставим стандартные интерфейсы, та­ кие, как REST API. Сейчас реализация доступа через API является лучшей практи­ кой, позволяющей сделать API готовым для использования с самого начала. Документация Документация также важна, как и написание программного кода. Когда мы гово­ рим, что у нас есть API для приложения, то его потребители в первую очередь за­ даются вопросом, можем ли мы поделиться документацией по нему. Самый удоб­ ный способ создания АРI-документации -использовать встроенные инструменты, которые поставляются с веб-фреймворками. Swagger -популярный инструмент для автоматического создания документации из комментариев, добавленных при написании кода. Итак, мы обсудили ключевые требования и в следующем подразделе можем углу­ биться в разработку веб-приложения с помощью Flask.\n--- Страница 303 ---\n308 Раздел 4. Python для веб-разработки, облака и сети Знакомство с фреймворком Flask Flask -это микрофреймворк для веб-разработки на Python. Термин «микро» ука­ зывает, что ядро Flask имеет облегченную конструкцию, но с возможностью рас­ ширения. Простой пример -взаимодействие с СУБД. Для сравнения, Django в своем составе имеет библиотеки для взаимодействия с самыми популярными база­ ми данных. Flask, в свою очередь, для достижения тех же целей позволяет исполь­ зовать расширения в зависимости от типа БД или подхода к интеграции. Филосо­ фия Flask заключается в использовании соглашений о конфигурации (Convention over configuration). Это означает, если мы будем следовать стандартным соглаше­ ниям веб-разработки, мы потратим меньше усилий на настройку. Поэтому Flask является лучшим выбором для новичков, изучающих веб-разработку на Python. Он также позволяет пошагово внедрять различные концепции разработки. В этом подразделе мы изучим следующие аспекты: ♦Создание базового веб-приложения с маршрутизацией. ♦Обработка запросов с разными типами НТТР-методов. ♦Отображение статического и динамического контента с помощью Jinja2. ♦Извлечение аргументов из НТТР-запроса. ♦Взаимодействие с СУБД. ♦Обработка ошибок и исключений. Прежде чем перейти к примерам, нужно установить Flask 2.х в виртуальной среде. Начнем с создания базового веб-приложения. Создание базового веб-приложения с маршрутизацией В предыдущей главе ( «Программирование на Python для облака») мы уже исполь­ зовали Flask при создании простого приложения для развертывания в GCP Арр Engine. Освежим наши знания и начнем с понимания, как строится веб-приложение и как в нем работает маршрутизация. Код выглядит следующим образом: #appl.py: маршрутизация в приложении Flask from flask import Flask арр = Flask (_ name _) @app.route ( '/') def hello () : return 'Hello World!' @арр. route ( '/qreetinq' ) def greeting(): return 'Greetings from Flask web арр!' if name app.run () ' main ':\n--- Страница 304 ---\nГлава 10. Использование Python для разработки веб-приложений и REST API 309 Пошагово проанализируем пример: 1.Инициализация: сначала мы должны создать экземпляр приложения (в нашем случае арр). Веб-сервер будет передавать этому экземпляру все запросы от кли­ ентов с помощью протокола WSGI (Web Server Gateway Interface). Экземпляр приложения создается с помощью оператора арр = Flask (_пате_). 2.Важно передать имя модуля в качестве аргумента конструктору Flask. Это по­ зволит узнать о расположении приложения, которое станет отправной точкой для определения расположения других файлов ( статические ресурсы, шаблоны и изображения). Переменная _пате_ соответствует соглашению (о конфигурации) и передается конструктору Flask, а он делает все остальное. 3.Маршрут: как только экземпляр арр получает запрос, он становится ответствен­ ным за выполнение фрагмента кода для обработки запроса. Этот фрагмент кода, который обычно является функцией Python, называется обработчиком. Хорошая новость в том, что каждый запрос обычно (но не всегда) сопоставлен с одним URL-aдpecoм, такая связь называется маршрутом. В примере мы выбрали про­ стой подход к определению этого сопоставления, используя декоратор route. Для примера, URL-aдpec /hello сопоставляется с функцией hello, а /greeting -с функцией greeting. Если есть предпочтение определять все маршруты в одном месте, можно использовать add_url_rule в экземпляре приложения. 4.Функция обработчика: после обработки запроса эта функция должна отпра­ вить ответ клиенту, который может быть простой строкой с HTML (или без не­ го) или же сложной статической или динамической веб-страницей на основе шаблона. Для демонстрации мы вернули простую строку. 5.Веб-сервер: Flask имеет сервер разработки, который можно запустить методом app.run() или с помощью команды flask run в оболочке. При запуске сервер по умолчанию ищет модуль арр. ру или wsgi. ру, и он будет загружен автоматически с сервером, если указать имя арр. ру для файла модуля ( опять же по соглашению о конфигурации). Если назвать модуль по-другому (как в нашем случае), нужно задать переменную среды FLASK _ АРР = <имя модуля>, с помощью которой веб­ сервер загрузит модуль. 6.Если проект Flask создан с помощью IDE, вроде PyChaпn Pro, переменная окру­ жения будет установлена как часть проекта. Если используется оболочка ко­ мандной строки, можно установить переменную окружения следующим выра­ жением в зависимости от операционной системы: export FLASK_APP = appl.py. #для macOS и Linux set FLASK�APP = appl.py. #для MS Windows 7.При запуске сервер прослушивает клиентские запросы по адресу http: / / localhost: 5000/ и по умолчанию доступен только на локальном компьютере. Если необходимо запустить сервер с другим именем хоста и портом, можно вьшол­ нить следующую команду (или эквивалентный оператор Python): Flask run --host <ip_address> --port <port_num>\n--- Страница 305 ---\n310 Раздел 4. Python для веб-разработки, облака и сети 8.Веб-клиент: можно протестировать приложение через браузер, введя URL в ад­ ресную строку или используя утилиту curl для простых НТТР-запросов. В на­ шем случае мы можем сделать это с помощью следующих команд curl: curl -Х GET http://localhost:5000/ curl -Х GET http://localhost:5000/greeting Когда мы рассмотрели основы Flask, можем перейти к изучению тем, связанных с запросами и отправкой динамических ответов клиентам. Обработка запросов с разными типами НТТР-методов НТТР использует модель «запрос-ответ» между клиентом и сервером. Клиент (на­ пример, веб-браузер) может вызывать методы для определения типа запроса к сер­ веру: GET, POST, PUT, DELETE, HEAD, РАТСН или OPTIONS. Методы GET и POST -самые часто используемые, поэтому для демонстрации принципов веб-разработки мы рассмот­ рим подробно только их. Также важно понять два ключевых компонента -это НТТР-запрос и НТТР-ответ. НТТР-запрос состоит из трех частей: ♦Строка запроса: включает используемый НТТР-метод, URI-aдpec запроса и ис­ пользуемый НТТР-протокол (версию): GET /home НТТР/1.1 ♦Поля заголовков: они содержат метаданные с информацией о запросе; каждая запись заголовка получает пару «ключ-значение», разделенные двоеточием (:). ♦Тело (опционально): это заглушка, куда можно добавить дополнительные дан­ ные; для веб-приложения можно отправлять информацию из формы в POST - запросе внутри тела НТТР-запроса; для REST API можно отправлять данные для запросов PUT или юsт внутри тела. При отправке НТТР-запроса веб-серверу в качестве результата мы получаем НТТР­ ответ. Он состоит из таких же частей, что и НТТР-запрос: ♦Строка состояния: указывает на успешное выполнение или ошибку; здесь ука­ зывается код: НТТР/1.1 200 ОК ♦Код состояния в диапазоне 200-299 указывает на успешное выполнение; коды в диапазоне 400-499 указывают на ошибки на стороне клиента, а в диапазоне 500-599 -на ошибки на стороне сервера. ♦Поля заголовков: аналогичны заголовкам НТТР-запроса. ♦Тело (опционально): хоть и не обязательная, но ключевая часть НТТР-ответа; может включать НТМL-страницы для веб-приложений или данные в любом другом формате. GET используется для отправки запроса на определенный ресурс, указанный в URL, с возможностью добавления строки запроса как части адреса. Символ«?» добавля-\n--- Страница 306 ---\nГлава 10. Использование Python для разработки веб-приложений и REST API 311 ется в URL для отделения строки запроса от основного адреса. Например, если в поиске Google мы ищем «Python», то в адресной строке увидим следующий URL: https://www.google.com/search?q=Python В этом адресе q=Python -это строка запроса. Она используется для переноса дан­ ных в виде пар «ключ-значение». Такой вариант доступа к ресурсам популярен бла­ годаря своей простоте, но имеет ограничения. Данные из строки запроса видны в URL, поэтому недопустимо отправлять конфиденциальную информацию, напри­ мер, имя пользователя и пароль. К тому же, длина строки запроса не может превы­ шать 255 символов. Однако из-за своей простоты этот метод нашел свою реализа­ цию в поисковых системах вроде Google и У АНОО. В методе юsт данные отправ­ ляются в теле НТТР-запроса, что позволяет обойти ограничения метода GET. Данные не отображаются как часть адреса, и нет ограничений на их размер. Также нет ог­ раничений на типы данных. Flask предоставляет несколько удобных способов определить, с помощью какого метода отправлен запрос: GET, юsт или какой-то другой. В следующем примере мы рассмотрим два подхода. В первом используется декоратор route с точным списком ожидаемых методов, во втором -специфичные для каждого НТТР-метода декора� торы get и post. Следующий пример демонстрирует оба варианта: #арр2.ру: сопоставление запроса с типом метода from flask import Flask, request арр = Flask(_name_) @арр. route ( '/ sul:xni t' , methods= [ 'GET' ] ) def req_with_get(): return \"Received а get request\" @app.post( '/sul:xnit' ) def req_with_post(): return \"Received а post request\" @арр. route ( ' / sul:xni t2 ' , methods = [ 'GET' , ' POST ' ] ) def both_get_post(): if request.method = 'POST': return \"Received а post request 2\" else: return \"Received а get request 2\" Разберем три определения маршрута и соответствующие функции: 1.В первом определении (@арр. route ( '/ submi t', methods= [ 'GET' J ) ) мы использовали декоратор route для сопоставления URL-aдpeca в запросах типа GET с функцией Python. С этим декоратором функция будет обрабатывать запросы методом GET только для адреса / submi t. 2.Во втором определении маршрута (@арр. post (' / submi t' J) мы использовали деко­ ратор post и указали вместе с ним только URL запроса. Это упрощенная версия\n--- Страница 307 ---\n312 Раздел 4. Python для веб-разработки, облака и сети сопоставления запроса в методе юsт с функцией Python. Новый параметр экви­ валентен первому определению, но с методом юsт в упрощенном виде вместо GET. То же самое можно сделать для метода GET с помощью декоратора get. 3.В третьем определении (@арр. route ( '/ submi t2' , methods = [ 'GET', 'POST' ] ) ) мы со­ поставили один URL в запросах для методов юsт и GET с одной функцией Python. Этот способ удобен, когда ожидается, что любой метод запроса будет обрабаты­ ваться одним обработчиком (функцией Python). Внутри функции мы использо­ вали атрибут method из объекта запроса для определения типа: GET или юsт. Обра­ тите внимание, веб-сервер делает объект запроса доступным для приложения Flask после импорта пакета request в программу. Этот подход позволяет клиен­ там отправлять запросы любым из двух методов, используя один и тот же URL. Как разработчики, мы сопоставили оба варианта с одной функцией Python. Будет удобнее протестировать код с помощью утилиты curl, поскольку непросто отправлять РОSТ-запросы без НТМL-формы. Следующие команды можно исполь­ зовать для отправки НТТР-запросов в веб-приложение: curl -Х GET http://localhost:5000/submit curl -Х POST http://localhost:5000/suЬmit curl -Х GET http://localhost:5000/submit2 curl -Х POST http://localhost:5000/submit2 Далее рассмотрим, как отобразить ответ от статических страниц и шаблонов. Отображение статического и динамического контента Статический контент важен, поскольку включает в себя файлы CSS и JavaScript. Они могут обслуживаться напрямую веб-сервером. Flask также может взять на себя этот функционал, если создать каталог с именем static в проекте и перенаправить на него клиента. Динамический контент можно создать с помощью Python, но это трудоемкая зада­ ча, которая требует значительных усилий для обслуживания такого кода. Рекомен­ дуемый подход -использовать шаблонизатор, например Jinja2. Эта библиотека уже присутствует в составе Flask, поэтому она не требует дополнительной настрой­ ки. Ниже приведен пример с двумя функциями, одна из которых обрабатывает за­ прос для статического контента, а другая -для динамического: #аррЗ.ру: отображение статического и динамического контента from flask import Flask, render_template, url_for, redirect арр = Flask(_name_) @арр. route ( ' /hello' ) def hello () : hello_url = url_for ('static', filename ='appЗ_s.html') return redirect(hello_url)\n--- Страница 308 ---\nГлава 1 О. Использование Python для разработки веб-приложений и REST API @app.route('/greeting') def greeting(): msg = \"Hello from Python\" return render _ template ( 'аррЗ _ d. html', greeting=msg) Ключевые моменты кода: 313 1.Мы импортируем дополнительные модули из Flask, такие, как url_for, redirect и render_ template. 2.Для маршрута /hello мы создаем URL-aдpec с помощью функции url_for, указав каталог static и имя НТМL-файла в качестве аргументов. Отправляем ответ, ко­ торый является инструкцией для браузера о перенаправлении клиента на URL со статическим файлом. Инструкции по перенаправлению обозначены кодом со­ стояния в диапазоне 300-399, который автоматически задается для Flask при ис­ пользовании функции redirect. 3.Для маршрута /greeting мы отображаем шаблон appЗ_d.html с помощью функции render_template . Мы также передаем строку приветствия (greeting) как значение переменной для шаблона. Переменная greeting доступна шаблону, как показано в следующем фрагменте из файла appЗ_d.html: <!ООСТУРЕ html> <body> {% if greeting %} <hl> {{greeting}}!</hl> {% endif %} </body> </html> 4.Это простейший Jinjа-шаблон с оператором if, заключенным в {% %}, и перемен­ ной в двойных фигурных скобках { { J J. Мы не будем подробно рассматривать шаблоны Jinja2, но настоятельно рекомендуем это сделать вам с помощью доку­ ментации (https://jinja.pal/etsprojects.com/). Получить доступ к этому примеру приложения можно с помощью браузера и ути­ литы curl. Далее обсудим, как извлекать параметры из разных типов запросов. Извлечение параметров из НТТР-запроса Веб-приложения, в отличие от веб-сайтов, взаимодействуют с пользователем, а это невозможно без обмена данными между клиентом и сервером. Поэтому в этом под­ разделе мы обсудим, как извлекать данные из запроса. В зависимости от НТТР­ метода мы будем использовать разные подходы. Рассмотрим три типа запроса: ♦Параметры как часть URL-aдpeca запроса. ♦Параметры как строка запроса с методом GET. ♦Параметры в виде HTML с методом POST.\n--- Страница 309 ---\n314 Раздел 4. Python для веб-раэработки, облака и сети Пример кода с тремя разными маршрутами для трех типов запросов приведен ни­ же. Мы отображаем шаблон (app4.html), который аналогичен использованному ра­ нее в appЗ_d.html, за исключением того что переменная имеет имя name вместо greeting: #арр4.ру: извлечение параметров из разных запросов from flask import Flask, request, render_template арр = Flask(_name_ @арр. route ( '/hello') @app.route('/hello/<fname> <lname>') def hello_user(fname =None, lastname =None): return render_template ( 'арр4 .html', name=f\" {fname}{lname} \") @app.get( '/submit') def process_get_request_data(): fname = request.args['fname'] lname = request. args. get ( ' lname' , ' ' ) return render _ template ( 'арр4 .html', name=f\" {fname}{lname} \") @app.post( '/submit') def process_post_request_data(): fname = request.fo:i:m[ 'fname'] lname = request.fo:i:m.get('lname',' '] return render _ template ( 'арр4 .html', name=f\" {fname}{lname}\") Рассмотрим все подходы подробнее : 1.Для первого набора (app.route) мы определили маршрут таким образом, что лю­ бой текст после /hello/ считается параметрами запроса. Мы можем задать один или два параметра, или не задать ни одного, и наша функция способна обрабо­ тать любую комбинацию и в качестве ответа вернуть имя шаблону (которое мо­ жет быть пустым). Это подход хорош для простых случаев передачи параметров программе сервера и является популярным выбором при разработке REST API для доступа к одному экземпляру ресурса. 2.Для второго маршрута (app.get) мы извлекаем параметры строки запроса из объ­ екта словаря args. Получить значение можно по имени в качестве ключа словаря или с помощью метода GET со вторым аргументом в качестве значения по умол­ чанию. Мы использовали пустую строку как значение по умолчанию с GЕТ­ методом. Из двух рассмотренных вариантов лучше использовать GET, если в за­ просе нет значения по умолчанию и есть необходимость его установить. 3.Для третьего маршрута (app.post) параметры поступают в виде данных формы как часть тела НТТР-запроса, и мы будем использовать объект словаря для их извлечения. Мы также использовали имя параметра в качестве ключа словаря и метод GET в целях демонстрации.\n--- Страница 310 ---\nГлава 10. Использование Python для разработки веб-приложений и REST API 315 4.Для тестирования этих сценариев рекомендуется использовать утилиту curl, осо­ бенно для юsт-запросов. Мы протестировали приложение следующими коман­ дами: curl -Х GET http://localhost:5000/hello curl -Х GET http://localhost:5000/hello/jo%20so curl -Х GET 'http://localhost:5000/submit?fname =jo&lname =so' curl -d \"fname =jo&lname=so\" -Х POST http://localhost:5000/submit Далее обсудим, как работать с базой данных в Python. Взаимодействие с системами управления базами данных Полноценное веб-приложение требует хранения структурированных данных, по­ этому знания и опыт работы с БД необходимы для веб-разработки. Python и Flask интегрируются с большинством СУБД, будь то SQL или NoSQL. Сам Python по­ ставляется с облегченной базой данных SQLite, расположенной в модуле sqliteЗ. Мы выбрали ее, поскольку она не требует настройки отдельного сервера и хорошо работает для небольших приложений. В производственных средах рекомендуется использовать другие СУБД, например, MySQL, MariaDB или PostgreSQL. Для дос­ тупа и взаимодействия с БД используется одно из расширений Flask -Flask­ SQLAlchemy. Оно основано на библиотеке Python SQLAlchemy. Она предоставляет инст­ румент объектно-реляциоююго связывания (Object Relational Mapper, ОRМ), ко­ торый сопоставляет таблицы БД с объектами Python. Использование библиотеки ОRМ не только ускоряет цикл разработки, но и обеспечивает гибкость переключе­ ния основной СУБД без изменения кода. Поэтому для работы с СУБД рекоменду­ ется использовать SQLAlchemy или аналогичную библиотеку. Как обычно, для взаимодействия с любой БД нужно создать экземпляр приложе­ ния. Поэтому следующий шаг -настройка этого экземпляра с URL-aдpecoм для расположения БД (в случае с SQLiteЗ это файл). После создадим экземпляр SQLAlchemy, передав его экземпляру приложения. SQLiteЗ достаточно инициировать один раз. Это также можно сделать из программы, но мы не рекомендуем такой подход из-за сброса БД при каждом запуске приложения. Лучше инициализировать БД один раз из командной строки, используя экземпляр SQLAlchemy. Обсудим шаги инициализации БД подробнее после примера. Для демонстрации работы SQLAlchemy создадим простое приложение с операциями add, list и delete объектов student в таблице БД. Пример инициализации приложе­ ния и экземпляра базы данных ( SQLAlchemy ), а также создания объекта Model класса Student, представлен ниже: #арр5.ру (часть 1): взаимодействие с БД для операций create, delete и list с объектами from flask import Flask, request, render_template, redirect from flask_sqlalchemy import SQLAlchemy\n--- Страница 311 ---\n316 Раздел 4. Python для веб-разработки, облака и сети арр = Flask(_name_) арр. coмiq[ 'SQLALCВF.МY _ DАТАВАSЕ _ URI'] = 'sqlite: / / /student. dЬ' app.config['SQLALCНEМY_TRACK_MODIFICATIONS'] = False dЬ = SQLAlchemy (арр) class Student(dЬ.Мodel): id = dЬ.Column(dЬ.Integer, primary_key=True) name = dЬ.Column(dЬ.String(80), nullaЬle=False) grade = dЬ.Column(dЬ.String(20), nullaЬle =True) def _repr_(self): return '<Student %r>' % self.name Создав dЬ (экземпляр SQLAlchem y), можно работать с объектами БД. Прелесть биб­ лиотек ОRМ состоит в том, что в Python они позволяют определять схему БД как класс, в терминологии ОRМ она называется моделью (Model). В нашем примере мы создали класс student, унаследованный от базового класса dЬ.Model. В этом классе мы определили атрибуты id, name и grade, которые соответствуют трем столбцам в таблице Student экземпляра базы данных SQLi t�З. Для каждого атрибута определен его тип данных с максимальной длиной, наличие первичного ключа (Primary key) и возможность обнуления. Эти дополнительные определения атрибутов важны для оптимальной настройки таблиц БД. В следующем фрагменте кода продемонстрируем функцию list_students для полу­ чения списка учеников из базы данных. Эта функция сопоставлена с URL-aдpecoм /list нашего примера и возвращает все объекты student из таблицы, используя ме­ тод all в экземпляре query (атрибут экземпляра dЬ). Обратите внимание, что экземп­ ляр query и его методы доступны из базового класса dЬ.Model: #appS.py (часть 2) @арр. qet ( ' /list' ) def list_students(): student_list = Student.query.all() return render _ template ( 'appS. html' , students=student _ list) В следующем фрагменте напишем функцию (add_student) для добавления учеников в таблицу. Она связывается с URL-aдpecoм /add и ожидает, что имя учащегося и его класс будут переданы в качестве параметров запроса, используя GЕТ-метод. Для до­ бавления нового объекта в БД создадим новый экземпляр класса student с необхо­ димыми значениями атрибутов, а затем используем экземпляр dЬ.Session для добав­ ления его в слой ОRМ с помощью функции add. Эта функция сама по себе не доба­ вит экземпляр в базу данных. Мы будем использовать метод commit для отправки данных в таблицу. Как только новый ученик добавлен, мы передаем управление URL-aдpecy /list. Здесь используется перенаправление на этот URL-aдpec, по­ скольку нам нужно вернуть последний список учеников после добавления новой записи, а также повторно использовать функцию list _ students, которую мы уже реализовали.\n--- Страница 312 ---\nГлава 10. Использование Python для разработки веб-приложений и REST API Полный код функции add _ student выглядит следующим образом: #аррS.ру(часть 3) @app.get( ' /IIOd') def add_student(): fname = request.args['fname'] lname = request. args. get ( 'lname', ' ') grade = request.args.get('grade',' ') student = Student(name=f\"{fname) (lname}\", grade=grade) dЬ.session.lIOd(student) dЬ. session.callllit() return redirect(\"/list\")317 В последней части примера напишем функцию delete_student для удаления учени­ ков из таблицы. Она связывается с URL-aдpecoм /delete<int: id>. Обратите внима­ ние, мы ожидаем, что клиент отправит идентификатор ученика, который мы пере­ дали со списком учеников, используя запрос list. Для удаления конкретного уче­ ника мы запрашиваем его экземпляр по идентификатору. Для этого мы используем метод filter_by с экземпляром query. Получив экземпляр student, мы применяем ме­ тод delete экземпляра dЬ.Session и фиксируем изменения. Как и при использовании функции add_student, мы перенаправили клиента на URL-aдpec /list с целью вер­ нуть актуальный список учеников в шаблон Jinja: #appS.py (часть 4) @арр. get ( ' / delete/ <int: id>' ) def del_student(id): todelete = Student.query .filter_by(id=id).first() dЬ.session.delete(todelete) dЬ. session. calllli t () return redirect(\"/list\") Для вывода списка учеников в браузере, мы создали простой шаблон (appS.html), в котором находится список учеников в виде таблицы. Важно отметить, что мы ис­ пользуем цикл for для динамического создания строк НТМL-таблицы, как показано в следующем примере: <!ООСТУРЕ html> <Ьоdу> <h2>Students</h2> {% if studentsllenqth >О%} <taЬle> <thead> <tr> <th scope=\"col\">SNo</th> <th scope=\"col\">name</th> <th scope=\"col\">grade</th> </tr>\n--- Страница 313 ---\n318 </thead> <tbody> Раздел 4. Python для веб-разработки, облака и сети {% for student in students %} <tr> <th scope =\"row\">{{student.id}}</th> <td>{{student.name}}</td> <td>{{student.grade}}</td> </tr> {% endfor %} </tbody> </tаЫе> {% endif %} </body> </html> Перед запуском приложения нужно однократно инициализировать схему БД. Это можно сделать, используя программу, но нам необходимо убедиться, что код вы­ полнится только один раз, когда база данных еще не инициализирована. Лучший вариант -сделать это вручную с помощью оболочки Python. Она позволяет им­ портировать экземпляр dЬ из модуля приложения, а затем использовать метод dЬ.create_all для инициализации БД в соответствии с классами модели, определен­ ными в программе. Ниже приведены примеры команд, которые используются в приложении для инициализации БД: >>> from appS import dЬ >>> dЬ.create_all() Эти команды создадут файл student. dЬ в том же каталоге, где находится программа. Для сброса БД можно удалить файл student.dЬ и повторно выполнить команды инициализации или использовать метод dЬ. drop _ all в оболочке Python. Протестировать приложение можно с помощью утилиты curl или через браузер, используя следующие URL-aдpeca: ♦http://localhost:5000/list; ♦http://localhost:5000/add?fname=John&Lee=asif&grade=9; ♦http://localhost:5000/delete/ <id>. Далее мы узнаем, как обрабатывать ошибки в веб-приложениях на базе Flask. Обработка ошибок и исключений в веб-приложениях До этого момента в наших примерах мы не обращали внимания на ситуации, когда пользователь может вводить неверный URL-aдpec или отправлять недопустимый набор аргументов. Мы не рассматривали такие сценарии намеренно с целью снача­ ла сосредоточиться на ключевых компонентах веб-приложений. Прелесть веб­ фреймворков в том, что они обычно поддерживают обработку ошибок по умолча-\n--- Страница 314 ---\nГлава 1 О. Использование Python для разработки веб-приложений и REST API 319 нию. В случае возникновения какой-либо ошибки автоматически возвращается со­ ответствующий код состояния. Коды четко определены как часть НТТР-протокола. Например, коды с 400 по 499 указывают на ошибки с клиентскими запросами, а коды с 500 по 599 -на проблемы с сервером. Ниже приведены наиболее распро­ страненные ситуации (табл. 10.1): Таблица 10.1. Распространенные ошибки НТТР Код ошибки Имя Описание 400 Bad Request Указывает на неверный URL или содержание запроса 401 Unauthorized Имя пользователя или пароль в запросе не указа- ны или указаны неверно 403 Forbidden Пользователь пытается обратиться к ресурсу, но ему запрещен доступ 404 NotFound Ресурс, к которому мы пытаемся обратиться, недоступен. Обычно из-за неверно указанного URL 500 Internal Server Error Запрос верный, но что-то случилось на стороне сервера Полный список кодов состояний и ошибок доступен по адресу: https://httpstatuses.com/. Flask также поставляется с фреймворком обработки ошибок в своем составе. При обработке клиентских запросов, если программа прерывает свою работу, по умол­ чанию возвращается ошибка 500 Internal Server Error. Если клиент запрашивает URL-aдpec, не связанный ни с одной функцией, Flask возвращает ошибку 404 Not Found. Эти типы ошибок реализованы как подклассы класса HTTPException из библио­ теки Flask. Обрабатывать эти исключения можно с помощью пользовательского поведения или пользовательских сообщений, для этого нужно зарегистрировать наш обработчик в приложении Flask. Обратите внимание, что обработчик -это функция Flask, кото­ рая срабатывает только при возникновении ошибки·, и можно связывать с ним кон­ кретные или общие исключения. Рассмотрим пример для демонстрации концепции в общих чертах, как это работает. Сначала мы создадим простое веб-приложение с двумя функциями (hello и greeting) для обработки двух URL-aдpecoв, как показано в следующем фрагменте: #аррб.ру(часть 1): обработка оuмбок и исключений import json from flask import Flask, render_template, abort from werkzeug.exceptions import HTTPException арр = Flask{_name_\n--- Страница 315 ---\n320 @app.route ( '/') def hello () : return 'Hello World!' @арр. route ( '/ greeting' ) def greeting(): х = 10/0 Раздел 4. Python для веб-разработки, облака и сети return 'Greetings from Flask web арр!' Для обработки ошибок мы регистрируем обработчик в экземпляре приложения с помощью декоратора errorнand ler. Для примера, который приведен ниже, мы заре­ гистрировали обработчик page_not_found для кода ошибки 404. Для кода 500 мы заре­ гистрировали обработчик internal error. В конце мы зарегистрировали generic _ handler для класса HTTPExcepti on. Этот универсальный обработчик будет пе­ рехватывать ошибки и исключения, отличные от 404 и 500. Пример кода со всеми тремя обработчиками: #аррб.ру(часть 2) @app.errorhandler(404) def page_not_found(error): return render_template('error404.html'), 404 @app.errorhandler(SOO) def internal_error(error): return render_template('error500.html'), 500 @app.errorhandler{Н'l\"l'PException) def generic_handler(error): error_detail = json.dumps({ )) \"code\": error.code, \"name\": error.name, \"description\": error.description, return render_template('error.html', err_msg = error_detail), error.code Для демонстрации мы также написали базовые шаблоны с пользовательскими со­ общениями ( error404. html, error500. html и error. html). Шаблоны error404. html и error500. html используют жестко закодированное сообщение, а шаблон error. html ожидает пользовательское сообщение от веб-сервера. Для тестирования примеров, сделаем следующие запросы через браузер или утилиту curl: ♦GET http: / /localhost: 5000/: в этом случае мы ожидаем нормального ответа. ♦GET http: / /localhost: 5000/hello: мы ожидаем ошибку 404, поскольку с этим адре­ сом не связана функция Python, и приложение отобразит шаблон error404 .html.\n--- Страница 316 ---\nГлава 10. Использование Pythoп для разработки веб-приложений и REST API 319 нию. В случае возникновения какой-либо ошибки автоматически возвращается со­ ответствующий код состояния. Коды четко определены как часть НТТР-протокола. Например, коды с 400 по 499 указывают на ошибки с клиентскими запросами, а коды с 500 по 599 -на проблемы с сервером. Ниже приведены наиболее распро­ страненные ситуации (табл. 10.1): Таблица 10.1. Распространенные ошибки НТТР Код ошибки Имя Описание 400 Bad Request Указывает на неверный URL или содержание запроса 401 Unauthorized Имя пользователя или пароль в запросе не указа- ны или указаны неверно 403 Forbidden Пользователь пытается обратиться к ресурсу, но ему запрещен доступ 404 NotFound Ресурс, к которому мы пытаемся обратиться, недоступен. Обычно из-за неверно указанного URL 500 Intemal Server Епоr Запрос верный, но что-то случилось на стороне сервера Полный список кодов состояний и ошибок доступен по адресу: https :/ lhttpstatuses. сот/. Flask также поставляется с фреймворком обработки ошибок в своем составе. При обработке клиентских запросов, если программа прерывает свою работу, по умол­ чанию возвращается ошибка 500 Internal Server Error. Если клиент запрашивает URL-aдpec, не связанный ни с одной функцией, Flask возвращает ошибку 404 Not Found. Эти типы ошибок реализованы как подклассы класса HTTPException из библио­ теки Flask. Обрабатывать эти исключения можно с помощью пользовательского поведения или пользовательских сообщений, для этого нужно зарегистрировать наш обработчик в приложении Flask. Обратите внимание, что обработчик -это функция Flask, кото­ рая срабатывает только при возникновении ошибки, и можно связывать с ним кон­ кретные или общие исключения. Рассмотрим пример для демонстрации концепции в общих чертах, как это работает. Сначала мы создадим простое веб-приложение с двумя функциями (hello и greeting) для обработки двух URL-aдpecoв, как показано в следующем фрагменте: #аррб.ру(чаС'l'Ь 1): обработка ошибок и исключений import json from flask import Flask, render_template, abort from werkzeug.exceptions import HTTPException арр = Flask( name\n--- Страница 317 ---\n322 Раздел 4. Python для веб-разработки, облака и сети том концепции операций CRUD (Create, Read, Update, Delete). Именно здесь НТТР-методы напрямую сопоставляются с операциями, например, GET для опера­ ции Read, POST для операции Create, PUT для операции Update, DELETE для операции Delete. Мобильное приложение Веб-приложение Веб-браузер RЕSТ-клиент НТТР-запрос XML, JSON, HTML и др. НТТР-ответ Рис. 10.1. Взаимодействие между клиентом и сервером на основе REST API При создании REST API с НТТР-методами нужно быть осторожным при выборе метода, учитывая его идемпотен.тность. В математике операция считается идем­ потентной, если дает одинаковый результат при многократном повторении выпол­ нения. С точки зрения проектирования метод юsт не является идемпотентным. Это означает, мы должны убедиться, что клиенты не инициируют юsт-запрос несколько раз для одного набора данных. Методы GET, PUT и DELETE являются идемпотентными, хотя вполне есть возможность получить ошибку 404, если попытаться удалить один и тот же ресурс во второй раз. Но с точки зрения идемпотентности это является до­ пустимым поведением. Использование Flask для REST API REST API в Python можно создать с помощью различных библиотек и фреймвор­ ков. Самые популярные из них -это Django, Flask (с расширением Flask-RESTful) и FastAPI. Каждый из них имеет свои достоинства и недостатки. Django отлично подходит для создания REST API, если веб-приложение разрабатывается также с использованием Django. Однако задействовать его только для разработки API было бы излишним. Расширение Flask-RESTful без проблем работает с веб-приложением Flask. И Django, и Flask имеют сильную поддержку сообщества, что иногда являет­ ся важным фактором при выборе. FastAPI считается лучшим по производительно­ сти и хорошо подходит, когда необходимо создать только REST API для приложе­ ния. Однако FastAPI не имеет такой же поддержки сообщества, как Django и Flask. Мы остановили свой выбор на расширении RESTful с целью продолжить изучение Flask. Обратите внимание, мы можем создать простой веб-АРI, используя только Flask, что мы и сделали в предыдущей главе, когда разрабатывали приложение на базе веб-сервиса для развертывания в Google Cloud. В этом подразделе мы сосредо­ точимся на использовании архитектурного стиля REST при создании API. Это оз-\n--- Страница 318 ---\nГлава 1 О. Использование Python для разработки веб-приложений и REST API 321 ♦GET http://localhost:5000/greeting: мы ожидаем ошибку 500 из-за попытки разде­ лить число на ноль для вызова ошибки ZeroDivisionError; поскольку ошибка на стороне сервера, она активирует обработчик internal_error, который отображает общий шаблон error500. html. ♦POST http://localhost:5000/: для эмуляции роли универсального обработчика от­ правим запрос, который вызывает код ошибки, отличный от 404 и 500; для этого достаточно отправить запрос юsт на адрес, который ожидает GET, и сервер вы­ даст ошибку 405 (неподдерживаемый метод НТТР); у нас в приложении нет спе­ циального обработчика для этого кода ошибки, но мы зарегистрировали универ­ сальный обработчик из класса HTTPException, который обработает эту ошибку и отобразит универсальный шаблон error. html. На этом можно завершить тему использования фреймворка Flask для разработки веб-приложений. Далее рассмотрим создание REST API с помощью расширений Flask. Создание REST API REST (или ReST) -это аббревиатура от Represen tational State Transfer; пред­ ставляет собой архитектуру, позволяющую клиентским машинам запрашивать ин­ формацию о ресурсах на удаленном компьютере. API расшифровывается как Application Programming Interface (интерфейс прикладного программирования) и представляет собой набор правил и протоколов для взаимодействия с прикладным ПО приложения, запущенным на разных машинах. Потребность во взаимодействии между различными программными сущностями появилась достаточно давно. За последние несколько десятилетий было предложено и изобретено множество тех­ нологий для эффективного и беспрепятственного взаимодействия на программном уровне, включая удаленный вызов процедур (Remote Procedure Call, RPC), удален­ ный вызов методов (Remote Method Invocation, RМI), СОRВА и веб-сервисы SOAP. Эти технологии имеют ограничения из-за привязки к определенному языку программирования, привязки к проприетарному транспортному механизму или ис­ пользованию только определенного типа формата данных. Эти ограничения были почти полностью устранены в RESTful API, который широко известен как REST API. Благодаря своей простоте и гибкости протокол НТТР удобно использовать в каче­ стве транспортного механизма. Еще одно преимущество заключается в поддержке разных форматов данных для обмена (текст, XML, JSON). REST API не привязан к одному языку, что делает его лучшим выбором при создании API для веб-взаимо­ действия. На рис. 10.1 приведено архитектурное представление вызова REST API от RЕSТ-клиента к REST-cepвepy с помощью НТТР. REST API основан на НТТР-запросах и использует собственные методы, например GET, PUT, юsт и DELETE. Использование НТТР-методов упрощает реализацию клиент­ ского и серверного ПО с точки зрения проектирования. REST API разработан с уче-\n--- Страница 319 ---\n324 Раздел 4. Python для веб-раэработки, облака и сети хож на argparse, который является популярным инструментом для парсинга аргу­ ментов командной строки. Далее рассмотрим разработку REST API для доступа к данным в БД. Разработка REST API для доступа к базе данных Для демонстрации работы Flask и расширения Flask-RESTful при создания REST API мы изменим наше приложение арр5. ру и предоставим доступ к объекту student (объект Resource) с помощью архитектурного стиля REST. Ожидается, что аргумен­ ты, отправленные методам PUT и юsт, находятся в теле запроса, и API отправит от­ вет в формате JSON. Измененный код приведен ниже: #api_app.py: прило,�сение REST API AJUI ресурса student from flask_sqlalchemy import SQLAlchemy from flask import Flask fran flask_restful i.nport Resource, Api, reqparse арр = Flask(_name_) api = Api (арр) app.config['SQLALCНEМY_DATAВASE_URI'] = 'sqlite:///student.c!Ь' app.config['SQLALCHEМY_TRACK_MODIFICATIONS'] = False с1Ь = SQLAlchemy(app) В предыдущем фрагменте мы начали с инициализации приложения и экземпляра БД. Далее создали экземпляр API с помощью экземпляра Flask, выполнив выраже­ ние api = Api (арр). Этот экземпляр API является ключом для разработки остальной части приложения, и мы будем использовать его в дальнейшем. Затем нужно настроить экземпляр reqparse, зарегистрировав аргумент, который мы ожидаем для парсинга от НТТР-запроса. В нашем примере мы зарегистрировали два аргумента строкового типа (name и grade), как показано в следующем фрагменте: parser = reqparse.RequestParser() parser.add_argument('name', type=str) parser.add_argument('grade', type=str) На следующем шаге мы создадим объект Student, аналогичный тому, что в примере appS.py, но добавив в него метод serialize для преобразования объекта в формат JSON. Это важный шаг для сериализации ответа JSON перед его отправкой обратно клиентам API. Существуют и другие решения данной задачи, но мы выбрали этот вариант из-за его простоты. Пример кода выглядит следующим образом: class Student(c!Ь.Model): id = dЬ.Column(clЬ.Integer, primary_key=True) пате= dЬ.Column(dЬ.String(80), nullaЬle =False) grade = dЬ.Column(dЬ.String(20), nullaЬle =True)\n--- Страница 320 ---\nГлава 10. Использование Python для разработки веб-приложений и REST API 323 начает, что мы будем использовать НТТР-метод для выполнения операций с ресур­ сом, который будет представлен объектом Python. ВАЖНОЕ ПРИМЕЧАНИЕ Пордержка Flask-RESТful уникальна тем, что предоставляет удобный способ задать код и заголовок ответа как часть оператора return. Для использования Flask с расширением Flask-RESTful нужно сначала его устано­ вить. Это можно сделать в виртуальной среде с помощью команды pip: pip install Flask-RESTful Прежде чем обсуждать реализацию, познакомимся с некоторыми терминами и кон­ цепциями. Ресурс Это ключевой элемент REST API, и он поддерживается расширением Flask­ RESTful. Объект ресурса определяется путем расширения нашего класса из базово­ го класса Resource, который доступен в библиотеке Flask-RESTful. Базовый класс Resource предлагает несколько магических функций для помощи в разработке и ав­ томатически связывает НТТР-методы с методами Python, определенными в объекте ресурса. Конечная точка API Это точка входа для установки соединения между ПО клиента и ПО сервера. Гово­ ря простыми словами, конечная точка -это альтернативная терминология для URL-aдpeca сервера или службы, где программа прослушивает АРI-запросы. С по­ мощью Flask-RESTful мы определяем конечную точку, связывая определенный URL-aдpec (или несколько адресов) с объектом ресурса. Мы реализуем объект, расширяя базовый класс Resource. Маршрутизация Концепция маршрутизации для API аналогична маршрутизации для веб-прило­ жений с Flask с той лишь разницей, что в первом случае необходимо сопоставить объект Resource с одним или несколькими URL-адресами конечных точек. Парсинг аргументов Парсинг аргументов запроса возможен с использованием строки запроса или дан­ ных в НТМL-форме. Однако это не лучший вариант, поскольку ни строка запроса, ни НТМL-форма не предназначены для использования с API. Рекомендуемый под­ ход заключается в извлечении аргументов �апрямую из НТТР-запроса. Для этой задачи расширение Flask-RESTful предлагает специальный класс reqparse. Он по-\n--- Страница 321 ---\n326 Раздел 4. Python для веб-разработки, облака и сети qrade = arqs [ 'qrade' ] student = Student(name =name, grade=grade) dЬ.session .add(student) dЬ.session.commit() return student, 200 Для метода post класса StudentListDao мы использовали парсер reqparse для извлече­ ния аргументов name и grade из запроса. Остальная часть реализации юsт-метода совпадает с реализацией в примере appS.py. В следующих двух строках мы сопоставили URL-aдpeca с объектами Resource. Все запросы, поступающие для /students/<student_id >, будут перенаправлены в класс ресурсов StudentDao. Все запросы для /students будут перенаправлены в класс ресур­ са StudentListDao: api.add_resource(StudentDao, '/students/<student_id>') api.add_resource(StudentListDao, '/students') Обратите внимание, мы пропустили реализацию метода PUT из класса StudentDao, но для полноты картины его можно найти в исходном коде к этой главе. Для примера мы не добавляли обработку ошибок и исключений с целью избежать усложнения кода, но настоятельно рекомендуется включить это в окончательную реализацию. На данном этапе мы рассмотрели базовые концепции и принципы реализации в разработке REST API. Далее мы расширим наши знания и создадим полноценное веб-приложение. Пример: создание веб-приложения с помощью REST API В этой главе мы узнали, как создать простое веб-приложение с помощью Flask и как добавить REST API к уровню бизнес-логики с помощью расширения Flask. В реальном мире веб-приложения обычно состоят из трех уровней: веб-уровень, уро­ вень бизнес-логики и уровень доступа к данным. С ростом популярности мобиль­ ных приложений архитектура эволюционировала, и стало возможным использовать REST API в качестве строительного блока для бизнес-логики. Это позволяет созда­ вать мобильные и веб-приложения, используя один и тот же уровень бизнес­ логики. Более того, тот же API можно использовать для В2В-взаимодействия с дру­ гими поставщиками. Схема такой архитектуры приведена на рис. 10.2. В этом примере мы разработаем веб-приложение поверх REST API, которое мы ранее создали для объекта модели Student. В общих чертах приложение будет включать компоненты, показанные на рис. 10.3.\n--- Страница 322 ---\nГлава 10. Использование Python для разработки веб-приложений и REST API def serialize(self): return { 'id': self .id, 'narne': self.narne, 'grade': self.grade 325 Далее мы создали два класса Resource для доступа к объектам учеников в базе дан­ ных (StudentDao И StudentListDao): ♦studentDao предлагает методы get и delete для отдельных экземпляров ресурса; они связаны с методами GET и DELETE протокола НТТР. ♦StudentListDao предлагает методы get и post; первый предоставляет список всех ресурсов типа Student с помощью НТТР-метода GET, а второй добавляет новый объект ресурса с помощью НТТР-метода юsт; это стандартный шаблон проек­ тирования при реализации функционала CRUD для веб-ресурса. Что касается методов, реализованных для классов StudentDao и StudentListDao, мы возвращаем код состояния и сам объект в одном операторе. Эту удобную возмож­ ность предоставляет расширение Flask-RESTful. Пример кода для StudentDao: class StudentDao(Resource): def get(self, student_id): student = Student.query.filter_by(id=student_id) .\\ first_or_404(description='Record with id={} is not availaЬle'.format(student_id)) return student.serialize() def delete(self, student_id): student = Student.query.filter_by(id=student_id) .\\ first_or_404(description='Record with id={} is not availaЬle'.format(student_id)) dЬ.session.delete(student) dЬ.session.commit() return ' ' , 204 Пример кода для StudentListDao: class StudentListDao(Resource): def get (self) : students = Student.query.all() return [Student.serialize(student) for student in students ] def post(self): args = parser.parse_args() nаше = args [ 'nаше' ]\n--- Страница 323 ---\n328 Раздел 4. Python для веб-разработки, облака и сети Приложение webapp.py будет основано на Flask. Оно (далее webapp) не будет зависеть от api_app.py (далее apiapp) в том смысле, что оба приложения будут работать от­ дельно как два экземпляра Flask в идеале на двух отдельных машинах. Но, если мы запускаем оба экземпляра на одном компьютере в целях тестирования, мы должны задействовать разные порты и использовать IР-адрес локальной машины в качестве хоста. Flask использует адрес 127. о. о .1 для встроенного веб-сервера, что может по­ мешать запустить на нем два экземпляра. Оба приложения будут взаимодейство­ вать только через REST API. Кроме того, мы разработаем несколько шаблонов Jinja для отправки запросов на выполнение операций создания, обновления и удаления. Мы будем повторно использовать код api_py.py как есть и напишем приложение webapp.py с такими функциями, как просмотр списка учеников, добавление и удале­ ние учеников, а также изменение данных о них. Для каждой из них мы добавим функции Python: 1.Начнем с инициализации экземпляра Flask, как мы это делали в предыдущих примерах. Код выглядит следующим образом: #wеЬарр.ру: взаимодействие с уровнем бизнес-логики через REST API #для создания, удаления и просмотра объектов from flask import Flask, render_template, redirect, request import requests, json арр = Flask( name 2.Далее добавим функцию list для обработки запросов с URL-aдpeca «/»: @app.get( '/') def list(): response = requests.get('http://localhost:8080/students') data = json.loads(response.text) return render_template('main.html', students =data) 3.Во всех функциях используется библиотека requests для отправки запроса REST API в приложение apiapp, которое размещено на том же компьютере в нашей тестовой среде. 4.Реализуем функцию add для обработки запроса на добавление нового ученика в БД. С этой функцией будет сопоставляться только запрос с методом юsт. При­ мер кода выглядит следующим образом: @арр. post ( ' / ' ) def add(): fname = request.form['fname'] lname = request. form [ 'lname'] grade = request.form['grade'] payload = { 'name': f\"{fname) {lname)\", 'grade': grade) respone = requests.post('http://localhost:8080 /students', data=payload) return redirect(\"/\")\n--- Страница 324 ---\nГлава 10. Использование Python для разработки веб-приложений и REST API webapp.py api_app.py Веб-nриложение Мобильное Уровень бизнес-логики / доступа к данным в в в -----Уровень данных----- Рис. 10.2. Архитектура веб- и мобильных приложений ,, ,, / Webapp static i:::, templates SWen!Us!Dan � БД Student Веб-приложение (Flask) Бизнес-логика (Flask/Flask RESTful) ORM (SQLAlchemy) Рис. 10.3. Пример неб-приложения на базе REST API 327 Мы уже разработали уровни бизнес-логики и доступа к данным (ОRМ), а также предоставили функционал через две конечные точки API (ранее в подразделе «Использование Flask для REST API» ). Теперь разработаем часть приложения для веб-доступа и будем использовать API, предлагаемый бизнес-логикой.\n--- Страница 325 ---\n330 Раздел 4. Python для веб-разработки, облака и сети -2--а-Корневой URL-aдpec «/» запустит главную страницу main .·html, которая позволит добавить нового ученика, а также предоставит список имеющихся учеников. На рис. 10.4 приведен скриншот главной страницы, которая будет отображаться с помощью шаблона main.html: Students Add а Student First name Last name Grade - Students No Name John Lee 2 Brian Miles Grade 10 7 Actions / Update 1 / Delete 1 1 Update 11 Delete J Рис 10.4. Главная страница приложения webapp 3.Если мы введем имя, фамилию и класс ученика и нажмем кнопку Submit, вы­ полнится юsт-запрос с данными этих трех полей. Веб-приложение передаст этот запрос в функцию add, которая использует соответствующий REST API прило­ жения apiapp для добавления нового ученика, и снова отобразит главную стра­ ницу с обновленным списком. 4.На главной странице webapp (main.html) мы добавили две кнопки (Update и Delete) к каждой записи об ученике. При нажатии на Delete бра уз ер выполняет GЕТ-запрос с URL-aдpecoм /delete/<id>, который делегируется функции delete. Она, в свою очередь, будет использовать соответствующий REST API для уда­ ления ученика из БД и снова отобразит главную страницу с обновленным спи­ ском учеников. 5.При нажатии на Update браузер выполнит GЕТ-запрос с URL-aдpecoм /update/<id>. который будет передан функции load_student_for_update. Она загру­ зит информацию об ученике с помощью REST API, затем передаст данные в рт­ вет и отобразит шаблон update.html. Он покажет пользователю НТМL-форму с данными ученика, которые можно изменять. Форма для этого сценария показана на рис. 10.5.\n--- Страница 326 ---\nГлава 1 О. Использование Python для разработки веб-приложений и REST API 329 5.Обратите внимание, для вызова в приложении apiapp мы создали объект payload и передали его как атрибут data в rоsт-метод модуля requests. 6.Далее добавим функцию DELETE для обработки запроса на удаление ученика из БД. Ожидается, что тип запроса, связанный с этим методом, предоставит иден­ тификатор ученика как часть URL-aдpeca: @app.get('/delete/<int:id>') def delete (id) : response = requests.delete('http://localhost:8080 /students/'+str(id)) return redirect(\"/\") 7.Добавим две функции для реализации изменения данных. Первая (update) обно­ вит данные ученика таким же образом, как и функция post. Перед запуском update приложение webapp предложит пользователю форму с текущими данными объекта student. Вторая функция (load_student_for_update ) получит объект student и отправит его в шаблон для редактирования пользователем. Код обеих функций выглядит следующим образом: @app.post( '/update/<int:id>' ) def update (id) : fname = request. form [ 'fname' ] lname = request.form['lname'] grade = request.form['grade'] payload = { 'name' : f\" { fname} { lname} \", 'grade': grade} respone = requests.put('http://localhost:8080 /students/' + str(id), data = payload) return redirect(\"/\") @app.qet( '/update/<int:id>' ) def load_student_for_update(id): response = requests.get('http://localhost:8080 /students/'+str(id)) student = json.loads(response.text) fname = student['name'].split() [О] lname = student['name'] .split() [1] return render_template('update.html', fname=fname, lname=lname, student= student) Код внутри этих функций не отличается от предыдущих примеров, поэтому мы не будем подробно рассматривать каждую строку, а выделим основные моменты веб­ приложения и его взаимодействия с приложением REST API: 1.Мы используем два шаблона (main.html и update.html ). Мы также используем об­ щий для них шаблон base.html. Он в основном создается, с помощью UI­ фреймворка bootstrap. Здесь мы не будем подробно обсуждать шаблоны Jinja и bootstrap, но вы можете ознакомиться с ними по ссылкам, предложенным в кон­ це этой главы. Там также можно найти примеры этих шаблонов.\n--- Страница 327 ---\n332 Раздел 4. Python для веб-разработки, облака и сети SQLAlchemy. В дополнение мы узнали роль веб-АРI для мобильных, В2В-и веб­ приложений. На простом примере изучили расширение Flask для разработки REST API. В конце мы реализовали пример веб-приложения со списком учеников, кото­ рое состоит из двух неза_висимых приложений. Одно из них предлагает REST API для уровня бизнес-логики поверх СУБД. Другое предоставляет пользователям веб­ интерфейс и использует интерфейс REST API первого приложения для обеспечения доступа к таблице учеников. Примеры кода в этой главе позволят приступить к созданию собственных веб­ приложений и написанию REST API. Знания будут полезны всем, кто хочет стать веб-разработчиком и работать над созданием REST API. В следующей главе мы увидим, как использовать Python для разработки микросер­ висов, которые являются новой парадигмой разработки ПО. Вопросы 1.В чем цель TLS? 2.В каких случаях Flask лучше Django? 3.Какие НТТР-методы наиболее часто используются? 4.Что такое CRUD и как это связано с REST API? 5.Использует ли REST API только JSON в качестве формата данных? Дополнительные ресурсы ♦«Разработка веб-приложений с использованием Flask на языке Python» (Flask Web Development), автор: Мигель Гринберг (Miguel Grinberg). ♦«Advanced Guide to Python 3 Programming», автор: Джон Хант (John Hunt). ♦<<REST A.Pls with Flask and Python», авторы: Джейсон Маерз (Jason Myers) и Рик \\ Копланд (Rick Copeland). ♦<<Essential SQLA.lchemy», 2-е издание, автор: Хосе Сальватьерра (Jose Salvatierra). ♦«Bootstrap 4 Quick Start», автор: Джейкоб Летт (Jacob Lett). ♦Документация по Jinja: https://jinja.palletsprojects.com/. Ответы 1.Главная цель TLS -обеспечить шифрование данных, которыми обмениваются две системы в Интернете.\n--- Страница 328 ---\nГлава 1 О. Использование Python для разработки веб-приложений и REST API Students Update Student First name �----- ---- - : John '--------- Last name 1 J Grade !10�---- -·-_ _j нем Рис. 10.5. Пример формы для изменения данных ученика 331 После внесения изменений, если пользователь отправит форму, нажав на кнопку Update, браузер выполнит юsт-запрос с URL-aдpecoм /update/<id>. Мы зарегистри­ ровали функцию update для этого запроса. Она будет извлекать данные и передавать их в REST API. После обновления информации об ученике мы снова отображаем страницу main.html с обновленными сведениями. В этой главе мы не рассматривали подробные детали чистых веб-технологий, та­ ких, как HTML, Jinja, CSS и UI-фреймворки. Прелесть веб-фреймворков в том, что они позволяют использовать любые веб-технологии пользовательских интерфей­ сов, особенно если мы создаем приложения с помощью REST API. На этом мы завершаем обсуждение создания веб-приложений и разработку REST API с помощью Flask и его расширений. Разработка не ограничивается одним язы­ ком или фреймворком. Ключевые принципы и архитектуры остаются неизменными для всех фреймворков и языков. Принципы, которые мы рассмотрели, помогут лучше понять другие веб-инструменты для Python, а также другие языки. Заключение ! В этой главе мы узнали, как использовать Python и веб-фреймворки вроде Flask для разработки веб-приложений и REST API. Мы начали с анализа требований к веб­ разработке, которые включают веб-фреймворк, UI-фреймворк, веб-сервер, СУБД, поддержку API, безопасность и документацию. Затем на примерах узнали, как ис­ пользовать Flask для создания веб-приложений. Обсудили типы запросов с разны­ ми НТТР-методами и способы парсинга данных в запросе. Мы также изучили, как использовать Flask для взаимодействия с СУБД, используя ОRМ-библиотеки вроде\n--- Страница 329 ---\n11 Разработка микросервисов на Python Монолитные приложения с одноуровневым ПО много лет были популярным вари­ антом для разработки, однако развертывать их на облачных платформах неэффек­ тивно с точки зрения резервирования и использования ресурсов. Также дела обсто­ ят и при развертывании крупномасштабных монолитных приложений на физиче­ ских машинах. Разрабатывать и обслуживать такие приложения всегда дорого. Разделение приложений на несколько уровней в какой-то мере решило проблему, но настоящим спасением стала микросервисная архитектура, которая обеспечива­ ет динамическое выделение ресурсов и сокращает затраты на разработку и обслу­ живание. Она позволяет создавать приложения из слабо связанных сервисов и раз­ вертывать их на динамически масштабируемых платформах , таких, как контейне­ ры. Организации вроде Amazon, Nettlix и Facebook уже перешли от монолитной модели к микросервисам. Иначе они не смогли бы обслуживать огромное количе­ ство клиентов. Темы этой главы: ♦Введение в микросервисы. ♦Практические рекомендации по созданию микросервисов. ♦Создание приложений на базе микросервисов. В этой главе вы познакомитесь с микросервисами и научитесь создавать приложе­ ния на их основе. Технические требования В этой главе понадобится: ♦ Python 3. 7 или более поздней версии. ♦Библиотека Flask с расширением RESTful.\n--- Страница 330 ---\nГлава 10. Использование Python для разработки веб-приложений и REST API 333 2.Flask -лучший выбор для малых и средних приложений, особенно если требо­ вания проекта меняются часто. 3.GET И POST. 4.CRUD расшифровывается как Create, Read, Update и Delete. Это операции, ко­ торые используются в разработке ПО. С точки зрения API каждая операция CRUD сопоставляется с одним из НТТР-методов (GET, юsт, ют и DELETE). 5.REST API поддерживает любой строковый формат, например, JSON, XML или HTML. Поддержка формата данных больше связана со способностью НТТР пе­ редавать данные как часть тела НТТР.\n--- Страница 331 ---\n336 Клиентский браузер ii 1 Сервис веб-nриложения Сервис авторизации Раздел 4. Python для веб-разработки, облака и сети Мобильное приложение Сервис каталога продуктов CJ1l Сервис инвеитаризацми npoд)'t(ТOII в Рис. 11.1. Пример приложения с микросервисной архитектурой 2.Возможность разрабатывать, тестировать и обслуживать отдельные микросерви­ сы небольшими независимыми командами. Крайне важно иметь независимые и автономные команды для разработки крупномасштабных приложений. 3.Одна из проблем монолитных приложений- управление конфликтующими версиями библиотек, без которых не обойтись. При использовании микросерви­ сов конфликты версий сведены к минимуму. 4.Возможность развертывать и исправлять . отдельные микросервисы независимо друг от друга. Благодаря этому можно использовать CUCD для сложных при­ ложений. Это также позволяет применять патчи или обновлять только один компонент приложения. В случае с монолитным приложением приходится по­ вторно развертывать его целиком, а значит, есть вероятность сломать другие части приложения. При использовании микросервисов только один или два сер­ виса будут развернуты без риска нарушить работу остальных. 5.Возможность изолировать ошибки и сбои на уровне микросервиса, а не всего приложения. Если есть сбой в одном сервисе, можно выполнить отладку и ис­ править ошибку или остановить сервис для обслуживания, не затронув осталь­ ной функционал приложения. Ошибки в монолитных приложениях могут вы­ вести из строя все приложение. Несмотря на преимущества микросервисная архитектура имеет недостатки: 1.Повышенная сложность создания приложений на базе микросервисов. В основ­ ном это связано с тем, что каждый микросервис должен предоставлять API для взаимодействия с ним клиентских сервисов и программ. Также разработку ус­ ложняет необходимость обеспечивать безопасность каждого микросервиса. 2.Повышенная потребность в ресурсах по сравнению с монолитными приложе­ ниями. Каждый микросервис требует независимого размещения дополнительной",
      "debug": {
        "start_page": 297,
        "end_page": 331
      }
    },
    {
      "name": "Глава 11. Разработка микросервисов на Python 334",
      "content": "--- Страница 332 --- (продолжение)\nГлава 11. Разработка микросервисов на Python 335 ♦Библиотека Django с фреймворком Django REST. ♦Аккаунт Docker, а также установленные на вашей машине Docker Engine и Docker Compose. ♦Аккаунт GCP для развертывания микросервиса в GCP Cloud Run (достаточно бесплатной пробной версии). Пример кода для этой главы можно найти по адресу: https :/ /github. com/PacktPuЬlishing/Python -for-Geeksltree/master/Chapter 11. Начнем обсуждение со знакомства с микросервисами. Введение в микросервисы Микросервис -это независимая программная сущность, которая должна иметь следующие характеристики: ♦Слабая связь с другими сервисами и независимость от других программных компонентов. ♦Простота разработки и обслуживания небольшой командой, не зависящей от других команд. ♦Независимая установка в виде отдельной сущности, предпочтительно в кон­ тейнере. ♦Простые в использовании интерфейсы с синхронными (REST API) или асин- хронными протоколами (Kafka или RabbitMQ). Ключевые слова с точки зрения микросервиса: независимое развертывание, слабое связывание и простота обслуживания. Каждый микросервис может иметь собст­ венные серверы БД для избегания совместного использования ресурсов с другими микросервисами. Это обеспечит устранение зависимостей между ними. Микросервисная архитектура -это парадигма разработки ПО, которое состоит исключительно из микросервисов. Такая архитектура включает даже объект глав­ ного интерфейса, например, веб-приложение. Пример ПО на базе микросервисов показан на рис. 11.1. В этом примере есть отдельные микросервисы, такие, как Сервис авторизации, Сервис каталога продуктов и Сервис инвентаризации продуктов. Мы создали веб­ приложение тоже в виде микросервиса, который использует три отдельных микро­ сервиса через REST API. Мобильное приложение может быть создано с использо­ ванием тех же микросервисов через шлюз API. Очевидное преимущество микро­ сервисной архитектуры -возможность повторного использования компонентов. Есть также и другие преимущества: 1.Гибкость при выборе любых технологий и языков программирования в соответ­ ствии с индивидуальными требованиями микросервисов. Есть даже возмож­ ность использовать устаревший код на любом языке, если предоставить к нему доступ через АРI-интерфейс.\nГлава 11. Разработка микросервисов на Python 335 ♦Библиотека Django с фреймворком Django REST. ♦Аккаунт Docker, а также установленные на вашей машине Docker Engine и Docker Compose. ♦Аккаунт GCP для развертывания микросервиса в GCP Cloud Run (достаточно бесплатной пробной версии). Пример кода для этой главы можно найти по адресу: https :/ /github. com/PacktPuЬlishing/Python -for-Geeksltree/master/Chapter 11. Начнем обсуждение со знакомства с микросервисами. Введение в микросервисы Микросервис -это независимая программная сущность, которая должна иметь следующие характеристики: ♦Слабая связь с другими сервисами и независимость от других программных компонентов. ♦Простота разработки и обслуживания небольшой командой, не зависящей от других команд. ♦Независимая установка в виде отдельной сущности, предпочтительно в кон­ тейнере. ♦Простые в использовании интерфейсы с синхронными (REST API) или асин- хронными протоколами (Kafka или RabbitMQ). Ключевые слова с точки зрения микросервиса: независимое развертывание, слабое связывание и простота обслуживания. Каждый микросервис может иметь собст­ венные серверы БД для избегания совместного использования ресурсов с другими микросервисами. Это обеспечит устранение зависимостей между ними. Микросервисная архитектура -это парадигма разработки ПО, которое состоит исключительно из микросервисов. Такая архитектура включает даже объект глав­ ного интерфейса, например, веб-приложение. Пример ПО на базе микросервисов показан на рис. 11.1. В этом примере есть отдельные микросервисы, такие, как Сервис авторизации, Сервис каталога продуктов и Сервис инвентаризации продуктов. Мы создали веб­ приложение тоже в виде микросервиса, который использует три отдельных микро­ сервиса через REST API. Мобильное приложение может быть создано с использо­ ванием тех же микросервисов через шлюз API. Очевидное преимущество микро­ сервисной архитектуры -возможность повторного использования компонентов. Есть также и другие преимущества: 1.Гибкость при выборе любых технологий и языков программирования в соответ­ ствии с индивидуальными требованиями микросервисов. Есть даже возмож­ ность использовать устаревший код на любом языке, если предоставить к нему доступ через АРI-интерфейс.\n--- Страница 333 ---\n338 Раздел 4. Python для веб-разработки, облака и сети 4.Интерфейсы взаимодействия: для взаимодействия следует применять четко определенные интерфейсы микросервисов, желательно REST API или собы­ тийно-ориентированные API. Микросервисы должны избегать прямых вызовов друг друга. 5.Использование шлюза API: микросервисы и их клиентские приложения­ потребители должны взаимодействовать друг с другом через шлюз API. Он помогает позаботиться об аспектах безопасности, таких, как аутентификация и балансировка нагрузки, «из коробки». Более того, при появлении новой версии микросервиса можно использовать шлюз API для перенаправления на него кли­ ентских запросов, не влияя на клиентское ПО. 6.Ограниченный стек технологий: хотя архитектура позволяет использовать любые фреймворки и языки программирования в отдельных сервисах, не реко­ мендуется задействовать при разработке разные технологии без веских на то причин. Многообразие стека технологий может быть привлекательно в учебных целях, но на практике усложнит обслуживание и устранение неполадок. 7.Модель развертывания: микросервисы не обязательно развертывать в кон­ тейнерах, но это рекомендуемый подход. Контейнеры имеют много встроенных возможностей, например, автоматизированное развертывание, кросс-платфор­ менная поддержка и совместимость. Кроме того, с помощью них можно выде­ лять ресурсы в зависимости от требований и обеспечивать справедливое их распределение между микросервисами. 8.Контроль версий: для каждого микросервиса требуется отдельная система контроля версий. 9.Организация команд: архитектура микросервисов позволяет создавать выде­ ленные команды программистов для каждого микросервиса. Следует помнить об этом принципе при организации команд разработки для крупномасштабного проекта. Количество людей должно основываться на правиле двух пицц, соглас­ но которому должно быть столько инженеров, что их можно накормить двумя большими пиццами. Одна команда может заниматься одним или несколькими микросервисами в зависимости от их сложности. 1 О. Централизованное логирование/мониторинг: как уже упоминалось, в прило­ жениях с микросервисной архитектурой поиск неполадок может отнимать много времени, особенно если сервисы работают в контейнерах. Следует использовать открытый исходный код или профессиональные инструменты мониторинга и устранения неполадок для снижения подобных расходов эксплуатации. Несколь­ ко примеров таких инструментов: Splunk, Grafana, Elk и Арр Dynamics. Далее углубимся в создание приложения с помощью микросервисов. Создание приложений на базе микросервисов Прежде чем вдаваться в детали, важно проанализировать несколько фреймворков и вариантов развертывания.\n--- Страница 334 ---\nГлава 11. Разработка микросервисов на Python 337 памяти в контейнере или на виртуальной машине, даже если это Java Virtual Machine (JVM). 3.Дополнительные усилия для отладки и устранения неполадок в разных микро­ сервисах, которые могут быть развернуты в отдельных контейнерах или системах. Далее рассмотрим лучшие практики по созданию микросервисов. Практические рекомендации по созданию микросервисов Начиная работу над новым приложением, мы должны ответить на главный вопрос, стоит ли использовать в нем микросервисную архитектуру. Начинать нужно сана­ лиза требований к приложению и возможности разделить их на независимые ком­ поненты. Если прослеживается частая зависимость компонентов друг от друга, это указывает, что разделение приложения на части стоит доработать или вовсе отка­ заться от микросервисной архитектуры. Это решение важно принять на начальном этапе. Существует мнение, что лучше начинать разработку с использованием монолитной архитектуры во избежание до­ полнительных затрат на микросервисы в самом начале. Однако этот подход не ре­ комендуется. После создания монолитное приложение уже трудно преобразовать в микросервисы, особенно если оно уже развернуто в рабочей среде. Такие компа­ нии, как Amazon и Netflix уже перешли на микросервисную архитектуру, но они сделали это в рамках своей эволюции технологий и, разумеется, они имеют доста­ точно человеческих и технологических ресурсов для такой трансформации. Если вы уже четко решили задействовать микросервисы, следующие рекомендации помогут в проектировании и развертывании: 1.Независимость и слабая связь: эти требования входят в само определение по­ нятия микросервисов. Каждый из них должен быть построен независимо от ос­ тальных и иметь максимально слабые связи. 2.Предметно-ориентированное проектирование (Domain-Driven Design, DDD): цель архитектуры не в разделении на максимальное количество небольших мик­ росервисов. Нужно помнить, что каждый сервис имеет свои издержки. Мы должны создавать столько микросервисов, сколько требуется бизнесу (предмет­ ной области). Рекомендуем изучить DDD, представленное Эриком Эвансом в 2004 году. 3.Если мы пытаемся применить DDD к микросервисам, это предполагает сначала разработку стратегического проектирования для определения различных кон­ текстов путем объединения связанных бизнес-областей и их подобластей. За стратегическим проектированием может следовать тактическое проектирование, которое фокусируется на разбиении основных предметных областей на детали­ зированные строительные блоки и сущности. Это разделение предоставит чет­ кие рекомендации для сопоставления требований с возможными микросервисами.\n--- Страница 335 ---\n340 Раздел 4. Python для веб-разработки, облака и сети Варианты развертывания микросервисов Как только микросервис создан, следует задать еще один важный вопрос -как его развернуть в качестве изолированной и независимой сущности. В целях обсужде­ ния предположим, что микросервисы созданы с интерфейсами HTTP/REST. Можно развернуть их все на одном веб-сервере как разные веб-приложения или выделить веб-сервер для каждого микросервиса. Один микросервис можно развернуть на од­ ной машине ( физической или виртуальной) или на разных машинах и даже в от­ дельных контейнерах. На следующей схеме (рис. 11.2) описаны все возможные мо­ дели развертывания: • • • • • 1 С О Физическая Обозначения: Микросервис : . .i Веб-сервер ервер припоженияили виртуальная машина Модель А Модель В Модель С Рис. 11.2. Модели развертывания микросервисов Рассмотрим подробнее: Контейнер Модель D ♦Модель А: здесь мы развертьmаем четыре разных микросервиса на одном веб­ сервере. В этом случае есть большая вероятность, что они используют общие библиотеки, которые также расположены на одном веб-сервере. Это может при­ вести к конфликтам библиотек, поэтому такая модель не рекомендуется. ♦Модель В: здесь четыре микросервиса развернуты на одной машине, но каждо­ му из них выделен свой веб-сервер с целью сделать их независимыми. Эта мо­ дель подходит для сред разработки, но может не подходить для производствен­ ных сред. ♦Модель С: здесь для каждого из четырех микросервисов задействованы четыре отдельных виртуальных машины. На каждой машине размещается только один микросервис с веб-сервером. Эта модель подходит для производственных сред, если нет возможности использовать контейнеры. Основным предостережением такой модели являются дополнительные издержки, которые принесет каждая из виртуальных машин. ♦Модель D: здесь каждый микросервис развернут как контейнер на одной или нескольких машинах. Это не только экономично, но также позволяет соблюсти соответствие спецификациям микросервисов. Такая модель рекомендована, если есть возможность ее использовать.\n--- Страница 336 ---\nГлава 11. Разработка микросервисов на Python 343 приложений. Сейчас в этот файл включено только приложение admin. Файлы asgi.py и wsgi.py доступны для запуска веб-сервера ASGI или WSGI, а в файле settings .ру задается, какой из них будет использоваться. Adminapp ,., 81 grades _iniL.py asgi.py settings.py urls.py wsgi.py \" Главный проект Определ екке параметров проекта \" к спксок пр�mожек кй дпя ра3вертыванкя \" Определение корневого URL дnя каждого пр11Л.ожеккя \" Ут11Л кта командко й страхи Рис. 11.4. Файловая структура нового проекта Django 7.Создаем новое приложение Django (микросервис Grades), выполнив следующую команду в главном каталоге проекта grades: pythonЗ manage.py startapp grades_svc 8.Команда создаст новое приложение (с веб-компонент ами) в отдельном каталоге с именем grades_svc, указанным в команде. Также будет создан экземпляр БД SQLiteЗ по умолчанию. Параметр, что SQLiteЗ будет использоваться по умолча­ нию, указан в файле settings.py, но его можно изменить, если нужно использо­ вать другую БД. 9.К автоматически созданным файлам в каталоге grades_svc добавим еще два фай­ ла -urls.py и serializers.py. Полная структура каталогов проекта с двумя до­ полнительными файлами показана на рис. 11.5 вместе с описанием назначения каждого файла. 10.Далее в эти файлы поочередно добавим код для микросервисов. Начнем с опре­ деления класса Grade, расширив класс Model из пакета БД models. Полный код файла models. ру выглядит так: from django.dЬ import models class Grade(models.Model): grade_id = models.CharField(max_length=20) building = models.CharField(max_length=200) teacher = models.CharField(max_length=200) def str (self) : return self.grade_id\n--- Страница 337 ---\n342 Раздел 4. Python для веб-разработки, облака и сети ♦Повторно используем приложение apiapp из предыдущей главы; для текущего примера оно будет называться Students; этот модуль будет без изменений. ♦Обновим приложение webapp из предыдущей главы с целью использовать микро­ сервис Grades и добавим дополнительные атрибуты Grade для каждого объекта Student; это также потребует небольших изменений в шаблонах Jinja. Начнем с создания микросервиса Grades с помощью Django. Создание микросервиса Grades Для разработки микросервиса мы будем использовать Django Rest Framework (DRF). Django использует различные компоненты для создания REST API и мик­ росервисов. Этот пример также даст вам представление о разработке с помощью Django. Поскольку мы уже знакомы с Flask и основными концепциями веб-разработки, нам будет легко начать использовать Django. Разберем некоторые этапы: 1.Сначала создадим каталог проекта или новый проект в IDE с виртуальной сре­ дой. Если вы не используете IDE, можете создать и активировать виртуальную среду в каталоге проекта следующими командами: python -m venv myenv source myenv/bin/activate 2.Жизненно важно создать отдельную виртуальную среду для каждого веб­ приложения. Использование глобальной среды для зависимостей библиотек мо­ жет привести к ошибкам, которые сложно устранить. 3.Для разработки с Django понадобятся, как минимум, две библиотеки, которые можно установить следующими командами pip: pip install django pip install django-rest-framework 4.После установки Django можно создать проект с помощью утилиты командной строки django-admin. Следующая команда создаст проект grades для нашего мик­ росервиса: django-admin startproject grades 5.Команда создаст веб-приложение admin в каталоге grades и добавит файл manage.py в проект. Неб-приложение admin включает в себя скрипты запуска встроенного веб-сервера, файлы настроек и маршрутизации URL. Как и django­ admin, manage. ру также является утилитой командной строки и предлагает анало­ гичные функции, но в контексте проекта Django. Файловая структура каталога нового проекта будет выглядеть, как представлено на рис. 11.4. 6.Как показано на рис. 11.4, файл settings .ру содержит параметры на уровне про­ екта, включая список приложений для развертывания на веб-сервере. Файл urls.py содержит информацию о маршрутизации для различных развернутых\n--- Страница 338 ---\nГлава 11. Разработка микросервисов на Python serializer = GradeSerializer(grades_list, many=True) return Response(serializer.data) def create(self, request): pass: def retrieve(self, request, id=None): pass: 345 13.Обратите внимание, для полноты микросервиса мы написали методы для до­ бавления нового объекта Grade и получения объекта Grade по идентификатору в фактической реализации. В примере мы приводим только метод list, поскольку это единственный подходящий метод для нашего примера. Также важно под­ черкнуть, что объекты представления должны быть реализованы как классы, и в них не следует размещать логику приложения. После реализации основных методов в grades _ svc добавим наше приложение в про­ ект Django для развертывания, а также укажем маршруты на уровне приложения и API: 1.Добавим приложение grades_svc , а также rest-framework в список установленных приложений INSTALLED _ APPS в файле set tings. ру: INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.conte nttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', I grades _ SVC I 'rest_framework', 2.Распространенной ошибкой разработчиков является добавление новых компо­ нентов в один файл settings.py, поскольку его сложно поддерживать в большом проекте. Лучше разделить его на несколько файлов и загружать их в главный файл настроек. 3.Следующим шагом является добавление конфигурации URL-aдpeca на уровне приложения admin, а затем на уровне нашего приложения. Сначала мы добавим URL-aдpec для нашего приложения в файл urls.py в приложении admin. Это также обеспечит видимость приложения в admin: urlpatterns = [ path ( 'admin/', admin. site. urls), path(' ', include('grades_svc.urls')), 4.В файле urls.py приложения admin мы перенаправляем каждый запрос в наш микросервис, кроме запросов с URL-aдpeca admin/.\n--- Страница 339 ---\n344 )(paHlfТ Тра1138.КЦl{И, которые п:римеtt11ются к Оп:ределяет модели данных \" приложения Определяет марwрут1�азц11ю \" МеждУ НПР-методами и функцкям:и Python Раздел 4. Python для веб-разработки, облака и сети grades db.sqtiteЗ 1' manage.py т II grades_svc migrations _iniL,PY admin.py apps.py models.py \" ЭК3емп:ляр БД \" Катапог нового пр1mожени я Оnредеnяет список \" приложекий. которые будут развернуты с Django .serializers.py \" Определяет_ серlt8ЛК$1iЦИЮ для моделеи данных tests.py urfs.py views.py llt Реапизует фуккции RЕSТ­ иктерфейса Рис. 11.5. Полная структура каталога приложения grades_svc 11.Нужно сделать модель видимой на панели инструментов приложения admin, за­ регистрировав класс Grade в файле admin.py: from django.contrib import admin from .models import Grade admin.site.register(Grade) 12.Реализуем метод для извлечения списка объектов Grade из базы данных. Доба­ вим класс GradeViewSet, расширив класс ViewSet в файле views. ру: from rest_framework import viewsets, status from rest_framework.response import Response from .models import Grade from .serializers import GradeSerializer class GradeViewSet(viewsets.ViewSet): def list(self, request): grades_list = Grade.objects.all()\n--- Страница 340 ---\nГлава 11. Разработка микросервисов на Python 347 его в контейнер и запустим как контейнерное приложение. Этому посвящен сле­ дующий подраздел. Контейнеризация микросервиса Кон.тейн.еризация (Containerization) -это тип виртуализации ОС, при котором приложения выполняются в отдельных пользовательских пространствах, но ис­ пользуют одну ОС. Отдельное пользовательское пространство называется кон.тей­ н.ером. Самая популярная платформа для создания, администрирования и выполне­ ния приложений в виде контейнеров -Docker. Он занимает более 80% рынка, но существуют также и другие среды выполнения, например, CoreOS rkt, Mesos, lxc и containerd. Прежде чем использовать Docker, рассмотрим основные его компоненты: ♦Docker Engine: основное приложение Docker для создания, сборки и запуска контейнерных приложений. ♦Образ Docker: это файл, который используется для запуска приложения в кон­ тейнерной среде; приложения, разработанные с помощью Docker Engine, хра­ нятся в виде Dосkеr-образов, которые представляют собой набор кода, библио­ тек, файлов ресурсов и �ругих зависимостей, необходимых для выполнения приложения. ♦Docker Hub: онлайн-репозиторий образов, которым можно поделиться с коман­ дой или сообществом; также известен как реестр Docker. ♦Docker Compose: инструмент для создания и запуска контейнерных приложе­ ний с помощью УАМL-файла вместо СLI-команд в Docker Engine; Docker Compose предлагает простой способ развертывания и выполнения нескольких контейнеров с атрибутами конфигурации и зависимостями; для создания и за­ пуска контейнеров рекомендуется использовать Docker Compose или аналогич­ ную технологию. Для использования Docker Engine и Docker Compose необходимо иметь аккаунт в реестре Docker. Кроме того, перед выполнением следующих шагов нужно загру­ зить и установить Docker Engine и Docker Compose на компьютер: 1.Сначала создадим список зависимостей проекта: pip freeze -> requirements.txt 2.Эта команда создаст список зависимостей и экспортирует их в файл requirements. txt, который будет использоваться Docker Engine для загрузки биб­ лиотек внутри контейнера поверх интерпретатора Python. Содержимое файла для нашего проекта будет следующим: asgiref==З.4.1 Django==З.2.5 django-rest-framework ==0.1.0 djangorestframework ==З.12.4 pytz==2021.1 sqlparse ==0.4.1\n--- Страница 341 ---\n346 Раздел 4. Python для веб-разработки, облака и сети 5.Следующий шаг -установка маршрутов в нашем приложении на основе раз­ личных НТТР-методов. Для этого нужно добавить файл urls.py в каталог grades_svc со следующими определениями маршрутов: from django.urls import path from .views import GradeViewSet urlpatterns = [ path(grades/', GradeViewSet.as_view({ 'qet':'list', #relevant for our sample application 'post': 'create' }) ) , path ( 'grades/<str: id>', GradeViewSet.as_view ( { 'qet': 'retrieve' })) 6.В этом файле мы присоединяем методы GET и юsт НТТР-запросов с URL­ aдpecoм grades/ к методам list и create класса GradeViewSet, который мы ранее реализовали в файле views.py. Точно так же мы присоединяем запрос GET с URL­ aдpecoм grades/<str:id> к методу retrieve класса GradeViewSet. Используя этот файл, можно добавить дополнительные связывания URL-aдpecoв с функция­ ми/методами Python. На этом мы завершаем реализацию микросервиса Grades. Следующим шагом будет его запуск на веб-сервере Django для проверки работоспособности. Но перед этим убедимся, что объекты модели переданы в БД. Это действие эквивалентно инициа­ лизации БД в случае с Flask. В случае с Django нужно выполнить следующие ко­ манды для подготовки изменений и последующего их выполнения: pythonЗ manage.py makemigrations pythonЗ manage.py migrate Часто разработчики пропускают этот важный шаг, и при попытке запустить прило­ жение возникают ошибки. Нужно убедиться, что все изменения выполнены, преж­ де чем запускать веб-сервер следующей командой: pythonЗ manage.py runserver Команда запустит веб-сервер (порт вооо по умолчанию) на нашем локальном хосте. Обратите внимание, настройки по умолчанию, включая БД и веб-сервер с атрибу­ тами хоста и порта, могут быть изменены в файле settings .ру. Кроме того, рекомен­ дуется настроить пользовательский аккаунт для приложения admin следующей ко­ мандой: pythonЗ manage.py createsuperuser Эта команда предложит выбрать имя пользователя, e-mail адрес и пароль для акка­ унта администратора. Когда микросервис начнет выполнять функции, мы упакуем\n--- Страница 342 ---\nГлава.11. Разработка микросервисов на Python 349 Здесь мы не будем подробно рассматривать работу Docker Engine и Docker Compose, но рекомендуем вам познакомиться с ними поближе в документации по адресу: https://docs.docker.com/. Повторное использование АРl-приложения Students Мы будем повторно использовать АРl-приложение students, которое разработали в предыдущей главе. Оно запустится вместе со своим встроенным сервером и будет называться микросервисом Students. В нем не будет никаких изменений. Обновление веб-приложения Students Приложение webapp, разработанное в предыдущей главе, использует только apiapp через REST API. В новой его версии будут использованы микросервисы Grades и Students для получения списка объектов Grade и списка объектов student. Функция list (файл webapp.py) будет объединять два списка объектов для предоставления веб-клиентам дополнительной информации. Функция будет выглядеть следующим образом: STUDENTS_MS = http://localhost:8080/students GRADES_МS = \"http://localhost:8000/grades\" @app.get { '/') def list {): student_svc_resp = requests.get{STUDENTS_MS) students = json.loads{student_svc_resp.text) grades_svc_resp = requests.get(GRADES_МS) grades_list = json.loads(grades_svc_resp.te xt) grades_dict = {cls_item['grade']: cls_item for cls_item in grades_list} for student in students: student [ 'building' ] = grades_dict [student [ 'grade']] [ 'building' ] student['teacher'] = grades_dict[student['grade']] ['teacher'] return render_template{'main .html', students =students) В новом коде мы создали словарь grades, используя словарное включение из списка объектов Grades. Этот словарь будет использоваться для вставки атрибутов grade в объекты student перед их отправкой в шаблон Jinja для отображения. В главном шаблоне (main.html) добавлены два дополнительных столбца (вuilding и тeacher) в таблицу Students, как показано на рис. 11.6. Мы рассмотрели создание микросервиса, а также его развертывание как в контей­ нере Docker, так и в качестве веб-приложения на веб-сервере. Кроме того, мы объе­ динили результаты двух микросервисов в веб-приложении.\n--- Страница 343 ---\n348 Раздел 4. Python для веб-раэработки, облака и сети 3.Создадим файл Dockerfile, который будет использоваться Docker Engine для соз­ дания нового образа контейнера. В нашем случае его содержимое будет сле­ дующим: FROM python:3.8-slim ENV PYTHONUNВUFFERED 1 WORКDIR /арр СОРУ requirements.txt /app/requirements.txt RUN pip install -r requirements.txt СОРУ . /арр СМD python manage.py runserver 0.0.0.0:8000 4.Первая строка устанавливает базовый образ ( Python: з. 8-slim) для контейнера, ко­ торый уже доступен в репозитории Docker. Вторая строка задает переменную окружения для удобства логирования. Остальные строки говорят сами за себя, поскольку это команды Unix. 5.Далее создадим файл Docker Compose (ctocker-co mpose.yml): version: '3. 7' services: gradesms: build: context: dockerfile: Dockerfile ports: -8000:8000 volumes: - . : /арр 6.Это У АМL-файл, в котором контейнеры определены как сервисы. Поскольку у нас только один контейнер, мы определили сервис gradesms. Обратите внимание, build указывает на только что созданный Dockerfile и предполагает, что тот на­ ходится в том же каталоге, что и файл docker-compose.yml. Порт контейнера 8000 сопоставляется с портом веб-сервера 8000. Это важный шаг, который позволяет разрешить трафик в приложение внутри контейнера. 7.Помещаем текущий каталог (.) в каталог / арр внутри контейнера. Это позволит отобразить сделанные в системе изменения в контейнер и наоборот. Этот шаг важен, если мы создаем контейнеры во время цикла разработки. 8.Запустить контейнер можно следующей командой Docker Compose: docker-compose up 9.Команда создаст новый образ контейнера. Для загрузки базового образа из рее­ стра Docker потребуется доступ в Интернет. После создания образ автоматиче­ ски будет запущен.\n--- Страница 344 ---\nГлава 11. Разработка микросервисов на Python RUN pip install Flask gunicorn #Запуск веб-сервера при старте контейнера. Мы будем использовать #gunicorn и привяжем наше api_app как основное приложение CMD ехес gunicorn --bind:$PORT --workers 1 --threads 8 api_app:app 351 3.Этого файла будет достаточно для развертывания приложения в GCP Cloud Run. Но сначала нужно создать образ контейнера с помощью GCP Cloud SDK. Для этого требуется создать проект GCP с помощью Cloud SDK или консоли GCP. Шаги по созданию проекта GCP и привязке к нему биллинг-аккаунта для выставления счетов мы рассматривали в предыдущих главах. Будем предпола­ гать, что у нас уже есть проект с именем students-run в GCP. 4.Когда проект готов, можно создать образ контейнера АРI-приложения students следующей командой: gcloud builds submit --tag gcr.io/students-run/students 5.Обратите внимание, gcr расшифровывается как Google Container Registry (Ре­ естр контейнеров Google ). 6.Для создания образа необходимо указать атрибут tag в следующем формате: <hostname>/<Project ID>/<Image name> 7.В нашем случае имя хоста будет gcr. io, и он находится в США. Можно также использовать локально созданный образ, но сначала нужно задать атрибут tag в указанном выше формате, а затем отправить его в реестр Google. Это можно сделать, выполнив следующие команды Docker: docker tag SOURCE_IМAGE <hostname>/<Project ID>/<Image name>:tagid docker push <hostname>/<Project ID>/<Image name> #или, если мы хотим отправить конкретный тег docker push <hostname>/<Project ID>/<Image name>:tag 8.Как видно, команда gcloud build может выполнить два шага в одной команде. 9.Далее запустим загруженный образ контейнера. Это можно сделать следующей командой Cloud SDK: gcloud run deploy --image gcr.io/students-run/students 10.Выполнение образа также можно запустить из консоли GCP. Как только кон­ вейер будет успешно развернут и запущен, выходные данные этой команды (или консоли GCP) будут включать URL-aдpec микросервиса. Для использования новой версии микросервиса Students из GCP Cloud Run следует обновить веб-приложение, переключившись на URL-aдpec нового развернутого сервиса в GCP Cloud Run. Если протестировать веб-приложение с помощью ло­ кально развернутого микросервиса Grades и удаленно развернуть микросервис students, мы получим те же результаты, что и на рис. 11.6, и сможем выполнить все операции, как если бы микросервис Students был развернут локально.\n--- Страница 345 ---\n350 Раздел 4. Python для веб-разработки, облака и сети Students Add а Student First name Grade - Students Gl'llde Вulldlng 1 .!ом Lee 10 2 8rian Miles 7 Bullding 110 NA Mlss Нannah MlssFflY 1 Update 11 Deltll 1 1 Uimt• 11 Deltte ! Рис. 11.6. Обновленная главная страница с данными о корпусе (Building) и учителе (Teacher) Развертывание микросервиса Students в GCP Cloud Run Пока что мы использовали микросервис Students в качестве веб-приложения с REST API, размещенного на сервере Flask. Настало время поместить его в контейнер и развернуть в GCP. Google Cloud Platform имеет движок среды выполнения (Cloud Run) для развертывания контейнеров и запуска их в качестве сервисов (микросер­ висов). Необходимые для этого шаги описаны ниже: 1.Для упаковки кода Students в контейнер сначала нужно определить список зави­ симостей и экспортировать их в файл requirements. txt, выполнив следующую команду в виртуальной среде проекта: pip freeze -> requirements.txt 2.Затем создадим Dockerfile в корневом каталоге проекта, подобно тому который мы подготовили для микросервиса Grades. Содержимое Dockerfile будет следующим: FROM python:3.8-slim ENV PYTHONUNBUFFERED True WORКDIR /арр СОРУ / #Установка производственных )рабочих) зависимостей. RUN pip install -r requirements.txt\n--- Страница 346 ---\nГлава 11. Разработка микросервисов на Python 353 Ответы 1.Можно, но лучше развернуть его в контейнере. 2.Технически это осуществимо, но это не лучший вариант. Сбой базы данных приведет к остановке обоих микросервисов. 3.Docker Compose -это инструмент для развертывания и запуска контейнерных приложений с помощью файла У АМL. Он предоставляет простой формат для определения различных сервисов (контейнеров) с атрибутами развертывания и выполнения. 4.REST API - самый популярный интерфейс обмена данными, но не единствен­ ный. Микросервисы также могут использовать RPC и протоколы на основе со­ бытий для обмена данными.\n--- Страница 347 ---\n352 Раздел 4. Python для веб-разработки, облака и сети На этом можно завершить обсуждение, как со.здавать микросервисы с помощью различных фреймворков Python, развертывать их локально и в облаке, а также как использовать их в веб-приложении. Заключение В этой главе мы познакомились с микросервисной архитектурой и обсудили ее дос­ тоинства и недостатки. Рассмотрели рекомендации по созданию, развертыванию и эксплуатации микросервисов. А также проанализировали варианты развертывания, доступные в Python, включая Flask, Django, Falcon, Nameko, Bottle и Tomado. Мы выбрали Flask и Django для создания микросервисов. Для реализации нового при­ мера был использован Django вместе с его Django REST Framework (DRF). В этой реализации микросервиса мы также увидели, как в целом работает фреймворк Django. Затем мы подробно изучили, как упаковать созданный микросервис в кон­ тейнер с помощью Docker Engine и Docker Compose. В конце мы преобразовали АРI-приложение Students в образ Docker и развернули его в GCP Cloud Run. Кроме того, мы обновили веб-приложение Students и задействовали два микросервиса, развернутых в разных уголках мира. Примеры кода для этой главы наглядно демонстрируют создание и развертывание микросервисов для разных сред. Эти знания полезны всем, кто хочет создавать приложения на базе микросервисов. В следующей главе мы рассмотрим, как ис­ пользовать Python для разработки бессерверных функций, которые являются еще одной парадигмой разработки ПО для облака. Вопросы 1.Можно ли развернуть микросервис без контейнера? 2.Могут ли два микросервиса использовать одну базу данных, но с разной схемой? 3.Что такое Docker Compose и как он помогает развертывать микросервисы? 4.Является ли REST единственным форматом обмена данными для микросервисов? Дополнительные ресурсы ♦«Нands-On Docker for Microservices with Python», автор: Джейми Буэльта (Jaime Buelta). ♦«Python Microservices Development», автор: Тарек Зиаде (Tarek Ziade). ♦«Предметно-ориентироваююе проектирование: структуризация сложных про­ граммных систем», автор: Эрик Эванс (Eric Evans). ♦Краткие руководства по созданию и развертыванию микросервисов в Google Cloud Run: https://cloud.google.com/runldocs/quickstartsl.",
      "debug": {
        "start_page": 332,
        "end_page": 347
      }
    },
    {
      "name": "Глава 12. Создание бессерверных функций на Python 354",
      "content": "--- Страница 348 --- (продолжение)\nГлава 12. Создание бессерверных функций на Python Технические требования В этой главе понадобится: ♦Python 3. 7 или более поздней версии. ♦Аккаунт GCP (достаточно бесплатной пробной версии) для развертывания бес- серверной функции в Cloud Functions. ♦Аккаунт (бесплатный) в SendGrid для отправки электронной почты. Пример кода для этой главы находится по адресу: https://github.com/PacktPuЬ/ishing/Python-for-Geeks/tree/master/Chapterl2. Сначала узнаем, что такое бессерверная функция. Знакомство с бессерверными функциями Бессерверная функция -это модель, которая позволяет разрабатывать и выполнять программные компоненты или модули, не беспокоясь о платформе хостинга. Эти модули и компоненты известны как лямбда-функции или облачные функции. Amazon стал первым провайдером, предложившим их на такой платформе, как А WS Lambda. За ним последовали Google и Microsoft, представив Google Cloud Functions и Azure Functions соответственно. Как правило, бессерверная функция состоит из четырех компонентов (рис. 12.1): Событие Результат Внешние ресурсы Рис. 12.1. Компоненты бессерверной функции Рассмотрим компоненты подробнее: ♦Функциональный код: программный модуль, который выполняет определен­ ные задачи в соответствии с бизнес-целями или функциональными требования­ ми; например, можно написать функцию для обработки входного потока данных\nГлава 12. Создание бессерверных функций на Python Технические требования В этой главе понадобится: ♦Python 3. 7 или более поздней версии. ♦Аккаунт GCP (достаточно бесплатной пробной версии) для развертывания бес- серверной функции в Cloud Functions. ♦Аккаунт (бесплатный) в SendGrid для отправки электронной почты. Пример кода для этой главы находится по адресу: https://github.com/PacktPuЬ/ishing/Python-for-Geeks/tree/master/Chapterl2. Сначала узнаем, что такое бессерверная функция. Знакомство с бессерверными функциями Бессерверная функция -это модель, которая позволяет разрабатывать и выполнять программные компоненты или модули, не беспокоясь о платформе хостинга. Эти модули и компоненты известны как лямбда-функции или облачные функции. Amazon стал первым провайдером, предложившим их на такой платформе, как А WS Lambda. За ним последовали Google и Microsoft, представив Google Cloud Functions и Azure Functions соответственно. Как правило, бессерверная функция состоит из четырех компонентов (рис. 12.1): Событие Результат Внешние ресурсы Рис. 12.1. Компоненты бессерверной функции Рассмотрим компоненты подробнее: ♦Функциональный код: программный модуль, который выполняет определен­ ные задачи в соответствии с бизнес-целями или функциональными требования­ ми; например, можно написать функцию для обработки входного потока данных\n--- Страница 349 ---\n12 Создание бессерверных функций на Python Бессерверные вычисления (Serverless Computing) -это новая модель облачных вычислений, в которой управление физическими или виртуальными серверами и инфраструктурным ПО (например, СУБД) отделено от самого приложения. Разра­ ботчики могут сосредоточиться исключительно на написании кода, пока ресурсами инфраструктуры управляет кто-то другой. Облачные провайдеры -лучший вари­ ант для внедрения этой модели. Контейнеры не только подходят для сложных раз­ вертываний, но и являются прорывной технологией для эры бессерверных вычис­ лений. В дополнение к контейнерам существует еще-одна<fiорма бессерверных вы­ числений -функция как услуга (Function as а Service , FaaS). Согласно этой парадигме, облачные провайдеры предлагают платформу для разработки и запуска функций приложения или бессерверных функций, которые выполняются обычно в ответ на событие, или прямой их вызов. Этот сервис предоставляют все публичные облачные провайдеры, например, Amazon, Google, Microsoft, IВМ и Oracle. В этой главе основное внимание будет уделено пониманию и созданию бессерверных функций с использованием Python. Темы этой главы: ♦Знакомство с бессерверными функциями. ♦Варианты развертывания бессерверных функций. ♦Создание бессерверных функций на практическом примере. Из этой главы вы узнаете о роли бессерверных функций в облачных вычислениях и научитесь писать их на Python.\n--- Страница 350 ---\nГлава 12. Создание бессерверных функций на Python режиме в реальном времени. В частности, бессерверные функции можно интегри­ ровать с датчиками Интернета вещей (lnternet of Things, loT), которые исчисля­ ются тысячами. Бессерверные функции способны эффективно обрабатывать запросы от такого количества источников. В мобильном приложении они могут использо­ ваться в качестве бэкенд-сервиса (Backend Service) для выполнения определенных задач или обработки данных без ущерба для ресурсов мобильного устройства. Еще одним из примеров является виртуальный ассистент Amazon Alexa. Невозможно вместить абсолютно все навыки и интеллект в само устройство, поэтому здесь ис­ пользуются функции Amazon Lambda. Кроме того, они обеспечивают возможность масштабирования в зависимости от спроса. Некоторые функции могут использо­ ваться чаще других, например, запросы на прогноз погоды. В следующем разделе рассмотрим различные варианты развертывания бессервер­ ных функций. Варианты развертывания бессерверных функций Использование виртуальной машины или другого ресурса среды выполнения в пуб­ личных облаках для приложений, к которым обращаются нерегулярно, может ока­ заться коммерчески непривлекательным решением. В таких ситуациях на помощь приходят бессерверные функции. Облачный провайдер предлагает динамически управляемые ресурсы для приложения и взимает плату только за время их фактиче­ ского использования в ответ на событие. Иными словами, бессерверная функция - это метод бэкенд-вычислений, который предоставляется по запросу, оплачивается по факту использования и предлагается только в публичных облаках. Мы рассмотрим несколько вариантов развертывания бессерверных функций в облаке: 1.А WS Lambda: это считается одним из первых подобных сервисов. Функции AWS Lambda можно писать на Python, Node.js, PowerShell, Ruby, Java, С# и Go. Они могут выполняться в ответ на такие события, как загрузка файлов в храни­ лище Amazon SЗ, уведомление от Amazon SNS или прямой вызов API. Функции А WS Lambda не хранят состояние. 2.Azure Functions: были представлены почти через два года после запуска А WS Lambda. Эти функции можно привязывать к событиям в облачной инфраструк­ туре. Microsoft поддерживает их создание и отладку с помощью Visual Studio, Visual Studio Code, IntelliJ и Eclipse. Функции Azure можно писать на С#, F#, Node.js, PowerShell, РНР и Python. Кроме того, Microsoft предлагает устойчивые функции (DuraЫe Functions), которые позволяют писать функции с отслежива­ нием состояния в бессерверном окружении. 3.Google Cloud Functions: GCP предлагает Google Cloud Functions как бессервер­ ные функции, которые могут быть написаны на Python, Node.js, Go, .NET, Ruby и РНР. Как и конкуренты, облачные функции от Google могут запускаться\n--- Страница 351 ---\n356 Раздел 4. Python для веб-разработки, облака и сети или создать запланированное действие для проверки определенных ресурсов в целях мониторинга. ♦События: бессерверные функции не предназначены для использования в каче­ стве микросервисов; они запускаются триггером, который может быть иниции­ рован событием в системе «издатель-подписчик» (PuЫish-Subscribe system, PuЬ/Sub system), или поступать как НТТР-вызов на основе внешнего события, такого, как срабатывание датчика. ♦Результат: когда бессерверная функция запущена и выполняет свою задачу, ре­ зультатом может быть или простой ответ вызывающему объекту, или запуск другого действия для смягчения последствий изначального события; примером такого результата может служить запуск другого облачного сервиса, например, службы СУБД, или отправка электронной почты. ♦Ресурсы: иногда функциональный код использует дополнительные ресурсы для выполнения задачи, например, сервис БД или облачное хранилище для доступа к файлам. Преимущества бессерверных функций Бессерверные функции позволяют использовать все преимущества бессерверных вычислений: ♦Простота разработки: они избавляют разработчиков от инфраструктурных сложностей, позволяя сосредоточиться на функциональном аспекте программы. ♦Встроенная масштабируемость: они поставляются со встроенной масштаби­ руемостью, позволяющей справиться с любым ростом трафика в любое время. ♦Эффективность затрат: они не только снижают расходы на разработку, но также предлагают оптимизированное развертывание и рабочий режим; как пра­ вило, это модель оплаты по факту использования, то есть плата взимается толь­ ко за время, в течение которого выполняется функция. ♦Независимость от технологий: они совместимы с любыми технологиями. их можно создавать на множестве языков программирования с использованием различных облачных ресурсов. Обратите внимание, бессерверные функции имеют также некоторые ограничения. Например, на уровне системы контроль будет меньше, а устранение неполадок - сложным. Варианты использования Существует несколько возможных вариантов применения бессерверных функций. Например, их можно использовать для обработки данных, когда получено событие загрузки файла в облачное хранилище или когда данные поступают в потоковом\n--- Страница 352 ---\nГлава 12. Создание бессерверных функций на Python Создание облачной функции на основе НТТР с помощью консоли GCP Сначала создадим простую облачную функцию, которая предоставляет текущую дату и время для НТТР-триггера. Обратите внимание, НТТР-триггер -это самый простой способ вызова облачной функции. Для начала нам потребуется проект GCP. Можно создать новый на консоли GCP или использовать уже имеющийся. Инструкции по созданию проекта и привязке биллинг-аккаунта приведены в главе 9 ( «Программирование на Python для облака»). После п_одготовки проекта создание новой облачной функции будет состоять из трех шагов. Настройка атрибутов облачной функции При инициировании рабочего процесса Create Function на консоли GCP нам пред­ лагается указать определение облачной функции следующим образом (рис. 12.2): (···) Cloud Functions ф Conf1guration -О Code Basics FunctJon name • my-datetime Reglon asia•southeast1 Trigger QНТТР Trtggertype НТТР f-Сору function URL ГсJ https://asia-southeast1•students-run.cloudfunctions.net/my-datetlme Authentication @ Allow unauthenticated invocations Check tl,1s if you are creatlng а puЫic API or webslte. О Require authentication Manage authorlsed users w�h Cloud IAM. 0 Requlre НТТРS 8 - CANCEL ,,. О 1 Рис. 12.2. Действия по созданию облачной функции на консоли GCP (1/2) Определение облачной функции в общих чертах: 1.Мы указываем Function Name (в нашем случае это my-datetime) и выбираем Region для размещения функции.\n--- Страница 353 ---\n358 Раздел 4. Python для веб-разработки, облака и сети НТТР-запросами или событиями в инфраструктуре Google Cloud. Google позво­ ляет использовать Cloud Build для автоматического тестирования и развертыва­ ния Cloud Functions. Существуют и другие облачные провайдеры, которые предлагают свои решения для бессерверных функций. Например, IВМ предлагает Cloud Functions на базе проекта Apache Open Whisk с открытым исходным кодом. Oracle предлагает свою платформу бессерверных вычислений на основе проекта Fn, который также имеет открытый исходный код. Прелесть таких проектов в возможности разрабатывать и тестировать код локально. Кроме того, они позволяют без изменений переносить код из одного облака в другое или даже в локальное развертывание. Стоит упомянуть еще один известный фреймворк для бессерверных вычислений - Serverless Framework. Это не платформа развертывания, а программный инстру­ мент, который можно использовать локально при создании и упаковке кода для бессерверного развертывания, а затем использовать этот пакет для развертывания в любом публичном облаке. Этот фреймворк поддерживает такие языки программи­ рования, как Python, Java, Node.js, Go, С#, Ruby и РНР. В следующем подразделе мы напишем несколько бессерверных функций на Python. Написание бессерверных функций Здесь мы узнаем, как создавать бессерверные функции в одном из публичных обла­ ков. Несмотря на то что пионером бессерверных функций стал Amazon А WS, пред­ ставив в 2014 году AWS Lambda, в наших примерах мы будем использовать плат­ форму Google Cloud Functions. Такой выбор обусловлен тем, что мы уже хорошо знакомы с этой платформой из предыдущих глав и MO?l(HO использовать тот же ак­ каунт GCP для дальнейшего развертывания бессервернь�х-функций. Однако мы на­ стоятельно рекомендуем изучить и другие платформы, особенно если вы планируе­ те использовать их бессерверные функции в дальнейшем. Основные принципы по­ строения и развертывания на разных платформах аналогичны. GCP Cloud Functions предлагает несколько способов разработки и развертывания бессерверных функций (в дальнейшем в контексте GCP мы будем называть их об­ лачными функциями). В нашем примере мы рассмотрим два типа событий, которые можно описать следующим образом: ♦Первая облачная функция будет построена и развернута с помощью консоли GCP Console; она будет активироваться НТТР-вызовом (или событием). ♦Вторая облачная функция станет частью примера приложения, которое отсле­ живает событие в облачной инфраструктуре и в ответ на событие выполняет от­ правку электронной почты; в этом примере мы будем использовать Cloud SDK. Начнем создание облачной функции с помощью консоли GCP.\n--- Страница 354 ---\nГлава 12. Создание бессерверных функций на Python Создание облачной функции на основе НТТР с помощью консоли GCP Сначала создадим простую облачную функцию, которая предоставляет текущую дату и время для НТТР-триггера. Обратите внимание, НТТР-триггер -это самый простой способ вызова облачной функции. Для начала нам потребуется проект GCP. Можно создать новый на консоли GCP или использовать уже имеющийся. Инструкции по созданию проекта и привязке биллинг-аккаунта приведены в главе 9 ( «Программирование на Python для облака»). После п_одготовки проекта создание новой облачной функции будет состоять из трех шагов. Настройка атрибутов облачной функции При инициировании рабочего процесса Create Function на консоли GCP нам пред­ лагается указать определение облачной функции следующим образом (рис. 12.2): ( ·) Cloud Functions ф Configuration -О Code Basics Functlon name • my-datetime Reglon -. ' asia,southeast1 Trigger QНТТР .· Trtggertype НТТР � Сору function URL ГQ https://asia-southeast1-students-Nn.cloudfunctions.neVmy-<!atetime Authentication @ Allow unauthenticated invocations Che<:k thls if you are creating а puЫic АР! or webslte. О Require authentication Manage authorlsed users with Cloud IAM. О Require НТТРS О - CANCEL о Рис. 12.2. Действия по созданию облачной функции на консоли GCP (1/2) Определение облачной функции в общих чертах: 1.Мы указываем Function Name (в нашем случае это my-datetime) и выбираем Region для размещения функции.\n--- Страница 355 ---\n360 Раздел 4. Python для веб-разработки, облака и сети 2.Для параметра Trigger type выбираем нттР. Выбор триггера для функции - са­ мый важный шаг. Есть и другие доступные триггеры, например, Cloud PuЬ/Sub и Cloud Storage. На момент написания книги в GCP были добавлены еще не­ сколько триггеров для ознакомления. 3.Для простоты мы не будем включать ау,:ентификацию доступа к нашей функции. После нажатия на кнопку Save будет предложено ввести параметры RUNTIME, BUILD AND CONNECTIONS SETTINGS (для настройки выполнения, сборки и подключений соответственно) (рис. 12.3): RUNТIME, BUILO ANO CONNECTIONS SETTINGS RUNТIME [ Memory ellocated • . 128MiB BUILD CONNECТIONS л ( Tlmeout* ------ ------ ---� ] l 60 seconds � Runtime service account • [ R1.11tlme servlce account �рр Engine default service account Auto-scaling • ( Maxlmum numЬer of lnstances -----------. l 1 Runtime environment variaЫes • +AOD VARIABLE 1111 CANCEL Рис. 12.3. Действия по созданию облачной функции на консоли GCP (2/2) Эти параметры можно указать следующим образом: 1.RUNТIМЕ-атрибуты можно оставить по умолчанию, кроме Memory allocated, который будет уменьшен до 128 МiВ. Мы выбрали Арр Engine default service account в параметре Runtime service account. Для Auto-scaling мы оставим зна­ чение по умолчанию, но можно задать максимальное количество экземпляров. 2.На вкладке RUNTIМE можно добавить переменные окружения (Runtime environment variaЬles), если это необходимо. Для нашей функции мы не будем их добавлять. 3.На вкладке BUILD можно добавить переменные окружения для сборки (Build environment variaЬles), но мы также не будем ничего добавлять. 4.На вкладке CONNECTIONS можно оставить параметры по умолчанию и, таким образом, разрешить всему трафику доступ к облачной функции.\n--- Страница 356 ---\nГлава 12. Создание бессерверных функций на Python После настройки вьшолнения, сборки и подключений следующим шагом будет до­ бавление кода реализации. Добавление кода Python в облачную функцию После нажатия на кнопку Next, как показано на рис. 12.3, GCP Console предложит определить или добавить детали реализации функции (рис. 12.4): (•·•) Cloud Functions � Edit function ф Configuratlon -8 Code Runtime - Python 3.8 Sourcecode CD lnline Editor , 1 + 8 main.py 1iiJ requirements.txt PREVIOUS - CANCEL 1 2 з 4 6 7 в 9 10 11 12 13 14 15 16 17 ( Entry polnt . , 8 ! today_datatime from datet1me 1Jllport date, datet1me def today_datat1me( request): request_json • request.get_json() if request. args and · requester' in request. args: 1 requster_n811e = request.args.get( · requester') el1f request_json and 'requester' in request_Json: , requster_name = request_json[ · requester') requster _na11e � f' anonymous' today = date. today () now • datetime. now() resp = • {date: • + today. strft1me( '118 \\d, \\У\") + \\ ! · ·, t1111e: • + now.strftilne(\"\\Н:\\11:\\S\") + '}' return f' Hello · + requster _name + • ! Неге 1s today date and time: \\n\" + resp Рис. 12.4. Реализация облачной функции на консоли GCP Варианты для добавления кода Python: 1.Можно выбрать несколько вариантов среды выполнения (параметр Runtime), например, Java, РНР, Node.js или различные версии Python; мы выбрали Python 3.8. 2.Атрибут Entry point должен быть именем функции в коде. Google Cloud Functions будет вызывать функцию в коде на основе этого атрибута. 3.Исходный код Python может быть добавлен с помощью встроенного редактора Inline Editor справа. Его также можно загрузить, используя ZIР-файл с локаль­ ного компьютера или из облачного хранилища. Или указать расположение репо­ зитория GCP Cloud Source для исходного кода. В примере мы будем использо­ вать lnline Editor; 4.Для Python платформа GCP Cloud Functions автоматически создает два файла: main. ру и requirements. txt. Первый будет содержать код, а второй -зависимости от сторонних библиотек.\n--- Страница 357 ---\n362 Раздел 4. Python для веб-разработки, облака и сети ------------------ 5.Пример кода, показанный в lnline Editor, сначала проверяет, отправил ли вызы- вающий объект атрибут requester в НТТР-запросе. На основе значения этого ат­ рибута будет отправлено сообщение с текущей датой и временем. Мы реализо­ вали аналогичный пример кода с двумя отдельными веб-АРI, используя Flask в главе 9 («Программирование на Python для облака») для демонстрации возмож­ ностей GCP Арр Engine. Как только код написан, развернем функцию на платформе Google Cloud Functions. Развертывание облачной функции Следующий шаг -развернуть функцию, нажав на кнопку Deploy, как показано на рис. 12.4. GCP начнет развертывание немедленно. Это займет несколько минут. Важно понимать, Google Cloud Functions развертываются с помощью контейнеров так же как и микросервисы в GCP Cloud Run. Ключевое отличие в том, что их мож­ но вызывать с помощью разных типов событий, и они используют модель оплаты по мере использования. После развертывания функцию можно продублировать, протестировать или уда­ лить из списка Cloud Functions, как показано на скриншоте (рис. 12.5): (···) Cloud Functions Functions � Filter Filter functions о •Nвme 1' Region о о handle.storage_delete us-central1 о о handle_storage_upload us-central1 о о my-datetime us-eastl DcREATE FUNCTION Trigger Bucket: ch12-cfunc-testing Bucket: ch12-cfunc-testing НПР CREFRESH о Memory allocated Actions 256Mi8 256 MiB 128 м,в Го Сору function ► Test function s Viewlogs i Delete Рис. 12.5. Главное представление Google Cloud Functions 11 Далее кратко рассмотрим, насколько удобно тестировать и исправлять неполадки облачной функции с помощью GCP Console. Если нажать на кнопку Test function, консоль откроет тестовую страницу на вкладке TESTING (рис. 12.6). Для тестиро­ вания функции передадим атрибут requester в формате JSON: {\"requester\":\"John\"} Нажав на [ ]TEST ТНЕ FUNCTION, можно просмотреть результаты в разделе Output и записи журнала в разделе Logs в нижней части экрана (рис. 12.6). По-\n--- Страница 358 ---\nГлава 12. Создание бессерверных функций на Python скольку используется НТТР-триггер, также можно протестировать функцию с по­ мощью браузера или утилиты CURL прямо из Интернета. Однако следует убедиться, что облачная функция включает члена allUsers с ролью Cloud Functions Invoker. Эти параметры можно настроить на вкладке PERМISSIONS, но не рекомендуется это­ го делать без настройки механизма аутентификации: (••1 Cloud Functions MEl'RICS DEl'дlLS 1 (·r•queoter· :· Jolln\"} 2 1 f-Function details SOURCE VARIAВU:S /EDIТ 8 DEI.EТE IDCOPY TIUGGER PERMISSIONS LOGS ТESТING ( )н�;1 THF HJNCTION Testing 1n the Cloud Consofe h•• а 60-secood limeout. Note that this ls dlfferent from the lim� нt ln the function configu111tion. outpul O Complote S Н.llo Johnl Нtre 1• todoy doto ond t1\"\"': (date:July 18, 2821, t1.81: 87:14:52} Logo Oi:.tched (up to 100 -.trles).111-all lops Lotding Sc.nned up to 8-3/82/2821, tЭ:59. $conned 911 8. ► а 2821·87·18тt'l:24:S1.717mS2•z мy• dotet1м' kt8djoh-q2 Function QЮltion l'tlrted . - ► а 2821·87·18Т87:24:S2.8Зil451128Z мy-doteti88 ke8djoll-q2 Funotion execution t-316 ■s. f1nished with statu1 code: 288 Рис. 12.6. Тестирование облачной функции с помощью консоли GCP Создать облачную функцию с помощью консоли GCP довольно просто. Далее рас­ смотрим пример реального применения облачных функций. Практический пример: создание приложения для уведомлений о событиях в облачном хранилище В этом примере мы создадим облачную функцию , которая запускается для событий в Google Storage bucket. При получении события функция отправит уведомление по электронной почте списку адресатов. Схема приложения выглядит следующи м образом (рис. 12.7): lfжfl.118 ·О •1 { } ] SendGrid C::::J C::::J Сегмент Облачная функция Email-cnyжбa Google Storage Рис. 12.7. Облачная функция, ожидающая события в Google Storage bucket\n--- Страница 359 ---\n364 Раздел 4. Python для веб-разработки, облака и сети Обратите внимание, облачная функция может быть настроена на прослушивание одного или нескольких событий. Google Cloud Functions поддерживает следующие события в Google Storage: ♦finalize: это событие создается при добавлении или замене нового файла в хра­ нилище; ♦delete: это событие представляет собой удаление файла из хранилища; это отно­ сится к сегментам без контроля версий; обратите внимание, файл на самом деле не удаляется, а архивируется, если для сегмента хранилища все же настроен контроль версий; ♦archive: это событие возникает при архивировании файла; операция архивации запускается, когда файл удаляется или перезаписывается для сегментов храни­ лища с контролем версий; ♦metadata update: это событие возникает при обновлении метаданных файла. Получив событие от Google Storage bucket, облачная функция извлекает атрибуты из объектов context и event, переданных ей в качестве аргументов. Затем она ис­ пользует стороннюю службу электронной почты (например, SendGrid от Twilio) для отправки уведомлений. Предварительно у вас уже должен быть создан бесплатный аккаунт SendGrid (https://sendgrid.coml). В аккаунте должен быть создан хотя бы один пользователь­ отправитель. Также нужно настроить секретный ключ API, который можно исполь­ зовать вместе с облачной функцией для отправки электронных писем. Twilio SendGrid позволяет бесплатно отправлять до 100 сообщений в день, этого будет достаточно для тестирования. В примере мы напишем код для облачной функции локально, а затем развернем его на платформе Google Cloud Functions с помощью Cloud SDК. Мы будем делать это пошагово, как показано ниже: 1.Создадим сегмент хранения для привязки к нашей облачной функции и будем загружать или удалять файлы из него для генерации событий. Создать новый сегмент можно следующей командой Cloud SDK: gsutil mЬ gs://<bucket name> gsutil mЬ gs://muasif-testcloudfn #Образец сегмента создан 2.Для упрощения генерации событий отключим контроль версий в этом сегменте следующей командой: gsutil versioning set off gs://muasif-testcloudfn 3.Когда сегмент хранилища будет готов, создадим локальный каталог проекта и настроим виртуальную среду: python -m venv myenv source myenv/bin/activate 4.Затем установим пакет sendgrid для Python с помощью утилиты pip: pip install sendqrid\n--- Страница 360 ---\nГлава 12. Соэдание бессерверных функций на Python 5.После установки сторонних библиотек необходимо создать файл с зависимостя­ ми requirements. txt: pip freeze -> requirements.txt 6.Далее создадим новый файл main.py с функцией handle_storage_e vent. Она будет точкой входа для облачной функции. Код в ней выглядит следующим образом: #main.py from sendgrid import SendGridAPIClient from sendgrid.helpers.mail import Mail, Email, То, Content def handle_storage_event(event, context): from_email = Email(11abc@domainl.com11 ) to_emails = To(11zyz@domain2 .com11 ) suЬject = 11Your Storage Bucket Notification11 content = f11Bucket Impacted: { event [ 'bucket' ] } \\n 11 + \\ f11File Impacted: {event['name']} \\n 11 + \\ f\"Event Time: {event['timeCreated']} \\n\" + \\ f\"Event ID: {context.event_id} \\n\" + \\ f\"Event Туре: {context.event_type}\" mail = Мail(fran_email, to_emails, suЬject, content) sg = SendGridAPIClient() response = sg.send(mail) print(response.status_code) # for logging purpose print(response.headers) 7.Ожидается, что функция handle_storage_event будет получать объекты event и con­ text в качестве входных аргументов. Объект event -это словарь, который со­ держит данные события. Доступ к этим данным можно получить по ключам: bucket, name и timeCreated. Объект context предоставляет контекст события, на­ пример, event_id и event_type. Кроме того, мы используем библиотеку sendgrid для подготовки содержимого электронных сообщений, а затем отправки их с информацией о событии списку контактов. 8.Подготовив файлы main. ру и requirements. txt, можно инициировать операцию развертывания следующей командой Cloud SDK: gcloud functions deploy handle_storage_create \\ --entry-point handle_storage_event --runtime python38 \\ --trigger-resource gs://muasif-testcloudfn/\\ --trigger-event google.storage.oЬject.finalize --set-env-vars SENDGRID API КEY=<Your SEND-GRID КЕУ> 9.Эту команду нужно выполнить в проекте GCP с включенным биллингом, как обсуждалось ранее. Мы указали имя облачной функции handle_storage_cr eate, а атрибуту entry-point задали функцию handle _ storage _ event в коде Python. Для со­ бытия finalize задали trigger-event . С помощью set-env-vars задали SENDGRID _ API _ КЕУ для сервиса SendGrid.\n--- Страница 361 ---\n366 Раздел 4. Python для веб-раэработки, облака и сети 10.Команда deploy упакует код Python из текущего каталога, подготовит целевую платформу с учетом зависимостей из файла requirements. txt, а затем развернет код на платформе GCP Cloud Functions. В нашем случае мы также можем соз­ дать файл . gcloudignore для исключения файлов и каталогов, которые команда deploy должна игнорировать. 11.После развертывания нашей функции мы можем протестировать ее, загрузив файл с локального компьютера в сегмент хранилища следующей командой: gsutil ер testl.txt gs://muasif-testcloudfn 12.Как только копирование будет завершено, событие finalize запустит облачную функцию. В результате мы получим электронное письмо с информацией о со­ бытии. Также можно проверить логи Cloud Functions следующей командой: gcloud functions logs read --limit 50 В этом приложении облачная функция привязана только к событию finalize. Это обусловлено тем, что можно привязать одну облачную функцию только к одному событию. Однако функция -это объект развертывания, а не сам программный код. Нам не нужно снова писать или дублировать код Python для обработки собы­ тий другого типа. Если мы хотим получать уведомления о других типах событий (например, Delete), можно создать новую облачную функцию, используя тот же код, но уже для нового события: gcloud functions deploy handle_storage_delete \\ --entry-point handle_storage_event --runtime python38 \\ --trigger-resource gs://muasif-testcloudfn/ \\ --trigger-event google.storage.oЬject.delete --set-env-vars SENDGRID API КEY=<Your SEND-GRID КЕУ> Обратите внимание на эту версию команды deploy. Мы изменили только имя об­ лачной функции и тип события-триггера. Команда создаст новую облачную функ­ цию, которая будет работать параллельно с предыдущей , но будет запускаться уже по другому событию (в этом случае delete). Можно протестировать событие delete с новой облачной функцией, удалив загру­ женный файл (или любой другой файл) из сегмента хранилища следующей коман­ дой Cloud SDK: gsutil nn gs://muasif-testcloudfn/testl.txt Используя тот же код Python, можно создавать больше облачных функций для дру­ гих событий. На этом можно завершить обсуждение, как создавать облачные функции для собы­ тий в хранилище. Все описанные для Cloud SDK шаги можно реализовать и на GCP Console.\n--- Страница 362 ---\nГлава 12. Создание бессерверных функций на Python Заключение В этой главе мы познакомились с бессерверными вычислениями и моделью FaaS, а также проанализировали основные составляющие бессерверных функций. Мы рас­ смотрели их ключевые преимущества и недостатки. Кроме того, изучили некото­ рые варианты развертывания для бессерверных функций: А WS Lambda, Azure Functions, Google Cloud Functions, Oracle Fn и IВМ Cloud Functions. В заключитель­ ной части главы мы создали простую облачную функцию Google Cloud Functions на основе НТТР-триггера с помощью GCP Console. В конце мы создали приложение для уведомлений о событиях в хранилище Google и облачную функцию с помощью Cloud SDК. Примеры кода к этой главе помогут понять, как использовать GCP Console и Cloud SDK для создания и развертывания облачных функций. Эти знания понадобятся всем, кто хочет развиваться в сфере бессерверных вычислений. В следующей главе мы рассмотрим, как использовать Python с машинным обучени­ ем (Machine Learning). Вопросы 1.Чем бессерверные функции отличаются от микросервисов? 2.Какое практическое применение бессерверных функций в реальных примерах? 3.Что такое устойчивые функции и какой облачный провайдер их предлагает? 4.Может ли одна облачная функция быть привязана к нескольким триггерам? Дополнительные ресурсы ♦«Serverless Computing with Google Cloud», автор: Ричард Роуз (Richard Rose). ♦«Mastering AWS Lambda», автор: Йохан Вадия (Yohan Wadia). ♦«Mastering Azure Serverless Computing», авторы: Лоренцо Барбьери (Lorenzo BarЬieri) и Массимо Бонанни (Massimo Bonanni). ♦Руководства по созданию облачных функций на платформе Google Cloud Functions https ://cloud.google. com/functionsldocs/quickstarts. Ответь� 1.Это разные варианты бессерверных вычислений. Обычно бессерверные функции запускаются событием и оплачиваются по мере использования. Микросервисы\n--- Страница 363 ---\n368 Раздел 4. Python для веб-раэработки, облака и сети обычно используются через вызовы API, и модель оплаты по мере использова­ ния к ним не применяется. 2.Amazon Alexa использует функции А WS Lambda для предоставления своим пользователям различных возможностей. 3.Устойчивые функции -эtо расширение Azure Functions от Microsoft, которое предлагает возможность отслеживать состояния в бессерверном окружении. 4.Нет, не может. Одну облачную функцию можно привязать только к одному триггеру.\n--- Страница 364 ---\n13 Python и машинное обучение Машинное обучение (Machine Learning, ML) -это построение моделей, основан­ ное на искусственном интеллекте (ИИ, Artificial Intelligence, AI) и на изучении закономерностей в данных, а также использование этих моделей для прогнозирова­ ния. Это одна из самых популярных методик ИИ, которая используется во многих сферах, например, в медицинской диагностике, обработке изображений, распозна­ вании речи, прогнозировании угроз, сборе данных, классификации и многих дру­ гих. Важность и польза машинного обучения очевидны. Python, будучи лаконич­ ным, но очень мощным языком, широко используется для реализации моделей ML. Язык имеет в своем арсенале такие библиотеки обработки и подготовки данных, как NumPy, Pandas и PySpark, что делает его предпочтительным выбором для раз­ работчиков при создании и обучении моделей. В этой главе мы рассмотрим, как использовать Python для задач машинного обуче­ ния оптимизированным способом. Это особенно важно, поскольку обучение моде­ лей требует больших вычислительных ресурсов, а оптимизация кода имеет осново­ полагающее значение при выборе Python для задач ML. Темы этой главы: ♦Введение в машинное обучение. ♦Использование Python для машинного обучения. ♦Тестирование и оценка моделей машинного обучения. ♦Развертывание моделей машинного обучения в облаке. Из этой главы вы узнаете, как использовать Python для создания, обучения и оцен­ ки моделей ML, а также развертывать их в облаке и использовать для прогнозиро­ вания.\n--- Страница 365 ---\n370 Раздел 4. Python для веб-разработки, облака и сети Технические требования Для этой главы понадобятся: ♦Python 3. 7 или более поздней версии. ♦Дополнительные библиотеки машинного обучения -SciPy, NumPy, Pandas и scikit-learn. ♦Аккаунт GCP для развертывания модели на платформе AI Platform (достаточно будет бесплатной пробной версии). Примеры кода для этой главы находятся по адресу: https :l/github. com/PacktPuЬlish ing/Python-for-Geeks/tree/master/Chapter 13. Начнем со знакомства с машинным обучением. Введение в машинное обучение В традиционном программировании мы предоставляем коду данные и набор пра­ вил их обработки для получения желаемого результата. Машинное обучение - принципиально иной подход, в котором мы предоставляем данные и желаемый ре­ зультат в качестве входных данных для получения набора правил. В машинном обучении это называется моделью. Концепция представлена на следующей схеме (рис. 13.1): Результат ( Правила i Данные L L. [ Результат j Рис. 13.1. Традиционное программирование и машинное обучение Правила Для понимания, как работает ML, необходимо познакомиться с его основными компонентами: 1.Набор данных: без хорошего набора ничего не получится. Данные -настоя­ щая сила ML. Они должны иметь широкое разнообразие и охватывать самые разные ситуации для представления модели, наиболее близкой к реальному про­ цессу или системе. Еще одно требование -данных должно быть много (поряд­ ка нескольких тысяч записей). Кроме того, они должны быть максимально точ­ ными и содержать значимую информацию. Данные используются для обучения системы, а также оценки ее точности. Информацию можно собирать из многих источников, и в большинстве из них они представлены в необработанном виде (сырые данные). Но их можно обработать, используя библиотеки вроде Pandas, как уже упоминалось ранее.",
      "debug": {
        "start_page": 348,
        "end_page": 365
      }
    },
    {
      "name": "Глава 13. Python и машинное обучение 369",
      "content": "--- Страница 366 --- (продолжение)\nГлава 13. Python и машинное обучение 371 2.Извлечение признаков: Перед использованием данных для создания модели нужно понять, какого они типа и как структурированы. Как только это сделано, можно выбрать, какие признаки будут использованы алгоритмом для построе­ ния модели. Также можно вычислить дополнительные признаки на основе ис­ ходного набора. Например, если есть необработанное изображение в виде пик­ селей, само по себе оно может не подойти для обучения. Но можно использовать длину или ширину формы внутри изображения в качестве признаков для фор­ мирования правил. 3.Алгоритм: Это программа, которая используется для создания модели из дос­ тупных данных. С математической точки зрения алгоритм пытается изучить це­ левую функцию f(X), которая выражает отношение входных данных (Х) к вы­ ходным данным (у) следующим образом: y=f(X). 4.Существует несколько алгоритмов для разных типов задач и ситуаций, посколь­ ку универсального подхода нет. Среди популярных можно выделить линейную регрессию, деревья классификации и регрессии, а также метод опорных векто­ ров. В этой книге мы не будем рассматривать математические подробности ра­ боты алгоритмов. Но советуем для ознакомления изучить источники из раздела дополнительных ресурсов в конце главы. 5.Модель: это математическое или машинное представление процесса, происхо­ дящего в повседневной жизни. С точки зрения машинного обучения это резуль­ тат работы алгоритма, который мы применяем к набору данных. Результат (мо­ дель) может представлять собой набор правил или определенную структуру данных, которые можно использовать для прогнозирования любых реальных со­ бытий. 6.Обучение: это не новый компонент или шаг машинного обучения. Говоря про обучение, мы имеем в виду применение алгоритма ML к набору данных для соз­ дания модели. Говорят, что модель, которую мы получаем на выходе, обучена на определенном наборе данных. Существуют три способа обучения: •Обучение с учителем: мы предоставляем не только набор данных, но и же­ лаемый результат. Цель состоит в определении, как вход (Х) сопоставлен с выходом (у) на основе имеющихся данных. Такой подход используется в классификации и регрессии. Примерами таких задач являются классификация изображений или прогнозирование цен на жилье (регрессия). В первом слу­ чае можно научить модель определять вид животного на изображении (на­ пример, кошка или собака), на основе формы, длины и ширины объекта. Для этого надо пометить каждое изображение в обучающем наборе названием животного. Во втором случае для прогнозирования цен необходимо предос­ тавить данные о домах в районе, где они находятся, например, расположение, количество комнат и санузлов и т. д. •Обучение без учителя: в этом случае мы обучаем модель, не зная желаемого результата. Обучение без учителя обычно применяется в задачах кластериза-\nГлава 13. Python и машинное обучение 371 2.Извлечение признаков: Перед использованием данных для создания модели нужно понять, какого они типа и как структурированы. Как только это сделано, можно выбрать, какие признаки будут использованы алгоритмом для построе­ ния модели. Также можно вычислить дополнительные признаки на основе ис­ ходного набора. Например, если есть необработанное изображение в виде пик­ селей, само по себе оно может не подойти для обучения. Но можно использовать длину или ширину формы внутри изображения в качестве признаков для фор­ мирования правил. 3.Алгоритм: Это программа, которая используется для создания модели из дос­ тупных данных. С математической точки зрения алгоритм пытается изучить це­ левую функцию f(X), которая выражает отношение входных данных (Х) к вы­ ходным данным (у) следующим образом: y=f(X). 4.Существует несколько алгоритмов для разных типов задач и ситуаций, посколь­ ку универсального подхода нет. Среди популярных можно выделить линейную регрессию, деревья классификации и регрессии, а также метод опорных векто­ ров. В этой книге мы не будем рассматривать математические подробности ра­ боты алгоритмов. Но советуем для ознакомления изучить источники из раздела дополнительных ресурсов в конце главы. 5.Модель: это математическое или машинное представление процесса, происхо­ дящего в повседневной жизни. С точки зрения машинного обучения это резуль­ тат работы алгоритма, который мы применяем к набору данных. Результат (мо­ дель) может представлять собой набор правил или определенную структуру данных, которые можно использовать для прогнозирования любых реальных со­ бытий. 6.Обучение: это не новый компонент или шаг машинного обучения. Говоря про обучение, мы имеем в виду применение алгоритма ML к набору данных для соз­ дания модели. Говорят, что модель, которую мы получаем на выходе, обучена на определенном наборе данных. Существуют три способа обучения: •Обучение с учителем: мы предоставляем не только набор данных, но и же­ лаемый результат. Цель состоит в определении, как вход (Х) сопоставлен с выходом (у) на основе имеющихся данных. Такой подход используется в классификации и регрессии. Примерами таких задач являются классификация изображений или прогнозирование цен на жилье (регрессия). В первом слу­ чае можно научить модель определять вид животного на изображении (на­ пример, кошка или собака), на основе формы, длины и ширины объекта. Для этого надо пометить каждое изображение в обучающем наборе названием животного. Во втором случае для прогнозирования цен необходимо предос­ тавить данные о домах в районе, где они находятся, например, расположение, количество комнат и санузлов и т. д. •Обучение без учителя: в этом случае мы обучаем модель, не зная желаемого результата. Обучение без учителя обычно применяется в задачах кластериза-\n--- Страница 367 ---\n372 Раздел 4. Python для веб-разработки, облака и сети ции и ассоциации. Этот тип обучения, в основном, строится на наблюдениях и определении групп или кластеров точек данных со схожими характеристи­ ками. Обучение без учителя широко используется интернет-магазинами, вро­ де Amazon, для деления клиентов на группы (кластеризация) по их покупа­ тельскому поведению, а затем для предложения товаров, которые могут их заинтересовать. Кроме того, интернет-магазины пытаются найти связь (ассо­ циация) между различными покупками. Например, насколько вероятно, что клиент, купивший товар А, купит и товар Б. •Обучение с подкреплением: модель вознаграждается за принятие правиль­ ного решения в конкретной ситуации. В этом случае не используется никаких данных, модель учится на собственном опыте. Популярный пример -беспи­ лотные автомобили. 7.Тестирование: для тестирования модели необходим набор данных, который не использовался для обучения. В традиционном подходе задействуются две трети имеющихся данных для обучения и одна треть -для тестирования. Кроме описанных трех подходов существует также глубокое обучение. Это более сложный вид машинного обучения, основанный на процессе получения человече­ ским мозгом определенных знаний с помощью алгоритмов нейронной сети. В этой главе мы будем строить образцы моделей, используя обучение с учителем. В следующем подразделе рассмотрим доступные в Python библиотеки для решения задач машинного обучения. Использование Python для машинного обучения Python пользуется популярностью в сообществе data science из-за его простоты, кросс-платформенной совместимости и богатой поддержки анализа и обработки данных с помощью библиотек. Одним из ключевых этапов в машинном обучении является подготовка данных для построения моделей, и Python в этом плане выиг­ рывает. Единственный минус состоит в том, что это интерпретируемый язык, по­ этому скорость выполнения кода медленнее по сравнению, например, с С. Но это не является серьезной проблемой, поскольку существуют библиотеки, позволяю­ щие увеличить скорость языка за счет параллельного использования нескольких ядер центрального (ЦП) или графического (ГП) процессоров. Далее рассмотрим несколько библиотек Python для ML. Библиотеки машинного обучения в Python Существует несколько МL-библиотек в Python. Некоторые из них уже упомина­ лись, например, NumPy, SciPy и Pandas. Они широко используются для уточнения, анализа и обработки данных. В этой главе мы кратко рассмотрим самые популяр­ ные из них для создания МL-моделей машинного обучения.\n--- Страница 368 ---\nГлава 13. Python и машинное обучение 373 scikit-learn Эта библиотека пользуется популярностью, поскольку имеет множество встроен­ ных алгоритмов машинного обучения и инструментов оценки их производительно­ сти. Она включает в себя алгоритмы классификации и регрессии для обучения с учителем, а также кластеризации и ассоциации для обучения без учителя. К тому же scikit-leam, в основном, написана на Python и использует библиотеку NumPy для многих операций. Новичкам рекомендуется начать именно, с нее, а затем перехо­ дить к более продвинутым, например, TensorFlow. Мы будем использовать scikit­ leam в целях демонстрации самой концепции, а именно, создания, обучения и оцен­ ки МL-моделей. Кроме того, scikit-leam предлагает алгоритмы градиентного бустинга (Gradient Boost, GB). Они основаны на математическом понятии градиента, который пред­ ставляет собой наклон функции. В контексте машинного обучения он измеряет из­ менения в ошибках. Идея алгоритмов GB заключается в итеративной тонкой на­ стройке параметров для нахождения локального минимума функции (минимизация ошибок для МL-моделей). Эти алгоритмы используют ту же стратегию для итера­ тивного улучшения новой модели, учитывая производительность предыдущей. Это происходит с помощью тонкой настройки параметров для новой модели, а также задания в качестве цели принятие новой модели, если она лучше минимизирует ошибки, чем предыдущая. XGBoost XGBoost ( eXtreme Gradient Boosting) -это библиотека алгоритмов, которая ис­ пользует деревья решений с градиентным бустингом. Она популярна, поскольку чрезвычайно быстра и предлагает наилучшую производительность по сравнению с другими реализациями алгоритмов GB и традиционными алгоритмами ML. Как и XGBoost, scikit-leam также предлагает алгоритмы градиентного бустинга, которые принципиально не отличаются, но первая работает значительно быстрее. В основ­ ном, это достигается за счет максимального использования параллелизма между ядрами одной машины или в распределенном кластере узлов. Библиотека также может упорядочить деревья решений во избежание подгонки модели к данным. XGBoost не является полноценным фреймворком ML, она предлагает, в основном, алгоритмы (модели). Ее можно задействовать, но придется также использовать и scikit-leam для остальных служебных функций и инструментов, таких, как анализ и подготовка данных. TensorFlow TensorFlow -еще одна очень популярная МL-библиотека с открытым исходным кодом, разработанная командой Google Brain для высокопроизводительных вычис­ лений. Она особенно полезна при обучении и работе с глубокими нейронными се­ тями, поэтому является популярным выбором в сфере глубокого обучения.\n--- Страница 369 ---\n374 Раздел 4. Python для веб-разработки, облака и сети Keras Это API с открытым исходным кодом для глубокого обучения нейронных сетей в Python. Keras -более высокоуровневый API поверх TensorFlow. Работать через него удобнее, чем использовать TensorFlow напрямую, поэтому он рекомендован на начальном этапе работы при разработке моделей ML с помощью Python. Keras мо­ жет работать как с центральными, так и графическими процессорами. PyTorch PyTorch -библиотека с открытым исходным кодом. Представляет собой реализа­ цию для Python популярной библиотеки Torch на С. В следующем подразделе мы кратко обсудим рекомендации и практики по исполь­ зованию Python в ML. Рекомендации по обучающим данным Ранее мы уже подчеркивали, как важны данные при обучении моделей. В этом подразделе изучим некоторые передовые методы и рекомендации по подготовке и использованию данных для обучения модели ML: ♦Большой набор данных (несколько тысяч записей или хотя бы несколько сотен) имеет ключевое значение; чем больше данных, тем точнее модель. ♦Перед началом любого обучения данные необходимо очистить и уточнить; в них не должно быть отсутствующих или неточных полей; для таких задач удобно использовать библиотеки Python, например, Pandas. ♦Важно использовать набор данных без нарушения конфиденциальности и безо­ пасности информации; необходимо убедиться, что не используются данные ка­ ких-либо других организаций без соответствующего разрешения. ♦ГП отлично подходят для обработки больших объемов данных, их использова­ ние даст более быстрый результат; такие библиотеки, как XGBoost, TensorFlow и Keras, задействуют графические процессоры для обучения. ♦При большом обучающем наборе важно эффективно использовать системную память; следует загружать данные порционно или· использовать распределенные кластеры; рекомендуем использовать функцию генератора как можно чаще. ♦Также при выполнении ресурсоемких ·задач рекомендуется следить за потребле­ нием памяти и периодически освобождать ее, заставляя сборщик мусора удалять неиспользуемые объекты. После рассмотрения библиотек и рекомендаций для машинного обучения перейдем к работе с реальными примерами.\n--- Страница 370 ---\nГлава 13. Python и машинное обучение 375 Создание и оценка модели машинного обучения Прежде чем приступить к написанию программы, исследуем процесс создания мо­ дели. Процесс построения модели машинного обучения Различные компоненты ML мы уже обсуждали в подразделе «Введение в машинное обучение». Процесс обучения использует эти элементы как входные данные для тренировки модели. Он состоит из трех этапов, а каждый этап из нескольких шагов, как показано на рис. 13.2: Данные Раэдеnение и уточнение данных Извлечение особенностей Анализ данных Данные для тест11рования Перекрестная проверка ! ! :::::::,.! Данные для обучения Моделирование : Тонкая : !настройка j [ •. гиперпараметров •• [ • Тестирование Рис. 13.2. Шаги по созданию модели машинного обучения при использовании классического подхода Рассмотрим все этапы и шаги подробнее: • ' 1.Анализ данных: на этом этапе происходит сбор необработанных данных и ,1ре­ образование их в форму, которую можно анализировать и использовать в даль­ нейшем для моделирования и тестирования. Некоторые данные могут быть от­ брошены, например, записи с пустыми значениями. Анализ позволяет выбрать признаки (атрибуты), которые можно использовать для выявления закономерно­ стей в данных. Извлечение признаков -важный шаг. От них зависит успеш­ ность построения модели. Во многих случаях приходится корректировать при­ знаки после тестирования, с целью убедиться что набор характерных признаков верный. Традиционно данные разделяют на два набора, один из которых исполь­ зуется для обучения на этапе моделирования, а другой -для проверки точности обученной модели на этапе тестирования. Последний этап может быть пропу­ щен, если оценка модели выполняется другим подходом, например, с помощью кросс-валидации (Cross-validation). Мы рекомендуем не пропускать этап тести-\n--- Страница 371 ---\n376 Раздел 4. Python для веб-разработки, облака и сети рования и оставить часть данных (скрытых от модели), как показано на схеме выше. 2.Создание модели: этот этап посвящен обучению модели на основе данных и признаков, извлеченных на предыдущем шаге. В традиционном подходе обу­ чающие данные используются как есть, но для повышения точности можно за­ действовать следующие дополнительные методы: •Можно разделить обучающие данные на срезы и использовать один для оценки модели, а остальные -для обучения; затем повторить процесс для другой комбинации обучающих и оценочных срезов; такой подход к оценке называется кросс-валидацией. •МL-алгоритмы имеют несколько параметров, также известных как гиперпа­ раметры (Hyperparameters ), которые можно использовать для тонкой на­ стройки модели для наилучшего соответствия данным; обычно такая на­ стройка выполняется вместе с кросс-валидацией на этапе моделирования. •Значения признаков в данных могут использовать разные шкалы измерения, что затрудняет создание правил; в таких ситуациях нужно привести данные (значения признаков) к общей или нормализованной шкале (например, от О до 1 ); этот этап называется масштабированием данных или нормализацией; все шаги по нормализации и оценке или только их часть могут быть добавле­ ны в конвейер (например, Apache Beam) и выполняться вместе при оценке разных комбинаций для выбора лучшей модели; результатом этого этапа бу­ дет модель-кандидат. 3.Тестирование: на этом этапе используются данные из отложенного набора для проверки точности модели-кандидата, созданной на предыдущем этапе. Резуль­ таты тестирования можно использовать для добавления/удаления некоторых признаков и тонкой настройки модели, пока не будет достигнута приемлемая точность. После достижения необходимой точности можно приступать к реализации модели для прогнозирования на основе реальных данных. Создание примера машинного обучения В этом разделе мы создадим пример модели машинного обучения на Python, кото­ рый будет распознавать три типа растения Ирис. Для построения модели использу­ ем общедоступный набор данных с четырьмя признаками (длина и ширина чаше­ листиков и лепестков) и три типа цветков ириса. Для упражнения задействуем следующие компоненты: ♦Набор данных об ирисах из репозитория UC Irvine Machine Learning Repository (http://archive.ics.uci.edu/mll), который содержит 150 записей и три ожидаемых шаблона для идентификации; это уточненный набор данных, в кото­ ром уже определены необходимые признаки.\n--- Страница 372 ---\nГлава 13. Python и машинное обучение ♦Различные библиотеки Python: •Pandas и matplotlib для анализа данных. •scikit-leam для обучения и тестирования модели ML. Сначала напишем программу на Python для анализа набора данных. Анализ набора данных 377 Для простоты мы загрузили два файла с набором данных (iris.data и iris.names) из репозитория https :/larchive. ics. uci. edu/ml/machine-learning-databases/iris/. Можно напрямую получить доступ к файлу данных из этого репозитория через Python, но в примере мы будем использовать его локальную копию. Кроме того, scikit-leam предоставляет несколько наборов данных как часть библиотеки и может напрямую использоваться для оценки. Мы решили использовать фактические фай­ лы, поскольку это близко к реальным сценариям, когда мы сами собираем данные и используем их. Файл данных содержит 150 записей, отсортированных по ожидаемому результату. В нем предоставлены значения для четырех признаков, которые описаны в файле iris. names ( sepal-length, sepal -width, petal-length и petal-width). Согласно файлу, вы­ ходными результатами являются три вида растения ирис: Iris-setosa (ирис щетини­ стый), Iris-versicolor (ирис разноцветный) и Iris-virginica (ирис виргинский). Мы загрузим данные в pandas DataFrame и проанализируем их на наличие интересую­ щих нас атрибутов. Пример кода для анализа выглядит следующим образом: #iris_data_analysis.py from pandas import read_csv from matplotlib import pyplot data file = \"iris/iris.data\" iris_names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class'] df = read_csv(data_file, names=iris_names) print(df.sha pe) print(df.head(20)) print(df.describe{)) print(df.grou pby('class') .size()) il ящик с усами df.plot(kind='box', subplots=True, layout =(З,3), sharex =False, sharey=False) pyplot. show () # проверка гистограмм df .hist () pyplot. show ()\n--- Страница 373 ---\n378 Раздел 4. Python для веб-разработки, облака и сети В первой части анализа мы проверили несколько метрик в данных с помощью функций библиотеки Pandas: ♦Мы использовали метод shape для получения размера DataFrame, который дол­ жен составлять [150, 5], поскольку мы имеем 150 записей и пять столбцов (че­ тыре с признаками и один с ожидаемым результатом); на этом шаге выполняется проверка, что все данные корректно загружены в DataFrame. ♦Мы . проверили фактические данные методами head или tail; это применимо только для визуального осмотра данных, особенно если не видно, что находится в файле. ♦Метод describe предоставил различные статистические Ю1ючевые показатели эффективности (Кеу Performance lndicators, КРI, КПЭ) для данных: sepal-lenqth sepal-width petal-lenqth petal-width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.054000 3.758667 1.198667 std 0.828066 0.433594 1.764420 0.763161 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 ♦Эти КРI могут помочь выбрать подходящий алгоритм для набора данных. ♦Метод groupby используется при определении числа записей для каждого class (столбец с ожидаемыми результатами); в выводе будет указано, что для каждого вида ирисов имеется 50 записей: Iris-setosa 50 Iris-versicolor 50 Iris-virqinica 50 Во второй части анализа мы попытались использовать блочную диаграмму (также известную, как Ящик с усами) и гистограмму. Ящик с усами -это визуальный способ отображения КРI, полученных методом describe: минимальное значение, первый квартиль, второй квартиль (медиана), третий квартиль и максимальное зна­ чение. Этот график показывает, распределены ли данные симметрично, сгруппиро­ ваны в определенном диапазоне или смещены в одну сторону от распределения. Для нашего набора данных мы получим диаграммы, показанные на рис. 13.3. На графиках видно, что данные о длине (petal-length) и ширине (sepal-length) ча­ шелистиков сгруппированы между первым и третьим квартилями. В этом можно убедиться, проанализировав распределение данных с помощью гистограммы, пред­ ставленной на рис. 13.4. Проанализировав данные и выбрав подходящий алгоритм (модель), можно перейти к следующему шагу -обучению модели.\n--- Страница 374 ---\nГлава 13. Python и машинное обучение 7 б 5 2.s __---�------' 2.0 '-------.'-------' sepal-length б 4 2 petal-le.ngth 2.S 2.0 L5 1.0 0.5 sepal·width о.о _ ________ __. petal·width Рис. 13.3. Диаграммы «ящик с усами» для признаков из набора данных об ирисах 20 10 о 30 10 о sepal-length S 6 7 petal-length 2 4 6 8 30 20 10 о 30 20 10 sepal-width 3 4 petal-width 2 Рис. 13.4. Гистограмма с признаками из набора данных об ирисах Обучение и тестирование модели машинного обучения Для обучения и тестирования модели необходимо выполнить следующие шаги: 379 1.Сначала нужно разделить набор данных на две части: одна для обучения, другая - для тестирования. Такой подход называется методом удержания (Holdout). Биб­ лиотека scikit-learn имеет функцию train_test_split для удобного разделения: #iris_build_svm_model.py (#1) # Разделяем набор данных\n--- Страница 375 ---\n380 Раздел 4. Python для веб-разработки, облака и сети Х = df. drop ( 'class' , axis = 1) ) у = df [ 'class'] X_train, X_test, y_train, y_test = train_test_split(X, у, test_size=0.20, random_state =l, shuffle=True) 2.Перед вызовом train_test_split набор данных разделяется на набор признаков (в номенклатуре ML обычно обозначается х и должен быть в верхнем регистре) и набор ожидаемых результатов ( обозначается у и должен быть в нижнем реги­ стре). Затем оба набора данных (х и у) разделяются функцией train_test_split в соответствии с размером тестового набора test_size (в нашем случае 20% от все­ го набора данных). Мы также разрешаем перемешивание данных перед разделе­ нием. В результате получим четыре набора (x_train, y_train, X_test и y_test) для обучения и тестирования. 3.Далее создадим модель и предоставим для ее обучения данные х _ train и у_ train. Выбор алгоритма не так важен в этом упражнении. Для набора данных будем использовать алгоритм под названием машина опорных векторов (Support Vec­ tor Machine, SVC), известный также как метод опорных векторов. Пример кода Python: #iris_Ьuild_svm_model.py (#2) # делаем предсказания model = SVC(ga11111a='auto') model.fit(X_train, y_train) predictions = model.predict(X_test) 4.Для обучения модели мы использовали метод fit. В следующем выражении мы сделали прогнозы на основе данных тестирования (x_test). Они будут использо­ ваться для оценки эффективности обученной модели. 5. В конце прогнозы будут сравниваться с ожидаемыми результатами (y_test) с ис­ пользованием функций accuracy_score и classification_report из библиотеки scikit-learn: #iris_Ьuild_svm_model.py (13) # оценка предсказаний print(accuracy_score(y_test, predictions)) print(classification_report(y_test, predictions)) Консольный вывод будет следующим: 0.9666666 Iris-setosa 1.00 1.00 Iris-versicolor 1.00 0.92 Iris-virginica 0.86 1.00 accuracy macro avg 0.95 0.97 weighted avg 0.97 0.97 1.00 0.96 0.92 0.97 0.96 0.97 11 13 6 30 30 30\n--- Страница 376 ---\nГлава 13. Python и машинное обучение 381 Значение точности очень высокое (0.966), что указывает на способность модели прогнозировать вид ириса на тестовых данных с точностью 96%. Модель превос­ ходно справляется с определением видов lris-setosa и Iris-versicolor, но менее эф­ фективно с видом Iris-virginica (точность 86%). Есть несколько способов повысить эффективность модели, и мы обсудим их далее. Оценка модели с помощью кросс-валидации и тонкой настройки гиперпараметров В предыдущем примере мы акцентировали внимание именно на простом процессе обучения для лучшего понимания основных этапов создания МL-модели. Для про­ изводственных развертываний недостаточно полагаться на столь маленький набор данных, содержащий всего 150 записей. Кроме того, необходимо оценить модель на эффективность прогнозов, используя следующие методы: 1.k-блочная кросс-валидация: в предыдущей модели мы перемешивали данные перед разделением их на обучающий и тестовый наборы методом удержания. Из-за этого модель может выдавать разные результаты при каждом обучении, что приводит к нестабильности. Довольно сложно выбрать обучающие данные из небольшого набора так, что они будут похожи на реальные данные. Если мы хотим сделать модель более стабильной на небольшом наборе, рекомендуется использовать кросс-валидацию по k блокам. При таком подходе набор разделя­ ется на k блоков или срезов, где k-1 блок используются для обучения, а послед­ ний k-й -для оценки или тестирования. Процесс повторяется, пока в тестиро­ вании не поучаствует каждый блок. Это эквивалентно повторению метода удер­ жания k раз с использованием разных срезов данных для тестирования. 2.В дальнейшем мы разделим наш набор на пять фрагментов (k=5 для 5-кратной кросс-валидации). В первой итерации можно использовать первый срез (20%) для тестирования, а остальные четыре (80%) -для обучения. Во второй итера­ ции будет использован второй срез для тестирования, а остальные четыре -для обучения, и т. д. Так мы сможем оценить модель по всем пяти обучающим набо­ рам и выбрать лучший вариант. Схема выбора данных для обучения и тестиро­ вания показана на рис. 13.5: k=1 k=2 k=З k=4 k=5 Рис. 13.5. Схема кросс-валидации для 5 блоков данных\n--- Страница 377 ---\n382 Раздел 4. Python для веб-разработки, облака и сети 3.Точность кросс-валидации рассчитывается как средняя точность каждой модели, созданной на каждой итерации в соответствии со значением k. 4.Оптимизация гиперпараметров: в предыдущем примере был использован ал­ горитм машинного обучения с параметрами по умолчанию. Каждый алгоритм имеет множество гиперпараметров, которые можно изменять для настройки мо­ дели в соответствии с набором данных. Некоторые параметры можно задавать вручную, анализируя распределение данных, но выявлять влияние комбинаций этих параметров утомительно. Необходимо оценить модель, используя различ­ ные значения гиперпараметров, что может помочь в выборе наилучшей их ком­ бинации. Этот метод называется тонкой настройкой или оптимизацией гипер­ параметров. Кросс-валидация и тонкая настройка сложны в реализации даже с помощью про­ граммного кода. К счастью, scikit-learn содержит инструменты для решения таких задач парой строк кода на Python. Библиотека предлагает два типа инструментов: GridSearchCV и RandomizedSearchCV. Рассмотрим каждый из них. GridSearchCV Инструмент GridSearchCV оценивает любую заданную модель, используя кросс­ валидацию для всех возможных комбинаций значений, предоставленных для ги­ перпараметров. Каждая комбинация значений будет оцениваться с помощью кросс­ валидации на срезах набора. В следующем примере используем класс GridSearchCV из библиотеки scikit-learn для оценки SVС-модели для комбинации параметров с и gamma. с -это параметр регу­ ляризации, который помогает найти компромисс между низкой ошибкой обучения и низкой ошибкой тестирования. Чем выше значение с, тем большее количество ошибок допускается. Мы будем использовать значения 0.001, 0.01, 1, 5, 10 и 100 для с. Параметр gamma определяет нелинейные гиперплоскости или нелинейные ли­ нии для классификации. Чем выше gamma, тем больше данных модель может попы­ таться уместить. Для gamma мы также укажем значения 0,001, 0,01, 1, 5, 10 и 100. Полный код GridSearchCV выглядит следующим образом: #iris_eval_svc_model.py (час'l'Ь 1 из 2) from sklearn.model selection import train_test_split from sklearn.model selection import GridSearchCV ,RandomizedSearchCV from sklearn.datasets import load iris from sklearn.svm import SVC iris= load_iris() Х = iris.data у= iris.target X_train, X_test, y_train, y_test=train_test_split(X,y,test_size =0.2) params = {\"С\": [О. 001, О. 01, 1, 5, 10, 100], \"gamma\": (0.001, 0.01, 0.1, 1, 10, 100] J model=SVC () grid_cv=GridSearchCV(model, params, cv=S)\n--- Страница 378 ---\nГлава 13. Python и машинное обучение grid_cv.fit(X_train,y_train) print(f\"GridSearch- best parameter:{grid_cv.best_params_}\") print (f\"GridSearch- accuracy: { grid_ cv. best_ score _} \") print(classification_report(y_test, grid_cv.best_estimator_.predict( X_test))) Ключевые моменты кода: 383 ♦Мы загрузили данные напрямую из библиотеки scikit-leam в целях демонстра­ ции; этот код также можно использоваться для загрузки данных из локального файла. ♦На первом шаге важно определить словарь params для тонкой настройки; в этом словаре мы задаем значения параметров с и gamma. ♦Мы указали cv=S, это позволит оценить каждую комбинацию параметров с по- мощью кросс-валидации на пяти срезах. Результат программы даст наилучшее сочетание с и gamma, а также точность модели с помощью кросс-валидации. Консольный вывод для лучшей комбинации парамет­ ров и лучшей точности модели будет следующий: GridSearch- Ьest parameter: { 'С' : 5, 'gamna' : О .1} GridSearch- accuracy: О.9833333333333334 За счет оценки разных сочетаний параметров и использования кросс-валидации с помощью GridSearchCV общая точность модели повышается с 96% до 98%. Отчет о классификации (в выходных данных отсутствует) показывает, что точность для трех видов ирисов составляет 100% для наших тестовых данных. Однако этот инст­ румент невозможно использовать, когда есть большое число значений параметров и большой набор данных. RandomizedSearchCV Инструмент RandomizedSearchCV позволяет оценивать модель только для случайно выбранных значений гиперпараметров, а не для всех комбинаций. В качестве вход­ ных данных можно указать значения параметров и количество случайных итераций для выполнения. RandomizedSearchCV случайным образом выберет комбинацию пара­ метров в соответствии с количеством предоставленных итераций. Этот инструмент полезен, когда мы имеем дело с большим набором данных, а также при большом количестве комбинаций, оценка которых может потребовать очень много времени и вычислительных ресурсов. Код Python для RandomizedSearchCV будет аналогичным, как и для GridSearchCV, за ис­ ключением нескольких дополнительных строк: #iris_eval_svc_model.py (чаС'l'Ь 2 из 2) rand_cv =RandomizedSearchCV(model, params, n_iter = 5, cv=S) rand_cv.fit(x_train,y_train) print(f\" RandomizedSearch -best parameter: {rand_cv.best_params_}\") print(f\" RandomizedSearch -accuracy: {rand_cv.best_score_}\")\n--- Страница 379 ---\n384 Раздел 4. Python для веб-разработки, облака и сети Поскольку мы указали n _ iter=5, RandomizedSearchCV выберет только пять комбинаций параметров с и gamma и оценит по ним модель. После выполнения этого кода консольный вывод будет следующим: Randomi.zedSearch- Ьest parameter: { 'qamna' : 10, 'С' : 5} Randomi.zedSearch- accuracy: О.9333333333333333 Обратите внимание, результат может отличаться, поскольку этот инструмент мо­ жет выбрать другие комбинации значений для оценки. Если увеличить количество итераций (n_iter) для объекта RandomizedSearchCV, точность в выводе повысится. Ес­ ли n_iter не задать, оценка будет выполнена для всех комбинаций. Это означает, что результат будет аналогичен GridSearchCV. Как видите, лучшие выбранные комбинации от GridSearchCV и RandomizedSearchCV от­ личаются. Такой результат ожидаем, поскольку мы запускали инструменты с раз­ ным числом итераций. На этом обсуждение, как создавать пример модели машинного обучения с помо­ щью библиотеки scikit-leam, можно завершить. Мы рассмотрели основные этапы и концепции, необходимые для построения и оценки моделей. На практике для нор­ мализации данных мы также выполнили их масштабирование, которое может быть достигнуто с помощью встроенных классов scikit-leam (например, standardScaler) или с помощью собственных классов. Масштабирование -это операция преобра­ зования данных, которую можно объединять с задачей обучения в одном конвейере. Библиотека scikit-leam поддерживает объединение в конвейер нескольких операций или задач с помощью класса Pipeline. Он также может быть использован напрямую с RandomizedSearchCV или GridSearchCV. Узнать больше о работе масштабаторов и конвейеров с библиотекой scikit-leam можно в онлайн-документации (https://scikit­ learn.org/staЫe/user _guide.htm[). Далее обсудим, как сохранить модель в файл и восстановить ее из файла. Сохранение МL-модели в файл Оценив модель и выбрав лучшую в соответствии с набором данных, на следующем шаге можно реализовать ее для будущих прогнозов. Модель может быть реализо­ вана как часть любого приложения Python, включая веб-приложения Flask или Django, микросервисы и даже облачные функции. Главная задача заключается в переносе модели из одной программы в другую. Для этого можно использовать та­ кие библиотеки, как pickle или joЫib, которые можно задействовать для сериализа­ ции модели в файл. Затем этот файл можно использовать в любом приложении для повторной загрузки модели в Python и создания прогнозов методом predict. Для демонстрации этой концепции сохраним одну из ранее созданных МL-моделей (объект model в программе iris_build_svm_model.py) в файл model.pkl. Затем загрузим модель из этого файла с помощью pickle и сделаем прогноз на основе новых дан-\n--- Страница 380 ---\nГлава 13. Python и машинное обучение 385 ных для эмуляции использования модели в любом приложении. Полный пример кода выглядит следующим образом: #iris_save_load_predict_model.py # создаем модель, используя код из iris_build_svrn_model.py # сохраняем модель в файл with open(\"model.pkl\", 'wb') as file: pickle.dump(model, file) # загружаем модель из файла(в другом приложении) with open(\"model.pkl\", 'rb') as file: loaded_model = pickle.load(file) х _ new = [ [ 5 . 6, 2 . 6, 3 . 9, 1 . 2 ] ] y_new = loaded_model.predict(x_new) print(\"X=%s, Predicted=%s\" % (x_new[O], y_new[O])) Использовать библиотеку joЬlib проще, чем pickle, но может потребоваться ее уста­ новка в качестве зависимости scikit-leam, если это еще не было сделано. В следую­ щем примере показано, как использовать joЫib для сохранения лучшей модели (в соответствии с оценкой инструмента GridSearchcv), а затем для загрузки модели из файла: #iris_save_load_predict_gridmodel.py # создаем и обучаем grid_cv, используя код из iris_eval_svrn_model.py joЫib.dump (grid_ cv .best_ estimator _, \"model. joЬlib\") loaded_model = joЫib.load(\"model.joЫib\") х _ new = [ [ 5. 6, 2. 5, 3. 9, 1. 1] ] y_new = loaded_model.predict(x_new) print (\"X=%s, Predicted=%s\" % (x_new[O], .y_new[O])) Код библиотеки joЫib лаконичен и прост. Прогнозирующая часть аналогична пре­ дыдущему примеру с библиотекой pickle. Теперь, когда мы знаем, как сохранить модель в файл, можно перенести ее в любое приложение в локальной и даже облачной среде (например, GCP AI Platform). Да­ лее обсудим, как развернуть нашу модель на платформе GCP. Развертывание и прогнозирование МL-модели в GCP Cloud Провайдеры публичных облаков предлагают различные ИИ-платформы для обуче­ ния как встроенных, так и пользовательских моделей для развертывания и даль­ нейшего прогнозирования. Google предлагает Vertex AI, Amazon предлагает Amazon SageMaker, а Azure -Azure ML. Мы остановили свой выбор на Google,\n--- Страница 381 ---\n386 Раздел 4. Python для веб-разработки, облака и сети поскольку, предположительно, вы уже имеете аккаунт GCP и знакомы с основными принципами работы с ним. AI Platfoпn является частью Vertex AI Platfoпn и пред­ назначена для обучения и развертывания МL-моделей в масштабе. GCP AI Platfonn поддерживает такие библиотеки, как scikit-learn, TensorFlow и XGBoost. В этом подразделе мы рассмотрим, как развернуть уже обученную модель в GCP, а затем сделать прогноз результата с ее помощью. Google AI Platfoпn предлагает сервер прогнозирования (вычислительный узел) че­ рез глобальную (ml. googleapis. com) или региональную (<регион>-ml. googleapis. com) конечные точки. Глобальная АРI-точка рекомендована для пакетных прогнозов, доступных для TensorFlow на Google AI Platfoпn. Региональные обеспечивают до­ полнительную защиту от сбоев в других регионах. Мы развернем МL-модель, ис­ пользуя региональную конечную точку. Нам понадобится проект GCP. Можно создать новый или использовать сущест­ вующий из предыдущих упражнений. Действия по созданию проекта GCP и при­ вязке биллинг-аккаунта приведены в главе 9 («Программирование на Python для облака»). Когда проект готов, можно развернуть модель model. joЫib из предыдуще­ го подраздела: 1.Сначала создадим сегмент хранилища, где будет находиться файл модели. Можно использовать следующую команду Cloud SDK: gsutil mЬ gs://<bucket name> gsutil mЬ gs://muasif-svc-mode� #Образец сегмента создан 2.Когда сегмент готов, можно загрузить в него файл модели (model. joЫib) сле­ дующей командой: gsutil ер model.joЬlib gs://muasif-svc-model 3.Обратите внимание, имя файла модели должно иметь вид model. *. Это означает, что файл должен иметь имя model с расширениями pkl, joЫib или bst в зависимо­ сти от библиотеки, используемой для упаковки модели. 4.Теперь можно инициировать рабочий процесс для создания модели на AI Platfoпn, выполнив следующую команду. Обратите внимание, имя модели мо­ жет содержать только буквы, цифры и нижнее подчеркивание: gcloud ai-platform models create my_iris_model -region =us-centrall 5.Теперь можно создать версию нашей модели следующей командой: gcloud ai-platform versions create vl \\ --model=my_iris_model\\ --origin=gs://muasif-svc-model \\ --framework =scikit-learn \\ --runtime-version =2.4 \\ --python-version =З.7 \\ --region =us-centrall \\ --machine-type =nl-standard-2 6.Рассмотрим атрибуты этой команды: •model: указывает на имя модели, созданное на предыдущем шаге;\n--- Страница 382 ---\nГлава 13. Python и машинное обучение 387 •origin: указывает на сегмент хранилища, где находится файл; мы укажем только расположение каталога, а не путь к файлу; •frarnework: указывает на используемую библиотеку ML; GCP поддерживает scikit-learn, TensorFlow и XGBoost; •runtime-version: указывает на среду выполнения (в нашем случае на версию библиотеки scikit-learn); •python-version: указывает на версию Python (в нашем случае выбрана макси­ мальная версия, которую поддерживает GCP AI Platfoпn на момент написа­ ния книги-3.7); •region: указывает регион, выбранный для модели; •machine-type (опциональный): указывает на тип вычислительного узла, кото­ рый будет использоваться для развертывания модели; если атрибут не ука­ зан, используется тип nl-standarct-2; •команде versions create может потребоваться несколько минут для развер­ тывания новой версии. Как только она выполнится, мы получим примерно следующий результат: Using enclpoint [https://us-centrall-ml.googleapis.can/] Creatin 9 version (this might take а few minutes ) done. 7.Убедиться, что модель и версия корректно развернуты, можно командой describe в контексте versions: gcloud ai-platform versions descriЬe vl model=my_iris_model 8.Когда модель вместе с версией развернуты, можно использовать новые данные для прогнозирования. В целях тестирования были добавлены несколько записей данных, отличных от исходного набора (в файл input.json): [5.6, 2.5, 3.9, 1.1] [3.2, 1.4, 3.0, 1.8] 9.Для прогнозирования результата на основе записей в файле input.json можно использовать следующую команду: gcloud ai-platform predict --model my_iris_model --version vl --json-instances input.json 1 О. Вывод консоли покажет спрогнозированный класс для каждой записи, а также следующие данные: Using endpoint [https://us-centrall-1111.googleapis.can/] [.' Iris-versicolor' , 'Iris-virginica' ] Для использования развернутой модели в нашем приложении (локальном или об­ лачном) можно задействовать Cloud SDK или Cloud Shell, но рекомендуется вы­ брать Google AI API для предсказаний.\n--- Страница 383 ---\n388 Раздел 4. Python для веб-разработки, облака и сети Мы рассмотрели развертывание в облаке и варианты предсказаний с помощью Google AI Platfoпn. Однако модель можно перенести и на другие платформы, на­ пример, Amazon SageMaker или Azure ML. Дополнительную информацию по Amazon SageMaker можно найти по адресу: https://docs.aws.amazon.com/sagemaker/, а документацию по Azure ML по адресу: https://docs. microsoft.com/en-us/azure/ machine-learning/. Заключение В этой главе мы познакомились с машинным обучением и его основными компо­ нентами, включая наборы данных, алгоритмы и модели, а также обучение и тести­ рование моделей. После мы обсудили популярные фреймворки и библиотеки ML для Python, включая scikit-leam, TensorFlow, PyTorch и BGBoost. Мы также изучили рекомендации по очистке и разделению данных для обучения моделей. Для изуче­ ния библиотеки scikit-leam на практике мы создали пример модели с использовани­ ем алгоритма SVC. Далее обучили модель и оценили ее с помощью методов k­ блочной кросс-валидации и тонкой настройки гиперпараметров. Научились сохра­ нять модель в файл, а затем загружать ее обратно в программу. В конце мы проде­ монстрировали, как развернуть модель и использовать ее для прогнозов с помощью Google AI Platfoпn и нескольких команд GCP Cloud SDК. Эта глава будет полезна всем, кто хочет использовать Python для машинного обу­ чения. В следующей главе мы изучим, как использовать Python для автоматизации сетей. Вопросы 1.Чем обучение с учителем отличается от обучения без учителя? 2.Что такое k-блочная кросс-валидация и как она используется для оценки модели? 3.Что такое RandomizedSearchCV и чем он отличается от GridSearchcv? 4.Какие библиотеки позволяют сохранить модель в файл? 5.Почему для Google AI Platfoпn региональные конечные точки предпочтительнее глобальных? Дополнительные ресурсы ♦«Machine Learning Algorithms», автор: Джузеппе Бонаккорсо (Giuseppe Bonac­ corso ). ♦«40 Algorithms Every Programmer Should Know», автор: Имран Ахмад (lrnran Ahmad).\n--- Страница 384 ---\nГлава 13. Python и машинное обучение 389 ♦«Mastering Machine Learning with scikit-learn», автор: Гевин Хэкелинг (Gavin Hackeling). ♦«Python и машинное обучение: Машинное и глубокое обучение с использованием Python, scikit-learn и TensorFlow 2», авторы: Себастьян Рашка (Sebastian Raschka) и Вахид Мирджалили (Vahid Mirjalili). ♦Руководство по scikit-leam: https://scikit-learn.org/staЫel user _guide.html. ♦Руководство по Google AI Platform для обучения и развертывания моделей ма­ шинного обучения: https://cloud.google.com/ai-platform/docs. Ответы 1.При обучении с учителем мы предоставляем желаемый результат с обучающими данными. При обучении без учителя мы предоставляем только обучающие дан­ ные без результата. 2.Кросс-валидация -это статистический метод, используемый для измерения эффективности модели машинного обучения. При k-блочной кросс-валидации данные делятся на k срезов или блоков. Для обучения используется k-1 срезов, а для проверки точности -k-й блок. Процесс повторяется, пока каждый срез не поучаствует в проверке. Точность кросс-валидации модели рассчитывается как средняя точность всех моделей, созданных за k итераций. 3.RandomizedSearchCV -инструмент библиотеки scikit-leam для применения кросс­ валидации к модели ML со случайно выбранными гиперпараметрами. GridSearchCV предлагает аналогичные возможности, за исключением того что он проверяет модель для всех комбинаций значений гиперпараметров. 4.Pickle и JoЬlib. 5.Региональные конечные точки обеспечивают дополнительную защиту от сбоев в других регионах, а доступность вычислительных ресурсов для них выше, чем для глобальных.\n--- Страница 385 ---\n14 Python для автоматизации сети Традиционно за создание и обслуживание сетей всегда отвечали соответствующие специалисты, и для сферы телекоммуникаций этот подход также актуален. Однако управление и эксплуатация сетей вручную были медленными и иногда приводили к дорогостоящим сбоям из-за человеческих ошибок. Кроме того, для получения он­ лайн-услуги (например, подключение смены тарифа Интернета) клиенту приходи­ лось ждать по несколько дней. С появлением смартфонов и мобильных приложе­ ний, где достаточно нажать на кнопку для получения услуги, пользователи при­ выкли, что сетевые сервисы будут предоставлять все необходимое в течение нескольких минут или даже секунд. Традиционный подход к управлению сетями этого не позволяет. Более того, он является препятствием для внедрения новых продуктов и услуг поставщиками. Автоматизация сети (Network Automation) может улучшить эту ситуацию с по­ мощью ПО для автоматизированного управления и рабочих аспектов сети. Такой подход исключает человеческий фактор при настройке сетевых устройств и позво­ ляет значительно сократить эксплуатационные расходы благодаря автоматизации повторяющихся задач. Эта концепция помогает быстрее предоставлять услуги и позволяет поставщикам внедрять новые предложения. Python хорошо подходит для подобных задач. В этой главе мы познакомимся с возможностями языка для автоматизации сети. Python предоставляет такие библио­ теки, как Paramiko, Netmiko и NAP ALM, которые можно использовать для взаимо­ действия с сетевыми устройствами. Если они конфигурируются системой управле­ ния сетям.и (Network Management System, NMS) или сетевым контролле­ ром/ оркестратором, Python может взаимодействовать с ними по протоколам REST и RESTCONF. Сквозная (End-to-end) автоматизация сети невозможна без прослушивания событий, происходящих в ней в режиме реального времени. Обыч­ но эти события (или потоковые данные) доступны через такие системы, как Apache Kafka. Мы также рассмотрим работу с подобной событийно-ориентированной системой (Event-driven system).",
      "debug": {
        "start_page": 366,
        "end_page": 385
      }
    },
    {
      "name": "Глава 14. Python для автоматизации сети 390",
      "content": "--- Страница 386 --- (продолжение)\nГлава 14. Python для автоматизации сети 391 Темы этой главы: ♦Введение в автоматизацию сети. ♦Взаимодействие с сетевыми устройствами. ♦Интеграция с системами управления сетью (NМS). ♦Работа с событийно-ориентированными системами. В этой главе вы научитесь использовать библиотеки Python для получения инфор­ мации с сетевого устройства и отправки конфигурационных данных на них. Это базовые действия для любого процесса по автоматизации сети. Технические требования В этой главе понадобится: ♦Python 3. 7 или более поздней версии. ♦Установленные библиотеки Paramiko, Netmiko, NAPALM, ncclient и requests по­ верх Python. ♦Доступ к одному или нескольким сетевым устройствам по протоколу SSH. ♦Доступ к лаборатории для разработчиков Nokia для работы с их системой управ­ ления сетями, известной как Network Services Platform (NSP). Пример кода для этой главы можно найти по адресу: https ://github. com/PacktPuЬlish ing/Python -for-Geeks/tree/master/Chapter 14. ВАЖНОЕ ПРИМЕЧАНИЕ В этой главе потребуется доступ к физическим или виртуальным сетевым устройст­ вам и NMS. У вас такой возможности может не быть. Но вы можете использовать лю­ бое сетевое устройство с аналогичным функционалом. Мы сосредоточимся больше на реализации кода Python и сделаем удобным его повторное использование с другими устройствами или NMS. Начнем обсуждение с введения в базовые принципы автоматизации сети. Введение в автоматизацию сети Автоматизация сети -это использование технологий и ПО для автоматизации процессов по управлению сетями и их эксплуатации. Ключевым словом здесь явля­ ется автоматизация процесса. Это означает, что речь идет не только о развертыва­ нии и конфигурировании сети, но и о шагах для достижения этого. Например, ино­ гда этапы автоматизации требуют одобрения от различных заинтересованных сто­ рон перед отправкой конфигурации в сеть. Автоматизация такого этапа является частью автоматизации сети. Таким образом, процесс может меняться от одной ор­ ганизации к другой в зависимости от их внутреннего распорядка. Это затрудняет\nГлава 14. Python для автоматизации сети 391 Темы этой главы: ♦Введение в автоматизацию сети. ♦Взаимодействие с сетевыми устройствами. ♦Интеграция с системами управления сетью (NМS). ♦Работа с событийно-ориентированными системами. В этой главе вы научитесь использовать библиотеки Python для получения инфор­ мации с сетевого устройства и отправки конфигурационных данных на них. Это базовые действия для любого процесса по автоматизации сети. Технические требования В этой главе понадобится: ♦Python 3. 7 или более поздней версии. ♦Установленные библиотеки Paramiko, Netmiko, NAPALM, ncclient и requests по­ верх Python. ♦Доступ к одному или нескольким сетевым устройствам по протоколу SSH. ♦Доступ к лаборатории для разработчиков Nokia для работы с их системой управ­ ления сетями, известной как Network Services Platform (NSP). Пример кода для этой главы можно найти по адресу: https ://github. com/PacktPuЬlish ing/Python -for-Geeks/tree/master/Chapter 14. ВАЖНОЕ ПРИМЕЧАНИЕ В этой главе потребуется доступ к физическим или виртуальным сетевым устройст­ вам и NMS. У вас такой возможности может не быть. Но вы можете использовать лю­ бое сетевое устройство с аналогичным функционалом. Мы сосредоточимся больше на реализации кода Python и сделаем удобным его повторное использование с другими устройствами или NMS. Начнем обсуждение с введения в базовые принципы автоматизации сети. Введение в автоматизацию сети Автоматизация сети -это использование технологий и ПО для автоматизации процессов по управлению сетями и их эксплуатации. Ключевым словом здесь явля­ ется автоматизация процесса. Это означает, что речь идет не только о развертыва­ нии и конфигурировании сети, но и о шагах для достижения этого. Например, ино­ гда этапы автоматизации требуют одобрения от различных заинтересованных сто­ рон перед отправкой конфигурации в сеть. Автоматизация такого этапа является частью автоматизации сети. Таким образом, процесс может меняться от одной ор­ ганизации к другой в зависимости от их внутреннего распорядка. Это затрудняет\n--- Страница 387 ---\n392 Раздел 4. Python для веб-разработки, облака и сети создание единой платформы, способной выполнять автоматизацию сразу для мно­ гих клиентов. В настоящее время поставщики (вендоры) сетевых устройств предпринимают зна­ чительные усилия по предоставлению необходимых платформ, которые могут по­ мочь клиентам при построении индивидуальной автоматизации с минимальными усилиями. Например, Cisco предлагает Network Services Orchestrator (NSO), Juniper Networks предлагает платформу Paragon Automation, а Nokia-NSP. Одна из проблем таких платформ заключается в привязке к поставщику. Это озна­ чает, что платформа конкретного вендора может также управлять устройствами других производителей, но на практике такое взаимодействие будет утомительным и дорогим. Поэтому операторы телекоммуникационных услуг ищут и другие реше­ ния. Python и AnsiЫe -два популярных языка программирования, которые ис­ пользуются для автоматизации в отрасли телекоммуникаций. Прежде чем перейти к возможностям Python для таких задач, рассмотрим преимущества и недостатки сетевой автоматизации. Плюсы и минусы автоматизации сети В самом начале мы уже затронули несколько преимуществ. Все основные достоин­ ства можно резюмировать следующим образом: ♦Быстрое предоставление услуг: чем быстрее оператор предоставит услугу но­ вому клиенту, тем быстрее начнет получать доход, а клиент останется доволен скоростью обслуживания. ♦Снижение эксплуатационных расходов: за счет автоматизации повторяющих­ ся задач и мониторинга сети инструментами с обратной связью, можно сокра­ тить затраты. ♦Исключение человеческого фактора: большинство сетевых сбоев происходит из-за человеческих ошибок; их можно избежать, используя при настройке сети стандартные шаблоны, которые тщательно тестируются перед реализацией. ♦Единообразие при настройке сети: человек не способен учесть все шаблоны и соглашения об именовании, которые важны при управлении сетью; автоматиза­ ция обеспечивает согласованность при настройке, поскольку используется один и тот же скрипт или шаблон. ♦Мониторинг сети: инструменты и платформы позволяют получить доступ к возможностям мониторинга сети и визуализировать в ней все процессы; это по­ зволяет реагировать на пиковые нагрузки еще до того, как из-за нехватки ресур­ сов возникнут проблемы. Цифровая трансформация невозможна без автоматизации, но она имеет и свои не­ достатки: ♦Стоимость: создание или настройка ПО для автоматизации всегда требует за­ трат, которые должны быть учтены заранее.\n--- Страница 388 ---\nГлава 14. Python для автоматизации сети 393 ♦Сопротивление сотрудников: во многих организациях специалисты считают автоматизацию угрозой их рабочему месту, поэтому сопротивляются нововве­ дениям, особенно в эксплуатационных бригадах. ♦Структура организации: автоматизация обеспечивает реальную окупаемость инвестиций (Return Оп Investment, ROI), когда она используется на разных се­ тевых уровнях и доменах; проблема заключается в принадлежности этих доме­ нов разным отделам, каждый из которых имеет собственные стратегии автома­ тизации и предпочтения в платформах. ♦Выбор инструмента/платформы автоматизации: выбор платформы от по­ ставщиков является непростым решением, поскольку вариантов очень много (Cisco, Nokia, НР, Accenture и многие др.); часто поставщики сетевых услуг ис­ пользуют решения разных вендоров, и это создает новый набор проблем, свя­ занных с их взаимодействием. ♦Обслуживание: обслуживание инструментов и скриптов автоматизации так же важно, как их создание; это требует заключения контрактов на техническое об­ служивание у поставщиков или создания своей команды для этих целей. Далее рассмотрим, какие задачи можно автоматизировать в реальности. Варианты использования Python и другие инструменты помогают автоматизировать некоторые однообраз­ ные действия по управлению сетью. Но настоящие их преимущества проявляются в задачах, которые повторяются, подвержены ошибкам или являются трудоемкими при выполнении вручную. Лучшие варианты применения автоматизации сети (с точки зрения поставщика телекоммуникационных услуг) следующие: ♦Повседневная конфигурация сетевых устройств, например, создание новых IР­ интерфейсов и услуг по подключению к сети; выполнение этих задач вручную занимает много времени. ♦Настройка правил и политик брандмауэра для экономии времени; эти задачи являются утомительными, а любые ошибки могут привести к потере времени на их поиск и устранение. ♦Когда в сети тысячи устройств, обновление ПО становится масштабной задачей, которая может затянуться на 1-2 года; автоматизации позволяет ускорить эти процессы и обеспечить удобную проверку устройств до и после обновления. ♦Подключение новых устройств в сеть; если устройство должно быть установле­ но на территории клиента, можно сэкономить время, автоматизировав процесс подключения; этот процесс также известен как Zero Touch Provisioning (ZTP). После знакомства с основами автоматизации сети можно перейти к взаимодейст­ вию с сетевыми устройствами с помощью разных протоколов.\n--- Страница 389 ---\n394 Раздел 4. Python для веб-разработки, облака и сети Взаимодействие с сетевыми устройствами Python является популярным выбором при автоматизации сети, поскольку он прост в изучении и может использоваться для интеграции с сетевыми устройствами как напрямую, так и через NМS. На самом деле многие вендоры, вроде Nokia и Cisco, обеспечивают поддержку Python, поскольку возможность выполнения кода на са­ мом устройстве полезна для автоматизации задач. В этой части книги мы сосредо­ точимся на выполнении кода вне устройства, что даст возможность работать с не­ сколькими устройствами одновременно. ВАЖНОЕ ПРИМЕЧАНИЕ Для всех примеров кода в этой главе мы будем использовать виртуальное сетевое устройство от Cisco (IOS XR версии 7.1.2). Для интеграции с NMS будем использовать систему Nokia NSP. Прежде чем приступить к взаимодействию с сетевыми устройствами, рассмотрим сначала, какие протоколы связи нам доступны. Протоколы для взаимодействия с сетевыми устройствами Для прямого взаимодействия с сетевыми устройствами можно использовать не­ сколько протоколов, например, SSH (Secure Shell Protocol), SNМP (Simple Network Management Protocol) и NETCONF (Network Configuration). Некоторые из них работают поверх друг друга. Остановимся на самых популярных из них. SSH SSH -это сетевой протокол для защищенного обмена данными между любыми двумя устройствами или компьютерами. Весь трафик между объектами будет за­ шифрован перед отправкой по транспортному каналу. Обычно SSН-клиент задей­ ствуется для подключения к сетевому устройству с помощью команды ssh. При этом клиент использует в команде имя пользователя, вошедшего в систему: ssh <server ip or hostname> Можно использовать другого пользователя, отличного от вошедшего в систему, указав имя следующим образом: ssh username@<server IP or hostname> После установки соединения можно отправлять команды через командную строку (CLI)либо для получения конфигурации или оперативной информации, либо для настройки устройства. SSH версии 2 (SSHv2) -это популярный вариант при взаи­ модействии с устройствами для управления и даже автоматизации. Использование протокола SSH с Руthоn-библиотеками Paramiko, Netmiko и NAPALM мы обсудим в подразделе «Взаимодействие с сетевыми устройствами с помощью протоколов на основе SSH». Он также является основным транспортным\n--- Страница 390 ---\nГлава 14. Python для автоматизации сети 395 протоколом для многих других (расширенных) протоколов управления сетью, на­ пример, NETCONF. SNMP Этот протокол был стандартом больше 30 лет и по-прежнему широко используется для управления сетью. Однако сейчас его заменяют более продвинутые и масшта­ бируемые протоколы вроде NETCONF и gNMI. SNМP можно использовать как для конфигурации, так и для мониторинга сети, где обычно он и применяется. Сейчас он считается устаревшим протоколом, который появился в конце 1980-х годов ис­ ключительно для управления сетью. SNМP использует базу управляющей информации (Мanagement Information Base, МШ), которая является моделью устройства, которая была написана с использова­ нием языка SMI (Structure of Management lnformation). NETCONF Протокол NETCONF, представленный Инженерным советом Интернета (lnternet Engineering Task Force, IETF), пришел на смену SNМP. Он, в основном, исполь­ зуется для настройки и должен поддерживаться всеми новыми сетевыми устройст­ вами. NETCONF состоит из четырех уровней: ♦Содержимое: это уровень данных, основанный на языке моделирования У ANG; каждое устройство предлагает несколько У АNG-моделей для разных модулей; до­ полнительную информацию можно найти по ссылке https://github.com/YangModels/ yang. ♦Операции: действия или инструкции, которые отправляются клиентом на сер­ вер (также имеет название NETCONF Agent); эти операции заключены в сооб­ щения запросов и ответов; примеры операций NETCONF: get, get-config, edit­ config И delete-config. ♦Сообщения: это сообщения RPC, которыми обмениваются клиенты и NETCONF Agent; операции NETCONF и данные, закодированные как ХМL, упаковываются в сообщения RPC. ♦Транспорт: этот уровень обеспечивает канал связи между клиентом и сервером; сообщения могут использовать NETCONF поверх SSH или TLS с поддержкой сертификатов SSL. Протокол NETCONF основан на обмене ХМL-сообщениями по протоколу SSH че­ рез порт по умолчанию 830. Базы конфигураций для сетевых устройств обычно де­ лятся на два типа. Первый тип называется активной БД (или хранилищем) с теку­ щей конфигурацией, включая рабочие данные. Это обязательная база для каждого устройства. Второй тип называется база-кандидат (или хранилище-кандидат), ко­ торая хранит возможную конфигурацию до отправки в активную. Когда существу­ ет кандидат, изменения конфигурации нельзя напрямую вносить в активную БД.\n--- Страница 391 ---\n396 Раздел 4. Python для веб-разработки, облака и сети Как работать с NETCONF, используя Python, мы рассмотрим в подразделе «Взаимо­ действие с сетевыми устройствами с помощью NETCONF». RESTCONF RESTCONF -это тоже стандарт IETF, который предлагает подмножество функ­ ций NETCONF через интерфейс RESTful. Вместо RРС-вызовов с кодировкой ХМL он предлагает RЕSТ-вызовы на основе НТТР/НТТРS с возможностью использования сообщений XML или JSON. Если сетевое устройство поддерживает RESTCONF, можно использовать НТТР-методы (GET, РАТСН, PUT, юsт и DELETE) для управления се­ тью. Нужно понимать, что RESTCONF предоставляет ограниченный функционал NETCONF через НТТР/НТТРS. RESTCONF не поддерживает такие операции NETCONF, как коммиты (Commit), откаты (Rollback) или блокировка кон.фигу­ рации. gRPC/gNMI gNMI -это ин.терфейс сетевого взаимодействия (Network Management lnterface, NMI) от Google (g в начале), основанный на gRPC. gRPC -это уда­ ленный вызов процедур от Google для извлечения данных с низкой задержкой и высокой масштабируемостью. Изначально протокол gRPC был разработан для мобильных клиентов, которым требовалось обмениваться данными с облачными серверами со строгими требованиями к задержкам. Он очень эффективен для пе­ редачи структурированных данных через протокольн.ые буферы (Protocol buffer, Protobuf), которые являются ключевым компонентом протокола. При использо­ вании Protobuf данные упаковываются в двоичный формат вместо текстового (JSON или XML). Этот формат не только уменьшает размер данных, но и очень эффективен для сериализации и десериализации данных по сравнению с JSON и XML. Более того, данные передаются по НТТР/2 вместо НТТР 1.1. НТТР/2 пред­ лагает как модель «запрос-ответ», так и модель двун.аправлен.н.ой связи, которая позволяет клиентам устанавливать долгосрочные соединения, что значительно ускоряет передачу данных. Эти две технологии делают протокол gRPC в 7-10 раз быстрее, чем REST API. Другими словами, gNMI -это специфическая реализация протокола gRPC для управления сетью и телеметрии. Как и NETCONF, он использует YANG и предла­ гает мало операций (Get, set и suЬscribe). gNMI все чаще используется для сбора данных в телеметрии, чем для управления сетью. Главная причина в том, что он не обеспечивает такой гибкости, как NETCONF, зато он оптимизирован для сбора данных из удаленных систем, особенно в режиме реального или близкого к реаль­ ному времени. Далее мы рассмотрим библиотеки Python для взаимодействия с сетевыми устройст­ вами.\n--- Страница 392 ---\nГлава 14. Python для автоматизации сети Взаимодействие с сетевыми устройствами с помощью библиотек Python на основе SSH 397 Существует несколько библиотек для взаимодействия с сетевыми устройствами по SSH. Paramiko, Netmiko и NAP ALM -три популярных доступных варианта, кото­ рые мы рассмотрим далее. Paramiko Paramiko -это абстракция протокола SSHv2, которая включает функционал, как на стороне сервера, так и на стороне клиента. Здесь мы сосредоточимся только на возможностях клиента. При взаимодействии с сетевым устройством мы пытаемся либо получить данные конфигурации, либо задать новую конфигурацию для определенных объектов. В первом случае мы используем СLI-команды типа show в зависимости от ОС уст­ ройства. А во втором может потребоваться специальный режим выполнения СLI­ команд. При работе с библиотеками Python эти два типа команд обрабатываются по-разному. Для подключения к сетевому устройству (которое прослушивает как SSH-cepвep) потребуется использовать экземпляр класса paramiko. ssнclient или напрямую ис­ пользовать низкоуровневый класс paramiko. Transport. Класс Transport предлагает низкоуровневые методы для прямого управления коммуникациями на основе соке­ тов. Класс ssнclient служит оберткой и использует класс тransport для управления сеансом с помощью SSH-cepвepa, реализованного на сетевом устройстве. Можно использовать Paramiko для установки соединения с сетевым устройством (Cisco IOS XR в нашем случае) и выполнения shоw-команды (в нашем случае show ip int brief): #show_cisco_int_pnk.py import paramiko host='HOST ID' port=22 username='xxx' password='xxxxxx' # команда cisco ios для получения списка IР-интерфейсов cmd= 'show ip int brief \\n' def main (): try: ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko. AutoAddPolicy()) ssh.connect(host, port, username, password) stdin, stdout, stderr = ssh.exec_command(cmd) output_lines = stdout.readlines()\n--- Страница 393 ---\n398 Раздел 4. Python для веб-разработки, облака и сети response = ''.join(output_lines) print(response) finally: ssh. close () if name main() ' main Ключевые моменты кода: ♦Мы создали экземпляр ssнclient и открьши соединение с SSН-сервером. ♦Поскольку мы не используем ключ хоста, мы применили метод set_missing_host_key_policy во избежание каких-либо предупреждений и ошибок. ♦Установив SSН-соединение, мы отправили shоw-команду (show ip int brief) на хост-компьютер по каналу SSH и получили вывод в виде SSН-ответа. ♦Результатом работы программы будет кортеж объектов stdin, stdout и stderr; ес­ ли команда выполнена успешно, мы получим вывод из объекта stdout. При выполнении на устройстве Cisco IOS XR вывод программы будет следующим: Моn Jul 19 12:03:41.631 UТС Interface IP-Address Status Protocol LoopЬackO 10.180.180.10 Up Up GiqaЬitEthernet0/0/0/0 10.1.10.2 Up Up GiqaЬitEthernet0/0/0/0.100 unassigned Up Down GiqaЬitEthernet0/0/0/1 unassigned Up Up GiqaЬitEthernet0/0/0/1.100 150.150.150.1 Up Up GiqaЬitEthernet0/0/0/2 unassigned Shutdown Down Если нужно вьшолнить эту программу на устройстве другого типа, следует изме­ нить команду, заданную в качестве переменной cmd, в соответствии с типом устрой­ ства. Библиотека Paramiko обеспечивает низкоуровневый контроль над сетевым взаимо­ действием. Но иногда она может неправильно работать из-за нестандартной или неполной реализации протокола SSH на многих сетевых устройствах. Если вы столкнулись с такими проблемами, дело не на вашей стороне или в Paramiko, а в том, как устройство воспринимает ваши с ним взаимодействия. Низкоуровневый транспортный канал может решить эти проблемы, но для этого требуется немного сложного кода. В такой ситуации на помощь приходит библиотека Netmiko. Netmiko Netmiko -абстрактная библиотека для управления сетью, созданная на основе Paramiko. Она помогает устранить проблемы Paramiko за счет разного взаимодей­ ствия с разными сетевыми устройствами. Netmiko скрывает многие детали комму­ никации на уровне устройства и поддерживает несколько устройств от разных про­ изводителей (Cisco, Arista, Juniper и Nokia).\n--- Страница 394 ---\nГлава 14. Python для автоматизации сети 399 Для подключения к сетевому устройству с помощью СLI-команд типа show нужно задать определение device_type, которое используется для подключения к целевому устройству. Определение device_type представляет собой словарь, который должен включать тип устройства, IР-адрес хоста или пошюе доменное имя (Fully Qualified Domain Name, FQDN) устройства, имя пользователя и пароль для подключения. Можно задать номер порта для SSН-соединения, если целевой компьютер прослу­ шивает порт, отличный от 22. Следующий код можно использовать для выполне­ ния той же команды show, которую мы выполняли с библиотекой Paramiko: #show_cisco_int_nmk.py from netmiko import ConnectHandler cisco_rtr = { \"device_type\": \"cisco_ios\", \"host\": \"HOST_ID\", \"usernarne\" : 11ххх\", \"password\": \"ххххххх\", #\"global_delay_factor\": 2, def main(): command = \"show ip int brief\" with Connectнandler(**cisco_rtr) as net connect: print(net_connect.find_prompt()) print(net_connect.enaЬle()) output = net_connect.send_camand(command) print(output) Ключевые моменты кода: ♦Мы создали сетевое соединение с помощью класса ConnectHandler, используя ме­ неджер контекста, который будет управлять жизненным циклом подключения. ♦Netmiko предлагает простой метод find_prompt для захвата запроса (prompt) це­ левого устройства, который будет полезен для парсинга вывода от многих сете­ вых устройств; для Cisco IOS XR этого делать необязательно, но мы задейство­ вали метод, поскольку это рекомендованная практика. ♦Netmiko также позволяет войти в режим ЕпаЫе (запрос командной строки, #) на устройствах Cisco IOS с помощью метода еnаые; этого также делать необяза­ тельно, но является рекомендованной практикой, особенно если мы отправляем конфигурационные СLI-команды как часть одного скрипта. ♦Мы выполнили команду show ip int brief, используя метод send_command, и полу­ чили тот же результат, что и для программы show_cisco_i nt_pmk.py. Если сравнить два примера кода с одинаковой реализацией команды show, можно увидеть, что работать с Netmiko намного удобнее, чем с Paramiko.\n--- Страница 395 ---\n400 Раздел 4. Python для веб-разработки, облака и сети ВАЖНОЕ ПРИМЕЧАНИЕ Важно установить верный тип устройства для получения согласованных результатов, даже если используются устройства одного производителя. Это особенно важно при выполнении команд по конфигурации. Неправильный тип устройства может привести к конфликтующим ошибкам. Иногда выполняются команды, которые требуют больше времени, чем обычные команды show. Например , необходимо скопировать большой файл из одного места на устройстве в другое, и известно, что это займет несколько сотен секунд. По умолчанию Netmiko ожидает выполнения команды около 100 секунд. Можно доба­ вить глобальный коэффициент задержки при определении устройства: \"global_delay_factor\": 2 Время ожидания для всех команд этого устройства будет увеличено в 2 раза. В ка­ честве альтернативы можно задать коэффициент задержки для отдельной команды с помощью метода send _ command, передав следующий аргумент: delay_factor =2 Коэффициент необходимо указывать, если заранее известно, что потребуется большое время задержки. Когда мы его добавляем, следует также добавить еще один атрибут в качестве аргумента метода send_command, который прервет цикл ожи­ дания досрочно, если появится командный запрос на данное действие (например, # для устройств Cisco IOS). Это можно сделать с помощью атрибута: expect_string=r'#' В следующем примере мы отправим конфигурацию на устройство. Процесс на­ стройки с помощью Netmiko аналогичен выполнению команд show, поскольку биб­ лиотека позаботится о включении терминала конфигурации (при необходимости, в зависимости от типа устройст ва) и корректном выходе из него. Для примера будет задано описание интерфейса (description) следующим образом: #config_cisco_int_nmk.py from netmiko import ConnectHandler cisco_rtr = { \"device_type\": \"cisco_ios\", \"host\": \"H0ST_ID\", \"usernarne\": \"ххх\", \"password\": \"хххххх\", def main(): commands = [ \"int LoO\", \"description custan_description\", \"coпmit\"] with ConnectHandler(**cisco_rtr) as net_connect: output = net_connect.send_config_set(commands) print(output) print ()\n--- Страница 396 ---\nГлава 14. Python для автоматизации сети 401 Ключевые моменты кода: ♦Мы создали список из трех команд (int <id интерфейса>, description <новое описа­ ние> и commit); первые две команды также можно отправлять в качестве одной, но для наглядности мы их разделили; команда commit используется для сохранения изменений. ♦Когда мы отправляем команду на устройство, мы используем метод send_config_set для установки соединения в целях настройки; успешное выпол­ нение этого шага зависит от правильной установки типа устройства; это связано с тем, что разные типы устройств по-разному могут реагировать на команды конфигурации. ♦Этот набор из трех команд добавит или обновит атрибут description для указан- ного интерфейса. От программы не ожидается никакого специфического вывода, кроме запросов конфигурации устройства. Вывод консоли будет следующим: Моn Jul 19 13:21:16.904 UТС RP/0/RPO/CPUO:cisco(confiq)#int LoO RP/0/RPO/CPUO:cisco(confiq-if)#description custan_description RP/0/RPO/CPUO:cisco(confiq-if)#camdt Моn Jul 19 13:21:17.332 UТС RP/0/RPO/CPUO:cisco(confiq-if)# Netmiko предлагает гораздо больше возможностей, но мы оставим это для само­ стоятельного изучения в официальной документации (https://pypi.org/project/ netmiko/). Примеры кода, которые обсуждались в этом подразделе, использовались для устройства Cisco, но ту же программу можно использовать, изменив тип уст­ ройства и команды для любого другого устройства, если оно поддерживается биб­ лиотекой. Netmiko упрощает код для взаимодействия с сетевыми устройствами, но для полу­ чения и передачи конфигурации по-прежнему нужны СLI-команды. В Netmiko не­ легко программировать, но здесь может помочь другая библиотека -NAP ALM. NAPALM NAP ALM -это аббревиатура от Network Automation and Programmabllity Ab­ straction Layer with Multivendor. Библиотека обеспечивает следующий уровень абстракции поверх Netmiko, предлагая набор функций в виде единого API для взаимодействия с несколькими сетевыми устройствами. NAP ALM поддерживает меньше устройств, чем Netmiko. В версии 3 доступны основные драйверы для сете­ вых устройств Arista EOS, Cisco IOS, Cisco 1OS-XR, Cisco NX-OS и Juniper JunOS. Однако существуют драйверы, созданные сообществом, для многих других уст­ ройств, например, Nokia SROS, Aruba AOS-CX и Ciena_ SAOS. Как и в случае с Netmiko, мы создадим примеры для взаимодействия с сетевым устройством с помощью NAPALM. В первом примере мы получим список IP-\n--- Страница 397 ---\n402 Раздел 4. Python для веб-разработки, облака и сети интерфейсов, а во втором добавим или обновим атрибут description для IР­ интерфейса. Оба примера будут выполнять те же операции, что и в примерах с Paramiko и Netmiko. Для получения конфигурации сетевого устройства сначала нужно установить со­ единение. Мы будем делать это в обоих примерах. Установка соединения состоит из трех шагов: 1.Мы должны узнать класс драйвера, который поддерживает тип устройства . Для этого можно использовать функцию get_network_dr iver. 2.Узнав класс драйвера, можно создать объект устройства, предоставив аргументы host id, username и password конструктору класса. 3.Далее подключимся к устройству с помощью метода open объекта устройства. Все эти шаги могут быть реализованы на Python: from napalm import get_network_driver driver = get_network_driver( 'iosxr') device = driver('HOST_ID', 'хххх', 'хххх') device. open () Установив соединение, можно вызывать такие методы, как get_interfaces_ip (экви­ валент СLI-команды show interfaces ) или get_facts (эквивалент show version). Пол­ ный код для двух методов выглядит следующим образом: #show_cisco_int_npm.py from napalm import get_network_driver import json def main (): driver = get_network_driver( 'iosxr') device = driver('HOST_ID', 'root', 'rootroot') try: device. open () print(json.dumps(device.get_interfaces_ip(), indent=2)) #print(json.dumps(device.get_facts(), indent=2)) finally: device.close() Что интересно, вывод программы по умолчанию будет представлен в формате JSON. NAPALM по умолчанию преобразует вывод СLI-команд в словарь, который легко использовать в Python. Фрагмент выходных данных показан ниже: \"LoopЬack0\": { \"ipv4\": { \"10.180.180.180\": \"prefix_length\": 32\n--- Страница 398 ---\nГлава 14. Python для автоматизации сети } , \"МgmtEth0/RP0/Cro0/0\": \"ipv4\": { \"172.16.2.12\": { \"prefix_lenqth\": 24 403 В следующем примере мы попробуем добавить или обновить атрибут description для существующего IР-интерфейса: jconfiq_cisco_int_npm.py from napalm import get_network_driver import json def main(): driver = get_network_driver('iosxr') device = driver('HOST_ID', 'ххх', 'хххх') try: device.open() device.load_merqe_candidate(config='interface Lo0 \\n description napalm_desc \\n end\\n') print(device.caiq:iare_confiq()) device. catlllit_ confiq () finally: device. close () Ключевые моменты кода: ♦ Для настройки IР-интерфейса мы использовали метод load_merge_candidate и пе­ редали ему тот же набор СLI-команд, что и в примере с Netmiko. ♦Затем мы сравнили конфигурации до и после выполнения команд, используя ме­ тод compare _ config; это позволяет посмотреть, какая конфигурация добавлена, а какая удалена; ♦Мы зафиксировали все изменения, используя метод commit_con fig. Вывод примера будет отображать только разницу между изменениями: +++ @@ -47,7 +47,7 @@ interface LoopЬackO -description rrry custam description\n--- Страница 399 ---\n404 Раздел 4. Python для веб-разработки, облака и сети +description napalm added new desc ipv4 acldress 10.180.180.180 255.255.255.255 interface МgmtEth0/RP0/CPU0/0 Любая строка, которая начинается с«-» указывает на удаляемую конфигурацию, а любая строка с «+» -на добавляемую. Оба примера демонстрируют базовый набор возможностей NAP ALM для одного типа устройства. Библиотека может использоваться для одновременной настройки нескольких устройств и работать с различными наборами конфигураций. В следующем подразделе мы обсудим взаимодействие с сетевыми устройствами по протоколу NETCONF. Взаимодействие с сетевыми устройствами с помощью NETCONF Протокол NETCONF был создан для модельно-ориентированного (или объектно­ ориентированного) управления сетью, особенно в целях конфигурирования. При работе с сетевым устройством через NETCONF важно понимать две возможности устройства: ♦Для отправки сообщений в правильном формате вы должны понимать У АNG­ модели устройств; полезный ресурс с У АNG-моделями от разных вендоров мож­ но найти по адресу https:/lgithub. com/YangModels/yang. ♦Можно включить порты NETCONF и SSH для протокола NETCONF на сетевом устройстве; в нашем случае это будет виртуальное устройство Cisco IOS XR, как и в предыдущих примерах. Прежде чем переходить к управлению сетью, нужно сначала проверить NЕТСОNF­ возможности устройства и сведения о конфигурации источника данных NETCONF. Для всех примеров в этом подразделе будет использоваться клиентская библиотека NETCONF для Python -ncclient. Она предлагает удобные методы для' отправки запросов RPC. Ниже показан пример программы, использующий ncclient для полу­ чения возможностей и полной конфигурации устройства: #check_cisco_device.py from ncclient import manager with manaqer.connect(host='device_ip, username =xxxx, password=xxxxxx, hostkey_verify=False) as conn: capabilities = [] for capability in conn.server_capaЬilities: capabilities .append(capability) capabilities = sorted(capaЬilities) for сар in capabilities: print(cap) resul t = conn. qet_ confiq ( source =\" running\" ) print (result)\n--- Страница 400 ---\nГлава 14. Python для автоматизации сети 405 Объект manager из библиотеки ncclient используется для подключения к устройству по SSH, но через NETCONF-пopт взо (по умолчанию). Сначала мы получаем список возможностей сервера через экземпляр соединения, а затем выводим их в отформа­ тированном виде для удобного чтения. Дальше мы инициировали NЕТСОNF­ операцию get-config, используя метод get_config библиотеки класса manager. Вывод программы очень объемный и включает все возможности и конфигурацию устрой­ ства. Мы оставим возможность самостоятельно изучить результат и узнать о воз­ можностях вашего устройства. Важно понимать, целью этого подраздела является не изучение NETCONF, а пони­ мание, как использовать Python и ncclient для работы с NETCONF. Для этого на­ пишем два примера: один -для получения конфигурации интерфейсов устройст­ ва, другой -для обновления описания интерфейса, как мы делали с предыдущими библиотеками Py1hon. Получение интерфейсов через NETCONF Ранее мы узнали, что наше устройство (Cisco IOS XR) поддерживает интерфейсы с помощью реализации OpenConfig, которая доступна по адресу http://openconfig.net/ yang/interfaces ?module=openconfig-interfaces . Мы также можем проверить формат ХМL конфигурации интерфейса, который был получен в выводе метода get_config. В этом примере мы просто передаем ХМL­ фильтр с конфигурацией интерфейса в качестве аргумента методу get_config: #show_all_interfaces.py from ncclient import manager with manager.connect(host='device_ip', username=xxx, password='xxxx', hostkey_verify=False) as conn: result = conn.get_confiq(\"running\", filter = ( 'subtree', '<interfaces xmlns= \"http://openconfiq.net/yanq/interfaces\"/>')) print (result) Результат программы будет содержать список интерфейсов. Ниже показан только фрагмент для демонстрации: <rpc-reply message-id=\"urn:uuid:f4553429- ede6-4c79-aeea-573999Зcacf4\" xmlns:nc=\"urn:ietf:params:xml:ns:netconf:Ьase:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:Ьase:1.0\"> <data> <interfaces xmlns=\"http://openconfiq.net/yanq/interfaces\"> <interface> <name>LoopЬackO</name> <confiq> <name>LoopЬackO</name> <description>Confiqured Ьу NEТCCNF</description> </confiq> <!-rest of the output is skipped -->\n--- Страница 401 ---\n406 Раздел 4. Python для веб-разработки, облака и сети Для получения выборочного набора интерфейсов будем использовать расширен­ ную версию ХМL-фильтра на основе У АNG-модели интерфейса. Для следующего примера определим ХМL-фильтр со свойствами name интерфейсов в качестве кри­ териев фи,льтрации. Поскольку этот фильтр занимает несколько строк, он будет оп­ ределен отдельно как строковый объект. Код с ХМL-фильтром выглядит следую­ щим образом: #show_int_confiq.py from ncclient import manager t Создаем шаблон фильтра для интерфейса filter_temp = \"\"\" <filter> <interfaces xmlns=\"http://openconfiq.net/yanq/interfaces\"> <interface> <name>{int_name}</name> </interface> </interfaces> </filter>'\"\"' with manager.connect(host='device_ip', username=xxx, password='xxxx', hostkey_verify=False} as conn: filter = filter_temp.fo:i:mat(int_name = \"MgmtEthO/RPO/CPU0/0\"} result = m.qet_confiq(\"running\", filter} print (result} Выводом программы будет один интерфейс (в соответствии с конфигурацией уст­ ройства): <?xml version=\"l.0\"?> <rpc-reply messaqe-id=\"urn:uuid:cбl588b3- 1ЬfЬ-4aa4-a9de-2a98727elel5\" xmlns:nc=\"urn:ietf:params:xml:ns:netconf:Ьase:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:Ьase:1.0\"> <data> <interfaces xmlns=\"http://openconfiq.net/yanq/interfaces\"> <interface> <name>МgmtEthO/RPO/CPU0/0</name> <confiq> <name>МgmtEthO/RPO/CPU0/0</мme> </confiq> <ethernet xmlns=\"http://openconfiq.net/yanq/interfaces/ethernet\"> <confiq> <auto-neqotiate>false</auto-neqotiate> </confiq> </ethernet> <suЬinterfaces>\n--- Страница 402 ---\nГлава 14. Python для автоматизации сети <@!-oпmitted suЬ interfaces details to save space --> </suЬinterfaces> </interface> </interfaces> </data> </rpc-reply> 407 Также можно определить ХМL-фильтры в ХМL-файле, а затем считать его содер­ жимое в строковый объект. Еще один вариант -использовать шаблоны Jinja, если планируется много работать с фильтрами. Далее обсудим, как обновить описание интерфейса. Обновление описания интерфейса Для настройки атрибута интерфейса description необходимо использовать модель YANG, доступную по адресу: http://cisco.com/nslyang/Cisco-IOS-XR-ifmgr-cfg. Более того, блок XML для настройки интерфейса отличается от блока ХМL для по­ лучения конфигурации. Для обновления интерфейса необходимо использовать сле­ дующий шаблон, определенный в отдельном файле: <!--config-template.xml--> <config xmlns:xc=\"urn:ietf:params:xml:ns:netconf:Ьase:1.0\"> <interface-confiqurations xmlns=\"http://cisco.com/ns/yang/ Cisco-IOS-XR-ifm;Jr-cfg\"> <interface-confiquration> <active>act</active> <interface-name>{int_name}</interface-name> <description>{int_desc}</description> </interface-confiquration> </interface-confiqurations> </config> В этом шаблоне заданы заглушки для свойств интерфейса name и description. Далее мы напишем программу для чтения этого шаблона и вызова NЕТСОNF-операции edit-config с помощью метода edit_config библиотеки ncclient. В результате шаблон будет отправлен в хранилище-кандидат на устройстве: #config_cisco_int_desc.py from ncclient import manager nc_template = open(\"config-template.xml\") .read() nc_payload = nc_template.format(int_name ='Loopback0', int_desc=\"Configured Ьу NETCONF\") with manager.connect(host ='device_ip, username=xxxx, password=xxx, hostkey_verify=False) as nc:\n--- Страница 403 ---\n408 Раздел 4. Python для веб-разработки, облака и сети netconf_reply = nc.edit_config(nc_payload, target=\"candidate\") print(netconf_reply) reply = nc.camli.t() print (reply) Здесь важно отметить два момента. Во-первых, устройство Cisco IOS XR настроено на прием новой конфигурации только через кандидата. Если попытаться задать для атрибута target значение running, произойдет сбой. Во-вторых, сделать новую конфигурацию рабочей можно, только вызвав метод commit после операции edit­ config в том же сеансе. Вывод программы будет содержать два ответа окот сервера NETCONF: <?xml version=\"l.0\"?> <rpc-reply messaqe-id=\"urn:uuid:бd70d758- 6a8e-407d-8cЬ8-10f500e9f297\" xmlns:nc=\"urn:ietf:params:xml:ns:netconf:Ьase:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:Ьase:1.0\"> <ok/> </rpc-reply> <?xml version=\"l.0\"?> <rpc-reply messaqe-id=\"urn:uuid:2a97916ЬclЬ5f-427d-9553-delЬ56417d89\" xmlns:nc=\"urn:ietf:params:xml:ns:netconf:Ьase:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:Ьase:1.0\"> <ok/> </rpc-reply> На этом можно завершить обсуждение, как использовать Python для операций NETCONF. Мы рассмотрели работу двух основных из них (get-config и edit-config) с библиотекой ncclient. Далее рассмотрим интеграцию с системами управления сетью с помощью Python. Интеграция с системами управления сетью Системы управления сетью (Network Management System, NMS) или сетевые контроллеры -это приложения для управления сетью с графическим пользова­ тельским интерфейсом (Graphical User Interface, GUI). Они включают в себя ин­ вентаризацию сети, ее подготовку, управление сбоями, а также взаимодействие с сетевыми устройствами, которое происходит с использованием комбинации разных протоколов связи для разных задач, например, SSН/NETCONF для подготовки се­ ти, SNMP для предупреждений и мониторинга устройств и gRPC для сбора данных телеметрии. Эти системы также предлагают возможности автоматизации с помо­ щью механизмов обработки скриптов и рабочих процессов.\n--- Страница 404 ---\nГлава 14. Python для автоматизации сети 409 Основным ценным аспектом здесь является объединение всего функционала сете­ вых устройств в единую систему и ее последующее предоставление через северные интерфейсы (North Bound lnterface, NBI), которые обычно являются интерфейса­ ми REST или RESTCONF. Эти системы также могут отправлять уведомления о со­ бытиях в реальном времени, например, рассылать оповещения через Apache Kafka или другую событийно-ориентированную систему (Event-driven System). В этом подразделе мы рассмотрим несколько примеров использования REST API в NМS. Интеграцию с Apache Kafka обсудим дальше в подразделе «Интеграция с собы­ тийно-ориентированными системами». Для работы с NМS мы будем использовать общедоступную лабораторию Nokia на онлайн-портале для разработчиков (https:/lnetwork.developer.nokia.coml). Она имеет несколько IР-маршрутизаторов Nokia и NSP. Лаборатория предоставляется бес­ платно на ограниченное время (3 часа в день на момент написания книги). Вам не­ обходимо будет создать бесплатный аккаунт на портале. При бронировании вы получите электронное письмо с инструкциями о подключении к лаборатории, а также необходимые данные для VPN. Если вы являетесь сетевым инженером и имеете доступ к любой другой NМS или контроллеру, можете использовать их для упражнений в этом разделе, внеся необходимые корректировки. Для использования REST API от Nokia NSP необходимо взаимодействовать со шлюзом REST API, который управляет несколькими конечными точками для Nokia NSP. Можно начать работу со шлюзом REST API, используя сервисы определения местоположения. Использование конечных точек сервиса определения местоположения Сначала необходимо понять, какие конечные точки API доступны. Для этого Nokia NSP предлагает конечную точку сервиса определения местоположения, которая предоставляет список всех конечных точек API. Для работы с любым REST API в этом разделе мы будем использовать Руthоn-библиотеку requests. Она хорошо из­ вестна тем, что отправляет НТМL-запросы на сервер с помощью протокола НТТР, и мы уже использовали ее в предыдуших главах. Для получения списка конечных точек из Nokia NSP мы используем следующий код для вызова API сервиса опреде­ ления местоположения: #location_servicesl.py import requests payload = {} headers = {} url = \"https://<NSP URL>/rest-gateway/rest/api/vl/location/services\" resp = requests.request(\"GET\", url, headers=headers, data=payload) print(resp.text)\n--- Страница 405 ---\n410 .___ Раздел 4. Python для веб-разработки, облака и сети Ответ будет содержать несколько десятков конечных точек в формате JSON. Для по­ нимания, как работает каждый API, можно ознакомиться с онлайн-документацией Nokia NSP по адресу https://network. developer.nokia.com/api-documentation/. Если нужно найти определенную конечную точку, можно изменить значение перемен­ ной url в приведенном выше коде: url = \"https://<NSP URL>/rest-gateway/rest/api/vl/location/ services/endpoints?endPoint=/vl/auth/token С помощью этого нового URL-aдpeca мы пытаемся найти конечную точку для то­ кена авторизации (/vl/auth/token). Вывод программы для нового URL будет сле­ дующий: { \"response\": \"status\": О, \"startRow\": О, \"endRow\": О, \"totalRows\": 1, \"data\": { \"endpoints\": [ \"docUrl\" :\"https://<NSP_URL>/rest-gateway/api-docs#!/authent \", \"effectiveUrl\": \"https: //<NSP_URL>/rest-gateway/rest/api\", \"operation\": \"[POST]\" } ] } , \"errors\": null } } Обратите внимание, для использования API сервисов определения местоположения не требуется аутентификации. Однако понадобится токен аутентификации для вы­ зова любого другого API. Далее узнаем, как его получить. Получение токена аутентификации На следующем шаге мы используем effectiveUrl из вывода предыдущего кода для получения токена аутентификации. Этот API требует передачу username и password в кодировке Base64 в качестве атрибута Authorization заголовка НТТР. Код для вызо­ ва API аутентификации выглядит следующим образом: #get_token.py import requests from base64 import b64encode import json\n--- Страница 406 ---\nГлава 14. Python для автоматизации сети #получаем кодировку base64 message = 'username'+ ':' +'password' message _ bytes = message. encode ( 'UTF-8'} basic_token = b64encode(message_bytes} payload = json.dumps({ \"grant_type\": \"client credentials\" }} headers = { 'Content-Type': 'application/json', 'Authorization': 'Basic {}' .format(str(basic_token, 'UTF-8'}} url = \"https://<NSP SERVER ORL>/rest-gateway /rest/api/vl/auth/token\" resp = requests.request(\"POST\", url, headers=headers,data =payload} token = resp. j son () [ \"access _ token\"] print(resp} --- # При выполнении этого кода Python мы получим токен на один час, который можно # использовать для любого API NSP. { \"access_token\": \"VEtOLVNBTXFh ZDQЗMzESZjQtNWUxZjQ0YjNl\", \"refresh _ token\": \"UkVUS04tU0FNcWFSZlMТmQ0ZTASMDNlOTY =\", \"token_type\": \"Bearer\", \"expires_in\": 3600 411 Также доступен токен обновления, который можно использовать для обновления текущего токена до истечения срока его действия. Рекомендуется обновлять его каждые 30 минут. Это можно сделать с помощью того же токена аутентификации, но при этом необходимо отправить следующие атрибуты в теле НТТР-запроса: payload = json.dumps({ \"grant_type\": \"refresh_token\", \"refresh_token\": \"UkVUS04tU0FNcWFSZlMТmQ0ZTASMDNlOTY =\" }} Другой хорошей практикой является отзыв токена, когда он больше не нужен. Для этого можно использовать следующую конечную точку API: url = \"https://<NSP URL>rest-gateway/rest/api/vl/auth/revoe&tion\" Получение сетевых устройств и инвентаризация интерфейсов Получив токен аутентификации, можно использовать REST API для приема на­ строенных данных и добавления новой конфигурации. Начнем с простого кода для получения списка всех устройств в сети, управляемой NSP. В этом примере ис­ пользуется токен, который уже был извлечен с помощью API токена: llget_network_devices.py import requests pload={}\n--- Страница 407 ---\n412 Раздел 4. Python для веб-разработки, облака и сети headers = 'Authorization': 'Bearer {token}' .format(token) url = \"https://{{NSP_URL)):8544/NetworkSupervision/rest/api/vl/networkElements\" response = requests.request(\"GET\", url, headers=headers, data=pload) print(response.text) Вывод программы будет содержать список сетевых устройств с их атрибутами. Мы не будем демонстрировать вывод из-за большого объема данных. Следующий пример покажет, как получить список портов (интерфейсов) устройст­ ва с помощью фильтра. Обратите внимание, фильтры можно применить и к сете­ вым устройствам. В примере мы попросим NSP API предоставить список портов по имени порта (Port 1/1/1 в нашем случае): #get_JIOrts_filter.py import requests payload={} headers = { 'Authorization': 'Bearer {token}' .format(token) url = \"https://{{server)):8544/NetworkSupervision/rest/api/vl/ ports?filter=(name='Port 1/1/1') response = requests.request(\"GET\", url, headers =headers, data=payload) print(response.text) Результатом программы будет список портов с именем Port 1/1/1 на всех сетевых устройствах. Возможность получить порты на нескольких сетевых устройствах с помощью API -это одно из главных преимуществ работы с NMS. Далее мы узнаем, как обновить сетевой ресурс с помощью NMS API. Обновление порта на сетевом устройстве NМS API позволяет удобно создавать новые объекты или обновлять существую­ щие. Мы попробуем обновить описание порта, как в примерах ранее с библиотека­ ми Netmiko, NAP ALM и ncclient. Для обновления порта или интерфейса мы будем использовать другую конечную точку, доступную в модуле Network Function Manager for Packet (NFMP). NFMP -это модуль NMS для сетевых устройств Nokia на платформе Nokia NSP. Пошагово рассмотрим, как обновить описание пор­ та или внести любое другое изменение в сетевой ресурс: 1.Для обновления объекта или создания нового в уже существующем, потребуется полное имя объекта (Object Full Name, OFN), также известное как полное раз­ личимое имя (Fully DistinguishaЫe Name, FDN). OFN существующего объекта используется для обновления, а для создания нового используется OFN роди­ тельского объекта. Полное имя объекта выступает как первичный ключ для\n--- Страница 408 ---\nГлава 14. Python для автоматизации сети 413 уникальной идентификации объекта. В сетевых объектах Nokia, доступных в мо­ дулях NSP, каждый объект имеет атрибут OFN или FDN. Для получения OFN для обновляемого порта мы будем использовать API vl/rnanagedobjects/searchWithFilter со следующими критериями фильтрации: tupdate_port_clesc.py (часть 1) irnport requests irnport json token = <token obtain earlier> headers = { 'Content-Type': 'application/json', 'Authorization': 'Bearer {}' .forrnat(token) urll = \"https://NEМP_URL:8443/nfm-p/rest/api/vl/ rnanagedobjects/searchWithFilter\" payloadl = json.durnps({ }) \"fullClassNarne\": \"equiprnent .PhysicalPort\", \"filterExpression\": \"siteid ='<site id>' AND portNarne='123\", \"resultFilter\": [ \"objectFullNarne\", \"description\" response = requests.request(\"POST\", urll, headers=headers, data=payloadl, verify=False) port_ofn = response.json() [О] ['objectFullNarne'] 2.В этом коде мы задаем имя объекта fullClassNarnes. Полные имена класса объекта доступны в документации по объектной модели Nokia NFMP. Мы задаем filterExpression для поиска уникального порта по идентификатору сайта устрой­ ства и имени порта. Атрибут resultFilter ограничивает атрибуты, возвращаемые API в ответе. Нас интересует атрибут objectFullNarne в ответе этого API. 3.Затем мы используем другую конечную точку vl/rnanagedobjects/ofn для обновле­ ния атрибута сетевого объекта. В нашем случае мы меняем только атрибут description. Для операции обновления необходимо задать атрибут fullClassNarne в полезных данных и новое значение для атрибута description. К URL-aдpecy ко­ нечной точки мы добавим переменную port _ ofn, вычисленную на предыдущем шаге. Пример кода для этой части программы выглядит следующим образом: tupdate_port_clesc.py (часть 2) payload2 = json.durnps({ \"fullClassNarne\": \"equiprnent.PhysicalPort\", \"properties\": { \"description\": \"description added Ьу а Python\n--- Страница 409 ---\n414 Раздел 4. Python для веб-раэработки, облака и сети prograrn\" }) url2 = \"https:// NEМP_URL :8443/nfm-p/rest/api/vl/managedobjects/\"+port_ofn response = requests.request(\"PUT\", url2, headers=headers, data=payload2, verify=False) print(response.text) Автоматизация сети -это процесс создания и обновления множества сетевых объ­ ектов в заданном порядке. Например, можно обновить порт перед созданием служ­ бы IР-подключения для объединения двух и более локальных сетей. Такой сцена­ рий использования требует выполнения ряда задач для обновления всех задейство­ ванных портов, а также многих других объектов. NМS API позволяет организовать все эти задачи в программе для реализации автоматизированного процесса. Далее мы узнаем, как выполнить интеграцию с Nokia NSP или аналогичными сис­ темами для событийно-ориентированного взаимодействия. Интеграция с событийно-ориентированными системами Ранее мы обсуждали, как взаимодействовать с сетевыми устройствами и системами управления сетью с помощью модели «запрос-ответ», когда клиент отправляет запрос серверу, а тот посьmает ответ на этот запрос. Протоколы НТТР (REST API) и SSH основаны на этой модели. Она хорошо подходит для настройки системы или получения рабочего состояния сети на разовой или периодической основе. Но что если какое-то событие в сети потребует вмешательства рабочей команды? Предпо­ ложим, произошел аппаратный сбой оборудования или обрыв кабеля. Сетевые уст­ ройства в таких случаях подают сигналы оповещения, которые должны быть не­ медленно доведены до оператора (электронная почта, SМS-сообщение или панель мониторинга). Можно использовать модель «запрос-ответ» для ежесекундного опроса устройст­ ва и проверки, изменилось ли его состояние и возникло ли новое оповещение. Од­ нако это является неэффективным использованием ресурсов, которое приведет к излишнему трафику. Бьmо бы удобнее, если сетевые устройства или NМS сами от­ правляли сообщение клиентам при изменении состояния важных ресурсов. Та.кой тип модели называется событийно-ориентированной или управляемой событиями, и это популярный подход к мониторингу в реальном времени. Такие системы можно реализовать либо с помощью вебхуков (Webhook) или веб­ сокетов (WebSocket), либо с помощью потокового подхода. Веб-сокеты обеспечи­ вают двунаправленный транспортный канал по НТТР 1.1 через сокет TCP/IP. По­ скольку здесь не используется традиционная модель «запрос-ответ», веб-сокеты\n--- Страница 410 ---\nГлава 14. Python для автоматизации сети 415 хорошо подходят, когда нужно установить прямое (one-to-one) соединение между двумя системами. Это один из лучших вариантов взаимодействия между двумя программами в реальном времени. Веб-сокеты подцерживаются всеми стандарт­ ными браузерами, в том числе на устройствах iPhone и Android. Также это является популярным вариантом для многих социальных сетей, стриминговых приложений и онлайн-игр. Веб-сокеты -легкое решение для получения событий в реальном времени. Но ко­ гда множество клиентов хотят получать события из одной системы, более эффек­ тивным и масштабируемым решением будет потоковая передача. Такая событий­ но-управляемая модель обычно соответствует шаблону «издатель-подписчик» и включает три основных компонента: ♦Тема или топик (Topic): все потоковые сообщения или уведомления о событиях хранятся в теме, которая представляет собой что-то вроде каталога; топик по­ могает подписываться на интересующие нас темы, иначе мы будем получать все происходящие события. ♦Издатель (Producer или PuЫisher): это программа или часть ПО, которая от­ правляет события или сообщения в тему; в нашем случае это приложение NSP. ♦Подписчик или потребитель (Consumer или Subscriber): это программа, кото­ рая извлекает события или сообщения из темы; в нашем примере это написанная нами программа Python. Событийно-ориентированные системы можно использовать как с сетевыми устрой­ ствами, так и NMS. Платформы NМS используют системы событий (gRPC или SNMP) для получения сообщений от сетевых устройств в реальном времени и предлагают совместные интерфейсы для уровня оркестрации или для приложений по мониторингу и эксплуатации. В нашем примере мы будем взаимодействовать с событийно-управляемой системой на основе Apache Kafka от платформы Nokia NSP. Apache Kafka -это ПО с открытым исходным кодом, разработанное на Scala и Java. Оно обеспечивает реализацию шины для программного обмена сообщения­ ми на основе модели «издатель-подписчик». Прежде чем работать с Apache Kafka, перечислим список ключевых категорий (это другое название темы/топика в Apache Kafka), предлагаемых в системе Nokia NSP: ♦NSP-FAULT: категория событий, связанных со сбоями или аварийными оповеще­ ниями; ♦NSP-PACКET-ALL: категория используется для всех событий управления сетью, включая события подцержания активности (keep-alive ); ♦NSP-REAL-TIМE-KPI: категория событий для потоковой передачи уведомлений в ре- альном времени; ♦NSP-PACКET-STAТS: категория статистических событий. Полный список категорий доступен в документации Nokia NSP. Все они предлага­ ют дополнительные фильтры для подписки на события определенного типа. В кон­ тексте Nokia NSP мы будем использовать Apache Kafka для создания новой под­ писки, а затем для обработки событий. Начнем с управления подписками.\n--- Страница 411 ---\n416 Раздел 4. Python для веб-раэработки, облака и сети Создание подписок для Apache Kafka Для получения от Apache Kafka событий или сообщений нужно подписаться на те­ му или категорию. Обратите внимание, что одна подписка действительна только для одной категории. Обычно срок подписки истекает через 1 час, поэтому реко­ мендуется продлевать ее за 30 минут до окончания срока действия. Мы создадим новую подписку, используя API vl/notifications/subscriptions и сле­ дующий код: #suЬscriЬe.py import requests token = <token obtain earlier> url = \"https://NSP_URL:8544/nЬi-notification/api/vl/ notifications/suЬscriptions\" def create_subscription(cateqory): headers = {'Authorization': 'Bearer {}'.format(token) } payload = { \"categories\": [ \"name\": \" {}\". format (cateqory) response = requests.request(\"POST\", url, json=payload, headers=headers, verify=False) print(response.text) if name ==' main create _ subscription ( \"NSP-PACКET-ALL\") Вывод этой программы будет включать в себя важные атрибуты, например, subscriptionid, topicid И expiresAt: \"response\":{ \"status\" :О, \"startRow\" : О, \"enc:!Row\" : О, \"totalRows\" : 1, \"data\": { \"suЬscriptionid\":\"440e4924-d236-4fЬa-b590-a491661aae14\", \"clientid\": null, \"topicid\":\"ns-eq-440e4924-d236-4 fЬa-b590-a491661aae14\", \"timeOfSuЬscription\":1627023845731, \"expiresAt\":1627027445731,\n--- Страница 412 ---\nГлава 14. Python для автоматизации сети \"stage\": \"ACТIVE\" , \"persisted\" : true } , \"errors\":null 417 Атрибут subscriptionid используется для последующего продления или удаления подписки. Apache Kafka создаст тему специально для нее. Тема предоставляется через атрибут topicid, который можно использовать при подключении к Apache Kafka для получения событий. Это объясняет, почему мы называем общие темы категориями в Apache Kafka. Атрибут expiresAt указывает время истечения срок действия подписки. Как только подписка будет готова, можно подключиться к Apache Kafka для полу­ чения событий. Обработка событий от Apache Kafka Написание базового потребителя в Kafka занимает не более нескольких строк ко­ да Python при использовании библиотеки kafka-python. Для создания клиента Kafka мы будем использовать класс KafkaConsumer из этой библиотеки. Мы можем исполь­ зовать следующий код для использования событий из темы, на которую мы под­ писаны: #Ьasic_consumer.py topicid = 'ns-eg-ff15a252-f927-48c7-a98f-2965abбc187d' consumer = KafkaConsumer(topic_id, group_id='120', try: bootstrap_servers =[host_id], value deserializer =lamЬda m: json.loads (m.decode ( 'ascii')), api_version =(O, 10, 1)) for message in consumer: if message is None: continue else: print(json.dumps(message.value, indent=4, sort_keys=True)) except Keyboardinterrupt: sys.stderr.write('++++++ AЬorted Ьу user ++++++++\\n') finally: consumer.close() Важно отметить, если вы используете Python 3. 7 или выше, вам необходимо ис­ пользовать библиотеку kafka-python. С более ранней версией -библиотеку kafka.\n--- Страница 413 ---\n418 Раздел 4. Python для веб-разработки, облака и сети При использовании kafka с Python версии 3. 7 или выше существует ряд известных проблем. Например, существует ситуация, когда async является ключевым словом в Python версии 3.7 или выше, но в библиотеке kafka оно используется как перемен­ ная. Также существуют проблемы с версией API при использовании библиотеки kafka-python с Python 3.7 и более поздними версиями. Этого можно избежать, задав правильную версию API в качестве аргумента (в нашем примере это версия 0.10.0). Мы рассмотрели самый простой пример базового потребителя Kafka. В репозито­ рии для этой книги можно найти более сложный пример: https:/lgithub.com/nokial NSP-Integration-Bootstrapltree/master/kajka/ kajka _ cmd _ consumer. Продление и удаление подписки Продлить подписку в Nokia NSP можно через ту же конечную точку API, которая использовалась для создания подписки. Для этого надо в конец URL-aдpeca доба­ вить атрибут subscriptionid и ресурс renewals следующим образом: https://{{server}}:8544/nbi-notification/api/vl/notifications/ subscriptions/<suЬscriptionid>/renewals Удалить подписку можно через ту же конечную точку с атрибутом subscriptionid в конце URL-aдpeca, но используя НТТР-метод Delete. Конечная точка для запроса на удаление будет выглядеть следующим образом: https://{{ server}}:8544/nbi-notification/api/vl/notifications/ suЬscriptions/<subscriptionid> В обоих случаях мы не будем отправлять никаких аргументов в теле запроса. На этом можно завершить обсуждение интеграции с NМS и сетевыми контролле­ рами с помощью модели «запрос-ответ» и событийно-ориентированных систем. Оба подхода станут хорошей отправной точкой для интеграции с другими систе­ мами управления. Заключение В этой главе мы рассмотрели автоматизацию сети, включая ее преимущества и сложности для поставщиков телекоммуникационных услуг. Мы также изучили ключевые варианты использования автоматизации. Затем обсудили транспортные протоколы, доступные для взаимодействия с сетевыми устройствами. Познакоми­ лись с разными способами автоматизации сети. Сначала мы увидели, как взаимо­ действовать с сетевыми устройствами напрямую с помощью протокола SSH. После мы использовали библиотеки Paramiko, Netmiko и NAP ALM для передачи конфи­ гурации с сетевого устройства и на него. В дополнение мы изучили, как использо� вать NETCONF с Python для взаимодействия с сетевым устройством. Мы также рассмотрели примеры кода для работы с NETCONF и использовали библиотеку\n--- Страница 414 ---\nГлава 14. Python для автоматизации сети 419 ncclient сначала для получения конфигурации IР-интерфейса, а затем для его об­ новления на сетевом устройстве. В последней части главы мы рассмотрели, как взаимодействовать с системами управления сетью вроде Nokia NSP, используя Python в качестве клиента REST API и подписчика Kafka. Рассмотрели несколько примеров, как получить токен аутен­ тификации, а затем отправили REST API в NMS для получения данных конфигура­ ции и обновления конфигурации сети на устройствах. В эту главу включено несколько примеров, которые покажут, как использовать Python для взаимодействия с устройствами через SSH, NETCONF и REST API на уровне NМS. Эти знания важны инженерам по автоматизации, которые хотят эф­ фективно решать задачи с помощью Python. Данная глава завершает книгу. Мы рассмотрели не только основные концепции Python, но также дали представление об использовании языка во многих продвину­ тых областях, таких, как обработка данных, бессерверные вычисления, веб-разра­ ботки, машинное обучение и автоматизация сети. Вопросы 1.Как в библиотеке Paramiko называется часто используемый класс для подклю­ чения к устройству? 2.Из каких четырех уровней состоит NETCONF? 3.Можно ли отправить конфигурацию непосредственно в активную базу данных в NETCONF? 4.Почему gNМI лучше подходит для сбора данных, чем для настройки сети? 5.Предоставляет ли RESTCONF те же возможности, что и NETCONF, но через ин­ терфейсы REST? 6.Что такое издатель и потребитель в Apache Kafka? Дополнительные ресурсы ♦«Python для сетевых инженеров», (Mastering Python Networking), автор: Эрик Чоу (Eric Chou). ♦«Practical Network Automation», второе издание, автор: Абхишек Ратаи (Abhishek Ratan). ♦«Network Programmabllity and Automation», автор: Джейсон Эдельман (Jason Edelman). ♦Официальная документация по библиотеке Paramiko: http://docs.paramiko.org/. ♦Официальная документация по библиотеке Netmiko: https://ktbyers.github.io/.\n--- Страница 415 ---\n420 Раздел 4. Python для веб-раэработки, облака и сети ♦Официальная документация по библиотеке NAP ALM: https://napalm. readthedocs.io/. ♦Официальная документация по библиотеке ncclient: https :/ lncclient. readthedocs. io/. ♦Модели У ANG NETCONF: https://github.com/YangModels/yang. ♦Официальная документация по API Nokia NSP: https:/lnetwork. developer. nokia. com/api-documentation/. Ответы 1.Класс paramiko. SSHClient. 2.Содержимое, Операции, Сообщения и Транспорт. 3.Если сетевое устройство не поддерживает базу-кандидат, тогда обновления конфигурации непосредственно в активную базу допустимы. 4.gNMI основан на gRPC, протоколе от Google для вызовов RPC между мобиль­ ными клиентами и облачными приложениями. Протокол оптимизирован для пе­ редачи данных, что делает его более эффективным для сбора данных с сетевых устройств, но не для конфигурирования. 5.RESTCONF предоставляет большинство функций NETCONF через интерфейсы REST, но не все. 6.Издатель -клиентская программа, которая отправляет сообщения в тему (ка­ тегорию) Kafka в виде событий. Подписчик -это клиентское приложение, ко­ торое читает и обрабатывает сообщения из темы Kafka.\n--- Страница 416 ---\nПредметный указатель (( «Дзен Python» (The Zen of Python), 24 «клиент-сервер», 31 «Красный, Зеленый, Рефакторинг» (Red, Green, Refactor), 165 «модель-представление-контроллер» (Model View Controller, MVC), 305 «модель-представление-шаблон» (Model View Template, МVТ), 305 А Agile, 168 Amazon Web Services (AWS), 30 Apache Beam, 30 Apache Katka, 415 Apache Spark, 242 Application Programming lnterface, 321 ASGI (Asynchronous Service Gateway Interface), 339 Asyncio, 234 Atom, 42 в Business to Business (В2В), 307 с Chalice, 32 Counter, функция, 171 CRlSP-DM (Cross-Industry Standard Process for Data Mining), 29 D Data science, 43 DataFrame, 192 Development-Operations (DevOps), 168 DevOps (Development-Operations), 40 Docker, 347 Docstring, 32 Dunder (DouЫe Under), 36 Е ETL, 287 F filter, функция, 177 from, инструкция, 50 FTP,31 G Git, 39 GitHub, 40 Google Арр Engine (GAE), 43 Google Cloud Platform (GCP), 30 Google Container Registry, 3 51 gRPC, 396 н Hadoop MapReduce, 242 IDE. См. интегрированная среда разработки IDLE,42 importlib, библиотека, 52 init, файл, 63 J Java Virtual Machine (JVМ), 337\n--- Страница 417 ---\n422 JSON (JavaScript Object Notation), 139 JVМ (Java Virtual Machine), 251 м map, 175 Microsoft Azure, 30 МL. См. Машинное обучение МVР. См. Минимально жизнеспсобный продукт N NETCONF (Network Configuration), 394 NMS,390 Node.js, 32 р раndаs,библиотека, 192 PCollection, 288 РЕР 420: неявные пакеты пространства имен, 63 РЕР 8, 35 PortaЫe Operating System Interface (POSIX), 30 Prompt, 399 PTransform, 288 PyCharm,42 PyDev, 43 РуРА, 62 PySpark, 242 Python, 16 Python Packaging Authority (РуРА), 62 R RDD (Resilient Distributed Dataset), 247 reduce, функция, 177 REST API, 321 RESTCONF, 31 RESTful API, 321 s SCP, 31 SDK (Software Development Кit), 272 Предметный указатель Secure Shell (SSH), 31 Serverless, 32 Simple Storage Service (S3), 28 SMI (Structure of Management Information), 395 SNМP (Simple Network Management Protocol), 394 Spyder, 43 SSH (Secure Shell Protocol), 394 SSL, 307 str, 86 SuЫime Text, 42 Subversion (SVN), 42 т ТСР, 31 Test PyPI, 75 TLS, 307 u UАТ-тестирование (User Acceptance Testing), 146 UDP,31 V Visual Studio Code, 43 w WSGI (W еЬ Service Gateway Interface ), 339 WSGI-приложений, 32 у У АМL (У et Another Markup Language ), 139 z Zappa,32 Zero Touch Provisioning (ZTP), 393 zip, функция, 172\n--- Страница 418 ---\nПредметный указатель А Абсолютный путь, 54 Абстрактные методы, 100 Абстрактный класс, 100 Абстракция, 100 Автоматизация сети (Network Automation), 390 Автоматизация тестирования, 143 Алгоритм Монте-Карло, 265 Альфа-тестирование, 146 Анонимная функция, 121 Ассоциативный массив, 114 Б База управляющей информации (Management Infonnation Base, МIВ), 395 Базовый класс. См. родительский класс Бессерверная функция, 355 Бессерверные вычисления (Serverless Computing), 354 AWS Lambda, 357 Google Cloud Functions, 357,364 Azure Functions, 357 Бета-тестирование, 146 Блок управления задачей (Task Control Block, ТСВ), 221 Блок Управления Потоком (Thread Control Block, ТСВ), 208 Блок управления процессом, 221 Блок управления процессом (Process Control Block, РСВ), 221 Блокировка конфигурации, 396 Блочная диаграмма, 378 Бэкенд-сервис (Backend Service), 357: в Веб-приложение, 303 Веб-сервер, 306 Веб-сокет (WebSocket), 414 Веб-фреймворк, 304 Django, 304 Flask, 304 Вебхук (Webhook), 414 ВерблюжийРегистр (Came!Case), 37 Взаимоблокировка, 216 Включение (Comprehension), 190 · Включение словарей (Dictionary comprehension), 191 423 Включение списков (List comprehension), 190 Включение множеств (Set comprehension), 192 Вложенная функция. См. Внутренняя функция Вложенный класс, 87 Внутренний класс. См. Вложенный класс Внутренняя функция, 179 Выборочная сортировка, 100 Выполняемое зaдaниe{Running Executor), 264 г Гвидо ван Россум, 23, 35 Генератор (Generator), 120 геттер, 91 Гиперпараметры (Hyperparameters), 376 Гистограмма, 378 Глобальная блокировка интерпретатора (Global Interpreter Lock, GIL), 209 Глубокое обучение, 372 Градиентный бустинг (Gradient Boost, GB), 373 Граф, 247 Графический пользовательский интерфейс (Graphical User Interface, GUI), 276 Графический процессор (Graphics Processing Units, GPU), 221 д Дандер (Dunder), 86 Декларативное программирование, 79 Декоратор, 84 Дерево решений, 373 Деревья классификации и регрессии, 3 71 Деструкторы класса, 83 Дзен Python, 24 Динамическая типизация. См. Утиная типизация Документация Swagger, 307 Дочерний класс, 94\n--- Страница 419 ---\n424 ж Жесткий код (Hardcode ), 166 Средство логирования, 132 з Завершенное задание (Finished Executor), 264 Заводские приемочные испытания (Factory Acceptance Testing, FAT), 146 Замок (Lock), 215 Замыкание (Closure ), 180 Запись преобразования (Write transfoпn), 289 Защищенный метод (protected), 89 Зеленый поток (Green Thread), 233 и Идентификатор процесса (Process ID, PID), 221 Импорт модулей, 46 Инженерный совет Интернета (Internet Engineering Task Force, IETF), 395 Инкапсуляция данных, 87 Инструменты оболочки, 30 Интегрированная среда разработки (lntegrated Development Environment, IDE), 42 Интернет вещей (lnternet of Things, IoT), 357 Интерпретатор, 52 Интерфейс командной строки (Command Line Interface, CLI), 71 Интерфейс сетевого взаимодействия (Network Management lnterface, NMI), 396 Инфраструктура как услуга (lnfrastructure-as-a-Service, IaaS), 30 Искатель (fmder), 53 Исключение, 127 Искусственный интеллект (Artificial Intelligence, AI), 369 Исполнитель (Runner), 248 Итератор, 116 Итерация (lteration), 116 Предметный указатель к Каскадная модель, 28 Каскадная таблица стилей (Cascading Style Sheets, CSS), 306 Класс, 80 о абстрактный, 100 о вложенный, 87 Кластер (Cluster), 242 Кластерные вычисления (Cluster computing), 243 Ключевй показатель эффективности (Кеу Perfoпnance lndicators, КРI), 378 Колбэк (Callback), 236 Комментарий, 32 Коммит (Commit), 40 Конвейер обработки данных, 287 Конвейеры Azure (Azure Pipelines), 273 Конкурентная программа, 112 Конструктор класса, 83 Контейнер,347 Контейнер данных, 112 Контейнеризация (Containerization), 347 Контейнеры Docker, 30 Кооперативная многозадачность, 233 Кортеж (Tuple), 114 Корутин (Coroutine), 233 Критическая секция (Critical Section), 214 Кросс-валидация (Cross-validation), 375 Кумулятивная обработка (Cumulative Processing), 177 л Линейная регрессия, 3 71 Логгер, 132 Логирование (Logging), 131 Лямбда-функция (Lambda function), 178 м Магические методы (Magic Methods), 36 Масштабатор, 384 Масштабирование данных, 376 Машина опорных векторов (Support Vector Machine, SVC), 380\n--- Страница 420 ---\nПредметный указатель Машинное обучение (Machine Leaming), 28 Менеджер кластера (Cluster Manager), 247 Менеджер контекста, 122, 125 Метод, 84 Метод опорных векторов, 371 Метод удержания (Holdout), 379 Микросервис,335 Микросервисная архитектура, 334 Минимально жизнеспособный продукт (Minimum ViaЫe Product, МVР), 27 Многопоточность,208 Множество (Set), 115 Модель «запрос-ответ», 31 О модель машинного обучения, 369 Модель-кандидат, 376 Модификаторы доступа, 91 Модуль,58 Модуль АБС (Abstract Base Classes), 100 Мьютекс (Mutex, механизм взаимного исключения), 209 н Наследование, 94 Непрерывная доставка (Continuous Delivery, CD), 167 Непрерывная интеграция (Continuous Integration , CI), 167 Нормализация данных, 376 о Облачная IDE, 272 Облачная функция. См. Бессерверная функция Облачные вычисления (Cloud Computing), 271 Обработка естественного языка (Natural Language Processing, NLP), 268 Объект класса, 80 Объектно-ориентированное программирование (ООП), 79 Объектно-реляционное связывание (Object Relational Mapper, ОRМ), 315 Одиночка (Singleton), 48 Ожидающий объект (AwaitaЫe Object), 236 Окупаемость инвестиций (Retum On Investrnent, ROI), 393 425 Операции CRUD (Create, Read, Update, Delete), 322 Операции извлечения, преобразования и загрузки (Extract, Transform, Load - ETL), 287 Операция действия (Action), 249 Операция преобразования (Transformation), 249 Оркестратор, 390 Откат (Rollback), 396 Очередь, 113 п Пакет, 62 Парсер (Parser), 292 Парсинг (Parsing), 292 Первичный ключ (Primary key), 316 Перегрузка метода, 97 Переменная, 89 Переменная окружения (Runtime environment variaЫes), 360 Переопределение метода, 97 Платформа как услуга (Platform-as-a- Service, PaaS), 274 Подкласс. См. Дочерний класс Показатель F 1-score, 30 Полиморфизм, 97 Полное доменное имя (Fully Qualified Domain Name, FQDN), 399 Полное имя объекта (Object Full Name, OFN), 412 Полное различимое имя (Fully DistinguishaЫe Name, FDN). См. Полное имя объекта Пользовательский интерфейс (User Interface, UI), 304 Поставщик облачных сервисов (Cloud Service Provider, CSP), 31 Поток, 208 Поток-демон (Daemon Thread), 212 Правило двух пицц, 338 Предложения по улучшению Python (Python Enhancement Proposal, РЕР) 257,32\n--- Страница 421 ---\n426 Предметно-ориентированное проектирование (Domain-Driven Design, DDD), 337 Преобразование (Transform), 288 Примитив (Prirnitive ), 221 Программный интерфейс приложения (Application Programrning Interface, API), 100 Производный класс. См. Дочерний класс Производственная среда (Production environment, 138 Протокол SSL (Secure Sockets Layer), 307 Протокол TLS (Transport Layer Security), 307 Протокол WSGI (Web Server Gateway Interface), 309 Протокольный буфер (Protocol buffers, Protobuf), 396 р Рабочий узел (W orker node ), 248 Разработка через тестирование (Test­ driven development, TDD), 165 Распределенная файловая система Hadoop (Hadoop distributed file system, HDFS), 28 Распределенный набор данных, 242 Регрессионное тестирование, 146 Регулярное выражение (Regular Expression, regex), 31 Редактор Cloud Shell (Cloud Shell Editor), 285 Родительский класс, 94 Ромбовидное наследование ( diamond inheritance ), 96 с Сборщик мусора, 83 Северный интерфейс (North Bound Interface, NВI), 409 Семафор (Semaphore ), 215 Серверный процесс, 226 Сетевое программирование, 31 Сеттер, 91 Сеть как услуга (Network-as-a-service, NaaS), 31 Предметный указатель Система «издатель-подписчик» (PuЫish­ Subscribe system, PuЬ/Sub system), 356. Система контроля версий, 39 Система управления базами данных (СУБД), 307 Система управления сетью (Network Management System, NМS), 390 Системное программирование, 30 Сквозное тестирование (End-to-End, Е2Е), 145 Словарь (Dictionary), 114 Событийно-ориентированная система (Event-driven system), 390 Соглашение о конфигурации (Convention over configuration), 308 Соглашение об именовании, 60, См. РЕР 8 Сопрограмма, 234 Состояние гонки, 214,230 Специальный метод, 86 Список (List), 113 Среда выполнения (Runtime environment), 274 Среда разработки (Development environment), 138 Среда-оболочка (Shell environment), 273 Стек, 113 Строка (String), 112 Суперкласс. См. Родительский класс Сырые данные, 370 т Тасклет (Tasklet), 233 Тег-идентификатор (ID), 162 Тестирование Тестирование методом белого ящика (White-box Testing), 145 Тестирование методом черного ящика (Вlack-box Testing), 145 Тестирование на основе данных (Data- driven Testing, DDT), 161 Тест-кейс (test case), 146 Тестовый исполнитель, 148 Тестовый набор, 148 Тестовые фикстуры (Test Fixture), 147 Тонкая настройка гиперпараметров, 382\n--- Страница 422 ---\nПредметный указатель у Удаленный вызов методов (Remote Method Invocation, RМI), 321 Удаленный вызов процедур (Remote Procedure Call, RPC), 247, 321 Устойчивая функция (DuraЫe Functions), 357 Устойчивый распределенный набор данных (Resilient Distributed Datasets, RDD), 249 Утиная типизация, 104 ф Фабрика замыканий (Closure factory), 181 Фикстура, 14 7, 162 Фиктивное тестирование (Mock Testing), 145 Функция полезности, 84 Функция, 84 Функция как услуга (Function as а Service, FaaS), 354 Фьючерс (Future), 236 х Хеш-таблица, 114 ц Центральный процессор (ЦП), 208 Цикл событий (Event Loop), 233 ч 427 Частный метод (Private method), 89 Чтение преобразования (Read transform), 288 ш Шаблонизатор,306 Шард (Shard), 292 э Экземпляр класса, 80 Эксплуатационное приемочное тестирование (Operational Acceptance Testing, ОАТ), 146 я Язык гипертекстовой разметки (HyperText Markup Language, НТМL), 306 Ящик с усами, 378",
      "debug": {
        "start_page": 386,
        "end_page": 423
      }
    }
  ]
}
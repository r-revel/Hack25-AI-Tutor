{
  "title": "Байесовские модели восприятия и действия [2024] Вэй Цзи Ма, Конрад Кёрдинг, Дэниел Голдрайх",
  "chapters": [
    {
      "name": "Глава 1. Неопределенность и вывод 24",
      "content": "--- Страница 25 --- (продолжение)\nГлава 1 Неопределенность и вывод Как мы трансформируем наши сенсорные наблюдения в убеждения о состоянии мира? Всякий раз, когда мы что-то воспринимаем, делаем предсказание или об- думываем решение, мы рассуждаем, опираясь на вероятности, даже если не осознаем этого. Мы используем имеющуюся у нас информацию, чтобы сделать вывод или оценить что-то еще, что нас интересует. Имеющаяся у нас информация обычно неполная или зашумленная, поэтому наш вывод не является точным. Например, если мы наблюдаем блестящий пол (имеющая- ся информация), это говорит о том, что он может быть мокрым (предмет нашего интереса). Используя доступную сенсорную информацию и любые соответствующие знания, которые у нас могут быть, мы должны определить вероятность каждой интерпретации (мокрой или сухой). Как нам удается выносить здравые суждения в таких ситуациях? Краткое содержание главы Мы обрисовываем в общих чертах процесс перцептивного вывода, подчерки- вая неопределенность, присущую восприятию. Используя простые примеры, мы вводим вероятности, связанные с перцептивным выводом, правдоподо- бием, априорным и апостериорным распределением вероятностей, сосредо- точив внимание на лежащем в их основе неопосредованном мышлении1. Затем мы проиллюстрируем вездесущность перцептивного вывода в повседневной жизни серией примеров, связанных со зрительным и слуховым восприятием. В этой главе мы не используем математику, исследуя каждый пример только качественно и графически. Наша цель – дать интуитивное понимание про- цесса перцептивного вывода, которое послужит основой для более строгой математической формулировки в следующих главах. 1 Intuitions, мышление, не опосредованное сознанием. – Прим. перев.\nГлава 1 Неопределенность и вывод Как мы трансформируем наши сенсорные наблюдения в убеждения о состоянии мира? Всякий раз, когда мы что-то воспринимаем, делаем предсказание или об- думываем решение, мы рассуждаем, опираясь на вероятности, даже если не осознаем этого. Мы используем имеющуюся у нас информацию, чтобы сделать вывод или оценить что-то еще, что нас интересует. Имеющаяся у нас информация обычно неполная или зашумленная, поэтому наш вывод не является точным. Например, если мы наблюдаем блестящий пол (имеющая- ся информация), это говорит о том, что он может быть мокрым (предмет нашего интереса). Используя доступную сенсорную информацию и любые соответствующие знания, которые у нас могут быть, мы должны определить вероятность каждой интерпретации (мокрой или сухой). Как нам удается выносить здравые суждения в таких ситуациях? Краткое содержание главы Мы обрисовываем в общих чертах процесс перцептивного вывода, подчерки- вая неопределенность, присущую восприятию. Используя простые примеры, мы вводим вероятности, связанные с перцептивным выводом, правдоподо- бием, априорным и апостериорным распределением вероятностей, сосредо- точив внимание на лежащем в их основе неопосредованном мышлении1. Затем мы проиллюстрируем вездесущность перцептивного вывода в повседневной жизни серией примеров, связанных со зрительным и слуховым восприятием. В этой главе мы не используем математику, исследуя каждый пример только качественно и графически. Наша цель – дать интуитивное понимание про- цесса перцептивного вывода, которое послужит основой для более строгой математической формулировки в следующих главах. 1 Intuitions, мышление, не опосредованное сознанием. – Прим. перев.\n--- Страница 26 ---\nЦель восприятия  25 1.1. Цель восприятия Этот вопрос разрешить единственно разум обязан; Глаз же природу вещей познавать совершенно не может, А потому не вини его в том, в чем повинен лишь разум1. – Лукреций, De rerum natura [111] Люди, как и другие живые существа, наделены набором тонких органов чувств, с помощью которых они обнаруживают свойства окружающей сре- ды. Органы чувств реагируют на такие разнообразные свойства и явления окружающей среды, как свет (глаза), звук (уши), температура (кожа), тек - стура материала (кожа), химический состав (нос, язык) и положение тела (суставные и мышечные рецепторы, вестибулярные органы). Наши органы чувств составляют неотъемлемую часть нас самих, настолько, что мы обычно воспринимаем их присутствие как должное. Чтобы оценить роль, которую играют наши чувства, попробуйте представить себе жизнь без зрения, слуха, осязания, обоняния или вкуса. Какими бы сложными ни были органы чувств, их активация физически- ми раздражителями – это только первый шаг в восприятии. В повседнев- ной жизни нас не волнует длина волны (цвет) и интенсивность (яркость) попадающих в наши глаза световых волн или структура изменяющейся по амплитуде и времени акустической энергии, поступающей в наши уши. Мы заботимся не об этих паттернах сенсорной активации как таковых, которые мы называем сенсорными входами , или наблюдениями , а об их интерпретации. На самом деле качество нашей жизни – а часто и сама жизнь – зависит от нашей способности придумывать правильные интерпретации. Отражает ли этот рисунок света лицо друга? Является ли эта акустическая волна звуком ветра, воем собаки или голосом нашего спутника? Короче говоря, наш инте- рес заключается не в сенсорной информации как таковой, а в тех знаниях, которые она предоставляет о соответствующих состояниях мира2 [1]. Сделать интерпретативный переход от ощущения (активация органов чувств) к восприятию (вывод о состоянии мира) – сложная задача. Вообще говоря, эта книга о том, как мозг может оптимально выполнять эту задачу. Большой и быстро растущий объем экспериментальных и теоретических ра- бот показывает, что восприятие – это, по крайней мере неявно, процесс веро- ятностного вывода, в котором организм пытается сделать вывод о наиболее вероятном состоянии мира, используя сенсорные данные и все имеющиеся в его распоряжении соответствующие знания. Как гласит цитата Лукреция, восприятие иногда идет не тем путем, но ошибки часто можно рассматривать как побочные продукты разумной стратегии вывода. 1 Перевод: Ф. А. Петровский (1936). 2 Некоторые исследователи восприятия называют состояние мира дистальным стимулом (distal stimulus), а наблюдение – проксимальным стимулом (proximal stimulus).\n--- Страница 27 ---\n26  Неопределенность и вывод 1.2. Гипотезы и их вероятности Переход от ощущения к восприятию основан на условных вероятностях. Ус - ловная вероятность – это вероятность одного события при наличии другого: например, вероятность того, что у вас хорошее настроение, при условии что на улице идет дождь. Мы обозначаем условные вероятности как p(B | A), чита- ем «вероятность B при данном A». Осознают это люди или нет, в повседневной жизни мы очень часто делаем суждения об условной вероятности (рис. 1.1). (А) (D)(B) (E)(C) (F)p(эта книга самая интересная | из тех, что я читал недавно)p(я заболею, если съем это яблоко | его внешний вид, его запах, червь…)p(это голос мэра города | акустическая информация) p(я сделаю шаг и упаду | мои шнурки связаны между собой)p(переключение канала ТВ | в пульте нет батарейки)p(да, это он | личность, поведение, внешний вид) Рис. 1.1  Различные сценарии с вероятностными суждениями. Обозначение p(B | A) читается как «вероятность события B при данном событии A» Важно отметить, что условные вероятности не являются симметричными. В общем случае p(A | B) ¹ p(B | A). Например, большинство профессиональных баскетболистов высокие, но большинство высоких людей не являются про- фессиональными баскетболистами. Если A – это «профессиональный баскет - болист», а B – «высокий», то справедливо неравенство p (B | A) > p(A | B). Упражнение 1.1. Какая из двух условных вероятностей больше в каждом из приведенных ниже случаев и почему: p(дождь | облачно) или p (облачно | дождь); p(говорит по-французски | родился и вырос в Париже) или p(родился и вырос в Париже | говорит по-французски);\n--- Страница 28 ---\nГипотезы и их вероятности  27 p(холост | студент колледжа) или p (студент колледжа | холост); p(вы понимаете правило Байеса | вы читали эту книгу) или p(вы читали эту книгу | вы понимаете правило Байеса). Одним из способов визуализации вероятностей является использование площадей прямоугольников (рис. 1.2). Площадь прямоугольника A пропор- циональна вероятности события A, обозначаемой p(A), а площадь прямо- угольника B пропорциональна вероятности события B, обозначаемой p(B). Возвращаясь к примеру с баскетболом, высоких людей значительно больше, чем профессиональных баскетболистов, поэтому площадь лилового пря - моугольника намного больше, чем площадь бирюзового прямоугольника. Область перекрытия занимает почти всю площадь бирюзового прямоуголь- ника, показывая, что вероятность быть высоким, если человек является про- фессиональным баскетболистом, составляет почти 100 %. Однако площадь перекрытия намного меньше, чем площадь сиреневого прямоугольника, т. е. вероятность быть профессиональным баскетболистом просто при наличии высокого роста очень мала. Различие между p(A | B) и p(B | A) очевидно во множестве реальных примеров. Высокий Профессиональный баскетболист p(высокий | профессиональный баскетболист) > p(профессиональный баскетболист | высокий) Рис. 1.2  p(A | B) в общем случае не равна p(B | A). Площадь каждого прямоугольника представляет вероятность события во всем релевант - ном наборе (здесь все люди). Перекрытие двух прямоугольников пред- ставляет p(A, B). Размеры прямоугольников и их перекрытие здесь даны условно и не откалиброваны по фактическим данным о принадлежно- сти к баскетболу При восприятии известны сенсорные данные или наблюдения, которые непосредственно доступны наблюдателю, например схема активации фото- рецепторов сетчатки глаза. Учитывая эти наблюдения (A ), наблюдатель стре- мится сделать вывод о текущем состоянии мира (B ). Поскольку наблюдатель не знает истинного состояния мира, B – это гипотеза, которую наблюдатель развивает, и мы называем B гипотетическим, или предполагаемым, состоя- нием мира. Цель наблюдателя – оценить вероятность того, что мир находится в том или ином возможном состоянии. Например, наблюдатель может захо- теть узнать, насколько вероятно, что пол мокрый (гипотетическое состояние мира B ), при условии что пол блестит (наблюдение A ). Условная вероятность, представляющая интерес для наблюдателя, равна p(B | A) – это вероятность гипотетического состояния мира при данных сен- сорных наблюдениях. В зависимости от ситуации наблюдатель может быть озабочен оценкой условных вероятностей только двух гипотетических состояний мира (пол\n--- Страница 29 ---\n28  Неопределенность и вывод мокрый или сухой), нескольких различных состояний мира (животное на пути впереди – собака, кошка, кролик, енот или скунс) или даже континуума (бесконечное число) мировых состояний. В конечном счете нам хотелось бы выразить результаты нашего вывода, рассчитав вероятность каждого состоя- ния мира с учетом наблюдения (рис. 1.3). Это позволило бы нам принять обоснованное решение о состоянии мира. (А) (B) (C) p(собирается дождь | облачное небо)p(причина | симптомы) p(расстояние до автомобиля | видимое изображение) 0.5 0.00.1 0.01 0 ВероятностьВероятностьВероятность Нет 100 50 0 Да ОпьянениеОпухоль МигреньДругоеРасстояние, м Рис. 1.3  Различные виды распределений вероятностей: (А) две гипотезы; (B) множественные гипотезы; (C) непрерывное (бесконечное множество) гипотез Это мой друг? Предположим, вы видите вдалеке человека, который идет в вашу сторону, и задаетесь вопросом, не является ли он вашим другом (рис. 1.4). К какому бы выводу вы ни пришли, у вас будет определенная степень уверенности, и она может меняться со временем, пока вы продолжаете наблюдать за происходя- щим. Восприятие зрительных образов почти не требует от нас сознательных усилий, но распознавание сцен на самом деле является сложной вычисли- тельной задачей, которую мозг выполняет «за кадром». Как и все формы восприятия, распознавание сцен столь трудоемко, потому что сенсорный ввод, улавливаемый нервной системой (в данном случае зрительный об- раз), обычно совместим с несколькими интерпретациями. Зрительный образ может соответствовать вашему другу или постороннему человеку. В данном случае изображение предоставляет достаточную информацию, чтобы заклю - чить, что объект на самом деле является человеком, и оно может содержать данные о приблизительной форме человека (рост, обхват и т. д.). Со временем движущееся изображение может дополнительно предоставить информацию о походке человека. Тем не менее человек далеко, и ваши зрительные наблю- дения совместимы со многими возможными людьми.\n--- Страница 30 ---\nГипотезы и их вероятности  29 Посторонний Посторонний Посторонний Посторонний Посторонний ПостороннийДруг Друг Друг Друг Друг Друг(C) (B) (A) Правдоподобие Правдоподобие Априорное убеждениеАприорное убеждение «Я ожидаю встретить друга»«Моего друга нет в городе»p(obs | hyp) p(obs | hyp) p(obs | hyp)p(obs | hyp)p(hyp) p(hyp) Апостериорное убеждениеАпостериорное убеждение Рис. 1.4  Узнавание друга: (A) наблюдаемая сцена содержит изображение с низким разре- шением человека на расстоянии, похожего на вашего друга; (B) вы считаете, что вероятность получения данного зрительного образа от вашего друга больше, чем вероятность получения его от незнакомца (функция правдоподобия). Вы ожидали встретить своего друга в это время и в этом месте (априорное распределение). Следовательно, вы полагаете, что наблюдаемый человек является вашим другом (апостериорное распределение); (C) в этом альтернативном сценарии вы знаете, что ваш друг уехал из города, поэтому ваше априорное распределение явно свидетельствует в пользу гипотезы о незнакомце. Исходя из того же наблюдения (функция правдоподобия), вы заключаете, что человек, которого вы видите, не является вашим другом Правдоподобие (likelihood) гипотезы – это вероятность наблюдения при за- данном гипотетическом состоянии мира, т. е. p(наблюдение | гипотетическое состояние мира). График вероятностей всех гипотез резюмирует, насколько хорошо визуальное восприятие позволяет вам отличить одно возможное состояние мира от другого (например, друг или незнакомец). Это график функции правдоподобия (likelihood function). В общем случае на функцию правдоподобия влияет множество факторов. В нашем текущем примере рост человека (ваш друг высокий), цвет волос (каштановые) и манера держать голову (наклоненная) будут влиять на веро- ятность гипотез. Если вы продолжите наблюдать за приближением человека, график функции правдоподобия со временем будет становиться все более острым по мере поступления более четких и подробных наблюдений. Мы пока оставим в стороне то, как мы могли бы прийти к точной форме функ - ции правдоподобия. На данный момент достаточно понять, что функция правдоподобия представляет собой полное информационное содержание изображения, относящегося к рассматриваемому вопросу (это мой друг?). В частности, она представляет собой вероятность того, что зрительный образ соответствует вашему другу, по сравнению с вероятностью того, что тот же зрительный образ соответствует другому человеку. Хотя функция правдоподобия является важным компонентом нашего процесса вывода, ее недостаточно для решения стоящих перед нами задач. Функция правдоподобия отображает вероятность наблюдения при каждом гипотетическом состоянии мира: p(наблюдение | состояние мира). Мы стре-\n--- Страница 31 ---\n30  Неопределенность и вывод мимся найти апостериорное распределение – вероятность каждого возмож - ного состояния мира с учетом наблюдения: p(состояние мира | наблюдение). Чтобы определить апостериорное распределение, мы комбинируем функцию правдоподобия с априорным распределением, которое отображает априорную вероятность каждого состояния мира. Априорная вероятность гипотетиче- ского состояния мира, обозначаемая как p(состояние мира), – это вероят - ность состояния мира, основанная на всех имеющихся у вас знаниях, кроме наблюдения, – например, на вашей уверенности в том, что ваш друг будет присутствовать в наблюдаемой сцене, существовавшей еще до того, как вы посмотрели на улицу. Давайте рассмотрим два разных сценария, которые заставят вас иметь разные априорные распределения. Сценарий 1: вы договорились встретиться со своим другом на данной улице, приблизительно в то время, когда вы видите идущего к вам че- ловека, похожего на вашего друга. Сценарий 2: когда вы видите идущего к вам человека, похожего на вашего друга, вы удивляетесь, потому что знаете, что ваш друг должен быть в отпуске и не планировал возвращаться в город до следующей недели. Сенсорный ввод этих двух сценариев идентичен (рис. 1.4А), но ваш пер- цептивный вывод будет разительно отличаться. В первом сценарии вероят - ность p(друг) была высокой, и вы делаете вывод, что человек, идущий к вам, является вашим другом; во втором сценарии вероятность p(друг) низкая, и вы приходите к выводу, что этот человек, скорее всего, не ваш друг. Оче- видно, что ваши априорные вероятности играют решающую роль в процессе перцептивного вывода. Правило Байеса – фундаментальная теорема теории вероятностей – пока- зывает, как оптимально сочетать ожидание, представленное априорным рас - пределением, с наблюдением, представленным функцией правдоподобия, чтобы вычислить апостериорное распределение (рис. 1.4B–C). Апостериор- ная вероятность каждого состояния мира – это ваша вера в это состояние мира, основанная на всей соответствующей информации, имеющейся в ва- шем распоряжении. Правило Байеса гласит, что апостериорная вероятность пропорциональна произведению априорного знания и вероятности: Апостериорная вероятность = константа · априорное распределение · вероятность. (1.1) Апостериорный анализ основан на всех имеющихся у нас знаниях (т. е. на наших текущих сенсорных наблюдениях и соответствующих предшествую- щих знаниях). Когда мы начнем использовать фактические числа в главе 2, мы обсудим константу в этом уравнении, но пока это уравнение отражает соответствующие интуитивные предположения. Когда и априорное распре- деление, и вероятность выше для гипотезы A, чем для гипотезы B, тогда апостериорное значение также будет выше для гипотезы A. Однако если априорное знание благоприятствует гипотезе A, а вероятность – гипоте- зе B, то апостериорный вывод может быть любым, в зависимости от точных\n--- Страница 32 ---\nСенсорный шум и неоднозначность восприятия  31 числовых значений. Например, если одна величина (априорное знание или правдоподобие) лишь слегка поддерживает гипотезу А, а другая величина сильно поддерживает гипотезу В, то апостериорная вероятность склонится в пользу гипотезы В . 1.3. Сенсорный шум и неоднозначность восприятия В любой системе восприятия чем более плоское (т. е. более ровное) апосте- риорное распределение, тем более неоднозначны – то есть открыты для мно- жественных интерпретаций – наблюдения. Как мы видели, форма апосте- риорного распределения является результатом комбинации форм функции правдоподобия и априорного распределения. Неоднозначность восприятия часто возникает из-за широкой функции правдоподобия в отсутствие урав- новешивающего четкого априорного знания. Многие факторы могут снизить качество сенсорных данных и тем самым расширить функцию правдоподобия. К ним относятся физические особен- ности окружающей среды, ограничения органов чувств наблюдателя и его нервной системы (рис. 1.5). Одним из повсеместных факторов расширения функции правдоподобия является сенсорный шум. Под сенсорным шумом мы понимаем стохастическую изменчивость, присущую физическому про- цессу, порождающему сенсорное наблюдение. Из-за шума стимул, который повторяется одинаково в течение нескольких попыток, обычно каждый раз вызывает несколько разные сенсорные наблюдения. Рассеяние света, слу - чайная изменчивость окружающих звуков или биофизическая изменчивость частоты возбуждения сенсорных нейронов – все это примеры сенсорного шума. Как показывают эти примеры, сенсорный шум может возникать как во внешнем мире, так и внутри наблюдателя. Хотя сенсорный шум встречается повсеместно, это не единственная при- чина широты функций правдоподобия. Даже если бы весь сенсорный ввод можно было каким-то образом сделать бесшумным, многие функции визу - ального правдоподобия остались бы широкими по чисто геометрическим и оптическим причинам (рис. 1.6). Например, информация неизбежно те- ряется, когда трехмерный видимый мир отображается на двухмерное изо- бражение на сетчатке (или когда трехмерный слуховой мир отображается на два уха). Это схлопывание по измерению порождает множество случаев неоднозначности, в том числе двусмысленность размер–расстояние. Другим распространенным геометрическим источником функций широкого прав- доподобия является окклюзия, при которой объект частично закрывает поле зрения наблюдателя, так что сцена совместима со множеством альтерна- тивных конфигураций. В качестве последнего примера рассмотрим кажу - щуюся простой задачу восприятия оттенка серой поверхности по интенсив- ности света, который отражается от поверхности и попадает в ваши глаза. Конкретная интенсивность света, попадающего в ваши глаза, соответствует\n--- Страница 33 ---\n32  Неопределенность и вывод множеству комбинаций истинного оттенка поверхности и интенсивности источника света. Например, одинаковую интенсивность света, попадающего в глаза, может дать темно-серая бумага при солнечном свете или белая бума- га при тусклом свете. Поэтому если неизвестна интенсивность освещающего света, функция правдоподобия в этом сценарии широка. Мы рассмотрим эти и другие примеры с математической точки зрения в последующих главах. (C)(B)(A) Расстояние Периферийное зрение Ограничения нервной системыТемнота Стареющее зрениеТуман Блеск Закрытый вид Рис. 1.5  Источники сенсорной деградации, которые снижают качество зрительной инфор- мации, вызывая уплощение функций правдоподобия: (A) физические особенности окружаю- щей среды; (B) ограничения органов чувств наблюдателя. Большинство факторов, показанных в A и B, имеют аналоги в других смыслах. Например, в случае слухового восприятия расстоя­ ние, тихая речь, окружающий шум и стареющие органы слуха приводят к низкому качеству входных данных; (C) нервная система наблюдателя. В каждой сенсорной системе нейрон- ные ограничения, такие как ошибочные фоновые знания и нейронный шум, также создают проблемы для восприятия; (1) Жан Шофурье (1679–1757), «Пейзаж с городом или дворцом вдалеке»; (2) неизвестный британский художник, «Пейзаж с темным деревом», конец XVIII в.; (3) Камиль Писсарро (1830–1903), «Заходящее солнце и туман, Эраньи» (1891); (4) Дж. Тер- нер (1775–1851), «Замок Норхэм, Восход солнца» (ок. 1845 г.); (5) Майлз Биркет Фостер (1825–1899), «Обочина с живыми изгородями»; (6) периферийное зрение: «Глазная хирургия для коррекции косоглазия»; (7) структура глаза и зрительных нервов, иллюстрация из книги Питера Деграверса «Полный физико­медицинский и хирургический трактат о человеческом глазе и демонстрация естественного зрения» (Лондон: Б. Лоу, 1780 г.); (8) Жан­Батист Буржери (1797–1849), «Мозг», иллюстрация из книги Traité complet de l’anatomie de l’homme (1831–1854) Неоднозначность восприятия, возникающая из-за шума или других фак - торов, увеличивающих вероятность, может быть уменьшена или полностью предотвращена, если наблюдателю доступно четкое априорное распределе- ние. Априорные вероятности основаны на фоновых знаниях и, следователь- но, могут меняться со временем, по мере того как наблюдатель приобретает новые знания. Априорные знания также могут отличаться от одного наблю-\n--- Страница 34 ---\nБайесовский вывод в зрительном восприятии  33 дателя к другому. В целом те, у кого больше релевантных знаний, имеют более реалистичные априорные предположения, что способствует точному восприятию. Взгляните на рис. 1.6B. Какие предварительные знания о соба- ках могут помочь одному наблюдателю испытать меньшую двусмысленность в этой ситуации относительно другого наблюдателя, у которого нет таких знаний? (A) (B) (C) Глаз Рис. 1.6  Расширение функций правдоподобия, вызванное факторами, от - личными от сенсорного шума: (A) один и тот же размер изображения на сетчат - ке может соответствовать небольшому объекту, расположенному ближе к на- блюдателю, или более крупному объекту, находящемуся дальше; (B) куст мешает наблюдателю узнать, есть ли в этой сцене две (или даже больше) собаки или только одна очень длинная собака; (C) наблюдатель не может быть уверен в от - тенке поверхности, не зная также свойства падающего света 1.4. Байесовский вывод в зрительном восприятии Поскольку перцептивный вывод очень важен, мы хотим проиллюстрировать его дополнительными примерами из повседневной жизни. Наша цель – раз- вить интуитивное понимание вероятностей, априорных и апостериорных событий, а также оценить замечательную объяснительную силу байесовского вывода как модели восприятия. Мы покажем, что каждый пример имеет уни- кальные особенности, но каждый из них основан на объединении функции правдоподобия и априорного распределения по правилу Байеса для создания апостериорного перцептивного вывода. Мы надеемся, что эти примеры про- демонстрируют как богатство перцептивного вывода, так и широкую при- менимость байесовской перцептивной структуры.\n--- Страница 35 ---\n34  Неопределенность и вывод Восприятие влажности Когда люди перемещаются по миру, они полагаются на свои органы чувств, чтобы избежать опасностей. В современном мире опасности представлены во многих формах, например объект на нашем пути, быстро приближа- ющийся автомобиль или внезапная ступенька вниз. Еще одна опасность современной жизни – мокрый скользкий пол. Мокрый ли пол на рис. 1.7? Если это так (или может быть так), необходимо соблюдать осторожность и перемещаться медленно, небольшими шагами. Если это не так, мы можем без опаски целеустремленно шагать вперед. Как наше восприятие различает два варианта? (A) (B) Правдоподобие ПравдоподобиеАприорное убеждениеp(obs | hyp) p(obs | hyp) p(hyp)Мокрый Мокрый МокрыйСухой Сухой Сухой Рис. 1.7  Восприятие влажности: (А) функция правдоподобия, полученная из зрительного образа этой деревянной поверхности, благоприятствует «сухому» состоянию мира: p(наблю­ де ние | сухой) ≫ p(наблюдение | мокрый); (B) блестящая поверхность приводит нас к функции правдоподобия, которая благоприятствует «влажному» состоянию мира: p(наблю дение | мок­ рый) > p(наблюдение | сухой). Предостерегающий знак служит источником более выраженного априорного знания в пользу «мокрого» состояния мира Вероятность того, что поверхность будет блестящей, если она мокрая, боль- ше, чем вероятность того, что она будет блестящей, если она сухая. Таким об- разом, визуальная регистрация блестящей поверхности дает нам функцию правдоподобия, соответствующую мокрой поверхности. Важно понимать, что не только наши априорные предположения, но и наши вероятности за- висят от фоновых знаний. В общем случае, чтобы определить вероятности, наблюдатель должен иметь (неявное) понимание процесса, посредством которого различные состояния мира генерируют сенсорные данные. В дан- ном случае наблюдателю необходимо интуитивное знание законов оптики, а именно что мокрая поверхность намного лучше отражает свет.\n--- Страница 36 ---\nБайесовский вывод в зрительном восприятии  35 Как сказано выше, мозг комбинирует вероятности p(наблюдение | гипоте- тическое состояние мира) с априорными вероятностями p(гипотетическое состояние мира), чтобы сгенерировать вероятности, которые больше всего его интересуют: p(гипотетическое состояние мира | наблюдение). Эти по- следние вероятности называются апостериорными, чтобы подчеркнуть, что, в отличие от априорных вероятностей, они формируются после наблюдения (апостериори). Апостериорное распределение вероятностей представляет собой убеждение мозга в каждом возможном состоянии мира, основанное на всей соответствующей информации (т. е. наблюдениях и ожиданиях). Нам нужно вычислить апостериорную вероятность каждого гипотетическо- го состояния мира, p(состояние мира | наблюдение). Этот расчет включает перемножение априорных ожиданий и вероятностей. Напомним, что апри- орная вероятность p(мокрый) отражает априорное ожидание наблюдателя относительно влажности поверхности независимо от визуального наблю- дения. Например, еще до того, как наблюдатель войдет непосредственно в наблюдаемую зону, существует вероятность, что он выдвинет гипотезу мокрого пола. Как у наблюдателя возникают подобные априорные пред- положения? Фоновые знания, которые формируют априорные убеждения, могут быть приобретены в течение всей предшествующей жизни или совсем недавно. У наблюдателя может быть априорное убеждение, благоприятствующее су - хости, потому что подобные места по опыту наблюдателя большую часть вре- мени сухие. Однако если наблюдатель видит предупреждающий знак, указы- вающий на скользкую поверхность, его априорная оценка может измениться в пользу влажности. Чтобы показать зависимость априорного убеждения от фоновых знаний наблюдателя, мы иногда записываем его как p(состояние мира | B), где B снова означает информацию, полученную из предыдущего опыта. Поскольку и априорное убеждение, и правдоподобие зависят от фоновых знаний, апостериорное убеждение тоже зависит от фоновых знаний. Чтобы отметить эту зависимость, мы иногда записываем апостериорную вероят - ность отдельного состояния мира как p (состояние мира | наблюдение, B). Маскировка В животном мире выживание часто зависит от способности видеть, но не быть увиденным. Как отмечалось ранее, точная функция правдоподобия указывает на высокую информативность наблюдения, тогда как более плос - кая функция правдоподобия дает мало информации. Таким образом, в це- лом животному выгодно иметь острые чувства, которые производят четкие функции правдоподобия, когда животное смотрит на мир (рис. 1.8), но в то же время вести себя так или обладать такими физическими особенностями, которые формируют относительно плоскую функцию правдоподобия у дру - гих видов. В дикой природе многие виды развили черты и поведение, которые служат для маскировки их присутствия или идентичности (рис. 1.9). Эти разнообраз- ные примеры маскировки и мимикрии в животном мире можно рассмат -\n--- Страница 37 ---\n36  Неопределенность и вывод ривать как развитые стратегии, направленные на выравнивание функций правдоподобия наблюдателей. Возьмем, к примеру, гусеницу плодожорки (рис. 1.9А). Примечательно, что особи этого вида принимают цвет коры де- рева, на котором живут. Сливаясь с фоном, эти гусеницы защищают себя от хищных птиц. Зрительный образ, получаемый птицей, дает скудное указание на присутствие гусеницы. Хищники тоже выигрывают от маскировки. Рас - смотрим изображение леопарда, поджидающего добычу (рис. 1.9D). В высо- кой золотистой траве, чья окраска очень похожа на его собственную, леопард почти невидим для ничего не подозревающего наблюдателя. Хотя леопарды и другие крупные представители семейства кошачьих могут быстро бегать, им не хватает выносливости для длительных погонь. Их успех в охоте зависит от их способности незаметно приближаться к добыче. Примеров замаскиро- ванных хищников и добычи предостаточно в животном мире. Правдоподобие Априорное убеждениеp(hyp) Не тигр Не тигр Не тигр Не тигрНе тигр Не тигр Не тигрТигр Тигр Тигр ТигрТигр Тигр Тигрp(obs | hyp) p(obs | hyp) p(obs | hyp) p(hyp | obs) p(hyp | obs) p(hyp | obs)Видимая сцена Апостериорное убеждение Рис. 1.8  Влияние остроты зрения на апостериорное распределение. Три жертвы, имеющие одинаковое (20 %) априорное ожидание присутствия тигра (вверху слева), различаются по остроте зрения и, следовательно, формируют разные функции правдоподобия при столкно- вении с одной и той же наблюдаемой сценой. Чем более плоской является функция прав- доподобия, тем больше апостериорное распределение напоминает априорное. Вверху: для этого животного с плохим зрением наблюдаемая сцена порождает почти плоскую функцию правдоподобия. Таким образом, апостериорное распределение животного аналогично его априорному распределению; он мало чему научился из визуального наблюдения. В центре: животное с промежуточной остротой зрения имеет неплоскую функцию правдоподобия. Апо- стериорное распределение этого животного немного отличается от априорного. Внизу: для этого животного с превосходной остротой зрения сцена дает резкую функцию правдоподобия в пользу присутствия тигра. Апостериорное распределение указывает на вероятность присут - ствия тигра немного больше 50 %\n--- Страница 38 ---\nБайесовский вывод в зрительном восприятии  37 (C) (D) (A) (B) Рис. 1.9  Выравнивание функции правдоподобия в живом мире: (A) гусеница перечной огневки (Biston betularia) меняет свой цвет, чтобы слиться с фоном (ива или береза); (B) летаю- щая ящерица (Draco dussumieri) эффектно сливается с фоном коры дерева; (C) камбала лежит на морском дне; (D) маскирующийся леопард в Южной Африке (Panthera pardus) Пока функция правдоподобия наблюдателя не является идеально плоской, он способен хоть что-то извлечь из сенсорного ввода. Однако при наблюдении хорошо замаскированного животного функция правдоподобия наблюдателя почти неинформативна. Важно отметить, что форма функции правдоподобия зависит не только от наблюдаемой сцены, но и от остроты восприятия и со- образительности наблюдателя. Животное, которое для одного наблюдателя почти идеально замаскировано, может быть замечено другим наблюдателем, обладающим более тонким зрением или пониманием. Для наблюдателя, ко- торый по опыту знает, что гусеницы перечной моли, как правило, немного шире, чем ветки дерева, на котором они обитают, или что пятна леопарда немного отличаются по внешнему виду от окружающей растительности, одна и та же визуальная сцена даст более четкую функцию правдоподобия по срав- нению с наблюдателем, не имеющим этих фоновых знаний. Наряду с маскировкой эволюция породила сложные сенсорные системы и когнитивные способности, которые снижают неопределенность в отноше- нии присутствия и местонахождения других животных. В своего рода гонке вооружений животные развили все более острые сенсорные системы, чтобы обнаруживать своих все более и более скрытых противников. Показатель- ными примерами являются эволюция зрительной, слуховой и обонятельной систем млекопитающих, а также эволюция узкоспециализированных систем\n--- Страница 39 ---\n38  Неопределенность и вывод обнаружения, таких как ультразвуковая эхолокация, используемая насеко- моядными видами летучих мышей. В общем, животные выигрывают, если они остро воспринимают присутствие окружающих, в то время как окружа- ющие воспринимают их с трудом. По этой причине у животных развились впечатляющие системы восприятия, чтобы добиться более четкой функции правдоподобия для себя, и в то же время развилась маскировка, чтобы на- вязывать более широкие функции правдоподобия другим. 1.5. Байесовский вывод в слуховом восприятии До сих пор мы рассматривали зрительные примеры. Однако перцептивный вывод происходит в каждой сенсорной модальности. Например, люди жи- вут в акустически богатой среде: щебечут птицы, завывает ветер, лают со- баки, гудят автомобили, играет музыка и, что, пожалуй, самое главное, мы разговариваем друг с другом. Всякий раз, когда мы пытаемся определить источник звука (это лай собаки?), осознать его местонахождение (где эта лающая собака?) или интерпретировать его значение (какое слово вы только что произнесли?), мы выполняем перцептивный вывод. Чтобы оптимизиро- вать слуховое восприятие, байесовский наблюдатель будет комбинировать априорные вероятности с акустическими правдоподобиями, чтобы получить максимально точный вывод о восприятии. Птицы на проводе Люди полагаются, по крайней мере частично, на свой слух, чтобы определить местонахождение объектов. Мы и другие млекопитающие локализуем ис - точники звука, используя бессознательные расчеты, в том числе сравнивая интенсивность и время поступления звуков в два уха. Предположим, вы идете по улице прекрасным солнечным утром и заме- чаете силуэты пяти птиц, сидящих на проволоке (рис. 1.10А). Внезапно одна из птиц (не видно, какая) начинает мелодично петь. Какая птица поет? Ваша слуховая система быстро обрабатывает акустическое наблюдение, получая широкую функцию правдоподобия. Эта функция правдоподобия является непрерывной функцией от местоположения; то есть звук, который вы услы- шали, совместим с источником в континууме местоположений. Тем не менее некоторые места связаны с более высокой вероятностью, чем другие. Инте- ресно, что место с наибольшей вероятностью может не совпадать с точным местонахождением какой-либо птицы. Эта ситуация распространена в акус - тическом восприятии и может быть вызвана многими факторами. Например, если поющая птица не смотрела прямо на вас, то издаваемый ею звук мог отразиться от ближайших предметов, прежде чем достичь ваших ушей. Даже если звуковые волны смогли достичь ваших ушей по прямой, стохастическая изменчивость реакции вашей нервной системы может привести к тому, что\n--- Страница 40 ---\nБайесовский вывод в слуховом восприятии  39 функция правдоподобия достигнет пика в месте, которое немного смещено от местоположения источника. Правдоподобие ПравдоподобиеАприорное убеждение Априорное убеждение (A) (В)p(hyp | vis. obs) p(aud. obs | hyp) p(hyp | obs) p(hyp | vis. obs) p(aud. obs | hyp) p(hyp | obs)Расположение Расположение РасположениеРасположение Расположение РасположениеАпостериорное убеждение Апостериорное убеждение Рис. 1.10  Локализация источника звука: (A) зрительный образ птиц обеспечивает основу для предварительного распределения по местоположению источника звука. Широкая функ - ция правдоподобия отражает неточность акустического наблюдения. Апостериорное распре- деление свидетельствует в пользу гипотезы о том, что пела четвертая птица слева (*); (B) не- определенность восприятия заметно возрастает, если птицы собираются ближе друг к другу В отличие от функции правдоподобия визуальная информация в данном примере не непрерывна, а дискретна. Вы видите пять отдельных птиц. Ваше зрительное наблюдение, которое произошло до того, как птица зачирикала, дает вам априорное распределение. Таким образом, априорное распределе- ние отлично от нуля в пяти дискретных точках (мы предполагаем, что ваше зрительное восприятие очень точно сработало в этой высококонтрастной сцене). Обратите внимание, что априорные вероятности считаются равными для пяти птиц и что априорная вероятность того, что источник звука займет пустое место на проводе, равна нулю. Это просто означает, что до того, как вы услышали пение, вы с равной вероятностью предполагали, что любая из птиц будет петь. Используя правило Байеса, теперь мы можем рассчитать апостериорное распределение вероятностей для местоположения источника звука. Для каждого из пяти предполагаемых местоположений мы умножаем вероятность наблюдения на априорное распределение вероятностей. Полу -\n--- Страница 41 ---\n40  Неопределенность и вывод ченное апостериорное распределение указывает на четвертую птицу слева как на наиболее вероятный источник приятной мелодии. Интуиция подсказывает, что если бы птицы сидели ближе друг к другу на проволоке, наш вывод был бы менее точным. Этот результат действительно следует из байесовского вывода, как показано на рис. 1.10В. Здесь поющая птица изображена на прежнем месте, но трое других птиц сидят ближе к ней, чем раньше. Наше априорное распределение отражает новые положения птиц, но акустическое наблюдение и, следовательно, функция правдоподобия такие же, как и раньше. Апостериорное распределение теперь шире и ниже, что ука- зывает на то, что хотя наиболее вероятным певцом является та же самая птица, наша неопределенность заметно возросла. Действительно, по нашему мнению, поющая птица почти в равной степени могла быть третьей или четвертой слева. Прежде чем покинуть этот пример, мы хотели бы обратить ваше внима- ние на два альтернативных подхода к решению задачи, которые привели бы к одному и тому же ответу. В первом подходе, прежде чем смотреть на провод, мы могли бы начать с широкой однородной априорной оценки ги- потетических местоположений птиц, отражая тот факт, что до зрительного наблюдения мы понятия не имели, где какие птицы могут сидеть. Затем мы могли бы включить последующее визуальное наблюдение в функцию прав- доподобия и объединить ее с нашим плоским априорным распределением, чтобы получить апостериорное распределение по местонахождению птиц. Действительно, именно это первоначальное апостериорное распределение зрительного ввода мы использовали здесь в качестве априорного распреде- ления для нашего анализа слухового наблюдения. Этот подход иллюстрирует важную общую черту байесовского вывода: его можно делать итеративно, при этом апостериорное распределение одного вывода используется в ка- честве априорного распределения для следующего. О втором подходе, который приводит к тому же ответу, будет подробно рассказано в главе 5. Вкратце он заключается в следующем: начиная с плос - кого априорного распределения вероятностей по позициям, мы могли бы одновременно включить как зрительные, так и акустические наблюдения как функции правдоподобия в процедуру, называемую объединением сиг- налов. В этом подходе мы не будем использовать зрительную информацию для создания априорного распределения для последующего слухового на- блюдения, а вместо этого объединим зрительную функцию правдоподобия, которая имеет пять дискретных пиков, с непрерывной слуховой функцией правдоподобия. По сути, когда у нас есть два или более независимых ис - точника информации, мы можем выбрать, следует ли использовать разные источники последовательно, при этом апостериорные значения каждого наблюдения используются в качестве априорных для следующего, или все сразу, когда все наблюдения вводятся через функции правдоподобия. Таким образом, границы между функциями правдоподобия и априорными распре- делениями вероятностей часто размываются, а выбор того, как использовать информацию, остается за байесовским разработчиком моделей. Эта гибкость является не проблемой, а преимуществом байесовского подхода. Внутренняя согласованность правил байесовского вывода гарантирует, что если в выводе задействована вся доступная информация, результирующее апостериорное\n--- Страница 42 ---\nБайесовский вывод в слуховом восприятии  41 распределение будет одним и тем же независимо от выбранного маршрута. В рамках байесовской модели часто существует несколько способов прийти к одному и тому же решению. Мондегрин Восприятие речи требует сложных умозаключений на различных уровнях, хотя наш мозг делает это автоматически и без особых видимых усилий. Оче- видно, мы должны правильно воспринимать произнесенное слово. Легко неверно истолковать даже одно слово, произнесенное отдельно, особенно в присутствии окружающего шума (гул автомобильного двигателя, уличные звуки, болтовня окружающих людей, бегущая вода в раковине, шум вентиля- ции здания и т. д.). В таких условиях, близких к низкоконтрастному зрению, функции правдоподобия широки, и одно слово может быть неправильно воспринято за другое, похожее по звучанию. На самом деле неправильно воспринимаемая речь настолько распространена, что люди часто пропус - кают эти моменты, не задумываясь. Вы можете попытаться составить спи- сок таких случаев самостоятельно. Результаты и познавательны, и забавны. Например, в разговорах с другими мы ошибочно принимаем Монголию за магнолию, уточку за удочку, батон за бетон, сетку за ветку и т. д. Как пока- зывают многочисленные примеры, помимо похожего звучания разных слов, проблема восприятия речи возникает из-за того, что паузы между произно- симыми словами часто не длиннее пауз между слогами в одном слове. Сле- довательно, точное определение, где заканчивается одно слово и начинается следующее, – нетривиальная задача. Эта трудность синтаксического анализа может привести к ошибкам, при которых слоги из разных слов неправильно сочетаются в нашем восприятии. В детстве писательница Сильвия Райт любила слушать популярную в XVII ве - ке шотландскую балладу «Прекрасный граф о’Морей», которую ей часто рас - сказывала мать. Ей особенно нравились грустные, но красивые строки, опи- сывающие гибель графа и любви всей его жизни, леди Мондегрин: Вы, горы и долины, – Где вы были, Когда враги убили графа о’Морея И леди Мондегрин (208: 48–51). Слова, услышанные юной Сильвией Райт, были вовсе не теми, что говорила ее мать, как бы впечатляюще они ни звучали. На самом деле в балладе во- обще не упоминается леди Мондегрин. Несчастного мертвого графа уложили на траву в одиночестве; враги «повергли его на траву» (laid him on the green). Творческая, но ошибочная интерпретация услышанной баллады Сильвией Райт отражает ошибку восприятия. Она интерпретировала звуки «laid hi-» как «леди» (lady), а «-m on the green» как «Mondegreen». Позднее Сильвия Райт вве- ла термин «мондегрин» для обозначения неправильно расслышанного слова или фразы [208]. Поскольку разговорному языку вообще присуща фонетиче- ская двусмысленность, примеры мондегринов встречаются на каждом шагу. Когда Квинсленд в Австралии был затоплен тропическим циклоном Таша,\n--- Страница 43 ---\n42  Неопределенность и вывод газета Morning Bulletin of Rockhampton (6 января 2011 г.) сообщила трагиче- скую новость о том, что в результате наводнения «более 30 000 свиней унесло течением реки Доусон с прошлых выходных» (More than 30,000 pigs have been floating down the Dawson River since last weekend) [28]. Эта поразительная исто- рия, основанная на интервью между репортером и владельцем местной сви- нофермы, была чрезвычайно ошибочной. Хозяин упоминал не 30 thousands pigs (30 000 свиней), а 30 sows and pigs (30 свиноматок и поросят), унесенных вниз по течению! На следующий день газета опубликовала исправление. Книги и многие веб-сайты посвящены перечислению любимых мондегри- нов людей, особенно возникающих из-за неправильно услышанных текстов песен. Очень любопытно и поучительно заглянуть на веб-сайты, где слушате- ли публикуют свои версии одних и тех же песен, которые они не расслышали. Многочисленность людей, ошибочно услышавших знаменитые строки «пора- пора-порадуемся на своем веку красавице Игуку, счастливому клинку», по- видимому, отражают как фонетическую двусмысленность (функция широкого правдоподобия), так и невероятное содержание (низкая априорная вероят - ность) исходного текста. Что касается априорных вероятностей, предложение «Туалет справа», безусловно, является более распространенным предложе- нием, чем «Восходит ненастная луна», а «подводная лодка» в качестве вида транспорта, возможно, более правдоподобна, чем «летний бриз» (рис. 1.11). ПравдоподобиеАприорное убеждениеАпостериорное убеждениеp(hyp) p(obs | hyp) p(hyp | obs) Ветерок Ветерок Ветерок Субмарина Субмарина СубмаринаLucy in the sky with diamonds (Люси в искрящейся вышине) – The Beatles ”Lucy and this guy with diamonds” (Люси и этот парень с бриллиантами) ”Lucy in disguise with diamonds” (Люси по уши в бриллиантах) ”Lucy in the sky with Simon” (Люси в небесах с Саймоном)There’s a bad moon on the rise (Восходит ненастная луна) – Creedence Clearwater Revival “There’s a bathroom on the right” (Туалетная комната расположена справа) And you come to me on a summer breeze (Тебя принес мне летний ветерок) – Bee Gees (How Deep Is Your Love) “And you come to me on a submarine” (Ты приплыла ко мне на субмарине) The Death of Lady Mondegreen (Смерть леди Мондегрин) – Harper’s Magazine (Nov. 1954) Рис. 1.11  Мондегрины являются результатом сочетания фонетической неоднозначности (функции широкого правдоподобия) и низкого ожидания реальной фразы, которая поется или произносится (предварительное распределение в пользу «неправильной» гипотезы) Возникновение мондегринов убедительно свидетельствует о том, что вос - приятие речи, как и зрительное восприятие, является результатом комби- нации функций правдоподобия и априорных распределений. Люди обычно\n--- Страница 44 ---\nБайесовский вывод в слуховом восприятии  43 воспринимают речь точно, но, конечно, случайные ошибки неизбежны. Дей- ствительно, «чем неразборчивее исходный текст, тем больше вероятность того, что слушатели услышат то, что хотят услышать» [139]. Перефразируя в терми- нах байесовского вывода: чем более плоской является функция правдоподо- бия, тем больше будет влияние априорного распределения на результирующее апостериорное распределение. Таким образом, включение в вывод о восприя- тии априорного ожидания, которое в большинстве случаев повышает точность восприятия, может иметь неприятные последствия, создавая мондегрины в тех случаях, когда мы сталкиваемся с неожиданным (низкоаприорным) сло- вом, которое звучит как другое, более ожидаемое (высокоаприорное) слово. Располагая этими знаниями, довольно легко вызвать мондегрины у других людей. Просто выберите два разных слова или фразы, которые звучат одина- ково, убедитесь, что у вашего слушателя сформировано априорное убежде- ние в пользу одного из слов или фраз, а затем произнесите другое слово или фразу со схожим звучанием. Например, вы можете сказать другу: «Знаешь, люди очень хорошо распознают речь; на самом деле мы можем понимать речь намного лучше, чем даже самые лучшие компьютерные программы. Мы подсознательно знаем, как разломать печь. Что я только что сказал? Мы под- сознательно знаем, как…?» Если вы произнесли слова «разломать печь» ес - тественным тоном и вам удалось не выделить их на фоне остальной речи, ваш друг, вероятно, услышал «распознать речь», а не слова, которые вы на самом деле произнесли. С байесовской точки зрения функция широкого правдопо- добия, с которой столкнулся ваш друг, будет сочетаться с четким априорным распределением (учитывая предыдущее содержание вашего дискурса) в поль- зу гипотезы «распознать речь». Более детально этот пример рассмотрен в [177]. Даже когда слушатели правильно воспринимают каждое слово, они стал- киваются с последней проблемой: определить предполагаемое значение цепочки слов. Опять же, это часто требует оценки нескольких гипотез. Предположим, кто-то сказал вам, что некий мост «держится за счет тросов и опор». Утверждения, подобные этому, даже если их хорошо услышать или прочитать на печатной странице, тем не менее согласуются с двумя или бо- лее интерпретациями [39]; то есть такие утверждения вызывают широкую функцию правдоподобия, обусловленную не фонетической двойственно- стью, а неоднозначностью толкования смысла (этот мост прочный или на- оборот?); существует также более прямая семантическая двусмысленность (рис. 1.12). В других случаях сама структура предложения неоднозначна, что называется синтаксической двусмысленностью. Когда мы слышим или чи- таем двусмысленное предложение, мы естественным образом комбинируем функции правдоподобия с априорным распределением и обычно достигаем правильного восприятия. Мы, однако, иногда сбиты с толку – и удивлены – на мгновение, когда обе интерпретации приходят нам на ум. Это пришло в голову одному из авторов, когда приятель рассказал ему о путешествии по лесу, которое он совершил со своими родителями. «Повсюду была дикая при - рода, – воскликнул он. – Однажды я увидел медведя вместе с моей мамой» (рис. 1.13). Хотя эта книга не посвящена подобным ситуациям неоднозначно- сти, мы отмечаем здесь сам факт их существования, чтобы проиллюстриро- вать, что неопределенность и умозаключения играют роль на многих уровнях перцептивной и когнитивной обработки.\n--- Страница 45 ---\n44  Неопределенность и вывод У вас есть оговорки/ сомнения?Вовсе нет… Мы слышали, что это прекрасное место!Так­с, какой у тебя улов/доход? (reservations означает как «оговорки (отступления) в речи», так и «сомнения, подозрения» – прим. перев.)(net worth означает как «улов, попавший в сеть», так и «чистый бухгалтерский доход» – прим. перев.) Рис. 1.12  Семантическая неоднозначность языка Правдоподобие Априорное убеждениеp(hyp) p(obs | hyp) p(hyp | obs)Апостериорное убеждение Гипотезы 1 Гипотезы 1Гипотезы 1 Гипотезы 1 Гипотезы 2 Гипотезы 2Гипотезы 2 Гипотезы 2 «Во время совместной прогулки я и мама увидели медведя»«Я увидел медведя, гуляющего с моей мамой»«Я с ужасом видел, как медведь подошел к моей маме!» Рис. 1.13  Перцептивный вывод в условиях синтаксической неоднозначности. Каждое со- стояние мира (гипотеза) может быть описано многими различными способами, три из которых показаны на рисунке. Докладчик выбрал выражение, которое может описывать два состояния мира: «я увидел медведя вместе с моей мамой». Базовые знания человека предполагают, что медведи с меньшей вероятностью будут прогуливаться рядом с людьми, чем будут замече- ны на расстоянии, поэтому априорное распределение благоприятствует гипотезе 1. Функция правдоподобия показывает, что произносимое предложение имеет примерно одинаковую вероятность при двух гипотезах. Следовательно, апостериорное распределение благоприят - ствует гипотезе 1\n--- Страница 46 ---\nИсторический обзор: восприятие как бессознательное умозаключение  45 1.6. Исторический обзор: восприятие как бессознательное умозаключение Правило Байеса названо в честь английского министра и математика Тома- са Байеса (1702–1761), который интересовался проблемами обратной веро- ятности, главным образом тем, как вычислить p(B | A), когда известны p(A) и p(A | B). В книге Байеса An Essay towards Solving a Problem in the Doctrine of Chances («Опыт решения задачи доктрины вероятностей»), опубликованной посмертно в 1763 г., были заложены основы исчисления условной вероят - ности – области статистических рассуждений, которая сейчас называется байесовским выводом. Позднее правило Байеса независимо вывел француз- ский математик и физик Пьер-Симон маркиз де Лаплас (1749–1827). Лаплас чрезвычайно продуктивно применил эту формулу к задачам в широком диа- пазоне дисциплин. Важно отметить, что Лаплас также признал вездесущ- ность вероятности, заявив, что «наиболее важные вопросы жизни на самом деле, по большей части, являются только проявлениями вероятности. Строго говоря, почти все наши знания лишь вероятны» [106]. Действительно, сегодня байесовский вывод играет все более важную роль в необычайно разнообраз- ном наборе дисциплин, охватывающих почти все области науки и техники: нейробиологию, психологию, эволюционную и молекулярную биологию, гео- логию, астрономию, экономику, робототехнику и компьютерные науки, и это лишь малая их часть. Однако идея о том, что восприятие является формой бессознательного вывода, возникла независимо от Байеса и Лапласа. Это за- слуга нескольких ученых. Древний арабский физик и эрудит Ибн аль-Хайсам (965–ок. 1040) прозорливо отметил, что «не все, что воспринимается зре- нием, воспринимается грубым ощущением; вместо этого многие зримые характеристики будут восприняты посредством суждения в сочетании с ощущением видимой формы». Так, «знакомые видимые объекты воспри- нимаются зрением через определяющие признаки и через предшествующие знания» [10]. Значительно позже немецкий врач и физик Герман фон Гельм- гольц (1821–1894) снова высказал мысль о том, что восприятие есть форма бессознательного умозаключения, красноречиво заявив, что «предыдущий опыт действует в сочетании с наличными ощущениями, чтобы произвести перцептивный образ» [190]. Идеи аль-Хайсама и Гельмгольца прекрасно со- гласуются с мнением о том, что восприятие является формой байесовского вывода (рис. 1.14). Однако трудно установить, является ли та или иная форма перцептивного вывода сознательной или бессознательной, и мы не будем рассматривать этот вопрос в нашей книге.\n--- Страница 47 ---\n46  Неопределенность и вывод Апостериорная вероятностьПравдоподобие Априорная вероятность Томас Байес, 1702–1761 Ибн аль­ Хайсам, 965 – ок. 1040 «Знакомые видимые объекты воспринимаются зрением через определяющие признаки и через предшествующие знания». — Книга оптики Пьер­Симон Лаплас, 1749–1827 «Наиболее важные вопросы жизни на самом деле, по большей части, являются только проявлениями вероятности. Строго говоря, почти все наши знания лишь вероятны». — Философские размышления о вероятности Герман Людвиг фон Гельмгольц, 1821–1894 «Предыдущий опыт действует в сочетании с наличными ощущениями, чтобы произвести перцептивный образ». — Трактат по физиологической оптике p(hyp | obs) p(obs | hyp) p(hyp) · Риc. 1.14  Авторитетные ученые, развивавшие байесовский вывод и взгляды на то, что вос - приятие является бессознательным умозаключением. Стоит отметить, что список не ограничи- вается только ими, просто история сохранила для нас не все имена 1.7. Заключение В этой главе мы представили концепцию, согласно которой восприятие по своей сути является вероятностным и как таковое оптимально характери- зуется как процесс байесовского вывода. Что касается байесовского вывода, вы узнали следующее: условные вероятности, такие как p(A | B), представляют вероятность A при заданном B. В байесовском выводе о восприятии A и B обычно представляют состояние мира и наблюдение; функция правдоподобия p(наблюдение | состояние мира) отражает информационное содержание сенсорного наблюдения, относящееся к различению одного состояния мира от другого;\n--- Страница 48 ---\nРекомендуемая литература  47 чем более плоской является функция правдоподобия, тем меньше по- лезной информации мы получаем от наших органов чувств. Если функ - ция правдоподобия совершенно плоская, то наблюдатель ничему не научился из наблюдения; в некоторых случаях, например в примере вывода «Это мой друг?», вероятность меняется со временем; априорное распределение вероятностей по состояниям мира p(сос тоя- ние мира) суммирует информационное содержание наших прошлых наблюдений, т. е. исходные знания, которые у нас есть о мире. Восприя - тие основано не только на сенсорных наблюдениях, но и на ожиданиях, основанных на предыдущем опыте; более плоские априорные распределения означают, что мы меньше знаем о потенциальных состояниях мира; согласно правилу Байеса, апостериорная вероятность каждого гипо- тетического состояния мира p(состояние мира | наблюдение) вычисля- ется, исходя из вероятностей и априорных знаний о состояниях мира; процедуры байесовского вывода в равной степени применимы к си- туациям, в которых гипотетические состояния мира дискретны или непрерывны; сценарии восприятия, будь то зрение, слух или другие чувства, под- вержены разным уровням неопределенности; восприятие речи сопряжено с фонетической и синтаксической дву - смысленностью, что часто приводит к возникновению функций плоского правдоподобия. Комбинация априорных значений и веро- ятностей может привести к неверным интерпретациям, таким как мондегрины. 1.8. Рекомендуемая литература Alhacen. Alhacen’s Theory of Visual Perception: A Critical Edition, with Eng lish Translation and Commentary, of the First Three Books of Alhacen’s “De As pec­ tibus”, the Medieval Latin Version of Ibn Al­Haytham’s “Kitb Al­Manz. ir. Edi ted by A. Mark Smith. Vol. 1. Philadelphia, PA: American Philosophical Society, 2001. Thomas Bayes. An Essay towards Solving a Problem in the Doctrine of Chances. Biometrika 45, no. 3–4 (1958): 296–315. Peter Brugger and Susanne Brugger. The Easter Bunny in October: Is It Disguised as a Duck? Perceptual and Motor Skills 76, no. 2 (1993): 577–578. Daniel Burdon. Pigs Float down the Dawson. Morning Bulletin, February 9, 2011. https://realtegan.blogspot.com/2011/02/best ­correction­ever.html. Gloria Cooper. Red Tape Holds up New Bridge, and More Flubs from the Nation’s Press. New York: TarcherPerigee, 1987. Wilson S. Geisler and Randy L. Diehl. A Bayesian Approach to the Evolution of Perceptual and Cognitive Systems. Cognitive Science 27, no. 3 (2003): 379–402.\n--- Страница 49 ---\n48  Неопределенность и вывод Gary Hatﬁeld. Perception as Unconscious Inference . In Perception and the Physical World: Psychological and Philosophical Issues in perception. Chi- chester: John Wiley, 2002. Hermann von Helmholtz. Treatise on Physiological Optics. Edited by James P . C. Southall. Vol. 3, The Perceptions of Vision. New York: Optical Society of America, 1925. Pierre-Simon Laplace. A Philosophical Essay on Probabilities. Edited and trans- lated by Andrew I. Dale. New York: Springer Science and Business Media, 2012. Mohamed A. F. Noor, Robin S. Parnell, and Bruce S. Grant. A Reversible Color Polyphenism in American Peppered Moth (Biston betularia cognataria) Caterpillars. PloS One 3, no. 9 (2008): e3142. Pamela Licalzi O’Connell. Sweet Slips of the Ear: Mondegreens. New York Times, April 9, 1998. Russell Smith. Milk Drinkers Turn to Powder and Other Pun­ishing Headlines. Globe and Mail (September 23). Stephen M. Stigler. Who Discovered Bayes’ Theorem? American Statistician 37, no. 4a (1983): 290–296. J. V. Stone, I. S. Kerrigan, and J. Porrill. Where Is the Light? Bayesian Perceptual Priors for Lighting Direction. Proceedings of the Royal Society B: Biological Sciences 276, no. 1663 (2009): 1797–1804. Dave Tompkins. How to Wreck a Nice Beach: The Vocoder from World War II to HipHop, the Machine Speaks. New York: Melville House, 2011. Michel Treisman. Motion Sickness: An Evolutionary Hypothesis. Science 197, no. 4302 (1977): 493–495. Sylvia Wright. The Death of Lady Mondegreen. Harper’s Magazine 209, no. 1254 (1954): 48–51. 1.9. Задачи Задача 1.1. Перефразируйте с точки зрения байесовского вывода восприя тия следующее высказывание Ибн аль-Хайтама, записанное примерно 1000 лет назад: «Когда взгляд увидит среди цветов в каком-нибудь саду розовый цвет, мы немедленно приходим к заключению, что это розы, потому что такой цвет присущ только розам. Но этого не происходит, когда зрение восприни- мает в саду миртово-зеленый цвет. Ибо, когда мы видим в саду только мир- товую зелень, мы не приходим к выводу, что это мирт, просто из восприятия зелени, потому что многие растения зеленые, а кроме того, некоторые рас - тения похожи на мирт по оттенку и форме» (Alhacen, De aspectibus, книга 2, Alhacen’s Theory of Visual Perception, пер. Смита). Задача 1.2. Почему мы называем себя в самом начале телефонного разгово- ра даже с уже знакомыми людьми, но не делаем этого при личной встрече? Изложите свой ответ в рамках байесовского вывода о восприятии. Задача 1.3. Когда собеседник говорит тихо или когда разговор происходит в присутствии значительного окружающего шума, мы иногда затыкаем уши\n--- Страница 50 ---\nЗадачи  49 и/или внимательно смотрим на губы говорящего. Почему, с точки зрения байесовского восприятия, мы это делаем? Задача 1.4. Чтобы понять, как шумная обстановка порождает неопреде- ленность, рассмотрим слово лифт. Предположим, вы видите это слово на- писанным (или слышите, как оно произносится) с пропущенной буквой л (что делает ее неизвестной): _ифт (например, из-за окружающего слухового шума). Перечислите все слова, которые совместимы с вашим наблюдени- ем. Теперь рассмотрим случай, когда отсутствуют буквы л и ф: _и_т. Каков эффект отсутствия л, ф и двух букв одновременно с точки зрения условных вероятностей, имеющих отношение к восприятию? Задача 1.5. Фонетический алфавит НАТО, используемый многими военными, морскими и другими организациями при радиосвязи, представляет каждую букву словом: A (альфа), B (браво), C (Чарли), D (дельта), E (эхо), F (фокстрот) и т. д. Для какой цели это служит в радиосвязи? Объясните применительно к условным вероятностям. В частности, рассмотрим радиосвязь в условиях значительного фонового шума, в которой отправитель желает произнести слово FACE (лицо). Сравните вероятности p(слуховой сигнал, слышимый по- лучателем | слово FACE, произнесенное отправителем) и p(слуховой сигнал, слышимый получателем | другое слово, такое как RACE, произнесенное от - правителем), когда отправитель использует обычный алфавит, и еще раз, когда отправитель использует фонетический алфавит. Задача 1.6. Англоязычные носители иногда неправильно воспринимают анг лийские слова, когда слушают песни, исполняемые на иностранном язы- ке, с которым они незнакомы, а также воспринимают слова в музыке, воспро- изводимой задом наперед. Дайте байесовское объяснение этим явлениям. Задача 1.7. Иногда стоит нажать кнопку вызова лифта, как кабина лифта сразу приходит в движение (это видно на дисплее, показывающем текущий этаж кабины). Докажите, что в этой ситуации вероятность гипотезы о том, что внутри никого не окажется, когда она прибудет, намного выше, чем ве- роятность гипотезы о том, что внутри окажутся люди. Априорные знания в этой задаче не учитываются. Задача 1.8. Предположим, вы видите кого-то, с кем незнакомы, лишь мель- ком взглянув на него с расстояния около 10 м. Если вы заинтересованы в оценке возраста этого человека, как бы вы поступили? Какие факторы, по- мимо внешности человека, могли бы повлиять на вашу оценку? Предоставьте байесовское описание ваших рассуждений. Как часть вашего ответа нари- суйте примеры вашей функции правдоподобия, априорного распределения и результирующего апостериорного распределения. Задача 1.9. Исследовательская статья под названием «Пасхальный кролик в октябре: он маскируется под утку?» поясняет, что «о внешнем виде пасхаль- ного кролика в его нерабочие дни известно очень мало» [26]. В ходе исследо- вания авторы показали «неоднозначный рисунок утки/кролика (без указания авторства из номера Fliegende Blatter от 23 октября 1892 г.) 265 испытуемым в пасхальное воскресенье и 276 испытуемым в воскресенье в октябре 1992 г.\n--- Страница 51 ---\n50  Неопределенность и вывод В том же году авторы сообщают: «Если на Пасху рисунок значительно чаще распознавался как кролик, то в октябре большинство испытуемых считали его птицей». Рисунок, показанный авторами в своем исследовании, был по- хож на следующий: Кролик или утка, Fliegende Blätter, выпуск Fliegende Blatter от 23 октября 1892 г. Дайте байесовское объяснение результатов, полученных авторами. Задача 1.10. На изображениях ниже показана полая маска для лица, по- степенно поворачиваемая от вида сбоку (слева) до вида с изнанки (справа). Правое изображение выглядит как обычное выпуклое лицо, хотя на самом деле это полая (вогнутая) сторона маски. Это называется иллюзией полого лица, или иллюзией полой маски. Дайте байесовское объяснение этой ил- люзии. Задача 1.11. Приведите три примера из повседневной жизни (перцептивных или когнитивных), в которых вы пытались сделать вывод о состоянии мира на основе неполной или несовершенной информации. Для каждого примера укажите наблюдение(я), интересующее состояние мира и источник(и) не- определенности.\n--- Страница 52 ---\nЗадачи  51 Задача 1.12. Мишель Трейсман попытался объяснить явление укачивания (морскую болезнь) в контексте эволюции ([179], Motion Sickness, 493–495). В течение миллионов лет, на протяжении которых развивался человеческий мозг, случайное употребление токсичной пищи могло вызвать галлюцинации в виде раскачивания и вращения окружающей обстановки, а последующие за этим тошнота и рвота были всего лишь защитной реакцией организма, избавляющегося от токсинов. Возможно, наш современный мозг все еще ис - пользует априорные вероятности, генетически унаследованные от тех дней; просто они основаны не на нашем личном опыте, а на опыте наших предков! Здесь мы не углубляемся в достоинства этой теории, а пытаемся представить ее в байесовской форме. Предположим, вы находитесь в комнате без окон на корабле в море. Вашему мозгу доступны два набора сенсорных наблюдений: зрительные и вестибулярные. Предположим, что мозг рассматривает три гипотезы того, что вызвало эти наблюдения: гипотеза 1: комната не движется, и ваше движение в комнате вызы- вает оба набора наблюдений; гипотеза 2: ваше движение в комнате вызывает ваши зрительные на- блюдения, тогда как ваше движение в комнате и движение комнаты в мире вместе вызывают вестибулярные наблюдения; гипотеза 3: у вас галлюцинации – ваше движение в комнате и прогло- ченные токсины вместе вызывают оба набора наблюдений. (a) В доисторические времена окружающий мир почти никогда не двигался. Время от времени человек может случайно проглотить токсины. Пред- полагая, что ваши врожденные априорные знания основаны на этих до- исторических частотах событий, нарисуйте столбчатую диаграмму, что- бы представить ваши априорные вероятности трех приведенных выше гипотез. Числовые данные не нужны. (b) В комнате без окон на корабле наблюдается большое расхождение меж - ду вашими зрительными и вестибулярными наблюдениями. Нарисуй- те столбчатую диаграмму, иллюстрирующую вероятность трех гипотез в этой ситуации (т. е. насколько вероятны эти конкретные сенсорные наблюдения при каждой гипотезе). Числовые данные не нужны. (c) Нарисуйте столбчатую диаграмму, иллюстрирующую апостериорные ве- роятности трех гипотез. Числовые данные не нужны. (d) Объясните, используя апостериорные вероятности, почему вас может стошнить в этой ситуации.",
      "debug": {
        "start_page": 25,
        "end_page": 52
      }
    },
    {
      "name": "Глава 2. Применение правила Байеса 52",
      "content": "--- Страница 53 --- (продолжение)\nГлава 2 Применение правила Байеса Как мы делаем количественные выводы? В главе 1 мы установили, что правило Байеса применимо для понимания вос - приятия. В этой главе мы опишем вычисления, выполняемые в соответствии с правилом Байеса. Мы покажем на примерах, как вычислять апостериорные распределения, исходя из априорных распределений и функций правдопо- добия. Краткое содержание главы Мы начинаем с описания этапов байесовского моделирования. Далее мы представляем интуитивно понятный вывод правила Байеса с использова- нием представлений в форме площадей и математических уравнений. Мы описываем простые сценарии восприятия с категориальными (в частности, бинарными) переменными, чтобы проиллюстрировать, как наблюдатели вы- полняют байесовский вывод. 2.1. Этапы байесовского моделирования Каждая байесовская модель состоит из ряда этапов, которые необходимо пройти по порядку. Первый этап – определение порождающей модели, ко- торая представляет статистическую структуру мира и наблюдения. Второй этап – определение вывода: как следует обновить убеждения наблюдателя о состоянии мира на основании имеющихся наблюдений? Второй этап завер- шается описанием того, как наблюдатель принимает решение. В следующей главе вы перейдете к третьему этапу. Пока мы перечислили этапы в качестве «шпаргалки».\nГлава 2 Применение правила Байеса Как мы делаем количественные выводы? В главе 1 мы установили, что правило Байеса применимо для понимания вос - приятия. В этой главе мы опишем вычисления, выполняемые в соответствии с правилом Байеса. Мы покажем на примерах, как вычислять апостериорные распределения, исходя из априорных распределений и функций правдопо- добия. Краткое содержание главы Мы начинаем с описания этапов байесовского моделирования. Далее мы представляем интуитивно понятный вывод правила Байеса с использова- нием представлений в форме площадей и математических уравнений. Мы описываем простые сценарии восприятия с категориальными (в частности, бинарными) переменными, чтобы проиллюстрировать, как наблюдатели вы- полняют байесовский вывод. 2.1. Этапы байесовского моделирования Каждая байесовская модель состоит из ряда этапов, которые необходимо пройти по порядку. Первый этап – определение порождающей модели, ко- торая представляет статистическую структуру мира и наблюдения. Второй этап – определение вывода: как следует обновить убеждения наблюдателя о состоянии мира на основании имеющихся наблюдений? Второй этап завер- шается описанием того, как наблюдатель принимает решение. В следующей главе вы перейдете к третьему этапу. Пока мы перечислили этапы в качестве «шпаргалки».\n--- Страница 54 ---\nЭтапы байесовского моделирования  53 Этап 1: порождающая модель Вернемся к примеру из раздела 1.4, где оценивается влажность поверхности на основе зрительной информации (блестит она или нет). Здесь у нас есть два возможных состояния мира (мокро или сухо). У нас есть два потенциаль- ных наблюдения (блестит поверхность или нет). Каждая байесовская модель начинается с описания процесса, посредством которого, как мы полагаем, порождаются наблюдаемые данные; это так называемая порождающая мо- дель. Порождающая модель – это полное статистическое описание того, что происходит в задаче. Она всегда охватывает интересующее состояние мира и сенсорные наблюдения субъекта, но может также включать и другие пере- менные. С математической точки зрения порождающая модель определяет распределения вероятностей всех переменных в задаче. Исследователи часто визуализируют порождающую модель в виде диаграммы, называемой графо- вой моделью (рис. 2.1А). В графовой модели каждый узел представляет собой переменную, а каждая стрелка – статистическое влияние одной переменной на другую. Сенсорные наблюдения всегда должны быть внизу диаграммы. (A) Порождающая модель (B) ВыводОбщая Пример Состояние мира Гипотетическое состояние мираНаблюдение Конкретное наблюдениеСухой (dry) или мокрый (wet) Сухой (dry) или мокрый (wet)Блестит (shiny) или нет Наблюдение: блеститРаспределение вероятностей мирового состояния Распределение вероятностей наблюденийp(wet) = 0.1 p(dry) = 0.9 p(shiny | wet) = 0.8 p(not shiny | wet) = 0.2 p(shiny | dry) = 0.4 p(not shiny | dry) = 0.6 Prior(wet) = p(wet) = 0.1 Prior(dry) = p(dry) = 0.9 Likelihood(wet) = p(shiny | wet) = 0.8 Likelihood(dry) = p(shiny | dry) = 0.4Априорные вероятности Правдоподобия Рис. 2.1  (A) Порождающую модель можно представить в виде диаграммы с узлами и стрел- ками. Каждый узел является переменной, а наблюдения всегда расположены внизу. Каждый узел связан с распределением вероятностей. Каждый узел со стрелкой, указывающей на него, связан с условным распределением вероятностей. Числа даны для нашего примера с блес­ тящим полом; (B) на этапе вывода наблюдатель имеет конкретное наблюдение (в примере: пол блестит) и пытается вычислить вероятности гипотетических состояний мира. Те же самые числа теперь используются в качестве априорных значений и вероятностей. Вывод «перево- рачивает» порождающую модель Порождающая модель определяет вероятности состояний мира, например что большинство полов сухие. Предположим, что, по нашему опыту, 10 % всех полов мокрые. Отсюда: р(мокрый) = 0,1; (2.1) р(сухой) = 0,9. (2.2)\n--- Страница 55 ---\n54  Применение правила Байеса Вероятности появления при отсутствии каких-либо наблюдений будут на- зываться базовыми частотностями (base rate) в статистике или встречаемо- стями (prevalences) в медицине. Порождающая модель также определяет вероятности наблюдений, обуслов- ленных состояниями мира. Предположим, что, по нашему опыту, блестят 80 % мокрых полов и только 40 % сухих. Эти вероятности являются условными и принимают форму p(наблюдение | состояние мира). Эти вероятности долж - ны быть определены для каждой комбинации наблюдения и состояния мира. В примере мы предполагаем, что р(блестящий | мокрый) = 0.8; (2.3) p(неблестящий | мокрый) = 0.2; (2.4) р(блестящий | сухой) = 0.4; (2.5) p(неблестящий | сухой) = 0.6. (2.6) Этап 2а: вывод с использованием правила Байеса Если мы видим, что пол блестит, нужно сделать вывод о том, мокрый он или нет. Вывод можно сделать, используя правило Байеса – главное правило рас - чета вероятностей: (2.7) где А и В – любые две случайные величины. Упражнение 2.1. Докажите правило Байеса. Подсказка: согласно цепному пра- вилу вероятность наличия и А, и В равна p(A, B) = p(A)p(B | A). Для получения дополнительных сведений обратитесь к разделу B.11 приложения. В контексте вывода А – это скорее гипотетическое, чем фактическое состоя- ние мира, и мы часто будем обозначать его через Н (от англ. hypothesized – ги - потетический, предполагаемый). B – это наблюдение или набор наблюдений, которые мы будем обозначать как «obs» (от англ. observation – наблюдение). В примере с блестящим полом предполагается, что пол мокрый или сухой. Правило Байеса. Таким образом, правило Байеса в контексте вывода можно записать в следующей форме: (2.8) Здесь p(H) называется априорной вероятностью H, p(obs | H) называется правдоподобием H, а p(H | obs) – апостериорной вероятностью H. Это урав- нение отражает интуитивно понятный характер правила Байеса примени- тельно к оценке гипотез: если гипотеза чаще оказывается верной в мире, то еще до того, как мы примем во внимание наблюдения, эта гипотеза является более вероятной (априорность), и если наблюдения более ожидаемы при гипотезе (правдоподобие), то эта гипотеза также более вероятна.\n--- Страница 56 ---\nЭтапы байесовского моделирования  55 В этой главе и большей части книги мы предполагаем, что наблюдатель правильно понимает порождающую модель; то есть она дает точное описа- ние истинной статистики состояний мира и наблюдений. Это означает, что априорные вероятности наблюдателя численно идентичны частотам соот - ветствующих состояний мира в порождающей модели и что вероятности наблюдателя правильно представляют вероятности наблюдения с учетом состояний мира. Однако между предполагаемой порождающей моделью на- блюдателя (иногда называемой внутренней моделью наблюдателя) и факти- ческой порождающей моделью может существовать несоответствие. Напри- мер, по какой-то причине вы можете полагать, что 50 % всех полов мокрые, хотя на самом деле их доля составляет 10 %. Мы прокомментируем такое несоответствие моделей в нескольких местах позже в книге. Априорность. Априорное распределение (prior) отражает предполагае- мую частоту появления значений интересующего состояния мира, в данном случае влажности. Эти значения получаются из частот этих мировых состоя- ний в порождающей модели (рис. 2.1). Prior(мокрый) = p(мокрый) = 0.1; (2.9) Prior(сухой) = p(сухой) = 0.9. (2.10) Правдоподобие. Теперь обсудим правдоподобие. Предположим, вы заме- тили, что пол блестит (рис. 2.2). Тогда p(мокрый) – это вероятность того, что пол будет блестящим, если он мокрый, а p(сухой) – это вероятность того, что пол будет блестящим, если он сухой. Эти значения получены из порождаю- щей модели, но нам нужно использовать только те вероятности, которые от - носятся к фактическому наблюдению «блестящий». Правдоподобие таково: Likelihood(мокрый) = p(блестящий | мокрый) = 0.8; (2.11) Likelihood(сухой) = p(блестящий | сухой) = 0.4. (2.12) (C) Правдоподобие Априорное убеждениеp(hyp)p(obs | hyp) p(hyp | obs)Апостериорное убеждение1.0 0.5 0.01.0 0.5 0.01.0 0.5 0.0 Мокрый Мокрый Мокрый Сухой Сухой Сухой(А) (В) (D) 0.8 0.4 0.10.1820.8180.9 Рис. 2.2  Пример перцептивного вывода из наблюдения блеска: (A) этот пол мокрый?; (B) мокрый пол с большей вероятностью будет блестеть, чем сухой; (C) большинство полов сухие; (D) апостериорное распределение подтверждает гипотезу о том, что пол сухой Важно понимать, что суммы правдоподобий не обязательно должны рав- няться 1. Функция правдоподобия не является распределением вероятно- стей по интересующему состоянию мира.\n--- Страница 57 ---\n56  Применение правила Байеса Примечание по терминологии: никогда не говорите «правдоподобие наблюдения». Правдоподобие всегда является функцией гипотетического состояния мира. Оно равно вероятности данного наблюдения при гипоте- тическом состоянии мира. Протопостериор. Числитель в правиле Байеса – произведение правдопо- добия и априорной вероятности – не имеет официального названия, но мы будем называть его протопостериором гипотезы H : Protoposterior(H ) = p(obs | H)p(H) = p(H, obs). (2.13) В нашем примере протопостериорами двух гипотез являются Protoposterior(мокрый) = Prior(мокрый) · Likelihood(мокрый) = 0.1 · 0.8 = 0.08; (2.14) Protoposterior(сухой) = Prior (сухой) · Likelihood(сухой) = 0.9 · 0.4 = 0.36. (2.15) Протопостериор – это вероятность гипотезы и наблюдения вместе. В на- шем примере 8 % полов мокрые и блестящие, а 36 % полов сухие и блестящие. Как и вероятности, протоапостериорные значения в целом не будут в сумме равны 1. Нормализация. Правило Байеса (2.8) говорит нам разделить протопосте- риорные значения на p(obs), вероятность наблюдений независимо от того, каким состоянием мира они были произведены. Оказывается, вероятность p(obs) равна сумме протопостериоров по всем гипотезам: (2.16) (2.17) Упражнение 2.2. Докажите это утверждение. Подсказка: рассмотрите все спо- собы, которыми можно достичь данного наблюдения. Для получения допол- нительной информации также обратитесь к разделу B.11 приложения. В нашем примере p(блестящий) = Protoposterior(мокрый) + Protoposterior(сухой) (2.18) = 0.08 + 0.36 = 0.44. (2.19) Этот расчет показывает, что 44 % всех полов блестят. Апостериорные вероятности. Чтобы получить апостериорные вероят - ности, сумма которых равна 1, мы нормализуем протопостериорные веро- ятности, то есть разделим каждую на их сумму p(obs). В этом смысле p(obs) является нормирующим множителем в правиле Байеса. В нашем примере Posterior(мокрый) = p(мокрый | блестит) = = 0.182; (2.20)\n--- Страница 58 ---\nПлощадное представление  57 Posterior(сухой) = p(сухой | блестит) = = 0.818. (2.21) Таким образом, p(obs) гарантирует, что, в отличие от правдоподобий или протопостериорных вероятностей, апостериорные вероятности всегда в сум- ме равны 1, то есть нормированы. В результате нашего наблюдения за тем, что пол блестит, мы обновили наши убеждения в гипотезах. Наше убежде- ние в том, что пол сухой, уменьшилось с априорных 90 % до апостериорных 82 %. Функция правдоподобия поддерживает «мокрую» гипотезу (т. е. мокрые полы блестят чаще, чем сухие), поэтому мы потеряли некоторую уверенность в гипотезе о том, что пол сухой. Тем не менее мы по-прежнему поддержива- ем эту гипотезу, потому что сила свидетельств в пользу «мокрой» гипотезы, как показано отношением правдоподобия для гипотез , недостаточно велика, чтобы компенсировать отношение априорных значе- ний, что подтверждает гипотезу о том, что пол сухой: Упражнение 2.3. Предположим, пол не блестит. Какова апостериорная вероят - ность того, что он сухой? 2.2. Альтернативная форма правила Байеса Объединим уравнения (2.8) и (2.17), чтобы получить альтернативную форму правила Байеса: (2.22) Эта форма ясно показывает, что знаменатель представляет собой сумму членов, таких же, как те, что в числителе. Числитель относится только к одной конкретной H (той, что в левой части уравнения), а знаменатель представля- ет собой сумму по всем H . 2.3. Площадное представление Чтобы углубить наше понимание правила Байеса, давайте переформули - руем задачу о блестящем полу, представив соответствующие вероятности в виде площадей прямоугольников (рис. 2.3). Большой внешний прямоуголь- ник с площадью, равной 1, представляет собой совокупность возможностей, а именно то, что пол либо мокрый, либо сухой. Априорная вероятность того,\n--- Страница 59 ---\n58  Применение правила Байеса что пол мокрый p(мокрый), представлена площадью левого прямоуголь- ника; априорная вероятность того, что пол сухой p(сухой), представлена площадью правого прямоугольника. Вероятность того, что пол будет блестя- щим, p(блестящий), представлена заштрихованной синей областью. Синий заполняет 80 % левого прямоугольника и 40 % правого прямоугольника; эти вероятности представляют p(блестящий | мокрый) и p(блестящий | сухой) со- ответственно. Вероятность того, что пол и мокрый, и блестящий, – p(мокрый, блестящий) – представляет собой синюю область в левом прямоугольнике. Вероятность того, что пол и сухой, и блестящий, – p(сухой, блестящий) – пред- ставляет собой синюю область в правом прямоугольнике. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Блестящий Мокрый Сухой Рис. 2.3  Площадное представление байесовского вывода. Апостериорная вероятность того, что пол мокрый, при условии что он блестит, равна площади синего цвета внутри левого прямоугольника, деленной на общую площадь синего цвета. Апостериорная вероятность того, что пол сухой, при условии что он блестит, равна площади синего цвета внутри правого пря- моугольника, деленной на общую площадь синего цвета Теперь давайте предположим, что вы бросаете дротик, который имеет одинаковую вероятность воткнуться в любом месте большого внешнего пря- моугольника площадью 1. Предположим, что ваш дротик попал в синюю область (т. е. вы попали в блестящий пол). Какова вероятность того, что он также находится в левом прямоугольнике (мокрый пол)? По размышлении должно быть ясно, что эта вероятность равна доле синей области, которая также является «мокрой», то есть p(мокрый | блестящий) = (2.23) Аналогичные рассуждения приводят к заключению, что p(сухой | блестящий) = (2.24) Чтобы понять это, представьте, что вы бросаете 100 дротиков, которые рав- номерно и случайным образом втыкаются внутри большого внешнего пря-\n--- Страница 60 ---\nОшибка прокурора  59 моугольника площадью 1. Всякий раз, когда дротик приземляется в синей области, вы записываете, находится ли он внутри прямоугольника «мокрый» или «сухой». В среднем вы обнаружите, что 44 из ваших 100 дротиков попадают в синюю область, а из этих 44 дротиков 8 попадают в «мокрый» прямоугольник и 36 – в «сухой». Следовательно, вероятность того, что блестящий пол мокрый, равна 8/44, а вероятность того, что он сухой, равна 36/44, как мы нашли выше. Этап 2b: применение апостериорного распределения Результатом применения правила Байеса является апостериорное распре- деление гипотетического состояния мира p(H|obs). Но, получив апостериор- ное распределение, мы еще не закончили, потому что обычно цель модели состоит в том, чтобы предсказать, что решит или воспримет наблюдатель. Решение – это ответ на вопрос «какая гипотеза верна?». Восприятие – это то, что воспринимает наблюдатель. Это может быть ответом на вопрос «Что ты видишь?» (или слышишь, обоняешь, чувствуешь и т. д.), но это не обязательно связано с вопросом – это может быть непроизвольный опыт. Можно утверж - дать, что все восприятия являются решениями, но это в основном тема для философских дискуссий. Примечание по терминологии: не путайте термины «восприятие» и «на- блюдение», даже если они могут показаться взаимозаменяемыми. Наблюде- ние – это вход в процесс вывода, восприятие – его выход. Применение (readout) апостериорного распределения – это отображение апостериорного распределения на решение или восприятие. Самый очевид- ный вывод – выбрать максимум, т. е. гипотезу с наибольшей апостериорной вероятностью. Этот выбор также называется максимальной апостериорной (maximum a posteriori, MAP) оценкой. В нашем примере восприятие MAP будет заключаться в том, что пол сухой. Более того, наблюдатель будет уме- ренно уверен в этом выводе, поскольку соответствующая апостериорная вероятность составляет 81.8 %. Одно из различий между решением и восприятием заключается в том, что первое может включать в себя внешние вознаграждения и издержки, а второе – нет. Например, ошибочно принять мокрый пол за сухой может обойтись для здоровья гораздо дороже, чем наоборот. Как следствие, даже с апостериорной вероятностью 81.8 % того, что пол сухой, все же может быть хорошим решением ходить более осторожно. Подробнее о сочетании логи- ческого вывода с полезностью мы поговорим в главе 13. Позже мы сделаем две оговорки относительно универсальности оценки MAP: во-первых, в случае непрерывной переменной выбор максимума не всегда является лучшим применением; во-вторых, может присутствовать шум принятия решения, вызывающий отклонения от оценки MAP . 2.4. Ошибка прокурора Теперь мы рассмотрим несколько интересных примеров, которые помогут закрепить наше понимание описанных выше шагов, а также намекнут на\n--- Страница 61 ---\n60  Применение правила Байеса широкий диапазон сценариев, к которым применим байесовский вывод. Первый из них – знаменитая ошибка прокурора. Как вы знаете, условные вероятности несимметричны. В общем случае p(A | B) ¹ p(B | A). Например, в примере с блестящим полом мы обнаружили, что p(блестящий | мокрый) = 0.8, тогда как p(мокрый | блестящий) = 0.182. Эта асимметрия хорошо видна на диаграмме площадей (рис. 2.3), где 80 % «мокро- го» прямоугольника заполнено синим цветом, но только 18.2 % синей области приходится на «мокрый» прямоугольник. К сожалению, люди, не обученные вероятностным рассуждениям, иногда совершают ошибку, приравнивая p(A | B) и p(B | A). Эта ошибка называется ошибкой прокурора, или ошибкой условной ве- роятности. Заблуждение прокурора берет свое название от ложного аргумен- та, иногда выдвигаемого в судах, что p(подсудимый невиновен | доказатель- ства) = p(доказательства | подсудимый невиновен). Например, предположим, что на оружии, оставленном на месте преступления, обнаружен частичный смазанный отпечаток пальца. Поиск по базе данных отпечатков пальцев пока- зывает, что у человека, проживающего в том же городе, есть отпечаток пальца, который совпадает с отпечатком, оставленным на оружии. Судебно-медицин - ский эксперт свидетельствует, что только 1 из 1000 случайно выбранных людей может дать такое совпадение. Прокурор утверждает, что, исходя из показаний судебно-медицинского эксперта, вероятность невиновности подсудимого со- ставляет всего 1 к 1000. Прокурор путает p(наблюдение | невиновен) – показа- ния судебно-медицинского эксперта – с p (невиновен | наблюдение). Правило Байеса позволяет правильно вычислить p(A | B) из p(B | A) и других соответствующих вероятностей и использовалось для этой цели в некото- рых судах [50]. Предположим, что в городе проживает 1 000 001 (1 миллион плюс 1) взрослый житель. Учитывая только то, что подсудимый живет в го- роде, их априорные вероятности быть невиновным (H1) или виновным (H2), следовательно, равны: (2.25) (2.26) Тот факт, что отпечаток пальца обвиняемого совпадает с отпечатком паль- ца на месте преступления, приводит к правдоподобиям: (2.27) (2.28) Протопостериоры таковы: (2.29)\n--- Страница 62 ---\nСмена априорного убеждения: пример багажной карусели  61 (2.30) Выполним нормирование: Normalization = Protoposterior(H1) + Protoposterior(H2); (2.31) (2.32) Таким образом, (2.33) (2.34) Подсудимый почти наверняка невиновен, несмотря на аргумент прокуро- ра! Другой способ объяснить это: в городе 1 000 001 человек, 1 000 000 из ко- торых невиновны и 1 из которых виновен. Следовательно, если бы у нас были отпечатки пальцев всех жителей города, мы бы ожидали 1001 совпадение, из которых только 1 от виновного гражданина. Таким образом, вероятность вины при совпадении отпечатков пальцев равна 1/1001. 2.5. Смена априорного убеждения: пример багажной карусели Теперь мы переходим к несколько более сложному примеру, в котором реле- вантные вероятности меняются со временем. Многим авиапассажирам дове- лось нетерпеливо ждать в зоне выдачи багажа в аэропорту, наблюдая, как их чемоданы падают по желобу во вращающуюся багажную карусель (рис. 2.4А). Эта ситуация предоставляет возможности для вероятностных рассуждений [155], а также для перцептивного вывода. Предположим, что вы участвуете в этом ритуале современного авиаперелета вместе с 99 другими пассажира- ми вашего рейса, каждый из которых, как и вы, зарегистрировал одно место багажа. Объяление через динамики напоминает вам, что «многие чемоданы похожи друг на друга. Пожалуйста, внимательно проверьте свой багаж перед выходом из терминала». Действительно, ваш багаж – одна из самых популяр- ных моделей на рынке, черный прямоугольный чемодан среднего размера, который используют 5 % всех путешественников. На расстоянии от багажной ленты вы можете различить только форму, размер и цвет багажа, выезжаю- щего на карусели, но не какие-либо подробные маркировки на нем. Теперь давайте предположим, что первый же чемодан с вашего рейса, попавший\n--- Страница 63 ---\n62  Применение правила Байеса на багажную карусель, действительно имеет ту же форму, размер и цвет, что и ваш. Это ваш чемодан? Правдоподобие ПравдоподобиеАприорное убеждение Априорное убеждениеp(hyp) p(hyp)p(obs | hyp) p(obs | hyp) p(hyp | obs) p(hyp | obs)Апостериорное убеждение Апостериорное убеждениеМой МойМой МойМой МойНе мой Не мойНе мой Не мойНе мой Не мой(А) (В) (С)1 10.05 0.050.01 0.070.168 0.5880.832 0.4120.99 0.93 Рис. 2.4  Ожидание влияет на восприятие: (A) первый и 86­й чемоданы совпадают с ваши- ми по форме, размеру и цвету; (B) функция правдоподобия, априорное распределение веро- ятностей и апостериорное распределение вероятностей при наблюдении первого чемодана, попавшего на багажную карусель. Ваше апостериорное распределение указывает на то, что чемодан, скорее всего, не ваш; (C) функция правдоподобия, предварительное распределение и апостериорное распределение при наблюдении 86­го чемодана, попавшего на багажную карусель. Та же вероятность, что и в (А), в сочетании с другим априорным ожиданием дает апостериорное распределение, которое поддерживает гипотезу о том, что чемодан принад- лежит вам На этот вопрос нельзя ответить однозначно «да» или «нет». Напротив, во- прос требует вероятностного суждения. Вы можете считать более или менее вероятным, что чемодан ваш, но пока не можете быть уверены. Вместо уве- ренности восприятие чаще всего характеризуется разной степенью уверен- ности, которая может быть выражена в виде вероятностей от невозможности до достоверности, занимающих определенное место в ряду чисел от 0 до 1 (от 0 до 100 %). Когда вы наблюдаете чемодан на багажной карусели, у вас будет интуитивное ощущение вероятности того, что это ваш багаж: p(это мой чемодан | форма, размер, цвет). Но как вы могли прийти к такой оценке вероятности? Правдоподобия. В основе неопределенности восприятия лежит тот факт, что разные состояния мира могут порождать одно и то же сенсорное наблю- дение. Не только «многие чемоданы выглядят одинаково», но многие объ- екты, люди и события производят почти идентичные наблюдения того или иного рода (виды, звуки и т. д.). Поэтому информация, предоставляемая орга- нами чувств, как правило, неточна и открыта для множества интерпретаций. Какая информация содержится в вашем наблюдении? Если чемодан, кото- рый вы наблюдаете, действительно ваш, он будет иметь ту же форму, размер и цвет. Соответственно, p(наблюдаемая форма, размер, цвет | мой чемодан) = 1.\n--- Страница 64 ---\nСмена априорного убеждения: пример багажной карусели  63 Но даже если наблюдаемый чемодан не ваш, он имеет некоторый шанс со- впасть по форме, размеру и цвету с вашим багажом. Поскольку такой же чемодан используют 5 % путешественников, p(наблюдаемая форма, размер, цвет | не мой чемодан) = 0.05. Эти две условные вероятности и есть правдопо- добия. Напомним, что правдоподобие гипотезы – это вероятность сенсорных наблюдений, если гипотеза верна, или, другими словами, насколько ожидае- мы наблюдения, если гипотеза верна. Как и прежде, график правдоподобия каждого возможного состояния мира, так называемая функция правдоподо- бия, суммирует степень, в которой наблюдение совместимо с каждой интер- претацией состояния мира (рис. 2.4). Чем менее информативно наблюдение, тем «шире» или «площе» будет функция правдоподобия; чем информативнее наблюдение, тем «уже» или «острее» она становится. Если вы посмотрите на свой чемодан вблизи, вы заметите опознаватель- ные знаки – бирку с именем, кусок веревки, который вы прикрепили к ручке, и т. д., – которые позволят вам однозначно опознать ваш багаж. Другими словами, по мере того как чемодан приближается к вам на карусели, ваша функция правдоподобия, как правило, значительно улучшается. Так за срав- нительно короткое время вы можете убедиться, что чемодан, за которым вы наблюдали, на самом деле не ваш. Априорная вероятность. Давайте теперь рассмотрим, как ваш вывод вос - приятия меняется в более длительном временном масштабе, пока вы тер - пеливо ждете у карусели, наблюдая, как чемодан за чемоданом падают из желоба на карусель. Когда самый первый чемодан с вашего рейса упадет на карусель и вы заметите сходство с вашим собственным имуществом, вы бу - дете полны надежды, но в то же время, вероятно, останется легкое сомне- ние в принадлежности чемодана. Ваш скептицизм оправдан, потому что не только 5 % сумок похожи на ваши, но и вероятность того, что ваш чемодан окажется первым на карусели, составляет всего 1 из 100. Ведь на вашем рейсе было 100 пассажиров, каждый из которых взял в дорогу один чемодан. Те- перь давайте предположим, что вы ждете у карусели, наблюдая за каждым чемоданом и более внимательно проверяя те, которые напоминают ваш соб- ственный, только для того, чтобы обнаружить, что прошло 10 минут и про- ехали 85 чемоданов, но вашего багажа среди них не было. Предположим, что в этот момент появляется 86-й чемодан, и он снова похож на ваш. На этот раз вы будете более уверены, чем раньше, что чемодан ваш, несмотря на то что наблюдение, а значит, и функция правдоподобия идентичны для первого и 86-го чемоданов. Это показывает, что ваше восприятие p(состояние ми- ра | наблю дение) не совпадает с вероятностью p(наблюдение | состояние мира). Короче говоря, вывод о восприятии основан не только на наблюдении (как это отражено в функции правдоподобия), но и на ожидании. Как объясня- лось ранее, мы представляем ожидание с помощью априорной вероятности. Априорная вероятность мирового состояния основана на всей соответствую- щей информации, кроме текущего наблюдения. В данном примере ваш опыт терпеливого ожидания, пока 85 чемоданов выезжали на карусели, вместе с вашим базовым знанием о том, что на вашем рейсе было 100 предметов багажа, дали вам информацию о том, что априорная вероятность того, что ваша сумка появится следующей, составляет 1 к 15 (т. е. 6.7 %), что больше,\n--- Страница 65 ---\n64  Применение правила Байеса чем 1 %, который был для первого чемодана. Хотя априорные вероятности зависят от опыта и имеющихся знаний, в целях краткости мы обычно опус - каем обусловливающий символ (|) и записываем априорные вероятности просто как p(гипотетическое состояние мира), например p(чемодан мой) и p(чемодан не мой). Мы можем построить априорную вероятность каждого гипотетического состояния мира в виде априорного распределения вероят - ностей (рис. 2.4B–C). Апостериорная вероятность. Вычислим апостериорную вероятность того, что первый чемодан, который вы увидите на багажной ленте, будет вашим. Сначала перечислим возможные состояния мира или гипотезы: H1 (чемодан мой) и H2 (чемодан не мой). Затем мы записываем априорные ве- роятности каждой гипотезы, учитывая знание, что этот чемодан появляется первым: р(Н1) = 0.01; (2.35) р(Н2) = 0.99. (2.36) Потом запишем вероятности, которые выражают вероятность сенсорного наблюдения (форма, размер и цвет увиденного багажа) при каждой гипотезе: p(наблюдение | H1) = 1; (2.37) p(наблюдение | H2) = 0.05. (2.38) Поскольку априорная вероятность того, что первый чемодан ваш, равна 1 %, вероятность того, что он не ваш, составляет 99 %. Обратите внимание: так как на багажной карусели наблюдается чемодан, который соответствует вашему по форме, размеру и цвету, мы установили вероятность 1 для H1. Это логично, ведь если это ваш чемодан, то зрительный образ обязательно будет соответствовать его форме, размеру и цвету. Наконец, введем априорные вероятности и правдоподобия в правило Байеса, чтобы вычислить апосте- риорные вероятности гипотез: p(H1 | наблюдение) = (2.39) p(H2 | наблюдение) = (2.40) На этом этапе следует учитывать несколько соображений. 1. Прежде всего важно понять, что вы извлекли уроки из наблюдения, обновив вашу априорную вероятность для H1 (0.01) до гораздо большей апостериорной вероятности (0.168). Апостериорная вероятность для H1 увеличилась, потому что наблюдение больше соответствовало H1, чем H2. В общем случае чем сильнее наблюдение отдает предпочтение одной гипотезе над другой, тем больше мы узнаем. 2. Тем не менее вы больше уверены в том, что сумка не ваша (83.2 %), чем в обратном (16.8 %). Несмотря на благоприятное наблюдение, вы\n--- Страница 66 ---\nПлоское априорное распределение: пример гештальт ­восприятия  65 полагаете, что сумка, скорее всего, не ваша, потому что начали с очень низкой априорной вероятности для H1. По сути, наблюдение за сумкой, похожей на вашу, не очень эффективно благоприятствует H1, чтобы преодолеть ваше хорошо обоснованное априорное предубеждение против H1. 3. Важно отметить, что апостериорная вероятность p(H1 | наблюдение) = 16.8 % не равна правдоподобию p(наблюдение | H1) = 100 %. Как объ- яснялось выше, в общем случае p (A|B) ¹ p(B | A). 4. Обратите внимание, что в этом примере гипотеза H1 с максимальной вероятностью (так называемая оценка максимального правдоподобия, или MLE) не совпадает с гипотезой с максимальной апостериорной вероятностью (оценка MAP) – H2. Эта ситуация не редкость в перцеп- тивном выводе. Иногда оценки MLE и MAP совпадают, но часто это не так. Теперь предположим, что вы продолжаете ждать появления своего чемо- дана, но не видите его среди первых 85, попавших на карусель. Чтобы вы- числить апостериорную вероятность того, что 86-й чемодан, который также совпадает с вашим по форме, размеру и цвету, является вашим, нужно сле- довать той же процедуре, но с новыми априорными вероятностями 1/15 для H1 и 14/15 для H2 (рис. 2.4C). Упражнение 2.4. Убедитесь, что апостериорные вероятности при оценке 86-го чемодана примерно равны p(H1 | наблюдение) = 0.588 и p(H2 | наблюде- ние) = 0.412. Таким образом, вероятность того, что чемодан, который вы наблюдаете, принадлежит вам, резко возросла с 16.8 % (первый увиденный чемодан) до 58.8 % (86-й увиденный чемодан), несмотря на то что в обоих случаях наблю- дение и, следовательно, функция правдоподобия одинаковые. Это очень ясно свидетельствует о том, что апостериорное распределение зависит не только от сенсорных данных, но и от априорного распределения. 2.6. Плоское априорное распределение: пример гештальт-восприятия После этих наглядных примеров мы возвращаемся к восприятию. Распростра- ненное заблуждение относительно байесовских моделей восприятия заклю- чается в том, что мы всегда должны иметь неравные априорные вероятности. На самом деле важные результаты можно получить из сценариев, включаю- щих плоские априорные распределения. Предположим, вы наблюдаете пять точек, движущихся вниз, как показано стрелками на рис. 2.5. Большинство людей воспримут такой стимул как движение одной группы или объекта вниз. Традиционное представление о восприятии состоит в том, что мозг имеет тенденцию группировать точки вместе из-за их общего движения. Это\n--- Страница 67 ---\n66  Применение правила Байеса отражено в гештальт-принципе «общей судьбы». Принципы гештальта, или законы гештальта, первоначально сформулированные Вертхаймером [200], описывают, как люди группируют разрозненные элементы в соответствии с их расположением или свойствами. Такие принципы, однако, являются просто нарративными обобщениями феноменологии. Байесовская модель может обеспечить более глубокое объяснение восприятия и в некоторых слу - чаях даже сделать количественные прогнозы. Рис. 2.5  Пять точек, которые движутся вниз Давайте воспользуемся байесовским подходом к пониманию сценария с пятью точками. Изображение каждой точки на сетчатке служит сенсорным наблюдением. Обозначим эти пять изображений на сетчатке как I1, I2, I3, I4 и I5. Этап 1: порождающая модель. Первым этапом в байесовском модели- ровании является формулировка порождающей модели: графическое или математическое описание возможностей, которые могли привести к сенсор- ным наблюдениям. Допустим, мозг рассматривает только две возможности: гипотеза 1: все точки являются частью одного и того же объекта, по- этому они всегда движутся вместе. Они движутся вместе вверх или вниз, каждая с вероятностью ½; гипотеза 2: каждая точка является отдельным объектом. Каждая точка независимо перемещается вверх или вниз с вероятностью 1/2. (Мы предполагаем, что точкам разрешено двигаться только вверх и вниз и что скорость и положение не играют роли в этой задаче.) На приведенной ниже диаграмме порождающей модели каждая гипотеза показана в большом прямоугольнике. Внутри каждого блока поля содержат переменные, а стрел- ки представляют зависимости между переменными, как и раньше. Упражнение 2.5. Впишите следующие имена переменных в правильные поля ниже для гипотезы 1 (слева) и гипотезы 2 (справа): изображения на сетчатке I1, I2, I3, I4 и I5 и направления движения s (одно направление движения), или s1, s2, s3, s4 и s5. Одна и та же переменная может встречаться несколько раз.\n--- Страница 68 ---\nПлоское априорное распределение: пример гештальт ­восприятия  67 Гипотезы 1 Гипотезы 2 Этап 2: умозаключение. Рассмотрим теперь конкретную наблюдаемую конфигурацию {вниз, вниз, вниз, вниз, вниз}, то есть все пять точек движутся вниз, что мы обозначим как Iobs. Задача мозга состоит в том, чтобы опреде- лить на основе Iobs, какая из двух гипотез (1 или 2), которые мы обозначим H1 и H2, является правильной. Как и прежде, вероятность каждой гипотезы – это вероятность сенсорных наблюдений, если эта гипотеза верна. Согласно информации, представленной в задаче, две вероятности равны Likelihood(H1) = p(Iobs | H1) = 1/2; (2.41) Likelihood(H2) = p(Iobs | H2) = (1/2)5 = 1/32. (2.42) Обратите внимание, что два правдоподобия не дают в сумме 1. Заметим также, что второе правдоподобие намного меньше первого. Это связано с тем, что при H2 требуется полное совпадение, чтобы все точки двигались вниз. Априорные вероятности. Предположим, что гипотеза 1 встречается в ми- ре так же часто, как и гипотеза 2. Наблюдатель может использовать эти часто - ты появления в качестве априорных вероятностей, отражающих ожидания в отсутствие конкретных сенсорных наблюдений. Тогда априорные вероят - ности гипотез 1 и 2 равны Prior(H1) = 1/2; (2.43) Prior(H2) = 1/2. (2.44) Протопостериоры. Протопостериоры двух гипотез будут следующими: Protoposterior(H1) = Prior(H1) · Likelihood(H1) = (2.45) Protoposterior(H2) = Prior(H2) · Likelihood(H2) = (2.46) Нормализация. Нормирование представляет собой сумму протопосте- риоров: Normalization = Protoposterior(H1) + Protoposterior(H2) = (2.47)\n--- Страница 69 ---\n68  Применение правила Байеса Апостериорные вероятности. Апостериорные вероятности получаются путем деления каждой протопостериорной вероятности на нормирование: Posterior(H1) = p(H1 | Iobs) = (2.48) Posterior(H2) = p(H2 | Iobs) = (2.49) Восприятие. В данном случае восприятие MAP равно H1, то есть точ- ки движутся вниз вместе. Это согласуется с законом общей судьбы, но мы пришли к нашему заключению, вычислив вероятности. Аналогичная логика применима и к другим принципам гештальта. В разделе 10.5.1 мы рассмот - рим гештальт-принцип «хорошего продолжения», который гласит, что кон- туры с округлыми краями с большей вероятностью будут восприниматься как непрерывные, чем контуры с краями, имеющими резкие или острые углы. 2.7. Оптимальность, эволюция и мотивы байесовского моделирования Понятие байесовского вывода тесно связано с понятием оптимального по- ведения. Обсуждение оптимальности требует определения целевой функции, которая определяет, насколько хороша или плоха реакция по отношению к истинному состоянию мира. В восприятии целевой функцией часто являет - ся точность (правильное сообщение об истинном состоянии мира) или ошиб- ка (степень отклонения от истинного состояния мира). В других решениях играют роль внешние вознаграждения и затраты. Наблюдатель может быть оптимален по отношению к определенной целевой функции. Если вспомнить пример из главы 1, наблюдатель может вычислить апостериорную вероят - ность «всего» 0.1 того, что лев прячется в траве; тем не менее оптимальным может быть решение покинуть этот район, потому что стоимость пребывания может быть чрезвычайно высокой, если лев действительно там есть. Вычис - ление апостериорной вероятности с учетом порождающей модели и целевой функции всегда является частью получения оптимального решения. Это свя- зано с тем, что апостериорное представление содержит все, что нужно знать об интересующем нас состоянии мира с учетом наблюдений. Мы вернемся к понятию оптимальности в главах 4 и 13. Важный нюанс связи между байесовским выводом и оптимальностью заключается в том, что если наблюдатель или агент использует неверную порождающую модель в выводе, их поведение не будет оптимальным. Ис - пользование неправильной порождающей модели в выводе – это форма не- соответствия моделей, которую мы обсудим в главе 3.\n--- Страница 70 ---\nЗаключение  69 Связь между байесовским выводом и оптимальностью служит достаточ- ным обоснованием для построения байесовских моделей. Существует аргу - мент, что в ходе эволюции некоторые общие и важные функции мозга могли быть в значительной степени оптимизированы, поэтому вполне вероятно, что в некоторых задачах будет найдено поведение, близкое к оптимально- му. Но этот аргумент более правдоподобен для эволюционно старых функ - ций – восприятия и движения, – чем для более поздних функций, таких как высшие когнитивные функции. Впрочем, нет необходимости опираться на этот аргумент, чтобы обосновать байесовское моделирование. Можно также рассматривать байесовскую модель как отправную точку для построения альтернативных субоптимальных моделей. Однако из факта, что апостериорное распределение является частью вы- вода оптимальной стратегии, вовсе не следует, что при оптимальном пове- дении мозг представляет апостериорные распределения. Это может просто означать, что отображение наблюдений в отклик у мозга такое же, как и при использовании байесовского рецепта. Такое поведение иногда называют псевдобайесовским. Демонстрация того, что мозг на самом деле отражает априорные, вероятностные и апостериорные события, является более слож - ной задачей, и мы обсудим ее в главе 15. 2.8. Заключение В этой главе была представлена точная формулировка правила Байеса и по- казано его применение к ряду задач дискретного оценивания. Вы видели, как правило Байеса делает возможными конкретные, осмысленные утверждения о вероятностях. Что касается использования правила Байеса, вы узнали сле- дующее: байесовское моделирование начинается с модели статистической структуры мира и наблюдений – порождающей модели; условные вероятности несимметричны. В общем случае p(A | B) ¹ p(B | A); правило Байеса вычисляет вероятности гипотез, обусловленных на - блюдением, – апостериорные вероятности p(H | obs) – из вероятностей наблюдения, обусловленного гипотезами, – правдоподобия p(obs | H) – и априорные вероятности р (Н); p(obs) в правиле Байеса нормализует вероятность и может быть пере- писано как p (obs) = априорные вероятности не всегда играют главную роль в байесовских моделях. В некоторых задачах, таких как гештальт-пример, априорное значение вероятности относительно не важно и доминирует правдо- подобие; в некоторых сценариях априорные знания и/или вероятности могут меняться со временем;\n--- Страница 71 ---\n70  Применение правила Байеса байесовский вывод – это компонент определения оптимальной стра- тегии в любой задаче логического вывода. Однако не все байесовские выводы оптимальны; наличие доказательств (почти) оптимального поведения не обязатель- но означает, что компоненты байесовской модели представлены во внутреннем устройстве мозга. 2.9. Рекомендуемая литература Gar Ming Chan. Bayes’ Theorem, COVID19, and Screening Tests. American Jour - nal of Emergency Medicine 38, no. 10 (2020): 2011–2013. Norman Fenton. Improve Statistics in Court. Nature 479, no. 7371 (2011): 36–37. Wilson S. Geisler and Jeffrey S. Perry. Contour Statistics in Natural Images: Grou ping across Occlusions. Visual Neuroscience 26, no. 1 (2009): 109–121. Thomas L. Griffiths and Joshua B. Tenenbaum. Optimal Predictions in Everyday Cognition. Psychological Science 17, no. 9 (2006): 767–773. Sharon Bertsch McGrayne. The Theory that Would Not Die. New Haven, CT: Yale University Press, 2011. Jason Rosenhouse. The Monty Hall Problem: The Remarkable Story of Math’s Most Contentious Brainteaser. Oxford: Oxford University Press, 2009. James V. Stone. Bayes’ Rule: A Tutorial Introduction to Bayesian Analysis. Sebtel, 2013. Max Wertheimer. Gestalt Theory. In A Source Book of Gestalt Psychology, edited by Willis D. Ellis, 1–11 Kegan Paul, Trench, Trubner, 1938. 2.10. Задачи Задача 2.1. Придумайте три примера случайных величин A и B из повседнев- ной жизни, для которых интуитивно справедливо правило p(A | B) ¹ p(B | A). В каждом случае укажите, какая вероятность выше, и объясните почему. Задача 2.2. Представьте, что вы собрали данные о наблюдениях за вымер- шей птицей додо на протяжении всей истории. Мы назовем эти данные S. Предположим, вас интересует время вымирания додо, обозначенное E. Тогда интересующая вас функция правдоподобия имеет вид (a) p(E | S) как функция S ; (b) p(E | S) как функция E ; (c) p(S | E) как функция S ; (d) p(S | E) как функция E . Кстати, в статье [153] было рассчитано это правдоподобие.\n--- Страница 72 ---\nЗадачи  71 Птица додо (Raphus cucullatus); Фредерик Уильям Фрохок, 1905 г. Задача 2.3. В начале июля 2021 г. после массовых мероприятий, связанных с празднованием Дня независимости, сотни людей в Провинстауне, штат Массачусетс, заразились вирусом, вызывающим COVID-19. Согласно статье, опубликованной в «Вашингтон пост» [85], «отрезвляющий научный анализ показал, что три четверти инфицированных людей были полностью вакци- нированы». Далее в статье подчеркивается, что инфицированные полностью вакцинированные люди вряд ли будут страдать тяжелым заболеванием. Тем не менее процитированное заявление, по понятным причинам, встревожило многих читателей, поскольку предполагало, что вакцины против COVID-19 плохо предотвращают заражение. Важная информация, не представленная в статье, заключалась в том, что подавляющее большинство жителей Про- винстауна были полностью вакцинированы против COVID-19. Интерпре- тируйте эти данные, и читатели, возможно, ощутят тревогу в свете ошибки прокурора. Задача 2.4. В некотором университете 15 % всех студентов обучаются гума- нитарным специальностям, 55 % всех студентов являются студентами бака- лавриата и 18 % магистрантов обучаются гуманитарным специальностям. Какова вероятность того, что случайный студент-гуманитарий является ма- гистрантом? Задача 2.5. Правило Байеса играет важную роль в медицинской диагности- ке. Мы проиллюстрируем это на примере следующей задачи: 1 % населения страдает болезнью D; этот показатель также называют распространенностью заболевания. Диагностический тест на D находится в стадии пилотного тес - тирования. Вероятность того, что испытуемый, не страдающий D, даст поло- жительный результат (уровень ложной тревоги), составляет 2 %. Вероятность того, что кто-то с D даст отрицательный результат (промах), составляет 3 %.\n--- Страница 73 ---\n72  Применение правила Байеса (a) Сделайте беглое предположение о вероятности того, что у человека с по- ложительным результатом теста на самом деле есть D. (b) Рассчитайте эту вероятность. Если результат сильно отличается от ваше- го ответа на вопрос (а), в чем заключается ошибочность вашего предпо- ложения? (c) Этот вариант был предложен Хуэйхуэй Чжаном, в то время студентом Пекинского университета. Предположим теперь, что есть дополнитель- ная переменная, которую мы проигнорировали, а именно идет ли кто- то к врачу, чтобы сделать диагностический тест. Эта вероятность выше, если у кого-то есть заболевание (поскольку, вероятно, будут симптомы), чем если у кого-то нет болезни. Предположим, что для этого отношение вероятности 5:1. Теперь пересчитайте вероятность того, что у человека с положительным результатом теста действительно есть заболевание D. Близка ли она к вашему беглому предположению? В качестве дополнительного упражнения см. задачу B.7. Задача 2.6. Объясните с точки зрения здравого смысла, почему правдопо- добия не должны в сумме равняться 1, в то время как априорные и апосте- риорные вероятности должны. Задача 2.7. Докажите, используя правило Байеса, что когда функция правдо- подобия совершенно плоская (имеет одно и то же значение для всех гипотез), апостериорное распределение идентично априорному распределению. Задача 2.8. Вернемся к задаче о багажной карусели из раздела 2.5. Докажите, используя правило Байеса, что если вы видите на багажной карусели чемо- дан, не похожий на ваш (например, маленький и красный, когда ваш большой и черный), апостериорная вероятность того, что это ваш багаж, равна нулю. Задача 2.9. Продолжим задачу о багажной карусели из раздела 2.5. Вы один из 100 пассажиров, ожидающих свой чемодан на багажной карусели в аэропор- ту. Он выглядит так же, как 5 % чужих чемоданов. Выведите общее выражение для вероятности того, что чемодан, который вы наблюдаете (и который ви- зуально соответствует вашему чемодану), принадлежит вам, в зависимости от количества просмотренных вами предметов багажа. Сколько чемоданов вы должны наблюдать (и не найти свой), прежде чем апостериорная вероят - ность того, что наблюдаемый чемодан (визуально совпадающий с вашим), превысит 70 %? Задача 2.10. Эта задача раскрывает ключевую особенность байесовского вывода; а именно что наблюдение увеличивает или уменьшает поддержку гипотезы не за счет правдоподобия гипотезы, а за счет отношения правдо- подобий между гипотезами. Как следствие мы можем опустить несуществен- ные детали (т. е. детали, которые одинаково масштабируют вероятности всех гипотез) из нашего определения наблюдения. В примере с багажной каруселью (раздел 2.5) мы определили зрительное наблюдение как форму, размер и цвет чемодана, и поэтому мы приняли p(наблюдение | H1) равным 1 при совпадении этих трех критериев. Но, ко-\n--- Страница 74 ---\nЗадачи  73 нечно же, точный «внешний вид» чемодана на багажной карусели включает в себя гораздо больше, чем просто форму, размер и цвет. Например, когда чемодан падает на багажную ленту, он может оказаться в любой из мно- жества различных ориентаций. Предположим, нам нужно расширить наше определение наблюдения, включив в него ориентацию чемодана, а также три других признака. Для простоты предположим, что существует 360 воз- можных углов (по одному на каждый градус окружности) и две возможные стороны (правая сторона вверх или вниз), всего 720 возможных ориентаций, в которых чемодан может располагаться на карусели. (a) Если мы далее предположим, что каждая ориентация равновероятна, то вероятность наблюдения при первой гипотезе будет больше не 1, а 1/720. Точно так же вероятность наблюдения с учетом гипотезы 2 будет больше не 0.05, а 0.05/720. Поскольку вероятности изменились, не должно ли из- мениться и апостериорное распределение? Объясните свой ответ. (b) Теперь предположим, что не все 720 ориентаций имеют вероятность 1/720, но каждая ориентация, взятая по отдельности, по-прежнему имеет одинаковую вероятность при обеих гипотезах. Например, каждый чемо- дан (ваш или нет) останавливается вертикально и выровнен параллельно краю карусели с вероятностью 0.2. Повлияет ли на ваш вывод ориентация наблюдаемого чемодана? Задача 2.11. Предположим, вы ждете определенный автобус в городе, в ко- тором всего десять автобусных маршрутов; маршрут, по которому следует каждый автобус, обозначается целым числом в углу его лобового стекла. Вы замечаете автобус издалека и, естественно, задаетесь вопросом, тот ли это автобус, которого вы ждете. (a) На основе видимого изображения трудноразличимого номера автобус - ного маршрута и вашего интуитивного понимания того, как могут вы- глядеть разные номера маршрута, предложите функцию правдоподобия, которая строит график p(визуальное наблюдение | гипотетический авто- бусный маршрут) для всех чисел от 1 до 10. (Предположим, что название места назначения для вас ничего не значит.)\n--- Страница 75 ---\n74  Применение правила Байеса (b) Вам стало известно, что по улице, на которой вы находитесь, ездят только автобусы 3, 4, 5 и 6. Кроме того, вы знаете, что автобусы 3 и 4 ходят в два раза чаще, чем автобусы 5 и 6. Основываясь на этих базовых знаниях, постройте априорное распределение номеров автобусов. (c) Используйте правило Байеса, чтобы вычислить апостериорное распре- деление номеров автобусов. Задача 2.12. Эта задача была предложена Джонатаном Горнетом, когда он был студентом Нью-Йоркского университета. Вы ученик математического факультета. Профессор пишет на доске символ, похожий на «u» или «v», а вы пытаетесь определить, какой это символ. Сенсорные наблюдения состоят из изображения рукописной буквы на сетчатке глаза, которое мы обозначаем Iobs. Мы делаем следующие предположения: других возможных букв нет; нет соответствующего контекста; «u» встречается в 1.5 раза чаще, чем «v»; вероятность того, что «u» производит Iobs, равна 0.0008, тогда как веро- ятность того, что «v» производит Iobs, равна 0.0010. (a) Объясните, почему вероятности в этой последней строке такие низкие. (b) Вычислите апостериорные вероятности p («u» | Iobs) и p («v» | Iobs). (c) Какой вывод вы сделаете? (d) Если бы были другие возможные символы, что вы могли бы сказать об их вероятностях и/или априорных значениях? Объясните. Задача 2.13. Мы можем исследовать центральную задачу цветового зрения в контексте байесовской теории. Как ни странно, эту задачу можно про- иллюстрировать без применения цвета. В разделе 1.3 мы указывали на тот факт, что определенная интенсивность света, попадающего в ваши глаза, согласуется с несколькими комбинациями истинного оттенка поверхности и интенсивности освещающего источника света. Чтобы выразить эту мысль более точно, мы используем следующую формулу для интенсивности, из- меряемой сетчаткой: Ретинальная интенсивность = оттенок поверхности · интенсивность света. (2.50) Оттенок поверхности следует интерпретировать как отраженную долю падающего света; это число от 0 до 1. Мы также принимаем интенсивность света в диапазоне от 0 до 1. Например, если оттенок поверхности равен 0.5 (средний уровень серого), а интенсивность света равна 0.2 (очень тусклый свет), то ретинальная интенсивность равна 0.5 · 0.2 = 0.1. (a) Предположим, что наблюдаемая вами ретинальная интенсивность рав- на 0.3. Соедините на диаграмме ниже все комбинации гипотетического оттенка поверхности и гипотетической интенсивности света, которые могли создать эту ретинальную интенсивность. У вас должна получиться изогнутая, а не прямая линия.\n--- Страница 76 ---\nЗадачи  75 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0Предполагаемая интенсивность светаПредполагаемый оттенок поверхности (b) Объясните утверждение: «Кривая, которую мы только что нарисовали, представляет собой комбинации оттенка поверхности и интенсивности света, которые имеют высокое правдоподобие». (c) Объясните, как эта кривая связана с понятием неоднозначности вос - приятия. Задача 2.14. Представьте, что вы живете в очень скучном мире, состоящем из сетки квадратов 2×10: В этом мире всегда случаются только две вещи: с вероятностью 40 % в этом мире появится вертикальная черта, состоящая из двух черных квадратов в столбце, выбранном так, что каждый возможный столбец равновероятен. С вероятностью 60 % один черный квадрат появится в случайном месте в верхнем ряду, а другой черный квадрат появится в случайном месте в ниж - нем ряду. Делая вывод, мы будем называть эти возможности гипотезами 1 и 2 соответственно. (a) Предположим, вы получили следующий зрительный образ: Численно покажите, что исходя из этого изображения апостериорная ве- роятность гипотезы 1 равна 0 %. Запишите все шаги вашего рассуждения, сопровождая каждый шаг кратким пояснением.\n--- Страница 77 ---\n76  Применение правила Байеса (b) Предположим, вы получили следующий зрительный образ: Численно покажите, что на исходя из этого изображения апостериорная вероятность гипотезы 1 составляет примерно 87 %. Запишите все шаги ваше- го рассуждения, сопровождая каждый шаг кратким пояснением. (c) Как ваш ответ на задание (b) объясняет, почему люди-наблюдатели склонны воспринимать второе изображение как содержащее один объ- ект? Задача 2.15. Представьте, что вы живете в немного более интересном мире, состоящем из сетки квадратов 5×5. Опять же, в этом мире случаются только две вещи: «большой блок»: с вероятностью 50 % в случайном месте появится боль- шой блок, состоящий из 2×2 маленьких черных квадратов; «маленькие блоки»: с вероятностью 50 % выпадут 4 независимых чер- ных блока, каждый занимающий по одной клетке, и опять же в случай- ных местах. Блоки будут появляться один за другим, и ни один новый блок не может занимать место уже существующего. Делая вывод, мы будем называть эти возможности гипотезами 1 и 2 соот - ветственно. (a) Нарисуйте диаграмму порождающей модели. Объясните, почему вы на- рисовали ее именно так. (b) Сначала мы рассмотрим гипотезу 1. Сколько существует возможностей разместить в этой сетке 5×5 объект 2×2? Предполагая равные вероят - ности возможных мест появления, вычислите вероятность того, что он появится именно в наблюдаемом месте.\n--- Страница 78 ---\nЗадачи  77 (c) Является ли ответ на задание (b) правдоподобием, априорной или апо- стериорной вероятностью по отношению к гипотезе о том, что существу - ет единственный объект? Объясните ответ. Теперь рассмотрим гипотезу 2. (d) Заполните пропуски: чтобы поместить один из этих четырех маленьких блоков, в сетке есть ____ из ____ расположений, которые согласуются с на- блюдениями. Это дает вероятность ____. Чтобы затем разместить второй блок, в сетке осталось ____ мест из ____, которые согласуются с наблюде- ниями. Это дает вероятность ____. (e) Повторите для третьего и четвертого маленьких блоков. (f) Перемножьте четыре вероятности, которые вы нашли в (d) и (e). (g) Является ли ответ на задание (f) правдоподобием, априорной или апо- стериорной вероятностью гипотезы? Объясните ответ. (h) Во сколько раз вероятность, которую вы нашли в (b), больше вероятности, которую вы нашли в (f)? (i) Как ваш ответ на вопрос (h) объясняет, почему люди-наблюдатели склон- ны воспринимать изображение как содержащее один большой блок? Задача 2.16. Это продолжение задачи 1.12 о байесовском объяснении ука- чивания в соответствии с гипотезой Трейсмана. В этой задаче вы рисовали гистограммы без чисел. Теперь, когда вы знаете, как работает правило Байе- са, вы можете выполнить тот же расчет более количественно. (a) Придумайте разумные значения для априорных вероятностей гипотез 1, 2 и 3. В сумме они должны быть равны 1. (b) Снова предположим, что вы испытываете большое расхождение между вашими зрительными и вестибулярными наблюдениями. Придумайте разумные значения для вероятностей гипотез 1, 2 и 3. (Достаточно, что- бы это были относительные величины.) (c) На основе (a) и (b) рассчитайте апостериорные вероятности трех гипотез. Убедитесь, что их сумма равна 1. Также убедитесь, что они соответствуют гипотезе Трейсмана. Задача 2.17. Согласно исследованиям восприятия трехмерных объектов, принцип общего вида [56, 96] утверждает, что наблюдатель не находится в осо- бом положении относительно сцены. Этот принцип объясняет, почему изо- бражение (A) обычно воспринимается как изображение квадрата, а не как куб (B), рассматриваемый под особым углом зрения. (A) (B ) Обсудите принцип общего вида с точки зрения байесовской теории.",
      "debug": {
        "start_page": 53,
        "end_page": 78
      }
    },
    {
      "name": "Глава 3. Байесовский вывод в условиях зашумленных измерений 78",
      "content": "--- Страница 79 --- (продолжение)\nГлава 3 Байесовский вывод в условиях зашумленных измерений Как байесовский наблюдатель делает вывод о состоянии мира на основе зашумленных измерений? В главе 1 мы представили концепцию байесовского вывода на различных примерах из повседневной жизни. В главе 2 показали, как выполнять расчеты по правилу Байеса. Однако примеры, которые мы там использовали, вклю- чали выводы категориальных переменных. Использование категориальных переменных упрощает байесовские вычисления, но на практике многие пе- ременные являются непрерывными, а не категориальными. К примерам не- разрывно меняющихся состояний мира, которые мозгу может понадобиться вывести, относятся ориентация сегмента линии, местоположение источника звука, скорость движущегося объекта, цвет поверхности или время, прошед- шее между двумя событиями. В этой главе мы обсудим байесовские модели для таких непрерывных переменных. Краткое содержание главы В этой главе мы в основном рассматриваем переменные состояния мира с действительными значениями; такие переменные могут принимать значе- ния от отрицательной бесконечности до бесконечности. Мы рассматриваем наблюдателя, который выводит эти переменные из зашумленных сенсорных наблюдений. Мы введем понятие измерения, которое представляет собой абстракцию сенсорных наблюдений, обитающих в том же пространстве, что и сама переменная состояния мира. Несовершенства внутреннего представ- ления стимула затем можно удобно смоделировать как гауссов шум измере- ний. Затем мы обсудим, как наблюдатель комбинирует априорную вероят - ность с правдоподобием для вычисления апостериорной вероятности.\nГлава 3 Байесовский вывод в условиях зашумленных измерений Как байесовский наблюдатель делает вывод о состоянии мира на основе зашумленных измерений? В главе 1 мы представили концепцию байесовского вывода на различных примерах из повседневной жизни. В главе 2 показали, как выполнять расчеты по правилу Байеса. Однако примеры, которые мы там использовали, вклю- чали выводы категориальных переменных. Использование категориальных переменных упрощает байесовские вычисления, но на практике многие пе- ременные являются непрерывными, а не категориальными. К примерам не- разрывно меняющихся состояний мира, которые мозгу может понадобиться вывести, относятся ориентация сегмента линии, местоположение источника звука, скорость движущегося объекта, цвет поверхности или время, прошед- шее между двумя событиями. В этой главе мы обсудим байесовские модели для таких непрерывных переменных. Краткое содержание главы В этой главе мы в основном рассматриваем переменные состояния мира с действительными значениями; такие переменные могут принимать значе- ния от отрицательной бесконечности до бесконечности. Мы рассматриваем наблюдателя, который выводит эти переменные из зашумленных сенсорных наблюдений. Мы введем понятие измерения, которое представляет собой абстракцию сенсорных наблюдений, обитающих в том же пространстве, что и сама переменная состояния мира. Несовершенства внутреннего представ- ления стимула затем можно удобно смоделировать как гауссов шум измере- ний. Затем мы обсудим, как наблюдатель комбинирует априорную вероят - ность с правдоподобием для вычисления апостериорной вероятности.\n--- Страница 80 ---\nЭтапы байесовского моделирования  79 3.1. Этапы байесовского моделирования Психофизика изучает то, как организмы воспринимают управляемые стиму - лы или действуют в ответ на них. Например, экспериментатор может пока- зать вам две линии на экране компьютера и спросить, какая из них длиннее. Когда линии очень похожи по длине, это сложная задача, и вы будете делать ошибки. Эти ошибки могут рассказать экспериментатору о том, как вы ре- шаете задачу. Начиная с работ Густава Теодора Фехнера, примерно с 1860 г., исследователи использовали психофизические методы для исследования природы перцептивной обработки. Задача психофизики, которую мы будем использовать в качестве основного примера в этой главе, – это задача на слуховую локализацию. Представьте, что вы стоите перед проекционным экраном, на котором отображается горизонтальная линия, протянувшаяся по ширине экрана. За экраном, на той же высоте, что и линия, расположено в ряд большое количество очень маленьких динамиков (звукоизлучателей). Звук будет исходить из одного из этих динамиков. Ваша задача как испыту - емого состоит в том, чтобы указать курсором место, откуда вы восприняли исходящий звук. Вы делаете это много раз; каждая пара стимул–реакция представляет собой испытание (trial). В этом эксперименте испытуемый оценивает непрерывную величину, а именно положение источника звука вдоль линии. Субъект имеет сенсор- ные наблюдения, но, возможно, также и предварительные знания. Следова- тельно, возникает задача байесовского вывода. Первые этапы построения байесовской модели для этой простой задачи концептуально такие же, как и в разделе 2.1, но здесь мы добавляем третий этап (рис. 3.1). Порождающая модель Вывод Состояние мира sСенсорное наблюдениеОценка состояния мира sˆ Рис. 3.1  Схема байесовской модели. Вероятности состояний мира s и сенсорных наблю- дений при данных состояниях мира составляют порождающую модель. Выявление распре- делений этих вероятностей представляет собой этап 1. В каждом испытании наблюдатель выполняет вывод, чтобы получить оценку состояния мира. Задание выражения для этой оцен- ки – этап 2. Во многих испытаниях сама оценка следует распределению для заданного истин- ного s. Указание этого распределения – этап 3 Этап 1 состоит из определения порождающей модели, т. е. распределения вероятностей, связанных с переменными состояния мира и сенсорными на- блюдениями. Порождающая модель – это полное статистическое описание\n--- Страница 81 ---\n80  Байесовский вывод в условиях зашумленных измерений того, что происходит в задаче. В примере с блестящим полом в разделе 2.1 порождающая модель определила вероятность того, что пол мокрый, того, что пол блестит, если он мокрый, и того, что пол блестит, если он сухой. Многие распределения в порождающей модели определяются планом экс - перимента. Например, вероятность появления звука в определенном месте указывается в плане эксперимента. Однако нам необходимо сделать предпо- ложение о распределении сенсорных наблюдений. Многие байесовские мо- дели допускают возможность того, что сенсорные наблюдения зашумлены. Термин «шум» имеет разное смысловое наполнение в различных научных областях, но в этой книге мы имеем в виду, что один и тот же стимул не всегда вызывает одно и то же внутреннее представление в мозгу. Таким об- разом, шум подразумевает случайную изменчивость сенсорных наблюдений от испытания к испытанию. Шум может быть вызван самыми разными фак - торами, как внешними (в окружающем мире), так и внутренними (в мозгу). Этап 2 описывает, как наблюдатель делает вывод, то есть как он оценивает состояние мира на основе сенсорных наблюдений и предварительных ожи- даний. Как и в главе 2, этап 2 состоит из двух промежуточных этапов. Этап 2а использует правило Байеса для вычисления апостериорного распределения наблюдателя, то есть распределения вероятностей наблюдателя по интере- сующему состоянию мира с учетом сенсорных наблюдений. Процесс вывода наблюдателя «инвертирует» порождающую модель. В примере с блестящим полом процесс вывода состоял из вычисления вероятности того, что пол мок - рый, исходя из сенсорных наблюдений и априорной информации. Порожда- ющая модель, принятая наблюдателем, наряду с сенсорными наблюдения- ми полностью определяет апостериорное распределение; дополнительная информация не требуется. Этап 2b определяет, как наблюдатель получает оценку состояния мира из апостериорного распределения. Это может быть, например, мода или среднее значение апостериорного распределения. При наличии шума в наблюдениях выполняется этап 3 байесовского мо- делирования. Процесс вывода описывает вычисление наблюдателем апосте- риорного распределения вероятностей по состояниям мира и выбор оценки состояния мира; в конце процесс вывода резюмируется как отношение вво- да-вывода между сенсорными наблюдениями и оценкой состояния мира. Этап 3 выводит распределение вероятностей оценки с учетом стимула, что делается путем объединения отношения ввода-вывода с распределением сенсорных наблюдений. В этой главе мы обсудим этапы 1 и 2, а этап 3 для той же задачи будет рассмотрен в главе 3. 3.2. Этап 1: порождающая модель Порождающая модель представляет собой описание статистической струк - туры задачи. В задаче слуховой локализации состояние мира, которое на- блюдатель пытается вывести, представляет собой единственное свойство стимула, а именно его горизонтальное положение в континууме; громкость звука, частота или другие характеристики не представляют интереса для\n--- Страница 82 ---\nЭтап 1: порождающая модель  81 этой задачи. Мы часто будем называть релевантное для задачи свойство стимула, обозначаемое буквой s, просто «стимул». Сенсорные наблюдения, генерируемые местоположением звука, состоят из сложного паттерна слу - ховой нейронной активности. Для целей нашей модели и отражая обычную практику моделирования психофизических данных, мы сводим сенсорные наблюдения к одному скаляру, а именно к зашумленному измерению, обо- значаемому x. Измерение «обитает» в том же пространстве, что и сам стимул, и использует те же единицы измерения, что и стимул. Сейчас мы подробнее остановимся на этом понятии. 3.2.1. Измерение: абстрактное сенсорное представление Физический раздражитель (стимул) вызывает активность нервной системы. Эта активность будет случайным образом варьироваться от испытания к ис - пытанию, даже если сам стимул каждый раз идентичен. Такая изменчивость, или шум, имеет множество источников. Наши органические сенсоры подвер- жены случайной изменчивости из-за внутренних стохастических процессов. Например, тепловой шум влияет на реакцию волосковых клеток внутреннего уха, воспринимающих звуковые волны. Процесс преобразования, с помощью которого нервная система улавливает физическую энергию и преобразует ее в электрический отклик – поглощение фотонов фоторецепторами, – также является стохастическим. На субклеточном уровне высвобождение нейро- трансмиттера и открытие и закрытие ионных каналов являются стохастиче- скими процессами. Мы можем ожидать, что шум будет иметь поведенческие последствия. Например, если мы поместим указательный палец правой руки на верхнюю часть стола и попытаемся поместить указательный палец левой руки в соответствующее место под столом, будет наблюдаться заметная раз- ница положений (типичная вариабельность в этой задаче составляет около 2 см). Это указывает на шум в наших внутренних проприоцептивных пред- ставлениях о расположении конечностей. Точно так же трудно оценить, тя- желее ли один объект, чем другой, основываясь на нашем ощущении силы, потому что внутреннее измерение силы зашумлено, что заставляет нас ис - пользовать весы. Эти примеры предполагают, что связь между стимулом и реакцией сенсора является стохастической. В этой книге, как и в большинстве поведенческих моделей, мы не модели- руем нейронную репрезентацию стимула напрямую, потому что в этом нет необходимости при использовании только поведенческих данных. Вместо этого мы определяем измерение как абстракцию или редукцию нейронной репрезентации в самом стимульном пространстве. Например, если истинное местоположение s звука находится на 3° правее относительно направления вперед, то его измерение x может составлять 2.7° или 3.1°. Термин «измере- ние» происходит от аналогии с проведением физических измерений. Если длина палки 89.0 см, вы можете измерить ее длину как 89.5, 88.1, 88.9 см и т. д. Мы говорим, что измерение «обитает» в том же пространстве, что\n--- Страница 83 ---\n82  Байесовский вывод в условиях зашумленных измерений и стимул, потому что оно использует те же единицы измерения, что и стимул. Измерение температуры само по себе является температурой, измерение цвета само по себе является цветом и т. д. В нашем примере исследования общим пространством стимула и измере- ния является линия, на которой расположены динамики, но это может быть и многое другое, например положительные действительные числа (для сти- мула, такого как длина или вес), круг (пример: направление движения) или многомерное пространство. 3.2.2. Графовая модель Наш пример содержит две переменные: переменную состояния мира или стимул (истинное местоположение звука, s) и измерение стимула наблюдате- лем x. Эти две переменные присутствуют в порождающей графовой модели, которая изображена на рис. 3.2. Напомним, что графовая модель состоит из узлов, содержащих случайные переменные, и стрелок, представляющих стохастические зависимости между переменными. Каждый узел связан с распределением вероятностей. Переменная в конце стрелки имеет рас - пределение вероятности, которое зависит от значения переменной или пере- менных в начале стрелки. Другими словами, можно понимать, что стрелка представляет влияние одной переменной на другую. Ни одна стрелка не указывает на переменную s, поэтому распределение s является равномерным распределением ps(s). Это распределение представляет собой общую частоту появления каждого возможного значения стимула. Стрелка, указывающая от s к x, указывает, что распределение x зависит от значения s. Математически это выражается как условное распределение вероятностей px|s(x | s). Условные распределения вероятностей формально определены в разделе B.11.3. Если вы незнакомы с распределениями вероятностей или долго с ними не встре- чались, самое время прочитать приложение B. Сейчас мы подробно опишем компоненты порождающей модели. Примечание 3.1 Обозначения для распределений вероятностей Строго говоря, вероятность должна быть обозначена как случайной величиной, так и ее значением. Другими словами, ps(2) будет обозначать распределение вероят - ностей случайной величины s, оцененной при значении 2. Значение часто являет - ся обобщенным, что приводит к несколько избыточным обозначениям, таким как ps(s). Поэтому мы обычно опускаем нижний индекс, указывающий имя случайной величины, и вместо этого присваиваем это имя значению. Это сокращенное обо- значение практически всегда однозначно. Иногда необходимо включать нижний индекс, например когда подставляется конкретное значение и нужно отслеживать, какое распределение рассматривается. Для получения более подробной информа- ции см. приложение А. В текущей главе мы сохраним нижние индексы в дидакти- ческих целях.\n--- Страница 84 ---\nЭтап 1: порождающая модель  83 px|s(x | s)ps(s) xs Риc. 3.2  Схема байесовской модели. Первым этапом в байесовском моделировании является определение порождающей модели. Эта диаграм- ма является графическим представлением по- рождающей модели, обсуждаемой в данной главе. Каждый узел представляет случайную величину, каждая стрелка – влияние. Здесь s – истинный сти- мул, а x – зашумленное измерение стимула 3.2.3. Распределение стимулов Распределение, связанное со стимулом s, обозначается как ps(s). Это рас - пределение состояний мира или – в нашем текущем примере – распреде- ление стимулов отражает, как часто встречается каждое возможное значе- ние s. В нашем примере исследования экспериментатор запрограммировал компьютер на получение стимула в каждом испытании из гауссова, или нормального, распределения со средним значением μ и дисперсией σs2. Это распределение определяется следующей функцией плотности вероятности (примечание 3.2): (3.1) Пример такой плотности показан на рис. 3.3А. Среднее значение, равное нулю, означает, что экспериментатор чаще воспроизводит тон по направле- нию прямо вперед относительно испытуемого, чем в любом другом месте. Примечание 3.2 Гауссово (нормальное) распределение Наиболее часто используемым непрерывным распределением вероятностей явля- ется распределение Гаусса, или нормальное распределение. Его функция плотности имеет вид\n--- Страница 85 ---\n84  Байесовский вывод в условиях зашумленных измерений (3.2) Это знаменитое колоколообразное распределение (или колоколообразная кривая). Иногда мы будем использовать уравнение p(y) = �(y; μ, σ2) (3.3) как сокращенную запись уравнения (3.2). Параметры μ и σ являются средним зна- чением и стандартным отклонением случайной величины соответственно. Множи- тель нужен для того, чтобы полная вероятность – интеграл по p(y) – равнялась 1; такой множитель называется нормировочной константой (раздел B.5.6). Показатель степени имеет максимальное значение (ноль) при y = μ и, следовательно, является модой распределения Гаусса. При удалении от максимума показатель сте- пени затухает. Он станет равен -1, как только разница между y и μ достигнет σ2. В этом случае распределение Гаусса уменьшится в e раз. Распределения Гаусса воз- никают, когда множество случайных колебаний может повлиять на наблюдаемую переменную. Более формальная версия этого утверждения называется централь- ной предельной теоремой. Типичным примером является рост людей, который под- чиняется приблизительно гауссову распределению, скорее всего потому, что на рост влияют многие факторы. Предположим, что средний рост женщин составляет 165 см со стандартным отклонением 10 см. В этом случае мы обнаружим много женщин с ростом от 155 до 175 см (в пределах одного стандартного отклонения от среднего), меньшее количество женщин между 145 и 155 и между 175 и 185 см и очень мало женщин выше 185 см или ниже 145 см (более двух стандартных от - клонений от среднего). Распределения Гаусса удобны для аналитических расчетов; например, перемножение двух гауссианов дает еще один гауссиан. Они также удоб- ны для моделирования; например, чтобы взять выборки из распределения Гаусса со средним значением μ и дисперсией σ2, можно взять выборки из распределения со средним значением 0 и стандартным отклонением 1 (стандартное нормальное распределение), умножить их на σ и прибавить μ. Упражнение 3.1. Если в уравнение (3.2) подставить y = μ и σ = 0.1, получим p(y) = 3.99. Как вероятность может быть больше 1? Если ответ на этот вопрос сразу не ясен, прочтите в приложении раздел B.5.4 о разнице между функ - циями массы и плотности вероятности. 3.2.4. Распределение измерений В следующих нескольких подразделах мы отступим от примера локализации источника звука; наш разговор будет касаться общих тем. Распределение измерения – это распределение измерения x для заданного значения стиму - ла s. Это условное распределение px|s(x | s) описывает вероятность появления значений измерения, когда одно и то же значение стимула s повторяется много раз. Если изменчивость измерения зависит от многих источников, мы получим распределение измерения, которое будет примерно гауссовым. Это\n--- Страница 86 ---\nЭтап 1: порождающая модель  85 утверждение является – в общих чертах – следствием центральной предель- ной теоремы (примечание 3.2). В то время как гауссова форма распределения стимулов, уравнение (3.1), была выбрана для удобства, гауссова форма рас - пределения измерений является довольно фундаментальной, независимой от плана эксперимента и общей для большинства байесовских моделей, ко- торые мы обсуждаем в этой книге. Уравнение для распределения измерения имеет вид: (3.4) где σ – стандартное отклонение шума при измерении, также называемое уровнем шума измерения, или уровнем сенсорного шума. Это гауссово распре- деление показано на рис. 3.3В для одного значения s. Чем выше σ, тем шум- нее измерение и тем шире его распределение. Обратное значение дисперсии измерения иногда называют достоверностью (reliability), или точностью (precision), измерения. Уровень шума измерения определяется множеством внутренних и внеш- них факторов, которые зависят от природы стимула и сенсорной модаль- ности, используемой для его измерения. При визуальном измерении про- странственного положения небольшого стимула на σ влияют такие факторы, как эксцентриситет сетчатки (расстояние от стимула до точки, на которой Вероятность Стимул s Измерение xпри μ = 0 μσsσпри s = 2 s –10 –10 –8 –8 –6 –6 –4 –4 –2 –2 0 0 2 2 4 4 6 6 8 8 10 10(A) (B) Рис. 3.3  Распределения вероятностей, принадлежащих двум случайным переменным в порождающей модели: (A) гауссово распределение по стимулу ps(s). Поскольку s непре- рывно, это распределение является функцией плотности вероятности. Его среднее значение µ = 0, а стандартное отклонение σs = 1.5. В нашем примере исследования мы не используем единицы измерения, но если вы предпочитаете конкретику, то можете рассматривать их как сантиметры, дюймы или градусы угла зрения; (B) предположим, что теперь мы фиксируем кон- кретное значение s, например 2. Затем мы предполагаем, что измерение x следует гауссову распределению вокруг этого значения со стандартным отклонением σ, например 1. На линии внизу показано несколько выборок х\n--- Страница 87 ---\n86  Байесовский вывод в условиях зашумленных измерений фиксируется взгляд), контраст и размытие (см. также рис. 1.5). Когда место- положение измеряется на слух, на σ влияют такие факторы, как отношение сигнал/шум, угол относительно прямого направления и шаг между поло- жениями источника. Независимо от модальности, на σ также будут влиять время экспозиции сигнала и уровень внимания. Примечание 3.3 Шум и неоднозначность Как объяснялось в главе 1, существует множество источников неопределенности в восприятии. В этой главе мы рассматриваем неопределенность, возникающую из- за шума в сенсорных измерениях наблюдателя. Поскольку наши сенсорные систе- мы повсеместно подвержены шуму измерений, эта форма неопределенности всег - да в той или иной степени присутствует в восприятии. Однако неопределенность может дополнительно возникать из-за неоднозначности самого стимула: разные состояния мира могут вызывать один и тот же сенсорный стимул. Примером не- однозначного изображения является блестящий пол, который может быть мокрым, а может и не быть (раздел 2.3). Далее в книге мы встретим дополнительные приме- ры неоднозначных изображений (например, неоднозначность размер–расстояние в разделе 9.2). Независимо от того, вызвана ли неопределенность только сенсорным шумом или также неоднозначностью стимула, она приводит к тому, что ненулевой вероятности наблюдения соответствует более одного гипотетического состояния мира. 3.2.5. Совместное распределение Вместе два распределения ps(s) и px|s(x | s) полностью определяют порожда- ющую модель. Их можно объединить в единое совместное распределение, которое выражает вероятность появления каждой комбинации s и x: ps,x(s, x) = ps(s)px|s(x | s). (3.5) Это математическое тождество определяет совместное распределение всех переменных в задаче. 3.3. Этап 2: вывод Организмы не располагают прямым знанием о состояниях мира. Мозг на- блюдателя должен сделать вывод о значении интересующего состояния мира на основе наблюдений. Если наблюдения принимают форму измерения, это означает вывод значения s из наблюдаемого измерения, которое мы будем обозначать xobs. Наблюдатель выдвигает различные гипотезы о том, чем мо-\n--- Страница 88 ---\nЭтап 2: вывод  87 жет быть s; обозначим предполагаемую стимульную переменную через shyp. Важно отличать ее от истинного стимула s. В данном испытании существует только одно истинное значение стимула, соответствующее тому факту, что существует только одна объективная реальность. Напротив, shyp принимает различные значения даже на протяжении одного испытания; эти значения представляют различные гипотезы, которые наблюдатель выдвигает отно- сительно стимула. Затем наблюдатель вычисляет априорное распределение, функцию правдоподобия и апостериорные значения shyp. Наконец, наблю- датель «выводит» апостериорное распределение, чтобы получить оценку стимула. В табл. 3.1 приведены различия между этапами 1 и 2 процесса мо- делирования. Таблица 3.1. Сравнение этапа 1 (порождающая модель) и этапа 2 (вывод) Этап 1: порождающая модельЭтап 2: вывод (решающая модель) Точка зрения мира наблюдателя (автора решений) Способ реализации через множество испытаний через единственное испытание Природа распределения (примечание 3.4)объективная (вероятность события)субъективная (степень доверия) Выход выражения для распределений всех переменных, включая наблюденияправило отображения наблюдений в решения (ожидаемое состояние мира) 3.3.1. Априорное распределение Вернемся теперь к нашему примеру с локализацией источника звука. В раз- деле 3.2 мы ввели распределение стимулов ps(s), которое отражает – в смысле функции плотности, – как часто каждое расположение источника встречается в эксперименте. Предположим, что наблюдатель изучил это распределение в результате интенсивного обучения. Тогда у наблюдателя уже будет ожида- ние стимула еще до того, как он появится, а именно что s = μ будет наиболее вероятным и что вероятность снижается в соответствии с изученной кривой Гаусса. Ожидание, которого наблюдатель придерживается относительно сти- мула, не получив никаких доказательств в данном испытании, составляет предварительное знание. Априорная плотность вероятности гипотетическо- го значения стимула s получается путем подстановки этого значения в рас - пределение стимула ps; таким образом, априорная вероятность равна ps(s). В процессе логического вывода ps(s) называется априорным распределением (рис. 3.4А). В отличие от распределения стимулов, априорное распределение существует для отдельного испытания: оно отражает убеждения наблюдателя в этом испытании. Таким образом, это пример субъективного распределения (примечание 3.4): вероятность интерпретируется как степень уверенности, а не как вероятность события.\n--- Страница 89 ---\n88  Байесовский вывод в условиях зашумленных измерений Вероятность Правдоподобиепри μ = 0 μσsσпри xobs = 1.3 xobs –10 –10 –8 –8 –6 –6 –4 –4 –2 –2 0 0 2 2 4 4 6 6 8 8 10 10(A) (B) Априорное распределение Функция правдоподобия функция от shyp Предполагаемый стимул shyp Предполагаемый стимул shyp Рис. 3.4  Рассмотрим одно испытание, в котором измеряется xobs. Наблюдатель пытается сделать вывод, какое значение стимула s произвело это измерение. Две функции, которые играют роль в процессе вывода наблюдателя (в одном испытании), – это априорная вероят - ность и правдоподобие. Аргументом как априорного распределения, так и функции правдопо- добия является гипотетический стимул shyp: (A) априорное распределение. Это распределение отражает убеждения наблюдателя о различных возможных значениях, которые может при- нимать стимул; (B) функция правдоподобия от стимула на основе измерения xobs. Функция правдоподобия сосредоточена на xobs Примечание 3.4 Объективные и субъективные вероятности Иногда делают различие между объективным и субъективным распределениями вероятностей. Объективные вероятности отражают теоретическую частоту возник - новения, в то время как субъективные вероятности привязаны к наблюдателю и от - ражают степень уверенности. Из трех этапов байесовского моделирования второй (вывод) имеет дело с субъективными распределениями вероятностей, потому что все распределения на этом этапе представляют убеждения наблюдателя о состоя- ниях мира в данном испытании. Первый и третий этапы имеют дело с объектив- ными распределениями вероятностей, поскольку сенсорные наблюдения и оценки могут (в принципе) подсчитываться. Различие между объективной и субъективной вероятностями обсуждается далее в разделе B.1 приложения. Это различие не важ - но для вычислений, оно имеет значение только для интерпретации. 3.3.2. Функция правдоподобия В интуитивном понимании функция правдоподобия представляет собой мнение наблюдателя о переменной только с учетом измерений – без ка - ких-либо предварительных знаний. Функция правдоподобия содержит всю информацию о переменной, которую можно объективно получить из изме- рения: больше информации получить нельзя, и любая другая информация будет неверной.\n--- Страница 90 ---\nЭтап 2: вывод  89 Функция правдоподобия выводится в аналитической форме из распреде- ления измерений (или, в более общем смысле, из распределения наблюдений с учетом состояния мира). В нашем текущем примере распределение из- мерений равно px|s(x | s). Это означает, что для данного наблюдения, которое мы обозначим через xobs, мы знаем функцию правдоподобия по shyp, которую обозначим через � (shyp; xobs): �(shyp; xobs) º px|s(xobs | shyp), (3.6) где мы используем символ º для обозначения определения. Эти слегка за- путанные обозначения отражают тот факт, что мы подставляем конкретное наблюдение xobs и конкретное гипотетическое состояние мира shyp в рас - пределение измерений px|s, полученное на этапе 1. В частности, для распре- деления измерений, заданного уравнением (3.4), функция правдоподобия имеет вид (3.7) На первый взгляд наше определение вероятности может показаться стран- ным: зачем нам определять p(x | s) под новым именем? Ключевой момент здесь в том, что функцией правдоподобия является функция shyp, а не xobs. Интерпретация функции правдоподобия осуществляется с точки зрения ги- потез. Когда наблюдатель сталкивается с конкретным измерением, какова вероятность этого измерения, когда shyp принимает определенное значение? Каждое возможное значение состояния мира – это гипотеза, а вероятность этой гипотезы – это уверенность наблюдателя в том, что измерение воз- никнет при этой гипотезе. Другими словами, правдоподобие гипотезы – это ощущение наблюдателем того, насколько измерение совместимо с гипоте- зой. Таким образом, фундаментальное различие между распределением из- мерений и функцией правдоподобия состоит в том, что первое представляет собой объективное распределение вероятностей, а второе – субъективные убеждения наблюдателя (примечание 3.4). Это аналогично различию между распределением состояния мира и априорным распределением. В контекс - те нашей задачи локализации источника звука функция правдоподобия на рис. 3.4В отражает уверенность наблюдателя в том, что измерение будет про- исходить из каждого гипотетического местоположения звука. Примечание 3.5 Правдоподобие чего? Функция правдоподобия численно равна условной вероятности, но всегда является функцией переменной после знака | (для нас состояние мира). Часто говорят «прав- доподобие измерений» или «правдоподобие наблюдений», но это неправильно. Следует говорить «вероятность измерений (при данном состоянии мира)» и «веро- ятность состояния мира (при данном измерении)».\n--- Страница 91 ---\n90  Байесовский вывод в условиях зашумленных измерений Функция правдоподобия в общем случае не нормирована, т. е. обычно не интегрируется до 1. Причина в том, что она является функцией переменной после знака «дано», а не перед ним. Вот почему функция правдоподобия на- зывается функцией, а не распределением (распределение всегда нормиро- вано). Функция правдоподобия в нашем примере, показанном на рис. 3.4Б, оказывается нормированной (относительно shyp), поскольку в гауссовом рас - пределении аргумент и среднее можно поменять местами без изменения распределения. Однако из главы 2 мы уже знаем, что правдоподобие не нуж - но нормировать. Оценка максимального правдоподобия. Используя функцию правдоподо- бия в уравнении (3.7), можно сделать наилучшее предположение о значении стимула. Это наилучшее предположение называется оценкой максимального правдоподобия (maximum-likelihood estimate, MLE) для s, и мы обозначаем его как sˆML; «крышечка» над буквой обозначает оценку. Формально определение MLE таково: (3.8) Здесь оператор argmax означает «аргумент максимума»: значение пере- менной, записанное под ним, для которого функция, следующая за ним, принимает наибольшее значение. В нашем примере MLE просто равен из- мерению xobs. Это означает, что местонахождение источника звука, который с наибольшей вероятностью будет производить измерения xobs, является са- мим xobs. Ширина функции правдоподобия. Ширина функции правдоподобия ин- терпретируется как уровень неуверенности наблюдателя, основанный только на наблюдениях. Узкая функция правдоподобия означает, что наблюдатель уверен, а широкая – что он не уверен. Хотя из уравнения (3.7) следует, что ширина функции правдоподобия идентична ширине распределения измере- ний, эти ширины имеют разные интерпретации. Последняя количественно определяет разброс измерений, первая – уровень неопределенности, осно- ванный на одном измерении. Примечание по терминологии: мы часто говорим о среднем значении и дисперсии функции правдоподобия. Это несколько неточно, потому что средние и дисперсии связаны с распределениями вероятностей. Однако мы можем нормировать функцию правдоподобия, чтобы она стала распределе- нием вероятностей, а среднее значение и дисперсия, на которые мы ссыла- емся, являются средним значением и дисперсией нормированной функции правдоподобия. 3.3.3. Апостериорное распределение Байесовский наблюдатель вычисляет апостериорное распределение состоя- ния мира на основе наблюдений, используя знание порождающей моде- ли. В нашем примере апостериорное распределение представляет собой\n--- Страница 92 ---\nЭтап 2: вывод  91 p(shyp | xobs) – функцию плотности вероятности для гипотетического стимула shyp при заданном измерении xobs. Правило Байеса принимает вид (3.9) Его также часто записывают как ps|x(shyp | xobs) µ px|s(xobs | shyp)ps(shyp), (3.10) или, используя уравнение (3.6), как ps|x(shyp | xobs) µ �(shyp; xobs)ps(shyp). (3.11) Это означает, что апостериорное распределение пропорционально про- изведению правдоподобия и априорного распределения. Это произведение мы назвали протопостериором в главе 2. В последних двух выражениях мы использовали знак пропорциональности (µ ) вместо множителя , по - скольку этот множитель действует просто как нормировочный коэффициент. Если мы не знаем нормировочный коэффициент, мы все равно знаем пол- ную форму апостериорного распределения вероятностей. Более подробное объяснение см. в примечании 3.6. В табл. 3.2 приведены отношения между функцией правдоподобия, нормированной функцией правдоподобия, про- топостериором и апостериорным распределением. Примечание 3.6 Зачем нужен знак пропорциональности? Обычно правило Байеса записывают в виде уравнения (3.10) со знаком пропорцио- нальности. Эта форма записи оправдана, потому что знаменатель правила Байеса – здесь px(xobs) – не зависит от аргумента апостериорного распределения (в данном случае интересующего нас мирового состояния s). Таким образом, он просто дей- ствует как мультипликативная константа. Мультипликативная константа не меняет формы функции или того, где эта функция максимальна. Конечно, мультиплика- тивная константа не является произвольным числом. Она должна быть такой, что- бы общая интегрированная вероятность равнялась 1. По этой причине также называется нормировочной константой. Мы можем записать p(xobs) как сумму или интеграл числителя по всем возможным значениям состояния мира: (3.12) Это уравнение аналогично (2.17). Однако там переменная состояния мира была дискретной, и поэтому нормализация опиралась на сумму, а не на интеграл. Обыч- ный способ работы с нормировочной константой состоит в том, чтобы сначала най- ти числитель px|s(xobs | shyp)ps(shyp), а затем при необходимости нормировать в конце. Нет ничего плохого в явном написании . Однако этот множитель просто оста- нется там до конца вычислений. В качестве альтернативы можно было бы сохра- нить его и найти внутреннюю часть интеграла вместе с нахождением числителя.\n--- Страница 93 ---\n92  Байесовский вывод в условиях зашумленных измерений Однако это было бы громоздко, поскольку вам пришлось бы записывать одно и то же выражение дважды, один раз в числителе и один раз внутри интеграла в знаме- нателе. Выгода от работы со знаком пропорциональности заключается в том, что вы сначала вычисляете весь числитель, а затем, в конце, вычисляете знаменатель, подставляя окончательное выражение числителя в интеграл. Иногда это оконча- тельное вычисление интеграла несложно, потому что интеграл имеет стандартную известную форму. Таблица 3.2. Отношения между правдоподобием, нормированным правдоподобием, протопостериором и апостериорным распределением. Игнорирование априорного распределения равнозначно допущению того, что это распределение плоское Априорное распределение игнорируетсяАприорное распределение учитывается С нормированием Функция правдоподобия Протопостериор Без нормирования Нормированная функция правдоподобияАпостериорное распределение Практический пример. Вычислим апостериорную вероятность в нашем демонстрационном исследовании при предположениях, которые мы сделали в предыдущем разделе о распределении стимулов и измерений. Подставив выражения для �(shyp; xobs) и ps(shyp) в уравнение (3.10), мы видим, что для вычисления апостериорного распределения необходимо вычислить произ- ведение двух функций Гаусса. Умножение двух функций Гаусса на одну и ту же переменную с последующим нормированием является обычным явле - нием в байесовских моделях восприятия. Результатом выступает новое рас - пределение Гаусса (примечание 3.7 и задача 3.4). Примечание 3.7 Произведение двух гауссианов Рассмотрим произведение двух распределений Гаусса по одной и той же случайной величине y. Одно имеет среднее μ1 и дисперсию σ12, а другое имеет среднее μ2 и дис - персию σ22: (3.13) (3.14) Перемножим эти распределения так же, как обычные функции, и нормируем ре- зультат (поскольку произведение не нормируется автоматически). Полученное рас - пределение вероятностей является другим нормальным распределением, теперь со средним\n--- Страница 94 ---\nЭтап 2: вывод  93 (3.15) и дисперсией (3.16) Поскольку это нормальное распределение, оно должно получить стандартную нор- мирующую константу нормального распределения, а именно 1, деленную на квад- ратный корень из дисперсии, умноженной на 2π. Эти результаты получены в за- даче 3.4. Спасительное обозначение: выражения, возникающие в результате умножения гауссиан, включают в себя несколько множителей σ2, и они появляются либо во вло- женных дробях, либо в выражениях, которые трудно интерпретировать. Это может доставить множество неприятностей. К счастью, существует полезное и интуитив- но понятное упрощение обозначений. Необходимо переписать все выражения от - носительно точностных величин. Точность (precision) определяется как величина, обратная дисперсии: (3.17) с добавлением индексов по мере необходимости. В новых обозначениях произве- дение двух приведенных выше гауссианов имеет среднее значение, равное и точность, равную J1 + J2. Эти выражения существенно проще. Мы рекомендуем вам использовать точностную запись всякий раз, когда перемножаются распреде- ления Гаусса. Применительно к нашему примеру мы обнаруживаем, что апостериорное распределение представляет собой новое распределение Гаусса (3.18) где среднее апостериорного распределения (3.19) и его дисперсия (3.20)\n--- Страница 95 ---\n94  Байесовский вывод в условиях зашумленных измерений С этого момента мы упростим обозначения, введя переменные точности (обратной дисперсии) (см. примечание 3.7): (3.21) (3.22) (3.23) В этих обозначениях уравнение (3.19) можно записать в виде (3.24) и (3.20) как Jpost = J + Js. (3.25) Пример этого апостериорного распределения показан на рис. 3.5. Предполагаемый стимул shyp–10 –2 –8 0 6 –6 2 8 –4 4 10xobs sˆPMВероятность или правдоподобиеАприорное распределение ps(shyp)Апостериорное распределение ps|x(shyp|xobs) Правдоподобие �(shyp; xobs) = px|s(xobs|shyp) Рис. 3.5  Апостериорное распределение получается путем умножения априорного распре- деления на функцию правдоподобия и нормирования полученной функции (протопостериор). Предполагаемое значение стимула с наибольшей апостериорной вероятностью является апо- стериорной средней оценкой стимула наблюдателем s ˆPM 3.3.4. Апостериорное среднее Теперь мы более внимательно посмотрим на апостериорное среднее, как оно дается уравнением (3.19) или (3.24).\n--- Страница 96 ---\nЭтап 2: вывод  95 Выражение среднего имеет вид wxobs + (1 - w)μ, где w определяется как (3.26) Поскольку J и Js оба неотрицательны, w представляет собой число от 0 до 1. Другими словами, апостериорное среднее представляет собой взвешенное сред- нее наблюдаемого измерения xobs и априорного среднего μ (примечание 3.8). Это средневзвешенное значение всегда будет находиться где-то между xobs и μ. Упражнение 3.2. Докажите это утверждение математически. Примечание 3.8 Средневзвешенные значения Предположим, что студент сдает промежуточный и выпускной экзамены и получает оценки μ (за промежуточный экзамен) и x (за выпускной экзамен). Однако промежу - точный этап менее важен, чем финальный. Поэтому преподаватель взвешивает ито- говую экзаменационную оценку с коэффициентом w = 0.7, а промежуточную оценку с коэффициентом 1 - w = 0.3. Тогда окончательная оценка студента представляет собой средневзвешенное значение wx + (1 - w)μ. Она будет лежать между μ и x. Где именно находится μpost, определяется весами w и 1 - w, приложенными к наблюдению и априорному среднему соответственно. Веса представляют собой нормализованные версии точности функции правдоподобия и апри- орного распределения. Если дисперсия правдоподобия ниже, чем у априор- ного распределения, точность правдоподобия (т. е. надежность измерения) выше, чем у априорного распределения. Как следствие вес для xobs выше, чем для μ, в результате чего апостериорное среднее лежит ближе к xobs, чем к μ. Конечно, верно и обратное: если дисперсия правдоподобия больше, чем априорного распределения, то апостериорное среднее будет ближе к априор- ному среднему, чем к измерению. По правилу Байеса априорные вероятности и правдоподобие – это всего лишь две части информации, которые необходи- мо объединить. Каждая часть информации имеет влияние, соответствующее качеству информации. Упражнение 3.3. В частном случае, когда априорная вероятность и правдопо- добие имеют одинаковую дисперсию (σ = σs), вычислите среднее значение апостериорной вероятности. Смысл средневзвешенного значения в уравнении (3.19) состоит в том, что априорное значение «оттягивает апостериорное значение» от измерения к своему собственному среднему, но его способность оттягивать зависит от того, насколько оно узко по сравнению с функцией правдоподобия. Если функция правдоподобия узкая – что происходит при низком уровне шума, – тогда апостериорная вероятность не сильно сдвинется с места: она будет центрирована близко к среднему значению функции правдоподобия, то есть к измерению. Эта аналогия часто справедлива, даже если функция правдо- подобия и априорное распределение не являются гауссовыми.\n--- Страница 97 ---\n96  Байесовский вывод в условиях зашумленных измерений 3.3.5. Ширина апостериорного распределения До сих пор мы обсуждали только апостериорное среднее. Дисперсия апосте- риорной вероятности определяется уравнением (3.20). Квадратный корень из нее – стандартное отклонение – является мерой ширины апостериорного распределения. Она интерпретируется как общий уровень неуверенности на- блюдателя в отношении стимула после объединения измерения с априорным знанием. Ширина отличается как от дисперсии функции правдоподобия, так и от дисперсии априорного распределения. Упражнение 3.4 (a) Покажите, что дисперсию апостериорного распределения также можно записать в виде Отметим, что это не наш любимый способ записи, поскольку его труднее интерпретировать, чем уравнение (3.20) и, в част - ности, уравнение (3.25). (b) Покажите, что дисперсия апостериорного распределения меньше как дисперсии функции правдоподобия, так и дисперсии априорного рас - пределения. Тот факт, что дисперсия апостериорного распределения меньше, чем дис - персии правдоподобия и априорного распределения по отдельности, можно истолковать следующим образом: объединение измерения с априорным зна- нием делает наблюдателя более уверенным в отношении стимула, чем когда наблюдатель имеет только измерение или только априорное знание. Упражнение 3.5. Какова дисперсия апостериорной вероятности в частном слу - чае, когда σ = σs? Каковы среднее и дисперсия апостериорного распределения, когда отношение σ/σs очень велико или очень мало? Объясните свои выводы. 3.3.6. Оценка апостериорного среднего Байесовский наблюдатель использует апостериорное распределение для по- лучения оценки интересующего нас состояния мира, в данном случае стимула s. Байесовский наблюдатель делает это, минимизируя ожидаемое значение некоторой величины. Для непрерывных переменных, таких как s, обычно предполагается, что наблюдатель минимизирует ожидаемую квадратичную ошибку. Как мы увидим в главе 13, это эквивалентно выбору среднего зна- чения апостериорного распределения, поэтому данный процесс называется оценкой апостериорного среднего1. В общем случае оценка апостериорного среднего (posterior mean estimate, PME), обозначаемая s ˆPM, определяется как 1 Также называется оценкой по методу наименьших квадратов Байеса. Альтерна- тивный подход состоит в том, чтобы минимизировать ожидаемую абсолютную ошибку вместо ожидаемой квадратичной ошибки; это привело бы к получению апостериорной медианы (см. задачу 13.3). Для гауссовых апостериорных распреде- лений среднее значение и медиана совпадают.\n--- Страница 98 ---\nНеопределенность и уверенность  97 (3.27) или, другими словами, как ожидаемое значение s при апостериорном рас - пределении p(s | xobs). Обсуждение ожидаемых значений представлено в раз- деле B.6 приложения. В нашем примере объединения измерения с гауссовым априорным рас - пределением мы уже вычислили апостериорное среднее в уравнении (3.24). Отсюда мы имеем (3.28) Таким образом, PME представляет собой средневзвешенное значение наблюдаемых измерений xobs и априорного среднего μ, взвешенное по об- ратным дисперсиям функции правдоподобия и априорного распределения соответственно. 3.3.7. Оценка MAP Вспомним другую апостериорную оценку из главы 2: моду апостериорного распределения, также называемую оценкой MAP . Это значение shyp, при ко- тором апостериорная плотность вероятности максимальна: (3.29) Для гауссова распределения мода и среднее значение идентичны, поэтому sˆMAP = sˆPM. В целом оценка MAP имеет то преимущество, что она обобщается на дискретные (категориальные) переменные, как мы видели в главе 2. Од- нако в непрерывном случае она не всегда минимизирует ожидаемую квад- ратичную ошибку. 3.4. Неопределенность и уверенность Неопределенность и уверенность – обычные термины в повседневной жизни, но их можно конкретизировать в контексте байесовского моделирования. 3.4.1. Неопределенность Каждая функция убеждения (функция доверия, belief function) на этапе выво- да (этап 2) связана с понятием неопределенности: априорная неопределен- ность отражает качество знаний наблюдателя о состоянии мира до того, как он сделает какие-либо наблюдения, неопределенность правдоподобия или\n--- Страница 99 ---\n98  Байесовский вывод в условиях зашумленных измерений сенсорная неопределенность отражают качество знаний наблюдателя о со- стоянии мира, основанных исключительно на наблюдениях, а апостериорная неопределенность отражает качество знаний наблюдателя о состоянии мира после проведения наблюдений. Во всех случаях неопределенность связана с наблюдателем и, следователь- но, является субъективной величиной (примечание 3.4). Было бы некоррект - но говорить о неопределенности в контексте порождающей модели (этап 1). В частности, фраза «неопределенность в измерении» отражает неправильное понимание термина; правильным использованием будет «неопределенность состояния мира, основанная на измерении». Как же тогда вычисляется неопределенность из распределения вероят - ностей или функции правдоподобия по непрерывной переменной? Обще- принятого определения не существует, но интуитивно использование этого термина можно уловить, если мы определим неопределенность как стан- дартное отклонение распределения вероятности или (нормированную) функцию правдоподобия. Например, узкое апостериорное распределение p(s | x) означает низкую апостериорную неопределенность, а широкое апо- стериорное распределение означает высокую апостериорную неопределен- ность. Неопределенность можно определить как некую монотонно возрас - тающую функцию стандартного отклонения распределения убеждений или функцию правдоподобия по состоянию мира. В расчетах в этой книге мы просто используем само стандартное отклонение. В качестве альтернативы мы могли бы определить неопределенность как межквартильный размах распределения или как другую метрику, основанную на квантилях распре- деления. Следует отметить, что ни определения, основанные на стандартном отклонении, ни определения на основе квантилей не могут быть обобщены на произвольные дискретные (категориальные) распределения. Упражнение 3.6. Почему они не могут быть обобщены? В нашем примере все распределения являются гауссовыми, и если неопре- деленность представляет собой стандартное отклонение, ее значения равны: априорная неопределенность: σs; неопределенность правдоподобия или сенсорная неопределенность: σ; апостериорная неопределенность: что также можно за - писать как Мы видим, что неопределенность правдоподобия имеет то же числовое значение, что и уровень сенсорного шума, но в более сложных примерах это не обязательно так. В нейронных порождающих моделях, таких как рассмот - ренные в главе 14, уровень сенсорного шума даже определяется в совершен- но другом пространстве, чем пространство измерений. Для гауссовых априорных распределений и правдоподобий апостериор- ная неопределенность всегда меньше как априорной неопределенности, так и неопределенности правдоподобия (упражнение 3.4b). Определение распространяется на негауссовы распределения (рис. 3.6). Однако для не- гауссовых априорных распределений и/или правдоподобий апостериорная\n--- Страница 100 ---\nНеопределенность и уверенность  99 неопределенность не обязательно всегда меньше как априорной неопреде- ленности, так и неопределенности правдоподобия (см. задачу 3.10). НеопределенностьНеопределенностьНеопределенность Предполагаемый стимул Предполагаемый стимул Предполагаемый стимул Рис. 3.6  Неопределенность, определяемая как стандартное отклонение случайной вели- чины. Каждое из показанных распределений может быть априорным распределением, (нор- мированным) правдоподобием или апостериорным распределением. Двойная стрелка имеет длину, равную двум стандартным отклонениям. (B) и (C) показывают бимодальные (двухвер- шинные) распределения. Неопределенность в (C) выше исключительно из­за большего раз- деления между пиками Примечание 3.9 Т ерминология: шум, неопределенность, изменчивость В этой книге термин «шум» зарезервирован для процесса, посредством которого генерируются наблюдения, то есть он описывает изменчивость наблюдений или измерений от испытания к испытанию. Таким образом, шум является частью по- рождающей модели. «Неопределенность», с другой стороны, отражает наличие или отсутствие знаний наблюдателя в отношении переменных в мире. Ширина апосте- риорного распределения является мерой неопределенности, а не шума. Неопреде- ленность является частью процесса вывода и субъективна в смысле примечания 3.4. Шум является одной из возможных причин неопределенности, но не единствен- ной. Например, когда объект частично закрыт и нет прямой информации о части объекта, находящейся за мешающим предметом, наблюдатель имеет неопределен- ность без шума. Изменчивость – это общий термин для всего, что меняется от испытания к испы- танию. Шум является формой изменчивости и может быть назван изменчивостью измерения. Оценка стимула также варьируется от испытания к испытанию. Это можно назвать «поведенческой изменчивостью». Неопределенность не является формой изменчивости. 3.4.2. Байесовская уверенность В повседневной жизни решения принимаются с большей или меньшей уве- ренностью. Вы можете быть уверены, что успеете перейти дорогу до того, как вас настигнет машина, что к вам идет ваш друг или что у кого-то итальян- ский акцент. Уверенность естественным образом вписывается в байесовскую структуру и связана с апостериорным распределением.\n--- Страница 101 ---\n100  Байесовский вывод в условиях зашумленных измерений При этом существуют разные подходы к определению байесовской уверен- ности. Один из них, который называется основанной на оценке байесовской достоверностью, опирается на оценку sˆ, которую наблюдатель делает для стимула. Тогда достоверность можно определить как некую монотонную функцию F апостериорной плотности вероятности, вычисляемую при оце- нивании: байесовская уверенность º F(ps|x(sˆ|xobs)). (3.30) «Монотонная функция» здесь означает то же самое, что и в нашем опре- делении неопределенности в разделе 3.4.1. Байесовская уверенность, осно- ванная на оценках, в равной степени применима как к непрерывным, так и к дискретным переменным. Второй подход, который мы называем основанной на интервалах байе- совской достоверностью1, имеет смысл только для непрерывных отчетов о состоянии мира. В этом подходе наблюдатель сообщает некоторую меру дисперсии апостериорного распределения либо в виде интервала (началь- ная точка и конечная точка), либо в виде размера такого интервала. Меры апостериорной неопределенности, как определено в предыдущем под- разделе, такие как апостериорное стандартное отклонение или межквар- тильный диапазон апостериорной вероятности, попадают в эту категорию. В качестве альтернативы экспериментатор может побудить испытуемого к установлению осмысленного доверительного интервала, вознаграждая его, если истинное мировое состояние находится в пределах этого интервала, но давая меньше вознаграждения за большие интервалы. Затем байесовский наблюдатель установит интервал, который максимизирует ожидаемое воз- награждение. Более того, сама оценка может быть включена в этот процесс оптимизации, а не задаваться заранее самостоятельно (как, например, PME). Мы обсудим побужденные байесовские доверительные интервалы в главе 13. Теперь применим уравнение (3.30) для байесовской достоверности, ос - нованной на оценках, с функцией тождества F, к апостериорному распреде- лению Гаусса в сочетании с PME. Тогда байесовская достоверность является максимумом гауссовой функции плотности вероятности, которая равна нор- мировочному коэффициенту или Мы продолжим обсуждение этого вопроса в задаче 3.8. Здесь мы рассмотрели байесовские определения уверенности. Ответ на вопрос, является ли какая-либо форма байесовской уверенности хорошей моделью для оценок человеческой уверенности, нуждается в эмпирической проверке. Многочисленные данные указывают на то, что рейтинги доверия людей не просто следуют уравнению (3.30). Когда речь идет о доверительном интервале, а не о рейтинге достоверности, в литературе подчеркивается, что такие интервалы в основном следуют предсказаниям байесовских моделей; однако небайесовские модели для интервалов широко не тестировались. 1 Чтобы отличить байесовские доверительные интервалы от частотных, в байесов- ских текстах обычно используется термин доверительный интервал. В этой книге мы не используем этот термин.\n--- Страница 102 ---\nНесоответствие модели в выводе  101 3.5. Несоответствие модели в выводе До сих пор мы обсуждали только оптимальный байесовский вывод. Однако это обсуждение основывалось на предположении, что наблюдатель обладает полным и правильным знанием порождающей модели (этап 1) и полностью использует эти знания во время вывода (этап 2). Возможен сценарий, когда наблюдатель использует другую, «предполагаемую» порождающую модель для выполнения вывода. Эта ситуация называется несоответствием модели и может быть обусловлена множеством причин, например: наблюдатель еще не завершил обуче ние порождающей модели; порождающая модель слишком сложна для изучения, и наблюдатель приближается к ней постепенно; порождающая модель в данном эксперименте отличается от модели в естественной среде, и наблюдатель использует последнюю для вы- вода; наблюдатель придерживается неправильных представлений о порож - дающей модели, например когда субъект не проинструктирован и не обучен распределению задач. Байесовские наблюдатели оптимальны, когда они обладают полными зна- ниями обо всех распределениях в порождающей модели и правильно их применяют, но наблюдатель с несовпадением правдоподобия или априорной вероятности во многих случаях будет субоптимальным. В нашем примере это означало бы наличие более высокой ожидаемой квадратичной ошибки, чем у оптимального наблюдателя. Мы вернемся к этому в главе 4. 3.5.1. Априорное несоответствие Частным случаем несоответствия модели является априорное несоответ - ствие. Априорное распределение ps(s) можно рассматривать как убеждение наблюдателя в том, что стимул был s еще до того, как наблюдатель получил какую-либо сенсорную информацию. В контексте психофизического экспе- римента нельзя слепо предполагать, что испытуемые изучают распределение стимулов и используют его в качестве априорного знания. В общем случае априорное знание наблюдателя может отличаться от распределения состоя- ний мира. Субъекты могут использовать априорную информацию, которую они привносят в эксперимент, например основанную на распределении со- стояний в естественном мире. Как известно, испытуемые обычно априорно убеждены, что свет падает сверху. Такие «естественные» априорные предпо- ложения, приобретенные на протяжении сенсорного опыта длиной в жизнь, может быть трудно изменить в течение относительно короткой продолжи- тельности эксперимента. Для преодоления естественного априорного убеж - дения может потребоваться обширная подготовка. Конечно, испытуемые могут использовать априорное промежуточное звено между «естественным распределением состояний мира» и «экспериментальным распределением состояний мира». Априорные убеждения также могут меняться со временем,\n--- Страница 103 ---\n102  Байесовский вывод в условиях зашумленных измерений поскольку наблюдатель подвергается большему количеству стимулов во вре- мя эксперимента. При априорном несоответствии наблюдатель будет вычислять PME, ис - пользуя порождающую модель, отличающуюся от правильной: qs|x(shyp | xobs) µ qs(shyp)px|s(xobs | shyp). (3.31) Например, если предполагаемое распределение стимулов qs(s) имеет сред- нее значение μassumed и дисперсию σ2 s,assumed , то PME при наличии измерения xobs составляет (3.32) Следовательно, этот наблюдатель будет давать в каждом испытании от - веты, отличающиеся от оптимального байесовского наблюдателя, и общая точность будет хуже. Необходимо подчеркнуть, что понятие оптимальности обычно основыва- ется на порождающей модели, связанной с экспериментальной статистикой. Использование порождающей модели, основанной на естественной статис - тике, было бы оптимальным в другом смысле, но на практике порождающая модель, связанная с естественной статистикой, редко известна исследовате- лям (заметные исключения см. в [57, 63]). 3.5.2. Неудовлетворительные априорные распределения Особым случаем априорного несоответствия является сценарий, когда на- блюдатель использует постоянное или плоское априорное распределение q(s) = константа. Сразу же возникает вопрос, какое значение принимает эта константа. Если бы s было ограничено интервалом, скажем [a , b], ответ был бы очевиден: поскольку площадь под априорным распределением равна 1 (рис. 3.7). Однако в этой главе областью определения s является вся числовая прямая. Равномерное распределение не может быть определено должным образом на всей числовой прямой, поскольку она имеет бесконеч- ную длину и равномерное распределение вероятностей будет иметь значе- ние 0 в каждой точке. Конечно, в любой реальной задаче s не может быть сколь угодно большим в любом направлении. Поэтому было бы разумно ограничить его область каким-то большим значением. Однако выбор этого значения, а вместе с ним и значения априорной константы, был бы произ- вольным. К счастью, эту головоломку не нужно решать, так как оказывается, что в выводе значение априорной константы не играет роли. А именно если это значение равно c , то апостериорное распределение равно ps|x(shyp | xobs) µ cpx|s(xobs | shyp) µ px|s(xobs | shyp). (3.33)\n--- Страница 104 ---\nАмплитудные переменные  103 Другими словами, постоянное априорное значение поглощается констан- той пропорциональности. После нормирования апостериорного распреде- ления c выпадает. Мы можем предположить, что ps(s) = c на всей числовой прямой, и апостериорное значение по-прежнему будет хорошо определено, несмотря на то что априорное распределение не нормировано. Ненорми- руемое априорное распределение (то есть такое, чей интеграл бесконечен, а не равен 1) называется неудовлетворительным априорным распределением (improper prior). 0.5 0.4 0.3 0.2 0.1 0.0 –6 –4 –2 0 2 4 6Вероятность Рис. 3.7  Различные равномерные априорные распределения на ограниченных интервалах 3.6. Гетероскедастичность В этой книге мы сосредоточимся на случае, когда уровень шума измерения σ в уравнении (3.4) не зависит от стимула. Однако на практике часто зави- симость существует. Например, шум при зрительном измерении положения увеличивается с эксцентриситетом сетчатки, а шум при слуховом измерении положения увеличивается при изменении угла от прямого направления. Ког - да дисперсия случайной величины зависит от среднего значения распреде- ления, мы наблюдаем пример гетероскедастичности. Гетероскедастичность не мешает нам сформулировать байесовскую модель. Однако она приводит к тому, что функции правдоподобия не являются гауссовыми, даже если рас - пределение измерений является гауссовым (задача 3.7). 3.7. Амплитудные переменные До сих пор мы предполагали, что область действия стимула – это вся ре- альная линия от -¥ до ¥. Есть много переменных состояния мира, кото- рые охватывают определенные области. В приложении B.7.6 мы обсуждаем\n--- Страница 105 ---\n104  Байесовский вывод в условиях зашумленных измерений периодические переменные, такие как углы. Здесь же мы рассматриваем переменные, которые принимают только положительные значения и, таким образом, имеют диапазон от 0 до ¥. Сюда относятся длина линии, глубина, вес, скорость, громкость, временная продолжительность и интенсивность света. Переменные, которые никогда не принимают отрицательных значе- ний, можно назвать амплитудными переменными (magnitude variable). Один из типов распределения вероятностей, предназначенных для амплитудных переменных, – это логарифмическое нормальное распределение (lognormal dist ribution). Поскольку область определения s равна [0, ¥), областью опре- деления логарифма s является вся числовая прямая1. Следовательно, можно определить распределение Гаусса для log s: (3.34) Приводя его к исходной переменной s , мы получаем (3.35) Обратите внимание на множитель 1/s, называемый якобианом. Прочтите раздел B.12.1 приложения, если вы не знаете, как преобразовывать распре- деления вероятностей. Уравнение (3.35) представляет собой логнормальное распределение с параметрами μ и σs2, также записываемое как Lognormal(s ; μ, σs2). Примеры показаны на рис. 3.8. Важно отметить, что параметры μ и σs2 не соответствуют непосредственно среднему значению и дисперсии. Сред- нее значение логнормально распределенной переменной s равно а ее дисперсия представляет собой довольно сложное выражение, которое мы не будем использовать. Медиана s при логнормальном распределении равна eμ, а мода равна eμ-σs2. Логнормальное распределение также может быть использовано как рас - пределение измерений: (3.36) Такое применение имеет особый смысл, поскольку важным свойством логнормального распределения является тот факт, что стандартное откло- нение пропорционально среднему значению. Оказывается, это эмпирически правильное описание человеческих суждений о величине – соотношение, называемое законом Вебера–Фехнера . Например, уровень шума при изме - рении наблюдателем длины линии пропорционален самой длине: отличить 1 Когда мы в этой книге говорим «логарифм», мы всегда имеем в виду натуральный логарифм (по основанию e ).\n--- Страница 106 ---\nПрименение байесовских моделей  105 расстояние 1,02 м от расстояния 1 м примерно так же сложно, как отличить 10,2 см от 10 см. Мы будем широко использовать логнормальные распреде- ления в главе 9. 3.8. Применение байесовских моделей Байесовская модель, описанная в этой главе, применялась к широкому кругу сценариев в областях зрительного восприятия, соматосенсорного восприя- тия, слухового восприятия и двигательного контроля. Здесь мы упомянем лишь некоторые исследования из обширной литературы по этой теме. При зрительном восприятии низкоконтрастные стимулы (например, се- рый объект, движущийся на фоне другого оттенка серого) кажутся движущи- мися медленнее, чем высококонтрастные стимулы (например, белый объект, движущийся на том же сером фоне), хотя в действительности они движутся с одинаковой скоростью [173]. Вайс и его коллеги объясняют эту и многие другие загадочные зрительные иллюзии тем, что мозг использует априор- ное распределение по скорости, которое выше для более низких скоростей (так называемое низкоскоростное априорное распределение) [199]. Объект с меньшей контрастностью предоставляет менее достоверную информацию, а значит, функция правдоподобия шире. Это приводит к тому, что апостери- орная оценка смещается больше к среднему значению априорной оценки. Если пик априорного распределения расположен в 0, то воспринимаемая скорость будет смещена в сторону более низкого значения, когда контраст - ность ниже. Впоследствии байесовская модель визуального восприятия дви- жения была усовершенствована путем оценки формы априорного распреде- ления [168]. В области соматосенсорного восприятия множество тактильных простран- ственно-временных иллюзий можно объяснить с помощью модели, опи- санной в этой главе, опять же с априорным ожиданием движения с низкой скоростью [65]. Предложенная модель обеспечивает единое объяснение не- скольких поразительных тактильных иллюзий, при которых постукивания, нанесенные в быстрой последовательности в разные точки на поверхности кожи, воспринимаются как происходящие ближе друг к другу, чем на самом деле. Примечательно, что воспринимаемое расстояние между постукивания- ми сокращается по мере уменьшения времени между постукиваниями, яв- ление, известное как сокращение воспринимаемой длины [65, 66]. Особенно интересен подтвержденный прогноз о том, что перцептивное сокращение длины более выражено для более слабых постукиваний, которые вызывают более широкие функции правдоподобия [178]. В области слухового восприятия байесовская модель объясняет наблюде- ние, что совы могут удивительно точно обрабатывать акустические стимулы, чтобы локализовать источник звука, который возникает в области прямо перед ними, но при этом они постоянно недооценивают положение источ- ников звука, которые возникают в эксцентрических областях [52]. Модель предполагает, что совы используют априорную центральность, то есть они\n--- Страница 107 ---\n106  Байесовский вывод в условиях зашумленных измерений ожидают, что источники звука, как правило, чаще встречаются в централь- ных областях. Байесовские модели применялись не только к пространственному, но и к временному восприятию. Например, модель Гольдрайха [65] дает объ- яснение иллюзии восприятия времени, при которой воспринимаемое вре- мя для пространственно разделенных стимулов увеличивается по мере увеличения расстояния между стимулами. Это явление замедления време- ни (также известное как каппа-эффект) происходит как в тактильной, так и в зрительной областях [36]. Используя визуальные стимулы, Джазайери и Шадлен [84] показали, что при предъявлении стимулов, длительность которых взята из разных распределений, полученные участниками оценки длительности согласуются с оценками байесовской модели наблюдателя, которая включает распределения длительности как априорные распреде- ления вероятностей. В области движения Кординг и Вольперт [101] изучают движения паль - цев, тянущихся к зрительной цели. В виртуальной реальности целевое по- ложение смещается вбок на расстояние, случайно взятое из распределения Гаусса со средним смещением 1 см. Оно действует как предварительное рас - пределение. Наблюдателей обучают до тех пор, пока они не изучат это рас - пределение. Движение пальца испытуемому не видно; шумное визуальное измерение положения пальца обеспечивается отображением облака точек на экране в середине движения. Важно отметить, что надежность визуального измерения зависит от каждого испытания. Было обнаружено, что наблюда- тели комбинируют зашумленное измерение с полученным знанием об апри- орном распределении способом, описываемым уравнением (3.28). Во втором эксперименте гауссово априорное распределение заменили бимодальным (двухвершинным) распределением; даже это априорное распределение было использовано испытуемыми в хорошем приближении, хотя это требовало обширной подготовки. В естественной статистике была обнаружена бимо- дальная априорная ориентация [63]; наблюдатели приняли это во внимание в задачах оценки ориентации. 3.9. Восприятие Когда мы моделируем восприятие в этой книге, мы в первую очередь модели- руем решения, которые люди принимают в задачах восприятия. Байесовские модели дают прогнозы для этих решений. Например, оценка апостериорного среднего – это то, что должен сообщать байесовский наблюдатель, чтобы минимизировать квадратичную ошибку в задаче непрерывной оценки. Этот пример можно сопроводить и утверждением с другим философским смыс - лом, согласно которому оценка должна заключаться в том, что восприни- мается, то есть в том, что наблюдатель испытывает (видит, слышит и т. д.). С нашей точки зрения, байесовские модели также обеспечивают достоверные описания феноменологических восприятий. Иллюзии восприятия – это об- ласть, в которой убедительные восприятия возникают без подсказки, и байе-\n--- Страница 108 ---\nЗаключение  107 совские модели часто дают правдоподобные качественные объяснения этих восприятий, как в случае с иллюзией впалого лица (задача 1.10). Для нас не совсем ясно, проводить ли границу между перцептивными решениями и восприятиями, и если проводить, то где. Одним из отличитель- ных критериев может быть необходимость дополнительного вопроса. Когда вам показывают анаморфотную иллюзию, вас не нужно спрашивать: «Похож ли этот плоский рисунок на трехмерную сцену?» – вы просто воспринимаете рисунок таким образом. Однако вполне возможно, что разум неявно задает себе подобные вопросы, например: «Каковы идентичность и расположение объектов на сцене?» Это размыло бы различие, но было бы интересно само по себе. Различие между перцептивными решениями и восприятиями приобре- тает дополнительное измерение, когда принимается во внимание неопре- деленность. Байесовские модели в их сильной форме предсказывают, что разум вычисляет апостериорное распределение для каждого интересующего состояния мира. В восприятии это означает, что результат перцептивных вычислений включает как минимум ощущение неопределенности или уве- ренности в состоянии мира и, самое большее, полное распределение вероят - ностей. Утверждалось, что перцепция не сопровождается ни тем, ни другим [20], хотя могут быть перцептивные решения. Мы полагаем, что этот научный вопрос еще не закрыт (см. также [43, 128]) и что ответить на него эмпириче- ски может быть очень сложно. 3.10. Заключение Мы рассмотрели вывод непрерывной переменной на основе зашумленного сенсорного наблюдения. Сенсорный шум имеет множество источников. Мы определили измерение стимула как абстракцию зашумленного внутреннего представления этого стимула. Измерение обитает в том же пространстве, что и сам стимул. Мы определили первые два этапа построения байесовской модели при наличии шума измерений: порождающую модель и процесс логиче- ского вывода. Порождающая модель состояла из распределения стимулов (предпола- гаемого гауссовым для удобства) и распределения измерения, обуслов- ленного стимулами (предполагаемого гауссовым, как это подтвержда- ется центральной предельной теоремой). Во время вывода наблюдатель использует распределения стимулов и измерений из порождающей модели для формирования априорного распределения и функции правдоподобия соответственно. Апостериорное распределение – это нормированное произведение априорного распределения и правдоподобия. Если распределения априорной вероятности и правдоподобия являются гауссовыми, то апостериорное распределение также является гауссовым.\n--- Страница 109 ---\n108  Байесовский вывод в условиях зашумленных измерений При тех же предположениях апостериорное среднее представляет со- бой взвешенное среднее между наблюдаемым измерением и априор- ным средним. Кроме того, апостериорное распределение более узкое, чем априорное распределение и правдоподобие, что отражает сниже- ние неопределенности. В общем случае, если целью является минимизация ожидаемой квад- ратичной ошибки, байесовский наблюдатель оценивает стимул как апостериорное среднее. Достоверность можно определить как значение апостериорного рас - пределения при апостериорном среднем. Наблюдатель может быть байесовским, но использовать «неправиль- ную» порождающую модель. Это называется несоответствием моде- ли. Несоответствие модели может привести к неоптимальному по- ведению. 3.11. Рекомендуемая литература Wendy J. Adams, Erich W. Graf, and Marc O. Ernst. Experience Can Change the ‘Light ­from­above’ Prior. Nature Neuroscience 7, no. 10 (2004): 1057–1058. Brian J. Fischer and José Luis Peña. Owl’s Behavior and Neural Representation Predicted by Bayesian Inference. Nature Neuroscience 14, no. 8 (2011): 1061– 1066. Daniel Goldreich and Jonathan Tong. Prediction, Postdiction, and Perceptual Length Contraction: A Bayesian Low­Speed Prior Captures the Cutaneous Rabbit and Related Illusions. Frontiers in Psychology 4 (2013): 221. Mehrdad Jazayeri and Michael N. Shadlen. Temporal Context Calibrates Interval Timing. Nature Neuroscience 13, no. 8 (2010): 1020–1026. Konrad P . Kording and Daniel M. Wolpert. Bayesian Integration in Sensorimotor Learning. Nature 427, no. 6971 (2004): 244–247. Alan A. Stocker and Eero P . Simoncelli. Noise Characteristics and Prior Expec­ tations in Human Visual Speed Perception. Nature Neuroscience 9, no. 4 (2006): 578–585. Xue-Xin Wei and Alan A. Stocker. A Bayesian Observer Model Constrained by Efficient Coding Can Explain ‘ Anti­Bayesian’ Percepts. Nature Neuroscience 18, no. 10 (2015): 1509–1517. Yair Weiss, Eero P . Simoncelli, and Edward H. Adelson. Motion Illusions as Optimal Percepts. Nature Neuroscience 5, no. 6 (2002): 598–604. 3.12. Задачи Задача 3.1. Пусть s – наблюдаемый стимул, x – измерение, ps(s) – распреде- ление стимула и px|s(x | s) – распределение измерения.\n--- Страница 110 ---\nЗадачи  109 (a) Запишите апостериорное распределение по предполагаемому стимулу s с учетом наблюдаемого измерения xobs. (b) Какой из членов вашего выражения называется функцией правдоподо- бия? (c) В чем разница между функцией правдоподобия и распределением из- мерений? Задача 3.2. В этой задаче мы численно находим апостериорное распределе- ние. Предположим, что распределение стимула ps(s) является гауссовым со средним значением 20 и стандартным отклонением 4. Распределение изме- рений px|s(x | s) является гауссовым со стандартным отклонением σ = 5. Байе- совский наблюдатель делает вывод s из наблюдаемого измерения xobs = 30. Далее мы найдем апостериорную плотность вероятности с помощью чис - ленных методов. (a) Определите вектор предполагаемых значений стимула s: (0, 0.2, 0.4, …, 40). (b) Вычислите функцию правдоподобия и априорное распределение этого вектора значений s . (c) Выполните векторное (точечное) умножение правдоподобия и априор- ной вероятности. (d) Разделите это произведение на значение суммы по всем s (этап норми- ровки). (e) Преобразуйте эту функцию массы апостериорной вероятности в функ - цию плотности вероятности, разделив ее на размер шага, который вы использовали в своем векторе значений s (например, 0.2). (f) Постройте правдоподобие, априорное и апостериорное распределение на одном и том же графике. (g) Является ли апостериорное распределение шире или уже, чем правдо- подобие и априорное распределение? Оправдались ли ваши ожидания, основанные на уравнениях, которые мы обсуждали? (h) Измените стандартное отклонение распределения измерений на боль- шое значение. Что происходит с апостериорным распределением? Как вы можете это объяснить? (i) Измените стандартное отклонение распределения измерений на не- большое значение. Что происходит с апостериорным распределением? Как вы можете это объяснить? Задача 3.3. Расширим предыдущую задачу, изменив распределение сти - мулов. Начните с условия из предыдущей задачи. Предположим, что рас - пределение стимулов ps(s) равномерно на интервале [- 15, 25] и равно 0 вне этого интервала. Распределение измерений px|s(x | s) является гауссовым со стандартным отклонением σ = 5. Байесовский наблюдатель делает вывод s из наблюдаемого измерения xobs = 30. Снова численно найдите апостериорную плотность вероятности. (a) Каково значение p (s) на интервале [- 15, 25]? (b) Повторите задания (a)–(f) из предыдущей задачи для этого нового рас - пределения стимулов.\n--- Страница 111 ---\n110  Байесовский вывод в условиях зашумленных измерений (c) Является ли апостериорное распределение гауссовым? (d) Численно найдите среднее значение апостериорного распределения. (e) Численно найдите дисперсию апостериорной вероятности. Задача 3.4. Рассмотрим более подробно основное вычисление вывода в этой главе, в котором используется умножение двух гауссовых распределений. Распределение стимулов ps(s) является гауссовым со средним значением μ и дисперсией σs2. Распределение измерений px|s(x|s) является гауссовым со средним значением s и дисперсией σ2. (a) Запишите уравнения для px|s(x | s) и ps(s). (b) Байесовский наблюдатель делает вывод s из измерения xobs. Используйте правило Байеса, чтобы записать уравнение для апостериорной вероят - ности ps|x(shyp | xobs). Подставьте выражения для px|s(xobs | shyp) и ps(shyp), но пока не упрощайте результат. Числитель является произведением двух гауссианов. Как мы обсуждали в разделе 3.3.3, знаменатель, px(xobs), является нормировочным коэффи- циентом, который гарантирует, что интеграл равен 1. Сейчас мы про- игнорируем его и сосредоточимся на числителе. (c) Примените правило eAeB = eA+B, чтобы упростить числитель. (d) Разложите два квадратичных члена в показателе степени. (e) Перепишите показатель степени в виде as2 + bs + c с константами a, b и c. Важно отметить, что, поскольку c просто приводит к постоянному масштабированию, нет необходимости его вычислять. (f) Перепишите выражение, полученное в (e), в более простой форме ec1(s+c2)2+c3 с константами c1, c2 и c3. Подсказка: любая квадратичная функ - ция вида as2 + bs + c может быть записана как эта фор- ма называется дополнением до полного квадрата. (g) Теперь перепишите ваше выражение в форме Выразите μcombined и σcombined через x , σ, μ и σs. (h) Напомним, что ps|x является распределением вероятностей и поэтому его интеграл должен быть равен 1. Однако выражение, которое вы полу - чили в (g), не нормировано должным образом, поскольку мы проигно- рировали px(xobs). Измените выражение таким образом, чтобы оно было правильно нормировано, но без явного вычисления px(xobs) (подсказка: зависит ли eZ от s?). Многие задачи байесовского вывода включают произведение двух или более гауссовых распределений, так что этот вывод пригодится в буду - щем. Задача 3.5. На рисунке ниже показаны функция правдоподобия и апостери- орное распределение. Они являются гауссовыми с σposterior = 1.2 и σlikelihood = 1.5. Предположим, что априорное распределение также является гауссовым. Какие из следующих утверждений верны? Обоснуйте свое мнение. Вам по- надобятся и график, и числа в задании.\n--- Страница 112 ---\nЗадачи  111Вероятность sПравдоподобие Апостериорное распределение (a) Априорное распределение центрировано слева от функции правдоподо- бия и ýже ее. (b) Априорное распределение центрировано слева от функции правдоподо- бия и шире ее. (c) Априорное распределение центрировано справа от функции правдопо- добия и ýже ее. (d) Априорное распределение центрировано справа от функции правдопо- добия и шире ее. Задача 3.6. Помимо гауссовых, есть и другие распределения, для которых мы можем вычислить апостериорные вероятности аналитически. Рассмот - рим распределение стимулов ps(s), равное 0 при s < 0 и экспоненциальному распределению ps(s) = λe-λs с λ > 0 при s ³ 0. Распределение измерений px|s представляет собой обычное гауссово распределение со средним значением s и дисперсией σ2. Байесовский наблюдатель делает вывод s из измерения xobs. (a) Выведите уравнение для оценки апостериорного среднего. (b) Предположим, что λ = 1 и xobs = 1. Постройте априорное распределение, нормированную функцию правдоподобия и апостериорное распределе- ние на одном графике. Задача 3.7. В этой главе мы предполагали, что σ не зависит от стимула s. Это предположение часто нарушается в реальных задачах, что приводит к гете- роскедастичности. Предположим, что распределение измерения имеет вид (3.37) где σ (s) – следующая функция s : σ(s) = 1 + s2. (a) Для s = 0, 1, 2 постройте распределение измерений (три кривые на одном графике, выделенные цветом). Все три должны выглядеть как гауссов «колокол». (b) Для xobs = 0, 1, 2 постройте функцию правдоподобия для гипотетическо- го s (три кривые на одном графике, выделенные цветом). Ни одна из них не должна выглядеть как гауссов «колокол».\n--- Страница 113 ---\n112  Байесовский вывод в условиях зашумленных измерений (c) Объясните, как такое возможно, что все распределения измерений явля- ются гауссовыми, а правдоподобия – нет. (d) Если бы априорное распределение было гауссовым, было бы таковым апостериорное распределение? Объясните свой ответ аналитически. Задача 3.8. Эта задача основывается на разделе 3.4.2. (a) Покажите, что в нашем примере достоверность на основе байесовской оценки, связанная с PME, равна (b) Почему столь же обоснованным определением байесовской уверенности в этом случае будет (c) Объясните для обоих выражений, почему зависимость от σ и σs имеет смысл. Задача 3.9. В этой задаче объединены понятия из разделов 3.4.2 и 3.5. Когда наблюдатель использует неверное априорное распределение для вычисле- ния апостериорного (несоответствие априорного распределения), может ли достоверность решения быть выше, чем когда он использует правильное априорное распределение? Если да, то приведите пример. Если нет, докажите математически, что это невозможно. Задача 3.10. В разделе 3.4.1 мы отметили, что для гауссовых априорных распределений и правдоподобий апостериорная неопределенность, опреде- ляемая как стандартное отклонение, всегда меньше как априорной неопре- деленности, так и неопределенности правдоподобия. Предложите пример, включающий негауссовы априорные вероятности или вероятности, в кото- рых это свойство больше не выполняется. Задача 3.11. Некоторые переменные стимулов являются периодическими (или циклическими) и принимают значения между (к примеру) 0 и 2π . При- мерами являются ориентация, направление движения и время суток. В раз- деле приложения B.7.6 мы обсуждаем распределение фон Мизеса, которое подходит для таких переменных. Предположим, что направление движения соответствует распределению фон Мизеса с круговым средним μ и парамет - ром концентрации κs (3.38) Далее предположим, что распределение измерения также является распре- делением фон Мизеса с круговым средним s и параметром концентрации κ: (3.39) (a) Байесовский наблюдатель делает вывод s из измерения xobs. Покажите, что апостериорное распределение по s является распределением фон Мизеса с (круговым) средним μpost, заданным формулой\n--- Страница 114 ---\nЗадачи  113 cos μpost = κs cos μ + κ cos xobs; (3.40) sin μpost = κs sin μ + κ sin xobs; (3.41) а параметр концентрации описывается выражением (b) Сравните со случаем гауссова распределения и укажите на отличия.",
      "debug": {
        "start_page": 79,
        "end_page": 114
      }
    },
    {
      "name": "Глава 4. Распределение отклика 114",
      "content": "--- Страница 115 --- (продолжение)\nГлава 4 Распределение отклика Как байесовская модель предсказывает реакцию человека­наблюдателя на задачу восприятия? В главе 3 мы смоделировали, как сочетаются априорные вероятности и прав- доподобия; теперь мы задаемся вопросом, как связать такую модель с по- ведением. В психофизических экспериментах исследователь предоставляет стимулы и измеряет реакцию участника. Наша цель состоит в том, чтобы предсказать вероятность каждого возможного ответа на стимулы. Пока что наша модель (этап 2) предсказывает оценку состояния мира наблюдателем с учетом сенсорных наблюдений, но не с учетом стимулов. Пора добавить третий этап моделирования, чтобы восполнить этот пробел. Краткое содержание главы Мы обсудим, почему при повторных воздействиях одного и того же стиму - ла апостериорная средняя оценка байесовского наблюдателя (реакция на- блюдателя) является случайной величиной, а затем получим распределение вероятностей откликов байесовского наблюдателя. В этом заключается тре- тий этап байесовского моделирования. Распределение откликов позволяет исследователям сравнивать предсказания байесовской модели с реальным поведением наблюдателя в контексте психофизического эксперимента. Мы обсуждаем смещение и дисперсию PME и сравниваем их с таковыми для MLE. Наконец, возвращаемся к теме оптимальности. 4.1. Унаследованная изменчивость Экспериментатор может только управлять стимулом. Чтобы сравнить нашу байесовскую модель с поведением наблюдателя в психофизической задаче, нам нужно знать, каковы предсказания байесовской модели относительно\nГлава 4 Распределение отклика Как байесовская модель предсказывает реакцию человека­наблюдателя на задачу восприятия? В главе 3 мы смоделировали, как сочетаются априорные вероятности и прав- доподобия; теперь мы задаемся вопросом, как связать такую модель с по- ведением. В психофизических экспериментах исследователь предоставляет стимулы и измеряет реакцию участника. Наша цель состоит в том, чтобы предсказать вероятность каждого возможного ответа на стимулы. Пока что наша модель (этап 2) предсказывает оценку состояния мира наблюдателем с учетом сенсорных наблюдений, но не с учетом стимулов. Пора добавить третий этап моделирования, чтобы восполнить этот пробел. Краткое содержание главы Мы обсудим, почему при повторных воздействиях одного и того же стиму - ла апостериорная средняя оценка байесовского наблюдателя (реакция на- блюдателя) является случайной величиной, а затем получим распределение вероятностей откликов байесовского наблюдателя. В этом заключается тре- тий этап байесовского моделирования. Распределение откликов позволяет исследователям сравнивать предсказания байесовской модели с реальным поведением наблюдателя в контексте психофизического эксперимента. Мы обсуждаем смещение и дисперсию PME и сравниваем их с таковыми для MLE. Наконец, возвращаемся к теме оптимальности. 4.1. Унаследованная изменчивость Экспериментатор может только управлять стимулом. Чтобы сравнить нашу байесовскую модель с поведением наблюдателя в психофизической задаче, нам нужно знать, каковы предсказания байесовской модели относительно\n--- Страница 116 ---\nУнаследованная изменчивость  115 откликов наблюдателя, когда истинным стимулом является s. Другими сло- вами, нам нужно знать распределение оценки стимула, когда истинным сти- мулом является s. Мы обозначаем это распределение p(sˆ | s) и называем его распределением оценки (estimate distribution), или распределением отклика (response distribution). Сначала опишем основной источник изменчивости в распределении оценки. В самой простой форме байесовского моделирования, даже когда измере- ние зашумлено, все, что происходит после измерения, является его детер- минированной функцией. Так, функция правдоподобия детерминистически выводится из измерения, апостериорное распределение детерминистиче- ски вычисляется из правдоподобия, и оценка (например, апостериорного среднего) детерминистически вычисляется из апостериорного распределе- ния. Таким образом, как только вам становится известно значение наблю- даемого зашумленного измерения xobs, вы можете вычислить все остальное вплоть до реакции наблюдателя. Однако даже когда этап 2 полностью детер- минирован, само измерение меняется от испытания к испытанию, несмот - ря на постоянство истинного стимула. Это означает, что вышеуказанные величины (правдоподобие, апостериорное распределение и апостериорный вывод) будут меняться вместе с измерением. Рисунок 4.1 иллюстрирует этот сценарий. В частности, поскольку xobs является случайной величиной при заданном s, такова же и оценка стимула. Следовательно, в отклике на повторные предъ- явления одного и того же стимула оценка апостериорного среднего будет случайной величиной с распределением вероятностей. Другими словами, стохастичность оценки стимула или реакции субъекта «наследуется» от сто- хастичности измерения. Предполагаемый стимулxobs1xobs2xobs3 sˆ1 sˆ2 sˆ3Априорное распределениеАпостериорное распределение ПравдоподобиеВероятность (степень доверия) Рис. 4.1  Функции правдоподобия (красный) и соответствующие апостериорные распре- деления (синий) в трех примерах испытаний. Априорное распределение показано желтым цветом. Смысл здесь заключается в том, что функция правдоподобия, апостериорное распре- деление и PME не являются фиксированными объектами: они меняются от испытания к ис - пытанию, потому что это делают измерения xobs. Для разработчика байесовской модели (на этапе 3) представляют интерес среднее значение и дисперсия по испытаниям PME\n--- Страница 117 ---\n116  Распределение отклика 4.2. Распределение отклика Вычислим конкретное распределение отклика в примере из главы 3. Ранее мы установили, что для простой модели с гауссовыми распределениями сти- мулов и измерений апостериорное распределение также является гауссовым со средним значением, определяемым уравнением (3.19): (4.1) где и . В разделе 3.3.6 мы обсуждали, что для наблюдателя разум- ным способом оценки стимула является среднее значение апостериорного распределения: sˆPM = μpost. (4.2) В нашей простой модели оценка также является откликом, то есть реакци- ей наблюдателя в эксперименте. Как нам перейти отсюда к распределению вероятностей отклика на сти- мул? Оценка sˆPM является линейной функцией xobs. Кроме того, из этапа 1 мы знаем, что, когда истинный стимул равен s, xobs следует распределению Гаусса со средним значением s и дисперсией σ2. Теперь мы можем использовать свойства линейных комбинаций случайных величин (см. примечание 4.1 и упражнение 4.1), чтобы показать, что когда истинным стимулом является s, оценка стимула s ˆPM следует распределению Гаусса со средним значением (4.3) и дисперсией (4.4) или, если воспользоваться сокращенной записью для гауссовых распреде- лений (3.3), (4.5) Примечание к обозначениям: �[sˆ | s] означает ожидаемое значение s при условном распределении p(sˆ | s). Ожидаемое значение случайной величины с условным распределением вероятностей также называется условным ма- тематическим ожиданием (conditional expectation). Мы используем нижний индекс под �, Var и т. д. только для обозначения случайных величин в случае неоднозначности. Подробнее об этом говорится в приложении А.\n--- Страница 118 ---\nРаспределение отклика  117 Примечание 4.1 Свойства линейных комбинаций случайных величин В байесовских моделях мы часто сталкиваемся со случайными величинами, кото- рые представляют собой взвешенные суммы других случайных величин. Полезны следующие свойства. 1. Общее: если распределение случайной величины X имеет среднее значение μ и дисперсию σ2, а a и b являются константами, то распределение случайной ве- личины aX + b имеет среднее значение μ + b и дисперсию a2σ2. 2. Общее: если случайные величины X и Y независимы и имеют средние значения μX и μY и дисперсии σX2 и σY2 соответственно, то распределение случайной вели- чины X + Y имеет среднее значение μX + μY и дисперсию σX2 + σY2. 3. Характерно для гауссовых распределений: если случайная величина X следует га- уссову распределению, то aX + b также следует этому распределению. 4. Характерно для гауссовых распределений: если случайные величины X и Y неза- висимы и каждая следует гауссову распределению, то X + Y также следуют этому распределению. Упражнение 4.1. Воспользуйтесь свойствами из примечания 4.1, чтобы вы- вести распределение откликов. (a) Объедините первые два свойства, чтобы показать, что распределение случайной величины aX + bY имеет среднее значение aμX + bμY и дис - персию a2σ2 X + b2σ2 Y. (b) Используя этот результат, докажите уравнения (4.3) и (4.4). Дисперсия оценки апостериорного среднего отличается от дисперсий, с которыми мы сталкивались до сих пор, – распределения измерений, функ - ции правдоподобия и апостериорного распределения. Таблица 4.1 поможет различить разные распределения. Таблица 4.1. Средние значения и дисперсии всех распределений, упомянутых в примере в главах 3 и 4. Мы используем обозначения и Среднее Дисперсия Этап 1: порождающая модельРаспределение стимула p(s) μ σs2 Распределение измерения p(x | s) s σ2 Этап 2: выводАприорное распределение p(s) μ σs2 Функция правдоподобия �(s hyp; xobs) p(x obs | shyp) xobs σ2 Апостериорное распределение p(s hyp | xobs)\n--- Страница 119 ---\n118  Распределение отклика Таблтца 4.1 (окончание) Среднее Дисперсия Этап 3: распределение откликаРаспределение отклика p(s ˆ | s) 4.3. Распределение убеждений или распределение отклика? Теперь мы готовы рассмотреть одну из самых больших путаниц в байесовском моделировании – между распределением убеждений и отклика. Априорные вероятности, нормированные вероятности и апостериорные вероятности представляют собой степень уверенности наблюдателя в различных гипо- тетических состояниях мира (его убеждения). Эти убеждения определяются в каждом отдельном испытании и являются внутренними для наблюдателя; они не поддаются непосредственному измерению. Распределение отклика, с другой стороны, представляет собой внешнюю сводку поведения наблю- дателя в нескольких испытаниях. Оно поддается непосредственному изме- рению и существует, даже если наблюдатель не является байесовским и не придерживается какого-либо распределения убеждений. Горизонтальная ось распределения убеждений представляет собой гипотетическую переменную состояния мира, а горизонтальная ось распределения отклика – оценку пере- менной состояния мира. В табл. 4.2 представлены распределения убеждений и ответов рядом друг с другом. Один из способов, которым выявляется различие между распределениями убеждений и отклика, заключается в сравнении их дисперсии. В частности, дисперсия оценки апостериорного среднего отличается от дисперсии апостериорной оценки из уравнения (3.20) Кон - цептуально это возможно, поскольку они имеют совершенно разный смысл: первая – это изменчивость поведения, измеряемая экспериментатором, вто- рая – неуверенность наблюдателя в данном испытании. В общем случае эти две величины будут разными. Разница становится очевидной, когда мы наносим на график соответ - ствующие стандартные отклонения как функцию уровня шума измерения (рис. 4.2). По мере увеличения уровня шума измерения задняя часть стано- вится все шире и шире. Первоначально то же самое относится и к распреде- лению PME. Однако, когда σ становится достаточно большим, стандартное отклонение оценки апостериорного среднего снова уменьшается.\n--- Страница 120 ---\nРаспределение убеждений или распределение отклика  119 Таблица 4.2. Сравнение распределения убеждений и отклика в байесовских моделях. Возможно, будет полезно рассмотреть эту таблицу вместе с табл. 3.1, в которой сравниваются этапы 1 и 2 Распределения убеждений (априорное, правдоподобие, апостериорное) Распределение отклика Этап байесовского моделированияЭтап 2 (вывод) Этап 3 (распределение отклика) Природа распределения (см. примечание 3.4)Субъективное (степень доверия)Объективное (вероятность события) С точки зрения Наблюдателя (принимающего решение)Экспериментатора Горизонтальная ось Гипотетическое состояние мира Оценка состояния мира Как получается В любом отдельном испытании Из множества испытаний Только в байесовской моделиДа Нет 20 15 10 5 0 05 10 15 20Стандартное отклонение Шум измерения σПравдоподобие Апостериорное распределение Распределение отклика Рис. 4.2  Стандартное отклонение нормированной функции правдоподобия, апостериорного распределения и распределения отклика (распределение оценки апостериорного среднего) в зави- симости от уровня шума измерения, когда распределение стиму- лов имеет стандартное отклонение σs = 8. По мере того как σ ста - новится очень большим, стандартное отклонение апостериорного распределения сходится к стандартному отклонению априорного распределения σs, в то время как стандартное отклонение распре- деления отклика в конечном итоге уменьшается до 0 Упражнение 4.2. Почему интуитивно понятно, что стандартное отклонение апостериорной средней оценки снова уменьшается?\n--- Страница 121 ---\n120  Распределение отклика 4.4. Оценка максимального правдоподобия Вместо того чтобы использовать в качестве оценки стимула апостериорное среднее значение, наблюдатель может просто использовать само измерение. Из раздела 3.3.2 вы знаете, что в нашем примере измерение также представ- ляет собой MLE: sˆML = xobs. (4.6) Точно так же, как мы изучали распределение апостериорной средней оценки sˆPM для данного s, мы можем изучить распределение максимального правдоподобия sˆML для данного s. Но мы уже знаем это распределение, так как оно равно распределению измерений px|s: гауссово со средним значением s и дисперсией σ2. Примеры sˆML и sˆPM для стимулов s, взятых случайным образом из гауссова рас - пределения ps(s), представлены на рис. 4.3A. Они показывают, что MLE в сред- нем равен стимулу, тогда как PME склонен сдвигаться к априорному среднему. Чем выше шум измерения σ, тем ближе значение PME к априорному среднему. 30 20 10 0 –10 –20 –30 300 200 100 030 20 10 0 –10 –20 –30 300 200 100 030 20 10 0 –10 –20 –30 300 200 100 0–30 –20 –10 –30 –20 –10–30 –20 –10 –30 –20 –10–30 –20 –10 –30 –20 –1010 20 30 10 20 3010 20 30 10 20 3010 20 30 10 20 300 00 00 0Мат. ожидание стимула Ожидаемая квадрат. ошибкаНизкий шум: σ = 4 Стимул sСредний шум: σ = 8 Стимул sВысокий шум: σ = 16 Стимул s(A) (В) MLE: нет смещения, только дисперсия Квадратичное смещение PME Дисперсия PMEStd. of MLE Bias of PME Std. of PME MLE PME 16.0 (12.8)64.0 (32.0)256.0 (51.2) Рис. 4.3  Сравнение PME и MLE. В этом примере распределение стимула имеет µ = 0 и σs = 8: (A) диаграммы рассеяния PME и MLE в сравнении с истинным стимулом. Пунктирные линии показывают ожидаемые значения. Чем больше шум, тем меньше наклон ожидаемого значения PME; (B) среднеквадратичная ошибка как функция стимула для PME и MLE. Среднеквадратич- ная ошибка (сплошные линии) представляет собой сумму квадратов смещения и дисперсии. Хотя PME смещена, ее дисперсия (светло­голубая пунктирная линия) ниже, чем у MLE (зеленая линия). Стимулы, которые часто возникают в соответствии с распределением стимулов (за- тенение указывает на вероятность), таковы, что общая (усредненная по стимулу) MSE от PME (голубое число в скобках) всегда ниже, чем от MLE (зеленое число)\n--- Страница 122 ---\nСмещение и среднеквадратичная ошибка  121 4.5. Смещение и среднеквадратичная ошибка Мы видели, что PME представляет собой средневзвешенное значение между измерением и средним значением гауссова распределения стимулов. Сле- довательно, средняя PME представляет собой средневзвешенное значение между истинным стимулом и средним значением распределения стимула. Как следствие среднее значение оптимальной оценки не равно истинному стимулу: апостериорная средняя оценка смещена от стимула к среднему значению априорного распределения. Смещение оценки sˆ (которой может быть PME, MLE, MAP или любая дру - гая оценка) определяется как разница между средней оценкой и истинным стимулом: Bias[s ˆ | s] º �[sˆ | s] - s. (4.7) Упражнение 4.3 (a) Рассчитайте смещение MLE для заданного s . (b) Рассчитайте смещение PME для заданного s . Разработчики байесовских моделей часто говорят, что байесовская оцен- ка оптимальна. Но PME (и практически любая другая байесовская оценка) необъективна – ее среднее значение отличается от истинного стимула. Это кажется противоречивым: не лучше ли всегда иметь непредвзятую оценку? Чтобы разрешить этот парадокс, мы должны понять, что предвзятость – это не единственное, на что нужно обращать внимание. Оказывается, оценка апостериорного среднего хороша в том смысле, что она минимизирует об- щую среднеквадратичную ошибку (MSE) между оценкой и истинным стиму - лом. Квадрат ошибки в одном испытании равен (s ˆ - s)2. MSE sˆ для заданного s равна MSE[s ˆ | s] º �sˆ|s[(sˆ - s)2 | s]. (4.8) MSE имеет два компонента: не только смещение, но и дисперсию. Иными словами, MSE[s ˆ | s] = Bias[s ˆ | s]2 + Var[s ˆ | s]. (4.9) Это отношение также называется разложением среднеквадратичной ошибки на смещение и дисперсию. И смещение, и дисперсия плохи по-разному. На рис. 4.4 показано, как одна и та же ожидаемая квадратичная ошибка может возникать из-за низкого смещения при высокой дисперсии, а также из-за высокого смещения при низкой дисперсии. MSE в уравнении (4.9) зависит от s, и мы будем называть ее среднеквадра- тичной ошибкой, обусловленной стимулом. Мы можем усреднить эту величину по всем s , чтобы получить общую среднеквадратичную ошибку:\n--- Страница 123 ---\n122  Распределение отклика MSE[s ˆ] º �sˆ,s[(sˆ - s)2] (4.10) = �s[�sˆ|s[(sˆ - s)2 | s]] (4.11) = �s[MSE[s ˆ | s]]. (4.12) Низкое смещение, высокая дисперсия (A) Истинный s Истинный sОценочный sˆ Оценочный sˆВысокое смещение, низкая дисперсия, одинаковая среднеквадратичная ошибка (B) Рис. 4.4  Одна и та же среднеквадратичная ошибка может быть получена оценкой с низким смещением и высокой дисперсией (A) и оценкой с высоким смещением и низкой дисперсией (B) Интуитивно понятно, что это ожидаемая квадратичная ошибка по всему эксперименту, полученная путем усреднения по многим испытаниям, каж - дое из которых имеет случайно выбранное значение s и отклик субъекта sˆ. Оказывается, PME является оптимальной в том смысле, что она минимизи- рует общую – независимую от s – среднеквадратичную ошибку MSE[s ˆ]. Да- вайте проверим справедливость этого утверждения в двух крайних случаях. Когда измерение очень сильно зашумлено, вы будете меньше всего оши- баться в среднем, если полностью проигнорируете измерение и всегда бу - дете выбирать среднее значение распределения стимула μ. Ваша дисперсия гарантированно будет равна нулю. Цена, которую вы платите, – это большое смещение, если реальный стимул оказывается далек от среднего, но именно такие значения стимула встречаются очень редко. С другой стороны, если измерение xobs совсем не содержит шума, вы должны оценить стимул как xobs. Очевидно, что для промежуточных уровней шума стратегия выбора оценки между средним стимулом и xobs приведет к тому, что вы в среднем откло- нитесь на минимально возможную величину. Таким образом, оптимальная стратегия будет включать смещение в сторону среднего стимула. Конечно, этот аргумент еще не однозначно свидетельствует в пользу PME. Мы можем рассчитать MSE, обусловленную стимулом, и общую MSE от PME. Для этого начнем с уравнения (4.9), выберем sˆ = sˆPM и подставим урав- нения (4.7), (4.3) и (4.4): (4.13) (4.14)\n--- Страница 124 ---\nСмещение и среднеквадратичная ошибка  123 Данное выражение можно упростить еще больше, но мы остановимся на этой форме, где первый член по-прежнему является квадратом смещения, а второй член остается дисперсией. Квадрат смещения растет квадратич- но с увеличением расстояния между s и средним стимулом, в то время как дисперсия не зависит от s. Мы изобразили оба члена в зависимости от s на рис. 4.3B. Для сравнения мы также построили MSE, обусловленную стимулом, для MLE. MLE является несмещенной, поскольку �[sˆML | s] = s, и, следовательно, ее обусловленная стимулом MSE равна ее дисперсии, которая имеет постоян- ное значение σ2. График позволяет нам понять, почему оценка PME является оптимальной, даже если она необъективна. Для любого s его дисперсия ниже, чем дисперсия MLE. Квадрат смещения складывается с дисперсией, но их сумма по-прежнему остается ниже дисперсии MLE, пока стимул достаточно близок к среднему стимулу. Именно здесь и будет оказываться большинство стимулов, поскольку распределение стимулов является гауссовым (показано интенсивностью серого затенения). Когда мы оцениваем ожидаемое значение по s в уравнении (4.12) для рас - чета общей MSE, чего можно достичь при достаточно продолжительном экс - перименте, мы получаем меньшее число для PME, чем для MLE (подробнее в задаче 4.9). Короче говоря, стратегия, в которой объединены априорное среднее и среднее значение правдоподобия, в долгосрочной перспективе более выгодна. Таким образом, уклон в сторону среднего априорного на самом деле является признаком оптимальной стратегии. Как заметил Э. Т. Джейнс, слова «предвзятость» и «смещение» являют - ся неудачными терминами, поскольку они имеют негативную коннотацию при использовании в повседневной речи: «Когда мы называем величину “предвзятостью”, это звучит как что-то ужасно предосудительное, от чего мы должны избавиться любой ценой. Если бы ее вместо этого назвали “состав- ляющей ошибки, ортогональной дисперсии”, то всем было бы ясно, что эти два вклада в ошибку равноценны. Это цена, которую приходится платить за выбор технической терминологии, несущей эмоциональную нагрузку, пред- полагающую оценочные суждения» [13: 514]. 4.5.1. Перспектива «обратного смещения» Поскольку это очень важный момент для понимания, мы рассмотрим второй ответ на вопрос «как смещенная оценка может быть оптимальной?». Мы можем посмотреть на диаграмму рассеяния (рис. 4.3А) с другой точки зре- ния (рис. 4.5). До сих пор мы вычисляли общую MSE, взяв математическое ожидание сначала по sˆ для данного s, а затем по s, как в уравнении (4.12). Однако можно также изменить порядок вычислений и взять математическое ожидание сначала по s для данного sˆ, а затем по sˆ. Этот альтернативный под- ход наводит нас на новые соображения. Мы увидим, что средняя разница между MLE и стимулом во всех испытаниях равна нулю. Средняя разница между PME и стимулом во всех испытаниях также равна нулю, но PME рас - пределяется вокруг стимула с меньшей дисперсией.\n--- Страница 125 ---\n124  Распределение отклика 20 0 –20 –20 –20–20 –20–20 –20–20 –200 00 00 00 020 2020 2020 2020 2020 0 –20 sˆMLsˆML = 10sˆPMsˆPM = 10s ss = 10 s = 10sˆML sˆPMp(sˆML|s = 10) p(sˆPM|s = 10)s sp(s|s ˆML = 10) p(s|s ˆPM = 10)sˆML – s sˆPM – sp(sˆML – s) p(sˆPM – s)(A1) (B1)(A2) (B2)(A3) (B3)(A4) (B4) Рис. 4.5  PME (s ˆPM) является более точной, чем MLE (s ˆML): (A1) 1 000 000 стимулов были взяты из нормального распределения (μ = 0, σs = 8), каждое из которых дало зашумленное измерение (σ = 4). График рассеяния s по сравнению с sˆML (показана подвыборка из 1000 сти- мулов). Диагональная линия sˆML = s; вертикальная линия s = 10; горизонтальная линия, sˆML = 10; (A2) при каждом s sˆML распределяется со средним значением s и дисперсией σ2 = 16. Распре- деление p(sˆML | s = 10) показано для иллюстрации; (A3) для данного sˆML распределение s не центрировано на sˆML. Распределение p(s | sˆML = 10) показано для иллюстрации; это апостери- орное распределение по s при x = 10; (A4) для каждого s p(sˆML – s | s) имеет среднее значение 0 и дисперсию 16. Следовательно, общее распределение ошибок p(sˆML – s) также имеет среднее значение 0 и дисперсию 16. Поскольку это распределение ошибок имеет среднее значение 0, его дисперсия равна MSE[s ˆML]; (B1) график рассеяния s по сравнению с sˆPM для тех же точек данных, что и на (A). Диагональная линия sˆPM = s; вертикальная линия s = 10; горизонтальная линия, sˆPM = 10; (B2) PME смещена в сторону априорного среднего; распределение p(sˆPM | s = 10) показано для иллюстрации; (B3) p(s | sˆPM) – это апостериорное распределение по s при за- данном измерении, которое привело к sˆPM. Таким образом, p(s | sˆPM) имеет среднее значение sˆPM и дисперсию 12.8; p(s | sˆPM = 10) показано для иллюстрации; (B4) для каждого sˆPM p(sˆPM – s | sˆPM) имеет среднее значение 0 и дисперсию 12.8. Следовательно, общее распределение ошибок p(sˆPM – s) также имеет среднее значение 0 и дисперсию 12.8. Поскольку это распределение ошибок имеет среднее значение 0, его дисперсия равна MSE[s ˆPM] На рис. 4.5 представлен один миллион стимулов, выбранных из распреде- ления стимулов (μ = 0, σs = 8), каждый из которых дает зашумленное изме- рение (σ = 4). На рис. 4.5A показаны статистические параметры для MLE, а на рис. 4.5B – для PME. Для заданного значения стимула s MLE распределяется со средним значением s (A2); таким образом, MLE является несмещенной. Напротив, для данного s PME не распределяется со средним значением s (B2); таким образом, PME смещена. Однако важно отметить, что в испытаниях, когда наблюдатель сообщает о конкретной PME, стимул действительно со- средоточен на этой оценке (B3), и это не относится к испытаниям с конкрет - ной MLE (A3). По сути, это стимул смещен по отношению к MLE! Напротив,\n--- Страница 126 ---\nСмещение и среднеквадратичная ошибка  125 стимул является несмещенным по отношению к PME. Что наиболее важно, хотя средняя разница во всех испытаниях между любой оценкой и стимулом равна нулю, MSE ниже для PME (B4), чем для MLE (A4). Таким образом, PME является более точной оценкой, чем MLE. Чтобы лучше понять это, обратите внимание, что для конкретной MLE (т. е. конкретного измерения x) распределение стимулов является апостериорным распределением p(s | x). Апостериорное распределение не центрировано на из- мерении; напротив, оно смещено от измерения к априорному среднему (урав- нение 4.1). Следовательно, стимул смещен по отношению к MLE. Напротив, стимул является несмещенным по отношению к PME. Чтобы понять почему, обратите внимание, что каждая MLE (т. е. каждое измерение) отображается на конкретную PME (уравнение 4.1). Следовательно, каждый горизонтальный ряд точек на графике A1 просто смещается по вертикали к априорному среднему (0 в этом примере) на графике B1, так что его положение равняется PME. На- пример, читатель может проверить из уравнения (4.1), что строка с sˆML = 10 на графике A1 сдвинута на sˆPM = 8 на графике B1. Поскольку график B1 отображает апостериорное среднее по оси ординат, необходимо, чтобы среднее значение распределения стимулов в каждом горизонтальном ряду (при достаточно боль- ших выборках) совпадало с линией тождества. Следуя этой линии рассуждений, мы можем убедиться, что дисперсия каждого горизонтального ряда стимулов на графиках A1 и B1 является просто дисперсией апостериорного распределе- ния. Поскольку дисперсии горизонтальных рядов на графиках A1 и B1 равны, а стимул смещен только по отношению к MLE (сравните графики A3 и B3), от - сюда следует, что MSE для MLE должна быть больше, чем для PME. Фактически, не выполняя дополнительных математических операций, мы можем видеть, что общая MSE для PME равна дисперсии апостериорного распределения, тогда как общая MSE для MLE равна дисперсии распределе- ния измерений. Чтобы увидеть это, просуммируем ошибки (s ˆPM - s) из всех горизонтальных строк на панели B1. Поскольку распределение стимулов сосредоточено в каждой строке на PME (например, панель B3), результиру - ющее общее распределение ошибок имеет среднее значение, равное нулю (панель B4). Следовательно, дисперсия этого распределения (т. е. дисперсия апостериорного распределения) является общей MSE PME. Следуя аналогич- ной цепочке рассуждений, мы можем суммировать ошибки (s ˆML - s) из всех вертикальных столбцов на графике A1. Поскольку распределение MLE со- средоточено в каждом столбце на s (например, график A2), результирующее общее распределение ошибок имеет нулевое среднее значение (график A4). Следовательно, дисперсия этого распределения (т. е. дисперсия распределе- ния измерения) является общей MSE для MLE. В заключение отметим, что MSE[s ˆPM] < MSE[s ˆML], потому что дисперсия апостериорного распределения меньше, чем дисперсия распределения измерений. 4.5.2. Все расходы оплачены До сих пор при сравнении апостериорной оценки с MLE мы рассматривали только непрерывные переменные. Как это сравнение будет выглядеть для\n--- Страница 127 ---\n126  Распределение отклика дискретных (категориальных) переменных? Мы имеем в виду, в частности, номинальные переменные, где категории имеют метки, но не имеют опре- деленного порядка. (Ординальные, т. е. упорядоченные дискретные, пере- менные находятся где-то посередине между номинальными и непрерывны- ми переменными.) Для номинальных переменных апостериорное среднее не имеет смысла, потому что нет лежащего в основе пространства. Вместо этого, как мы обсуждали в главах 2 и 3, байесовский наблюдатель выберет категорию с наибольшей апостериорной вероятностью, или, другими слова- ми, выполнит оценку MAP . Как и PME в непрерывном случае, MAP смещена в сторону более частых значений, и поэтому не менее интересно спросить, можно ли извлечь из этого пользу. Чтобы разобраться в этом вопросе, рассмотрим следующий пример. Некая страна состоит из трех штатов s. В табл. 4.3 для каждого из штатов показаны доли работников различных профессий х. Эти пропорции представляют со- бой условные вероятности p (x = профессия | s). Таблица 4.3. Процент работников в гипотетической стране с тремя штатами, рассчитанный по штатам. Жирным шрифтом в строке выделено решение с максимальной вероятностью, если случайно выбранный человек имеет указанную профессию s = 1 s = 2 s = 3 Общее население, тыс.19 771 (79.0 %) 3446 (13.8 %) 1805 (7.2 %) точность MLE, % x = учитель 1.5 % 0.8 % 1.1 % 86 x = фермер 0.4 % 2.6 % 3.6 % 28 x = продавец 8.4 % 9.3 % 8.3 % 15 x = другое 89.7 % 87.3 % 87.0 % 79 Теперь предположим, что в рамках кампании по связям с общественно- стью случайным образом выбран один житель этой страны, который выиграл отпуск с оплатой всех расходов. Если известен род занятий этого человека, как мы думаем, из какого он штата? Оценка максимального правдоподобия привела бы нас к выводу, что если этот человек окажется учителем, то он из штата 1; если он фермер, то из штата 3; а если он работник торговли, то из штата 2, как выделено в табл. 4.3. Легко понять, в чем ошибочность этого рассуждения. Население трех шта- тов сильно различается (см. вторую строку в таблице; значения исчисляются тысячами), и априори более вероятно, что случайно выбранный гражданин будет из более густонаселенного штата. При более вдумчивом рассмотрении становится ясно, что если случайно выбранный гражданин является ферме- ром, то для того, чтобы максимизировать вероятность правильного прогноза в отношении места проживания этого человека, недостаточно учитывать только долю населения каждого штата, состоящую из фермеров. Вместо этого мы должны подсчитать абсолютное число фермеров в каждом штате, то есть долю фермеров в населении штата, умноженную на общее население штата. Эти значения рассчитаны в табл. 4.4.\n--- Страница 128 ---\nДругие оценки  127 Таблица 4.4. Те же данные в абсолютных числах. Жирным шрифтом в строке выделено решение MAP, если случайно выбранное лицо имеет указанную профессию. Вероятность того, что решение MAP верное, получается путем деления выделенного жирным шрифтом числа в каждой строке на сумму чисел в строке. Это также апостериорная вероятность оценки MAP. Например, p(s = 2 | x = фермер) = 90 / (79 + 90 + 65) s 1 s 2 s 3 точность MAP, % x = учитель 297 28 20 86 x = фермер 79 90 65 38 x = продавец 1661 320 150 78 x = другое 17 735 3008 1570 79 Наиболее вероятным местом проживания человека данной профессии является штат, в котором проживает наибольшее количество людей этой профессии. Это дает частично другой набор решений (выделено). В то время как наилучшее предположение о домашнем штате учителя не изменилось, лучшим предположением о домашнем штате фермера теперь является 2, а для продавца – 1. Это оценка MAP , и мы видим в правом столбце таблицы, что для фермера и продавца оценка MAP является более точной, чем MLE. Хотя оценка MAP , таким образом, «смещена» в сторону штатов с более высокой численностью населения, это смещение является полностью обо- снованным и действительно оптимальным. Обратите также внимание, что оценка MAP не всегда соответствует штату с наибольшей численностью на- селения. В случае фермера доля фермеров в штате 2 достаточно велика, что- бы преодолеть общий перевес населения в штате 1. Это говорит о том, что наблюдение, действующее через функцию правдоподобия, может быть до- статочно сильным, чтобы преодолеть априорное распределение. На этом мы заканчиваем обзор дискретных оценок. 4.6. Другие оценки Если бы мы не слышали о байесовском выводе, то могли бы подумать, что разумной оценкой стимула будет среднее значение распределения стимула μ и измерения xobs (4.15) Другой способ рассматривать эту среднюю оценку состоит в том, что это PME при неправильном предположении σ = σs. Действительно, небайесов- скую оценку часто можно интерпретировать как PME при неправильном предположении о порождающей модели. Однако никакая оценка не может иметь более низкую MSE, чем PME, если PME использует правильную порож - дающую модель; мы исследуем этот момент в задаче 4.9.\n--- Страница 129 ---\n128  Распределение отклика 4.7. Шум принятия решения и шум отклика До сих пор мы описывали отображение измерения в оценку стимула как детерминированное, а именно определяемое апостериорным средним. Это оптимально, но не обязательно реалистично. К этому отображению может быть добавлен шум. Одной из форм шума является аддитивный гауссов шум с постоянной дисперсией. Другая форма шума решения возникает в случае выбора оценки стимула из апостериорного распределения. Однако доказа- тельства того, что мозг выполняет выборку из апостериорного распределе- ния, по нашему мнению, немногочисленны. Кроме того, мы предполагали, что реакция наблюдателя равна оценке стимула. На практике каждая реакция в континууме будет подвержена не- которому шуму отклика (скажем, двигательному), отражающему, например, точность, с которой наблюдатель может позиционировать компьютерный курсор. Кроме того, память наблюдателя об оценке может немного ухудшить- ся между моментом формирования оценки и моментом отправки отклика. Следовательно, распределение отклика наблюдателя не обязательно совпа- дает с распределением оценки, и полная модель задачи будет включать шум отклика. Например, отклик наблюдателя может быть получен из распреде- ления Гаусса со средним значением, равным оценке стимула, но с некоторой дисперсией σ2 motor . Мы рассмотрим этот случай в задаче 4.12. Однако шум отклика не является ключевым элементом байесовского вывода. 4.8. Заблуждения Чтобы построить байесовскую модель, мы должны сформулировать по- рождающую модель и сделать правильный вывод на ее основе. Некоторые аспекты этого вывода противоречат интуитивному пониманию, и даже уче- ные, занимающиеся исследованиями байесовских моделей, иногда путаются в результирующих отношениях между переменными. Здесь мы рассмотрим несколько неправильных представлений, которые могут возникнуть при байесовском моделировании. Остальные заблуждения мы рассмотрим в за- дачах в конце главы. Функция правдоподобия определяется стимулом. Одно из заблуждений состоит в том, что функция правдоподобия байесовского наблюдателя опре- деляется стимулом, и, следовательно, это всегда одна и та же функция, пока стимул не меняется. В действительности на рис. 4.1 мы видели, что функция правдоподобия меняется от испытания к испытанию, если наблюдения за- шумлены, поэтому одно значение стимула порождает разные функции прав - доподобия. Неправильное представление может возникнуть из-за того, что функцию правдоподобия перепутали с распределением измерений. Обычно предполагается, что распределение измерений определяется стимулом.\n--- Страница 130 ---\nЗаблуждения  129 Связанное с этим заблуждение состоит в том, что апостериорное распреде- ление определяется стимулом. В действительности апостериорное распреде- ление также меняется от попытки к попытке, даже когда стимул неизменен (рис. 4.1). Распределение откликов равно функции правдоподобия. Иногда можно встретить высказывания наподобие «Мы строим правдоподобие оценки». Это высказывание смешивает вероятность из этапа 2 с распределением от - ветов из этапа 3 или с распределением измерений из этапа 1. Аргументом функции правдоподобия является предполагаемый стимул, а не оценка. Ре- зультатом этапа 3 является распределение оценки, а не вероятность оценки. И если «оценку» путают с «измерением» (которое, в конце концов, являет - ся MLE), то распределение этого измерения также не является функцией правдоподобия, хотя функция правдоподобия выводится из распределения измерений. Возможные исправления: «Мы строим распределение оценки» (если ось x представляет оценку), «Мы строим функцию правдоподобия для стимула в конкретном испытании» (если ось x представляет предполагаемый стимул) или «Мы строим распределение измерения» (если ось x представляет измерение). Распределение отклика является произведением распределения из- мерений и априорного распределения. Вот еще одна заманчивая ошибка: «Предполагая, что наблюдатель сообщает PME, для получения плотности вероятности отклика наблюдателя на заданный стимул я перемножаю рас - пределение измерений p(x | s) с априорным распределением вероятностей. Это правильно, потому что после нормирования это дает мне распределение, центрированное между априорным средним и истинным стимулом». Более изощренная версия, ведущая к тому же выводу, звучит так: «MLE стимула равна измерению x [верно]. Таким образом, распределение MLE для данного истинного стимула s равно распределению шума при s [верно]. Следова- тельно, распределение PME при заданном стимуле s может быть получено путем умножения распределения измерения на априорное распределение [ошибка]». Математически это рассуждение будет равносильно выражению (ошибка) p (sˆ | s) µ px|s(sˆ | s)ps(sˆ), (4.16) где px|s(sˆ | s) – распределение измерения px|s, оцененное при sˆ, а ps(sˆ) – апри- орное распределение, оцененное в s ˆ. Оба утверждения предлагают чрезвычайно удобный способ прямого вы- числения распределения откликов вместо выполнения этапов 2 и 3 процесса моделирования. Оба утверждения также слишком хороши, чтобы быть прав- дой. Чтобы понять, почему они неверны, мы могли бы заменить распреде- ления, которые использовали в главе 3 и текущей главе, ps(s) = �(s; μ, σs2) и px|s(x | s) = �(x; s, σ2). Тогда уравнение (4.16) даст гауссово распределение со средним значением и дисперсией . Среднее значение будет пра- вильным согласно уравнению (4.3). Однако дисперсия будет отличаться от правильной дисперсии из уравнения (4.4). Интуитивную причину, по которой ответ должен быть неправильным, можно найти в предельной нуле -\n--- Страница 131 ---\n130  Распределение отклика вой надежности (σ → ¥). В этом случае наблюдатель всегда будет оценивать стимул как среднее значение априорного распределения, и, таким образом, дисперсия распределения отклика должна быть равна 0. Неправильный аргу - мент даст дисперсию σs2. Строго говоря, основная ошибка здесь состоит в том, что уравнение (4.16) не является правильным применением правила Байеса. Во-первых, оно не имеет математически правильной формы p(y | x) µ p(x | y) p(y). Более того, в байесовском моделировании поведения правило Байеса действует не на уровне оценок по многим испытаниям, а на уровне одного испытания. Следовательно, обе составляющие в правой части уравнения (4.16) неверны. Функция правдоподобия не должна быть функцией MLE или измерения, а должна быть функцией предполагаемого значения стимула s, опять же в одном испытании. Аргументом априорного распределения явля- ется не оценка стимула, а гипотетический стимул, также в одном испытании. Когда измерение равно истинному стимулу, распределение отклика равно апостериорному распределению. Следующее заблуждение, которое мы рассматриваем, звучит примерно так: «Для нахождения распределения оценки наблюдателя при заданном стимуле я могу просто использовать “ти- пичное” апостериорное распределение, которое получается, когда измере- ние оказывается равным истинному стимулу s. Это даст мне распределение, центрированное между априорным средним и истинным стимулом». Эта ошибка математически идентична предыдущей, но к ней привели не- сколько иные рассуждения. Предположим, мы правильно вычислили апосте- риорное распределение, p(s | x). Теперь подставим истинный стимул вместо x: p(s | x = s). Это законная, хотя и не особо значимая функция s: она отража- ет убеждения наблюдателя о стимуле, когда измерение x просто совпадает с истинным стимулом. Последним шагом ошибочного аргумента было бы рассмотрение распределения p(s | x = s) как распределения ответов p(sˆ | s). Мы исследуем этот вопрос далее в задаче 4.14. В нашем гауссовом примере это снова привело бы к выводу, что дисперсия распределения отклика составляет , тогда как на самом деле это диспер- сия апостериорного распределения. Приравнивание дисперсии распреде- ления отклика и апостериорного распределения является частой ошибкой. Чтобы правильно описать взаимосвязь априорного распределения шума и распределения PME, необходимо пройти три этапа, описанных в главе 3 и в этой главе; альтернативы этому нет. Априорная вероятность равна общей вероятности ответа. Это заман- чивое заблуждение может принимать различные формы, например: «Априорная вероятность отклика справа равна 0.5»; «У меня есть простой способ получить априорное распределение, ис - пользуемое наблюдателем, непосредственно из данных. Я могу просто подсчитать оценки наблюдателя по всем испытаниям в эксперименте. В конце концов, чем выше априорная вероятность значения стимула, тем чаще наблюдатель будет сообщать об этом значении стимула». В ходе эксперимента мы могли отслеживать распределение отклика на- блюдателя. Заманчиво, но неверно рассматривать общее распределение от - клика наблюдателя как его априорное распределение. Другими словами,\n--- Страница 132 ---\nРазмышления о байесовских моделях  131 фраза вроде «p (s) – это априорная вероятность ответа s» неверна. Правильно говорить так: «p (s) – это априорная вероятность (убеждение наблюдателя) того, что мир находится в состоянии s ». Чтобы увидеть разницу между априорным распределением и общим рас - пределением отклика, рассмотрим наш гауссов пример. Интуитивно понят - но, что если наблюдатель использует правильное априорное распределение, то все его отклики будут смещены к среднему значению априорного распре- деления, и, таким образом, распределение отклика будет более узким, чем априорное распределение. Мы можем продемонстрировать разницу между двумя распределения - ми более формально. Когда измерение представляет собой xobs, PME рав - на Это случайная величина, потому что xobs – случайная величина. Для расчета распределения PME, обусловленного истинным стимулом s, мы использовали тот факт, что xobs при заданном s образует гауссово распре- деление со средним значением s и дисперсией σ2. Аналогично мы теперь рассматриваем распределение PME по всем s. Распределение xobs по всем s является гауссовым со средним значением μ и дисперсией σ2 + σs2. Тогда PME будет иметь среднее значение μ и дисперсию (см. задачу 4.11). Это выражение, представленное как функция от σ на рис. 4.2, показывает, что общее распределение откликов не идентично априорному распределению (дисперсия которого, конечно, не зависит от σ ). 4.9. Размышления о байесовских моделях Байесовская модель, обсуждаемая в этой главе, хотя и проста, во многих от - ношениях представляет байесовское моделирование в целом. Суть байесов- ских наблюдателей в том, что они рассматривают все возможные значения переменной состояния мира и вычисляют вероятности этих значений. Дру - гими словами, байесовский наблюдатель не придерживается ограниченного набора гипотез, если только на это не указывают свидетельства. Одна из великих возможностей байесовского моделирования заключается в том, что оно позволяет построить полную модель перцептивной задачи до получения каких-либо экспериментальных данных: байесовская модель указывает, как наблюдатель должен выполнять задачу, чтобы быть оптималь- ным. Поэтому байесовское моделирование является примером норматив- ного моделирования: байесовская модель устанавливает норму – наивысшую точность, которую может достичь наблюдатель. Это отличается от обычной практики во многих психологических исследованиях, когда моделирование, если оно вообще проводится, часто выполняется после наблюдения опреде- ленных закономерностей в данных. В байесовском моделировании можно разработать модель и выполнить ее симуляцию, даже не начав натурный эксперимент.\n--- Страница 133 ---\n132  Распределение отклика 4.10. Заключение В этой главе мы завершили разработку прототипичной байесовской модели, начатую в главе 3, вычислив распределение отклика для заданного стимула. Вы узнали следующее: байесовское моделирование состоит из трех этапов: определение по- рождающей модели, получение выражения для оценки апостериорного среднего наблюдателя (PME) и получение распределения PME по мно- жеству испытаний; среднеквадратичная ошибка (MSE) служит мерой эффективности оцен- ки. Мы различали обусловленную стимулом MSE и общую MSE; MSE, обусловленная стимулом, представляет собой сумму квадратов смещения и дисперсии; по сравнению с распределением PME, распределение MLE является несмещенным, хотя оно имеет более высокую дисперсию для часто встречающихся стимулов, и в результате оно в целом хуже; легко перепутать функции и распределения на разных этапах, посколь- ку они выглядят одинаково (в этой главе все они гауссовы). Их нужно тщательно различать, как мы это сделали в табл. 4.1; если не следовать трем этапам, можно сделать много концептуальных ошибок. В частности, попытки найти короткий обходной путь для рас - чета распределения отклика обречены на неудачу; модель PME является минимальной. Она может быть расширена раз- личными формами шума решения и шума отклика. 4.11. Рекомендуемая литература Edwin T. Jaynes. Probability Theory: The Logic of Science. Cambridge: Cambridge University Press, 2003. Shane T. Mueller and Christoph T. Weidemann. Decision Noise: An Explanation for Observed Violations of Signal Detection Theory. Psychonomic Bulletin and Review 15, no. 3 (2008): 465–494. 4.12. Задачи Задача 4.1. Сопоставьте следующие функции, которые играют роль в байе- совском моделировании, с описаниями. Функции: a) распределение оценки апостериорного среднего; b) априорное распределение; c) функция правдоподобия;\n--- Страница 134 ---\nЗадачи  133 d) апостериорное распределение; e) распределение измерений. Описания: a) является результатом вывода об отдельном испытании; b) описывает, как генерируются потенциально зашумленные наблюде- ния; c) можно напрямую сравнить с человеческими реакциями в психофизи- ческом эксперименте; d) часто моделируется как функция Гаусса с центром в измерении; e) может отражать статистические показатели реального мира. Задача 4.2. Для гауссовой модели, обсуждаемой в этой главе: a) докажите математически, что дисперсия PME (уравнение 4.4) меньше или равна дисперсии MLE; b) как их сравнивать, если дисперсия априорного распределения намного больше, чем дисперсия измерения? Объясните, почему ответ интуи- тивно понятен. Задача 4.3. Верно или неверно? Если утверждение неверно, поясните по- чему: a) функция правдоподобия всегда равна распределению измерения; b) значение стимула, которое максимизирует апостериорную вероят - ность, является значением измерения; c) мы можем получить распределение отклика для данного стимула, ум- ножив распределение измерения на априорное распределение; d) распределение отклика на данный стимул всегда является гауссовым; e) если в ходе эксперимента байесовский наблюдатель сообщает одно значение оценки стимула чаще, чем другое значение, это означает, что априорная вероятность первого выше. Задача 4.4. В задаче 3.2 мы построили апостериорное распределение вместе с функцией правдоподобия. Повторите задания (a)–(f) этой задачи, но вмес - то использования одного значения измерения xobs начните с фиксированно- го значения стимула s = 10. Из этого значения s извлеките пять значений xobs из распределения измерения. Вы должны заметить, что от испытания к ис - пытанию и функция правдоподобия, и апостериорная функция плотности вероятности «прыгают» подобно тому, как показано на рис. 4.1. Задача 4.5. См. рис. 4.2. Докажите математически, что при прочих равных условиях стандартное отклонение распределения отклика максимально, ког - да σ = σs. (Подсказка: каково значение производной в максимуме функции?) Задача 4.6. Наблюдатель делает вывод о стимуле s на основе измерения xobs. Как и в этой главе, распределение измерения p(x|s) является гауссовым со средним значением s и дисперсией σ2. В отличие от основного материала главы, мы используем априорное распределение p(s) = e-λs, (4.17)\n--- Страница 135 ---\n134  Распределение отклика где λ – положительная константа. Это неудовлетворительное априорное рас - пределение (см. раздел 3.5.2), но это нас не останавливает. (a) Выведите уравнение для PME. (b) Выведите уравнение для распределения PME для данного s . Задача 4.7. Мы определяем относительное смещение (relative bias) как от - ношение между смещением и разницей между средним значением распре- деления стимула и истинным стимулом. (a) Для PME выведите выражение для относительного смещения как функ - цию отношения (Подсказка: выражение будет содержать только R, никаких других переменных.) (b) Постройте относительную погрешность как функцию R . (c) Показывает ли этот график то, что вы ожидаете от байесовского наблю- дателя? Объясните логически. Задача 4.8. Рассмотрим оценку априорного среднего sˆprior mean = μ, которая полностью игнорирует измерение и просто возвращает априорное среднее. Один из следующих вопросов – с подвохом. (a) Получите выражение для общей MSE этой оценки. (b) Насколько большим должно быть σ, чтобы эта оценка имела более низ- кую MSE, чем MLE? (c) Насколько большим должно быть σ, чтобы эта оценка имела более низ- кую MSE, чем PME? Задача 4.9. Уравнение (4.9) представляет MSE PME для данного s. Используя это уравнение, вычислим среднее значение этой величины по всем s, то есть общую MSE в уравнении (4.12). (a) Покажите математически, что общая MSE PME равна . (Подсказка: �[(s - μ)2] = σs2.) (b) Убедитесь, что выражение в (a) возвращает числа, показанные на рис. 4.4B фиолетовым цветом. (c) Рассмотрим некую оценку, являющуюся линейным преобразованием из- мерения: sˆlinear = axobs + b, где a и b – константы. Рассчитайте ее общую MSE. (Подсказка: � [s2] = σs2 + μ2.) (d) Необязательно: докажите математически, что среди этих оценок PME имеет наименьшую общую MSE. (Подсказка: возьмите частные произ- водные.) Задача 4.10. В этой задаче вам нужно численно сравнить свойства оценки MAP и MLE. См. раздел 4.5 и параметры на рис. 4.3. (a) Воспроизведите рис. 4.3А, используя 1000 испытаний. (b) Воспроизведите кривые MLE и PME на рис. 4.3B, используя соответству - ющие математические выражения. (c) Для каждого из трех уровней шума смоделируйте s на 10 000 испытаний. Для каждого s вычислите квадрат смещения и дисперсию PME, используя выражения из (b). Добавьте два количества, чтобы получить обусловлен- ную стимулом MSE. Найдите среднее по всем значениям s, которые вы\n--- Страница 136 ---\nЗадачи  135 нанесли на график, чтобы получить общую MSE PME. Полученные зна- чения должны быть близки к числам, связанным с PME (12.8, 32.0, 51.2) на рис. 4.3B. Задача 4.11. В разделе 4.5 было сказано, что общая MSE представляет собой среднее значение обусловленной стимулом MSE. Однако общая дисперсия – это не просто среднее значение обусловленных стимулом дисперсий. (a) Покажите, что для всех испытаний в эксперименте MLE имеет среднее значение μ и дисперсию σ2 + σs2. (Подсказка: используйте примечание 4.1 про линейную комбинацию случайных величин.) Мы будем называть это общей дисперсией MLE. (b) Используя дисперсию из (а), покажите, что во всех испытаниях в экспе- рименте PME имеет среднее значение μ и дисперсию Мы будем называть это общей дисперсией PME. (c) Покажите, что общая дисперсия PME всегда меньше, чем общая диспер- сия MLE. (d) При отсутствии шума измерения (σ = 0) часть (c) предсказывает, что общая дисперсия PME равна σs2. Объясните, почему это имеет смысл. (e) Выполните аналогичную проверку работоспособности, соответствую- щую ситуации σ → ¥. (Подсказка: если шум измерения чрезвычайно велик, что можно сказать об оценке наблюдателя?) Задача 4.12. Мы упоминали, что отклик наблюдателя может быть искажен реакцией или моторным шумом. Предположим, что моторный шум под- чиняется гауссову распределению со средним значением, равным оценке апостериорного среднего, и со стандартным отклонением σm. (a) Каково распределение отклика наблюдателя, когда истинным стимулом является s ? (b) Подумайте, как экспериментально отличить моторный шум от шума из- мерения. Задача 4.13. Студент утверждает: «Чтобы получить плотность вероятности PME наблюдателя для данного стимула, я умножаю распределение измере- ний p(x | s = μ), где μ – априорное среднее, на априорную плотность вероят - ности p (s)». Другими словами, (неверно) p (sˆPM | s) = px|s(sˆPM | μ)ps(sˆPM). (4.18) (a) Хотя этот подход дает распределение, центрированное между априор- ным средним и истинным стимулом, данное утверждение концептуаль- но неверно. Почему? (b) Покажите математически, что дисперсия полученного распределения неверна. (c) В качестве конкретного примера определите, какую дисперсию предска- зал бы этот студент, если бы шум измерения был чрезвычайно большим (в пределе σ → ¥), и что предсказал бы правильный расчет. Объясните оба ответа с логической точки зрения.\n--- Страница 137 ---\n136  Распределение отклика Задача 4.14. Студент утверждает: «Чтобы получить распределение отклика p(sˆPM | s) в модели, я могу просто использовать апостериорное распределение p(s|xobs), полученное, когда измерение xobs оказывается равным s». Другими словами, (неверно) p (sˆPM | s) = ps|x(sˆPM | s). (4.19) (a) Хотя этот подход дает распределение, центрированное между априор- ным средним и истинным стимулом, данное утверждение концептуаль- но неверно. Почему? (b) Покажите математически, что дисперсия полученного распределения неверна. (c) В качестве конкретного примера определите, какую дисперсию предска- зал бы этот студент, если бы шум измерения был чрезвычайно большим (в пределе σ → ¥), и что предсказал бы правильный расчет. Объясните оба ответа с логической точки зрения. Задача 4.15. Студент утверждает: «Чтобы получить распределение отклика p(sˆPM | s) в модели, я могу усреднить апостериорные распределения p(s|xobs) по всем xobs для заданного s ». Другими словами, (неверно) (4.20) (a) Хотя этот подход дает распределение, центрированное между априор- ным средним и истинным стимулом, данное утверждение концептуаль- но неверно. Почему? (b) Покажите математически, что дисперсия полученного распределения неверна. (c) В качестве конкретного примера определите, какую дисперсию предска- зал бы этот студент, если бы шум измерения был чрезвычайно большим (в пределе σ → ¥), и что предсказал бы правильный расчет. Объясните оба ответа с логической точки зрения. Задача 4.16. В этой задаче мы подгоняем (обучаем) и сравниваем модели на синтетических данных из задачи непрерывного оценивания. Прочтите приложение C, если вы незнакомы с подгонкой и сравнением моделей. Загру - зите файл estimation.csv по ссылке https://osf.io/84kpb. Строки соответству - ют испытаниям (всего 500 испытаний). Первый столбец содержит значения предъявляемого стимула, второй столбец – оценки стимула, возвращенные испытуемым. (a) Нанесите данные на диаграмму рассеяния оценка–стимул, используя черные точки. Нарисуйте пунктирную черную линию, чтобы указать диа- гональ. Выберите диапазоны осей соответствующим образом. Обозначь- те оси. (b) Попытайтесь определить на глаз: наблюдатель находит MLE или прини- мает во внимание априорное распределение? Почему вы так думаете? Теперь подгоним две модели к данным. В обеих моделях мы предпола- гаем, что измерения наблюдателя подчиняются гауссову распределению\n--- Страница 138 ---\nЗадачи  137 со средним значением, равным истинному стимулу, и неизвестным стан- дартным отклонением σ. Мы также предполагаем, что все испытания независимы. (c) Модель 1 утверждает, что наблюдатель находит MLE. В рамках этой моде- ли запишите уравнение для логарифмической функции правдоподобия по σ в обозначениях стимулов s1, …, sn и оценок sˆ1, …, sˆn. Максимально упростите его. (d) Постройте логарифмическую функцию правдоподобия σ на сетке от 0.1 до 10 с шагом 0.02. (e) Какова MLE для σ на этой сетке? (f) В качестве альтернативы поиску по сетке используйте встроенный ал- горитм оптимизации, чтобы найти MLE для σ. Обоснуйте выбор алго- ритма. (g) Модель 2 утверждает, что наблюдатель выполняет оценку апостериор- ного среднего, используя гауссово априорное распределение со сред- ним значением, равным 0, и неизвестным стандартным отклонением σprior. В соответствии с этой моделью запишите уравнение для функции логарифмического правдоподобия комбинации (σ , σprior). Максимально упростите его. (h) Используя сетки от 0.1 до 10 с шагом 0.02 как для σ, так и для σprior, по - стройте логарифмическую функцию правдоподобия в виде тепловой карты. Добавьте цветовую легенду. Пометьте оси; обязательно проверьте соответствие осей переменным. (i) Каковы MLE для σ и σprior на этой сетке? (j) Используйте встроенный алгоритм оптимизации, чтобы найти MLE для σ и σprior. (k) Какая модель побеждает в соответствии с информационным критерием Акаике? (l) Какая модель побеждает в соответствии с байесовским информацион- ным критерием? Задача 4.17. В этой задаче мы подбираем и сравниваем модели на дан- ных реального оценочного эксперимента. Это эксперимент по локализации источника звука людьми, проведенный в лаборатории Вей Цзи Ма. В каж - дом испытании звук предъявлялся в одном из пяти мест на горизонтальной линии: -6, -3, 0, 3, 6 (условные единицы). Источники звука были скрыты, и испытуемый не знал, что мест всего пять. Испытуемый сообщал, откуда он услышал исходящий звук, выбирая из двадцати одного дискретного мес - тоположения: - 10, - 9, -8, , 8, 9, 10. Загрузите файл данных эксперимента localization.csv по ссылке https://osf.io/84kpb. Строки соответствуют испыта- ниям (всего 400 испытаний). Первый столбец содержит значения истинного местоположения, второй столбец – значения местоположения, которые со- общил испытуемый. (a) Данные. Отдельно для каждого из пяти истинных местоположений под- считайте долю ответов испытуемого в каждом месте ответа. Постройте пять сплошных линий на одном графике разными цветами, чтобы раз- личать пять истинных местоположений.\n--- Страница 139 ---\n138  Распределение отклика (b) Сначала мы рассмотрим «нулевую модель» под названием модель 0 (нет параметров). Согласно этой модели, наблюдатель выбирает случайное место (из двадцати одной возможности) с равными вероятностями. Рас - считайте логарифмическую вероятность, AIC и BIC для модели 0. Для этого вам даже не нужны ответы. Объясните, почему они не нужны. Теперь мы представляем еще две содержательные модели: модель 1 (один параметр): наблюдатель выполняет оценку место- положения по методу максимального правдоподобия, то есть имеет плоское априорное распределение; модель 2 (два параметра): наблюдатель выполняет оценку MAP мес - тоположения с априорной гауссовой моделью со средним значени- ем 0 и неизвестным стандартным отклонением σs. В обеих моделях мы делаем обычное предположение, что наблюдатель производит зашумленное измерение местоположения звука и что это измерение следует гауссову распределению со средним значением, рав- ным истинному местоположению, и неизвестным стандартным откло- нением σ . Задания (c)–(g) относятся к моделям 1 и 2. (c) Предсказания модели I. Не обращайте внимания на тот момент, что наблюдатель дает только дискретные ответы, и вместо этого представьте, что он сообщает непрерывную оценку местоположения sˆ. Для модели 1 запишите уравнение для функции плотности вероятности по sˆ, когда истинный стимул находится в месте s . Повторите для модели 2. (d) Предсказания модели II. Здесь ситуация усложняется: отклик не явля- ется непрерывным, так как есть только двадцать одно возможное место- положение источника звука. Поэтому мы предполагаем, что наблюдатель сначала вычисляет непрерывную оценку sˆ, а затем сообщает о ближайшем возможном местоположении отклика. Таким образом, вероятность со- общить о конкретном местоположении – это вероятность того, что sˆ по - падет в сегмент размера 1 вокруг этого местоположения. Исключение: для двух крайних мест отклика (- 10 и 10) сегмент бесконечно широк с одной стороны. Предположим, что истинное местоположение s равно -6 и что σ = 3 и σs = 10. Как для модели 1, так и для модели 2 вычислите вектор из двадцати одного числа, который дает вероятность сообщения о каждом из двадцати одного местоположения, как предсказывает модель. Постройте оба прогноза модели на одном графике разными цветами. (e) Предсказания модели III. Повторите задание (d), за исключением по- строения графика, для каждого из пяти истинных местоположений и для каждой комбинации значений параметров. Выберите возможные значе - ния σ и σs в диапазоне от 0.1 до 15 с шагом 0.1. Для модели 1 сохраните результаты в матрице размера 21 (отклики) × 5 (истинные местополо- жения стимула) × 150 (значения σ), а для модели 2 – в матрице разме- ра 21×5×150×150. Во избежание числовых проблем сначала установите значения вероятности, которые равны нулю в числовом выражении, равными наименьшему ненулевому значению в матрице1. После этого 1 Это простое решение, но не самое лучшее. Лучшим решением является использо- вание обратной биномиальной выборки [187].\n--- Страница 140 ---\nЗадачи  139 убедитесь, что отдельно для каждой истинной комбинации местополо- жения и параметра вероятности отклика снова составляют 1 по двадцати одному местоположению отклика. (f) Подгонка модели I. Предположим, что испытания независимы друг от друга. Используйте «справочные таблицы» прогнозов моделей из зада- ния (e) для вычисления логарифмической вероятности каждой комбина - ции параметров отдельно для моделей 1 и 2. Это должно дать вам вектор длиной 150 для модели 1 и матрицу размером 150×150 для модели 2. Постройте первый с помощью линейного графика, а второй с помощью тепловой карты. (g) Аппроксимация модели II. Для моделей 1 и 2 найдите MLE параметра(ов) на их сетках. (h) Сравнение моделей. Найдите для моделей 1 и 2 максимальное логариф - мическое правдоподобие, AIC и BIC. С учетом задания (b), какая модель выигрывает по AIC? Какая модель выигрывает по BIC? Сформулируйте свой общий вывод о поведении испытуемого так, как вы бы сделали это в научной статье. (i) Проверка модели. Отдельно для каждой модели добавьте аппрокси- мацию модели к пяти кривым в задании (а). Используйте пунктирные линии с цветами, соответствующими цветам данных. Постройте каждую модель на отдельном графике, чтобы получить три графика, каждый из которых содержит пять сплошных линий (данные) и пять соответствую- щих пунктирных линий (аппроксимация модели). Для моделей 1 и 2 ис - пользуйте MLE из задания (g) и интерполяционные таблицы из зада- ния (e).",
      "debug": {
        "start_page": 115,
        "end_page": 140
      }
    },
    {
      "name": "Глава 5. Комбинация признаков и накопление свидетельств 140",
      "content": "--- Страница 141 --- (продолжение)\nГлава 5 Комбинация признаков и накопление свидетельств Как мы объединяем несколько сенсорных сигналов в одно восприятие? В главах 3 и 4 мы изучали простую порождающую модель, содержащую один стимул и одно измерение. В этой главе мы изучаем расширенный сценарий, в котором есть два измерения, основанных на сенсорных входных данных, которые также называются сенсорными сигналами, или раздражителями (cue). Для краткости мы будем далее называть их просто сигналами. Измерения могут быть слуховыми и зрительными характеристиками лежащего в осно- ве стимула, например места, в котором мяч падает на землю. Наблюдатель определяет значение стимула на основе обоих сигналов. С математической точки зрения эта модель является прямым расширением модели из глав 3 и 4. Тем не менее есть четыре причины для отдельного изучения этой по- рождающей модели. Во-первых, комбинации сигналов очень часто встре - чаются в повседневной жизни. Во-вторых, это исторически первая и до сих пор заметная область применения байесовского моделирования. В-третьих, эта порождающая модель – наш первый пример, в котором байесовский наблюдатель вычисляет функцию правдоподобия для интересующего со- стояния мира из двух более простых правдоподобий. В-четвертых, идея этой главы состоит в том, что байесовский вывод не обязательно должен вклю- чать априорные предположения, чтобы быть полезным. Ключевым аспектом байесовского принятия решений является не наличие априорности, а вывод с распределением вероятности вместо точечной оценки. Краткое содержание главы Мы начнем с обсуждения основных понятий, лежащих в основе объединения сигналов. Затем рассмотрим этапы 1–3 байесовского вывода для объедине- ния сигналов. Мы покажем, как интеграция сенсорных данных во времени математически эквивалентна комбинации сигналов. Наконец, мы обсудим\nГлава 5 Комбинация признаков и накопление свидетельств Как мы объединяем несколько сенсорных сигналов в одно восприятие? В главах 3 и 4 мы изучали простую порождающую модель, содержащую один стимул и одно измерение. В этой главе мы изучаем расширенный сценарий, в котором есть два измерения, основанных на сенсорных входных данных, которые также называются сенсорными сигналами, или раздражителями (cue). Для краткости мы будем далее называть их просто сигналами. Измерения могут быть слуховыми и зрительными характеристиками лежащего в осно- ве стимула, например места, в котором мяч падает на землю. Наблюдатель определяет значение стимула на основе обоих сигналов. С математической точки зрения эта модель является прямым расширением модели из глав 3 и 4. Тем не менее есть четыре причины для отдельного изучения этой по- рождающей модели. Во-первых, комбинации сигналов очень часто встре - чаются в повседневной жизни. Во-вторых, это исторически первая и до сих пор заметная область применения байесовского моделирования. В-третьих, эта порождающая модель – наш первый пример, в котором байесовский наблюдатель вычисляет функцию правдоподобия для интересующего со- стояния мира из двух более простых правдоподобий. В-четвертых, идея этой главы состоит в том, что байесовский вывод не обязательно должен вклю- чать априорные предположения, чтобы быть полезным. Ключевым аспектом байесовского принятия решений является не наличие априорности, а вывод с распределением вероятности вместо точечной оценки. Краткое содержание главы Мы начнем с обсуждения основных понятий, лежащих в основе объединения сигналов. Затем рассмотрим этапы 1–3 байесовского вывода для объедине- ния сигналов. Мы покажем, как интеграция сенсорных данных во времени математически эквивалентна комбинации сигналов. Наконец, мы обсудим\n--- Страница 142 ---\nЧто такое объединение сигналов  141 представленные в научной литературе исследования, в которых рассматри- вается, насколько хорошо люди на самом деле комбинируют сигналы. 5.1. Что такое объединение сигналов? При попытке понять чью-то речь полезно не только внимательно слушать, но и одновременно наблюдать за движениями лица говорящего и невербаль- ными жестами. Это пример объединения сигналов. Объединение сигналов особенно важно, когда некоторые сигналы зашумлены, например когда вы пытаетесь понять речь в присутствии фоновых звуков. Способность объ- единять сигналы повышает производительность в перцептивных задачах. В многочисленных повседневных перцептивных ситуациях мы получаем и комбинируем сигналы от различных сенсорных модальностей, но делаем это так легко, что можем не осознавать деталей восприятия. Пробуя пищу, мы можем думать, что сосредоточены исключительно на вкусовом восприятии, но на самом деле мы воспринимаем вкус пищи, комбинируя вкусовые, обо- нятельные, термические и механические (текстурные) сигналы. Оценивая наше ускорение во время движения в движущемся транспортном средстве, мы можем думать, что полагаемся только на зрение, но мы также используем проприоцептивные сигналы, передаваемые мышечными веретенами и сухо- жилиями Гольджи, которые сообщают о длине и напряжении мышц, а так - же вестибулярные сигналы, передаваемые датчиками нашего внутреннего уха (полукружными каналами и отолитовыми органами), которые сообщают о вращательном и линейном ускорении головы. На самом деле мы объединяем сигналы не только между сенсорными мо- дальностями, но и внутри одной модальности. Каждая модальность предо- ставляет набор различных сигналов. В зрении, например, относительная активация фоторецепторов, настроенных на разные длины волн, говорит нам о цвете объекта, узор отраженного света указывает на текстуру поверх - ности объекта, а сравнение изображений на сетчатках двух глаз сообщает нам о глубине объекта. Наш мозг легко комбинирует эти и другие зрительные подсказки, чтобы сделать вывод об идентичности объекта. Точно так же от - дельные рецепторы в коже передают мозгу механическую, тепловую и ноци- цептивную1 информацию, и даже внутри каждого из этих соматосенсорных отделов мы получаем множество сигналов. Например, различные подтипы механорецепторов предоставляют информацию о статическом давлении (ре- цепторы Меркеля), растяжении кожи (рецепторы Руффини), низкочастотной вибрации (рецепторы Мейснера) и высокочастотной вибрации (рецепторы Пачини). Когда мы проводим кончиком пальца по неизвестной поверхности, то получаем информацию о текстуре поверхности, трении, твердости и дру - гих качествах, комбинируя сигналы от этих рецепторов. Это позволяет нам делать точные перцептивные выводы, отличая, например, ощущение шелка от ощущения бархата или шерсти. 1 Болевые рецепторы. – Прим. перев.\n--- Страница 143 ---\n142  Комбинация признаков и накопление свидетельств Почему мозг должен комбинировать сигналы? Чтобы ответить на этот вопрос, давайте рассмотрим последствия очевидной альтернативной стра- тегии, когда мозг использует единственный наиболее информативный сиг - нал, который у него есть под рукой, и игнорирует остальные. Эта стратегия «победитель получает все» неоптимальна по двум причинам. Во-первых, наши сенсоневральные реакции зашумлены, в результате чего любая оцен- ка параметра, основанная даже на самом надежном сигнале, подвержена некоторой неопределенности; стратегия, которая не включает другие, хотя и менее надежные признаки, отбрасывает информацию, которая может быть использована для повышения точности оценки. Во-вторых, даже когда инди- видуальный сигнал не засорен нейросенсорным шумом, он все равно бывает неоднозначен; стратегия, которая не включает в себя все доступные сигналы, часто не может преодолеть двусмысленность. Чтобы проиллюстрировать эти положения, рассмотрим два примера. Предположим, что мы хотим сделать вывод о месте, в котором мяч падает на землю. Это событие дает как зрительные, так и слуховые сигналы. Теперь предположим, что мы основываем наш вывод о местоположении исключи- тельно на зрительном сигнале, потому что мяч падает в хорошо освещенной области прямой видимости, в условиях, при которых зрение более инфор- мативно, чем слух. Поскольку наши фоторецепторы и нейронные реакции обладают внутренним шумом, даже оценка, основанная на этом наиболее надежном сигнале, будет иметь некоторую неопределенность, что отража- ется в ширине апостериорного распределения по местоположению. Ниже мы покажем, что включение менее надежного сигнала (например, слухового сигнала в этом примере), тем не менее дает полезную информацию. Таким образом, объединяя сигналы, мы получаем более точную оценку, чем та, которая может быть получена по одному лучшему сигналу. Далее предположим, что мы хотим сделать вывод об идентичности сфери- ческого объекта, помещенного на столешницу в темноте. Когда мы кладем руку на объект, наши проприоцепторы сообщают нам его размер, но мы не можем однозначно идентифицировать объект только по его размеру. В этом случае наша неуверенность в идентичности объекта (в отличие от его точного размера) возникает не только из-за нейросенсорного шума. Напротив, наша неопределенность проистекает из естественной неоднозначности размера как признака идентичности объекта: даже если мы точно знаем размер, это мало что говорит об идентичности объекта, потому что разные объекты (на- пример, яблоко и апельсин) могут иметь одинаковый размер. Хотя размер может быть единственным наиболее информативным сигналом в этом сце- нарии (например, сильно сужая набор возможных объектов до таких боль- ших, как яблоки и апельсины), при дальнейшем исследовании – поднятии объекта и манипулировании им – мы могли бы извлечь из наших мышц и механорецепторов понимание веса объекта и текстуры поверхности, еще больше сузив возможный набор объектов. Короче говоря, мы преодолеваем двусмысленность, комбинируя сигналы. По какому принципу наша нервная система объединяет две части инфор- мации? Очевидно, что хотя стратегия «победитель получает все» слишком экстремальна, логично предположить, что люди будут больше полагаться на\n--- Страница 144 ---\nЧто такое объединение сигналов  143 те сигналы, которые наиболее информативны для решения поставленной задачи. Если в определенном сценарии зрение более информативно, чем слух, например когда мы хотим найти человека, который разговаривает в шумном помещении, то мы должны в основном полагаться на зрение. Когда слух более информативен, чем зрение, мы ожидаем обратного – и дей - ствительно ночью мы часто полагаемся в первую очередь на нашу слуховую систему. На протяжении последних десятилетий многие ученые изучали комбина- ции сигналов в лаборатории. В типичном эксперименте испытуемый окру - жен массивом громкоговорителей и светодиодов (рис. 5.1). Аудиовизуальный стимул создается одновременным появлением короткого звукового сигнала и световой вспышки. Испытуемому предлагается указать воспринимаемое местоположение звукового сигнала. С помощью этого аппарата ученые мо- гут исследовать, как визуальные и слуховые сигналы сочетаются в нервной системе. Светодиоды Фиксирующие фотодиодыЗвукоизлучатели Кронштейн Рис. 5.1  Экспериментальная установка для проверки слуховой или муль- тисенсорной локализации. Динамик издает короткий звуковой сигнал. В то же время светодиод может давать кратковременную вспышку. Субъект направляет лазерную указку на воспринимаемое положение источника звука. Рисунок вос - произведен из [194] Результаты этих экспериментов показывают, что когда звуковой сигнал и вспышка возникают в одном и том же или почти в одном месте, испыту - емые используют зрительный стимул, чтобы определить местонахождение слухового стимула, даже если им приказано игнорировать визуальный сти- мул. Таким образом, испытуемые естественным образом и интуитивно ком- бинируют сигналы, по-видимому, полагая, что звуковой сигнал и вспышка исходят из одного источника. Действительно, в условиях, когда слуховые и визуальные сигналы исходят из мало различающихся мест, испытуемых легко ввести в заблуждение, поскольку их оценки «слуховой локализации» искажены присутствием зрительного сигнала. Важно отметить, что чем точ- нее зрительная подсказка по сравнению со слуховой, тем сильнее субъекты полагаются на зрительную подсказку при построении своей оценки локали- зации.\n--- Страница 145 ---\n144  Комбинация признаков и накопление свидетельств 5.2. Формулировка байесовской модели Далее мы сформулируем байесовскую модель для оптимальной комбина- ции сигналов. При разработке нашей формулировки мы используем пример аудио визуальной оценки местоположения, но тот же подход можно приме- нить и к другим сценариям объединения сигналов. Мы будем следовать тем же трем этапам байесовского моделирования, которые описаны в главах 3 и 4. 5.2.1. Этап 1: порождающая модель Наша модель состоит из трех узлов: стимула s и двух измерений x1 и x2 (рис. 5.2). Со стимулом связано распределение стимула p(s). В отличие от главы 3, здесь мы предполагаем, что априорное распределение является плос ким. Мы делаем это не только потому, что это наиболее распространен- ное допущение в исследованиях комбинаций признаков, но и потому, что оно наглядно демонстрирует, что байесовская модель может быть интерес - ной и важной даже без неоднородности априорного распределения. (Другая иллюстрация этого соображения была приведена в разделе 2.6.) Любое измерение зашумлено, и нам нужно сделать предположения о рас - пределении, как в разделе 3.2.4. Обозначим распределения измерений как p(x1 | s) и p(x2 | s). Отдельные стрелки, указывающие на x1 и x2 на рис. 5.2, от - ражают ключевое предположение, а именно что эти измерения условно неза- висимы. Условная независимость двух случайных величин (примечание 5.1) означает, что они независимы друг от друга, когда они обусловлены другой переменной, в данном случае стимулом s. В частности, это означает, что хотя зрение и слух зашумлены, шум, искажающий два потока, некоррелирован, т. е. между двумя модальностями нет ковариации шума. Важно различать условную независимость и просто независимость. Наши зрительные и слухо- вые измерения не будут независимыми друг от друга, если не будут обуслов- лены стимулом. Когда стимул находится слева, оба измерения скорее будут x2 x1sСтимул Слуховое измерение Зрительное измерение Рис. 5.2  Порождающая модель комбинации сигналов. Стимул влияет как на слуховые, так и на зрительные измерения, но они условно независимы друг от друга\n--- Страница 146 ---\nФормулировка байесовской модели  145 «левыми», а когда стимул находится справа, оба измерения скорее будут «правыми». Однако мы предполагаем, что при повторных предъявлениях одного и того же стимула вариабельность слуховых и зрительных измере- ний от испытания к испытанию будет некоррелированной. Таким образом, зрительные и слуховые измерения считаются независимыми друг от друга только тогда, когда они обусловлены s . Предположение об условной независимости легче обосновать, когда два измерения исходят от двух разных сенсорных модальностей (например, слу - ха и зрения), чем от одного и того же (например, оба от зрения). При этом все дальнейшие расчеты в этой главе – и в большей части литературы по комбинациям сигналов – основаны на допущении условной независимости. Математически условная независимость x1 и x2 при данном s выражается как p(x1, x2 | s) = p(x1 | s)p(x2 | s). (5.1) Независимость означала бы, что p(x1, x2) = p(x1 | s)p(x2 | s), но здесь это не- верно. Упражнение 5.1. На что была бы похожа порождающая модель, если бы два измерения не считались условно независимыми, а вместо этого коррелиро- вали с заданным s ? Примечание 5.1 Условная независимость Условная независимость возникает, когда две случайные величины независимы только при заданном значении третьей. Например, болезнь Альцгеймера и потреб- ность в очках для чтения – это два события, которые не являются независимыми, потому что и то, и другое, как правило, встречается у пожилых людей. Однако среди восьмидесятилетних (т. е. с учетом возрастной группы) эти два события, вероятно, более или менее независимы. Еще одним известным примером является то, что количество убийств в городе и продажи мороженого не являются независимыми случайными величинами: и то, и другое более вероятно в жаркие дни. Но при за- данной температуре они условно независимы. Суть в том, что вы обуславливаете значение причины зависимости двух перемен- ных. Если x, y и z обозначают три случайные величины, то x и y независимы при заданном z, если p(x, y | z) = p(x | z)p(y | z). Будьте аккуратны и не путайте условную независимость с независимостью, которая описывается выражением р(х, у) = р(х)р(у). Другой способ интерпретации уравнения (5.1) состоит в том, чтобы начать с прави- ла произведения вероятностей: p(x, y | z) = p(x | y, z)p(y | z). Затем, чтобы получить уравнение (5.1), мы должны сделать предположение, что p(x | y, z) = p(x | z). Другими словами, знание z полностью определяет наше понимание вероятности x. Когда мы знаем z, дополнительное знание также и y ничего не дает для нашей оценки вероятности x.\n--- Страница 147 ---\n146  Комбинация признаков и накопление свидетельств Упражнение 5.2. Предложите свой пример условной независимости из реаль- ной жизни. Для каждого отдельного измерения мы выбираем гауссово распределение: (5.2) (5.3) Как и в предыдущих двух главах, для удобства мы будем использовать переменные точности. Определим эти переменные: (5.4) (5.5) На этом спецификация порождающей модели завершена. 5.2.2. Этап 2: вывод Наблюдатель делает вывод о стимуле s из измерений xobs,1 и xobs,2. Чтобы уменьшить сложность записи, мы отныне будем опускать индекс «hyp» в shyp, который напоминал нам, что на этапе 2 наблюдатель рассматривает гипоте- зы о стимуле. Теперь вам придется самостоятельно следить, на каком этапе вы находитесь. Правдоподобия по стимулу представляют собой те же выражения, что и распределения шума, но рассматриваются как функция от s : (5.6) (5.7) Мы называем эти выражения элементарной функцией правдоподобия, опре- деляемой как функция правдоподобия по признаку стимула, связанному с отдельным измерением. Апостериорное распределение по стимулу вы- числяется по правилу Байеса:\n--- Страница 148 ---\nФормулировка байесовской модели  147 p(s | xobs,1, xobs,2) µ p(s)p(xobs,1, xobs,2 | s). (5.8) Мы исключили множитель p(x) по причине, раскрытой в приложении 3.6: он действует только как нормирующий коэффициент, поэтому если мы нормируем распределение в конце, мы автоматически учтем этот множи- тель. Поскольку распределение стимулов является плоским, априорное рас - пределение также является плоским, а апостериорное определяется только вероятностью p(x1, x2 | s). Чтобы продвинуться дальше, воспользуемся урав- нением (5.1) в предположении условной независимости измерений. Тогда апостериорное распределение описывается выражением p(s | xobs,1, xobs,2) µ p(s)p(xobs,1 | s)p(xobs,2 | s) (5.9) µ p(xobs,1 | s)p(xobs,2 | s). (5.10) Этот шаг – использование структуры порождающей модели для выражения вероятности с помощью элементарных вероятностей – является единствен- ным концептуально новым элементом в этой главе по сравнению с главой 3. Выражение вероятности по переменной состояния мира с точки зрения эле- ментарных вероятностей лежит в основе байесовских моделей вывода для многих задач. Мы можем подставить в это уравнение два гауссовых распределения и пе- реписать выражение в соответствии с примечанием 3.7. Полученное апосте- риорное распределение представляет собой другое гауссово распределение (рис. 5.3): (5.11) со средним (5.12) и дисперсией (5.13) Мы также можем записать апостериорное среднее как μpost = w1xobs,1 + w2xobs,2, (5.14) где веса пропорциональны точности: (5.15)\n--- Страница 149 ---\n148  Комбинация признаков и накопление свидетельств (5.16) Оценка апостериорного среднего (PME) равна sˆPM = μpost. (5.17) Напомним, что PME – это оценка, которая минимизирует ожидаемую квад- ратичную ошибку. В этом случае PME равна максимальной апостериорной оценке (MAP), а также оценке максимального правдоподобия (MLE). x2 x1 s sˆЗдесь шире! Комбинированный p(s|x1, x2) ∝ p(x1|s)p(x2|s) Первый сигнал �1(s) = p(x1|s)Второй сигнал �2(s) = p(x2|s) Рис. 5.3  Вычисление апостериорного распределения в комбинации сигна- лов. Обратите внимание, что обе красные кривые являются функциями прав- доподобия. Априорное распределение плоское и здесь не показано Вы могли заметить, что вычисление апостериорного распределения на- блюдателем в точности аналогично вычислению в главе 3, где мы объедини- ли один сигнал с априорным распределением. Теперь второй сигнал взял на себя роль априорного распределения, в то время как новое априорное рас - пределение является плоским. Эта эквивалентность уже была предсказана в разделе 1.5. Веса в уравнениях (5.16) и (5.17) в сумме дают 1: w1 + w2 = 1, следовательно, PME представляет собой средневзвешенное значение двух измерений. Усред- нение с весами, пропорциональными обратной дисперсии, на сегодняшний день является наиболее часто используемой моделью объединения сигна- лов. Можно провести аналогию с полицейским следователем, пытающимся реконструировать преступление на основании показаний двух свидетелей. Один свидетель в момент совершения преступления находился в состоянии алкогольного опьянения, другой – нет. Показания трезвого свидетеля будут зашумлены мало, в отличие от показаний пьяного свидетеля. Хороший сле- дователь примет во внимание оба показания, но придаст больший вес по- казаниям менее зашумленного свидетеля. Дисперсия апостериорной вероятности в уравнении (5.13) является мерой неопределенности наблюдателя. При сделанных нами ранее предположени-\n--- Страница 150 ---\nИскусственный конфликт сигналов  149 ях она меньше, чем дисперсии каждой из элементарных функций правдо- подобия. Интуитивно это говорит о том, что объединение сигналов снижает неопределенность: наблюдатель более уверен в комбинированной оценке, чем в оценке, которая была бы получена по одному признаку. Упражнение 5.3. Докажите математически, что дисперсия апостериорной ве- роятности никогда не превышает дисперсию каждого правдоподобия. 5.2.3. Этап 3: оценка распределения В качестве третьего этапа в нашей байесовской модели нас интересует рас - пределение PME по многим испытаниям. PME задается как функция из- мерений x1 и x2 в уравнении (5.12), но измерения сами по себе являются случайными величинами – их значения меняются от испытания к испы- танию. Как следствие PME также варьируется от испытания к испытанию. Поскольку в поведенческом эксперименте мы никогда не знаем измерений в одном испытании (они находятся в голове наблюдателя), нам приходится сравнивать поведение с распределением PME по многим испытаниям. Что- бы найти среднее значение и дисперсию PME, мы применяем правила для линейных комбинаций нормально распределенных переменных. Средние значения x1 и x2 равны s. Следовательно, модель предсказывает, что среднее значение PME будет равно w1s + w2s = w1s + (1 - w1)s = s. Другими словами, для комбинации сигналов с плоским априорным распределением оценка апостериорного среднего является несмещенной. (Напомним, что система- тическая ошибка определяется как разница между средней оценкой и ис - тинным стимулом.) Дисперсия PME равна: (5.18) Таким образом, в этой задаче комбинации сигналов дисперсия распреде- ления PME оказывается идентичной дисперсии апостериорного распреде- ления. Это отличается от главы 4, где дисперсия оценочного распределения не совпадала с дисперсией апостериорного распределения. Это различие возникает из-за того, что в текущей главе было выбрано плоское априорное распределение. Когда априорное распределение является гауссовым, дис - персия PME будет отличаться от дисперсии апостериорного распределения также и для комбинации сигналов (см. раздел 5.4). 5.3. Искусственный конфликт сигналов Мы показали, что в байесовской модели комбинации сигналов PME в сред- нем равна истинному стимулу. Это не очень интересно, так как не делает различий между байесовской моделью комбинации сигналов и моделью,\n--- Страница 151 ---\n150  Комбинация признаков и накопление свидетельств в которой наблюдатель использует только один из сигналов. (Дисперсия PME на самом деле различается, но лучше иметь две меры, чем одну.) Поэтому в экспериментах с комбинацией сигналов обычная уловка состоит в том, чтобы ввести небольшой конфликт между истинными стимулами в двух модальностях. Другими словами, наблюдатель не знает, что существует не один s, а два слегка смещенных стимула, s1 и s2. Все остальное остается прежним. Конечно, для этого необходимо, чтобы наблюдатель по-прежнему верил в существование единственного лежащего в основе стимула, несмотря на не- соответствие, внесенное экспериментатором. Экспериментатор иногда явно просит наблюдателя представить, что два сигнала генерируются одним сти- мулом, например слуховым и зрительным измерениями, которые генерирует мяч, ударяющийся об экран. Хотя исследователь намеренно использовал два разных стимула, тем не менее наблюдатель делает неверный вывод об одном общем стимуле. На первый взгляд, это кажется неоптимальным. Однако на этот подход можно взглянуть с другой стороны. Логично предположить, что наблюдатель применяет априорное распределение, основанное на естест - венной статистике; в реальном мире, когда наблюдатель одновременно ви- дит удар мяча о землю и слышит удар, зрительные и слуховые стимулы почти всегда возникают в результате одного и того же события и, следовательно, происходят из одного и того же места. В лабораторном эксперименте иссле- дователь придумал ситуацию, которая редко встречается в реальном мире и поэтому легко неверно интерпретируется наблюдателем. Имейте в виду, что даже при наличии действительно одного стимула слуховые и зрительные измерения будут отличаться друг от друга в каждой попытке из-за шума (если только шум не полностью коррелирован каким-либо образом). Следователь- но, сам факт того, что два сигнала различаются, не означает, что они воз- никли в результате воздействия двух стимулов в разных местах. Таким об- разом, вывод наблюдателя в эксперименте может быть оптимальным и при использовании априорной оценки реального мира. Конечно, наблюдатель поверит в единственный стимул только в том слу - чае, если вносимые расхождения малы. В противном случае наблюдатель заметит конфликт. Например, если звук прыгающего мяча возникает на достаточно большом расстоянии от зрительного образа мяча, наблюдатель поймет, что предъявлялись два отдельных стимула. Точно так же, если фильм плохо дублирован, расхождение во времени между движением рта говоряще- го и голосом будет слишком большим, чтобы остаться незамеченным. Когда наблюдатель не обязательно верит, что существует единственная общая при- чина, процесс вывода наблюдателя меняется. Эта интересная ситуация будет обсуждаться в разделе 10.3. Если наблюдатель действительно считает, что существует единственная общая причина, то этап 2, описанный выше, остается без изменений. Однако на этапе 3 средние значения x1 и x2 равняются не s, а s1 и s2 соответственно. Как следствие средняя PME будет: �[sˆPM | s1, s2] = w1s1 + w2s2. (5.19)\n--- Страница 152 ---\nОбобщения: априорные распределения, множественные сигналы  151 Эта оценка смещена. Например, смещение по отношению к стимулу s1, которое мы обозначаем здесь как Bias1, равно: Bias1[sˆPM | s1, s2] = �[sˆPM | s1, s2] - s1 (5.20) = w1s1 + (1 - w1)s2 - s1 (5.21) = (1 - w1)(s2 - s1). (5.22) Это предсказанное смещение можно сравнить с экспериментальными данными, и оно действительно часто хорошо совпадает (раздел 5.7). 5.3.1. Различение распределений Как и в главе 4, важно различать апостериорное (одно испытание, этап 2) и оценочное распределения (несколько испытаний, этап 3); см. табл. 5.1. Просто так получается, что когда априорное распределение плоское, как мы до сих пор предполагали, распределение оценок имеет ту же дисперсию, что и апостериорное, но в общем случае это не так. Таблица 5.1. Обзор распределений, упомянутых в этой главе; i может принимать значения 1 или 2 Распределение Аргумент Среднее Дисперсия Точность Распределение измеренияИзмерение xi Стимул si σi2Ji Функция правдоподобияПредполагаемый стимул sИзмерения xi σi2Ji Апостериорное распределениеПредполагаемый стимул sJ1 + J2 Распределение откликаОжидание стимула s ˆMLJ1 + J2 5.4. Обобщения: априорные распределения, множественные сигналы В главах 3 и 4 мы изучали комбинацию гауссова априорного распределения (среднее μ, стандартное отклонение σs, точность Js) с одним измерением. В этой главе мы изучали комбинацию двух условно независимых измерений. Эти две комбинации легко сочетаются. Апостериорное распределение при- нимает вид\n--- Страница 153 ---\n152  Комбинация признаков и накопление свидетельств p(s | xobs,1, xobs,2) µ p(s)p(xobs,1 | s)p(xobs,2 | s), (5.23) а PME определяется выражением (5.24) Наши рассуждения можно обобщить на несколько сигналов. Комбини- ровать N сигналов с одним и тем же лежащим в их основе стимулом так же просто, как комбинировать два. Предположим, что измерения x1, x2, , xN ус- ловно независимы при заданном s. Соответствующая порождающая модель показана на рис. 5.4. Тогда апостериорное распределение p(s | xobs,1  xobs,N ) µ p(s)p(xobs,1 | s)  p(xobs,N | s) (5.25) (5.26) где в (5.26) мы использовали обозначение произведения. Таким образом, априорное распределение умножается на вероятности, полученные из от - дельных измерений. Если измерения нормально распределены со средним значением s и стандартными отклонениями σ1, σ2, …, σN соответственно, то апостериорное распределение будет иметь среднее (5.27) и дисперсию (5.28) x1 x2 xN …s Рис. 5.4  Порождающая модель сочетания откликов со стимулом s и N условно независимыми откликами, соответствующими измерениям x1, … , xN\n--- Страница 154 ---\nНакопление свидетельств  153 5.5. Накопление свидетельств Основной способ, с помощью которого организмы улучшают свои знания о мире, – это наблюдение в течение более длительного времени. За большее время можно собрать больше свидетельств. С математической точки зрения накопление свидетельств представляет собой комбинацию сигналов с тече- нием времени и может быть описано с использованием того же математи- ческого аппарата. На самом деле накопление свидетельств часто рассмат - ривается как прототип байесовского вывода, когда апостериорный анализ обновляется на каждом временном шаге на основе новой информации. Давайте выведем формальное обоснование. Рассмотрим наблюдателя, ко- торый производит серию условно независимых, нормально распределенных измерений x1, x2, …, xT, по одному в каждый момент времени. Следователь- но, порождающая модель такая же, как на рис. 5.4, но с заменой сигналов временными точками. Далее мы предполагаем, что все измерения имеют одинаковые средние значения s и стандартные отклонения σ1, σ2, , σT со- ответственно. При этих предположениях уравнение (5.26) применимо с за- меной N на T : p(s | xobs,1  xobs,T) µ p(s)p(xobs,1 | s)  p(xobs,T | s) (5.29) (5.30) Следовательно, с теми же заменами применимы и уравнения (5.27) и (5.28). В частности, дисперсия апостериорного распределения вероятности будет непрерывно уменьшаться до 0 по мере накопления большего количества свидетельств. В контексте накопления свидетельств имеет смысл рассматривать вы- числение апостериорного распределения как рекурсивный процесс, при ко- тором апостериорное распределение после получения измерения в данный момент времени выступает в качестве априорного в следующий момент времени. Математически мы можем переписать уравнение (5.30) как урав- нение обновления p(s | xobs,1, , xobs,t+ 1) µ p(s | xobs,1, , xobs,t)p(xobs,t+ 1 | s), (5.31) где подразумевается, что p(s | xobs,1, , xobs,t) при t = 0 является априорным распределением. Другими словами, апостериорное распределение в момент времени t умножается на вероятность в момент времени t + 1, чтобы после нормирования получить апостериорное распределение в момент времени t + 1. Этот процесс называется байесовским обновлением апостериорного рас - пределения и представляет собой наиболее фундаментальную концепцию в применении байесовского моделирования к временным данным.\n--- Страница 155 ---\n154  Комбинация признаков и накопление свидетельств Моделирование накопления свидетельств сопряжено с несколькими важ - ными оговорками: прекращение. Мы не уточнили, как и когда прекращается накопление свидетельств. Это сложная проблема моделирования, которая имеет долгую историю; условная независимость. Мы предположили, что измерения в разные моменты времени условно независимы. Это предположение легко на- рушить: во многих случаях процессы с длительными временными мас - штабами (например, медленные колебания внимания) будут вызывать корреляцию измерений во времени. Тогда уравнения (5.27) и (5.28) уже не будут применяться. В частности, дисперсия апостериорной вероят - ности может асимптотически стремиться к значению, превышающе- му 0; стационарность. Еще одно важное предостережение состоит в том, что мы предполагали, что истинное состояние мира s не меняется со вре- менем. Часто сам стимул меняется по мере накопления доказательств. В главе 12 мы обсудим пример, где это действительно происходит; отношение к информации зависит от времени. В нашей модели на- копления свидетельств ранняя информация обрабатывается так же, как и более поздняя информация: временные точки можно поменять местами, не влияя на вывод. Этот подход игнорирует зависящие от времени эффекты, такие как забывание. Нормативным способом мо- делирования забывания является преднамеренное игнорирование ин- формации, которая больше не имеет отношения к задаче. Забывание естественным образом возникает в порождающей модели, в которой состояние мира меняется с течением времени. Мы рассмот рим его в главе 12. 5.6. Объединение сигналов при неоднозначности До сих пор мы рассматривали объединение сигналов в условиях сенсорного шу ма. Однако, как мы видели, неопределенность иногда возникает не из-за сенсорного шума, а из-за присущей входным данным неоднозначности. На- пример, пытаясь идентифицировать объект в руке на основе его размера и веса, мы можем столкнуться с неуверенностью не из-за сенсорного шума (при наличии достаточного времени для получения надежных измерений этих переменных), а скорее из-за того, что несколько объектов могут иметь одинаковый размер и вес. Тем не менее логика процесса вывода в этом сце- нарии одинакова: различные возможные объекты являются гипотезами (как правило, дискретными: яблоко, апельсин и т. д.), и измерение каждого при- знака (размера, веса и т. д.) имеет определенную вероятность при каждой гипотезе. В зависимости от гипотезы это функция правдоподобия отдельных признаков. Когда признаки независимы и обусловлены идентичностью объ-\n--- Страница 156 ---\nПрименение объединения сигналов  155 екта, вероятность конкретной гипотезы является произведением вероятно- стей отдельных признаков. Что усложняет дело, так это то, что признаки не стремятся быть условно независимыми. Например, даже если ограничиться апельсинами, вес и размер, как правило, сильно коррелируют. Формально это задача классификации, а не оценки. Мы обсудим задачи классификации в условиях неоднозначности в главе 8. 5.7. Применение объединения сигналов Существует давняя традиция исследовать объединение сигналов, анализи- руя, как люди складывают информацию о положении, полученную от зрения и слуха. Во многих случаях зрение очень точное (точность в пределах угловых минут дуги), в то время как слух относительно неточен (точность около 10°). Это приводит к тому, что когда доступна надежная зрительная информация, люди обычно полагаются в первую очередь на зрение – поведение, предска- зываемое уравнением (5.19). Чтобы проверить более тонкие предсказания байесовской модели, необ- ходимо было создать ситуации, в которых зрение и слух одинаково точны. В оригинальном исследовании Алаис и Берр [9] достигли этого, размывая зрительные входные данные. Авторы использовали несколько уровней раз- мытия, чтобы зрительная точность непредсказуемо менялась от испытания к испытанию. Они оценивали зрительную точность, предъявляя только зри- тельные стимулы и используя тот факт, что априорное значение является плоским, поэтому дисперсия PME только для зрения будет равна дисперсии функции правдоподобия (и распределения шума). Точно так же они предъяв- ляли лишь слуховые стимулы, чтобы оценить дисперсию слуховой функции правдоподобия. Используя полученные оценки для σA и σV, авторы предска- зали, какое значение люди будут придавать зрению и слуху при объединении этих сигналов. Они обнаружили, что поведение человека хорошо предсказы- вается уравнением (5.19) с этими весами. Более того, та же модель успешно предсказала дисперсию PME. Важная техническая деталь во многих исследованиях объединений сиг - налов заключается в том, что эксперименты обычно не требуют оценок в континууме (как на рис. 5.1), а вместо этого используют так называемую двухвариантную парадигму вынужденного выбора, в которой испытуемому предъявляются стимулы через два интервала и требуется сделать выбор меж - ду ними. Например, испытуемым предъявляют два набора аудиовизуальных стимулов и спрашивают, в каком из двух слуховой стимул был левее. Это по- зволяет исследователю оценить дисперсию (точность) сигналов способом, на который не влияет априорная оценка испытуемого. Мы рассмотрим детали этой процедуры позже. Байесовская модель объединения сигналов была проверена и на других сенсорных модальностях. Классическое исследование принадлежит Эрнсту и Бэнксу [47]. Авторы изучали оценку испытуемыми размеров объекта, ко- торый можно было как видеть, так и осязать. В нормальных условиях зре-\n--- Страница 157 ---\n156  Комбинация признаков и накопление свидетельств ние часто точнее, чем осязание; авторы размыли зрительный стимул, чтобы уменьшить его точность. Они обнаружили, что при различной зрительной точности вес, присвоенный испытуемыми зрению, был близок к значению, предсказанному уравнением (5.19). Многие эксперименты исследовали интеграцию двух сигналов, исходящих из одной сенсорной модальности. Одним из примеров является оценка на- клона (ориентация плоскости) на основе визуально наблюдаемой текстуры и бинокулярного несоответствия [95]. В типичном исследовании испытуемым показывают поверхность с текстурой, указывающей на заданный наклон. Ин - формацию о текстуре можно сделать более или менее информативной. На- пример, круги дают очень информативную подсказку, тогда как случайный белый шум дает очень неинформативную подсказку. Сигналом диспарат - ности (стереонесоответствия) также можно манипулировать и, что важно, изменять его независимо от текстуры. Независимо изменяя признаки текс - туры и диспаратности, авторы таких исследований обычно обнаруживали, что испытуемые объединяют эти признаки в соответствии с предсказания ми байесовской модели. Например, поскольку сигнал текстуры изменяется для обозначения различных наклонов, он оказывает примерно линейное вли- яние на расчетный наклон. Наклон линии графика этого влияния, т. е. вес текстуры, хорошо согласуется с предсказанием байесовской модели. Когда мы хотим оценить положение нашей руки в двумерной плоскости, например на столе, мы должны решить задачу двумерной оценки. Чтобы выполнить эту оценку, мы можем использовать проприорецепторы, которые сигнализируют о положении тела. Мы также можем использовать зрение. Проприоцептивные и зрительные сигналы положения рук имеют разные свойства. Проприоцепция, как правило, шумная, но хорошо оценивает из- менения в положении наших мелких суставов. Зрение довольно хорошо ра- ботает при оценке направления, но плохо оценивает глубину. В основопо- лагающем труде ван Бирс и его сотрудники [184] исследовали, как нервная система сочетает визуальные и проприоцептивные сигналы в этой задаче. Было обнаружено, что объединение сигналов происходит почти точно так, как предсказывает байесовская модель. Мы упоминали искусственный конфликт сигналов при аудиовизуальной ло- кализации. Известным примером искусственного конфликта сигналов в аудио- визуальном восприятии речи является эффект Мак-Гурка [125]. Когда наблюда- тель слышит, как кто-то говорит «baba», в то время как он смотрит видео, где тот же человек говорит «gaga», у него может возникать восприятие человека, говорящего «dada». Демонстрации эффекта Мак-Гурка можно найти в интерне- те. Эффект Мак-Гурка можно истолковать как пример объединения сигналов, когда наблюдатель делает вывод о едином общем состоянии мира на основе слуховых и зрительных наблюдений. Наблюдатель, как правило, не осознает конфликта, пока не услышит звук с закрытыми глазами. Аудиовизуальное вос - приятие речи моделировали с помощью байесовских моделей [18, 117]. Наконец, объединение сигналов может иметь место и между отдельными людьми. Увлекательное исследование [15] демонстрирует, как два человека объединяют информацию о зрительном стимуле посредством вербального общения.\n--- Страница 158 ---\nРекомендуемая литература  157 5.8. Заключение В этой главе мы представили модели, в которых необходимо объединять не- сколько сигналов. Вы узнали следующее: объединение сигналов – это частая и важная перцептивная деятель- ность, которая нередко происходит автоматически и вне нашего со- знательного контроля; как и при объединении априорного распределения с правдоподобием, все, что нужно сделать байесовскому наблюдателю, – это перемножить два распределения вероятностей и нормировать их; в отличие от стратегии «победитель получает все», оптимальное байе- совское решение (оценка апостериорного среднего) заключается во взвешивании каждого сигнала в соответствии с его надежностью; байесовская модель учитывает «человеческий» характер данных в са- мых разных ситуациях; объединение сигналов показывает, что плоская априорная вероятность не мешает байесовской модели быть интересной и полезной; объединение сигналов может быть протяженным во времени, и в этом случае его иногда называют накоплением свидетельств, интеграцией свидетельств или принятием решения. При последующих измерениях неопределенность уменьшается. PME представляет собой линейную комбинацию отдельных измерений, взвешенных по их точности. 5.9. Рекомендуемая литература Bahador Bahrami et al. Optimally Interacting Minds. Science 329, no. 5995 (2010): 1081–1085. Marc O. Ernst and Martin S. Banks. Humans Integrate Visual and Haptic Infor­ mation in a Statistically Optimal Fashion. Nature 415, no. 6870 (2002): 429–433. Vikranth Rao Bejjanki, Meghan Clayards, David C. Knill, and Richard N. As- lin. Cue Integration in Categorical Tasks: Insights from Audio­ Visual Speech Perception. PloS One 6, no. 5 (2011): e19812. Anne-Marie Brouwer and David C. Knill. The Role of Memory in Visually Guided Reaching. Journal of Vision 7, no. 5 (2007): 1–12. L. Yuille and Heinrich H. Bulthoff. Bayesian Decision Theory and Psychophysics. Perception as Bayesian Inference, edited by David C. Knill and Whitman Richards. 123–162. Cambridge: Cambridge University Press, 1996. David Alais and David Burr. The Ventriloquist Effect Results from Near­Optimal Bimodal Integration. Current Biology 14, no. 3 (2004): 257–262. Robert A. Jacobs. Optimal Integration of Texture and Motion Cues to Depth. Vi- sion Research 39, no. 21 (1999): 3621–3629. David C. Knill and Jeffrey A. Saunders. Do Humans Optimally Integrate Stereo and Texture Information for Judgments of Surface Slant? Vision Research 43, no. 24 (2003): 2539–2558.\n--- Страница 159 ---\n158  Комбинация признаков и накопление свидетельств Harry McGurk and John MacDonald. Hearing Lips and Seeing Voices. Nature 264, no. 5588 (1976): 746–748. Julia Trommershäuser, Konrad P . Körding, and Michael S. Landy. Sensory Cue Integration. Oxford: Oxford University Press, 2011. Robert J. van Beers, Anne C. Sittig, and Jan J. van der Gon Denier. How Hu mans Combine Simultaneous Proprioceptive and Visual Position Information. Expe ri- mental Brain Research 111, no. 2 (1996): 253–261. 5.10. Задачи Задача 5.1. Наблюдатель объединяет условно независимые сигналы A и B с гауссовым шумом измерения. Когда B станет более надежным: (a) оценка наблюдателя сместится в сторону А ; (b) оценка наблюдателя сместится в сторону B ; (c) оценка наблюдателя останется прежней; (d) недостаточно информации для ответа. Задача 5.2. Верны ли утверждения? Объясните свой ответ. (a) В модели объединения сигналов в этой главе предполагается, что изме- рения независимы друг от друга. (b) Конфликты между двумя измерениями, генерируемыми одним источни- ком, редко возникают в восприятии реального мира. Задача 5.3. Эта задача была предложена в 2016 г. Ником Джонсоном, в то время аспирантом Нью-Йоркского университета. В рамках модели объедине - ния двух независимых сигналов с гауссовым шумом измерения при плоском априорном распределении: (a) дисперсия апостериорного распределения вероятности никогда не пре- вышает 50 % наибольшей дисперсии вероятностей одиночного сигнала. Покажите это, используя уравнения; (b) дисперсия апостериорного распределения вероятности всегда состав- ляет от 50 до 100 % наименьшей дисперсии вероятностей одиночного сигнала. Покажите это, используя уравнения; (c) верно ли утверждение (a), если априорное распределение является гаус - совым, а не плоским? Если да, то докажите. Если нет, приведите контр- пример; (d) верно ли утверждение (b), если априорное распределение является гаус - совым, а не плоским? Если да, то докажите. Если нет, приведите контр- пример. Задача 5.4. Предположим, что p1(x), p2(x), …, pN(x) – гауссовы распределения, где pi(x) (для каждого i = 1, …, N) имеет среднее значение μi и точность Ji. Мы умножаем эти распределения, затем нормируем: q(x) = kp1(x)p2(x)  pN(x), (5.32)\n--- Страница 160 ---\nЗадачи  159 где k такое, что q(x) нормировано. Нам нужно показать, что q(x) является гауссовым распределением со средним значением (5.33) и точностью (обратная дисперсия) (5.34) (a) Покажите это, используя прямой расчет, как в главе 3. (b) В качестве альтернативы проведите доказательство по индукции; для N = 1 уравнения тривиально верны. Предположите, что оно верно для некоторого N, и докажите, что в таком случае оно было бы верно и для N + 1. (c) Используя результат предыдущего задания, выведите уравнение (5.24). Задача 5.5. Эта задача основывается на разделе 5.5 о накоплении дока- зательств. Наблюдатель делает вывод о стимуле s из последовательности измерений xobs,1, xobs,2, …, xobs,T, сделанных в ходе одной попытки. Распреде- ление стимулов является гауссовым со средним значением μ и дисперсией σs2. Распределение t-го измерения, p(xt | s), является гауссовым со средним значением s и дисперсией σ2 (одинаково для всех измерений); мы предпо- лагаем условную независимость. (a) Каковы среднее и дисперсия апостериорного распределения? Вы можете начать с уравнений в разделе 5.4. (b) Для данного стимула s мы определяем относительное смещение как разницу между средней PME и самим s, деленную на разницу между средним значением распределения стимула и s. Получите выражение для относительного смещения с точки зрения μ, σ, σs и T. Максимально упростите выражение. (c) Интерпретируйте выражение в (b): объясните логически, какой смысл имеют зависимости от переменных. (d) Вычислите дисперсию PME для данного s . (e) Постройте эту дисперсию как функцию T для всех девяти комбинаций σs ∈ {1, 2, 5} и σ ∈{1, 2, 5}. Создайте один график для каждого значения σs, т. е. три графика, каждый из которых содержит три кривые (с цветовой кодировкой). (f) Интерпретируйте графики, построенные в задании (e): объясните, какой смысл несет в себе форма каждого графика. Задача 5.6. В этой задаче мы исследуем субоптимальную оценку в контексте объединения сигналов. Предположим, что наблюдатель оценивает стимул s по двум условно независимым измерениям xobs,1 и xobs,2, распределенным по Гауссу. Априорное распределение плоское. (a) Для начала вспомните об оптимальной оценке. Выразите PME в кон текс - те измерений.\n--- Страница 161 ---\n160  Комбинация признаков и накопление свидетельств (b) Какова дисперсия PME между испытаниями? (c) Теперь предположим, что наблюдатель использует оценку вида sˆ = wxobs,1 + (1 - w)xobs,2, где w может быть любой константой. Покажите, что эта оценка не смещена (как и PME); это означает, что среднее значение оценки для заданного s равно s . (d) Какова дисперсия этой оценки в зависимости от w? Постройте график данной функции. При каком значении w она минимальна и имеет ли это значение смысл? Сформулируйте окончательный вывод. Задача 5.7. В главе 3 и в этой главе мы смогли вывести аналитические вы- ражения для апостериорного распределения и распределения откликов. Од- нако для более сложных психофизических задач (например, далее в нашей книге) аналитических решений часто не существует, но мы все же можем использовать численные методы. Чтобы познакомиться с такими методами, мы проработаем модель комбинации сигналов, упомянутую в этой главе, используя численные методы. Мы предполагаем, что экспериментатор вво- дит сигнальный конфликт между слуховым и зрительным стимулами: s1 = 5 и s2 = 10. Стандартные отклонения слухового и зрительного шумов равны σ1 = 2 и σ2 = 1 соответственно. Мы предполагаем плоское априорное рас - пределение s . (a) Случайным образом выберите слуховое измерение xobs1 и визуальное измерение xobs2 из их соответствующих распределений. (b) Нанесите на один рисунок соответствующие элементарные функции правдоподобия p (xobs,1 | s) и p (xobs,2 | s). (c) Рассчитайте комбинированную функцию правдоподобия p(xobs,1, xobs,2 | s) путем перемножения элементарных функций правдоподобия. Построй- те эту функцию. (d) Рассчитайте апостериорное распределение путем численного нормиро- вания комбинированной функции правдоподобия. Постройте это рас - пределение на том же рисунке, что и функции правдоподобия. (e) Численно найдите PME s, то есть такое значение s, при котором апосте- риорное распределение максимально. (f) Сравните с PME s, рассчитанной по уравнению (5.14) с использованием измерений, приведенных в (а). (g) В приведенном выше примере мы смоделировали одно испытание и вычислили PME наблюдателя для s с учетом зашумленных измерений в этом испытании. Если аналитического решения для распределения PME не существует, мы можем повторить описанную выше процедуру много раз, чтобы аппроксимировать это распределение. Здесь мы также практикуем этот вычислительный метод, хотя в данном случае доступно аналитическое решение. Выберите 100 пар (xobs,1, xobs,2) и численно най- дите PME наблюдателя для каждой пары, как в (e). (h) Вычислите среднее значение PME, полученных в (g), и сравните со сред- ней оценкой, предсказанной с использованием уравнения (5.19). (i) Постройте гистограмму PME.\n--- Страница 162 ---\nЗадачи  161 (j) Относительное ( слуховое) смещение определяется как среднее значение PME минус истинный слуховой стимул, деленное на истинный визуаль- ный стимул минус истинный слуховой стимул. Вычислите относительное слуховое смещение для ваших оценок. Задача 5.8. Основным допущением при выводе нашей модели объединения сигналов было то, что измерения x1 и x2 условно независимые (уравнение 5.1). Рассмотрим обобщение, в котором это не так. Заменим уравнение (5.19) дву- мерным нормальным распределением с одинаковыми средними значениями s для обоих измерений, стандартными отклонениями σ1 и σ2 и корреляцией ρ: (5.35) Предположим, что априорное распределение плоское. (a) Этап 2: выведите уравнения для среднего и дисперсии апостериорного распределения по s. Подсказка: следуйте шагам задачи 3.4 (дополнение до полного квадрата). (b) Выполните проверку работоспособности, установив ρ = 0, что соответ - ствует условно независимым измерениям. Вы должны получить уравне- ния (5.27) и (5.28). (c) Постройте дисперсию апостериорного распределения как функцию от ρ для σ1 = 1, σ2 = 2. (d) Интерпретируйте график. В частности, объясните, какой смысл имеют зависимости переменных. Задача 5.9. Объединение данных опроса нескольких людей для получения среднего значения генеральной совокупности также можно считать объ- единением сигналов. Предположим, социолог спрашивает респондентов, в какой степени они согласны или не согласны с тем или иным утвержде- нием. Для простоты мы представляем степень согласия вещественной пере- менной x. Поскольку люди в популяции различаются, мы моделируем x как следующую нормальному распределению со средним значением s и диспер- сией σ2. Дисперсия обычно зависит от того, как и когда собираются данные. Для социолога интерес представляет переменная s. Опросник собирает от - веты xobs,1, xobs,2, …, xobs,N от N человек; эти ответы предполагаются условно независимыми. (a) Покажите математически, что (нормированная) функция правдоподо- бия s является нормальным распределением со средним значением, рав- ным выборочному среднему ответов, обозначенному x, и дисперсией . Следовательно, MLE s – это просто x . (b) Агрегатор опросов пытается выполнить «объединение сигналов» одного опроса с респондентами N1 и индивидуальной дисперсией σ12 и другого с респондентами N2 и индивидуальной дисперсией σ22 (теперь нижние\n--- Страница 163 ---\n162  Комбинация признаков и накопление свидетельств индексы относятся к разным опросам, а не к отдельным лицам в опросе). Обозначим соответствующие MLE через sˆ1 и sˆ2. Покажите, что комбини- рованная MLE s равна (5.36) где (5.37) для i = 1, 2. (a) Объясните этот результат.",
      "debug": {
        "start_page": 141,
        "end_page": 163
      }
    },
    {
      "name": "Глава 6. Обучение как вывод 163",
      "content": "--- Страница 164 --- (продолжение)\nГлава 6 Обучение как вывод Как мы учимся из наблюдений? В большей части этой книги мы предполагаем, что порождающая модель и ее параметры уже известны субьекту, принимающему решения. В экспе- риментах этого можно достичь за счет комбинации предоставления четких инструкций, воздействия на испытуемого большим количеством стимулов и предоставления блока обучающих испытаний (данные которых не анали- зируются). Но процесс обучения порождающих моделей сам по себе является обширной областью исследований и послужит темой этой главы. Краткое содержание главы Мы начнем с обсуждения того, как наблюдатель может узнать вероятность бинарного события из повторных наблюдений. Будет показано, что апосте- риорную вероятность можно записать в рекурсивной форме, обновляя ее после каждого временного шага. Также будет показано, что при заданном числе наблюдений каждого исхода порядок наблюдений влияет на эволюцию апостериорного распределения, но не меняет окончательное апостериорное распределение. Далее мы рассмотрим неоднородные и сопряженные априор- ные распределения, а также связь между байесовским обуче ни ем и обуче ни- ем с подкреплением. Затем обсудим повышение точности гауссова распреде- ления на основе наблюдений, изучение параметра отношения между двумя переменными и изучение безусловно различных причинно-следственных моделей мира. В разделе 6.3 будет описана связь между байесовским обуче- ни ем и обуче ни ем с подкреплением. 6.1. Множество форм обучения Нас повсюду окружает обуче ние. Все высшие живые существа учатся, и обуче- ние происходит во всех областях поведения, от получения навыка различать объекты и управлять своими мышцами в младенчестве до изучения нового языка. Обучение позволяет наблюдателю или агенту лучше выполнять за- дачу или лучше адаптироваться к окружающей среде. Некоторые формы\nГлава 6 Обучение как вывод Как мы учимся из наблюдений? В большей части этой книги мы предполагаем, что порождающая модель и ее параметры уже известны субьекту, принимающему решения. В экспе- риментах этого можно достичь за счет комбинации предоставления четких инструкций, воздействия на испытуемого большим количеством стимулов и предоставления блока обучающих испытаний (данные которых не анали- зируются). Но процесс обучения порождающих моделей сам по себе является обширной областью исследований и послужит темой этой главы. Краткое содержание главы Мы начнем с обсуждения того, как наблюдатель может узнать вероятность бинарного события из повторных наблюдений. Будет показано, что апосте- риорную вероятность можно записать в рекурсивной форме, обновляя ее после каждого временного шага. Также будет показано, что при заданном числе наблюдений каждого исхода порядок наблюдений влияет на эволюцию апостериорного распределения, но не меняет окончательное апостериорное распределение. Далее мы рассмотрим неоднородные и сопряженные априор- ные распределения, а также связь между байесовским обуче ни ем и обуче ни- ем с подкреплением. Затем обсудим повышение точности гауссова распреде- ления на основе наблюдений, изучение параметра отношения между двумя переменными и изучение безусловно различных причинно-следственных моделей мира. В разделе 6.3 будет описана связь между байесовским обуче- ни ем и обуче ни ем с подкреплением. 6.1. Множество форм обучения Нас повсюду окружает обуче ние. Все высшие живые существа учатся, и обуче- ние происходит во всех областях поведения, от получения навыка различать объекты и управлять своими мышцами в младенчестве до изучения нового языка. Обучение позволяет наблюдателю или агенту лучше выполнять за- дачу или лучше адаптироваться к окружающей среде. Некоторые формы\n--- Страница 165 ---\n164  Обучение как вывод обучения основаны на восприятии; например, опытный футболист сможет гораздо лучше, чем новичок, предсказать, куда приземлится мяч. Другие более познавательны, например вы сейчас учитесь байесовскому модели - рованию. Некоторые формы обучения включают в себя запоминание или подражание – например, когда молодая певчая птица заучивает свою песню от родителей, – в то время как другие опираются на обратную связь о состоя- нии мира. Большая часть обучения происходит даже в отсутствие какой-либо обратной связи, например младенцы учатся частично анализировать разго- ворную речь, отслеживая частоты совпадения слогов. Обучение происходит в различных временных масштабах. Мы можем изучить строение незнако- мого пространства за секунды. Чтобы изучить чьи-то черты характера, иног - да требуется одно действие, а иногда – годы совместной жизни. Основное изучение статистических свойств окружающей среды происходит во время развития человека, но продолжается на протяжении всей его жизни, поко- лений и даже эволюции в целом. В байесовском контексте основной формой обучения является изучение параметра порождающей модели. Например, к моменту начала экспери - мента субъект обычно не знает точное распределение стимулов, а предва- рительные инструкции не могут передать эту информацию. Ключевая идея этой главы заключается в том, что изучение такого параметра само по себе является процессом вывода: параметр берет на себя роль состояния мира в предыдущих главах. Таким образом, мы повторно используем механизм, представленный ранее в другом контексте. Сложность заключается в том, что обуче ние и обычный вывод часто должны происходить одновременно: хотя наблюдатель еще не уверен в параметрах порождающей модели, ему все равно приходится принимать решения для вывода на основе этой модели. Мы устраним это осложнение. Существует тесная связь между изучением ценности фиксированного со- стояния мира и накоплением свидетельств с течением времени; об этом говорилось в разделе 5.5. Фактически базовая форма порождающей модели идентична: единое мировое состояние, порождающее условно независимые наблюдения. При процессе, который обычно называют накоплением свиде- тельств, интеграция осуществляется в более коротком временном масштабе (от десятков миллисекунд до секунд), при этом все наблюдения производятся в рамках одного эксперимента. Состояние мира по-прежнему меняется в ис - пытаниях на более долгом временном отрезке. В экспериментальных усло- виях обычно не требуется никаких суждений, пока не завершено накопление свидетельств. При процессе, который обычно называют обуче ни ем, каждое испытание соответствует одному наблюдению, а состояние мира может быть зафиксировано в гораздо более разнообразном временном масштабе, от се- кунд до лет. В экспериментальных условиях ход обучения можно исследо- вать, попросив испытуемого высказать свое мнение по каждому испытанию. Примечание об обозначениях: в главе 5 мы исключили индекс «hyp» из shyp на этапе 2 байесовского моделирования. С этого момента мы также бу - дем опускать нижний индекс «obs» (observed, наблюдаемый), чтобы указать конкретную ценность наблюдения на этапе 2. Он использовался в учебных целях, но усложняет запись и обычно не применяется в научной литературе.\n--- Страница 166 ---\nИзучение вероятности бинарного события  165 Вы должны иметь в виду, что на этапе 1 наблюдение или измерение всегда является случайной переменной, тогда как на этапе 2 это всегда конкретное значение этой переменной. (И чтобы запутать ситуацию еще больше, на эта- пе 3 это конкретное значение снова становится переменной.) Кроме того, мы в основном будем отбрасывать нижний индекс, обозначающий случайную величину, из обозначения распределения вероятностей; например, мы будем писать p(s) вместо ps(s). Подробнее об этом соглашении см. в приложении A. 6.2. Изучение вероятности бинарного события Представьте, что вы потерпели кораблекрушение и оказались на необитае- мом острове без источников пресной воды. Хорошая новость в том, что часто идут дожди. Плохая новость заключается в том, что интервал между дождли- выми днями непредсказуем. Чтобы распределить собранную дождевую воду, вы хотели бы оценить вероятность того, что в данный день пойдет дождь. Через три дня вы заметили закономерность: сухо – дождь – сухо. Ваше лучшее предположение о вероятности на данный момент может быть 1/3. Однако, учитывая небольшое количество дней, проведенных на острове, вы не очень уверены в этом предположении. Как опытный байесовский экс - перт, у которого много свободного времени, вы хотели бы количественно оценить эту неопределенность или, что еще лучше, рассчитать апостериор- ное распределение вероятностей того, что пойдет дождь. Другими словами, какова вероятность того, что вероятность дождя равна 0.1? 0.3? 0.5? 0.9? И так далее. До сих пор в этой книге мы еще не рассматривали вероятность как интересующую нас переменную состояния мира, но она вполне законна, очень распространена в реальном мире и на самом деле не меняет матема- тическую базу. Единственное, к чему нужно привыкнуть, – так это к немного неуклюжему выражению «вероятностное распределение вероятности». На этапе 1 мы должны сформировать порождающую модель. Графовая форма модели показана на рис. 6.1B. Она содержит переменную верхнего уровня, представляющую вероятность дождя в данный день (т. е. потенци- альную дождливость), которую мы будем обозначать через r. Как упомина- лось во вступлении к главе, мы предполагаем, что это мировое состояние не меняется в течение очень долгого времени; в нашем примере это может означать, что в течение месяца вероятность дождя постоянна. Очевидно, что это упрощение, и также возможно изучить медленно меняющееся состояние мира; мы рассмотрим эту ситуацию позже. Порождающая модель очень по- хожа на модель комбинации сигналов (рис. 5.4), но природа переменных иная: вероятность вместо реального состояния мира, бинарные значения вместо действительных измерений и временные точки вместо сигналов.\n--- Страница 167 ---\n166  Обучение как вывод x1 x2 xtr Погода в t­й день (сухо/дождь)(A) (В) Рис. 6.1  Изучение вероятности бинарного события: (A) изо- бражение для украшения этой главы; (B) порождающая модель задачи изучения вероятности r на основе t условно независимых бинарных наблюдений Мы должны сделать предположение об априорном распределении r. Вы можете заметить, что в мире мало мест, где дождь идет почти каждый день (скажем, r > 0.9) или почти никогда не идет (скажем, r < 0.1). В отсутствие каких-либо наблюдений вполне можно предположить, что r, скорее всего, лежит в интервале [0.4, 0.6]. Это примеры аспектов априорного распреде- ления r. На данный момент мы предполагаем полностью равномерное рас - пределение по r : р(r) = 1. (6.1) Упражнение 6.1. Почему это правильно нормированное распределение? В нижней части порождающей модели расположена последовательность бинарных наблюдений, соответствующих наблюдаемой погоде (дождь/сухо) в дни с 1 по t, которую мы обозначаем через x1, x2, …, xt. Каждое xi может быть равно 0 (сухо) или 1 (дождь). В определенный день p(xi = 1) = r; (6.2) p(xi = 0) = 1 - r. (6.3) Это пример процесса Бернулли: случайная величина с двумя возможными исходами, каждый из которых имеет фиксированную вероятность. Наиболее известным примером процесса Бернулли является подбрасывание монеты, и по аналогии с ним мы моделируем противостояние дождя и засухи как взве - шенное подбрасывание монеты. У нас есть не одно наблюдение, а их последо- вательность, по одному на каждый день. Для простоты вы предполагаете, что погода условно независима, т. е. зависит только от r и не зависит от погоды в предыдущие дни. (В действительности погода в разные дни очень далека от условной независимости, и в некоторых местах могут быть длительные периоды засушливых дней по сравнению с дождливыми.) Затем мы можем записать распределение вероятностей, связанное с последовательностью: p(x1, x2, …, xt | r) = p(x1 | r)p(x2 | r) ··· p (xN | r), (6.4) которое также можно записать в виде произведения\n--- Страница 168 ---\nИзучение вероятности бинарного события  167 (6.5) На этом определение порождающей модели завершено. Теперь мы готовы сделать вывод! На этапе 2 мы вычисляем апостериорное распределение по r с учетом ваших имеющихся наблюдений (сухо – дождь – сухо), которое мы будем обозначать через x . Начнем с правила Байеса: p(r | x) µ p(r)p(x | r). (6.6) Теперь мы используем уравнение (6.1) для априорной вероятности и урав - нение (6.4) для вероятности: p(r | x) µ p(x1 | r)p(x2 | r) ··· p (xt | r). (6.7) Сейчас мы понимаем, что каждый множитель в этом произведении равен либо r, либо 1 - r, поскольку это единственные возможные значения. Таким образом, мы можем упростить выражение до p(r | x) µ rnrain(1 - r)ndry, (6.8) где nrain и ndry – количество дождливых и сухих дней соответственно, наблю- давшихся до сих пор (с nrain + ndry = t). Этот тип распределения называется бета­распределением (см. примечание 6.1). Нормировка обеспечивается так называемой бета­функцией, но это не существенно для нашего понимания. Что важно в уравнении (6.8) для апостериорного распределения, так это то, что каждый раз, когда вы наблюдаете дождливый день, оно умножается на возрастающую функцию r, а именно f(r) = r, и каждый раз, когда вы наблю- даете сухой день, оно умножается на убывающую функцию, а именно f(r) = 1 - r. Мы можем назвать эти отдельные множители моментальными, или мгновенными, функциями правдоподобия, поскольку они привязаны к допол- нительному наблюдению в один момент времени. На рис. 6.2 показана эво- люция апостериорной вероятности при наблюдениях «сухо – дождь – сухо». Ежедневная интеграция новых данных – это применение правила Байеса с использованием функции правдоподобия дня. День 1: сухо День 2: дождь День 3: сухо1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.5 1.0 0.0 0.5 1.0Правдоподобие Апостериорное распределение(C) (B) (A) Правдоподобие Априорное распределениеДень 1 День 2 День 3Априорное распределение День 1 День 2 День 3 Вероятность дождя r Вероятность дождя r Рис. 6.2  Эволюция апостериорной вероятности по вероятности Бернулли: (A) наблюдения в дни 1, 2 и 3; (B) мгновенные вероятности в дни 1, 2 и 3; (C) апостериорные распределения в дни 1, 2 и 3\n--- Страница 169 ---\n168  Обучение как вывод Примечание 6.1 Бета-распределение Бета-распределение определяется случайной величиной Y, которая принимает значения от 0 до 1. Чаще всего эта случайная величина сама является вероятностью, поэтому бета-распределение – это распределение вероятностей по вероятностной переменной. Плотность вероятности бета-распределения равна (6.9) Здесь α и β – параметры распределения, причем оба должны быть положительны- ми; B(·,·) обозначает бета­функцию – специальную функцию, роль которой (и фак - тически определение) заключается в нормировании бета-распределения. Для це- лей этой книги (и для большей части байесовского моделирования) нам больше ничего не нужно знать о бета-функции. На случай, если вам это понадобится, все распространенные пакеты численных расчетов имеют предварительно запрограм- мированную бета-функцию. Несколько примеров бета-распределения показаны на рис. 6.3. Среднее значение бета-распределенной случайной величины y равно (6.10) а дисперсия (6.11) 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0 0.0 0.2 0.4 0.6 0.8 1.0Вероятностьα = 1; β = 1 α = 3; β = 1 α = 1; β = 2 α = 2; β = 2 α = 2.5; β = 4 Рис. 6.3  Примеры бета­распределения для различных комбинаций параметров 6.2.1. Прогноз Вычислив апостериорное распределение p(r | x1, x2, …, xt) = p(r | x) и изнемогая от жажды после нескольких дней пребывания на необитаемом острове, вы\n--- Страница 170 ---\nИзучение вероятности бинарного события  169 наверняка зададитесь вопросом: какова вероятность того, что на следующий день пойдет дождь? Это пример проблемы, о которой мы упоминали в раз- деле 6.1: наблюдатели должны делать выводы, используя порождающую мо- дель, все еще будучи неуверенными в параметрах этой модели. Решение со- стоит в усреднении (интегрировании) по неизвестному параметру (здесь r): мы вычисляем вероятность любого будущего результата, интегрируя его вероятность при каждом состоянии мира, умноженную на апостериорную вероятность этого состояния мира: (6.12) По сути, мы утверждаем, что наша вера в то, что завтра будет дождь, – это наша вера в то, что r имеет заданное значение и пойдет дождь, если r имеет это значение, или что r имеет другое значение и пойдет дождь, если r имеет другое значение, и т. д. Результат называется апостериорным прогностиче- ским распределением (posterior predictive distribution), а среднее значение представляет собой пример маргинализации (marginalization), которую мы более подробно обсудим в главе 8 (см. также приложение, раздел B.11.2). Поскольку здесь мы имеем дело с бинарным исходом, вероятность кото- рого при заданном r равна просто самому r, наш интеграл сводится к вы- ражению (6.13) Внимательный читатель узнает в нем формулу среднего значения апосте- риорного распределения. Таким образом, вероятность, с которой мы ожи- даем дождя на следующий день, является оценкой апостериорного среднего (PME) нормируемого параметра r . Исходя из того, что уравнение (6.8) является бета-распределением, из уравнения (6.10) для среднего значения бета-распределенной переменной и того факта, что nrain + ndry = t, мы находим, что оценка апостериорного среднего r равна (6.14) Это выражение было получено в XVIII веке Пьером-Симоном Лапласом и известно как правило последовательности Лапласа. Эта оценка апостери- орного среднего отличается от оценки апостериорной моды (оценки MAP) (6.15) Этап 3 неприменим, так как мы не рассматриваем ни внутренний шум, ни шум решения. Другими словами, оценке не присуща изменчивость.\n--- Страница 171 ---\n170  Обучение как вывод 6.2.2. Уравнения обновления Точно так же, при накоплении свидетельств, которое мы обсуждали в раз- деле 5.5, мы можем записать апостериорную вероятность в рекурсивной форме, обновляя ее после каждого временного шага. В частности, аналог уравнения (5.31) выглядит так: p(r | x1, …, xt+1) µ p(r | x1, …, xt)p(xt+1 | r), (6.16) где подразумевается, что p(r | x1, …, xt) при t = 0 является априорным рас - пределением. Это возможно благодаря допущению условной независимости в уравнении (6.4), которое воплощает нашу веру в то, что характер дня за- висит только от общей вероятности дождя, что все дни условно независи- мы при данной вероятности. Однако запись апостериорного распределения с использованием уравнения обновления не так полезна, поскольку явное решение – уравнение (6.8) – является очевидным. 6.2.3. Неопределенность Как и в разделе 3.4.1, стандартное отклонение апостериорной вероятности можно использовать в качестве меры неопределенности. В нашем случае стандартное отклонение: (6.17) Упражнение 6.2. Покажите это, используя уравнение (6.11) для дисперсии бета-распределенной случайной величины. 6.2.4. Биномиальное распределение Порядок наблюдения (дождливые дни и засушливые дни) влияет на эволю- цию апостериорного распределения, но не на окончательное апостериорное распределение. Например, апостериорное распределение для наблюдений «сухо – дождь – сухо» будет идентичным для наблюдений «дождь – сухо – сухо» или «сухо – сухо – дождь». Другими словами, мы могли бы на этапе 1 суммировать последовательность бинарных наблюдений как один подсчет. Тогда порождающая модель приобрела бы вид (6.18) Это пример биномиального распределения. Оно определяется по отсчетам (целым числам), начиная с 0. Если бы мы использовали эту порождающую модель, она не изменила бы никакую часть нашего вывода на этапе 2.\n--- Страница 172 ---\nИзучение вероятности бинарного события  171 Упражнение 6.3. Почему не изменила бы? 6.2.5. Неравномерное априорное распределение До сих пор мы предполагали равномерное распределение по r (6.1). При обобщенном подходе обычно полагают, что априорное распределение яв- ляется бета-распределением: p(r) µ rα0−1(1 - r)β0−1, (6.19) где α0 и β0 – параметры (оба должны быть положительными), а нормировку мы опустили. Когда α0 = β0 = 1, априорное распределение является равномер- ным. С учетом уравнения (6.19) можно сказать, что апостериорное распре- деление по r тоже является бета-распределением, но теперь с параметрами α = α0 + nrain, (6.20) β = β0 + ndry. (6.21) Упражнение 6.4. Покажите математически, почему это так. Давайте подведем итоги того, что мы сделали. Ранее мы показали, что правдоподобие, соответствующее модели Бернулли или биномиальной по- рождающей модели, пропорционально бета-распределению. Теперь мы по- казали, что умножение бета-априорного распределения на бета-правдопо- добие дает апостериорное бета-распределение. Это элегантно, потому что, переходя от априорного распределения к апостериорному, нам нужно обно- вить только параметры распределения, как мы делали в главе 3 при переходе от нормального априорного к нормальному апостериорному распределению. Априорное распределение, которое таково, что апостериорное является членом того же параметрического семейства распределений, что и априор- ное, называется сопряженным априорным для этой порождающей модели. Другими словами, бета-распределение является сопряженным априорным распределением Бернулли (или биномиальным). Но разве можно обосновать выбор априорного распределения просто со- ображениями элегантности? Разве оно не должно отражать статистические характеристики мира? И да, и нет. На практике трудно узнать или задать априорное распределение вероятности, которое наблюдатель хранит в сво- ей голове. Лучшее, что мы часто можем сделать, – это выбрать достаточно гибкое распределение (обычно это означает наличие по крайней мере двух параметров) и подогнать параметры к данным отдельного субъекта (и, если применимо, к конкретным экспериментальным условиям). С этой точки зре - ния бета-априорное распределение ничем не лучше любого другого вариан- та, и в таком случае элегантность может стать решающим фактором. Наконец, несмотря на то что пребывание на острове может не иметь осо- бого значения в качестве экспериментальной парадигмы, многие важные задачи имеют одинаковую математическую структуру. Например, в повто- ряющейся игре на доверие субъект может выяснять, насколько заслуживает\n--- Страница 173 ---\n172  Обучение как вывод доверия его партнер, исходя из бинарных наблюдений. В подобной ситуации меняющееся доверие заняло бы место вероятности дождя в нашем примере с островом. 6.3. Связь между байесовским обучением и обучением с подкреплением Существует интересная связь между байесовским обуче ни ем и обуче ни ем с подкреплением. В разделе 6.2.2 мы записали уравнение обновления для апостериорного распределения, когда априорное является равномерным, – уравнение (6.16). Аналогичным образом мы могли бы записать рекурсив- ное уравнение обновления для оценки апостериорного среднего в момент времени t (которая представляет собой ожидаемое значение того, будет ли дождь в момент времени t + 1): (6.22) (Здесь мы опустили нижний индекс «PM» у rˆ, чтобы упростить запись.) Это уравнение напоминает правило Рескорлы–Вагнера (Rescorla-Wagner rule) в обуче нии с подкреплением (примечание 6.2), согласно которому значение состояния обновляется на основе ошибки предсказания – разницы между полученным и предсказанным вознаграждениями. В уравнении (6.22) ожи- дание дождя после дня t - 1 выступает в качестве значения состояния в мо- мент t, а то, действительно ли шел дождь в день t, действует как полученное вознаграждение. Множитель действует как скорость обучения, которая показывает, в какой степени прогноз обновляется. Интуитивно понятно, что скорость обучения уменьшается по мере увеличения t: поскольку прогноз основан на большем количестве наблюдений, любое единичное наблюдение имеет меньшее влияние на прогноз. Это верно и за пределами этого конкрет - ного примера: байесовское правило обучения обычно имеет скорость обуче- ния, которая уменьшается со временем определенным образом, полностью продиктованным предположениями модели. Эта зависимость от времени контрастирует со стандартным правилом Рескорлы–Вагнера, в котором ско- рость обучения по умолчанию является постоянной (хотя это правило можно обобщить на переменную скорость обучения). Мы докажем уравнение (6.22) в задаче 6.2, а пока воспользуемся числовым примером. Поскольку мы начинаем с равномерного априорного распределе- ния, априорное ожидание дождя равно Допустим, в первый день ваше- го пребывания на острове идет дождь. Тогда x1 = 1, и «ошибка предсказания» равна Апостериорное ожидание дождя после 1-го дня равно\n--- Страница 174 ---\nСвязь между байесовским обучением и обучением с подкреплением  173 что действительно является ожидаемым значением в соот - ветствии с апостериорным распределением p (r|s1) µ r. Хотя здесь мы связали байесовское обуче ние и обуче ние с подкреплением, необходимо сделать несколько оговорок. Во-первых, связь была только на уровне вывода текущего, а не полного апостериорного распределения. Пра- вило Рескорлы–Вагнера и многие другие модели обучения с подкреплением не рассматривают неопределенность явно, хотя попытки исправить это про- должаются. Во-вторых, хотя в нашем случае ожидание дождя естественным образом интерпретировалось как ценность, не всякое байесовское обуче ние можно интерпретировать как обновление величины, связанной с ценностью; следующий раздел проиллюстрирует это. В-третьих, в байесовском обуче нии в целом нет гарантии, что разница между двумя последовательными обнов- лениями пропорциональна «ошибке прогноза». Примечание 6.2 Модель Рескорлы–Вагнера Одной из самых простых моделей обучения является модель Рескорлы–Вагнера, которая описывает, как значение состояния обновляется с течением времени в за- висимости от вознаграждения. Обозначим через Vt(s) значение состояния s в мо- мент времени t, которое можно рассматривать как ожидаемое вознаграждение s в долгосрочной перспективе. Агент получает вознаграждение Rt в момент времени t, находясь в состоянии s (в этой модели действия неявны). Затем значение Vt(s) этого состояния может быть обновлено на следующем временном шаге до Vt+1(s) = Vt(s) + λ(Rt -Vt(s)). (6.23) Здесь Rt - Vt представляет собой разницу между полученным и ожидаемым возна- граждениями; эта разница также называется ошибкой предсказания вознаграждения. Параметр λ называется скоростью обучения. (Более распространенное обозначение скорости обучения – α, но мы уже использовали этот символ в разделе 6.2 для одно- го из параметров бета-распределения.) Это число от 0 до 1 описывает, в какой мере функция ценности агента реагирует на ошибку предсказания вознаграждения. Если λ = 0, функция ценности полностью не реагирует. Если λ = 1, имеем Vt+1(s)Rt, а это значит, что старое значение не играет роли, а новое значение представляет собой просто полученное вознаграждение; это обычно слишком сильная реакция. На практике значение λ мало, но зависит от человека и от условий эксперимента. Базовая структура модели Рескорлы–Вагнера, в которой значение модифициру - ется масштабированной версией ошибки предсказания, является общей для боль- шей части обучения с подкреплением и его применения в нейробиологии. Обучение с подкреплением – это ветвь машинного обучения, изучающая, как сущности, взаи- модействующие с миром, могут выяснить, какие действия приносят пользу в том или ином состоянии мира. Эти сущности обычно называют агентами, а не наблю- дателями, потому что их действия являются частью причинно-следственной це- почки, продолжающейся после действия. Модель обучения с подкреплением с той же базовой структурой, но также учитывающая будущие вознаграждения, называ- ется Q­обуче ни ем. В настоящее время машинное обуче ние использует многослой- ные нейронные сети для решения сложных задач обучения с подкреплением (глу - бокое обуче ние с подкреплением).\n--- Страница 175 ---\n174  Обучение как вывод 6.4. Изучение точности нормального распределения В этом разделе мы рассмотрим очень простую форму обучения без учителя, а именно то, как наблюдатель может узнать точность распределения на ос - новании выборок (рис. 6.4А). Вспомним гауссово распределение стимулов, рассмотренное в главах 3 и 4: (6.24) Для удобства дальнейшей работы перепараметризируем его с точки зре- ния точности стимула Также сделаем явной зависимость распреде- ления от среднего стимула μ и точности стимула Js: (6.25) В главе 3 мы предполагали, что наблюдатель знает это распределение, в частности что он знает μ и Js. На самом деле эта информация должна быть изучена. Для простоты рассмотрим случай, когда μ известно, но нужно вы- учить значение Js; следовательно, с этого момента мы не упоминаем μ как переменную, которую нужно обусловливать. JsJs s1 s2 sts μ(A) (B) Рис. 6.4  Задача обучения точности: (A) образцы стимула s, взятые из распре- деления Гаусса со средним значением μ и точностью Js. Длина двойной стрелки в два раза превышает соответствующее стандартное отклонение; (B) порож­ дающая модель. Js – интересующий параметр, а s1, … , st – наблюдения Этап 1: порождающая модель. Порождающая модель на рис. 6.4B содер- жит интересующую нас переменную состояния мира Js вверху. Наблюдения, расположенные внизу, представляют собой набор стимулов s1, …, st, которые мы вместе обозначим вектором s. Мы предполагаем, что стимулы извлечены из распределения независимо в соответствии с μ и Js, так что\n--- Страница 176 ---\nИзучение точности нормального распределения  175 p(s | Js) º p(s1, s2, …, st | Js) (6.26) = p(s1 | Js)p(s2|μ, Js)  p(st | Js). (6.27) Используя знак произведения, это можно записать как Каж - дое отдельное si подчиняется уравнению (6.25), так что (6.28) Этап 2: вывод. Вывод состоит из изучения параметра Js по конкретным наблюдаемым выборкам s1, s2, st. Следовательно, для нас представляет инте- рес апостериорная вероятность p(Js | s). Чтобы вычислить ее, мы применяем правило Байеса и предполагаем равномерное априорное распределение: p(Js | s) µ p(s | Js)p(Js) (6.29) µ p(s | Js). (6.30) Теперь воспользуемся уравнением (6.27), чтобы записать апостериорную вероятность как пропорциональную произведению мгновенных функций правдоподобия, каждая из которых основана на отдельном наблюдаемом стимуле: (6.31) (6.32) (6.33) Это выражение отражает несколько интересных аспектов. Во-первых, наблюдаемые стимулы появляются только в определенной комбинации, а именно Это сумма квадратов разности между наблюдаемыми стимулами и известным средним μ. Эта комбинация имеет смысл, так как чем больше ее значение, тем ниже точность (как правило). Во-вторых, зави- симости от Js мы раньше не встречали. Этот тип распределения называется гамма ­распределением (примечание 6.3). Это обычное распределение для переменных, которые не могут принимать отрицательные значения (напри- мер, Js). Следовательно, в нашем примере каждое отдельное правдоподобие, а также апостериорная вероятность имеют форму гамма-распределения. Если бы мы выбрали неравномерное априорное распределение, удобным выбором было бы и гамма-распределение. Таким образом, гамма-распреде- ление является сопряженным априором для вывода точности нормального распределения.\n--- Страница 177 ---\n176  Обучение как вывод Примечание 6.3 Гамма-распределение Гамма-распределение определяется случайной величиной Y, которая принима- ет значения на положительной действительной числовой оси, такие как точность в нашем примере. Плотность вероятности гамма-распределения равна (6.34) Здесь k и θ – параметры распределения, причем оба они должны быть положи- тельными. k называется параметром формы, а θ – параметром масштаба. Г(·) обо- значает гамма­функцию – специальную функцию, которая также предварительно запрограммирована во всех пакетах численных расчетов. Несколько примеров гам- ма-распределения показаны на рис. 6.5. Среднее значение гамма-распределенной случайной величины y равно �[y] = kθ. (6.35) Дисперсия Var[y] = kθ2. (6.36) 0.5 0.4 0.3 0.2 0.1 0.0 0 2 4 6 8 10Вероятностьk = 1; θ = 1 k = 1; θ = 2 k = 2.5; θ = 2 k = 7; θ = 0.5 k = 3.5; θ = 1 Рис. 6.5  Примеры гамма­распределения при различных комбинациях параметров Как и в разделах 5.5 и 6.2.2, мы можем записать апостериорную вероят - ность в виде уравнения рекурсивного обновления. На рис. 6.6 показаны при- меры процесса обучения. В качестве последней части этапа 2 мы можем зафиксировать результат. Если наша цель состоит в том, чтобы минимизировать ожидаемую квадра- тичную ошибку при оценке Js, мы должны использовать PME. (Эта цель мо- жет быть поставлена под сомнение, поскольку квадрат ошибки в основном имеет смысл, если переменная может принимать значения по всей прямой.) Мы займемся этим в задаче 6.5. Опять же, этап 3 неприменим, так как мы не рассматриваем ни внутренний шум, ни шум решения.\n--- Страница 178 ---\nИзучение точности нормального распределения  177 (μ = 0) –5 5(A) (В) (С)s1s2s1s4Вероятность Апостериорное распределение Точность Js Точность JsАприорное распределение s1 s2 s3 s4s1 s2 s3 s4 Рис. 6.6  Эволюция апостериорного распределения по Js: (A) наблюдения: s1 = –1.2, s2 = –0.5, s3 = 0.25 и s4 = 0.9; (B) нормированные функции правдоподобия, связанные с самым послед- ним наблюдением; (C) апостериорные распределения с учетом всех наблюдений до текущего времени. Все нормированные функции правдоподобия и апостериорные распределения яв- ляются гамма­распределениями. Априорное распределение является несобственным (ненор- мируемым; см. раздел 3.5.2), что указано пунктирной линией 6.4.1. Почему бы не вывести дисперсию? Мы рассмотрели пример в контексте вывода параметра точности нормально- го распределения. Однако мы могли бы иначе сформулировать его как вывод дисперсии или стандартного отклонения нормального распределения. Эти подходы не являются эквивалентными, потому что равномерное априорное распределение точности – это не то же самое, что равномерная априорная дисперсия. (Если это не ясно, прочитайте раздел B.12.1 приложения о преоб- разованиях случайных величин.) Если бы было выбрано равномерное апри- орное распределение дисперсии, то апостериорное распределение было бы обратным гамма­распределением. Как правило, работать с гамма-распреде- лениями немного проще, чем с обратными гамма-распределениями. 6.4.2. Прогноз Как и в разделе 6.2.1, наблюдателя можно попросить спрогнозировать сле- дующее наблюдение st+1, в то время как изучение параметра точности Js все еще продолжается (посредством обратной связи между испытаниями по значению si). Апостериорное прогностическое распределение по st+1 полу - чается маргинализацией по Js: (6.37) Уравнение (6.37) можно считать априорным относительно наблюдения st+1, которое имеет наблюдатель после момента времени t. Эта «априорная»\n--- Страница 179 ---\n178  Обучение как вывод информация впоследствии может быть объединена с дополнительной ин- формацией, такой как зашумленные измерения. Мы вернемся к уравнению (6.37) в задаче 6.9. 6.5. Изучение наклона линейной зависимости До сих пор мы рассуждали о том, как байесовский наблюдатель изучает па- раметр распределения вероятностей либо по двоичному результату, либо по действительнозначной переменной. Несколько более сложная ситуация возникает, когда параметр, который необходимо изучить, определяет от - ношение между двумя переменными. Тем не менее байесовский подход не требует особых изменений. Для иллюстрации этого мы рассмотрим младенца, который учится конт - ролировать свои конечности. В первом приближении командный сигнал (например, степень возбуждения нейронов моторной коры), посылаемый в спинной мозг, линейно регулирует мышечную силу. Но каков наклон за- висимости между двигательной командой и выходной силой для конкретной мышцы? Без этих знаний младенец будет неуклюжим; по мере обучения его координация движений улучшается. Рассмотрим первые попытки малыша научиться правильной зависимости между моторной командой и реакцией мышц. Для простоты предположим, что малыш уже понимает, что взаи- мосвязь между командой и реакцией является линейной (хотя очевидно, что этому тоже нужно научиться), но он не знает наклон линии. Его цель – оценить наклон k, связывающий командный сигнал s с силой мышечного сокращения F : F(s) = ks. (6.38) На рис. 6.7 показаны результаты десяти итераций, в которых младенец ис - пользует разную величину командного сигнала, чтобы оттолкнуться руками от пола, оценивая силу, которую он производит. Он может судить о силе, основываясь на обратной связи от своих проприорецепторов. Эти сенсорные сигналы зашумлены, поэтому мы моделируем измерение силы ft в t-м испытании как выборку, полученную из гауссова распределения вокруг фактического усилия: p(ft | k, st) = �(ft; kst, σ2). (6.39) Таким образом, наблюдения представляют собой пары команда–изме- рение (st, ft), где мы предполагаем, что st точно известно. В отличие от раз- дела 6.4, мы также предполагаем, что младенец знает уровень шума изме- рения σ; если бы это было не так, то вывод, описанный здесь, пришлось бы объединить с выводом в разделе 6.4.\n--- Страница 180 ---\nИзучение наклона линейной зависимости  179 20 10 0 20 10 0 20 10 00 0 05 5 501234 01234 0123401234 01234 0123410 10 10Испытание = 1 Испытание = 5 Испытание = 15Результирующее усилие Результирующее усилие Результирующее усилие Вероятность Вероятность Вероятность Вероятность Вероятность Вероятность(A) (В) (С) Управляющий сигнал Управляющий сигнал Управляющий сигналНаклон Наклон НаклонНаклон Наклон Наклон Рис. 6.7  Наблюдатель изучает наклон, связывающий величину сигнала моторной команды (ось x) и мышечное усилие (ось y): (A) графики рассеяния, показывающие накопление данных от испытаний 1 до 5 и 15. Линия на каждом графике показывает оценку апостериорного сред- него наклона на основе всех испытаний до показанного включительно (точка данных обведе- на зеленым); (B) функции правдоподобия одного измерения из соответствующих испытаний; (C) апостериорное распределение наклона. Реальное значение наклона, использованное для создания данных, было k = 1.5 при σ = 2. Мы предположили, что априорное распределение p(k) является плоским После каждого мышечного усилия, зная распределение шума, младенец может построить функцию правдоподобия, отражающую вероятность из- мерения с учетом наклона. Мы предполагаем априорное распределение p(k). Затем он может вычислить свое апостериорное распределение наклона; его можно записать как вариант уравнений (5.30) и (6.8) (6.40) или как разновидность уравнений обновления (5.31) и (6.16):\n--- Страница 181 ---\n180  Обучение как вывод p(k | s1, f1, s2, f2, …, sT+1, fT+1) µ p(k | s1, f1, s2, f2, …, sT, fT)p(fT+1 | k, sT+1). (6.41) Отличие от примеров из предыдущих разделов состоит в том, что теперь команды s1, …, sT нуждаются в правдоподобии, но поскольку мы предпо- лагаем, что эти команды известны младенцу, мы можем просто обусловить вероятность каждой из них. 6.6. Изучение структуры причинно-следственной модели Важной формой обучения является изучение причинно-следственной струк - туры мира: облака могут вызывать дождь (но не наоборот), кнопка может вы- зывать включение устройства, а курение вызывает рак. Причинное обуче ние лежит в основе классического обусловливания и ассоциативного обучения. Большая часть науки посвящена пониманию причинно-следственных струк - тур, лежащих в основе сложных систем – молекулярных путей, нейронных цепей, глобальной экономики и климата. В некоторых случаях возможно вмешательство. В нашем примере мы рассматриваем наблюдателя, который пытается определить причинно-следственные связи между тремя узлами, А, В и С (рис. 6.8). Для простоты предположим, что возможны только три структуры и что наблюдатель знает об этом. Этап 1: генеративная модель. Три структуры, которые мы обозначаем H1, H2 и H3, априори имеют одинаковую вероятность: p(H1) = p(H2) = p(H3) = 1/3. (6.42) Затем мы предполагаем, что все узлы изначально неактивны (выключены). Если узел в структуре активируется (включается), то в результате могут вклю- читься только узлы, непосредственно соединенные стрелкой. Однако сущест - вует вероятность «отказа» 0.2, когда, несмотря на наличие соединительной стрелки, узел на конце стрелки не активируется. Если несколько стрелок исходят из одного и того же узла, то причинные эффекты вдоль этих стрелок независимы. Узлы самопроизвольно не включаются. Рассмотрим в качестве примера H1. Причинно-следственные правила этой структуры включают (но не ограничиваются) следующее: p(A вкл → B вкл | H1) = 0.8; p(B вкл → A вкл | H1) = 0; p(C вкл → B вкл | H1) = 0; p(B вкл → C вкл | H1) = 0.8; p(A вкл → C вкл | H1) = 0.8 · 0.8 = 0.64.\n--- Страница 182 ---\nИзучение структуры причинно­следственной модели  181 Этап 2: вывод. Рассмотрим сценарий, в котором наблюдатель видит, что B включается, и в результате C также включается, а A нет. Таким образом, воз- никает наблюдение: «В вкл → А выкл, С вкл». Правдоподобие гипотезы – это вероятности такого наблюдения при данной гипотезе. Используя причинно- следственные структуры, мы находим �(H1; B вкл → A выкл, C вкл) = p(B вкл → A выкл, C вкл | H1) = 1 · 0.8 = 0.8; �(H2; B вкл → A выкл, C вкл) = p(B вкл → A выкл, C вкл | H2) = 1 · 0 = 0; �(H3; B вкл → A выкл, C вкл) = p(B вкл → A выкл, C вкл | H3) = 0.2 · 0.8 = 0.16. Объединив априорные вероятности с правдоподобиями, мы теперь можем вычислить апостериорные вероятности причинно-следственных структур, которые примерно равны 0.833, 0 и 0.167 соответственно. Упражнение 6.5. Выполните вычисления и убедитесь в этом. A A AB BB CC CГипотеза 1 Гипотеза 2 Гипотеза 3 Рис. 6.8  Три возможные причинно­следственные струк - туры, состоящие из трех узлов. Хотя эти диаграммы выгля- дят как порождающие модели, они имеют другое значение; каждая диаграмма сама по себе является состоянием мира в порождающей модели Итак, из наших наблюдений мы извлекли какие-то знания о причинной структуре мира. Дополнительные наблюдения позволят нам узнать больше и, возможно, даже определить причинно-следственную структуру с высокой степенью достоверности. Очевидно, что это был очень простой пример, а вы- вод причинно-следственных связей в реальном мире во многих отношениях сложнее. Во-первых, причинно-следственные структуры могут иметь более трех узлов. Во-вторых, мы предполагали, что причинно-следственные пра- вила и связанные с ними вероятности известны, что часто не так. В-третьих, мы ограничили задачу вывода тремя возможными структурами; часто про- исходит комбинаторный взрыв количества возможных структур. С другой стороны, в реальном мире мы можем не зависеть от данных нам наблюдений, но при этом иметь возможность вмешиваться в систему.\n--- Страница 183 ---\n182  Обучение как вывод Примечание 6.4 Небайесовское обуче ние в искусственных нейронных сетях Популярными моделями обучения являются искусственные нейронные сети. В сво- ей простейшей форме – многослойных персептронах – они принимают вход x и вы- числяют выход yˆ, который должен быть похож на истинные выходные данные y (скажем, в среднеквадратичном смысле). Простейшей функцией отображения вхо- да на выход будет линейная функция Мы можем назвать ее однослой- ной нейронной сетью. Затем мы могли бы построить двухслойную нейронную сеть с одной матрицей весов (W12) от первого до второго слоя и вектором векторов (W23) от второго до третьего слоя как Наложение двух слоев линей- ных преобразований – это еще одно линейное преобразование, только записанное более сложным образом. Но зависимости в реальном мире обычно нелинейны. По- этому нейронные сети обычно объединяют в себе линейные и нелинейные пре- образования. Например, они могут использовать спрямленную линейную функцию (rectified linear unit): φ(z) = max(0, z) и, скажем, два слоя (этот класс функций может аппроксимировать наиболее значимые функции человеческого поведения). То есть они могут быть описаны уравнением (6.43) В этой нелинейной функции искусственные нейронные сети стремятся реализо- вать градиентный спуск относительно вектора всех весов W функции потерь, роль которой играет, например, квадрат ошибки C(y ˆ, y) = (y ˆ - y)2. (6.44) где η – константа (обычно малая). Это правило обучения позволяет постепенно улучшать производительность модели. Эта схема обновления напоминает ту, которую мы видели в модели Рескорлы–Ваг - нера; данную модель можно интерпретировать как градиентный спуск по квадрату разницы между полученным и ожидаемым вознаграждениями. Позже мы обсудим, как можно использовать обуче ние искусственных нейронных сетей для получения правдоподобия в тех областях, где это сложно (примечание 14.4), и как искусствен- ные нейронные сети конкурируют с байесовскими моделями в описании поведе- ния (примечание 15.2)1. 6.7. Другие формы обучения Здесь мы не рассматриваем многие другие формы обучения: в рассмотренных нами примерах выводимая переменная не меняется. Это отличается от форм вывода, в которых интересующая переменная изменяется от испытания к испытанию. Мы рассмотрим это в главе 12; 1 В этой книге мы избегаем использования матричной алгебры, но в области ней- ронных сетей без нее не обойтись.\n--- Страница 184 ---\nРекомендуемая литература  183 подборка наблюдений называется обучающими данными, и модели обычно обучают с использованием этих данных. Оптимальное обуче- ние также достигается с помощью вычислений, но в данной книге мы этого не делаем; мы также не обсуждали активное обуче ние, при котором обучаемая модель управляет в некоторой степени приобретением наблюдений; краткое введение в обуче ние нейронных сетей дано в примечании 6.4. 6.8. Заключение В этой главе описано обуче ние на основе наблюдений как байесовское вы- числение. Вы усвоили следующее: изучаемые параметры распределения можно рассматривать как задачу байесовского вывода, где параметры рассматриваются как состояние мира; апостериорное распределение может быть построено на основе пара- метра распределения Бернулли (или биномиального распределения) и параметра точности гауссова распределения; это вычисление включает перемножение функций правдоподобия, аналогично накоплению доказательств. Сопряженные априорные рас - пределения могут облегчить интеграцию априорных знаний с новыми данными; апостериорное прогнозирующее распределение охватывает изучение параметров в последующее прогнозирование или вывод; в обуче нии с подкреплением байесовское правило для изучения веро- ятности вознаграждения имеет как сходства, так и различия с прави- лом Рескорлы–Вагнера; иногда причинная структура мира неизвестна и должна быть изучена из наблюдений. 6.9. Рекомендуемая литература Daniel E. Acuña and Paul Schrater. Structure Learning in Human Sequential DecisionMaking. PLoS Computational Biology 6, no. 12 (2010): e1001003. Patricia W. Cheng. From Covariation to Causation: A Causal Power Theory. Psychological Review 104, no. 2 (1997): 367–405. Anna Coenen, Bob Rehder, and Todd M. Gureckis. Strategies to Intervene on Causal Systems Are Adaptively Selected. Cognitive Psychology 79 (2015): 102– 133. Alison Gopnik, Clark Glymour, David M. Sobel, Laura E. Schulz, Tamar Kushnir, and David Danks. A Theory of Causal Learning in Children: Causal Maps and Bayes Nets. Psychological Review 111, no. 1 (2004): 3–32.\n--- Страница 185 ---\n184  Обучение как вывод Charles Kemp, Andrew Perfors, and Joshua B. Tenenbaum. Learning Over hy­ po theses with Hierarchical Bayesian Models. Developmental Science 10, no. 3 (2007): 307–321. Pierre-Simon Laplace. Memoir on the Probability of the Causes of Events. Statistical Science 1, no. 3 (1986): 364–378. Tamas J. Madarasz, Lorenzo Diaz-Mataix, Omar Akhand, Edgar A. Ycu, Joseph E. LeDoux, and Joshua P . Johansen. Evaluation of Ambiguous Associations in the Amygdala by Learning the Structure of the Environment. Nature Neuroscience 19, no. 7 (2016): 965–972. Jenny R. Saffran, Richard N. Aslin, and Elissa L. Newport. Statistical Learning by 8­Month­Old Infants. Science 274, no. 5294 (1996): 1926–1928. Joshua B. Tenenbaum. Bayesian Modeling of Human Concept Learning. Advances in Neural Information Processing Systems 11, edited by Michael S. Kearns, Sara A. Solla, and David A. Cohn: 59–68. Cambridge, MA: MIT Press, 1998. Ting Xiang, Terry Lohrenz, and P . Read Montague. Computational Substrates of Norms and Their Violations during Social Exchange. Journal of Neuroscience 33, no. 3 (2013): 1099–1108. Fei Xu and Joshua B. Tenenbaum. Word Learning as Bayesian Inference. Psy- cho logical Review 114, no. 2 (2007): 245–272. 6.10. Задача Задача 6.1. Вернемся к примеру вывода вероятности Бернулли в разделе 6.2. Мы отметили, что вероятность дождя на следующий день определялась оцен- кой апостериорного среднего и отличалась от наиболее вероятного значения интенсивности осадков r на острове, которое является оценкой MAP . (a) Предположим, вы пробыли на острове четыре дня, и дождь шел только один раз. Покажите, что оценка MAP для r равна 0.25 и, согласно правилу Лапласа (уравнение 6.14), PME равна 0.33. (b) Объясните логически, почему ваша оценка вероятности дождя на сле- дующий день отличается (в данном случае в большую сторону) от вашей оценки наиболее вероятной интенсивности дождя. (c) Проверьте уравнение (6.14) численно, разбив r на несколько сотен или тысяч значений, равномерно распределенных между 0 и 1. Учитывая тот факт, что дождливым является один день из четырех, вычислите веро- ятность для каждого значения r и введите их в формулу Байеса с равно- мерным априорным распределением. Вычислите апостериорное среднее как Задача 6.2. Вернемся к примеру вывода вероятности Бернулли в разделе 6.2. (a) Докажите уравнение (6.22) для PME, исходя из уравнения (6.14). (b) Обобщите уравнение (6.22) на случай бета-априорного распределения с параметрами α0 и β0. Задача 6.3. Начнем с уравнения (6.8) для апостериорного распределения по r в случае с островом.\n--- Страница 186 ---\nЗадача  185 (a) Запишите выражение для апостериорного распределения r, если вы на- блюдаете только дождливые дни. (b) Постройте график апостериорного распределения для nrain = 1, 2, 5, 10 (четыре кривые на одном графике). (c) Сколько дней должен идти дождь, чтобы апостериорное среднее было больше 0.9? Задача 6.4. Эта задача основана на разделах 5.5 и 6.3. Накопление свиде- тельств можно сформулировать как правило обучения. (a) Покажите с помощью математического вывода, что оценку апостериор- ного среднего можно записать в рекурсивной форме sˆt+1 = sˆ + λt+1(xt+1 - sˆt), и найдите выражение для расчета «скорости обучения» λt через σ и t. (b) Скорость обучения увеличивается или уменьшается с течением време- ни? Объясните, почему это происходит. (c) Обобщите (a) и (b) на ситуацию, когда каждое измерение xt имеет свою собственную дисперсию σt2. Задача 6.5. Вспомните раздел 6.4 про изучение параметра точности гауссова распределения. (a) Используя оценку апостериорного распределения в уравнении (6.33), найдите выражение для оценки апостериорного среднего. (b) Объясните это выражение. (c) Можно ли записать обновление PME Js в форме Рескорлы–Вагнера, ана- логично разделу 6.2.2? Если нет, то почему? (d) Найдите выражение для апостериорного стандартного отклонения. (e) Измените уравнение (6.33), если обучаемая модель имеет априорное распределение p(Js), которое представляет собой гамма-распределение с масштабирующим параметром k0 и параметром формы θ0. (f) Измените ответы к заданиям (a), (b) и (d) в соответствии с новыми усло- виями. Задача 6.6. Создайте фильм, в котором каждый кадр соответствует одному проходу изучения параметра наклона k в отношении F = ks (см. раздел 6.5). Младенец выполняет двадцать попыток. В каждом испытании он посылает команду s, полученную из равномерного распределения в [2, 10], а изме- рение результирующего усилия взято из гауссова распределения со сред- ним значением 1.5s и стандартным отклонением 2. Априорное распределе- ние k является плоским. Каждый кадр должен выглядеть как ряд на рис. 6.7: в цент ре – функция правдоподобия по k, вычисленная на основе измерения в t-м испытании; справа – апостериорное распределение на основе изме- рений, сделанных до t-го испытания включительно; слева – графическое представление данных с линией, соответствующей оценке апостериорного среднего k на основе измерений, сделанных до t-го испытания включитель- но. Убедитесь, что оси не меняются от кадра к кадру. Выберите достаточно большие диапазоны по обеим осям. Убедитесь, что числа на осях легко читае- мы, а линии на графиках достаточно толстые. Сохраните свой фильм. Задача 6.7. Каково распределение количества дней, прошедших между дожд ливыми днями, в примере острова из раздела 6.2? Объясните свой ответ.\n--- Страница 187 ---\n186  Обучение как вывод Задача 6.8. В разделе 6.2 мы предполагали, что бинарный исход не зависит от дней. Вместо этого предположим, что вероятность дождя в день, следую- щий после дождя, равна rr, а после засушливого дня – rd. (a) Найдите апостериорную вероятность того, что в день t + 1 после серии наблюдений х1, …, хt пойдет дождь. (b) Поясните смысл полученного выражения. Задача 6.9. В разделе 6.4.2 было показано, как апостериорное распределение по параметру точности Js используется в предсказании. (a) Вычислите интеграл для апостериорного прогностического распределе- ния st+1 (уравнение 6.37). (b) Как это распределение соотносится с ситуацией, когда Js известно? Задача 6.10. Эта задача объединяет задачу 5.9 с настоящей главой. Вы позна- комились с бета-распределением, которое обычно применяется при выводе из количества встречаемости двух категорий лежащих в их основе вероятно - стей этих категорий. Возьмем, к примеру, социолога, который интересуется долей населения, поддерживающей кандидата. Интересующее нас состояние мира, обозначаемое s, представляет собой вероятность (от 0 до 1). Ответ опрашиваемого индивидуума xi является бинарным независимо от того, под- держивает ли он кандидата. Априорное распределение социолога является плоским, а индивидуальные ответы условно независимы при заданных s. Предположим, что социолог опрашивает N человек и получает n положи- тельных ответов. (a) Математически покажите, что апостериорное распределение по s явля- ется бета-распределением с параметрами n + 1 и N - n + 1. В результате MLE равна а оценка апостериорного среднего равна (b) Агрегатор опросов пытается выполнить «объединение сигналов» одного опроса с респондентами N1 и другого с респондентами N2. Соответству - ющие количества положительных ответов в обоих опросах равны n1 и n2, а соответствующие MLE равны sˆML,1 и sˆML,2. Покажите, что комбиниро- ванная MLE s равна (6.45) Другими словами, комбинированная MLE представляет собой средне- взвешенное значение отдельных MLE. Это соответствует уравнению (5.14) для объединения двух измерений с нормальным распределением. (c) Ситуация с PME менее очевидна. Обозначим отдельные PME через sˆPM,1 и sˆPM,2. Покажите, что комбинированная PME s равна (6.46) (d) Покажите, что это выражение имеет смысл в частном случае N2 = 0.\n--- Страница 188 ---\nЗадача  187 Задача 6.11. Теория перспектив – популярная (и получившая Нобелевскую премию) теория поведенческой экономики, утверждает, что люди переоце- нивают малые вероятности, такие как p(я умру от Эболы), и недооценивают большие вероятности, такие как p(я умру от рака). В разделе 6.2 мы раз- мышляли о вероятностях вероятностей. Используйте эту концепцию, чтобы описать, почему люди действительно должны переоценивать малые и недо- оценивать большие вероятности.",
      "debug": {
        "start_page": 164,
        "end_page": 188
      }
    },
    {
      "name": "Глава 7. Различение и обнаружение 188",
      "content": "--- Страница 189 --- (продолжение)\nГлава 7 Различение и обнаружение Как определить, какой из двух стимулов имел место? В предыдущих главах мы обсуждали основы байесовского моделирования, часто используя пример задачи пространственной локализации. В такой за- даче интересующая переменная – положение на линии – является непрерыв - ной, то есть принимает континуум значений. Задача состояла в том, чтобы оценить положение в этом континууме, чтобы в принципе у испытуемого было бесконечное число возможных ответов. Однако многие, если не боль- шинство задач в лабораторных экспериментах требуют выбора только между двумя вариантами. Он называется бинарным выбором, или бинарным ре- шением. Бинарные решения похожи на другие задачи дискретного вывода, поскольку оценка MAP обеспечивает максимальную точность. Но бинарные решения уникальны тем, что априорное, апостериорное и оценочное распре- деления характеризуются одним числом. Это позволяет нам характеризовать поведение с помощью набора специализированных инструментов, таких как рабочие характеристики приемника. Краткое содержание главы Эта глава построена вокруг того же трехэтапного процесса, что и предыду - щие главы: порождающая модель, процесс вывода и распределение оценок. Мы будем использовать ту же базовую задачу, что и в главах 3 и 4, комбини- руя измерение с априорным распределением. В этой главе будет показана связь байесовских моделей с теорией обнаружения сигналов. 7.1. Примеры задач Бинарный выбор широко распространен в реальном мире, например: будет ли сегодня дождь? Могу ли я доверять этому человеку? Успею ли я на оста- новку до прихода автобуса, если побегу? Это письмо в электронной поч-\nГлава 7 Различение и обнаружение Как определить, какой из двух стимулов имел место? В предыдущих главах мы обсуждали основы байесовского моделирования, часто используя пример задачи пространственной локализации. В такой за- даче интересующая переменная – положение на линии – является непрерыв - ной, то есть принимает континуум значений. Задача состояла в том, чтобы оценить положение в этом континууме, чтобы в принципе у испытуемого было бесконечное число возможных ответов. Однако многие, если не боль- шинство задач в лабораторных экспериментах требуют выбора только между двумя вариантами. Он называется бинарным выбором, или бинарным ре- шением. Бинарные решения похожи на другие задачи дискретного вывода, поскольку оценка MAP обеспечивает максимальную точность. Но бинарные решения уникальны тем, что априорное, апостериорное и оценочное распре- деления характеризуются одним числом. Это позволяет нам характеризовать поведение с помощью набора специализированных инструментов, таких как рабочие характеристики приемника. Краткое содержание главы Эта глава построена вокруг того же трехэтапного процесса, что и предыду - щие главы: порождающая модель, процесс вывода и распределение оценок. Мы будем использовать ту же базовую задачу, что и в главах 3 и 4, комбини- руя измерение с априорным распределением. В этой главе будет показана связь байесовских моделей с теорией обнаружения сигналов. 7.1. Примеры задач Бинарный выбор широко распространен в реальном мире, например: будет ли сегодня дождь? Могу ли я доверять этому человеку? Успею ли я на оста- новку до прихода автобуса, если побегу? Это письмо в электронной поч-\n--- Страница 190 ---\nРазличение  189 те – спам или нет? На каждый из этих вопросов можно ответить только да/ нет, поэтому соответствующая случайная величина (будет ли сегодня дождь и т. д.) является бинарной. Во многих примерах, с которыми мы сталкивались в предыдущих главах, также использовались бинарные решения. Два типа бинарных решений особенно важны хотя бы потому, что они со- ответствуют популярным психофизическим парадигмам. Представьте, что вы радиолог, пытающийся определить, присутствует ли опухоль на зашум- ленном рентгеновском снимке. Такие задачи, в которых наблюдатель решает, присутствует стимул или нет, являются задачами обнаружения. Теперь пред- ставьте, что вы стоите на обочине дороги и видите вдалеке силуэт движуще- гося автомобиля. Вы пытаетесь определить, приближается автомобиль к вам или, наоборот, удаляется от вас. Если наблюдатель выбирает между двумя ненулевыми значениями или категориями переменной-стимула (направле- ние движения), это называется задачей различения. Задачи на обнаружение и различение возникают во многих лабораторных экспериментах. Объект движется влево или вправо (различение)? Видна ли на дисплее вертикальная линия (обнаружение)? Вы почувствовали прикосновение к пальцу (обна - ружение)? И так далее. Даже когда основная переменная стимула является непрерывной (например, продолжительность), обычно задачу формулиру - ют в терминах выбора между двумя вариантами (например, какой стимул длился дольше), при этом непрерывно меняя стимулы от испытания к ис - пытанию. 7.2. Различение В этой главе стимул s может принимать только два значения, которые мы на - зываем s+ и s-, и наблюдатель выбирает между ними. Например, наблюдатель сообщает, наклонен ли ориентированный узор на 1° вправо или на 1° влево от вертикали. В научной литературе эксперименты такого типа называют двухвариантной задачей с вынужденным выбором (two-alternative forced choice task), или задачей «да/нет» (yes/no task). 7.2.1. Этап 1: порождающая модель Порождающая модель – это отображение s → x, как в главе 3. Отличие лишь в том, что стимул s принимает только два возможных значения, также на- зываемых альтернативами и обозначаемых s+ и s-. Эти значения представ- ляют собой некоторые числа, такие как -1 и 1 или 0 и 3. Они не являются случайными величинами. В главе 8 мы рассмотрим, что происходит при выборе между двумя классами стимулов. Распределение стимулов представ- ляет собой дискретное распределение вероятностей со значениями p(s = s+) и p(s = s-), которые в сумме дают 1 и отражают частоты, с которыми пред- ставлены значения стимулов. Измерение x следует обычному распределению\n--- Страница 191 ---\n190  Различение и обнаружение гауссова шума p(x | s). Распределение шума для обоих возможных значений s показано на рис. 7.1. s = s− s = s+Вероятность Измерение x Рис. 7.1  Распределения шума для задачи «да/нет» по различению s+ и s− 7.2.2. Этап 2: вывод Предположим, что s+ = 1° и s- = -1° и что в данном испытании ваше из- мерение равно 0.1°. Какой стимул вы наблюдаете – s+ или s-? Вы, вероятно, примете решение в пользу s+ просто потому, что измерение ближе к s+, чем к s-. Но байесовский наблюдатель будет рассуждать иначе. Чтобы убедиться в этом, представьте, что вы знаете, что s- встречается в мире (или в экспери- менте) гораздо чаще, чем s+. В этом случае измерение, которое лишь немного ближе к s+, чем к s-, вероятно, следует отнести к s-. Далее мы выясним, как принимает решение байесовский наблюдатель. Упражнение 7.1. Опишите реальную задачу различения, в которой один сти- мул, скажем, s+ гораздо более вероятен, чем другой стимул s-. Описание логики байесовского наблюдателя требует вычисления апосте- риорного значения по s, т. е. p(s | x). Поскольку s принимает два значения, апостериорное распределение представляет собой дискретное распределе- ние вероятностей со значениями p(s = s+ | x) и p(s = s- | x), которые в сумме должны равняться 1 (рис. 7.2). Правило Байеса говорит нам, что (7.1) Для бинарной переменной апостериорное распределение однозначно определяется отношением апостериорных вероятностей двух альтернатив. Мы можем рассчитать это отношение, используя правило Байеса:\n--- Страница 192 ---\nРазличение  191 (7.2) (7.3) Это отношение называется апостериорным отношением, или апостериор- ными шансами. Его смысл – это вероятность одной альтернативы по отноше- нию к другой. Мы видим, что нормирование p(x) выпадает и не влияет на это отношение. Например, если апостериорная вероятность того, что стимулом был s+, равна 80 %, то апостериорное отношение равно Апостериор- ное отношение всегда положительно, но может достигать сколь угодно боль- ших значений. Например, если апостериорная вероятность s+ равна 99 %, то апостериорное отношение равно Зная апостериорное отношение, можно вычислить вероятность каждой из альтернатив, и наоборот. 0.7 0.30.55 0.45Вероятность Вероятность(A) (В) s– s– s+ s+Априорное распределение Апостериорное распределение Рис. 7.2  Пример априорного и апостериорного распределений по бинарной переменной В байесовских вычислениях вы часто будете встречать уравнение (7.3) с натуральным логарифмом обеих частей. Обозначим это логарифмическое отношение через d : (7.4) Далее мы покажем, что логарифмирование упрощает многие математиче- ские выкладки. Величина d называется логарифмическим апостериорным от- ношением (log posterior ratio, LPR; также иногда называют логарифмическим апостериорным коэффициентом). Если апостериорная вероятность s+ равна 0.80, то LPR составляет log 0.80 = log 4 = 1.39. LPR содержит ту же информа- цию, что и само апостериорное распределение; в конце концов, мы можем возвести его в степень и вычислить из него вероятность каждой альтерна- тивы, как сделали выше.\n--- Страница 193 ---\n192  Различение и обнаружение Апостериорные вероятности двух альтернатив можно восстановить из d следующим образом: (7.5) (7.6) Первое уравнение называется логистической функцией d, второе равно 1 минус первое. Упражнение 7.2. Покажите аналитически, что это отношение их разности, равное 1, верно. LPR принимает значения от -¥ до ¥. LPR также обладает свойством сим- метрии: перемена мест апостериорных вероятностей двух альтернатив экви- валентна изменению знака LPR. Например, если апостериорная вероятность s+ = 20 %, а s- = 80 %, то LPR составляет log 0.20 = log 0.25 = -1.39. Когда две альтернативы имеют одинаковую апостериорную вероятность, LPR равно 0. Если LPR положительное, то p(s = s+ | x) больше, чем p(s = s- | x). Отсюда оценка MAP (7.7) Таким образом, байесовское бинарное принятие решений может быть крат ко описано с применением LPR. Теперь рассмотрим некоторую общепринятую терминологию. Неравен- ство d > 0, используемое для определения оценки MAP , также называется ре- шающим правилом байесовского наблюдателя MAP , а d называется решающей переменной (отсюда и обозначение d от слова decision – решение). Правило принятия решений можно рассматривать как двоичный эквивалент отобра- жения x в s в задачах оценки (главы 3 и 4). Скалярное значение, с которым сравнивается переменная d для принятия решения (здесь 0), также называ- ется критерием принятия решения, или просто критерием. Термины «реша- ющее правило», «решающая переменная» и «критерий принятия решения» применяются не только в байесовском моделировании. Любое неравенство вида f(x) > k, где f – любая функция, а k – любой скаляр, может служить моде- лью того, как наблюдатель превращает измерение в решение. В общем случае критерием является любое фиксированное значение, с которым сравнива- ется решающая переменная. LPR также упрощают другие аспекты вывода. Поскольку логарифм произ- ведения представляет собой сумму логарифмов, правую часть уравнения (7.4) можно переписать в виде суммы: (7.8)\n--- Страница 194 ---\nРазличение  193 Правое слагаемое называется логарифмическим отношением правдоподобия (log likelihood ratio, LLR) и отражает количество доказательств, предоставляе- мых измерением x. Левое слагаемое представляет собой логарифмическое априорное отношение и отражает априорные убеждения наблюдателя отно- сительно двух альтернатив. Сумма этих двух слагаемых и есть LPR (рис. 7.3). p(s = s+) = 0.6 p(s = s+) = 0.5 p(s = s+) = 0.4p(s = s+) = 0.6 p(s = s+) = 0.5 p(s = s+) = 0.4 s– s+ s–2 1 0 –1 –22 1 0 –1 –2 Логарифмическое апостериорное отношение dЛогарифмическое апостериорное отношение d Измерение x Измерение x(В) (A) Логарифмическое отношение правдоподобия–1 012 Рис. 7.3  (A) Связь между LPR и логарифмическим отношением правдоподобия. Это соот - ношение является общим и относится не только к гауссовой модели. При переходе от крас - ного к фиолетовому и синему априорное значение в большей степени благоприятствует s+, а критерий логарифмического отношения правдоподобия смещается в сторону меньших зна- чений (пунктирные линии), указывая на то, что субъект более склонен выбирать s+; (B) связь между LPR и измерением в гауссовой модели. Зависимость является линейной с наклоном s σ2. Фиолетовая линия также представляет собой логарифмическое отношение правдоподобия, а соответствующий «нейтральный» критерий представляет собой среднюю точку s+ и s– Всякий раз, когда d больше нуля, наиболее вероятно s+. Таким образом, правило принятия решения MAP состоит в том, чтобы сообщать s+, когда сумма логарифмического априорного отношения и LLR положительна. Правило MAP: сообщать s+, когда (7.9) Когда s- происходит с большей вероятностью, чем s+ (другими словами, когда логарифмическое априорное отношение отрицательно), оптимальное правило принятия решения состоит в том, чтобы сообщать s+ только тог - да, когда измерение x предоставляет достаточно убедительные доказатель- ства в пользу s+ для преодоления априорной систематической предвзятости в пользу s-. Оценка MAP максимизирует вероятность правильного ответа. Чтобы по- нять, почему правило MAP (в данном случае уравнение 7.9) является хорошей стратегией, рассмотрим мир, в котором у человека либо карие, либо голу - бые глаза. Представьте, что вы стоите перед аудиторией и вас спрашивают о цвете глаз определенного человека. На качество информации влияет ваше расстояние от человека. Предположим, что в вашем регионе мира карие глаза\n--- Страница 195 ---\n194  Различение и обнаружение более распространены, чем голубые. Если вас спросят о ком-то поблизости, ваша сенсорная информация будет высокого качества, и вы сможете основы- вать свое решение преимущественно на этой сенсорной информации. Если вас спрашивают о ком-то вдалеке, качество сенсорной информации хуже или вообще неинформативно. Чем ниже качество зрительной информации, тем больше вы будете полагаться на свои знания о распространенности карих глаз среди населения в целом. Когда сенсорная информация вообще отсут - ствует, лучше всего всегда отвечать, что у человека карие глаза. Это возрас - тающее влияние априорного знания по мере снижения качества сенсорной информации точно выражается уравнением (7.9). LLR склонно стремить- ся к меньшей величине (либо положительной, либо отрицательной), когда сенсорная информация имеет более низкое качество («склонен стремить- ся», потому что этот член является случайной величиной, которая наследует свое распределение от распределения x). Относительное качество априорной информации и информации о вероятности определяет, какая информация доминирует. Таким образом, в задаче различения «да/нет» априорное убеж - дение приводит к смещению критерия принятия решения. 7.2.3. Гауссова модель Если измерение x следует гауссову распределению при условии s, мы можем дополнительно найти LLR, подставив выражение для p(x | s). Это дает особен- но простое выражение для LPR: (7.10) где мы ввели обозначение (7.11) для средней точки между s+ и s- и Ds = s+ - s- (7.12) для разницы между ними. Мы выведем уравнение (7.10) в задаче 7.5. На рис. 7.3 мы построили LPR d как функцию измерения x. Это линейная функ - ция измерения. Важно отметить, что она также зависит от уровня сенсорной неопределенности σ . Подводя итог, можно сказать, что в модели измерения Гаусса с равной дисперсией LPR является линейным. Хотя это свойство очень удобно, оно специфично для гауссовой модели равной дисперсии и редко верно в других случаях (см. задачи). LLR положительно, если x больше среднего значения s+ между и s-, и от - рицательно в противном случае. Это интуитивно понятно: измерение сви-\n--- Страница 196 ---\nРазличение  195 детельствует о s+, если оно находится ближе к s+, чем к s-. Коэффициент помогает определить величину LLR. Этот множитель говорит нам, что для одного и того же x сила свидетельства выше, когда два различаемых стимула находятся дальше друг от друга (т. е. больше разность s+ - s-) или когда шум измерения (σ ) меньше. 7.2.4. Решающее правило с точки зрения измерения Подставляя уравнение (7.10) для LLR в уравнение (7.9) для решающего пра- вила и решая его относительно x, мы приходим к оптимальному решающему правилу для нашей задачи различения «да/нет» с гауссовым шумом измере- ния: сообщить, что стимул равен s+, когда x > kMAP, (7.13) где критерий MAP для измерения равен (7.14) Хотя критерий MAP для LPR всегда равен 0, при измерении он зависит от различаемых стимулов, уровня неопределенности (или уровня шума) и априорных вероятностей. В общем случае наблюдатель должен знать зна- чения всех этих переменных, чтобы принимать оптимальные решения. В частном случае, когда априорное значение является плоским, т. е. p(s = s+) = p(s = s-) = 0.5 (фиолетовые линии на рис. 7.3), логарифмическое априорное отношение равно нулю, и наблюдатель сообщает s+ просто, когда x > s. (7.15) Это условие имеет смысл: x сравнивается с серединой между s- и s+, кото - рую можно назвать «нейтральным критерием». Сравнивая уравнения (7.14) и (7.15), мы можем определить член как сдвиг критерия из-за наличия неплоского априорного распределения. Если априорное распреде- ление благоприятствует s+, например ps(s+) = 0.6 (синяя линия на рис. 7.3B), то логарифмическое априорное отношение будет отрицательным числом (-0.405). Как следствие измерение x может быть ближе к s-, чем к s+, и тем не менее наблюдатель ответит, что sˆ = s+. Противоположное происходит, когда априорное распределение благоприятствует s- (красная линия). Дру - гими словами, априорное распределение смещает наблюдателя в сторону альтернативы с наибольшей априорной вероятностью. Это явление похоже на то, как гауссово распределение априорно смещает наблюдателя в сторо- ну своего среднего значения в задаче непрерывной оценки, обсуждаемой в главах 3 и 4.\n--- Страница 197 ---\n196  Различение и обнаружение 7.2.5. Несколько задач могут иметь одно и то же байесовское решающее правило Каждое бинарное решение имеет только одно байесовское решающее пра- вило, за исключением того факта, что одно и то же правило может быть записано в математически эквивалентных формах, например запись x > 0 эквивалентна ex > 1. Однако разные задачи могут иметь одно и то же реша- ющее правило. В качестве простого примера рассмотрим решающее правило в уравнении (7.15). Существует множество способов выбора пар стимулов (s+, s-), которые имеют одно и то же среднее значение и, следовательно, одно и то же решающее правило. Следовательно, восстановить задачу по решаю- щему правилу невозможно. 7.2.6. Этап 3: распределение отклика В этом разделе обсуждается распределение отклика по многим испытаниям, в которых условия эксперимента остаются фиксированными. В нашей задаче условие эксперимента полностью определяется стимулом s , который может принимать два значения. Таким образом, распределение оценки стимула определяется вероятностью сообщить либо sˆ = s+, либо sˆ = s-, когда x выво- дится или из p(x | s = s+), или из p(x | s = s-), всего четыре варианта. Эти четыре числа можно сократить до двух, поскольку вероятность оценки стимула как s+ равна 1 минус вероятность его оценки как s-. Таким образом, распределе- ние оценки определяется следующими двумя вероятностями, соответствую- щими правильным и неправильным сообщениям наблюдателя об s+: p(sˆ = s+ | s = s+) = p(d > 0 | s = s+); (7.16) p(sˆ = s+ | s = s-) = p(d > 0 | s = s-). (7.17) Для удобства в этом подразделе мы предполагаем, что априорное рас - пределение плоское. Решение уравнения (7.17) позволяет нам рассчитать прогнозы поведения наблюдателя при нескольких испытаниях. На третьем этапе байесовского моделирования нам нужно вычислить ве- роятность того, что равенство (7.15) удовлетворяется, когда x выбирается либо из p(x | s = s+), либо из p(x | s = s-). Мы можем рассматривать это как два эквивалентных пространства: d-пространство (рис. 7.4A) и x-пространство (рис. 7.4B). Здесь мы сосредоточимся на последнем (в задаче 7.9 – на первом). На рис. 7.4В скопирован график обоих распределений с рис. 7.1. Уравнение (7.15) выполняется, когда измерение оказывается справа от вертикальной линии в kMAP. Таким образом, графически вероятность выполнения уравне- ния (7.15) представляет собой площадь под функцией плотности вероятности справа от прямой. Математически вычисление этой площади соответствует интегрированию функции плотности от kMAP до бесконечности:\n--- Страница 198 ---\nРазличение  197 (7.18) (7.19) s = s− s = s− s = s+ s = s+ p(sˆ = s+|s = s+) p(sˆ = s+|s = s+) p(sˆ = s+|s = s–) p(sˆ = s+|s = s–)(A) (В)Вероятность 0 kMAP Логарифмическое апостериорное отношение dИзмерение x Рис. 7.4  Вероятности отклика в задаче различения. Здесь показаны два эквивалентных способа визуализации этапа 3: (A) в пространстве LPR (пространство d). Распределения пере- менной решения (LPR) обусловлены истинными состояниями мира, s+ или s–. Вероятность того, что оценка MAP равна s+, равна заштрихованной области, когда истинный стимул s– (серый) или s+ (бирюзовый). Средние значения распределений равны –(s)2 2σ2 и (s)2 2σ2, а стандартное отклонение каждого равно s σ (см. задачу 7.9). Оптимальный критерий равен 0; (B) в про- странстве измерений (пространство x). Средние значения распределений равны s– и s+, а стан- дартное отклонение каждого из них равно σ. Оптимальный критерий измерения задается уравнением (7.14). Заштрихованные области имеют то же толкование, что и в (A) Одна из великих трагедий математики заключается в том, что эти интегра- лы Гаусса не могут быть вычислены аналитически, то есть через известные функции. Поэтому нам приходится просто жить с интегральными выраже- ниями. Однако, поскольку эти интегралы чрезвычайно распространены, для них обычно вводят специальные обозначения. Будем обозначать интеграль- ное нормальное распределение через Φ, а интегральное стандартное нор- мальное распределение – через Φstandard . (В некоторых других публикациях Φ используется для интегрального стандартного нормального распределения.) Тогда можно показать, что (7.20)\n--- Страница 199 ---\n198  Различение и обнаружение (7.21) О том, как получены эти уравнения, более подробно говорится в примеча- нии 7.1. Уравнения (7.20) и (7.21) представляют предсказания того, как часто испытуемые будут оценивать стимул s+, когда он в действительности явля- ется s+ или когда он в действительности является s-. Эти прогнозы можно сравнить с экспериментальными результатами, по аналогии с тем, что мы нашли в разделе 4.2 для случая непрерывной оценки. Наконец, мы можем подставить уравнение (7.14) в уравнения (7.20) и (7.21), чтобы найти (7.22) (7.23) Теория обнаружения сигналов. Эта теория постулирует наличие наблюда- теля, который применяет некоторый критерий k к x. Данный критерий может совпадать или не совпадать с байесовским критерием kMAP. Во втором случае уравнения (7.20) и (7.21) останутся в силе, но с заменой kMAP на k. В теории обнаружения сигнала разница между k и kMAP также называется смещением (bias), но мы не используем этот термин, чтобы избежать путаницы со сме- щением оценки, как это определено в разделе 4.5. Мы обсудим взаимосвязь между байесовским выводом и теорией обнаружения сигналов более под- робно в разделе 7.6. Примечание 7.1 Интегральное нормальное распределение Интегральное распределение получается из обычного распределения вероятностей путем суммирования значений слева направо с сохранением текущего результата. Например, интегральное нормальное распределение, принадлежащее нормально- му распределению со средним значением μ и дисперсией σ2, определяется как (7.24) Здесь y – аргумент интегрального распределения, тогда как x – просто переменная интегрирования. Стандартное нормальное распределение – это нормальное рас - пределение со средним значением 0 и дисперсией 1. Дадим его интегральному рас - пределению специальное обозначение: Φstandard (y) º Φ(y; 0, 1). (7.25)\n--- Страница 200 ---\nОбнаружение  199 Имеют место следующие свойства: Φ(y; μ, σ2) = (y - μ; 0, σ2); (7.26) (7.27) Φ(y; μ, σ2) + Φ(-y; -μ, σ2) = 1; (7.28) Φstandard (y) + Φstandard (-y) = 1. (7.29) Упражнение 7.3 (a) Выведите эти свойства. (b) Покажите, как мы можем использовать некоторые из этих свойств для вывода уравнения (7.20) из уравнения (7.18) (или уравнения (7.21) из уравнения (7.19)). 7.3. Обнаружение В разделе 7.1 мы привели пример рентгенолога, определяющего наличие у пациента опухоли на основе рентгеновского снимка. Это пример задачи обнаружения: есть опухоль или нет? Есть много других примеров из по- вседневной жизни. Принимая душ, мы должны определить, звонил ли наш телефон. На дороге приходится определять, есть впереди ухаб или нет. Если у нас есть газовая плита, обнаружение запаха газа может уберечь нас от опас - ности. В общем случае задача состоит в том, чтобы определить, присутствует ли сигнал в шуме. Математически задача обнаружения тесно связана с обсуждаемой до сих пор задачей различения. В задаче различения наблюдатель должен был раз- личать два значения стимула, s+ и s-. В своей простейшей форме обнаруже- ние – это особый случай, когда s+ положительно, а s- = 0, то есть наблюдатель различает определенное ненулевое значение и ноль. Во многих случаях пере- менная s несколько абстрактна; например, она может быть комбинацией различных характеристик изображения, которые радиолог использует для оценки наличия опухоли. Однако для простоты мы по-прежнему концептуа- лизируем s как одномерную переменную. Таким образом, байесовская модель, которую мы описали для различения, также может быть использована для обнаружения. Например, решающая переменная получается путем подстановки s- = 0 в уравнение (7.10), чтобы получить (7.30) Точно так же из уравнения (7.20) следует вероятность правильного обна- ружения стимула\n--- Страница 201 ---\n200  Различение и обнаружение (7.31) В задаче обнаружения вероятность положительного отклика при наличии реального сигнала называется долей совпадений, долей обнаружения, чувстви- тельностью, или долей истинных положительных откликов, тогда как вероят - ность положительного отклика при отсутствии реального сигнала называет - ся долей ложных срабатываний, или долей ложноположительных результатов. Все эти вероятности можно рассматривать как площади под кривыми на рис. 7.4. Упомянутая терминология связана с теорией обнаружения сигналов. 1 минус частота совпадений – это доля промахов , или доля ложноотрица- тельных результатов, а 1 минус частота ложных тревог называется долей правильных отклонений, специфичностью, или долей истинно отрицатель- ных результатов (рис. 7.5). Эти четыре термина (рис. 7.5) также могут быть применены к задаче на различение, такой как различение ориентации -1° от ориентации 1°, но в таких задачах нет разницы, какой стимул считается «сигналом». В нашем примере доля совпадений и правильных отклонений одинакова, равно как и доля ложных срабатываний и промахов. Следовательно, доли совпадений и ложных срабатываний в сумме дают 1. Однако в общем случае эта сумма не обязательно должна равняться 1. Ложное срабатывание (ложноположительные)1ПрисутствуетПрисутствуетИтого Совпадения (истинно положительные)1 Правильный отказ (истинно отрицательные)ОтсутствуетОтсутствуетПромахи (ложноотрицательные)Сообщаемое состояние мираИстинное состояние мира Рис. 7.5  Термины, обозначающие доли четырех типов отклика в задаче обнаружения 7.4. Уверенность в решении В задаче бинарного решения знак LPR определяет решение MAP . Однако LPR также имеет величину или абсолютное значение. Решение, принятое с LPR, равным 0.1, принимается менее уверенно, чем решение с LPR, равным 1: ведь более низкое абсолютное значение означает, что апостериорные веро- ятности двух альтернатив ближе друг к другу. Следовательно, естественной мерой уверенности (confidence) в бинарном решении является величина LPR:\n--- Страница 202 ---\nУверенность в решении  201 (7.32) Это показано на рис. 7.6. Уверенность может снизиться из-за неплоского априорного распределения. Например, когда LLR равен 0.3, а логарифмиче- ское априорное отношение равно -0.4, достоверность уменьшается с 0.3 до 0.1 из-за введения неплоского априорного значения. 2 1Уверенность Уверенность 1Уверенность 2 –2 –1 0 1 2 Логарифмическое апостериорное отношение d Рис. 7.6  Две меры уверенности как функция от LPR. Красный: абсолютное значение LPR. Синий: апостериорная вероятность выбора MAP В главе 3 мы ввели другую меру уверенности, а именно апостериорную вероятность отклика. Мы будем называть эту меру confidence1: confidence1 = p(s = sˆ | x). (7.33) В текущей задаче нас интересует оценка MAP , поэтому уверенность confidence1 равна p(s = s+ | x), если p(s = s+ | x) > 0.5, и равна 1 - p(s = s+ | x), если p(s = s+ | x) < 0.5. Эти две меры, confidence1 и confidence2, связаны от - ношением (7.34) Мы выведем его в задаче 7.8. В уравнении (7.34) снова появляется логистическая функция, с которой мы впервые столкнулись в уравнении (7.5), и почти по той же причине. Посколь- ку логистическая функция монотонно возрастает, две меры уверенности на- ходятся во взаимно однозначном соответствии, и обе являются законными мерами уверенности. На рис. 7.6 оба показателя представлены как функция LPR. Упражнение 7.4. Вопрос для размышлений: имеет ли один из двух показателей для вас больше смысла, чем другой? Объясните свой ответ.\n--- Страница 203 ---\n202  Различение и обнаружение Установив, что уверенность (любая ее мера) соответствует расстоянию от начала координат на оси переменной решения, мы можем использовать байесовскую модель для прогнозирования не только ответов наблюдателя на задачу распознавания (или обнаружения), но и того, как часто это решение принимается с высокой или низкой уверенностью. Граница между низкой и высокой степенью уверенности – это параметр, который эксперимента- тор может подогнать к данным человека. В этом заключается идея экспери- мента по оценке уверенности: испытуемого просят не только дать бинарное суждение о задаче на различение, но впоследствии также оценить уверен- ность, скажем, как низкую, среднюю или высокую. Таким образом, теперь есть шесть возможных ответов: две бинарные оценки, умноженные на три градации достоверности. Ранее мы видели, что байесовский наблюдатель де- лает бинарное суждение, определяя, в какую из двух областей пространства решений (положительную или отрицательную) попадает LPR. Точно так же байесовский наблюдатель теперь выбирает один из шести возможных отве- тов, определяя, в какую из шести областей принятия решений попадает LPR (рис. 7.7). Три из этих областей вместе образуют отрицательную ось, а дру - гие три – положительную. Слева направо эти области будут соответствовать оценке стимула как s- с высокой, средней и низкой уверенностью и оценке стимула как s+ с низкой, средней и высокой уверенностью. Всего эти регио- ны разделяют пять критериев принятия решений. При наличии M оценок уверенности количество критериев равно 2M - 1. s = s− s = s+ Логарифмическое апостериорное отношение d0ВероятностьОтклик s− Отклик s+ Высокая Высокая Средняя СредняяНизкая Низкая Рис. 7.7  Оценки уверенности (здесь низкая, средняя, высокая) подразделяют области d < 0 (ответ s –) и d > 0 (ответ s+). В этом гипотетическом эксперименте наблюдатель имеет в общей слож - ности шесть категорий ответов, расположенных в указанном по- рядке. Когда d велико по абсолютной величине, оценка уверен- ности выше\n--- Страница 204 ---\nДополнительные характеристики распределения откликов  203 7.5. Дополнительные характеристики распределения откликов В бинарных задачах, которые являются темой этой главы, мы можем дать распределению откликов дополнительные характеристики. Дальнейшие рас - суждения в этом разделе относятся как к различению, так и к обнаружению, хотя терминология (доля совпадений, доля ложных срабатываний и т. д.) в ос - новном связана с обнаружением. 7.5.1. Рабочая характеристика приемника В разделе 7.3 мы определили доли совпадений и ложных срабатываний в от - ношении одного конкретного критерия принятия решения. В задаче с оцен- ками уверенности мы можем связать количество совпадений и ложных сраба - тываний с любым критерием, разделяющим две соседние области принятия решений. В примере с тремя критериями наивысший критерий отделил бы оценки s+, сделанные со средней уверенностью, от оценок, сделанных с вы- сокой уверенностью. Обобщенная доля совпадений байесовской модели рав- на площади под распределением p(d | s = s+) решающей переменной, когда s = s+, справа от конкретного критерия k (который ранее всегда был равен нулю). Аналогично обобщенная доля ложных срабатываний равна площади под распределением p(d | s = s-) решающей переменной, когда s = s-, справа от того же критерия k . В уравнениях: H(k) = p(d > k | s = s+); (7.35) F(k) = p(d > k | s = s-). (7.36) Если есть три оценки уверенности, это дает нам пять пар показателей совпадений и ложных срабатываний, по одной для каждого критерия. По- строение графика доли совпадений H(k) в зависимости от частоты ложных срабатываний F(k) дает нам пять точек на графике с горизонтальной и верти- кальной осями в диапазоне от 0 до 1. Например, у второй точки на этом гра- фике координатой y будет доля ответов s+, сделанных со средней или высо- кой уверенностью при распределении p(d | s = s+), а координатой x будет та же доля при распределении p(d | s = s-). В пределе (при наличии очень большого количества оценок уверенности) график будет представлять собой гладкую кривую, проходящую через начало координат и через точку (1, 1). Она будет соответствовать критерию решения k, постоянно перемещающемуся вдоль оси решения справа налево, при каждом значении которого возникает веро- ятность попадания и ложного срабатывания (рис. 7.8). Эта кривая называется рабочей характеристикой приемника (receiver operating characteristic, ROC)1. 1 Термин позаимствован из области радиолокации. – Прим. перев.\n--- Страница 205 ---\n204  Различение и обнаружение Он характеризует распределения решающей переменной при любом стимуле более полно, чем исходные показатели совпадений и ложных срабатываний; последние по сути являются лишь одной точкой на ROC. Характеристика ROC параметризована по критерию. ROC является одним из наиболее важных понятий в теории обнаружения сигналов. Критерий H F Логарифмическое апостериорное отношение d(A) (B) Кривая ROCДоля совпадений H Доля ложных срабатываний F Рис. 7.8  Теоретическая рабочая характеристика приемника: (A) распределе- ние LPR d, когда стимул s+ (бирюзовая кривая) или s– (серая кривая). Критерий (пунктирная золотая линия) определяет долю совпадений H (бирюзовая об- ласть) и долю ложных срабатываний F (серая область); (B) перемещая критерий справа налево и строя H от F, мы получаем теоретическую ROC В примере, рассматриваемом в этой главе, доля совпадений равна доле правильных отказов, а доля ложных срабатываний равна доле промахов. Как следствие ROC симметрична относительно отрицательной диагонали. Упражнение 7.5. Почему это так? Однако в общем случае это не так, и в задаче вы увидите пример ROC, асимметричной относительно отрицательной диагонали. В реальном эксперименте эмпирическая ROC получается из долей откли- ков в каждой из категорий откликов 2M для каждого из двух стимулов. Сде- лать это можно, создав таблицу из 2 строк и 2M столбцов (рис. 7.9, строки I и II). Верхний ряд соответствует истинному стимулу s = s+, нижний ряд – s = s-. Каждый столбец соответствует категории отклика. Левые M столбцов соответствуют откликам s = s- в порядке убывания уверенности. Правые M столбцов соответствуют откликам s = s+ в порядке возрастания уверенности. Каждая ячейка в таблице содержит частоту ответов в каждой категории, де- ленную на общее количество ответов во всех категориях для этого стимула. Таким образом, сумма чисел в каждой строке равна 1. Далее создадим новую таблицу, в которой каждая ячейка содержит сумму числа в соответствующей ячейке и всех ячейках справа от нее в той же строке в исходной таблице. Другими словами, новая таблица (строки II и IV на рис. 7.9) строится путем интегрального суммирования чисел исходной таблицы справа налево для каждой строки отдельно. В новой таблице каждый столбец соответствует\n--- Страница 206 ---\nДополнительные характеристики распределения откликов  205 паре долей (совпадение, ложное срабатывание). Самая левая пара по опреде- лению всегда должна быть равна (1, 1). Наконец, доля совпадений (строка II) отображается в зависимости от частоты ложных срабатываний (строка IV) (рис. 7.9). Когда модель точно описывает наблюдателя, ROC, полученная из этой модели, должна проходить через точки эмпирической ROC. Обратите внимание, что не имеет значения, где наблюдатель поместит свои критерии уверенности в d -пространстве, – ROC не изменится. Сообщаемое состояние мира с оценкой уверенностиИстинное состояние мираВысокое s− s− s+Среднее s−Низкое s−Низкое s+Среднее s+Высокое s+ Накопительная сумма Накопительная сумма0.065 I 0.010 III1.000 II 1.000 IV0.239 0.0370.935 0.9900.360 0.2580.695 0.9530.262 0.3720.335 0.6950.072 0.2390.074 0.3230.002 0.0840.002 0.084 Доля совпадений H Доля ложных срабатываний F1.0 0.5 0.0 0.0 0.5 1.0(A) (B) Рис. 7.9  Эмпирическая рабочая характеристика приемника: (A) строки I и III показывают доли откликов в гипотетическом эксперименте, когда состояние мира было s+ (I) или s– (III). Участник эксперимента сообщал s+ или s– с низкой, средней или высокой степенью уверен- ности (столбцы). Сначала мы берем интегральную сумму долей справа налево; это дает ряды II и IV. Затем мы строим строку II (частота совпадений) в зависимости от строки IV (частота лож - ных срабатываний). Это дает зеленые точки в (B). Черная кривая представляет теоретическую ROC, лежащую в основе этих данных 7.5.2. Различимость В предыдущих разделах мы использовали решающее правило d > 0, где d представляет собой LPR. Однако существуют сценарии, в которых наблюда- тель будет использовать критерий, отличный от 0. Один из возможных вари- антов состоит в том, что наблюдатель неоптимален и делает неправильное предположение об априорных вероятностях. В этом случае критерий будет заменен неизвестным числом. Второй сценарий заключается в том, что на- блюдатель может (обоснованно или нет) придавать большее значение пра- вильной оценке одного стимула, чем другого (мы подробнее остановимся на этой ситуации в следующих главах). В этих сценариях решающее правило принимает вид d > k. Доли совпа- дений и ложных срабатываний относительно этого неизвестного критерия (уравнение 7.36) принимают вид (7.37)\n--- Страница 207 ---\n206  Различение и обнаружение (7.38) Использование другого k изменяет различные показатели эффективности наблюдателя. Но при этом не меняется ROC наблюдателя. Фактически эффект изменения k заключается в простом перемещении наблюдателя в другую точку той же ROC. Тот факт, что критерий не влияет на ROC наблюдателя, предполагает, что должна быть возможность получить числовую меру эффек - тивности, которая не зависит от k. Одной из таких критериально независи- мых мер эффективности является площадь под ROC (area under curve, AUC), но наиболее распространенной мерой является различимость (discriminability). Различимость, обозначаемая d¢, – это способ количественной оценки того, насколько хорошо разделены распределения решающей переменной для двух альтернатив. Эта мера определяется как d¢ = Φ-1 standard (H) - Φ-1 standard (F), (7.39) где Φ-1 standard представляет собой функцию, обратную функции Φstandard . Это означает, что Φ-1 standard (y) дает такое значение x, для которого Φstandard (x) = y. Вы можете рассматривать такую функцию как «обратное вычисление». В не- которых публикациях для обозначения Φ-1 standard используют z (z-оценка). Различимость иногда называют чувствительностью, но мы избегаем этого термина, поскольку он также может обозначать долю совпадений. Уравнение (7.39) становится более наглядным, когда мы выбираем k = kMAP. Тогда после некоторых преобразований мы находим, что (7.40) Это замечательно простое выражение не зависит от критерия (или априор- ного распределения)! Независимо от того, насколько велика или мала пред- взятость наблюдателя, чувствительность отражает только распределения ре- шающей переменной, обусловленной s, то есть сенсорным свидетельством. Чем больше эти распределения перекрываются, тем меньше d¢. Поскольку это отношение разницы между двумя различаемыми стимулами и уровнем сенсорного шума, d¢ можно интерпретировать как отношение сигнала к шуму наблюдателя для задачи. Обратите внимание, что в некоторых публикациях в качестве определения d¢ вместо уравнения (7.39) используется уравнение (7.40). Однако оно гораздо менее общее. Введение показателей эффективности, инвариантных к изменению кри- терия, было одним из главных достижений теории обнаружения сигналов. Однако важным предостережением является то, что если модель шума не является гауссовой с равной дисперсией, то уравнение (7.40) перестает вы- полняться, и d¢ в соответствии с уравнением (7.39) становится зависимым от критерия.\n--- Страница 208 ---\nСвязь между байесовским выводом и теорией обнаружения сигналов  207 Примечание 7.2 Различимость или точность? Может показаться, что существует противоречие между различимостью, d¢ и точно- стью. В теории обнаружения сигналов различимость обычно рассматривается как лучшая мера эффективности, чем точность, поскольку она не зависит от критерия. С другой стороны, точность максимизируется байесовским наблюдателем MAP , по- этому имеет смысл в качестве меры эффективности использовать точность. Это кажущееся противоречие разрешается замечанием, что байесовский наблюдатель MAP использует не какой-то произвольный критерий, а оптимальный (тот, кото- рый максимизирует апостериорную вероятность). Таким образом, точность явля- ется вполне допустимой мерой эффективности. Тем не менее может быть полезно разделить точность на частоту совпадений H и единицу минус частоту ложных сра- батываний F. Различимость эквивалентна точности (т. е. полностью коррелирует с точностью), когда распределения решающей переменной, обусловленной s, явля- ются гауссовыми с той же дисперсией. В других случаях различимость в байесов- ских моделях имеет ограниченное применение. 7.6. Связь между байесовским выводом и теорией обнаружения сигналов Теория обнаружения сигналов широко применялась во многих областях, на- чиная от обнаружения объектов на радаре (для чего эта теория была первона- чально разработана) и заканчивая выполнением клинических диагностиче- ских тестов и изучением воспроизведения слов из памяти. В каждой из этих ситуаций наблюдатель использует зашумленную информацию (радарное изображение, физиологические показатели пациента или ощущение зна- комого слова) для классификации стимула по одной из двух категорий (на- личие или отсутствие объекта, наличие или отсутствие болезни, знакомое слово или нет). Моделирование в теории обнаружения сигналов в некотором смысле яв- ляется подмножеством байесовского моделирования, а в других отношени- ях – надмножеством. Это подмножество, потому что теория обнаружения сигналов в основном занимается бинарным различением или задачами об- наружения. С другой стороны, это надмножество, потому что в этой области оно не ограничивается оптимальным правилом принятия решения. В байе- совском моделировании обычно подчеркивают, что оптимальная оценка или правило принятия решения зависят от неопределенности, как это было в уравнении (7.14). Исследования, в которых наблюдатели являются опти- мальными, даже когда оптимальность требует знания сенсорной неопреде- ленности, учат нас способам представления неопределенности (т. е. байе- совскому подходу). Эта цель обычно не ставится в исследованиях моделей, относящихся к теории обнаружения сигналов.\n--- Страница 209 ---\n208  Различение и обнаружение 7.7. Промежуточные варианты Бинарные и непрерывные переменные – это два конца спектра. Переменная стимула, которая является дискретной, но имеет большое количество воз- можных значений, близка к непрерывной. Например, можно выбрать, в ка- ком из восьми направлений движется облако точек. Все распределения веро- ятностей в байесовской модели будут функциями массы вероятностей, а не функциями плотности вероятности. В этом смысле все байесовские выводы о дискретной переменной стимула очень похожи на бинарные решения. Од- нако многие понятия, представленные в этой главе, такие как LPR, правила принятия решений и ROC, не являются естественными концепциями, когда существует более двух вариантов выбора. Разнообразие типов бинарных решений, рассматриваемых в этой главе, было весьма ограниченным. Мы рассмотрели только те, в которых класс C однозначно определяет стимул (значения которого мы обозначили как s+ и s-). Гораздо более общим является случай, когда каждый класс C опреде- ляет распределение стимула. Например, в типичном задании на различение ориентации испытуемого просят различать не отклонение на 2° вправо и 2° влево от вертикали, а стимул в виде любого наклона влево или вправо. Чтобы правильно рассмотреть этот случай, нам нужно ввести понятие маргинали- зации, что мы и сделаем в следующей главе. 7.8. Заключение В этой главе мы рассмотрели байесовскую структуру для бинарного приня- тия решений, а также теорию обнаружения сигналов. Вы узнали следующее: байесовское бинарное принятие решений основано на LPR; при различении или обнаружении байесовское правило MAP может быть определено с точки зрения критерия, применяемого к измере- нию. Этот критерий зависит от априорного соотношения и уровня сен- сорной неопределенности; при использовании пропорций не делаются различия между двумя типами правильных ответов – совпадений и правильных отказов; различимость d ¢ определяется как d ¢ = Φ-1 standard (H) - Φ-1 standard (F); в рамках теории обнаружения сигналов доля совпадений и доля пра- вильных отказов зависят от различимости и критерия или системати- ческой ошибки. ROC представляет собой кривую, полученную путем варьирования критерия; когда условные распределения переменной решения являются гауссо- выми с одинаковой дисперсией, мы имеем ; модели теории обнаружения сигналов являются как подмножеством, так и надмножеством байесовских моделей.\n--- Страница 210 ---\nЗадачи  209 7.9. Рекомендуемая литература George A. Gescheider. Psychophysics: The Fundamentals. New York: Psychology Press, 2013. David M. Green and John A. Swets. Signal Detection Theory and Psychophysics. Vol. 1. New York: Wiley, 1966. Michael J Hautus, Neil A Macmillan, and C. Douglas Creelman. Detection Theo­ ry: A User’s Guide. (2021). W. Wesley Peterson, Theodore G. Birdsall, and William C. Fox. The Theory of Signal Detectability. Transactions of the IRE Professional Group on Information Theory 4 (1954): 171–212. Frederick A. A. Kingdom and Nicolaas Prins. Psychophysics: A Practical Intro­ duction. 2nd ed. London: Academic Press, 2016. Felix A. Wichmann and N. Jeremy Hill. The Psychometric Function: I. Fitting, Sampling, and Goodness of Fit. Perception and Psychophysics 63, no. 8 (2001): 1293–1313. Thomas D. Wickens. Elementary Signal Detection Theory. Oxford: Oxford Uni- versity Press, 2001. 7.10. Задачи Задача 7.1. В медицине часто встречаются термины чувствительность (sensitivity) и специфичность (specificity) диагностического теста на забо- левание; это синонимы истинно положительных и истинно отрицательных результатов соответственно. Кроме того, (объективно верная) априорная вероятность заболевания называется его распространенностью (prevalence). Положительная прогностическая ценность (positive predictive value, PPV) – это вероятность того, что у кого-то есть заболевание, при условии что у него по- ложительный результат теста. Используйте правило Байеса, чтобы показать, что PPV = .чувствительность · распространенность чувствительность · распространенность (1 специфичность) · (1 распространенность) (7.41) Задача 7.2. Предположим, что априорное и апостериорное распределения такие же, как на рис. 7.2. (a) Рассчитайте отношение правдоподобия. (b) Достаточно ли только сенсорного свидетельства (без априорного рас - пределения), чтобы указать, что стимул был s+ или s-? (c) Указывают ли априорное распределение и правдоподобие на одну и ту же альтернативу? Задача 7.3. В этой задаче мы численно исследуем связь между апостериор- ными вероятностями и LPR.\n--- Страница 211 ---\n210  Различение и обнаружение (a) Создайте вектор из 99 возможных апостериорных вероятностей s+ от 0.01 до 0.99 с шагом 0.01. Для каждого значения вычислите LPR Затем постройте это отношение как функцию апостериорной вероятно- сти s+. График должен показать, что каждая апостериорная вероятность соответствует ровно одному LPR и наоборот (мы имеем дело с монотон- ными функциями). Знать одно так же полезно, как знать другое. (b) Почему мы не включили апостериорные вероятности 0 и 1? (c) Предположим, вы знаете LPR d. Выразите апостериорную вероятность p(s = s+ | x) того, что s = s+, как функцию только от d. Сделайте то же самое для p (s = s- | x). (d) Если LPR равен 0.1, каковы апостериорные вероятности s+ и s-? Что де- лать, если LPR равен 1? Задача 7.4. Мы определили решающее правило как сообщение об одной аль- тернативе, когда d > 0, и другой, когда d < 0. Почему случай d (x) = 0 обычно не рассматривается? Что будет делать наблюдатель, если d (x) = 0? Задача 7.5. Докажите уравнение (7.10) для логарифмического правдоподо- бия в гауссовой модели измерения. Задача 7.6. Предположим, что стимул s может принимать два значения: s+ = 1° и s- = -1°. Предположим, что измерение нормально распределено вокруг s со стандартным отклонением 0.5°. В данном испытании измерение наблюда- теля составляет -0.1°, и s = s+ встречается в 80 % испытаний. О каком стиму - ле сообщит наблюдатель: s+ или s-? Опишите все этапы вашего рассуждения. Задача 7.7. Нам нужно выбрать критерий k для задачи принятия решений так, чтобы вероятность верного выбора была максимальной. Исходя из урав- нения (7.20), выведите выражение для критерия k . Задача 7.8. Ранее в главе мы ввели две меры уверенности: confidence1 и con- fidence2. Докажите уравнение (7.34) для отношения между двумя мерами уверенности. Задача 7.9. В контексте нашей задачи различения предположим, что апри- орное распределение является плоским, так что байесовская решающая переменная d становится LLR. Мы можем рассматривать d как случайную величину, которая «наследует» свое распределение от распределения x. По - кажите, что условные распределения переменной решения, как показано на рис. 7.4, равны (7.42) (7.43) Задача 7.10. Эту задачу разработал Рональд ван ден Берг, когда был аспиран - том у Вей Цзи Ма. В первом ряду рисунка ниже каждый график показывает\n--- Страница 212 ---\nЗадачи  211 распределения вероятностей (функции плотности вероятности) логарифми- ческого апостериорного отношения для каждой из двух альтернатив в задаче принятия бинарных решений. Во втором ряду отображаются ROC. Укажите, к какому участку в верхнем ряду относится каждая ROC. 0.10 0.05 0.001.0 0.5 0.0 1.0 0.5 0.01.0 0.5 0.01.0 0.5 0.01.0 0.5 0.01.0 0.5 0.00.4 0.2 0.0 0 0246 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.00246 0246 10 20Вероятность ВероятностьH H H HВероятность Вероятность d d F F F Fd d(A) (B) (1) (2) (3) (4)(C) (D) Задача 7.11. Здесь мы моделируем ROC в задаче обнаружения. Наблюда- тель пытается обнаружить сигнал силой s+ = 3 в шуме (s- = 0). Шум имеет нормальное распределение со стандартным отклонением σ = 2. В каждом испытании экспериментатор предъявляет шум (вероятность 0.4) или шум плюс сигнал (вероятность 0.6). Задача наблюдателя – реагировать на наличие сигнала или его отсутствие. (a) Смоделируйте стимул (сигнал или шум) в каждом из 100 000 испытаний. Сохраните как вектор-столбец. (b) Смоделируйте измерения в каждом испытании. (c) На основе измерений в задании (b) рассчитайте две гистограммы из- мерений: одну для испытаний, когда сигнал присутствовал, и одну для испытаний, когда сигнал отсутствовал. Используйте в качестве основы для ваших гистограмм набор из пятидесяти сегментов, линейно распо- ложенных между -10 и 10. Нормируйте обе гистограммы. Постройте их на одном графике в виде линий (не в виде столбцов). (d) На основе измерений в задании (b) рассчитайте LPR для каждого испы- тания. Рассчитайте и постройте гистограммы LPR, аналогичные гисто- граммам измерения в задании (c). (e) Теперь предположим, что в каждом испытании наблюдатель также дает оценку уверенности, сообщая о «высокой уверенности», когда абсолют - ное значение LPR превышает 2, «средней уверенности», когда оно нахо- дится между 1 и 2, и «низкой уверенности», когда оно находится между 0 и 1. Постройте таблицу 2×6 из двух возможных стимулов (сигнал при- сутствует или отсутствует) и шести возможных ответов. В каждую ячейку поставьте долю ответа (нормализованную по строке).\n--- Страница 213 ---\n212  Различение и обнаружение (f) Рассчитайте эмпирическую ROC путем интегрального суммирования частот откликов. (g) Нанесите полученные точки поверх теоретической ROC на основе урав- нений (7.38). (h) Проведите моделирование и опишите, что происходит с ROC, когда вы уменьшаете уровень сигнала до s+ = 2. (i) Объясните изменение. Почему оно является достаточно очевидным? Задача 7.12. Рассмотрим сочетание объединения сигналов (глава 5) с зада- чей на различение из текущей главы. Судья пытается определить, виновен ли подозреваемый. Априорное убеждение присяжных о виновности составляет 0.5. У судьи есть три условно независимых свидетельства. Если бы у него было только одно из этих свидетельств (любое из них), апостериорная вероятность того, что подозреваемый виновен, составила бы 60 %. Какова апостериорная вероятность того, что подозреваемый виновен, если у судьи есть все три свидетельства? В этой задаче используется только уравнение допущения условной независимости (5.1) из главы 5, а не уравнение допущения гауссо- ва шума (5.3). Надо заметить, что с использованием правила Байеса в мире юрисдикции согласны не все, но вы должны ответить на вопрос так, как будто эта проблема уже решена. Задача 7.13. Как и предыдущая задача, эта задача связана с комбинацией сигналов для бинарного стимула. Однако теперь мы конкретизируем условия и предположим гауссову модель измерения. Рассмотрим стимул s, который с равной вероятностью принимает два значения: s+ и s-. Наблюдатель про- изводит не одно, а два условно независимых измерения x1 и x2, взятых из нормальных распределений со средним значением s и дисперсией σ2. (a) Получите выражение для логарифмического апостериорного отношения. (b) Каково оптимальное правило принятия решений с точки зрения изме- рений? (c) Повторите задания (a) и (b) для N условно независимых измерений вмес - то двух. Задача 7.14. Здесь мы расширяем математический аппарат этой главы на зашумленные измерения с неравной (зависящей от стимула) дисперсией. Рассмотрим задачу различения с двумя возможными значениями стиму - ла, s+ и s-. Предположим, что вероятности равны (плоское априорное рас - пределение). В этой главе мы предполагали, что распределения измерений p(x | s = s+) и p(x | s = s-) были нормальными с одинаковой дисперсией. Теперь предположим, что их дисперсии различны и имеют значения σ+2 и σ-2 соот - ветственно. (a) Покажите, что логарифмическое апостериорное отношение определяет - ся выражением (7.44)\n--- Страница 214 ---\nЗадачи  213 (b) Теперь предположим, что s+ = 3, s- = 0, σ+ = 3 и σ- = 1. Постройте LPR как функцию x . (c) Объясните форму графика этой функции. Сравните с рис. 7.3. (d) Не предполагая конкретных значений, упростите байесовское правило принятия решений d > 0 до набора неравенств для x. Почему вы полу - чили два неравенства, а не одно? (e) Выведите выражения для нахождения доли совпадений и ложных сра- батываний через стандартное интегральное нормальное распределение Φstandard . (f) Рассчитайте количество совпадений и ложных срабатываний для значе- ний в задании (b). Задача 7.15. Продолжим обобщение на случай неравных дисперсий, но те- перь решим задачу с помощью моделирования. Предположим, что s+ = 3, s- = 0, σ+ = 3 и σ- = 1. (a) Смоделируйте стимул (сигнал или шум) в каждом из 100 000 испытаний. Сохраните как вектор-столбец. (b) Смоделируйте измерения в каждом испытании. (c) Основываясь на измерениях в задании (b), рассчитайте две гистограммы измерений: одну для испытаний, когда сигнал присутствовал, и одну для испытаний, когда сигнал отсутствовал. Используйте в качестве основы для ваших гистограмм набор из пятидесяти сегментов, линейно распо- ложенных между -10 и 10. Нормируйте обе гистограммы. Постройте их на одном графике в виде линий (не в виде столбцов). (d) Основываясь на измерениях в задании (b) и ответе на задание 7.14a, рассчитайте LPR для каждого испытания. Рассчитайте и постройте гис - тограммы LPR, аналогичные гистограммам измерения в задании (c). (e) Основываясь на измерениях в задании (b), рассчитайте количество со- впадений и ложных тревог. Сравните с результатом в задании 7.14f.",
      "debug": {
        "start_page": 189,
        "end_page": 214
      }
    },
    {
      "name": "Глава 8. Бинарная классификация 214",
      "content": "--- Страница 215 --- (продолжение)\nГлава 8 Бинарная классификация Как определить, к какой из двух категорий относится стимул? В главе 7 мы рассмотрели задачи на различение и обнаружение, которые требуют от наблюдателя выбора между двумя конкретными значениями сти- мула s+ и s- и бинарного решения. Как в реальном мире, так и в лаборатории выбор в бинарном решении часто делается не между двумя конкретными значениями стимула, а между двумя категориями или классами, каждый из которых содержит множество значений стимула (в некоторых случаях бес - конечное). Краткое содержание главы В данной главе мы сосредоточимся на задачах бинарной классификации. В по - добных задачах наблюдателя просят сообщить не стимул, а только категорию стимула. Тем не менее значение стимула все еще неизвестно наблюдателю. Такие задачи богаче различения и обнаружения, потому что можно изучить зависимость поведения наблюдателя от стимула. С точки зрения матема- тического аппарата бинарная классификация требует, чтобы байесовский наблюдатель интегрировал все возможные значения этого неизвестного сти- мула – операция, называемая маргинализацией. Маргинализация является центральной операцией в байесовских моделях практически всех задач, кро- ме самых простых. Таким образом, эта глава является воротами в обширную область применений байесовских моделей. 8.1. Примеры задач бинарной классификации Задачи бинарной классификации широко распространены в мире. Вот три примера: необходимо решить, движется ли автомобиль вдалеке по проселочной дороге к вам или от вас. Вы классифицируете вектор скорости по его знаку (положительный или отрицательный);\nГлава 8 Бинарная классификация Как определить, к какой из двух категорий относится стимул? В главе 7 мы рассмотрели задачи на различение и обнаружение, которые требуют от наблюдателя выбора между двумя конкретными значениями сти- мула s+ и s- и бинарного решения. Как в реальном мире, так и в лаборатории выбор в бинарном решении часто делается не между двумя конкретными значениями стимула, а между двумя категориями или классами, каждый из которых содержит множество значений стимула (в некоторых случаях бес - конечное). Краткое содержание главы В данной главе мы сосредоточимся на задачах бинарной классификации. В по - добных задачах наблюдателя просят сообщить не стимул, а только категорию стимула. Тем не менее значение стимула все еще неизвестно наблюдателю. Такие задачи богаче различения и обнаружения, потому что можно изучить зависимость поведения наблюдателя от стимула. С точки зрения матема- тического аппарата бинарная классификация требует, чтобы байесовский наблюдатель интегрировал все возможные значения этого неизвестного сти- мула – операция, называемая маргинализацией. Маргинализация является центральной операцией в байесовских моделях практически всех задач, кро- ме самых простых. Таким образом, эта глава является воротами в обширную область применений байесовских моделей. 8.1. Примеры задач бинарной классификации Задачи бинарной классификации широко распространены в мире. Вот три примера: необходимо решить, движется ли автомобиль вдалеке по проселочной дороге к вам или от вас. Вы классифицируете вектор скорости по его знаку (положительный или отрицательный);\n--- Страница 216 ---\nПримеры задач бинарной классификации  215 необходимо решить, является ли человек, приближающийся к вам, тем другом, которого вы ждете, или нет. Вы классифицируете изображение как «друг» или «чужак»; необходимо решить, является облако дождевым или нет. Задачи бинарной классификации также очень распространены в экспери- ментах по восприятию в лаборатории. Вот три других примера: испытуемый должен решить, имеет ли шумное облако движущихся точек чистое направление движения вправо или влево. Он классифи- цирует чистый вектор движения по его знаку (влево или вправо); за долю секунды испытуемый должен решить, присутствует ли в при- родной сцене животное; испытуемый должен решить, относится ли стимул, имеющий размер и ориентацию, к категории 1 или категории 2, которые определены экспериментатором. Хотя любой из приведенных выше примеров можно использовать для по- строения формальной модели, здесь мы будем использовать чрезвычайно простой пример с одним признаком, так что потребуется лишь несколько дополнительных предположений, и мы сможем сосредоточиться на сущ- ности классификации. Рассмотрим следующую распространенную визуаль- ную задачу. Вам на короткое время показывают ориентированный паттерн (визуальный шаблон), подобный изображенному на рис. 8.1А. Ориентация паттерна выбирается произвольно из многих возможных значений, и ваша задача состоит в том, чтобы сообщить нажатием клавиши, был ли шаблон наклонен влево (против часовой стрелки) или вправо (по часовой стрелке) от вертикали. Данные в этой задаче состоят из стимула и ответа о категории по каждому испытанию. Если стимулы дискретны, например от -5° до 5° с шагом в 1°, то обычно для каждого предъявляемого стимула рассчитыва- ется доля попыток, в которых испытуемый сообщает об одной альтернативе, скажем «вправо». Эту пропорцию можно изобразить как функцию стимула (рис. 8.1В). Результатом является психометрическая кривая. Это нанесенная на график сводка реакции человека в зависимости от физической величины, которая варьируется экспериментатором. Очевидно, что чем сильнее сти- мул ориентирован вправо, тем больше будет доля ответов «вправо». Одним из результатов байесовского моделирования в этом разделе будет модель психометрической кривой. Но сначала рассмотрим два предостережения относительно психометрической кривой: если стимул может принимать большое количество значений или явля- ется непрерывным, то для построения визуально полезной психомет - рической кривой значения стимула должны быть разделены на группы или иным образом сгруппированы вместе. Хотя такая группировка полезна для визуализации, она теряет информацию. Поэтому любой количественный анализ данных в идеале основан на необработанных данных; даже если стимул может принимать лишь небольшое количество зна- чений, как на рис. 8.1В, психометрическая кривая не отражает всех необработанных данных: здесь не хватает количества испытаний, в ко-\n--- Страница 217 ---\n216  Бинарная классификация торых предъявлялся каждый стимул. Во многих экспериментах это число одинаково для всех возможных стимулов и для всех испытуемых, поэтому его нужно сообщить только один раз. 1.0 0.8 0.6 0.4 0.2 0.0(A) (B) Доля ответов «вправо» Ориентация стимула s, °–5 –4 –3 –2 –1 012345 Рис. 8.1  Эксперимент по бинарной классификации: (A) пример стимула Габора; (B) психометрическая кривая Несмотря на то что сценарии подобного типа часто называют задачами «различения», этот термин неточен (табл. 8.1), поскольку они относятся к бо - лее широкому понятию классификации, или категоризации. В чем разница? При различении количество значений, которые может принимать стимул, и количество возможных ответов строго равны двум. При классификации стимул может принимать более двух значений, но количество возможных ответов (обычно два) меньше, чем количество значений стимула. Таким об- разом, дискриминация и ограниченная форма обнаружения в главе 7 явля- ются частными случаями классификации. В нашем классификационном эксперименте мы будем следовать тому же рецепту, что и в предыдущих главах: порождающая модель, вывод и рас - пределение оценок. Однако порождающая модель теперь будет иметь ин- тересный дополнительный компонент, а именно распределения стимулов, обусловленных классами. Таблица 8.1. Типы задач Тип задачи Кол-во отдельных стимуловВозможные ответы Различение 2 2 идентификатора стимула Обнаружение 2 (присутствует/ отсутствует)2 (присутствует/отсутствует) Идентификация n > 2 n идентификаторов стимула Поиск (обнаружение) n 2 (присутствует/отсутствует) Поиск (локализация) n n расположений Классификация/категоризация n < n категорий\n--- Страница 218 ---\nПорождающая модель  217 8.2. Порождающая модель Структура порождающей модели графически показана на рис. 8.2А. Она со- держит три узла: класс C, стимул s и измерение x. Наблюдателя просят со- общить C – представляющее интерес состояние мира. Как и в предыдущих главах, каждый узел связан со своим собственным распределением вероят - ностей. Далее мы обсудим их по порядку. Класс. Обозначим два возможных значения C как 1 (в примере: вправо) и -1 (в примере: влево). Здесь мы могли бы выбрать любые два значения, но в примере на рис. 8.1 состояние C естественным образом равно s, включая знак. С этим состоянием связано распределение p(C), определяемое двумя значениями p(C = 1) и p(C = -1), которые в сумме должны давать 1. Эти вероятности отражают предполагаемое преобладание правого и левого на- правлений наклона стимула в эксперименте. Во многих экспериментах класс выбирается случайно с вероятностью 0.5, так что если наблюдателю это из- вестно, то вероятности p(C = 1) и p(C = -1) равны 0.5, но мы не ограничива- емся этим случаем. Стимул. Когда класс C равен -1, экспериментатор случайным образом выбирает стимул из одного набора значений; когда C = 1, экспериментатор случайным образом выбирает стимул из другого набора значений. Обозна- чим соответствующие распределения стимулов через p(s | C = -1) и p(s | C = 1) соответственно. Это так называемые обусловленные классом распределения стимулов (class-conditioned stimulus distributions, CCSD). Здесь мы исходим из допущения, что предполагаемые CCSD наблюдателя совпадают с экспе- риментальными. Измерение. Последний этап порождающей модели такой же, как и в пре- дыдущих главах: мы предполагаем, что измерение наблюдателя x получено из нормального распределения с центром в стимуле со стандартным откло- нением σ : (8.1) 8.2.1. Зеркально симметричные распределения стимулов, обусловленные классами В случае бинарной классификации важно отметить, что некоторые CCSD представляют собой пары, образованные зеркальным отражением друг друга . Вот четыре конкретных примера CCSD этого типа (рис. 8.2B): случай 1: стимул является дискретным и выбирается с равной вероят - ностью из n возможных значений: для C = 1 это s1, s2, , sn (все поло- жительные числа), а для C = -1 они зеркально симметричные: -s1, -s2, , -sn (все отрицательные числа). Эта процедура называется методом постоянных стимулов;\n--- Страница 219 ---\n218  Бинарная классификация p(C) p(s|C )p(s|C = –1) p(s|C = 1) p(x|s)C s x(A) (В) Вероятность Вероятность Вероятность Вероятность 0 0 00–а –μσsа μ Стимул sСлучай 1 Случай 2 Случай 3 Случай 4 Рис. 8.2  (A) Структура порождающей модели; (B) примеры зеркально симметричных распределений стимулов, обусловленных классом случай 2: стимул является непрерывным и получен из равномерного распределения на интервале [- a, 0], когда C = -1, и из равномерного распределения на [0, a ], когда C = 1, где a – положительное число; случай 3: стимул является непрерывным и получен из гауссова рас - пределения со средним значением 0, но затем относится к классу -1 или 1 в соответствии с его знаком; случай 4: стимул является непрерывным и получен из гауссова распре- деления (дисперсия σs2) со средним значением -μ при C = -1 и средним значением μ при C = 1. 8.3. Маргинализация Итак, мы определили обусловленное стимулом распределение вероятности измерения p(x | s) и обусловленное классом распределение стимула p(s | C). Однако у нас нет обусловленного классом распределения измерения p(x | C), которое необходимо для вывода. Чтобы получить выражение для p(x | C), мы сначала введем важное общее математическое тождество, называемое мар- гинализацией. Маргинализация характерна для байесовских моделей и не- избежна во всех задачах, кроме самых простых. Сочетание правила Байеса и маргинализации лежит в основе практически всего байесовского модели- рования.\n--- Страница 220 ---\nМаргинализация  219 8.3.1. Сумма двух бросков кубика В теории вероятностей маргинализация – это операция превращения рас - пределения вероятностей по нескольким переменным в распределение по одной из них. Например, если a и b – дискретные случайные величины, а p(a, b) – их совместное распределение, то суммирование совместного рас - пределения по b дает распределение по a : (8.2) Используя определение условной вероятности, мы также можем записать это как (8.3) В качестве примера предположим, что мы бросаем две игральные кости по одной за раз. В этой игре мы получаем вознаграждение, если общий ре- зультат двух бросков равен 10. Какова вероятность того, что это произойдет? Чтобы выяснить это, мы можем рассмотреть вероятность каждого значения, полученного в результате первого броска, и вероятность того, что в сумме выпадет 10 с учетом этого первого значения: (8.4) Мы выполняем маргинализацию по значению первого броска. Чтобы вы- разить формулу маргинализации словами, заменим каждое умножение на «и», а каждое сложение на «или». Мы утверждаем, что вероятность выпаде- ния 10 – это вероятность того, что на первом кубике выпадет 1 И сумма будет равна 10, если выпадет 1, ИЛИ что на первом кубике выпадет 2 И сумма будет равна 10, если на первом кубике выпадет 2, и т. д. Чтобы вычислить сумму маргинализации, заметим, что если на первом кубике выпадает 1, 2 или 3, то сумма бросков двух кубиков никак не может достичь 10; если на первом кубике выпадает 4, 5 или 6, то на втором кубике должно выпасть 6, 5 или 4 соответственно, и каждое из этих событий происходит с вероятностью 1/6. Таким образом, мы имеем: (8.5) Заметим, что нас интересует только вероятность итогового значения, но для того, чтобы его вычислить, мы тем не менее должны рассмотреть все возможные значения первого броска. Поскольку мы должны принять во вни- мание первый бросок, который нас в действительности не заботит, значение первого броска называется мешающей переменной (nuisance variable).\n--- Страница 221 ---\n220  Бинарная классификация Примечание 8.1 Этимология Откуда произошло название «маргинализация»? В своей основной форме маргина- лизация описывается уравнением (8.2) p(a) = Σbp(a, b). Таким образом, мы можем рассматривать маргинализацию как сумму совместного распределения вероятно- стей по одному измерению, в оценке которого мы не заинтересованы. Если повто- рить суммирование для каждого значения соответствующего измерения (напри- мер, не только для суммы бросков 10, но и для всех сумм; не только для фермеров, но и для всех профессий), то маргинализация сведет полное совместное распре- деление к распределению только по интересующему измерению. В графическом представлении суммирование происходит по направлению к «краю» (margin) со- вместного распределения, откуда и возникло название процедуры (рис. 8.3). Первый бросок Сумма двух бросков Географический регионПекарь Врач Фермер Ученый Секретарь Учитель(A) (В) Рис. 8.3  Маргинализация. Каждая панель показывает совместное распределение ве- роятностей по двум переменным. Коричневые линии представляют маргинализацию по мешающей переменной – процедуру, которая сводит двумерное распределение к одно- мерному распределению по интересующей переменной: (A) пример с игральными костя- ми. Значение в каждом квадрате – это вероятность конкретной пары (значение перво- го броска, итоговое значение). Маргинализация по значению первого броска приводит к распределению вероятностей по итоговому значению (верхние числа); (B) пример с фермером. Значения в каждом квадрате (не показаны) представляют долю граждан, характеризуемых соответствующей парой (род занятий, географический регион). Мар- гинализация по региону дает распределение вероятностей по роду занятий (показано только небольшое подмножество профессий) В качестве второй иллюстрации используем модификацию примера, пред- ставленного в разделе 4.5.2. Допустим, мы хотим узнать вероятность того, что случайно выбранный гражданин определенной страны является фер- мером. В стране двенадцать географических регионов. Предположим, мы нашли альманах, в котором приведены данные о доле населения, прожива- ющего в каждом регионе, а также о доле фермеров в каждом регионе. Чтобы получить желаемую вероятность, необходимо перемножить эти две доли для каждого региона, а затем просуммировать по всем регионам. Здесь мешаю- щей переменной является регион проживания: (8.6)\n--- Страница 222 ---\nМаргинализация  221 Мы утверждаем, что вероятность случайно встретить фермера – это веро- ятность того, что мы случайно выберем человека из региона 1 И что случайно выбранный человек из региона 1 является фермером, ИЛИ что мы случайно выберем человека из региона 2 И что случайно выбранный человек из ре- гиона 2 – фермер и т. д. Мы можем выстроить эту процедуру иначе: сначала случайный выбор региона с вероятностью, пропорциональной населению региона, а затем случайный выбор человека из этого региона – это не влияет на результат. 8.3.2. Непрерывные переменные Во многих случаях нам приходится маргинализировать непрерывные ла- тентные переменные. Если b – непрерывная переменная, маргинализация заключается в интегрировании: (8.7) (8.8) В таких случаях процедура маргинализации остается прежней, с той лишь разницей, что дискретные суммы заменяются интегралами. Одним из примеров такой непрерывной маргинализации является случай, когда мы хотим вычислить распределение вероятностей суммы двух не- прерывных переменных a и b. Его можно рассматривать как непрерывный аналог примера «сумма двух бросков кубика». Мы предполагаем, что a и b имеют гауссово распределение: p(a) = �(a; μa, σa2); (8.9) p(b) = �(b; μb, σb2). (8.10) Обозначив переменную суммы через c , получаем p(c) = �(c; μa + μb, σa2 + σb2). (8.11) Упражнение 8.1. Покажите это, либо решив интеграл, либо обратившись к по- лученному ранее уравнению. 8.3.3. Условная маргинализация Основная форма маргинализации – уравнения (8.2) и (8.3) – сохраняет свою актуальность, если все вероятности уже обусловлены другими переменными, например с : (8.12)\n--- Страница 223 ---\n222  Бинарная классификация Предположим, что мы хотим узнать вероятность того, что случайно вы- бранный гражданин среднего возраста (допустим, человек в возрасте от 45 до 65 лет) из упомянутой выше страны является фермером. Если мы определим молодой возраст как C = 1, средний возраст как C = 2, а пожилой как C = 3, то при условии, что C = 2, мы могли бы вычислить вероятность: (8.13) Вероятность того, что случайно выбранный человек среднего возраста является фермером, равна вероятности того, что случайно выбранный че- ловек среднего возраста проживает в регионе 1 И что случайно выбранный человек среднего возраста из региона 1 является фермером, ИЛИ что слу - чайно выбранный человек среднего возраста проживает в регионе 2 И что случайно выбранный человек среднего возраста из региона 2 является фер- мером и т. д. В качестве еще одного примера условной маргинализации рассмотрим распространение определенного вируса. Предположим, нас интересует ве- роятность p(x = 1 | C = 1) того, что непривитый человек, контактировавший с инфицированным непривитым человеком (обозначим это воздействие C = 1), сам заразится вирусом (x = 1). Сложность заключается в том, что вирус имеет несколько штаммов, которые мы будем обозначать через si, где i = 1, …, n (здесь n – количество штаммов). Переменная si в данном случае является мешающей. Каждый штамм имеет свою долю передачи среди не- привитых людей p(x = 1 | si, C = 1) и свою собственную распространенность среди инфицированных непривитых людей p(si | C = 1). Чтобы получить ответ на наш вопрос, мы сначала перемножим эти две доли для каждого вариан- та, что даст p(x = 1 | si, C = 1)p(si | C = 1) для каждого i. Это следует толковать как вероятность того, что инфицированный непривитый человек является носителем штамма si И передает его. Наконец, выполним суммирование по всем штаммам согласно следующему уравнению (8.14) Упражнение 8.2. Оригинальное издание этой книги было подготовлено во время пандемии COVID-19. Подумайте, как эта формулировка задачи при- менима к пандемиям и какие допущения моделирования оправданы. В част - ности, что вы можете сказать о взаимодействии между людьми? 8.3.4. Использование порождающей модели Вернемся к аналитической части нашей главы. Напомним, что порождающая модель (рис. 8.2А) определяет распределение p(s | C) стимула, обусловленного классом, который может быть или не быть зеркально симметричным, и рас - пределение измерения p(х | с), обусловленное стимулом. Несмотря на свою\n--- Страница 224 ---\nВывод  223 центральную роль в порождающей модели, стимул s является мешающей переменной; это не наблюдение (измерение, x) и не представляющая интерес переменная (C ). Чтобы сделать вывод, мы должны вычислить обусловленное классом распределение измерения p(x | C), которое мы получаем путем мар- гинализации по s : при дискретном s : (8.15) при непрерывном s : (8.16) Эти правила выводятся в разделе B.11.3 приложения. Уравнения (8.15)–(8.16) являются полностью общими; это математические тождества, которые сохраняются независимо от контекста. Теперь восполь- зуемся структурой порождающей модели. В порождающей модели распре- деление x зависит только от s, а не напрямую от C. На рис. 8.2 об этом свиде- тельствует тот факт, что единственная стрелка, указывающая на x, исходит от s; не существует стрелки от C до x. Другими словами, если нас интересует распределение x, то при известном s знание C является избыточным. Ма- тематически это выражается в том, что условное распределение p(x | s, C) идентично p(x | s). Подставляя его в уравнения (8.15)–(8.16), мы приходим к следующим выражениям для вероятности класса: при дискретном s : (8.17) при непрерывном s : (8.18) Эти уравнения действуют как своего рода цепное правило, связывающее класс C с измерением x посредством промежуточной переменной – стиму - ла s. 8.4. Вывод В текущем испытании наблюдатель производит измерение х. Поскольку на- блюдателя интересует класс C, апостериорное распределение, которое мы хотим вычислить, теперь равно p(C | x), а не p(s | x). Впервые в книге стимул s не появляется в апостериорной части: он представляет не предмет прямого интереса, а только класс. Тем не менее логика вывода точно такая же, как\n--- Страница 225 ---\n224  Бинарная классификация и в предыдущих главах. Как и в главе 7, байесовский наблюдатель принимает решение на основе логарифмического апостериорного отношения, но теперь по классу: (8.19) (8.20) Правдоподобия класса 1 p(x | C = 1) и класса -1 p(x | C = -1) могут быть полу - чены из соответствующих распределений в порождающей модели. Посколь- ку s непрерывно, эти распределения задаются уравнением маргинализации (8.18). В результате мы приходим к следующим правдоподобиям: (8.21) (8.22) С логической точки зрения эти уравнения можно интерпретировать как «распространение» информации о неопределенности: px|s(x | s) как функция s является правдоподобием стимула и отражает сенсорную неопределенность. Напротив, px|C(x | C) как функция C представляет собой правдоподобие класса и отражает классовую неопределенность: (8.23) Аналогичное выражение справедливо и для дискретных s. Таким образом, неопределенность относительно переменной «нижнего уровня» s трансфор- мируется или распространяется на неопределенность относительно пере- менной класса «более высокого уровня». Это преобразование опосредовано полученными «сверху вниз» знаниями CCSD. В отличие от вероятностей, CCSD зависят только от задачи (точнее, от убеждений наблюдателя о задаче) и не меняются от испытания к испытанию. Графически вероятность класса 1 представляет собой «перекрытие» между вероятностью по s и CCSD для класса 1: сначала умножаем, затем находим площадь (рис. 8.4). Если получившаяся площадь небольшая, значит, пере - крытие невелико. Подставляя уравнения (8.17)–(8.18) обратно в уравнение (8.20), находим LPR: при дискретном s : (8.24)\n--- Страница 226 ---\nВывод  225 при непрерывном s : (8.25) Правило оптимального решения: «сообщить C ˆ = 1, если d > 0». (8.26) p(x|C = 1) = 0.093 p(x|C = 1) = 0.034–10 –10–5 –50 0s s5 510 10(A) (В) Рис. 8.4  Графическое объяснение расчета вероятности класса Как и в главе 7, априорное знание относительно C смещает решение на- блюдателя, и его влияние тем сильнее, чем слабее сенсорное свидетельство, выраженное логарифмическим отношением правдоподобия. За исключением нескольких особых случаев, невозможно получить ана- литическим путем решающее правило из уравнения (8.26). Наилучшей стра- тегией обычно является численное переопределение неравенства d > 0 от - носительно x . Плоское априорное распределение класса. Успешный аналитический вы- вод достижим в важном частном случае, а именно когда наблюдатель ис - пользует плоское априорное распределение p(C = 1) = p(C = -1) = 0.5, а CCSD зеркально симметричны. Априорное условие означает, что наблюдатель – правильно или неправильно – считает, что классы встречаются одинаково часто (см. обсуждение субоптимальных байесовских наблюдателей в разде- ле 3.5). Поскольку априорное распределение вероятности является плоским, оценка MAP эквивалентна заявлению о C = 1, когда вероятность p(x | C = 1) превышает вероятность p(x | C = -1). Это делает задачу полностью симмет - ричной, и единственным разумным кандидатом в оптимальное решающее правило является x > 0. Но доказать это непросто, и мы вернемся к доказа-\n--- Страница 227 ---\n226  Бинарная классификация тельству в задаче 8.7. Однако мы можем извлечь урок из этого особого случая. Правило принятия решений x > 0 не зависит от CCSD p(s | C), хотя общее правило d > 0 зависит (см. уравнение (8.25)). Это означает, что когда априорное распределение вероятностей является плоским, наблюдатель может иметь совершенно не- правильное мнение о форме CCSD, но принять оптимальное решение просто потому, что неправильное убеждение не имеет отношения к правилу приня- тия решения. Таким образом, неправильные представления о порождающей модели не всегда вызывают неоптимальное поведение. (Имейте в виду, что мы по-прежнему предполагаем p(C = 1) = 0.5, а CCSD зеркально симметрич- ны. Если какое-либо условие нарушается, то убеждения наблюдателя о CCSD имеют значение как для правила принятия решений, так и для эффективно- сти.) Короче говоря, в байесовском моделировании неправильные предпо- ложения наблюдателя о порождающей модели не всегда влияют на правило принятия решения. Примечание 8.2 Проблема с методом постоянных стимулов Хотя случай 2 (дискретный), вероятно, является наиболее распространенным CCSD в экспериментах по бинарной классификации, он неидеален с точки зрения байе- совского моделирования. Причина в том, что испытуемый вряд ли узнает точное экспериментальное распределение, потому что оно настолько «остроконечное», что расположение «шипов» нужно выучить. Поэтому наблюдатель, скорее всего, ап- проксимировал бы это распределение непрерывным распределением, но не ясно, каким именно. Это не имеет значения, если наблюдатель имеет априорное убежде- ние относительно класса, равное 0.5, но это будет иметь значение, если существует возможность априорной вероятности класса, отличной от 0.5. Разница не обяза- тельно будет большой, но вы не должны слепо рассчитывать на это. 8.5. Распределение отклика Когда стимулом является s, распределение отклика определяется вероят - ностью сообщить либо Cˆ = 1, либо Cˆ = -1 при данном s. Определим первую вероятность: p(Cˆ = 1 | s) = p(d > 0 | s) (8.27) = p(x > k | s), (8.28) где k – критерий, который мы можем численно вычислить на этапе 2. Урав- нение (8.28) – это вероятность того, что измерение x, полученное из p(x | s), будет больше, чем k. Далее мы можем вычислить эту вероятность, используя уравнение (8.1):\n--- Страница 228 ---\nРаспределение отклика  227 (8.29) (8.30) где мы использовали те же шаги, которые дали нам уравнение (7.20), а Φstandard – это интегральное стандартное нормальное распределение (см. примечание 7.1). Уравнение (8.30) имеет важные свойства. Во-первых, это монотонная функция: когда s увеличивается (например, ориентация ста- новится более наклоненной вправо), вероятность сообщения о C = 1 также увеличивается. Кривая имеет характерную сигмовидную форму, показанную на рис. 8.1В. Во-вторых, когда шума больше или стимул ближе к критерию, величина меньше по абсолютной величине (ближе к 0), а вероятность сообщения класса 1 будет ближе к 0.5; это логично, потому что в обоих сце- нариях задача будет сложнее. Психометрическая кривая. Теперь мы наконец готовы построить пси- хометрическую кривую, предсказанную байесовской моделью. Она опреде- ляется уравнением (8.30) как функция s. Предсказанная психометрическая кривая представляет собой интегральное нормальное распределение, ко- торое пересекает 0.5, когда s = k. На языке психофизиологии k – это точка субъективного равенства (point of subjective equality, PSE): значение стимула, при котором субъект (в данном случае байесовский наблюдатель) одинаково часто сообщает о двух классах. Наклон психометрической кривой обычно определяется как величина, обратная стандартному отклонению интеграль- ной нормы, то есть 1/σ . Поскольку в целом k зависит от уровня сенсорного шума σ, параметра (параметров) CCSD и логарифмического априорного от - ношения по классу, окончательная психометрическая кривая также зависит от всех этих параметров. Доля правильных ответов. В главе 7 мы рассчитали частоту совпаде- ний H и частоту ложных срабатываний F в задаче обнаружения. Знание H и F эквивалентно знанию H и частоты правильного отказа 1 - F. Здесь мы можем аналогичным образом вычислить вероятность правильного сообще- ния о классе 1, обозначаемого PC1, и вероятность правильного сообщения о классе -1, обозначаемого PC-1. Эти вероятности аналогичны вероятности в уравнении (8.30) для распределения откликов, за исключением того, что обусловливание относится к (истинному) классу С, а не к стимулу s. Рассмот - рим PC1: PC1 = p(Cˆ = 1 | C = 1) (8.31) = p(d > k | C = 1) (8.32) (8.33)\n--- Страница 229 ---\n228  Бинарная классификация где сумма берется по всем испытаниям эксперимента, обозначенным t, для которых Ct равно 1, ntrials,1 – их количество, st – стимул в t-м испытании. Сумму почти всегда находят численными методами. Выражение для PC-1 аналогично. Общая доля правильных ответов, согласно модели, представляет собой взвешенную сумму PC1 и PC-1: PC = p(C = 1)PC1 + p(C = -1)PC-1, (8.34) где p(C = 1) и p(C = -1) представляют собой истинные частоты испытаний C = 1 и C = -1. Прогнозы относительно доли правильных ответов представляют собой обедненное описание прогнозируемого поведения. Они содержат общее среднее по стимулам, тогда как психометрическая кривая предсказывает вероятность сообщения о классе 1 для каждого значения стимула. В более общем смысле, при оценке модели (после подгонки ее параметров) на ос - новании статистики ее прогнозов, используемая сводная статистика обыч- но является результатом компромисса между простотой визуализации (или простотой численного отчета) и гранулярностью статистики с более инфор- мативным содержанием. В этом компромиссе прогнозы на уровне испыта- ний обычно трудно визуализировать, но они наиболее информативны, тогда как доля правильных ответов наиболее легко отображается, но наименее информативна. Рекомендуется рассматривать соответствие модели данным, используя сводную статистику с различными уровнями детализации. 8.6. Другие распределения стимулов, обусловленных классами Хотя во многих экспериментах, использующих бинарную классификацию, CCSD являются взаимно зеркальными отражениями, это излишне строгое ограничение. Мир новых возможностей открывается, если мы рассмотрим другие условия. Например, в примерах с друзьями и животными в разделе 8.1 нет симметричных классов. В обоих случаях один класс представляет собой ограниченный набор стимулов (изображения друга/животного), тогда как другой класс представляет собой широкий, охватывающий класс (изображе- ния случайных людей или изображения, не содержащие животных). Чтобы проиллюстрировать такие случаи, упростим их суть: узкий класс «встроен» в широкий класс. Хотя изображения являются сложными стимулами, мы можем определить такие классы даже для однокомпонентных (одномерных) стимулов – экспериментаторы действительно так и поступают. Математиче- ская база принципиально не отличается от предыдущего раздела. Этап 1: порождающая модель. Обозначим классы как C = 1 и C = 2, поскольку их CCSD не являются взаимно зеркальным отражением и обо- значение C = -1 меньше подходит для второй категории. Стимулы класса 1\n--- Страница 230 ---\nДругие распределения стимулов, обусловленных классами  229 взяты из гауссова распределения со средним значением 0 и дисперсией σ12. Стимулы класса 2 берутся из гауссова распределения со средним значени- ем 0 и дисперсией σ22, которая больше, чем σ12. Распределения показаны на рис. 8.4А, а примеры стимулов – на панели В. Когда CCSD перекрываются (независимо от того, являются ли они взаим- но зеркальными отражениями), наблюдатель не может достичь идеальной производительности даже при отсутствии сенсорного шума. Эта ситуация порождает неоднозначность: один и тот же стимул мог исходить от более чем одного класса, хотя обычно с разной вероятностью. Как мы отмечали ранее (раздел 1.3), неоднозначность часто встречается в восприятии. Одним из примеров неоднозначности является восприятие объемной сцены одним глазом – изображение на сетчатке могло быть создано многими трехмер- ными сценами. Здесь CCSD представляет собой трехмерное обусловленное сценой распределение изображения на сетчатке. Этап 2: вывод. Наблюдатель делает вывод о классе C на основе измере- ния x. Как и в уравнениях (8.17)–(8.18), вероятность класса C определяется интегралом по s : (8.35) Этот интеграл имеет аналитическое решение, с которым мы уже встреча- лись в уравнении (8.11): �(C; x) = �(x; 0, σ2 + σ2 C), (8.36) где σC равно σ1 или σ2. Смысл здесь в том, что измерение x является резуль- татом двух независимых зашумленных процессов: одного внешнего с дис - персией σC2 и одного внутреннего с дисперсией σ2. Согласно примечанию 4.1, общая дисперсия представляет собой сумму этих дисперсий. Если бы априорное распределение было плоским, решение MAP было бы решением с максимальной вероятностью. Правило принятия решения по максимальному правдоподобию, в свою очередь, можно было бы вывести графически из построения обусловленных классом распределений измерений (class-conditioned measurement distribution, CCMD) p(x | C) при заданном уров- не шума σ (рис. 8.5C): для любого x решение с максимальным правдоподо- бием состоит в том, чтобы выбрать класс, для которого CCMD имеет более высокое значение. Таким образом, решение меняется в точках пересечения двух CCMD. Если априорное распределение не является плоским, это уже не так. В об- щем случае априорного распределения LPR принимает вид (см. задачу 8.8): (8.37) Мы изобразили это выражение на рис. 8.5D. Это первый случай, когда мы столкнулись с LPR, квадратичным по изме- рению; все предыдущие LPR были линейными. Однако в этом есть смысл.\n--- Страница 231 ---\n230  Бинарная классификация По рис. 8.5D видно, что вероятность x выше для класса 1, чем для класса 2, когда x попадает в узкую область около 0. Иными словами, наблюдатель MAP сообщает о классе 1, когда d > 0. Это неравенство можно переписать для из- мерения x. Во-первых, мы видим, что если то d от- рицательно и наблюдатель сообщает о классе 2 независимо от значения x . Упражнение 8.3. Почему это так? Вероятность ВероятностьЛогарифмическое апостериорное отношение d Логарифмическое апостериорное отношение d Вероятность –20 –20 –20–20 –200 0 00 020 20 2020 20Ориентация s Измерение x Измерение xИзмерение x Измерение xp(s|C = 1) σ1 = 3p(x|C = 1) p(s|C = 2) σ2 = 12p(x|C = 2)Без шума ( σ = 0) Без шума ( σ = 0)Без шума ( σ = 5) Без шума ( σ = 5)(C) (D)(A) (B) 2 0 –2 –42 0 –2 –4Примеры класса 1 Примеры класса 2 Рис. 8.5  Задача встроенного класса: (A) CCSD по ориентации (в градусах). Распределения имеют одно и то же среднее, но разные стандартные отклонения и поэтому не являются вза- имным зеркальным отражением; (B) типичные примеры стимулов в каждом классе; (C) рас - пределения измерений с учетом класса при двух разных уровнях шума. Чем выше уровень шума, тем шире CCMD. Вертикальные пунктирные линии указывают точки пересечения CCMD C = 1 и C = 2; (D) соответствующий LPR по C как функция измерения при допущении равных априорных распределений. Чем выше уровень шума, тем медленнее снижается свидетельство в пользу C = 1. Горизонтальная пунктирная линия указывает значение d = 0, которое соот - ветствует точкам пересечения в (C). Чем выше шум, тем больше область, для которой d > 0, и наблюдатель сообщает «C = 1» Во-вторых, если это условие не выполняется, то правило принятия реше- ния MAP принимает вид (8.38) Другими словами, есть два критерия принятия решения об измерении, по одному с каждой стороны от 0. Наблюдатель сообщает о классе 1, когда\n--- Страница 232 ---\n«Следуйте по стрелкам»  231 измерение попадает в рамки этих критериев. Как и в разделе 8.4, критерии зависят от уровня сенсорной неопределенности (шумов измерений), пара- метров CCSD σ1 и σ2 и логарифмического априорного отношения. Обратите внимание, что априорное распределение C играет относительно незначи- тельную роль. В данном случае мы снова видим, что байесовский вывод касается не только априорных значений. Этап 3: распределение отклика. Вероятность того, что байесовский на- блюдатель сообщит о классе 1, можно выразить через интегральные нор- мальные функции. Мы сделаем это в задаче 8.5. 8.7. «Следуйте по стрелкам» Поскольку мы будем изучать порождающие модели возрастающей слож - ности, полезно иметь четкий и простой рецепт получения выражения для апостериорного распределения нашего выбора на основе информации, пре- доставленной порождающей моделью. Напомним, что порождающая модель точно определяет, какие именно распределения заданы в задаче. Каждая переменная, на которую не указывают стрелки, следует обычному распре- делению вероятностей. Каждая переменная, на которую указывают стрелки, следует условному распределению, где условием является переменная (пере- менные), от которой(ых) исходят стрелки; ее распределение не зависит ни от каких других переменных в задаче. Это дает нам рецепт, который мы ищем: 1. Найдите совместное распределение по всем переменным в порождаю- щей модели, следуя по стрелкам. Начните сверху с переменных, на ко- торые не указывают стрелки. Продвигаясь вниз, запишите априорное или условное распределение вероятностей, связанное с каждым узлом, и перемножьте все распределения, полученные таким образом. 2. Вычислите условное распределение, записав его определение и марги- нализировав совместное распределение (т. е. суммируя или интегрируя переменные, не входящие в условное распределение). Когда вычисляется апостериорное распределение состояния мира, мар- гинализация выполняется для каждой переменной в генеративной модели, кроме наблюдений и переменной состояния мира. Обратите внимание, что в этом рецепте центральным является совместное распределение, а не веро- ятность или априор. На самом деле правило Байеса, выражающее совмест - ность через правдоподобие и априорную вероятность, – это лишь первый шаг в рецепте оценки совместного распределения! В данной главе совместная вероятность равна p(C, s, x). Следуя стрелкам на рис. 8.4А, мы находим p(C, s, x) = p(C)p(s | C)p(x | s). Интересующая нас апо- стериорная вероятность – это p(C | x), которую мы получаем путем маргина- лизации. Для непрерывного случая: p(C | x) µ p(C, x) (8.39) (8.40)\n--- Страница 233 ---\n232  Бинарная классификация (8.41) (8.42) Если порождающая модель выглядит как последовательность переменных, каждая из которых получает стрелку только от предыдущей: C → s → x, она называется цепью Маркова. С другими цепями Маркова мы познакомимся в главе 12. Как и в разделе 3.3.3, знак пропорциональности в (8.39) говорит: «мы будем вычислять ненормированное апостериорное распределение (то, что мы называем в этой книге протопостериором); чтобы получить апосте- риорное распределение, в конце выполните нормирование. Когда оценка MAP является единственной интересующей величиной, нормирование не требуется. 8.8. Заключение В этой главе мы проанализировали, как можно выполнить бинарную класси- фикацию с точки зрения байесовской статистики. Вы узнали, что: психометрические кривые характеризуют зависимость вывода наблю- дателя от стимула; при классификации стимул может принимать более двух значений, но количество возможных ответов (обычно два) меньше, чем количество значений стимула; даже в случае плоских априорных распределений байесовская класси- фикация может быть сложной из-за необходимости маргинализации стимула; распределения стимулов, обусловленные классами, часто выбирают зеркально симметричными, но отказ от этого правила не только обес - печивает больший реализм, но и упрощает изучение правил принятия решений, на которые влияет неопределенность; в конечном счете байесовские модели предназначены для представ- ления информации в виде вероятностных распределений, но они не обязательно должны быть априорными. 8.9. Рекомендуемая литература Rachel N. Denison, William T. Adler, Marisa Carrasco, and Wei Ji Ma. Humans Incorporate Attention­Dependent Uncertainty into Perceptual Decisions and Conﬁdence. Proceedings of the National Academy of Sciences 115, no. 43 (2018): 11090–11095. Thomas L. Griffiths and Joshua B. Tenenbaum. Theory­Based Causal Induction. Psychological Review 116, no. 4 (2009): 661–716.\n--- Страница 234 ---\nЗадачи  233 Daniel Kersten, Pascal Mamassian, and Alan Yuille. Object Perception as Baye­ sian Inference. Annual Review of Psychology 55 (2004): 271–304. Zili Liu, David C. Knill, and Daniel Kersten. Object Classiﬁcation for Human and Ideal Observers. Vision Research 35, no. 4 (1995): 549–568. Gregory L. Murphy, Stephanie Y. Chen, and Brian H. Ross. Reasoning with Un­ cer tain Categories. Thinking and Reasoning 18, no. 1 (2012): 81–117. Ahmad T. Qamar, R. James Cotton, Ryan G. George, and Wei Ji Ma. Trial­to­ Trial, Uncertainty­Based Adjustment of Decision Boundaries in Visual Categorization. Proceedings of the National Academy of Sciences 110, no. 50 (2013): 20332– 20337. 8.10. Задачи Задача 8.1. Как выглядела бы психометрическая кривая незашумленного наблюдателя? Задача 8.2. В контексте раздела 8.5 75%­ный порог наблюдателя может быть определен как значение стимула, для которого наблюдатель (имеющий шум наблюдения σ) сообщает о «правильных» 75 % испытаний минус значение стимула, для которого наблюдатель сообщает о «правильных» 50 % испыта- ний. Скольким стандартным отклонениям σ соответствует 75%-ный порог? Напомним, что Задача 8.3. Рассмотрим бинарную классификацию с общим априорным рас - пределением вероятности класса p(C) и гауссовым распределением измере- ний. См. CCSD в разделе 8.2. (a) Выведите решающее правило байесовского наблюдателя в случае 2 (CCSD, однородные на интервале). Ответ будет включать более одно- го интегрального стандартного нормального распределения (Фstandard ), и это не очень изящное правило. (b) Повторите для случая 4 (CCSD являются эквивариантными гауссовыми распределениями). Задача 8.4. Рассмотрим случай 3 в разделе 8.2, где стимул является непре- рывным и получен из гауссова распределения со средним значением 0, где класс равен - 1, если стимул ниже нуля, и 1 в противном случае. (a) Покажите, что логарифмическое отношение правдоподобия (LLR) равно (8.43)\n--- Страница 235 ---\n234  Бинарная классификация (b) Пусть σ = 1. Постройте LLR как функцию x в двух случаях: σs = 1 и σs = 10 (две кривые на одном графике). Вы должны увидеть, что σs оказывает большое влияние. (c) Пусть σ = 1 и логарифмическое априорное отношение равно 0.2. Числен- но решите уравнение d = 0 по x в двух случаях: σs = 1 и σs = 10. Вы можете сделать это, выбрав мелкую сетку для x, а затем найдя пересечение d с нулем с помощью интерполяции. (d) Используйте два числа, найденных в задании (c), для построения пси- хометрических кривых оптимального наблюдателя при σs = 1 и σs = 10 (две кривые на одном графике). Вы должны увидеть, что σs не оказыва- ет большого влияния. Таким образом, параметры CCSD иногда сильно влия ют на уверенность (логарифмическое апостериорное отношение; LPR), в то же время минимально влияя на выбор (решение). Задача 8.5. Рассмотрим задачу бинарной классификации (C1 или C2) из раз- дела 8.6, где оба класса равновероятны (p (C = 1) = p(C = 2) = 0.5), оба стимула взяты из гауссовых распределений со средним значением 0 и где дисперсия стимулов C1 и C2 составляет σ12 = 9 и σ22 = 144 соответственно. (a) Постройте распределения p(s | C = 1) и p(s | C = 2). Используйте график, чтобы объяснить, почему даже оптимальный наблюдатель не может быть прав на 100 % в этой задаче. (b) Для общих σ1 и σ2 выведите уравнение (8.37) для LPR. (c) Выведите выражение для вероятности того, что оптимальный наблюда- тель сообщает о классе 1, когда истинным стимулом является s. Исполь- зуйте стандартное кумулятивное нормальное распределение Фstandard , определенное в примечании 7.1. (d) Постройте эту вероятность как функцию s (между -30 и 30) для σ = 10. Это психометрическая кривая оптимального наблюдателя с σ = 10. Сде- лайте то же самое для σ = 1. Постройте оба случая на одном графике. (e) Объясните различия между двумя кривыми. (f) Выведите выражения для «частоты совпадений» (вероятность сообще- ния о классе 1, когда истинный класс равен 1), «частоты ложных тревог» (вероятность сообщения о классе 1, когда истинный класс равен 2) и доли правильных сообщений. (g) Постройте все три выражения как функцию σ на одном графике. (h) Объясните полученный результат. Задача 8.6. В этой задаче мы рассматриваем ошибки наблюдателя. Предпо- ложим, что мы проводим эксперимент с бинарными откликами (r = 0 или r = 1) и что p(r | s) выражает прогнозируемую вероятность отклика наблюда- теля – в рамках произвольной модели, байесовской или небайесовской – при воздействии стимула s . (a) Предположим, что наблюдатель случайно нажимает не ту клавишу в доле λ всех испытаний. Как это меняет прогнозируемую вероятность отклика наблюдателя? Заметьте, в большей части исследований по психофизике человека делают такое предположение и называют соответствующий эффект преходящим.\n--- Страница 236 ---\nЗадачи  235 (b) Предположим, что наблюдатель делает случайное предположение о до- ле g всех испытаний (например, потому что он иногда отвлекался и не видел стимула). Как это меняет прогнозируемую вероятность ответа на- блюдателя? Задача 8.7. Рассмотрим бинарную классификацию с плоским априорным распределением, зеркальным CCSD (т. е. p(s | C = 1) = p(-s | C = -1)) и рас - пределением измерений p(x | s), которое симметрично относительно s (хотя и не обязательно гауссово). Покажите, что наблюдатель MAP руководствуется правилом принятия решений «сообщить, что C = 1, если x > 0». Задача 8.8. Рассмотрим наблюдателя, выполняющего бинарную классифи- кацию с распределением классов p(C) и зеркальными непересекающимися CCSD p(s | C = -1), отличными от нуля при s < 0, и p(s | C = 1), отличными от нуля при s > 0. В настоящей главе мы описали стратегию принятия решения байесовского наблюдателя в этой задаче. Студент предлагает альтернатив- ную стратегию принятия решения, а именно что наблюдатель сначала вы- числяет общее распределение стимула, p(s) = p(s | C = 1)p(C = 1) + p(s | C = -1)p(C = 1), (8.44) затем использует его как априорное распределение для вычисления апо- стериорного распределения s, а потом сравнивает среднее значение этого распределения, которое является оценкой s, с 0 (средняя точка наших двух CCSD). (a) Покажите с помощью уравнений, что результирующее решающее пра- вило эквивалентно решающему правилу байесовского наблюдателя, ко- торый допускает неверные CCSD при выводе, а именно q(s | C = -1) µ −sp( s | C = -1); (8.45) q(s | C = 1) µ sp(s | C = 1). (8.46) (b) Охарактеризуйте применительно к CCSD случаев 1 и 2 отличия стратегии студента от оптимальной стратегии с точки зрения психометрической кривой и доли правильных ответов. Вы должны сделать свой собствен- ный выбор параметров. Решение задачи включает в себя изучение того, влияет ли на что-то этот выбор. Задача 8.9. Рассмотрим проблему, связанную с несоответствием моделей. Психологи-экспериментаторы часто исходят из того, что убеждения наблю- дателя о CCSD не влияют на поведение, пока CCSD (будь то истинные или предполагаемые) являются взаимно зеркальными. Однако это гарантируется только в том случае, если p(C = 1) = 0.5. Здесь мы покажем это конкретно на примере 3 из раздела 8.2. CCSD являются полугауссовыми с σs = 1. Для априорной вероятности класса 1 примем распределение p(C = 1) ∈ {0.1, 0.3, 0.5, 0.7, 0.9}. Для сенсорного шума примем σ ∈ {0.5, 1, 2}. (a) Рассмотрим субоптимального наблюдателя, который вместо правила принятия решений MAP использует правило «сообщать, что C = 1, когда x > 0» (которое, как мы знаем, оптимально только при p(C = 1) = 0.5). Вы-\n--- Страница 237 ---\n236  Бинарная классификация числите долю правильных ответов этого субоптимального наблюдателя для каждой комбинации p (C = 1) и σ . (b) Повторите вычисления для оптимального наблюдателя. Вы можете ис - пользовать выражение для LLR из уравнения (8.43). (c) Нанесите все результаты на один и тот же график доли правильных от - ветов как функции p(C = 1), с линиями разного цвета, соответствующими разным значениям σ. Используйте сплошные линии для субоптималь- ных значений, пунктирные линии – для оптимальных. Из этой задачи можно извлечь урок планирования эксперимента: если су - ществует вероятность того, что наблюдатель имеет неплоское априорное распределение (а такая возможность существует), то рекомендуется выпол- нить моделирование, чтобы определить, имеет ли значение предположение, которое делает наблюдатель о CCSD. Задача 8.10. В этой задаче исследуется взаимодействие между объедине- нием сигналов (глава 5) и бинарной классификацией (текущая глава). Рас - смотрим задачу, в которой категория C принимает значения -1 (слева) и 1 (справа). В каждом испытании экспериментатор выбирает значение C, при этом оба значения имеют одинаковую вероятность. Затем экспериментатор получает значение стимула из распределения CCSD p(s | C). Наконец, наблю- датель производит два измерения x1 и x2, которые мы предполагаем условно независимыми при заданном s, то есть проведенными независимо от p(x | s). Студент утверждает, что в этой задаче вероятность C равна p(x1, x2 | C) µ p(x1 | C)p(x2 | C), (8.47) где (8.48) для i = 1, 2. (a) Почему это неправильно? (b) Запишите правильное уравнение для p(x1, x2 | C) относительно известных вероятностей. (c) Разработайте пример модели MAP , в которой ошибочные утверждения студента дают психометрическую кривую p(Cˆ = 1 | s) как функцию s, ко - торая отличается от правильной вероятности. Вам нужно будет сделать определенные предположения о p (s | C) и p (x | s). Задача 8.11. Выполните подгонку (обуче ние) простой модели бинарной классификации. Прочтите приложение C, если вы незнакомы с обуче ни ем и сравнением моделей. Стимул принимал значения от -5 до 5 с шагом 1; мы обозначим их через sj, где j = 1, 2, , 11. В каждой попытке испытуемый отвечал, был стимул положительным (вправо; Cˆ = 1) или отрицательным (влево; Cˆ = -1). Загрузите файл psychometric.csv с https://osf.io/84kpb/. Строки соответствуют испытаниям (всего 500 испытаний). Первый столбец содержит значения предъявляемого стимула s, второй столбец – классификационные отклики испытуемого C ˆ.\n--- Страница 238 ---\nЗадачи  237 (a) Постройте психометрическую кривую (без соединительной линии), то есть долю «правильных» ответов в зависимости от стимула. (b) Теперь подгоните модель к уравнению (8.30) с k = 0: (8.49) Объясните, почему логарифмическое правдоподобие σ принимает фор- му (8.50) где nj+ и n j- – количество испытаний, при которых стимул был sj, а ответ был C ˆ = 1 или C ˆ = -1 соответственно. (c) Постройте логарифмическую функцию правдоподобия как функцию σ. Используйте сетку для σ от 0.1 до 5 в 1000 шагов. (d) Постройте функцию правдоподобия (без логарифма) по σ, используя ту же сетку. (e) Объясните, почему значения правдоподобия чрезвычайно малы. (f) Найдите на сетке MLE для σ . (g) Вместо использования сетки найдите MLE для σ, используя встроенный алгоритм численной оптимизации. Обоснуйте свой выбор алгоритма. (h) Постройте наилучшую модель в виде линии на графике, полученном в задании (a). Используйте оценку максимального правдоподобия (MLE) σ из задания (g) или, если вы не выполнили эту часть, из задания (f). Задача 8.12. Это продолжение задачи 8.11, и в нем используется тот же файл данных psychometric.csv. Мы сравним простую модель из задачи 8.11 с более общей моделью, в которой наблюдатель иногда угадывает случайным обра- зом. Эта модель имеет вид (8.51) где λ – неизвестная скорость угадывания. Эта модель обладает большей гиб- костью благодаря дополнительному параметру. (a) Постройте ландшафт логарифмического правдоподобия этой модели в виде тепловой карты. Для σ используйте ту же сетку, что и в задаче 8.11. Для λ используйте сетку от 0 до 0.3 в 1000 шагов. (b) Найдите MLE σ и λ на их соответствующих сетках. (c) Теперь забудьте о сетках и вместо этого найдите MLE σ и λ, используя встроенный алгоритм оптимизации. Обоснуйте свой выбор алгоритма. (d) Постройте на графике психометрическую кривую (светлые кружки) вмес - те с лучшими аппроксимациями как простой, так и более сложной мо-\n--- Страница 239 ---\n238  Бинарная классификация дели (сплошные линии разных цветов). Используйте MLE из задания (c) или, если вы не выполнили эту часть, из задания (b). (e) Рассчитайте AIC для обеих моделей. Вычислите разницу AIC. Сделайте вывод (без формул). (f) Рассчитайте BIC для обеих моделей. Вычислите разницу BIC. Сделайте вывод (без формул). (g) Вычислите десятикратную перекрестную проверку логарифмических вероятностей для обеих моделей. Вычислите разницу логарифмического правдоподобия. Сделайте вывод (без формул).",
      "debug": {
        "start_page": 215,
        "end_page": 239
      }
    },
    {
      "name": "Глава 9. Мешающие переменные верхнего уровня и неоднозначность 239",
      "content": "--- Страница 240 --- (продолжение)\nГлава 9 Мешающие переменные верхнего уровня и неоднозначность Что нам делать с аспектами мира, влияющими на наши наблюдения, но не имеющими прямого отношения к вопросу, который мы хотим задать? В главе 8 вы познакомились с понятием мешающих переменных, которые от - ражают изменчивые состояния мира, не представляющие первостепенного интереса для лица, принимающего решения, но должны учитываться при выводе, поскольку они влияют на наблюдения. Там мешающая переменная занимала промежуточное положение между интересующей нас переменной и наблюдениями. Темой настоящей главы являются мешающие переменные, которые сами по себе являются переменными верхнего уровня. Краткое содержание главы Мы рассмотрим два классических примера мешающих переменных верхнего уровня: восприятие глубины сцены и восприятие цвета поверхности. Эти примеры математически похожи, хотя они взяты из двух совершенно разных областей восприятия. Мы обсудим, как мешающие переменные верхнего уровня вызывают неоднозначность, и покажем, как наблюдатель должен выполнить маргинализацию, чтобы вычислить функцию правдоподобия по интересующей переменной состояния мира. 9.1. Примеры задач Мы рассматриваем ситуации, в которых две переменные состояния мира вместе формируют наблюдение или измерение. Одна из этих переменных\nГлава 9 Мешающие переменные верхнего уровня и неоднозначность Что нам делать с аспектами мира, влияющими на наши наблюдения, но не имеющими прямого отношения к вопросу, который мы хотим задать? В главе 8 вы познакомились с понятием мешающих переменных, которые от - ражают изменчивые состояния мира, не представляющие первостепенного интереса для лица, принимающего решения, но должны учитываться при выводе, поскольку они влияют на наблюдения. Там мешающая переменная занимала промежуточное положение между интересующей нас переменной и наблюдениями. Темой настоящей главы являются мешающие переменные, которые сами по себе являются переменными верхнего уровня. Краткое содержание главы Мы рассмотрим два классических примера мешающих переменных верхнего уровня: восприятие глубины сцены и восприятие цвета поверхности. Эти примеры математически похожи, хотя они взяты из двух совершенно разных областей восприятия. Мы обсудим, как мешающие переменные верхнего уровня вызывают неоднозначность, и покажем, как наблюдатель должен выполнить маргинализацию, чтобы вычислить функцию правдоподобия по интересующей переменной состояния мира. 9.1. Примеры задач Мы рассматриваем ситуации, в которых две переменные состояния мира вместе формируют наблюдение или измерение. Одна из этих переменных\n--- Страница 241 ---\n240  Мешающие переменные верхнего уровня и неоднозначность представляет интерес для наблюдателя, а другая – мешающая переменная верхнего уровня. Распределение переменной верхнего уровня не зависит ни от каких других переменных в задаче. Порождающую модель можно пред- ставить в виде графовой структуры, как на рис. 9.1. Она имеет характер- ную V-образную форму. Примеров мешающих переменных верхнего уровня предостаточно: при определении расстояния до объекта его размер является меша- ющей переменной верхнего уровня, поскольку наблюдение – размер объекта на сетчатке – зависит не только от расстояния до него, но и от размера объекта; наоборот, при определении размера объекта расстояние до него яв- ляется мешающей переменной. Следовательно, ответ на вопрос, что является мешающей переменной, зависит от задачи; при определении цвета поверхности окраска падающего света являет - ся мешающей переменной верхнего уровня; при классификации объекта на основе визуального наблюдения угол обзора является мешающей переменной верхнего уровня; когда пилот пытается визуально определить угол захода на посадку своего самолета, наклон взлетно-посадочной полосы является меша- ющей переменной верхнего уровня; при попытке понять намерения человека из произнесенных им слов опыт владения языком у говорящего является мешающей переменной, потому что люди менее искусны в выражении важных нюансов на ран- ней стадии изучения языка. Мешающие переменные верхнего уровня вызывают неоднозначность даже при отсутствии шума. А именно если бы значение мешающей переменной было известно, можно было бы точно знать значение интересующей пере- менной. Однако поскольку значение мешающей переменной неизвестно, одно и то же наблюдение согласуется с несколькими значениями интересу - ющей нас переменной (часто с бесконечным их числом). Интересующая переменная состояния мираДругое состояние мира (мешающая переменная) Наблюдение или измерение Рис. 9.1  Порождающая модель с мешающей переменной верхнего уровня. Все примеры в этой главе имеют такую структуру\n--- Страница 242 ---\nРазмер как мешающая переменная верхнего уровня в восприятии глубины сцены  241 9.2. Размер как мешающая переменная верхнего уровня в восприятии глубины сцены Мешающая переменная может внести неоднозначность там, где в противном случае ее не было бы. Классический пример – неоднозначность размер– глубина. Рассмотрим осторожного водителя, который хочет поддерживать безопас ную дистанцию между своей машиной и впереди идущей (рис. 9.2А). Для этого он должен точно определить расстояние до следующей машины (расстояние в трехмерном мире называется глубиной ). В хороших услови- ях наблюдения у водителя есть много сигналов, помогающих восприятию глубины. Однако при плохих условиях, например в темноте или тумане, ко- личество признаков расстояния уменьшается. Одна из подсказок, которую водитель может использовать, при условии что он может видеть задние фо- нари впереди идущего автомобиля, пусть даже одним глазом, – это размер изображения автомобиля на его сетчатке. Таким образом, задача наблюда- теля состоит в том, чтобы оценить расстояние до следующего автомобиля D по ширине изображения этого автомобиля x на сетчатке. Для простоты мы полагаем, что изображение на сетчатке плоское, а не слегка изогнутое. Глазww xlDD w l(A) (B) Рис. 9.2  Восприятие глубины по размеру изображения на сетчатке: (A) на рас - стоянии D от наблюдателя автомобиль шириной w создает на сетчатке изображение шириной x. При отсутствии шума измерений из тригонометрии известно, что x lw D=, где l – расстояние от зрачка наблюдателя до его сетчатки; автомобиль меньшего раз- мера, расположенный ближе к наблюдателю, имел бы тот же угол обзора и создавал бы такое же изображение на сетчатке. Следовательно, наблюдатель может сделать вывод о расстоянии до автомобиля только в том случае, если у него есть априорные представления о размере автомобиля; (B) порождающая модель. Мешающая пере- менная w и представляющая интерес переменная состояния мира D необходимы для создания наблюдения x. Мы предполагаем отсутствие шума в наблюдении\n--- Страница 243 ---\n242  Мешающие переменные верхнего уровня и неоднозначность Этап 1: порождающая модель (рис. 9.2В). Порождающая модель содержит три переменные: ширину w, расстояние D и измеренную ширину x. Ввиду геометрии задачи ширина изображения на сетчатке полностью определяется значениями w и D : (9.1) где l – диаметр глаза (расстояние от зрачка до ямки), принимаемый за фик - сированную и известную константу. Чтобы сосредоточить внимание на не- однозначности, вызванной мешающей переменной верхнего уровня, мы предполагаем, что измерение не содержит шум. Мы определили x через w и D, но нам все еще нужно предоставить w и D с априорными распределениями. Предположим сначала, что эти переменные независимы: p(D, w) = p(D)p(w). (9.2) Кроме того, мы видим, что w и D ограничены значениями положительной числовой прямой: ни одна из них не может принимать отрицательных зна- чений. Для таких переменных с положительным значением (иногда называ- емых «переменными величины») нормальное распределение не подходит, потому что оно подразумевает наличие отрицательных значений. Обычное правильное решение состоит в том, чтобы вместо этого принять логнормаль- ное распределение (раздел 3.7). Примем логнормальное распределение по w: p(w) = Lognormal(w ; μw, σ2 w). (9.3) Это эквивалентно утверждению, что log w следует нормальному распреде- лению со средним значением μw и дисперсией σ2 w: p(log w) = �(log w; μw, σ2 w). (9.4) Для log D для простоты предположим плоское априорное распределение: p(log D) = константа. (9.5) (Это априорное распределение не будет плоским в пространстве D. Мы рассмотрим альтернативы в задаче 9.6.) Полезно переписать уравнение (9.1) в виде log x = log l + log w - log D. (9.6) Этап 2: вывод. Наблюдатель делает вывод D из имеющейся измеренной ширины x. Неоднозначность в этой задаче состоит в том, что для данного x существует бесконечно много комбинаций log w и log D, удовлетворяющих уравнению (9.1). Чтобы вычислить апостериорное распределение по log D, мы применяем правило Байеса: p(log D | log x) µ p(log D)p(log x | log D) (9.7) µ p(log x | log D), (9.8)\n--- Страница 244 ---\nРазмер как мешающая переменная верхнего уровня в восприятии глубины сцены  243 где использовали предположение из этапа 1 о том, что априорное распреде- ление log D является плоским. Пока мы еще не определили p(log x | log D). Что- бы сделать это, начнем с уравнения (9.6). Поскольку мы обусловливаем log D в p(x | log D), мы рассматриваем log D как константу. Более того, log l является константой. В результате log x равен log w плюс константа. Поскольку log w имеет нормальное распределение со средним значением μw и дисперсией σ2 w (уравнение 9.4), мы можем использовать свойства из примечания 4.1, чтобы найти, что log x, обусловленный log D, имеет нормальное распределение со смещенным средним значением: p(log x | log D) = �(log x; μw + log l - log D, σ2 w). (9.9) Отсюда уравнение (9.8) принимает вид: p(log D | log x) µ �(log x; μw + log l - log D, σ2 w) (9.10) = �(log D; μw + log l - log s, σ2 w), (9.10) где мы воспользовались правилом �(a; b, σ2) = �(a + c; b + c, σ2) для любых a, b и c. Упражнение 9.1 (a) Почему это правило интуитивно понятно? (b) Покажите, как мы применили это правило, чтобы получить уравнение (9.11). Уравнение (9.11) подразумевает, что само D (без логарифма) подчиняется логнормальному распределению: p(D | x) = LogNormal(D ; μw + log l - log x, σ2 w). (9.12) (Обусловливание по log x – то же самое, что обусловливание по x.) Мы по- строили несколько примеров апостериорных графиков на рис. 9.3. 1000 1000 2468 2468 500 500 10 10 0 0 Предполагаемый w/см Предполагаемый log w/см Предполагаемый log D/см Предполагаемый D/смУзкий Средний ШирокийУзкий Средний Широкий(А) (B) (С) (D) Априорное распределение wАприорное распределение log wАпостериорное распределение log DАпостериорное распределение D Рис. 9.3  Априорные и апостериорные распределения w и D в примере с автомобилем Последняя часть этапа 2 состоит в получении апостериорного распреде- ления. Если наблюдатель минимизирует ожидаемую квадратичную ошибку в логарифмической области (что имеет больше смысла, чем в области само-\n--- Страница 245 ---\n244  Мешающие переменные верхнего уровня и неоднозначность го D), то он сообщит среднее значение апостериорного значения по log D, которое равно log Dˆ = μw + log l - log x. (9.13) Предполагаемый log w/см Предполагаемый log w/см Предполагаемый log w/см10 8 6 4 210 8 6 4 210 8 6 4 2 2468 2468 2468 10 10 10 Предполагаемый log D/см Предполагаемый log D/см Предполагаемый log D/см(А) (В) (С) Правдоподобие Априорное распределение Апостериорное распределение Рис. 9.4  Двумерные распределения – априорное, правдоподобие и апостериорное – в примере с автомобилем Возвращаясь в исходное пространство, получаем (9.14) Уравнение (9.14) подразумевает, что байесовский наблюдатель в этой за- даче использует предварительное знание ширины автомобиля через пара- метр μw, который является средним значением логарифма ширины. Сначала можно подумать, что eμw – это просто �[w]. Но это не так. Среднее значение логнормально распределенной переменной не является помещенным в по- казатель степени первым параметром. На самом деле помещенный в по- казатель степени первый параметр является медианой переменной (см. раз- дел 3.7). Следовательно, eμw – это медиана w (вычисленная с использованием априорного распределения w). Наконец, поскольку мы предполагали, что наблюдение не содержит шум, равно отношению истинного расстояния к истинной ширине , известному с точки зрения экспериментатора. Как результат: (9.15) Хотя это отношение простое, оно демонстрирует интересный и глубокий момент, а именно то, что оценка интересующего состояния мира (здесь рас - стояние) может быть смещена априорным распределением мешающей пере- менной (здесь ширина). Смещение оценки проявляется следующим образом:\n--- Страница 246 ---\nРазмер как мешающая переменная верхнего уровня в восприятии глубины сцены  245 когда автомобиль шире медианы, оценка расстояния наблюдателем будет меньше, чем истинное расстояние. Вы думаете, что автомобиль ближе, чем он есть на самом деле, потому что он больше, чем вы ожидаете. И наоборот, если автомобиль ýже медианы, наблюдатель завысит расстояние до него. Это погрешность, но не из-за шума измерения, объединенного с априорным рас - пределением интересующего мирового состояния, как это было, например, в разделе 4.5. В данном случае шум измерения отсутствует, а погрешность возникает из-за маргинализации мешающей переменной. Правдоподобие относительно интересующей переменной (здесь D) «наследуется» от апри- орного предположения о мешающей переменной (здесь w). Это типично для логического вывода в порождающих моделях наподобие показанных на рис. 9.1: априорное убеждение относительно мешающей переменной влияет на правдоподобие интересующей переменной, поскольку две переменные «связываются» посредством наблюдения. С точки зрения порождающей мо- дели происходит преобразование переменных от w к x, превращая априорное распределение w в условное распределение x (при данном D); последнее за- тем используется для получения правдоподобия D . Мы видели, что априорное убеждение относительно мешающей пере- менной w сужает апостериорное распределение интересующего состояния ми ра D: априорное убеждение относительно ширины автомобиля разреша- ет неоднозначность в отношении расстояния. Это предварительное убежде- ние должно быть получено из опыта. Большинство из нас хорошо знакомы с автомобилями, поэтому мы обладаем значительными знаниями, полу - ченными благодаря многолетнему опыту, относительно распределения размеров и форм автомобилей на дорогах. Априорные убеждения о разме- рах объектов иногда называют формой монокулярного сигнала о глубине (расстоянии). До сих пор мы видели, как наше понимание размера объекта влияет на наше восприятие расстояния до него. И наоборот, наше понимание расстоя- ния до объекта также влияет на наше восприятие его размера. На рис. 9.5 по- казаны два примера иллюзии Понцо. Объекты на рисунке имеют одинаковый размер на странице (и, следовательно, на сетчатке), но самый верхний объект (книга и отрезок линии) кажется больше. Это происходит потому, что мозг интерпретирует двумерную картинку как трехмерную сцену, где контекст предполагает, что самый верхний объект находится дальше. Если один объ- ект дает изображение на сетчатке такого же размера, что и другой, и все же расположен дальше, значит, этот объект больше. Ваш палец, если смотреть на него на расстоянии вытянутой руки, может занимать ту же область сет - чатки, что и далекая гора (по этой причине вы можете заслонить пальцем гору). Неоднозначность размера и глубины возникает из-за того, что объ- екты с бесконечным множеством возможных физических размеров могут создавать изображения на сетчатке одного и того же размера в зависимости от их расстояния до наблюдателя. Чем больше мы уверены в расстоянии до объекта, тем больше мы можем быть уверены в его размере.\n--- Страница 247 ---\n246  Мешающие переменные верхнего уровня и неоднозначность (А) (B) Рис. 9.5  Два варианта иллюзии Понцо. В обоих случаях самый верхний объект может выглядеть выше (шире), даже если на рисунке он имеет такие же физические размеры, что и другие объекты 9.3. Маргинализация В разделе 8.3 мы видели, что байесовский субъект, принимающий решения, устраняет проблему с мешающей переменной посредством маргинализации. В предыдущем разделе мы избегали явного упоминания маргинализации. Тем не менее маргинализация происходила на заднем плане. В этом раз- деле мы рассмотрим маргинализацию подробнее, сопровождая объяснение полезной двумерной визуализацией. В качестве первого шага запишем дву- мерное апостериорное распределение как по интересующей переменной D, так и по мешающей переменной w : p(log D, log w | log x) µ p(log D, log w)p(log x | log D, log w). (9.16) С учетом уравнения независимости (9.2) оно принимает вид: p(log D, log w | log x) µ p(log D)p(log w)p(log x | log D, log w). (9.17) В уравнении (9.17) p(D)p(w) представляет собой двумерное априорное рас - пределение. Пример показан на рис. 9.4В. Но какова функция правдоподо- бия p(log x | log D, log w) для log D и log w? Из уравнения (9.6) мы знаем, что x является детерминированной функцией D и w. Это означает, что конкретная комбинация D и w либо полностью совместима с x, либо несовместима вовсе; промежуточных вариантов нет. Это означает, что двумерная функция правдо- подобия представляет собой четкую линию (рис. 9.4А): все комбинации D и w на этой линии имеют ненулевую вероятность (которую мы можем считать бесконечной), а все комбинации за пределами линии – нулевую. Математиче- ски такую детерминированную связь можно записать в виде дельта­функции: p(log x | log D, log w) = δ(log x - (log l + log D - log w)). (9.18) Информация о дельта-функции приведена в разделе B.8 приложения. Умно жение двумерной априорной вероятности на двумерное правдоподо-\n--- Страница 248 ---\nЦветовое зрение  247 бие дает двумерную протопостериорную вероятность, которую мы можем численно нормировать для получения двумерной апостериорной вероят - ности (рис. 9.4C). Эффект резкого правдоподобия состоит в том, чтобы взять «срез» из двумерного априорного распределения. Последним шагом являет - ся маргинализация мешающей переменной: (9.19) Это уравнение означает «свертывание» двумерного апостериорного рас - пределения в одномерное по D путем усреднения по второму измерению w. Вспомним из примечания 8.1, что слово «маргинализация» на самом деле озна чает «сползание в сторону края». Мы покажем в задаче 9.7, что формаль- но маргинализация приводит к тому же апостериорному распределению log D, что и в уравнении (9.11). Справочная информация о маргинализации приведена в разделе B.11.2 приложения. Упражнение 9.2. В уравнении (9.19) мы интегрируем по log w. При каких об- стоятельствах мы можем сделать это аналитически, а при каких нам нужно будет найти интеграл численно? 9.4. Цветовое зрение Теперь мы переходим к совершенно другой области восприятия, а именно к цветовому зрению. Хотя цветовое зрение обычно не обсуждается вместе с восприятием глубины, мы поступаем так в этой книге, потому что процесс вывода наблюдателя почти идентичен. Здесь мы только обрисуем составля- ющие этого примера, а подробно проработаем его в задаче 9.9. Суть вычислительной задачи не зависит от фактического цвета. Поэтому для простоты мы рассматриваем поверхности, которые окрашены в оттенок серого (т. е. где-то между белым и черным). Мы видим поверхность, ког - да есть источник света. Источник света испускает фотоны (частицы света), каждый из которых несет определенное количество энергии. Поверхность поглощает часть фотонов и отражает остальные. Часть отраженных фотонов достигает нашего глаза и запускает процесс зрительного восприятия. Этап 1: порождающая модель (рис. 9.6). Состояния мира – это оттенок поверхности и интенсивность освещения. В данном случае оттенок поверх - ности определяется градацией серого. Это одна из форм цвета поверхности. Технически оттенок серого – это коэффициент отражения: доля падающего света, которая отражается. Оттенок бумаги определяет ее отражательную способность ρ: черная бумага может поглощать 90 % падающего света и от - ражать только 10 %, тогда как качественная белая бумага может поглощать только 10 % и отражать 90 %. Интенсивность источника света (осветителя), обозначаемая буквой I, представляет собой количество излучаемого им све- та; она измеряется в «количестве фотонов» или мощности. Интенсивность источника света является мешающей переменной при определении оттен-\n--- Страница 249 ---\n248  Мешающие переменные верхнего уровня и неоднозначность ка поверхности. И коэффициент отражения, и интенсивность будут иметь связанные с ними распределения вероятностей. Мы рассмотрим примеры в задаче. Измерение – это количество света, измеренное сетчаткой, которое мы также будем называть интенсивностью на сетчатке и обозначать через x. При отсутствии шума измерения интенсивность на сетчатке x = ρI. Другими словами, если вы удвоите отражающую способность поверхности, это окажет на вашу сетчатку тот же эффект, что и удвоение интенсивности источника света. Поскольку интенсивность и отражательная способность представляют только положительные величины, мы сразу переходим в логарифмическую область и пишем log x = log ρ + log I. (9.20) Хотя переменные имеют совершенно разные значения, уравнение (9.20) соответствует уравнению (9.6) в примере с автомобилем. Этот факт свиде- тельствует о том, что фундаментальные вычисления могут быть одинаковы- ми для различных предметных областей. Оттенок поверхностиИнтенсивность освещения Интенсивность, измеренная сетчаткой Рис. 9.6  Порождающая модель восприятия цвета Этап 2: вывод. Наблюдатель должен сделать вывод о градации серого (коэффициенте отражения) ρ по интенсивности на сетчатке x. Из этапа 1 мы понимаем, что интенсивность на сетчатке дает неоднозначную информацию об оттенке поверхности. Конкретная интенсивность света на сетчатке согла- суется со множеством (фактически бесконечным числом) возможных ком- бинаций истинной отражательной способности поверхности и освещения. Например, одну и ту же интенсивность на сетчатке могут создавать темная бумага при солнечном свете и белая при тусклом освещении. Основная за- дача процесса вывода состоит в том, чтобы устранить неоднозначность. Для обозначения воспринимаемого оттенка поверхности мы условно используем термин «яркость». Но нельзя забывать, что в данном случае яркость объекта – это результат вывода, а не физическое состояние мира. До сих пор мы рассматривали поверхности в оттенках серого. Однако ана- логичные аргументы применимы к окрашенным поверхностям: интенсив- ность источника света заменяется набором интенсивностей на разных дли- нах волн, также называемым спектром мощности источника света. Градация серого заменяется цветом поверхности, или, технически, кривой отражения, которая определяет, какая доля фотонов на каждой длине волны отражается\n--- Страница 250 ---\nЦветовое зрение  249 поверхностью. Наконец, интенсивность на сетчатке заменяется спектром мощности света, падающего на сетчатку. Спектр мощности источника света является мешающей переменной при определении цвета поверхности. Другие способы устранения неоднозначности в отношении оттенка/цвета поверхности заключаются в том, чтобы устранить неопределенность в от - ношении интенсивности освещения, например с помощью источника света известной интенсивности или, как в иллюзии Понцо, с помощью контексту - альных сигналов – подсказок в наблюдаемой сцене, которые говорят нам об интенсивности или спектре мощности света. Тогда порождающая мо - дель будет такой, как на рис. 9.7А. Затем мы можем вывести интенсивность осве щения из других наблюдений и, используя эту информацию, вычислить апостериорное распределение оттенка поверхности. Этот процесс называет - ся дисконтированием источника света. Например, если другие наблюдения говорят нам, что поверхность находится в тени, то мы полагаем, что оттенок поверхности белее по сравнению с ситуацией, когда мы имеем ту же интен- сивность на сетчатке, но другие наблюдения говорят нам, что поверхность освещена солнечным светом. Оттенок поверхностиИнтенсивность освещения Интенсивность, измеренная сетчаткойДругие наблюдения(А) (В) (С) (D) Рис. 9.7  Контекст, помогающий устранить двусмысленность: (А) по- рождающая модель; (B) иллюзия одновременного контраста; (C) объ- яснение Эдвардом Адельсоном иллюзии одновременного контраста; (Г) какого цвета полосы на платье? Этот процесс вывода подразумевает, что можно воспринимать разные от - тенки (т. е. сообщать о разной яркости) на основе одинаковой интенсивности на сетчатке, пока нас заставляют верить, что освещение объектов различает - ся. Одна из иллюзий, иллюстрирующих этот эффект, – это иллюзия одновре-\n--- Страница 251 ---\n250  Мешающие переменные верхнего уровня и неоднозначность менного контраста (рис. 9.7B). Хотя оба центральных прямоугольника имеют одинаковый оттенок, правый выглядит темнее. Эдвард Адельсон предпо- ложил, что это происходит потому, что окружающие большие прямоуголь- ники предполагают различное освещение для двух половин изображения: например, правая половина может быть освещена, а левая половина может быть в тени (рис. 9.7C). В этом объяснении мозг использует дополнительные наблюдения (здесь окружающие прямоугольники), чтобы сделать вывод об освещении, и на основе результата этого вывода переходит к выводу оттен- ков внутренних прямоугольников. Точно так же можно воспринимать разные цвета на основе одинакового спектра цвета на сетчатке, если нас заставляют верить, что освещение раз- личается. Известным примером этого явления является «иллюзия платья» (рис. 9.7D). Некоторые люди воспринимают полосы на платье как черные и синие, тогда как другие считают их белыми и золотыми. Считается, что это явление отражает разницу в индивидуальных предположениях мозга относительно спектра мощности окружающего света. 9.5. Распознавание объектов Маргинализация мешающих переменных верхнего уровня может быть обна- ружена во многих формах восприятия. Возьмем, к примеру, распознавание объектов. Хотя этот пример не позволит нам детально разработать матема- тическую модель, мы используем его из-за большой важности для естествен- ного восприятия. Предположим, вы хотите идентифицировать объект на фотографии (рис. 9.8А). Вам важна только идентичность объекта, а не угол, под которым он был сфотографирован. Тем не менее угол камеры помогает определить изображение и, следовательно, визуальную информацию, полу - ченную вашей сетчаткой. Осуществляя идентификацию, ваш мозг должен каким-то образом не принимать во внимание угол камеры и делать вывод только о значении интересующей вас переменной состояния мира, т. е. об идентичности объекта. Другими словами, ваш мозг должен осознавать, что вы можете рассматривать объект под любым углом, и учитывать, как каждый объект (например, велосипед, автомобиль и т. д.) будет выглядеть под каж - дым углом. Если вы способны опознать велосипед под любым углом, ваша способность опознания не зависит от точки зрения. Порождающая модель этой задачи представлена на рис. 9.8C. Помимо узла для идентификатора объекта, она имеет узел для каждой мешающей переменной верхнего уровня; здесь показан только угол обзора. Наблюдение в данном случае является образом предмета. Мы предполагаем нулевой сен- сорный шум; если бы присутствовал сенсорный шум, в порождающей модели был бы дополнительный узел, представляющий зашумленное внутреннее представление изображения. Обозначим класс объекта через C, угол обзора через θ и изображение через s. Мы предполагаем, что класс и угол обзора не зависят друг от друга; это означает, что при фотографировании объектов\n--- Страница 252 ---\nРаспознавание объектов  251 разного класса нет каких-либо предпочтений ракурса. Распределения веро- ятностей в порождающей модели представляют собой распределение класса p(C), распределение угла обзора p(θ) и распределение стимула, зависящее как от класса, так и от угла обзора, p(s | C, θ). Апостериорное распределение по C равно p (C | s). Оно получается, если сначала применить правило Байеса p(C | s) µ p(s | C)p(C), (9.21) а затем записать вероятность класса как маргинализацию по θ : (9.22) (9.23) Идентичность С sσРакурс Изображение(С) (B) (A) Рис. 9.8  Распознавание объектов и мешающие переменные: (A, B) один и тот же объект, если смотреть на него под другим углом, дает изображение, которое попиксельно сильно от - личается. Угол обзора – мешающая переменная. Пример из работы Керстена и Юилле [92]; (C) порождающая модель задачи распознавания объектов В последнем равенстве мы опираемся на информацию о том, что C и θ независимы, так что p(θ | C) = p(θ). Чтобы понять смысл уравнения (9.23), давайте рассмотрим конкретный класс C, а именно велосипед. Уравнение утверждает, что вероятность зрительного образа, если объект является ве- лосипедом, – это вероятность того, что фотограф решил снимать под углом 0° относительно объекта И что велосипед, снятый под этим углом, даст на- блюдаемый зрительный образ, ИЛИ что фотограф решил снимать под углом 1° И что велосипед, снятый под этим углом, будет давать наблюдаемый зри- тельный образ, и т. д. для всех углов. Вычисляя уравнение (9.23) для многих различных классов объектов С (велосипед, автомобиль, человек и т. д.), на- блюдатель в принципе может сгенерировать функцию правдоподобия класса и, следовательно, апостериорное распределение вероятностей, мода которо- го является наиболее вероятной принадлежностью к классу. Упражнение 9.3. Как бы вы построили техническую систему, не зависящую от ракурса? Почему при разработке искусственных нейронных сетей не ис - пользуется маргинализация в явном виде? Выполните поиск по термину «дополнение данных» (data augmentation).\n--- Страница 253 ---\n252  Мешающие переменные верхнего уровня и неоднозначность 9.6. Заключение В этой главе мы представили мешающие переменные верхнего уровня. Вы узнали, что: при восприятии глубины сцены размер объекта часто фигурирует как мешающая переменная верхнего уровня; при восприятии оттенка объекта цвет источника освещения является мешающей переменной высшего уровня; при восприятии объекта угол зрения (ракурс) выступает как мешающая переменная верхнего уровня; в целом на большинство реальных задач влияет бесчисленное количест - во мешающих переменных верхнего уровня; все мешающие переменные верхнего уровня требуют от байесовского наблюдателя рассмотрения всех значений, которые они могут прини- мать. Наблюдатель делает это посредством маргинализации как интег - рирования всех возможных значений, взвешенных с соответствующи- ми вероятностями; в случае мешающих переменных верхнего уровня маргинализация подразумевает преобразование априорного распределения мешаю- щей переменной в функцию правдоподобия по интересующей пере- менной; маргинализация приводит к интегральным выражениям, которые не могут быть решены аналитически, за исключением простейших слу - чаев. 9.7. Рекомендуемая литература David H. Brainard and William T. Freeman. Bayesian Color Constancy. Journal of the Optical Society of America 14, no. 7 (1997): 1393–1411. Daniel Kersten, Pascal Mamassian, and Alan Yuille. Object Perception as Bayesian Inference. Annual Review of Psychology 55 (2004): 271–304. Daniel Kersten and Alan Yuille. Bayesian Models of Object Perception. Current Opinion in Neurobiology 13 (2003): 1–9. David C. Knill. Mixture Models and the Probabilistic Structure of Depth Cues. Vision Research 43, no. 7 (2003): 831–854. Rosa Lafer-Sousa, Katherine L. Hermann, and Bevil R. Conway. Striking Individual Differences in Color Perception Uncovered by ‘the Dress’ Photograph. Current Biology 25, no. 13 (2015): R545–R546. James V. Stone. Vision and Brain: How We Perceive the World. Cambridge, MA: MIT Press, 2012. Pascal Wallisch. Illumination Assumptions Account for Individual Differences in the Perceptual Interpretation of a Profoundly Ambiguous Stimulus in the Color Domain. ‘The Dress.’ Journal of Vision 17, no. 4 (2017): 5.\n--- Страница 254 ---\nЗадачи  253 9.8. Задачи Задача 9.1. В каждом из перечисленных ниже сценариев укажите по одной мешающей переменной: (a) (зрение) оценка веса предмета, который вы собираетесь поднять; (b) (зрение) оценка времени, которое потребуется приближающейся маши- не, чтобы доехать до вас; (c) (обоняние) определение того, испортилась ли пища; (d) (познание) оценка того, как кто-то отреагирует на вашу критику; (e) (познание) оценка ваших возможностей на основе вашего успеха в конк - ретной задаче. Задача 9.2. Когда вы читаете текст этой книги, вы в конечном счете потреб- ляете слова. Но входные данные вашего мозга – это зрительная сцена, кото- рую вы видите. Что вы маргинализируете, когда превращаете изображение страницы книги в слова и смыслы? Задача 9.3. Приятель говорит вам: «За последние две ночи я спал всего де- сять часов». Это сразу заставляет задуматься, сколько же он спал в каждую из этих ночей. (a) Переменными являются количество часов сна прошлой и позапрошлой ночью. Каково двумерное правдоподобие по обеим переменным, исходя из высказывания этого человека? (b) Как бы выглядело ваше двумерное априорное распределение? Объясните ответ. (c) Как в результате будет выглядеть двумерное апостериорное распреде- ление? (d) Если бы у вас было одно предположение о том, сколько часов приятель спал прошлой ночью, и вы пытались минимизировать квадрат ошибки, что бы вы предположили? Объясните ответ. Задача 9.4. В разделе 9.2 мы использовали логнормальное распределение по ширине w (т. е. p(w) = Lognormal(w ; μw, σ2 w)). Используя 1 метр в качестве неявной меры во всех случаях, выберем μw = 2 (автомобили могут быть ши- риной 2 м), σw = 0.3 (большинство автомобилей имеют ширину от 1.7 до 2.3 м) и диаметр глаза l = 0.025 (диаметр глаза – около 2.5 см). (a) Пусть даны три наблюдения ширины объекта на сетчатке: x = 1.6, x = 2 и x = 2.4. Для каждого значения ширины постройте апостериорную плот - ность вероятности в зависимости от расстояния (один график, три кри- вые, с цветовой кодировкой). (b) Объясните, как эти апостериорные распределения соотносятся друг с другом и почему. Задача 9.5. В разделе 9.2 мы обсуждали наблюдателя, который выводит рас - стояние до автомобиля, а ширина автомобиля является мешающей пере- менной. Теперь рассмотрим противоположную ситуацию: наблюдатель де- лает вывод о ширине автомобиля w, а расстояние до автомобиля D является\n--- Страница 255 ---\n254  Мешающие переменные верхнего уровня и неоднозначность мешающей переменной. Предположим, что имеется плоское распределение по log w и нормальное распределение по log D со средним значением μlog D и стандартным отклонением σlog D. (a) Этап 2: получите выражение для оценки w через x . (b) Этап 3: получите выражение для оценки w через истинное значение w и истинное расстояние D . (c) Интерпретируйте это выражение аналогично нашей интерпретации в разделе 9.2. Задача 9.6. В разделе 9.2 мы предполагали, что априорное значение плоское относительно log D. Теперь мы рассмотрим более реалистичное обобщение, в котором это априорное распределение является нормальным: p(log D) = �(log D; μD, σ2 D). (9.24) (a) Покажите, что апостериорное значение log D из уравнения (9.11) теперь принимает вид p(log D | log x) = �(log D; μpost, σ2 post), (9.25) где (9.26) (9.27) с и (b) Покажите, что оценка D из уравнения (9.15) теперь принимает вид (9.28) (c) Объясните это выражение. Задача 9.7. В этой задаче мы проработаем технический аспект, упомянутый в разделе 9.3. Объединив уравнения (9.18) и (9.19), найдите апостериорное распределение по D . Ответом должно быть уравнение (9.11). Задача 9.8. На следующем изображении вы наверня- ка видите выступающие (выпуклые) полусферы; од- нако если вы перевернете изображение вверх ногами, то увидите полые (вогнутые) полусферы. Это явление можно объяснить, используя априорное убеждение об источнике света, направленном сверху. (a) Нарисуйте граф порождающей модели. Наблюда- емое изображение обозначьте буквой I .\n--- Страница 256 ---\nЗадачи  255 (b) Запишите уравнение логарифмического апостериорного отношения (LPR) для выпуклости относительно вогнутости при заданном I и при- мените правило Байеса. Предполагаются равные априорные убеждения. (c) Оцените логарифмическое отношение правдоподобия (LLR) путем мар- гинализации по направлению света. Предположим для простоты, что свет может исходить только сверху или снизу. Это должно уменьшить маргинализацию каждой вероятности до двух членов (в общей сложно- сти до четырех членов в LLR). (d) Каждый из четырех членов содержит правдоподобие вида p(I | …). Какие два члена из этих четырех близки к нулю и почему? (e) Исходя из ответа на задание d, упростите LLR. (f) Как упрощенное выражение объясняет описанное выше восприятие? Задача 9.9. Вспомним раздел 9.4. Рассмотрим задачу оценки коэффициента отражения поверхности ρ, когда интенсивность света I неизвестна. Начнем с уравнения для логарифма интенсивности света на сетчатке log x = log ρ + log I. Мы предполагаем, что наблюдатель имеет априорное убеждение отно- сительно интенсивности света, которое может быть описано логнормальным распределением с параметрами μlog I и σlog I. (a) Распределение log x при заданном log ρ является нормальным со средним значением log ρ + μlog I и стандартным отклонением σlog I. Объясните по- чему. (b) Покажите, что вероятность ρ на основе измеренной интенсивности света на сетчатке x пропорциональна Отражательная способность поверхности ρ является долей и, следова- тельно, числом от 0 до 1. Стало быть, мы предполагаем, что априорное распределение ρ плоское между 0 и 1 и равно нулю в других местах. (c) Предположим, что x = 10, μlog I = 3 и σlog I = 1. Возьмите сетку для ρ от 0 до 1 с шагом 0.001. Постройте апостериорную функцию масс по ρ, убе- дившись, что распределение нормировано. (d) Численно найдите апостериорное среднее ρ . (e) Меняйте μlog I от 1 до 5 с шагом 0.1. Для каждого значения μlog I повторите задание (d). Постройте апостериорное среднее ρ как функцию μlog I. (f) Объясните полученный график и свяжите его с иллюзией одновремен- ного контраста на рис. 9.7.",
      "debug": {
        "start_page": 240,
        "end_page": 256
      }
    },
    {
      "name": "Глава 10. Одинаковый или разный? 256",
      "content": "--- Страница 257 --- (продолжение)\nГлава 10 Одинаковый или разный? Как мы решаем, являются два стимула одинаковыми или разными? В этой главе мы постараемся ответить на вопрос о том, как наблюдатели дела- ют выводы об отношениях между объектами. Мы сосредоточимся на фунда- ментальном и полезном аспекте родства: суждении «одинаковый–разный». В качестве примера важности таких суждений рассмотрим сценарий, когда для точного сегментирования визуальной сцены наблюдатель полагается на тот факт, что определенные элементы одинаковых объектов чаще всего имеют одинаковый цвет или ориентацию, в то время как у отличающихся объектов эти элементы имеют другой цвет. Хотя подобная оценка сходства кажется простой, она часто требует значительных вычислений. Когда на два стимула воздействует шум, они могут казаться одинаковыми, несмотря на то что они разные, или разными, несмотря на то что они одинаковы. Кроме того, наблюдатель не знает фактического стимула, который наблюдает. Сле- довательно, в большинстве случаев не обойтись без маргинализации. Краткое содержание главы Сначала мы рассмотрим суждение «одинаковый–разный», используя сти- мулы, которые могут принимать только два значения (аналогично главе 7). Затем мы переходим к стимулам, взятым из непрерывных распределений (аналогично главе 8). Таким образом, в этой главе будет синтезирован мате- риал из нескольких предыдущих глав, но в другом контексте: центральным является отношение между двумя стимулами, а не принадлежность одного стимула к определенной категории. 10.1. Примеры задач Вот несколько типичных примеров суждения «одинаковый–разный»: вы доисторический охотник-собиратель. Вы нашли в лесу куст ягод и хотите определить, являются ли эти ягоды такими же, как те, про которые вы точно знаете, что они съедобны;\nГлава 10 Одинаковый или разный? Как мы решаем, являются два стимула одинаковыми или разными? В этой главе мы постараемся ответить на вопрос о том, как наблюдатели дела- ют выводы об отношениях между объектами. Мы сосредоточимся на фунда- ментальном и полезном аспекте родства: суждении «одинаковый–разный». В качестве примера важности таких суждений рассмотрим сценарий, когда для точного сегментирования визуальной сцены наблюдатель полагается на тот факт, что определенные элементы одинаковых объектов чаще всего имеют одинаковый цвет или ориентацию, в то время как у отличающихся объектов эти элементы имеют другой цвет. Хотя подобная оценка сходства кажется простой, она часто требует значительных вычислений. Когда на два стимула воздействует шум, они могут казаться одинаковыми, несмотря на то что они разные, или разными, несмотря на то что они одинаковы. Кроме того, наблюдатель не знает фактического стимула, который наблюдает. Сле- довательно, в большинстве случаев не обойтись без маргинализации. Краткое содержание главы Сначала мы рассмотрим суждение «одинаковый–разный», используя сти- мулы, которые могут принимать только два значения (аналогично главе 7). Затем мы переходим к стимулам, взятым из непрерывных распределений (аналогично главе 8). Таким образом, в этой главе будет синтезирован мате- риал из нескольких предыдущих глав, но в другом контексте: центральным является отношение между двумя стимулами, а не принадлежность одного стимула к определенной категории. 10.1. Примеры задач Вот несколько типичных примеров суждения «одинаковый–разный»: вы доисторический охотник-собиратель. Вы нашли в лесу куст ягод и хотите определить, являются ли эти ягоды такими же, как те, про которые вы точно знаете, что они съедобны;\n--- Страница 258 ---\nБинарные стимулы  257 обобщение объединения сигналов (глава 5): два сигнала не обязательно имеют один и тот же источник, как мы предполагали в главе 5. Напро- тив, они вполне могут иметь два разных источника. Вы хотите знать, из одного источника исходят сигналы или из разных. Вывод в этой задаче в рамках перцептивных и сенсомоторных исследований также называется причинным выводом (cause inference); замыкание контура: два фрагмента линии принадлежат одному и тому же непрерывному контуру или они независимы? Многие другие формы перцептивной организации можно сформулировать как аналогичный вывод: складываются ли элементы в связное целое или нет?; обнаружение изменений: идентичны ли два изображения, разделен- ные во времени? Сюда же относится задача рабочей памяти – отложен- ная задача сопоставления с образцом; на более высоком когнитивном уровне оценка того, являются ли две величины одинаковыми, лежит в основе суждений о справедливости, а также формирует основу математики. 10.2. Бинарные стимулы Рассматриваемый в этой главе вывод включает в себя определение того, являются ли два стимула, которые мы будем обозначать s1 и s2, одинако- выми или разными. В этом разделе мы рассматриваем стимулы, которые могут принимать только два значения, как в главе 7. Мы выбираем значения, равные -μ и μ. Задача усложняется наличием шума измерений. Теперь мы изучим, как байесовский наблюдатель выносит суждение об одинаковости. 10.2.1. Этап 1: порождающая модель Порождающая модель показана двумя эквивалентными способами на рис. 10.1A–B. Переменная верхнего уровня – это двоичная переменная C, которая равна 1 для «одинаковый» (same) и 2 для «разный»; части рисунка A и B различаются тем, имеют ли две возможности для C явные различия. Для распределения C мы пишем p(C = 1) = psame; (10.1) p(C = 2) = 1 - psame. (10.2) Поскольку s1 и s2 могут быть равны только -μ и μ, существует четыре воз- можные комбинации s1 и s2. Распределения стимулов, обусловленные клас - сами, можно просто указать в явном виде для этих четырех комбинаций (рис. 10.1C). Для испытаний «одинаковый»: p(s1 = -μ, s2 = -μ) = 0.5; (10.3) p(s1 = -μ, s2 = μ) = 0; (10.4)\n--- Страница 259 ---\n258  Одинаковый или разный? p(s1 = μ, s2 = -μ) = 0; (10.5) p(s1 = μ, s2 = μ) = 0.5. (10.6) И для испытаний «разный»: p(s1 = -μ, s2 = -μ) = 0; (10.7) p(s1 = -μ, s2 = μ) = 0.5; (10.8) p(s1 = μ, s2 = -μ) = 0.5; (10.9) p(s1 = μ, s2 = μ) = 0. (10.10) Наконец, мы предполагаем, что измерения x1 и x2 являются зашумленны- ми версиями стимулов s1 и s2, при этом шум независим между измерениями и нормально распределен со стандартным отклонением σ : p(x1, x2 | s1, s2) = p(x1 | s1)p(x2 | s2); (10.11) p(x1 | s1) = �(x1; s1, σ2); (10.12) p(x2 | s2) = �(x2; s2, σ2). (10.13) На этом определение порождающей модели завершено. С С С = 1 С = 2 s1 x1x1 x1s1s2s x2x2 x2x2 x1 s2Стимул s1Стимул s2–μ –μ–μμ μμРазный (С = 2) Разный (С = 2)Одинаков. (С = 1) Одинаков. (С = 1)0(А) (B) (С) (D) Рис. 10.1  Суждение «одинаковый–разный»: (А) порождающая модель. C – переменная «одинаковости», s1 и s2 – стимулы, а x1 и x2 – зашумленные измерения; (B) эквивалентное изо- бражение, в котором сценарии C = 1 (одинаковые) и C = 2 (разные) сделаны явными; (C) таб­ ли ца состояний мира. Каждый из стимулов s1 и s2 может принимать значения µ и − µ, образуя четыре возможные комбинации; (D) пример испытания. Возможные значения стимула – это –µ и µ, а измерения в этом испытании – х1 и х2 10.2.2. Этап 2: вывод Пример испытания показан на рис. 10.1D: наблюдатель измеряет x1 и x2 и должен определить, были ли они вызваны одним и тем же стимулом. Если да, то остаются две возможности: этот стимул был -μ или μ. Существуют так - же две возможности, если измерения были порождены разными стимулами: x1 может исходить от - μ, а x2 – от μ или наоборот. Как и в предыдущих задачах на классификацию (главы 7 и 8), интересую- щая переменная состояния мира – это категориальная переменная высокого\n--- Страница 260 ---\nБинарные стимулы  259 уровня C, а не физический стимул. Апостериорное распределение C равно p(C | x1, x2). Поскольку C является бинарным, для удобства мы рассматриваем логарифмическое апостериорное отношение (LPR), обозначаемое d, которое представляет собой сумму логарифмического отношения правдоподобия и логарифмического априорного отношения: (10.14) (10.15) Мы оцениваем вероятности «одинаковый» p(x1, x2 | C = 1) и «разный» p(x1, x2 | C = 2) путем маргинализации по s1 и s2. Поскольку вместе эти переменные могут образовать только четыре комбинации значений, маргинализация принимает форму суммы: (10.16) Теперь мы можем заменить то, что мы знаем из порождающей модели, уравнениями (10.1)–(10.11). Мы пока не заменяем нормальные распределе- ния и получаем (10.17) Хотя это выражение длинное, оно интуитивно понятно: большой числи- тель и большой знаменатель напрямую описывают четыре возможности, упомянутые в начале этого раздела. Их соотношение является сенсорным свидетельством того, что измерения произошли от одного и того же стимула, относительно того, что они были получены от разных стимулов. Наконец, подставляя нормальные распределения из уравнения 10.13, мы находим (10.18) Правило принятия решения MAP , которое максимизирует точность, за- ключается в том, чтобы сообщать «одинаковый» (C ˆ = 1), когда d > 0. В общем случае это правило не поддается дальнейшему упрощению. Но при psame = 0.5 неравенство d > 0 принимает необычайно простой вид: sign(x1) = sign(x2). (10.19)\n--- Страница 261 ---\n260  Одинаковый или разный? Другими словами, наблюдатель сообщает, что два измерения исходят от одного и того же стимула, если они имеют одинаковый знак. В более общем случае решающее правило трудно интерпретировать, но его все же можно визуализировать, что мы и сделаем в задаче 10.4. Упражнение 10.1. Докажите уравнения (10.18) и (10.19). 10.2.3. Этап 3: оценка распределения Нам нужно вычислить вероятность того, что байесовский наблюдатель MAP со- общит «одинаковый» (C ˆ = 1) для заданных истинных стимулов: p(Cˆ = 1 | s1, s2). После вычисления вероятностей ответа наблюдателя, предсказанных моделью, экспериментатор может сравнить их с эмпирическими данными. Это оценоч- ное распределение можно записать как маргинализацию по измерениям: (10.20) или, поскольку отображение из x1 и x2 в Cˆ детерминировано, как (10.21) = p(d(x1, x2) > 0 | s1, s2). (10.22) Это вероятность того, что переменная решения d, являющаяся функцией x1 и x2, положительна, когда x1 и x2 следуют своим соответствующим распре- делениям, зависящим от s1 и s2. Геометрически эта вероятность представляет собой объем под двумерным распределением измерений в области, опреде- ляемой условием d(x1, x2) > 0. Поскольку мы знаем распределение измерений из уравнения (10.13), мы можем записать уравнение (10.22) в виде (10.23) В случае psame = 0.5, когда d(x1, x2) > 0 сводится к уравнению (10.19), мы можем добиться некоторого прогресса: когда у нас есть стимулы s1 и s2, ка - кова вероятность того, что сгенерированные измерения имеют одинаковый знак? Мы ответим на этот вопрос в задаче в конце главы. В целом, однако, из-за специфического вида переменной решения в урав- нении (10.18) мы не можем добиться дальнейшего аналитического прогресса. Это новая ситуация, поскольку во всех байесовских моделях, обсуждавшихся до сих пор в этой книге, распределение отклика можно было рассчитать аналитически или выразить через стандартную неэлементарную функцию (интегральную стандартную норму). В нынешней ситуации – и фактически в большинстве байесовских моделей, встречающихся в исследовательской практике, – вместо этого нам приходится прибегать к численным методам. Существует два основных класса численных методов вычисления многомер - ных интегралов, таких как уравнение (10.23).\n--- Страница 262 ---\nБинарные стимулы  261 Численное интегрирование. Самый простой метод численного интегриро- вания – это интегрирование по Риману. Сначала мы разбиваем пространство (здесь состоящее из x1 и x2) на тонкую регулярную сетку. Затем мы вычисля- ем подынтегральное выражение, т. е. двумерное распределение измерений p(x1, x2 | s1, s2) на этой сетке и численно нормализуем его. Потом вычисляем d для всех значений в сетке. Условие d > 0 возвращает нули и единицы и служит так называемой мас - кой (термин из области обработки изображений). Наконец, аппроксимируем интеграл как сумму, умноженную на этот размер шага. В сумме оставляем только те члены, которые удовлетворяют условию d > 0, умножая подынте- гральное выражение на маску. В этом методе следует позаботиться об ис - пользовании достаточно мелкой сетки. Усовершенствованной версией ри- мановского интегрирования является метод трапеций. Примечание 10.1 Вычисление многомерных интегралов Многомерные интегралы часто встречаются на третьем этапе байесовского моде- лирования. Мы упомянули три способа решения таких интегралов: аналитически, с помощью численного интегрирования и с помощью симуляции методом Мон- те-Карло. Но когда какой метод использовать? Везде, где возможен аналитический подход, используйте его, так как вычисление математического выражения в анали- тической форме является сколь угодно точным и очень быстрым. Когда это невоз- можно, ваше решение зависит от доступного вычислительного времени. За опреде- ленный промежуток времени вы можете выполнить только конечное число оценок правила принятия решений, поэтому вам следует выбирать их с умом. Численное интегрирование неэффективно, так как оно тратит много времени на вычисление оценки в маловероятных областях пространства измерений, а симуляция Монте- Карло является стохастической, поэтому ваш результат может меняться от запуска к запуску. Например, если у вас есть два измерения и вы задаете сетку 1000×1000 для численного интегрирования, вам потребуется вычислить один миллион оценок. В симуляции методом Монте-Карло один миллион оценок также даст вам точную оценку желаемой вероятности. Но если бы у вас было три измерения, численное интегрирование потребовало бы одного миллиарда оценок, заняло бы в 1000 раз больше времени и могло оказаться неосуществимым на практике, потому что ве- роятности ответа должны быть вычислены для многих пар стимулов и комбина- ций параметров. Если вы располагаете вычислительным бюджетом всего в один миллион оценок, вы, конечно, можете сделать сетку более грубой (100×100×100), но тогда моделирование методом Монте-Карло будет более надежным. Как правило, численное интегрирование не следует применять для более чем трех измерений. Симуляция Монте­Карло. Второй метод – имитационное моделирование (симуляция). Например, в уравнении (10.23) мы можем случайным обра- зом выбрать большое количество (порядка миллиона) пар измерений из их соответствующих распределений, обусловленных стимулами, а именно нормальных распределений внутри интеграла. Каждая пара представляет собой симуляцию испытания. К каждой паре измерений мы применяем ре- шающее правило, чтобы определить реакцию моделируемого наблюдателя.\n--- Страница 263 ---\n262  Одинаковый или разный? Доля смоделированных испытаний, для которых ответ был «одинаковый», является аппроксимацией лежащей в основе вероятности «одинакового» ответа на стимулы. Этот метод аппроксимации распределения вероятно- стей выборками является частным случаем метода, называемого симуляцией Монте-Карло. В некотором смысле имитационное моделирование методом Монте-Карло создает «эмпирическое» распределение с использованием ком- пьютерного объекта. Упражнение 10.2. Понятно ли вам, что симуляция Монте-Карло на самом деле является способом вычисления интеграла? 10.3. Непрерывные стимулы В стандартном сценарии объединения сигналов (глава 5) наблюдатель имеет два измерения и знает, что они были вызваны одним и тем же стимулом. Однако во многих повседневных ситуациях не ясно, были ли два измерения получены от одних и тех же стимулов или от разных. В этом случае наблю- датель должен сделать вывод о вероятности того, что измерения вызваны одиночным стимулом. Эта вероятность впоследствии будет играть роль при оценке значений стимулов. Данный вывод аналогичен суждению «одина- ковый–разный» в разделе 10.2, но использует непрерывные переменные. В контексте объединения сигналов задача вывода также называется причин- ным выводом (causal inference), тогда как частный случай в главе 5 называется вынужденным слиянием (forced fusion). В лабораторном эксперименте мы могли бы одновременно представить слуховой тон и визуальную вспышку (рис. 10.2А). Когда локация восприни- маемого звука и света почти не различается, наблюдатели могут заключить, что стимулы исходят из одного и того же источника; когда два стимула вос - принимаются дальше друг от друга, можно сделать вывод, что они исходят из разных источников. Рассматривая этот эксперимент в качестве примера, мы увидим, как байе- совский наблюдатель приходит к тому же заключению, но более точным образом. Модель причинного вывода была разработана в 2007 г. двумя ав- торами этой книги и соавторами [98], а также одновременно независимой группой [158]. 10.3.1. Этап 1: порождающая модель Порождающая модель в данном случае такая же, как на рис. 10.1A–B. Мы сно - ва начинаем с переменной «одинаковости» C. Если C = 1, то имеется только один стимул s. Если C = 2, то есть два стимула s1 и s2, которые мы предпо- лагаем возникающими независимо: p(s1, s2 | C = 2) = p(s1 | C = 2)p(s2 | C = 2). (10.24)\n--- Страница 264 ---\nНепрерывные стимулы  263 (А) (B) Стимул s230 20 10 0 –10 –20 –30 –30 –20 –10 10 20 30 0 Стимул s1С = 1 ( одинаковый ) С = 2 ( разный ) Рис. 10.2  Эксперимент причинного вывода: (A) звуки воспроизводятся из динамиков, уста- новленных в горизонтальный ряд за экраном, на который проецируются визуальные вспышки. Зрительный стимул представляет собой пятно света с центром на той же горизонтальной линии, что и динамики. И слуховые, и зрительные стимулы имеют очень маленькую продолжительность. Визуальная надежность определяется размером пятна. Испытания бывают либо унисенсорными (только слуховыми или только зрительными), либо мультисенсорными. При мультисенсорных пробах визуальный и слуховой стимулы предъявляются одновременно. В разных блоках экс - перимента наблюдатель либо локализует слуховой стимул, используя курсор на горизонтальном меридиане (вертикальная черная линия), либо сообщает, находятся ли зрительный и слуховой стимулы в одном и том же месте (нажав клавишу); (B) распределение обусловленных классом стимулов. «Одинаковые» испытания представлены одномерным распределением Гаусса по диа- гонали. «Разные» испытания представлены двумерным распределением Гаусса Мы предполагаем, что все три переменных стимула – s, s1 и s2 – подчиня- ются одному и тому же нормальному распределению со средним значением 0 и стандартным отклонением σs: p(s | C = 1) = �(s; 0, σs2); (10.25) p(s1 | C = 2) = �(s1; 0, σs2); (10.26) p(s2 | C = 2) = �(s2; 0, σs2). (10.27) Визуализация распределений стимулов, обусловленных классом (CCSD) p(s | C = 1) и p(s1, s2 | C = 2) (рис. 10.2B), ясно показывает, что эта задача логи- ческого вывода концептуально похожа на вложенную классификацию в гла- ве 8: одна узкая категория (C = 1), которая «встроена» в широкую категорию (C = 2), за исключением того, что здесь у нас два измерения. Обозначим два измерения через x1 и x2. Как обычно, мы моделируем их как условно независимые и нормально распределенные, но допускаем, что они имеют разные уровни шума σ1 и σ2, как в уравнении (10.13): p(x1, x2 | s1, s2) = p(x1 | s1)p(x2 | s2); (10.28) p(x1 | s1) = �(x1; s1, σ12); (10.29) p(x2 | s2) = �(x2; s2, σ22). (10.30)\n--- Страница 265 ---\n264  Одинаковый или разный? На этом определение порождающей модели завершено. 10.3.2. Этап 2: вывод Наблюдатель делает вывод, исходят ли два измерения от одного и того же стимула (C = 1) или от разных (C = 2). Таким образом, интерес представляет апостериорная вероятность p(C | x1, x2) при текущих измерениях x1 и x2. LPR d определяется уравнением (10.15). Как и в главе 8 и в разделе 10.2, мы оце- ниваем два правдоподобия по C p(x1, x2 | C), маргинализируя по стимулу или стимулам. Правдоподобие для случая «одинаковый»: �(C = 1; x1, x2) = p(x1, x2 | C = 1) (10.31) (10.32) Правдоподобие для случая «разный»: �(C = 2; x1, x2) = p(x1, x2 | C = 2) (10.33) (10.34) С учетом распределений из генеративной модели LPR равно (10.35) (10.36) где мы использовали прецизионные обозначения: и Упражнение 10.3. Докажите это. Последним компонентом этапа 2 является правило оптимального решения: Сообщите «одинаковый», если d > 0. (10.37) Столкнувшись со сложным выражением, таким как уравнение (10.36), час - то полезно попытаться построить и интерпретировать визуальное пред- ставление. Этот прием помогает как обнаружить ошибки, так и обрести интуитивное понимание. На рис. 10.3A построена тепловая карта LPR d в за - висимости от наблюдений x1 и x2 для конкретной комбинации параметров. Черные контуры показывают d = 0. Диагональ соответствует испытаниям, в которых измерения оказались идентичными друг другу. Чем ближе к диа- гонали лежит набор измерений, тем более вероятной становится гипотеза C = 1 по сравнению с C = 2. Это вполне логично: когда два измерения по- хожи, они, вероятно, произошли от одного и того же стимула. По сути, было бы слишком невероятным совпадением, если бы они произошли из разных\n--- Страница 266 ---\nНепрерывные стимулы  265 источников. Это та же самая логика, которую мы использовали в главе 2 для одновременно движущихся точек. Формально данная задача сложнее, но суть та же. Интересно, что в этом рассуждении априорное знание не играет никакой роли. При этом чем дальше от 0 лежит такая пара измерений, тем вероятнее, что стимулы были одинаковыми. Дело в том, что мы выбрали априорное рас - пределение, которое достигает максимума в начале координат. Поскольку стимулы берутся из этого распределения, даже если причины различны, два стимула и, следовательно, два измерения склонны располагаться близко друг к другу около 0. Когда измерения лежат близко друг к другу, но далеко от 0, это сложнее объяснить априорным распределением, и, следовательно, более вероятно, что стимулы были одинаковыми. 50 25 0 –25 –5050 25 0 –25 –501.0 0.8 0.6 0.4 0.2 0.02 0 –2 –4 –6 –8 –10 –12 –50 –50 –50 –25 –25 –25 0 0 0 25 25 25 50 50 50Измерение x2 Измерение x2 Доля сообщений Cˆ = 1 Измерение x1 Измерение x1 Различие стимулов s2 − s1(A) (В) (C)Логарифмическое отношение правдоподобияs1 = 0 s2 = 0 Рис. 10.3  (A) Сила аргумента в пользу общей причины стимулов, выраженная логарифми- ческим отношением правдоподобия, как функция измерений x1 и x2. Изолинии d = 0 показаны черным цветом. Здесь два интересующих нас аспекта: полоса вокруг диагонали и структура внутри этой полосы. Параметры: psame = 0.5, σ1 = 3, σ2 = 10, σs = 10; (B) те же изолинии d = 0, что и в (A), но с наложением двумерного распределения измерений при s1 = 5 и s2 = 8. Объем этого распределения между двумя границами решений равен вероятности дать ответ «оди- наковый» с учетом конкретных стимулов; (C) доля «одинаковых» ответов как функция рас - хождения стимула (разница между s2 и s1), когда s1 = 0 (поэтому расхождение = s2) или когда s2 = 0 (поэтому расхождение = s1) 10.3.3. Этап 3: вероятность отклика Цель состоит в том, чтобы вычислить вероятность получения наблюдателем вывода «одинаковый» для данной комбинации значений стимула (s1, s2), или, другими словами, p(Cˆ = 1 | s1, s2). По аналогии с разделом 10.2.3, эта вероят - ность равна площади под двумерным распределением измерений с учетом тех значений стимула, которые лежат между двумя границами (рис. 10.3B). Как и в разделе 10.2.3, вероятность ответа не может быть найдена аналити- чески. Вместо этого разработчик модели снова должен прибегнуть к чис - ленному интегрированию на сетке или к симуляции методом Монте-Карло. Мы применяем любой метод к парам стимулов s1, s2, где s1 равно 0, а s2 ва- рьируется с psame = 0.5. Результирующая вероятность сообщения о том, что оба сигнала исходят из одной и той же позиции (C ˆ = 1), представлена как\n--- Страница 267 ---\n266  Одинаковый или разный? функция несоответствия стимулов на рис. 10.3C. Мы видим, что чем больше пространственное несоответствие между двумя стимулами, тем реже на- блюдатель сообщает, что существует общая причина. Это вполне логично. Упражнение 10.4. Несоответствие стимулов равно s2 - s1, поэтому существует много комбинаций s1 и s2, которые приводят к одному и тому же несоот - ветствию. На рис. 10.3C мы устраняем эту неоднозначность, устанавливая s1 или s2 равными 0. Эти варианты дают слегка отличающиеся кривые. Почему? 10.3.4. Пересмотр этапа 2: вывод стимулов До сих пор мы обсуждали вывод о числе причин возникновения стимулов. Также может быть интересно апостериорное распределение по значениям стимула s1 и s2. Это распределение можно записать как p(s1, s2 | x1, x2) µ p(x1 | s1)p(x2 | s2)p(s1, s2). (10.38) Пока ничего особенного; мы могли бы сделать это в главе 5. Однако в те- кущей порождающей модели априорное распределение p(s1, s2) недоступно напрямую. Все, что мы знаем, – это распределение s1 и s2, зависящее от ко- личества причин C. Чтобы найти «априорное» p(s1, s2), мы выполняем мар- гинализацию по C : (10.39) Подставляя в уравнение (10.38), находим (10.40) (10.41) Таким образом, этот протопостериор представляет собой средневзвешен- ное значение функции правдоподобия при гипотезе C = 1, p(x1 | s1)p(x2 | s2)p(s1, s2 | C = 1) и функции правдоподобия при гипотезе C = 2, p(x1 | s1)p(x2 | s2)p(s1, s2 | C = 2). Эти правдоподобия взвешиваются априорными вероятностями C = 1 и C = 2 соответственно. Этот тип средневзвешенного значения возни- кает всякий раз, когда требуется маргинализация по дискретной переменной (здесь C ). Упражнение 10.5. Покажите, что альтернативным способом записи апостери- орного распределения по s1 и s2 является взвешенное среднее апостериорных распределений, обусловленных C с весами, заданными как p(C | x1, x2). Это изо- бражено на рис. 10.4 и также известно как «усреднение байесовской модели»,\n--- Страница 268 ---\nНепрерывные стимулы  267 поскольку каждое значение C интерпретируется как «модель мира», которую наблюдатель составляет для себя. Впервые в этой книге мы столкнулись с апостериорным распределением, которое не имеет единственного локального максимума (унимодальность). Это апостериорное распределение имеет два пика (бимодальность). Для би- модальных распределений оценка MAP и оценка апостериорного среднего не эквивалентны; последнее, возможно, имеет больше смысла, поскольку минимизирует ожидаемую квадратичную ошибку. Причинный вывод является важным обобщением объединения сигналов. Кординг и др. [98] показали, что причинный вывод точно описывает челове- ческое восприятие в задаче аудиовизуальной локализации. Когда наблюда- телей попросили сообщить, имеют ли оба стимула одну и ту же причину, их отчеты соответствовали прогнозу, показанному на рис. 10.3C. Наблюдатели также сообщали о местонахождении слухового стимула; их отчеты полно- стью соответствовали байесовским предсказаниям, которые мы проиллюст - рировали на рис. 10.4. p(C = 1| x1, x2) = 0.222 p(C = 1| x1, x2) = 0.580 (A) (B)Вероятность Вероятность –20 –20 0 0 20 20 40 40 Предполагаемые стимулы Предполагаемые стимулы Правдоподобие s1 исходя из х1 Правдоподобие s2 исходя из х2 Апостериорное распределение s2 при C = 1 Апостериорное распределение s2 при C = 2 Апостериорное распределение s2 при неизвестном C Риc. 10.4. Апостериорные и условно апостериорные распределения в причинном выводе: (A) при причинном выводе апостериорное распределение по стимулу может быть бимодальным (двухвершинным). Рассмотрим испытание, в котором х1 = –5 и х2 = 15. Здесь показаны правдоподобия по s1 и s2 и апостериорные рас - пределения по s2, если известно, что C = 1, если известно, что C = 2, и когда сущест­ вует неопределенность относительно C. Последнее апостериорное распределе- ние является взвешенным средним условных апостериорных распределений, где веса представляют собой апостериорные вероятности C = 1 и C = 2 соответ - ственно. Здесь апостериорная вероятность C = 1 составляет всего 22.2 %, поэтому в общей апостериорной вероятности преобладает вероятность, обусловленная C = 2; (B) то же самое, но при x1 = –5 и x2 = –2. Теперь апостериорная вероятность C = 1 выше, поэтому общая апостериорная вероятность больше похожа на равно- взвешенное среднее двух условных апостериорных распределений\n--- Страница 269 ---\n268  Одинаковый или разный? 10.4. Оценка сходства нескольких элементов До сих пор мы обсуждали вывод решения о том, являются ли два стимула оди- наковыми. Эту задачу можно обобщить на любое количество стимулов, ска- жем N. Это обобщение важно в когнитивной науке. Уильям Джеймс, один из отцов-основателей психологии, называл чувство сходства «центром и хреб- том нашего мышления» [81]. Оценка сходства важна в распознавании текстур, которые, как правило, состоят из элементов с одинаковой ориентацией. Также принято считать, что оценка сходства лежит в основе более высоких когни- тивных понятий, таких как равенство и эквивалентность в математике. Мно- гие виды животных, от медоносных пчел до голубей и дельфинов, способны обнаруживать сходство на довольно абстрактном уровне, что позволяет пред - положить, что эта концепция имела существенное эволюционное значение. Поставим задачу следующим образом. Когда стимулы одинаковы (C = 1), их общее значение s берется из распределения p(s). Когда стимулы различны (C = 2), их значения si, где индекс i теперь принимает значения от 1 до N, извлекаются независимо из одного и того же распределения p(s). Пример эксперимента по оценке сходства показан на рис. 10.5А. Испытуемые оце- нивали, имеют ли все N стимулов одну и ту же ориентацию. Стимулы могли иметь разную вытянутость, выбранную случайным образом. Вытянутость влияла на качество информации об ориентации: чем сильнее вытянут эл- липс, тем ниже будет шум ориентации. Высокая ВысокаяОдинаковый или разные? НизкаяНизкаяДоля ответов «разные» Стандартное отклонение (º)+++ +(А) (B) (С) 1.0 0.8 0.6 0.4 0.2 0.0 15 10 5 0С … …s1 x1sN xN Рис. 10.5  Суждение о сходстве (воспроизведено из [186]): (А) экспериментальная про- цедура. Испытуемые фиксировали взгляд на крестике и в течение 100 мс наблюдали изобра- жение, содержащее шесть эллипсов; затем они сообщали, имеют ли все эллипсы одинаковую ориентацию. Достоверность стимула задавали вытянутостью эллипса. В состоянии «низкая» все эллипсы имели малую вытянутость. В состоянии «высокая» они имели большую вытяну- тость. Испытуемые сообщали, были ли все стимулы одинаковыми; (B) доля ответов «разные» в зависимости от стандартного отклонения представленного набора для двух условий; (C) по- рождающая модель этой задачи\n--- Страница 270 ---\nОценка сходства нескольких элементов  269 Пример данных показан на рис. 10.5В: чем больше стандартное отклоне- ние выборки стимулов, показанных в данном испытании, тем чаще люди давали ответ «разные». Кроме того, свое влияние оказывала величина вытя- нутости. Порождающая модель задачи показана на рис. 10.5C. Переменные следующие: C – бинарная переменная, обозначающая сходство (1 для оди- наковых, 0 для разных), s – вектор представленных N ориентаций, а x – со - ответствующий вектор N измерений. Распределение стимулов p(s) является гауссовым со средним значением 0 и стандартным отклонением σs. Каждый xi взят из гауссова распределения со средним значением si и стандартным отклонением σ . Байесовский наблюдатель основывает свое решение (одинаковые или раз- ные) на апостериорных вероятностях C = 1 и C = 2 при заданных измерениях x º (x1, , xN)T. Поскольку C является бинарной случайной величиной, мы выражаем это апостериорное значение как LPR: (10.42) Оценка правдоподобия в этом выражении p(sx|C) требует маргинализации по ориентациям стимула s º (s1, , sN): (10.43) Как обычно, мы предполагаем, что стандартное отклонение σ шума, свя- занного со стимулом, известно наблюдателю для каждого стимула и каждого испытания. Следовательно, нам не нужно делать маргинализацию по σ, но мы можем рассматривать его как известный параметр. Когда C = 1, все элементы вектора s имеют одно и то же скалярное зна- чение s. Тогда интегрирование сводится к интегралу по этому скалярному значению. При этом мы предполагали, что измерения условно независимы, а это означает, что p(x | s) = p(x1 | s)p(x2 | s)  p(xN | s) (10.44) (10.45) где Õ обозначает произведение. Тогда вероятность ответа «одинаковые» равна �(C = 1; x ) = p(x | C = 1) (10.46) (10.47) Хотя этот интеграл кажется сложным, его можно вычислить, используя стандартное уравнение для произведения нормальных распределений.\n--- Страница 271 ---\n270  Одинаковый или разный? Аналогичным образом можно оценить вероятность гипотезы о том, что стимулы различны, т. е. С = 0. В этом случае все измерения совершенно неза- висимы друг от друга, так как не имеют общего s. Таким образом, N-мерный интеграл в уравнении (10.43) сводится к произведению N одномерных интег - ралов, по одному на каждое измерение (см. примечание 10.2): �(C = 0; x ) = p(x | C = 0) (10.48) (10.49) (10.50) Примечание 10.2 Разложение многомерного интеграла Предположим, у вас есть функция двух переменных x и y. Функция обладает тем особым свойством, что ее можно записать в виде произведения функции f(x), за- висящей только от x, и функции g(y), зависящей только от y. Тогда его интеграл по x и y можно упростить как (10.51) (10.52) Мы использовали тот факт, что выражение является просто постоянным числом (в частности, не функцией от x) и может быть вынесено из интеграла по x. Этот аргумент работает, только если x и y – разные переменные. Используя выражения для p (x | C = 1) и p(x | C = 0), мы можем оценить LPR в уравнении (10.42) и получить правило принятия решений. Оно пред- ставляет собой квадратичную функцию измерений. Мы завершим вывод в задаче 10.7. Ван ден Берг и др. [186] показали, что люди судят о сходстве способом, близким к предсказаниям этой модели. Этот пример показывает, как вывод относительно абстрактного качества, такого как «схожесть», может быть смоделирован байесовским способом с использованием той же самой процедуры, которую мы применяли для вывода физического стимула. 10.5. Организация восприятия Извлечение информации о структуре мира из сенсорного ввода является важной частью, если не конечной целью восприятия. Мир сильно структури- рован: форма объекта состоит из цепочки мелких линейных элементов, объ-\n--- Страница 272 ---\nОрганизация восприятия  271 екты упорядочены по глубине, а музыкальное произведение состоит из точно подобранных последовательностей тонов. Как показывает знаменитая иллюзия Канижа (рис. 10.6), мозг воспринимает визуальную структуру, даже если существуют лишь косвенные сенсорные доказательства ее существования. Действи- тельно, на каком-то уровне все наше вос - приятие структуры основано на косвенной информации. Когда мы идем по оживленной улице, наш мозг, казалось бы, без особых усилий отделяет несколько источников зву - ка от единого непрерывного потока. Можно утверждать, что распознавание объектов, не- зависимо от сенсорной модальности, состо- ит из обнаружения структур на нескольких уровнях. С момента зарождения психологии исследователей интересовало то, как мозг воспринимает структуру среди наборов составляющих элементов. По большей части объяснения явлений структурного восприятия были качест - венными и описательными. В основе этих объяснений лежат законы геш- тальта, или принципы гештальта, которые описывают, при каких обстоя- тельствах элементы воспринимаются как принадлежащие к целому (как в иллюзии Канижа). Существует длинный список таких законов: закон непрерывности: элементы, которые предполагают продолжение визуальной линии, обычно группируются вместе; закон замыкания: такие объекты, как формы, буквы, изображения и т. д., воспринимаются как цельные, даже если они не завершены (рис. 10.7А); закон подобия: элементы в ассортименте объектов перцептивно груп- пируются вместе, если они подобны друг другу (рис. 10.7Б); закон общей судьбы: объекты воспринимаются как связанные, если они указывают в одном направлении. Мы столкнулись с этим в раз- деле 2.6; закон близости: объекты, расположенные близко друг к другу, воспри- нимаются как образующие группу (рис. 10.7C–D); закон хорошего гештальта: объекты воспринимаются как сгруппиро- ванные вместе, если они образуют регулярный, простой и упорядочен- ный паттерн. Практически с самого момента своего появления законы гештальта под- вергались критике за расплывчатость, например в законе хорошего гешталь- та, где понятия «регулярный», «простой» и «упорядоченный» не определены. Более того, законы гештальта становятся еще более туманными, когда они противоречат друг другу. Байесовские модели могут улучшить эти законы, поскольку байесовские модели имеют нормативную основу и допускают точ- ные количественные описания. Основная идея заключается в том, что наблю-Рис. 10.6  Треугольник Канижа. Люди склонны видеть белый треугольник, лежащий поверх трех черных дисков\n--- Страница 273 ---\n272  Одинаковый или разный? датель рассматривает две гипотезы – например, элементы принадлежат друг другу или нет – и оценивает их апостериорные вероятности. В разделе 2.6 мы трактовали таким образом гештальт-закон общей судьбы. Далее рассмотрим гештальт-закон непрерывности с помощью байесовской линзы. (А) (B) (C) (D) Рис. 10.7  (А) Закон замыкания; (Б) закон подобия; (C–D) закон близости: левое множество рассматривается как девять несвязанных квадратов, а правое множество – как один квадрат 10.5.1. Формирование контура При визуальном сегментировании сцены наблюдатель должен определить, какие элементы линии принадлежат одной и той же границе или конту - ру. Эта задача называется формированием контура (contour integration)1. В одном задании Фельдман просил испытуемых оценить, образуют ли пять точек угол или плавную кривую (рис. 10.8А) [49]. В другом задании испы- туемые оценивали, образуют ли точки один или два контура (рис. 10.8В). Для обеих задач Фельдман разработал байесовскую модель. Ключевым ком- понентом этой модели является идея о том, что на гладком контуре угол между двумя последовательными сегментами линии скорее будет неболь- шим. Байесовская модель хорошо учитывает особенности человеческого восприятия. (A) Задача распознавания углов (B) Задача двух контуров Плавный изгиб Угол Два контура Один контур Рис. 10.8  Задачи формирования контура. Рисунок воспроизведен из [49]: (A) испытуемые оценивали, образуют точки угол или нет; (B) испытуемые оценива- ли наличие одного или двух контуров 1 Следует отличать от математического термина «интегрирование по контуру». В данном случае речь идет о когнитивном объединении элементов в контур. – Прим. перев.\n--- Страница 274 ---\nОрганизация восприятия  273 Чжоу, Ачерби и Ма [214] изучали восприятие сопоставления между гори- зонтальными отрезками, разделенными промежутком. Их главный интерес представлял эффект неопределенности, обусловленный эксцентриситетом сетчатки. Байесовский наблюдатель примет во внимание неопределенность особым образом, устанавливая критерий для объявления измеренного рас - согласования достаточным, чтобы сообщить о рассогласовании во внешнем мире. Человеческие решения также принимают во внимание неопределен- ность, что близко к байесовским предсказаниям. В этом исследовании распределение несовпадений задавал эксперимен- татор. Другой подход использует для определения порождающей модели природные статистические данные. Гейслер и Перри [57] проверили гипоте- зу о том, что природные статистические данные в сочетании с байесовской моделью могут предсказывать суждения о восприятии контура человеком. Сначала они формализовали проблему, введя соответствующие переменные (рис. 10.9). Состояние интересующего мира состоит в том, принадлежат ли два линейных элемента одному и тому же контуру. Это двоичная перемен- ная: C = 0 (нет) или C = 1 (да). Стимулы представляют собой два краевых элемента, и мы предполагаем, что имеет значение только их положение относительно друг друга. Параметры, используемые для описания этого от - носительного положения, показаны на рис. 10.9A: расстояние между средни- ми точками (d ), угол между эталонным элементом и линией, соединяющей средние точки (φ ), угол между эталонным элементом и ориентацией другого элемента (θ ) и, наконец, полярность контраста (ρ ) – если кто-то соединит два элемента по предполагаемому контуру, не поменяются ли местами темная и светлая стороны в одном из элементов? Высокая Средняя НизкаяВероятность принадлежности к одному контуру Рис. 10.9  Для пар краевых элементов вероятность того, что они принадлежат одному контуру, среди прочих факторов зависит от их пространственного раз- деления и относительного угла. Слева направо эта вероятность уменьшается, по- скольку один контур будет создавать данную конфигурацию со все более низкой вероятностью Порождающая модель этой задачи описывается распределениями вероят - ностей p(d, φ, θ, ρ | C = 1) и p(d, φ, θ, ρ | C = 0). Авторы оценили эти вероятности, анализируя природные сцены. Фотография природной сцены на открытом воздухе, например листьев, лежащих на лесной подстилке, сначала была ав- томатически проанализирована с помощью алгоритма, извлекающего края. Затем на изображении отмечали один пиксель и предъявляли наблюдателю (испытуемому). Наблюдатель указывал, какие другие пиксели изображения принадлежат тому же контуру. Люди были весьма последовательны в своих\n--- Страница 275 ---\n274  Одинаковый или разный? суждениях. Таким образом была получена порождающая модель. В отли- чие от предыдущих примеров в этой книге, в данном случае порождающая модель была задана только численно, то есть с помощью гистограмм, по- казывающих частоту появления каждой комбинации параметров. Еще одно отличие от большинства рассмотренных до сих пор порождающих моделей заключается в том, что сенсорный шум считался незначительным. Вся не- определенность в задаче проистекает из неоднозначности. Наконец, разница в том, что для построения порождающей модели привлекались наблюдатели; экспериментаторы не занимались построением модели. В последующем эксперименте наблюдатели оценивали, принадлежат ли два краевых элемента, частично закрытых другим объектом, к одному и тому же или к разным контурам. Ответы «одинаковый» и «разный» встречались в 50 % случаев. Байесовский наблюдатель сделал бы это суждение, вычислив апостериорное отношение. Когда априорное распределение, отражающее час тоты ответов «одинаковый» и «разный», является плоским, апостериор- ное отношение равно отношению правдоподобия. (10.53) Разработчики моделей смогли сделать прогнозы человеческих суждений на основе порождающей модели. Наглядный пример этих прогнозов показан на рис. 10.9B. Как и следовало ожидать, это показывает, что контуры имеют тенденцию быть гладкими. Наблюдатели-люди работали близко к байесов- скому наблюдателю с аналогичными закономерностями ошибок. Данное исследование тесно связано с вышеупомянутым гештальт-законом непрерывности. Теперь мы видим, что, опираясь на статистические показа- тели природных сцен, этот довольно расплывчатый принцип можно опреде- лить количественно: элементы группируются вместе, если они имеют более высокую вероятность принадлежать одному и тому же контуру, чем к раз- ным. Это наблюдение показывает, как байесовские модели могут улучшить качественные наблюдения в психологии. 10.5.2. Пересекающиеся линии Рассмотрим изображение на рис. 10.10А. Большинство людей интерпрети- руют его как две пересекающиеся линии, а не два соприкасающихся угла, хотя возможна любая интерпретация (рис. 10.10B; на самом деле возможных состояний мира больше). В этом случае закон непрерывности гласил бы, что люди склонны воспринимать изображение как два отдельных непрерывных объекта, потому что элементы с большей вероятностью группируются вмес - те, когда они совмещены. Здесь нам не нужно измерять природную статистику, чтобы сформулиро- вать общий аргумент о том, почему гипотеза 1 более распространена. По- рождающая модель показана на рис. 10.10C. Верхняя переменная относится к состоянию мира, соответствующему двум пересекающимся линиям (C = 1)\n--- Страница 276 ---\nОрганизация восприятия  275 или двум соприкасающимся углам (C = 2). Нам нужно параметризовать сти- мулы. Начнем с С = 1. (A) (В)(С) С Iθ Гипотезы 1 Гипотезы 2 Рис. 10.10  (А) Изображение; (B) две возможные интерпретации этого изображения (C = 1 и C = 2); (С) порождающая модель. C – состояние мира, θ – параметры, I – изображение Линия параметризуется двумя числами, в чем можно убедиться, написав уравнение прямой: y = ax + b. Таким образом, две линии в гипотезе 1 можно задать четырьмя параметрами. Теперь рассмотрим C = 2. Угол параметризу - ется четырьмя числами: двумя координатами начала угла, одним углом для первой стороны и одним углом для второй стороны. Таким образом, для ги- потезы 2 необходимо восемь параметров. Наконец, в обеих интерпретациях изображение однозначно определяется параметрами. Байесовский наблюдатель выполняет вывод, вычисляя апостериорное от - ношение состояний мира на основе данного изображения I : (10.54) Мы мало что знаем об априорной вероятности, но можем оценить отно- шение правдоподобия. Обозначим параметры в каждой гипотезе вектором θ. Следовательно, θ является четырехмерным, когда C = 1, и восьмимерным, когда C = 2. Поскольку θ действует как мешающий параметр, каждая из веро- ятностей вычисляется путем маргинализации по θ. Для упрощения рассуж - дений предположим, что все параметры принимают дискретные значения. Тогда маргинализация представляет собой сумму (10.55) Мы знаем, что изображение однозначно определяется параметрами и ги- потезой. Следовательно, p(I | C, θ) равна 0 для всех комбинаций параметров\n--- Страница 277 ---\n276  Одинаковый или разный? θ, кроме той, которая дает заданное изображение I. Обозначим эту комби- нацию параметров через θI. Для этой комбинации p(I | θ, C) равна 1. Тогда интеграл просто принимает вид p(I | C) = pθ|C(θI | C). (10.56) Теперь остается только найти вероятность θI при каждой гипотезе. Сле- дующий иллюстративный расчет адаптирован из [118]. Предположим, что все параметры независимы и каждый параметр принимает 100 возможных значений. Тогда вероятность θI (или любой другой комбинации параметров) по гипотезе 1 равна тогда как вероятность θI (или любой другой комби- нации параметров) по гипотезе 2 равна Это означает, что отношение правдоподобия равно Другими словами, гипотеза 1 в 100 млн раз более вероятна, чем гипотеза 2. Это объясняет, почему люди видят изображение как две пересекающие- ся линии, а не как два соприкасающихся угла. Интуитивно гипотеза C = 1 требует, чтобы противоположные углы изображения имели общую вершину и были равны, тогда как гипотеза C = 2 намного более свободно допуска- ет эту конфигурацию, а также огромное количество других конфигураций. Факт в том, что изображение соответствует ограниченным характеристи- кам, предсказанным C = 1, поэтому человеческий мозг поддерживает эту гипотезу. Конечно, точное числовое значение отношения правдоподобия будет за- висеть от наших предположений относительно априорных значений пара- метров в рамках каждой гипотезы. Однако любой байесовский наблюдатель, который начинает с широких априорных распределений, поддержит гипоте- зу С = 1, когда ему будет показано изображение на рис. 10.10А. Суть в том, что если две гипотезы могут одинаково хорошо объяснить наблюдения, байесов- ский наблюдатель отдаст предпочтение гипотезе с наименьшим числом па- раметров. В этом смысле бритва Оккама представляет собой эмерджентное свойство байесовского вывода: чем проще модели, тем лучше. Часто более сложные гипотезы (с большим количеством параметров) лучше объясняют данные. Конкретная комбинация параметров в рамках сложной гипотезы может (в отличие от рассмотренного здесь примера) соответство- вать данным более точно, чем любая комбинация параметров, допускаемая более простой гипотезой. Таким образом, существует компромисс между сложностью и силой гипотезы. Этот компромисс также отражен в уравнении (10.55), поскольку p (I | θ, C) является показателем силы гипотезы. Между прочим, байесовский наблюдатель, выбирающий между двумя перцептивными гипотезами, математически идентичен байесовскому экс - периментатору, который анализирует данные, чтобы сделать выбор между двумя конкурирующими моделями. Таким образом, сравнение байесовской модели следует тем же уравнениям, которые здесь обсуждаются для описа- ния человеческого мозга, включая компромисс между сложностью и силой. Этот вопрос более подробно раскрыт в разделе C.5.3 приложения.\n--- Страница 278 ---\nРекомендуемая литература  277 10.6. Заключение В этой главе мы представили байесовские модели принятия решения о том, являются ли наборы стимулов одинаковыми или разными (или связаны ли они между собой). Вы узнали, что: суждение о сходстве относится к отношениям между стимулами и как таковое представляет собой важный тип выводов о восприятии: математически оценка сходства включает в себя структурированные порождающие модели, которые различаются по сложности, то есть мо- дели имеют разное количество параметров; правило Байеса можно в равной степени использовать для расчета апостериорных распределений по структурам (моделям) в дополнение к апостериорным распределениям по «внутренним» переменным сти- мула. Оба этих расчета требуют маргинализации; законы гештальта были ведущей объяснительной основой восприятия зрительной структуры, включая такие решения, как принадлежность линейных элементов к контуру. Однако законы гештальта расплывча- ты и требуют более глубокого объяснения; восприятие зрительной структуры может быть, по крайней мере качест - венно, объяснено байесовскими принципами: элементы группируются вместе, когда вероятность благоприятствует этой интерпретации; компромисс между объяснительной силой и сложностью модели, кото- рая может объяснить некоторые гештальт-восприятия, также является важной темой анализа данных. 10.7. Рекомендуемая литература Jacob Feldman. Bayesian Contour Integration. Perception and Psychophysics 63 (2001): 1171–1182. Wilson S. Geisler and Jeffrey S. Perry. Contour Statistics in Natural Images: Grouping across Occlusions. Visual Neuroscience 26, no. 1 (2009): 109–121. Daniel Goldreich and Mary A. Peterson. A Bayesian Observer Replicates Con­ vexity Context Effects in Figure­ground Perception. Seeing and Perceiving 25, no. 3–4 (2012): 365–395. Konrad P . Kording, Ulrik Beierholm, Wei Ji Ma, Steven Quartz, Joshua B. Te- nenbaum, and Ladan Shams. Causal Inference in Multisensory Perception. PLoS One 2, no. 9 (2007): e943. David J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge: Cambridge University Press, 2003. Yoshiyuki Sato, Taro Toyoizumi, and Kazuyuki Aihara. Bayesian Inference Explains Perception of Unity and Ventriloquism Aftereffect: Identiﬁcation of Common Sources of Audiovisual Stimuli. Neural Computation 19, no. 12 (2007): 3335–3355.\n--- Страница 279 ---\n278  Одинаковый или разный? Ronald van den Berg, Michael Vogel, Krešimir Josić, and Wei Ji Ma Optimal Inference of Sameness. Proceedings of the National Academy of Sciences 109, no. 8 (2012): 3178–3183. Max Wertheimer, Gestalt theory. A Source Book of Gestalt Psychology, edited by Willis D. Ellis, 1–11. London: Kegan Paul, Trench, Trubner, 1938. Yanli Zhou, Luigi Acerbi, and Wei Ji Ma. The Role of Sensory Uncertainty in Simple Contourition. PLoS Computational Biology 16, no. 11 (2020): e1006308. 10.8. Задачи Задача 10.1. Приведите пример реальной задачи, которая требует вывода «одинаковый–разный» и не рассмотрена в этой главе. Сформулируйте ее с точки зрения байесовской теории. Задача 10.2. Представьте, что вы изучаете психофизиологию. Придумайте эксперимент, не рассмотренный в этой главе, чтобы выяснить, являются ли люди байесовскими наблюдателями в своих выводах об одинаковости. Задача 10.3. Вернитесь к разделу 10.2.2, посвященному выводу об одинако- вых и разных бинарных стимулах. Рассмотрим случай, когда стимулы оди- наковы в половине случаев: psame = 0.5. (a) Докажите, что условие d > 0 для d в уравнении (10.18) сводится к уравне- нию (10.19) sign(x1) = sign(x2). (Подсказка: может быть полезно исполь- зовать определение и свойства функции гиперболического косинуса.) (b) Выведите выражение для p(Cˆ = 1 | s1, s2) с использованием интегральных стандартных функций нормального распределения Φstandard (см. приме- чание 7.1). (c) На основании вашего ответа на задание (b) выведите выражение для доли правильных ответов. Упрощайте выражение до тех пор, пока в нем не останется только один Φstandard . Задача 10.4. Вернемся к разделу 10.2.2, посвященному выводу об одинако- вых и разных бинарных стимулах. Если psame = 0.5, решающее правило d > 0 не упрощается аналитически. Однако мы все еще можем визуализировать решающее правило в пространстве (x1, x2): в каких областях этого простран- ства байесовский наблюдатель отвечает «одинаковые», а в каких областях «разные»? Наша цель – получить интуитивное представление о том, как взаи- модействуют σ и psame, чтобы повлиять на оптимальное поведение. (a) Примите μ = 1. Рассмотрите три значения σ (0.5, 1 и 2) и три значения psame (0.4, 0.5 и 0.6). Для каждой комбинации σ и psame создайте график, показывающий границу между областями «одинаковые» и «разные» в пространстве (x1, x2), где любое измерение может принимать значения от -2 до 2. Постройте девять графиков в сетке 3×3 для удобства сравне- ния. (b) Опишите и интерпретируйте влияние σ и psame на границу принятия решения.\n--- Страница 280 ---\nЗадачи  279 Задача 10.5. Вернитесь к разделу 10.2.3 о распределении ответов в бинарной задаче «одинаковые–разные». Выберите psame = 0.4, μ = 1 и σ = 1. Используйте два численных метода для расчета вероятности того, что наблюдатель сооб- щит «одинаковые» в заданном состоянии стимула p(Cˆ = 1 | s1, s2). Обратите внимание, что нам нужно вычислить четыре числа, так как есть четыре воз- можные комбинации s1 и s2. (a) Используйте интегрирование Римана. Для x1 и x2 примените сетку от -5 до 5 с шагом 0.01. Вычислите четыре числа, которые мы ищем. (b) Используйте симуляцию методом Монте-Карло. Полученные результаты должны быть очень близки к результатам предыдущего задания. Задача 10.6. Постройте модель причинного вывода из раздела 10.3. (a) Закодируйте модель причинного вывода и воспроизведите рис. 10.3A. Используйте параметры, указанные в подписи к рис. 10.3. (b) Воспроизведите рис. 10.3C, используя интегрирование Римана. (c) Объясните, почему фиксация s1 = 0 и изменение s2 дают другой резуль- тат, чем при фиксации s2 = 0 и изменении s1. (d) Воспроизведите рис. 10.3C, используя симуляцию методом Монте-Карло. Задача 10.7. Здесь мы проработаем некоторые детали модели вывода «оди- наковый–разный» для сценария множества стимулов, упомянутой в разде- ле 10.4. Предположим, имеется N стимулов. Когда C = 1, все стимулы иден- тичны, и их общее значение взято из нормального распределения со средним значением 0 и стандартным отклонением σs. Когда C = 2, все стимулы из- влекаются независимо из одного и того же нормального распределения. Предположим, что p(C = 1) = 0.5 и выполняются независимые измерения со стандартным отклонением шума σ . (a) Выведите байесовское правило принятия решений MAP . Неравенство, представляющее правило, должно иметь квадратичную функцию от x1, , xN в левой части и постоянное выражение в правой части. (Сове- ты: (1) используйте точность вместо дисперсии; (2) используйте раздел приложения B.7.4. Окончательное правило будет иметь в левой части функцию измерений, которую можно относительно легко интерпрети- ровать.) (b) Вычислите вероятность сообщения «одинаковые» в испытании, соот - ветствующем сценарию «разные», когда стимулы s = (s1, , sN). Задача 10.8. В музыке звук характеризуется тоном, т. е. высотой звучания. Экспериментатор выполняет задание на обнаружение звуковых аномалий следующим образом. Он независимо выбирает два значения высоты тона, обозначаемых s (мы можем рассматривать тон как логарифм частоты, что близко к восприятию) из гауссова распределения p(s) со стандартным откло- нением σs (среднее значение не важно). Затем он предъявляет испытуемому последовательность из трех тонов: два с первым значением высоты тона и один со вторым значением. Три тона предъявляются в случайном порядке, и испытуемый сообщает, какой из трех является лишним. Предположим, что измеренная высота каждого тона независимо искажена гауссовым шумом с нулевым средним значением со стандартным отклонением σ .\n--- Страница 281 ---\n280  Одинаковый или разный? (a) Нарисуйте графовое представление порождающей модели и запишите выражения для распределения вероятностей по переменным. (b) Определите, как байесовский наблюдатель MAP должен оценить временнóе положение аномалии (1, 2 или 3) по измерениям тона. (c) Объясните, почему полученное вами правило выглядит интуитивно по- нятным. (d) Предположим, что σs = 2. Для каждого значения σ от 0.05 до 5 с шагом 0.05 смоделируйте 100 000 наборов измерений и используйте их для оценки вероятности правильного ответа оптимального наблюдателя. Постройте эту вероятность как функцию σ . (e) Каково значение асимптоты при σ → ¥? (f) Повторите задание (d) для двух специальных моделей. В первой спе- циальной модели наблюдатель определяет, какое измерение находится дальше всего от среднего значения трех измерений, и сообщает о место- положении этого измерения как о местоположении аномалии. Во второй специальной модели наблюдатель сравнивает расстояния между парами измерений, находит, какое из этих трех расстояний наименьшее, и выби- рает в качестве местоположения аномалии местоположение измерения, не включенное в эту пару. Покажите, что эти специальные модели при- водят к снижению точности, нанеся на один и тот же график вероятность правильности трех моделей как функцию σ .",
      "debug": {
        "start_page": 257,
        "end_page": 281
      }
    },
    {
      "name": "Глава 11. Поиск 281",
      "content": "--- Страница 282 --- (продолжение)\nГлава 11 Поиск Как найти цель среди множества подобных объектов? В этой главе мы продолжим наше исследование логических выводов со мно- жественными стимулами, изучая поиск. Поиск – это задача выявления цели среди отвлекающих факторов. Если известно, что цель присутствует, задача поиска превращается в задачу локализации цели – установление ее располо- жения. Если наличие цели неизвестно, задача поиска становится задачей об- наружения цели. Поиск может происходить в любой сенсорной модальности. Например, вы хотите узнать, где в толпе стоит ваш друг (зрение), определить, произносит ли кто-то ваше имя среди множества голосов (слух), выяснить, откуда исходит неприятный запах (обоняние), или найти в кармане монету определенного размера (тактильные ощущения). Интересно, что популярная N-вариантная задача с принудительным выбором, хотя традиционно и не рассматривается как таковая, является задачей поиска: в задачах подобного рода наблюдатель локализует цель, которая наверняка присутствует среди N стимулов, предъявляемых либо одновременно, либо последовательно. По- иск исследуют с помощью классического набора поведенческих эксперимен- тов, и в этой главе мы рассмотрим, как его можно смоделировать. Краткое содержание главы Мы начнем с обсуждения экологической важности зрительного поиска для животного в дикой природе и определения трех типов поиска: локализация цели, обнаружение цели и категоризация цели. Мы рассматриваем конкрет - ный пример локализации цели, в котором хищная птица пытается лока- лизовать рыбу, о которой известно, что она присутствует где-то во внешне однородном русле реки. Позже мы рассмотрим тот же пример в качестве задачи обнаружения цели, в которой хищник не уверен, присутствует ли в реке рыба. Мы рассматриваем как локализацию цели, так и ее обнаружение в присутствии шума измерений. Наконец, мы кратко обсудим зрительный поиск, который включает в себя движения глаз.\nГлава 11 Поиск Как найти цель среди множества подобных объектов? В этой главе мы продолжим наше исследование логических выводов со мно- жественными стимулами, изучая поиск. Поиск – это задача выявления цели среди отвлекающих факторов. Если известно, что цель присутствует, задача поиска превращается в задачу локализации цели – установление ее располо- жения. Если наличие цели неизвестно, задача поиска становится задачей об- наружения цели. Поиск может происходить в любой сенсорной модальности. Например, вы хотите узнать, где в толпе стоит ваш друг (зрение), определить, произносит ли кто-то ваше имя среди множества голосов (слух), выяснить, откуда исходит неприятный запах (обоняние), или найти в кармане монету определенного размера (тактильные ощущения). Интересно, что популярная N-вариантная задача с принудительным выбором, хотя традиционно и не рассматривается как таковая, является задачей поиска: в задачах подобного рода наблюдатель локализует цель, которая наверняка присутствует среди N стимулов, предъявляемых либо одновременно, либо последовательно. По- иск исследуют с помощью классического набора поведенческих эксперимен- тов, и в этой главе мы рассмотрим, как его можно смоделировать. Краткое содержание главы Мы начнем с обсуждения экологической важности зрительного поиска для животного в дикой природе и определения трех типов поиска: локализация цели, обнаружение цели и категоризация цели. Мы рассматриваем конкрет - ный пример локализации цели, в котором хищная птица пытается лока- лизовать рыбу, о которой известно, что она присутствует где-то во внешне однородном русле реки. Позже мы рассмотрим тот же пример в качестве задачи обнаружения цели, в которой хищник не уверен, присутствует ли в реке рыба. Мы рассматриваем как локализацию цели, так и ее обнаружение в присутствии шума измерений. Наконец, мы кратко обсудим зрительный поиск, который включает в себя движения глаз.\n--- Страница 283 ---\n282  Поиск 11.1. Разнообразие форм зрительного поиска Для животного в дикой природе эффективный зрительный поиск может быть вопросом жизни или смерти. Часто животные должны определить, при - сутствует ли хищник в наблюдаемой сцене. Хищник может быть скрыт или замаскирован, из-за чего его трудно отличить от окружающих визуальных элементов (рис. 11.1А). Хищники тоже вынуждены пытаться обнаружить и локализовать хорошо замаскированную добычу. Современные люди могут искать конкретный документ среди множества похожих бумаг на захламлен- ном столе (рис. 11.1B) или конкретный продукт среди множества похожих на полках продуктового магазина. (A) (В) Есть ли здесь цель? Рис. 11.1  Зрительный поиск: (А) в животном мире; (B) в лаборатории В лаборатории для изучения зрительного поиска исследователи использу - ют различные упрощенные задачи. Заниматься изучением психофизиологии с природными сценами сложно по нескольким причинам. Во-первых, при- родные сцены настолько богаты содержанием, что для их математического описания потребовалось бы многомерное пространство. Во-вторых, не ясно, что такое объект: все дерево является объектом или объект представляет собой отдельную ветвь или, может быть, даже лист? В-третьих, шум в раз- личных измерениях естественной сцены обычно имеет сложную и в значи- тельной степени неизвестную корреляционную структуру, которая выходит далеко за рамки гауссова распределения шума, которое мы рассматривали до сих пор. Поэтому в лабораторных задачах зрительного поиска с модельным подходом обычно используют чрезвычайно упрощенные сцены поиска, ко- торые содержат относительно небольшое количество объектов, сильно раз- личающихся только по одному измерению стимула (рис. 11.1Б). Очевидно, что существует большой разрыв между такими простыми, искусственными сценами и естественными задачами, но есть надежда, что вычислительные принципы, которые мы можем раскрыть, используя лабораторные задачи,\n--- Страница 284 ---\nЛокализация цели при маскировке  283 применимы и к искусственным задачам – мы полагаем, что лабораторные задачи позволяют нам изучать некоторые базовые блоки вычислений, кото- рые мозг выполняет в реальном мире. Лабораторные задачи зрительного поиска бывают как минимум трех ос - новных видов: локализация цели. Известно о наличии одной или нескольких целей. Наблюдатель должен решить, какие из представленных стимулов яв- ляются целями (дискретная задача) или где расположены цели (непре- рывная задача); обнаружение цели. Цели могут присутствовать или отсутствовать, и на- блюдатель должен решить, присутствуют ли какие-либо цели; категоризация цели. Наблюдателя спрашивают о категории цели. Мы рассмотрим только этот третий вариант в задаче 11.12. Далее мы покажем, что в вычислительном отношении эти три вида поис - ковых задач очень похожи. В каждой из этих парадигм необходимо учиты- вать несколько факторов: сколько может быть целей? В данной главе для простоты мы в основ- ном будем рассматривать случай одиночной цели (в отличие от за- дачи 11.7); являются ли отвлекающие факторы независимыми или как-то связа- ны друг с другом? На протяжении всей главы мы будем рассматривать в основном случай независимых отвлекающих факторов (другие вари- анты упоминаются в задачах к этой главе); осложнен ли поиск неоднозначностью, шумом или тем и другим? Во многих задачах зрительного поиска неоднозначности достаточно, что- бы усложнить задачу. Это также упрощает нашу трактовку, поскольку предполагается отсутствие шума при измерении, поэтому в текущей главе мы воспользуемся сценарием с неоднозначностью. В каждой парадигме местоположение цели априори неизвестно наблю- дателю. По этой причине наблюдатель должен рассмотреть все возможные места. В задачах локализации местоположение цели представляет собой ин- тересующую переменную состояния мира. В задачах обнаружения цели ее местоположение, наоборот, является мешающим параметром, и байесовский наблюдатель не учитывает эту переменную. 11.2. Локализация цели при маскировке Прекрасный естественный пример поиска можно увидеть у животных, пы- тающихся обнаружить замаскированных хищников или добычу. В главе 1 мы обсуждали маскировку как развитую стратегию создания широких функ - ций правдоподобия у наблюдателя. Здесь мы рассмотрим учебную зада- чу, вдохновленную образом камбалы (рис. 1.9). Предположим, что в реке особый вид рыб с точечной окраской, который иногда зависает прямо над руслом, покрытым галькой. Речная рыба часто смотрит вверх по течению,\n--- Страница 285 ---\n284  Поиск так как такая ориентация обеспечивает лучшую устойчивость против тече- ния и взгляд обращен в направлении потенциальной пищи, которую уносит вниз по течению. Мы предполагаем, что точечная рыба принимает только эту ориентацию (рис. 11.2А). Разобьем рассматриваемую область на сетку 10×10 и рассмотрим разновидность точечной рыбы размером 1×10. Если рыба присутствует, то априорная вероятность того, что она находится в любом из десяти рядов, равна. Мы предполагаем, что цвет рыбьей кожи иденти- чен цвету речного ила и что каждая точка на коже очень похожа на гальку. Предположим, что каждый квадрат сетки на русле реки независимо имеет вероятность a наличия камешка и 1 - a того, что камешка там нет. Для рыбы вероятность наличия точки в любом квадрате сетки на коже равна b, а веро- ятность отсутствия точки равна 1 - b. Образец рыбы ( b = 0.4)(А) Дно реки ( a = 0.7)(В) (С) 0.020 0.020 0.020 0.002 0.002 0.002 0.002 0.006 0.858 0.070sN s1L х Рис. 11.2  Задача локализации замаскированного объекта: (A) четыре пред- ставителя вида точечных рыб с вероятностью точки b = 0.4. Рыба имеет раз- мер 1×10; (B) сетка русла реки размером 10×10 с вероятностью наличия гальки а = 0.7 в одном ряду с рыбой (т. е. в одной позиции по оси y). Рядом с каждой строкой указана апостериорная вероятность того, что рыба присутствует в этой строке; (С) порождающая модель. Здесь количество строк N равно 10 Упражнение 11.1. Как вы думаете, какие факторы важны для маскировки? Какие из них регулярно используют животные в природе? Доводилось ли вам наблюдать маскирующееся животное? Мы начнем изучение поиска с задачи локализации. Представьте себе хищ - ника, который знает, что в этом районе есть рыба; возможно, хищник видел очертания рыбы, когда она двигалась в этом общем направлении мгнове- нием ранее. Задача хищника состоит в том, чтобы с помощью визуального наблюдения, как на рис. 11.2В, определить, в каком ряду сетки находится рыба. Каждая строка является «стимулом» si. Хищник знает, что ровно один из N представленных стимулов s1, …, sN является целью, и должен решить, какой именно. Обозначим индекс цели через L; таким образом, sL является целевым стимулом. Этап 1: порождающая модель (рис. 11.2C). Порождающая модель содер- жит местоположение цели L, которое представляет собой целое число от 1 до 10, количество возможных позиций y для рыбы и стимулы (s1, …, sN),\n--- Страница 286 ---\nЛокализация цели при маскировке  285 которые мы иногда будем обозначать буквой s (векторы обычно обознача- ют жирным шрифтом). Равномерно распределенная априорная вероятность равна p(L) = 1/N. Далее нам нужно указать распределение вектора стимула s при заданном местоположении цели L. В нашем сценарии внешний вид цели (рыбы) и каждого из отвлекающих факторов (рядов камешков) получается независимо из их соответствующих распределений. Таким образом: (11.1) Обозначим распределение целевого стимула через ptarget (s), а распределе- ние отвлекающего стимула через pdistractor (s). Тогда (11.2) Мы можем вычислить эти вероятности, используя долю точек на рыбе и долю гальки на поверхности дна реки. Этот расчет аналогичен расчету вероятности чередования засушливых и дождливых дней в разделе 6.2. На- пример, если стимул представляет собой рыбу, то вероятность конкретного рисунка точек получается путем перемножения десяти множителей, по од- ному для каждой горизонтальной позиции, где точка вносит множитель b, а пустая позиция вносит множитель 1 - b. В результате, если мы обозначим количество точек в строке i как ni, мы имеем: ptarget (si) = bni(1 - b)10- ni; (11.3) pdistractor (si) = ani(1 - a)10- ni. (11.4) На этом спецификация порождающей модели завершена. Этап 2: вывод. Даже в этом относительно простом случае визуального по- иска наблюдатель должен объединить информацию по N элементам: в конце концов, цель находится ровно в одном месте. Это означает, что свидетельство в пользу ряда, содержащего только камешки, должно считаться свидетель- ством в пользу одного из других рядов, содержащих рыбу. Теперь формализуем вывод. Наблюдение представляет собой набор сти- мулов s. Интересующая переменная состояния мира – это местоположение цели L, поэтому нам нужно вычислить апостериорную вероятность p(L | s). По правилу Байеса: p(L | s) µ p(s | L)p(L). (11.5) Учитывая равномерное априорное распределение и уравнение (11.1), мы получаем (11.6)\n--- Страница 287 ---\n286  Поиск Подставим уравнение (11.2): (11.7) Поскольку мы находимся на этапе 2, L – это предполагаемое местоположе- ние цели, а sL – наблюдаемое значение стимула в этом месте. Произведение вычисляется по всем мешающим факторам, то есть по всем стимулам, кроме целевого. Следовательно, мы также можем написать (11.8) Теперь произведение охватывает все стимулы и, следовательно, не зависит от L. Поскольку наше отношение выражает пропорциональность, мы можем написать: (11.9) Интересно, что дробь представляет собой отношение правдоподобия при- сутствия цели, если бы был только один стимул, или, другими словами, «ло- кальное» отношение правдоподобия классификации стимула sL как цели по сравнению с помехой. Наконец, чтобы получить правильную апостериорную вероятность, пра- вую часть уравнения (11.9) необходимо нормировать путем деления на сум- му этого выражения по всем L. Уравнение (11.9) представляет собой фунда- ментальное соотношение для локализации одной цели среди независимых помех. До сих пор мы использовали следующие аспекты порождающей модели: (а) что помехи извлекаются независимо из распределения помех; (b) что существует только одна цель, взятая независимо из распределения целей; (c) что, если цель присутствует, она имеет одинаковую вероятность находиться в каждом месте (хотя это легко обобщить, заменив везде 1/N на p(L)). В нашем сценарии маскировки мы еще не использовали свойства ptarget и pdistractor . Теперь сделаем это, подставив выражения из уравнений (11.3) и (11.4) в уравнение (11.9) для апостериорного распределения вероятности положения цели: (11.10) Упражнение 11.2. Покажите, что это выражение верно. Мы рассчитали эти апостериорные вероятности в примере на рис. 11.2В. Если хищник максимизирует точность, он будет выполнять оценку апосте- риорного максимума (MAP) L. Это означает выбор значения L, для которого правая часть уравнения (11.9) является наибольшей. Мы можем записать это как\n--- Страница 288 ---\nЛокализация цели с помощью зашумленных измерений  287 (11.11) Интересным аспектом данного правила является то, что каждый потенци- альный целевой стимул sL следует рассматривать индивидуально. Наблюда- тель вычисляет для каждого отдельного стимула отношение правдоподобия, как если бы он классифицировал (категоризировал) этот стимул как цель или отвлекающий фактор. Таким образом, стимул с наивысшим отношением правдоподобия категоризации также является стимулом с наибольшей апо- стериорной вероятностью стать целью в этой задаче поиска. Это не общий закон; подход в решающей степени зависит от предположения о независи- мости, которое мы сделали здесь. Поскольку шума измерения нет, этап 3 не нужен: для одного и того же набора стимулов байесовский наблюдатель всегда будет давать один и тот же ответ. (Это не так хорошо, как кажется, поскольку реальные люди не обя- зательно всегда будут давать один и тот же ответ. Чтобы учесть такую из- менчивость, в модель можно добавить шум принятия решений.) Однако вы- числение показателей точности (например, доли правильных ответов) для заданных a и b не является тривиальной задачей, так как это потребовало бы маргинализации по s, связанным с данными параметрами. Мы сделаем это в задаче 11.11. 11.3. Локализация цели с помощью зашумленных измерений В предыдущем примере стимулы были дискретными. Теперь мы рассмот - рим локализацию цели с помощью непрерывных стимулов, для которых мы также вводим знакомое понятие измерительного шума. Базовая структура задач локализации цели, которые мы рассматриваем, остается такой же, как и в примере с рыбой: наблюдатель ищет существующую единственную цель, значение которой выбирается из распределения целей, среди N - 1 помех, значения которых извлекаются независимо из распределения помех. Однако теперь все стимулы являются одномерными непрерывными переменными. Кроме того, наблюдатель имеет шум измерения. Теперь неопределенность возникает не только из-за перекрытия распределений цели и помехи, но и из-за шума измерения. Между прочим, локализация цели такого рода вычислительно идентична N-вариантной парадигме вынужденного выбора, также называемой N­ин­ тер вальным вынужденным выбором, в которой цель присутствует среди N сти - мулов, представленных последовательно. Например, в парадигме вынуж - денного выбора с двумя интервалами испытуемому могут последовательно предъявляться два изображения, одно из которых содержит только чистый шум, а другое – шум плюс слабый целевой объект. Испытуемый сообща- ет, какое изображение содержит целевой объект. (Заметим, что некоторые\n--- Страница 289 ---\n288  Поиск исследователи используют термин «N -вариантный вынужденный выбор» в более широком смысле, а именно для любой задачи с N вариантами вы- бора.) Таким образом, в оставшейся части этого раздела можно заменить пространство временем, чтобы получить модель для N -интервальной задачи с вынужденным выбором. Далее мы пройдем обычные три этапа. Этап 1: порождающая модель (рис. 11.3А). Мы снова обозначим место- положение уникальной цели через L, а вектор стимулов через s = (s1, …, sN). Теперь примем общее априорное распределение вероятности местоположе- ния цели p(L). Мы предполагаем независимость стимулов, обусловленных L, как в уравнении (11.1). При заданном L мы также снова имеем (11.12) В отличие от примера с рыбой, ptarget (s) и pdistractor (s) теперь являются не- прерывными распределениями; примеры показаны на рис. 11.3B. (А) (B) L … …s1 x1sN xN Вероятность –10 –5 0 5 10 Стимул sptarget (s) pdistractor (s) Рис. 11.3  Визуальный поиск с одной целью, независимыми помехами и из- мерительным шумом: (A) генеративная модель при наличии N стимулов. L – индекс цели, целое число от 1 до N; (B) пример распределения целей и по- мех. Здесь распределение цели является нормальным со средним значением 3 и стандартным отклонением 2, а распределение помех является нормальным со средним значением 0 и стандартным отклонением 3. Эти два распределения перекрываются, поэтому помехи можно спутать с целью Что касается измерений, мы сделаем наше обычное предположение, что они независимы, при условии что они обусловлены стимулами. Формально мы можем написать для вероятности измерений x при заданных стимулах s: (11.13) Каждое отдельное измерение следует гауссову распределению со средним значением, равным соответствующему стимулу:\n--- Страница 290 ---\nЛокализация цели с помощью зашумленных измерений  289 p(xi | si) = �(xi; si, σ2). (11.14) Вскоре мы увидим, что две формы условной независимости хорошо со- четаются друг с другом. Этап 2: вывод. В этой задаче правило Байеса принимает форму p(L | x) µ p(L)p(x | L). (11.15) Правдоподобие местоположения цели может быть записано как маргина- лизация по стимулам, так что (11.16) Благодаря условной независимости измерений мы можем дополнительно оценить правдоподобие как интеграл по каждой переменной стимула s1, , sN: (11.17) Два произведения объединяются, и каждый si появляется только в одном множителе результирующего произведения. В результате N-мерный инте- грал сводится к произведению одномерных интегралов (примечание 10.2): (11.18) Таким образом, по сравнению с уравнением (11.6) мы заменили правдо- подобия отдельных элементов p(si | L) маргинальными значениями. Дальней- ший вывод проводится аналогично предыдущему разделу, и для правдопо- добия мы можем записать выражение (11.19) Как и в примере с рыбой, второй множитель напоминает отношение прав- доподобия в задаче бинарной классификации, в частности уравнение (8.25): это «локальное» отношение правдоподобия того, что стимул sL является целью , а не отвлекающим фактором. Особый случай: одно целевое значение, одно отвлекающее значение Теперь рассмотрим особый случай, когда цель может принимать единствен- ное конкретное значение sT, а помеха может принимать единственное другое конкретное значение sD. Формально это записывается как набор дельта- функций (раздел приложения B.8):\n--- Страница 291 ---\n290  Поиск ptarget (s) = δ(s - sT ); (11.20) pdistractor (s) = δ(s - sD). (11.21) Поскольку теперь у нас есть шум измерения, локализация цели может оказаться сложной проблемой: из-за этого шума цель можно спутать с поме- хами. Предположим, что измерение xi подчиняется гауссову распределению со средним значением si и дисперсией σ2 i. Тогда в уравнении (11.19) дельта- функции и интегралы «уравновешивают» друг друга, и остается (11.22) Эта вероятность местоположения цели имеет тесную связь с задачей раз- личения из главы 7, а именно с уравнением (7.10): она равна отношению правдоподобия стимула sL, являющегося целью, по сравнению с помехой. 11.4. Обнаружение цели при маскировке Мы упоминали в разделе 11.1, что локализация – это лишь одна из возмож - ных форм поиска. Другой важной формой поиска является обнаружение цели – задача определения того, присутствует или отсутствует целевой объ- ект (или несколько целевых объектов) в сцене. В этой главе мы предполагаем, что в сцене присутствует не более одной цели. Теперь мы продолжим пример с маскировкой из раздела 11.2, но будем рассматривать его как задачу обна- ружения. Предположим, что хищник только что появился на месте действия и не может предполагать присутствие рыбы. На основании визуального на- блюдения за руслом реки хищник должен определить, есть здесь рыба ( С = 1) или нет (С = 0). Этап 1: порождающая модель. Порождающая модель (рис. 11.4А) похожа на модель в разделе 11.2, но теперь содержит дополнительную перемен - ную С, указывающую на отсутствие или наличие цели. Бинарная перемен- ная C извлекается из априорного распределения p(C). Это также приводит к тому, что порождающая модель имеет два компонента, как и в главе 10. Если цель отсутствует (C = 0), все стимулы si являются помехами, то есть рус - лом реки с вероятностью наличия гальки a для каждого квадрата. Если цель присутствует (C = 1), порождающая модель будет такая же, как на рис. 11.2C, где L обозначает местоположение цели, а s = (s1, …, sN) обозначает набор паттернов точек. Когда C = 1, присутствует только одна цель, и каждое мес - тоположение имеет равную вероятность p (L) = 1. Этап 2: вывод. Наблюдатель (хищник) теперь заинтересован в том, чтобы сделать вывод о наличии рыбы, то есть о бинарной переменной C, на основе наблюдения за руслом реки s. На рис. 11.4 показаны наблюдения за девятью случайно выбранными рыбами с b = 0.1, 0.4 или 0.7 (по три рыбы на каждый\n--- Страница 292 ---\nОбнаружение цели при маскировке  291 тип), парящими в воде над одним и тем же руслом реки, характеризующимся a = 0.7. Следуя нашей обычной процедуре, когда мы сталкиваемся с бинарной переменной, мы выражаем апостериорное распределение вероятностей C, используя логарифмическое апостериорное отношение: (11.23) (11.24) S1 S1LC = 1C = 0b = 0.1 b = 0.4 b = 0.7SN SN(A) (В) LR: 282 LR: 3.0 LR: 1.0LR: 1.3 LR: 5.3 LR: 1.0LR: 14 LR: 11.3 LR: 1.0 Рис. 11.4  Задача обнаружения замаскированного объекта: (A) схема порождающей моде- ли. Для текущего примера N = 10; (B) русло реки, для которого a = 0.7. По три рыбы случайным образом выбрали из видов, характеризующихся b = 0.1, 0.3 или 0.7 (всего девять рыб). Стрел- ки указывают истинное местонахождение рыбы. Отношение правдоподобия (LR) присутствия рыбы показано под каждым изображением Исследуем правдоподобия по отдельности, начиная с C = 0. Используя не- зависимость наблюдений, сразу находим для правдоподобия C = 0: �(C = 0; x ) º p(s | C = 0) (11.25) (11.26) Найти правдоподобие C = 1 немного сложнее, потому что местоположение цели L служит мешающей переменной, и нам нужно выполнить маргинали- зацию:\n--- Страница 293 ---\n292  Поиск �(C = 1; x ) = p(s | C = 1) (11.27) (11.28) (11.29) , (11.30) где в последнем равенстве мы использовали условную независимость сти- мулов. Внутри суммы мы обнаруживаем произведение из уравнения (11.6). Мы можем продолжить преобразование уравнения (11.30) и получим (11.31) Объединив уравнения (11.26) и (11.31), теперь мы можем вычислить отно- шение правдоподобия (LR) присутствия цели по сравнению с ее отсутствием (11.32) По аналогии с уравнением (11.9) дробь внутри суммы представляет собой «локальное» отношение правдоподобия классификации sL как цели по срав- нению с помехой. Обозначим соответствующее отношение правдоподобия как LRL, (11.33) так что уравнение (11.32) принимает вид: (11.34) Другими словами, при сделанных нами предположениях «глобальное» от - ношение правдоподобия LR равно среднему значению «локальных» отноше - ний правдоподобия. Если бы мы использовали общее априорное распреде- ление вероятностей p (L), это среднее значение было бы средневзвешенным с весами, заданными p (L). Логарифм общего отношения правдоподобия можно записать как (11.35)\n--- Страница 294 ---\nОбнаружение цели с помощью зашумленных измерений  293 где LLRL = log LRL. Уравнение (11.35) показывает, что в этой форме обна- ружения целей «глобальное» логарифмическое отношение правдоподобия присутствия цели является нелинейной функцией «локальных» логарифми- ческих отношений правдоподобия. Его можно рассматривать как правило пространственной интеграции свидетельств. Имейте в виду, что в этом вы- воде мы придаем большое значение специфическим статистическим свой- ствам моделируемой здесь задачи: что отвлекающие факторы независимы и что если есть цель, то она единственная. Если эти свойства неприменимы, то и уравнение (11.35) может оказаться неприменимым. В завершение нашего сценария маскировки мы можем использовать свой- ства ptarget и pdistractor . При этом уравнение (11.32) для LR присутствия рыбы по сравнению с ее отсутствием принимает вид (11.36) где nL – количество точек в L-й строке. В задаче 11.11 мы детально разберем, что следует из этого выражения. 11.5. Обнаружение цели с помощью зашумленных измерений Теперь рассмотрим обнаружение цели с помощью зашумленных измерений. Этот раздел можно рассматривать как комбинацию раздела 11.3 о локали- зации цели с зашумленными измерениями и раздела 11.4 по обнаружению цели без шума измерений. Этап 1: порождающая модель (рис. 11.5). Порождающая модель такая же, как в разделе 11.4, за исключением того, что (a) мы используем общее априорное распределение p(L) вместо равномерного; (b) теперь есть слой зашумленных измерений, порождаемых стимулами. Мы предполагаем, что измерения независимы, когда они обусловлены стимулами. Этап 2: вывод. Вывод происходит, как в разделе 11.4, за исключением того, что теперь даны измерения, а не стимулы; последние неизвестны на- блюдателю. Таким образом, логарифмическое апостериорное отношение равно: (11.37) (11.38) Вероятности получаются путем маргинализации стимулов s. Правдоподо- бие C = 0 теперь равно\n--- Страница 295 ---\n294  Поиск �(C = 0; x ) º p(x | C = 0) (11.39) (11.40) (11.41) (11.42) и аналогично для C = 1. Помимо этого, мы следуем разделу 11.4 и находим отношение правдоподобия присутствия цели: (11.43) (11.44) C = 0 C = 1 L … … … …sN sN xN xNs1 s1 x1 x1 Рис. 11.5  Графовая структура порождающей модели для задачи обнаружения одиночной цели. C = 0: цель отсутствует. C = 1: цель присутствует Это означает, что уравнение (11.34) для связи «глобального» отношения правдоподобия LR с локальным отношением правдоподобия LRL остается в силе. Связь с задачей встроенного класса из раздела 8.6. Теперь рассмотрим случай, когда N = 1, значение цели следует гауссову распределению со сред- ним значением 0 (или любым произвольным значением) и дисперсией σ2 1, а значение отвлекающего фактора следует гауссову распределению с тем средним значением и дисперсией σ2 2. Отсюда задача обнаружения цели сво- дится к задаче встроенного класса из раздела 8.6 (см., в частности, рис. 8.5). Важным частным случаем является то, что цель имеет единственное значе- ние (σT = 0). В общем случае с N таких же предположений задача встроенного класса по-прежнему является составляющим элементом задачи обнаруже- ния цели.\n--- Страница 296 ---\nПрименение  295 Особый случай: одно целевое значение, одно отвлекающее значение. Как и в разделе 11.3, мы рассматриваем случай одного целевого значения sT и одного отвлекающего значения sD. Тогда локальное логарифмическое от - ношение правдоподобия присутствия цели равно (11.45) 11.6. Применение Существует обширная литература по когнитивной психологии, посвященная зрительному поиску. В ней представлено много описательных и специаль- ных моделей. Гораздо реже встречаются вероятностные модели зрительного поиска. Чтобы создать условия, в которых будут применяться модели, экс - периментаторы обычно просили испытуемых фиксировать взгляд на центре экрана, демонстрировали предметы на фиксированном расстоянии от этого центра, снабжали предметы только одной характеристикой, относящейся к задаче, и отображали стимул в течение очень короткого периода времени (например, 100 мс) [141]. Такие эксперименты приносят в жертву некото- рое соответствие реальному миру ради удобства управления эксперимен- том и моделируемости. Байесовские модели простого зрительного поиска восходят как минимум к [143]. Однако на протяжении многих десятилетий, начиная с [136], байесовские модели уступали место моделям теории обна- ружения сигналов, в частности модели максимального выхода или макси- мальные модели. Максимальная модель имеет смысл, когда отвлекающие факторы однородны (идентичны друг другу) – случай, не рассматриваемый в этой главе; тогда она также очень похожа на байесовскую модель. Однако модель терпит неудачу, когда отвлекающие факторы неоднородны. Тогда байесовские модели поиска по-прежнему хорошо описывают эксперимен- тальные данные с участием человека, если дополнительно предполагается, что величина (стандартное отклонение) шума измерений увеличивается с увеличением числа стимулов [30, 123, 154]. Это увеличение шума также можно рассматривать как «ограничение ресурсов». В естественном зрительном поиске движения глаз имеют решающее зна- чение. В лабораторных экспериментах роль движений глаз иногда наме - ренно минимизируют, требуя от участника фиксации взгляда. Этот подход имеет преимущества как с точки зрения эксперимента, так и с точки зрения модели [141]. Тем не менее байесовские модели также могут применяться к задачам зрительного поиска, в которых разрешены движения глаз, напри- мер свободный просмотр сцены при локализации цели. В таких задачах цель часто состоит в том, чтобы предсказать следующее место фиксации взгляда. Апостериорные распределения можно использовать для оценки ожидаемых преимуществ различных возможных мест фиксации [132, 211]. Эти сцены по-прежнему не являются полностью естественными, но имеют тщатель-\n--- Страница 297 ---\n296  Поиск но контролируемые статистические параметры. Существует много работ по моделированию зрительного поиска в природных сценах. Это направление исследований более полезно для практического применения, чем все, что мы обсуждали в данной главе, но выходит за рамки данной книги, поскольку не является байесовским. 11.7. Заключение В этой главе мы представили байесовский вывод в задачах обнаружения и локализации целей. Вы узнали, что: поиск является важной задачей в естественной природе, например когда животное охотится за замаскированной добычей; поиск приводит к статистической зависимости между всеми потенци- альными целями. Это порождает более сложную задачу логического вывода, которую мы все еще можем решить с помощью байесовских методов; при зрительном поиске с разнородными мешающими факторами вы- вод требует маргинализации по значениям помехи (а в случае обнару - жения также по целевому местоположению). 11.8. Рекомендуемая литература Miguel P . Eckstein. Visual Search: A Retrospective. Journal of Vision 11, no. 5 (2011): 14. Wei Ji Ma, Vidhya Navalpakkam, Jeffrey M. Beck, Ronald van den Berg, and Alexandre Pouget. Behavior and Neural Basis of Near­Optimal Visual Search. Nature Neuroscience 14, no. 6 (2011): 783–790. Wei Ji Ma, Shan Shen, Gintare Dziugaite, and Ronald van den Berg. Requiem for the Max Rule? Vision Research 116 (2015): 179–193. Helga Mazyar, Ronald Van den Berg, and Wei Ji Ma. Does Precision Decrease with Set Size? Journal of Vision 12, no. 6 (2012): 10. Jiri Najemnik and Wilson S. Geisler. Optimal Eye Movement Strategies in Visual Search. Nature 434, no. 7031 (2005): 387–391. John Palmer, Preeti Verghese, and Misha Pavel. The Psychophysics of Visual Search. Vision Research 40, no. 10–12 (2000): 1227–1268. Ruth Rosenholtz. Visual Search for Orientation among Heterogeneous Distrac­ tors: Experimental Results and Implications for Signal­Detection Theory Models of Search. Journal of Experimental Psychology: Human Perception and Per for- mance 27, no. 4 (2001): 985–999. Shan Shen and Wei Ji Ma. A Detailed Comparison of Optimality and Simplicity in Perceptual Decision Making. Psychological Review 123, no. 4 (2016): 452–480. Scott Cheng-Hsin Yang, Mate Lengyel, and Daniel M. Wolpert. Active Sensing in the Categorization of Visual Patterns. Elife 5 (2016): e12215.\n--- Страница 298 ---\nЗадачи  297 11.9. Задачи Задача 11.1. Придумайте пример задачи поиска из повседневной жизни, которая не использовалась в качестве примера в этой главе. Задача 11.2. Мы упомянули, что в экспериментах по тестированию моделей из этой главы испытуемого обычно просят фиксировать взгляд на центре экрана, предметы предъявляются на фиксированном расстоянии от этого центра и имеют единственную релевантную характеристику, а на экране возникают в течение очень короткого периода времени (например, 100 мс). Объясните, как отклонения от каждого из этих элементов схемы эксперимен- та могут создать потребность в более сложных моделях. Задача 11.3. Рассмотрим случай из раздела 11.3, но без шума измерений. Предположим, что распределение цели является нормальным со средним значением μT и дисперсией σ2 T, а распределение отвлекающих факторов яв- ляется нормальным со средним значением μD и дисперсией σ2 D, как показано на рис. 11.3B. (a) Покажите, что апостериорное распределение вероятностей расположе- ния цели L равно (11.46) (b) Предположим, что N = 3, p(L) = (0.3, 0.3, 0.4), остальные параметры на рис. 11.3B. Предположим, что получены наблюдения s = (0.9, 6.1, -0.2). Вычислите апостериорное распределение вероятностей L . (c) Предположим, что распределение p(L) является равномерным. Кроме того, предположим, что σT < σD, что свойственно задаче поиска (цели обычно определены более узко, чем отвлекающие факторы). Покажите, что в таком случае оценка MAP сводится к выбору местоположения L, для которого sL ближе всего к (d) Предположим, что распределение p(L) является равномерным. Исполь- зуйте параметры на рис. 11.3B. Изменяйте N от 1 до 8. Для каждого зна- чения N правильно рассчитайте долю правильных ответов. Постройте ее график как функцию N . (e) Объясните логически, почему в этой модели доля правильных ответов уменьшается как функция N . Задача 11.4. Вернитесь к разделу 11.3, посвященному локализации цели с помощью зашумленных измерений. Выведите уравнение (11.19) шаг за шагом, начиная с уравнения (11.15). Задача 11.5. В этой главе мы рассмотрели задачи, в которых отвлекающие факторы независимы. В реальности это не обязательно так. Например, если ориентированные сегменты линий являются частью текстурированного фона, они будут, как правило, указывать в одном и том же направлении. Это важно для правила принятия решений и работы наблюдателя. Рассмот -\n--- Страница 299 ---\n298  Поиск рим крайнюю форму зависимости, а именно что все отвлекающие факторы идентичны друг другу. Однако их общее значение стимула, которое мы обо- значаем как sD, по-прежнему варьируется от испытания к испытанию в соот - ветствии с распределением pdistractor (sD). Далее предположим распределение по местоположению цели p(L), распределение стимула цели ptarget (s) и рас - пределение измерений p(xi | si). Получите выражение для апостериорного распределения расположения цели. Задача 11.6. Предположим, что N = 2, p(L = 1) = p1, отвлекающие факторы независимые, цель всегда имеет значение 0, мешающие факторы следуют нормальному распределению со средним значением 0 и дисперсией σ2 D, при- сутствует шум измерения с дисперсией σ2. Наблюдатель должен локализо- вать цель. (a) Покажите, что байесовский наблюдатель сообщает о местоположении 1, когда (11.47) (b) Предположим, что p1 = 0.6, σD = 10, σ = 2. Используя сетку по x1 и x2, чис - ленно найдите и нанесите на график психометрические кривые, которые показывают долю сообщений о «местоположении 1» в зависимости от значения стимула отвлекающего фактора. 1.0 0.8 0.6 0.4 0.2 0.0Доля сообщений о расположении 1 –30 –20 –10 0 10 20 30 Ориентация отвлекающего фактораЦель в расположении 2 Цель в расположении 1 (c) Повторите, используя выборку x1 и x2 вместо сетки. Задача 11.7. Как изменится уравнение (11.32), если в условии C = 1 вмес - то единственной цели каждый стимул в отдельности будет целью с вероят - ностью ϵ ? Задача 11.8. В этой задаче мы исследуем распределения логарифмического апостериорного отношения (LPR). Наблюдатель должен определить, при- сутствует или отсутствует целевая ориентация среди N ориентированных\n--- Страница 300 ---\nЗадачи  299 сегментов линии. Цель имеет ориентацию 0°, а каждый отвлекающий фактор имеет ориентацию 10°. В каждом испытании экспериментатор с равной ве- роятностью выбирает отсутствие или наличие цели. Когда цель присутствует, каждый стимул с равной вероятностью может быть целью. Предположим, что измерение в каждой локации искажено гауссовым шумом со стандартным отклонением 5°. (a) Предположим, что N = 2. Выполните симуляцию измерений для 5000 ис - пытаний с присутствием мишени и 5000 испытаний с отсутствием ми- шени. Для удобства и без ограничения общности вы можете считать, что когда цель присутствует, она находится в первом местоположении. (b) В каждом смоделированном испытании вычислите LPR. (c) Постройте на одном графике полученные гистограммы LPR, одну для присутствия цели и одну для отсутствия цели. Являются ли эти гисто- граммы симметричными? (d) Предположим, что наблюдатель выполняет оценку MAP . Рассчитайте долю правильных ответов. (e) Повторите вашу симуляцию для наборов размером от одного до двадца- ти. Постройте график зависимости доли правильных ответов в зависи- мости от размера набора. Задача 11.9. В этой задаче мы изучаем поиск с круговой переменной со- стояния мира. Наблюдатель определяет, присутствует ли цель, определяемая ориентацией, среди N сегментов линии. Априорная вероятность наличия цели равна 0.5. Цель всегда имеет ориентацию sT. Каждая ориентация отвле- кающего фактора извлекается независимо из равномерного распределения на [0, π). Наблюдение в i-м месте xi извлекается из распределения фон Мизеса с круговым средним si (истинная ориентация) и параметром концентрации κ: (11.48) где I0 – модифицированная функция Бесселя первого рода порядка 0. Инфор- мацию о распределении фон Мизеса см. в разделе B.7.6 приложения. (a) Покажите, что наблюдатель MAP отвечает «цель присутствует», если (11.49) (b) Смоделируйте 105 испытаний с N = 2, sT = 0 и κ = 10. В каждом испытании получите наблюдения из порождающей модели. Нужно отметить, что для извлечения выборок из распределения фон Мизеса, скорее всего, уже существует реализация на вашем любимом языке программирования. Затем вычислите LPR присутствия цели. (c) Постройте эмпирические распределения LPR, когда цель присутствует и когда она отсутствует (использование плавной кривой может быть лучше для представления, чем использование гистограмм). (d) Постройте ROC.\n--- Страница 301 ---\n300  Поиск (e) Повторите задания (c) и (d) для двух наборов разных размеров, N = 4 и N = 8. Постройте распределения LPR и ROC таким образом, чтобы вы могли легко сравнивать их по N . Объясните влияние N . Задача 11.10. Выведите LLR для набора размера N в каждом из следующих сценариев зрительного поиска. Предполагаются независимые гауссовы рас - пределения шума. (a) Цель всегда вертикальна. Отвлекающие факторы однородны, но их зна- чение выбирают в каждом испытании из равномерного распределения по ориентации. Наблюдатель сообщает, присутствует ли цель. (b) Цель всегда вертикальна. Отвлекающие факторы выбираются из гауссова распределения относительно вертикали с дисперсией σ2 D. Наблюдатель сообщает, присутствует ли цель. (c) Цель всегда вертикальна. Для каждого отвлекающего фактора незави- симо выбирается наклон на величину D влево или вправо от вертикали. Наблюдатель сообщает, присутствует ли цель. (d) Отвлекающие факторы всегда вертикальны. Цель извлекается в каждом испытании из гауссова распределения относительно вертикали с дис - персией σ2 T. Наблюдатель сообщает, присутствует ли цель. (e) Цель выбирают из симметричного распределения вокруг 0. Все отвлека- ющие факторы расположены вертикально. Наблюдатель сообщает, на- клонена цель вправо или влево от вертикали. (f) Отвлекающие факторы однородны, но их значение в каждом испытании извлекают из равномерного распределения по ориентации. Цель, если она присутствует, имеет такое значение, что разница цель–помеха со- ставляет D в каждом испытании. Наблюдатель сообщает, присутствует ли цель. (g) Цель всегда вертикальна и всегда присутствует. Отвлекающие факторы выбираются из равномерного распределения. Наблюдатель сообщает, в каком месте находилась цель. Задача 11.11. Эта задача основана на версии сценария маскировки, описан- ной в разделе 11.4, и, в частности, на уравнении (11.36) для LR присутствия рыбы относительно ее отсутствия. (a) Найдите числовое значение LR для каждой панели и сравните свои от - веты с цифрами на рис. 11.4. (b) Изменяйте b от 0.1 до 0.7 с шагом 0.1. Вместо предположения о конкрет - ных наблюдениях (сочетания рыбы и русла реки) на рис. 11.4 мы теперь моделируем для каждого значения b 10 000 наблюдений, случайно сгене- рированных с этим значением b (оставляя a = 0.7). Представьте каждое наблюдение вектором из десяти значений nL. Рассчитайте среднее зна- чение и стандартное отклонение log LR как функцию b и постройте эти значения со столбцами погрешностей, размеры которых представляют стандартные отклонения. (c) Модифицируйте вывод, ведущий к уравнению (11.36), для случая более мелкой рыбы длиной 3 единицы. Подсказка: вам нужно будет выполнить маргинализацию как по горизонтали, так и по вертикали.\n--- Страница 302 ---\nЗадачи  301 (d) Повторите задание (b) для этого нового выражения и нанесите на тот же график другим цветом. (e) Основываясь на графике в задании (d), докажите, что неоптимальная маскировка меньше влияет на видимость более мелкой рыбы. Задача 11.12. Рассмотрим более сложную задачу поиска и подбора модели. В задании Шена и Ма [167] испытуемые оценивают, наклонен ли стимул-цель влево (против часовой стрелки) или вправо (по часовой стрелке) относитель- но вертикали. Единственное отличие мишени от отвлекающих факторов заключается в том, что помехи идентичны друг другу в данном испытании, тогда как значение цели извлекается из распределения независимо. Мы вы- полним подгонку оптимальной байесовской модели, описанной в их статье. Эта модель имеет два параметра: уровень сенсорного шума σ и градиент λ. Информация о подгонке модели приводится в приложении C. (a) Для σ = 1 и λ = 0 рассчитайте, отдельно для каждого испытания, веро- ятность того, что наблюдатель сообщит «цель наклонена вправо» по- средством симуляции методом Монте-Карло. Для каждого испытания получите 1000 векторов измерений x . (b) Постройте мелкую сетку для σ и λ. Повторите задание (а) для каждой комбинации параметров в этой сетке. Сохраните результаты в трехмер- ной матрице, где одно измерение соответствует σ, одно – λ, а одно – ис - пытаниям. Загрузите файл visual_search.csv по адресу https://osf.io/84kpb/. Это дан- ные одного испытуемого в эксперименте Шена и Ма. Матрица в файле имеет 2000 строк (испытаний) и три столбца. Первый столбец – ориента- ция цели, второй – ориентация отвлекающего фактора, третий – отклик испытуемого (- 1 – влево, 1 – вправо). (c) Используйте результат задания (b) для расчета логарифмического прав- доподобия ответов испытуемого. (d) Найдите оценки максимального правдоподобия σ и λ методом поиска по сетке. (e) Используя эти оценки параметров, воспроизведите аппроксимацию мо- дели Opt на рис. 3C–D статьи Шена и Ма [167]. Кривые модели могут немного отличаться от рисунка в зависимости от выбранной вами сетки и из-за шума выборки.",
      "debug": {
        "start_page": 282,
        "end_page": 302
      }
    },
    {
      "name": "Глава 12. Вывод в меняющемся мире 302",
      "content": "--- Страница 303 --- (продолжение)\nГлава 12 Вывод в меняющемся мире Как мы оцениваем текущее состояние меняющегося мира? До сих пор в этой книге мы рассматривали только состояния мира, которые не меняются. Однако реальный мир часто меняется, и наблюдателю обычно приходится оценивать состояние мира в настоящее время или в какой-то момент в будущем на основе истории наблюдений. Краткое содержание главы Мы разработаем байесовскую модель для двух обобщенных задач с изме- няющимся состоянием мира: одно – в котором состояние мира непрерывно изменяется закономерным образом, другое – в котором изменения происхо- дят в дискретные моменты времени. В первом случае порождающая модель является примером скрытой марковской модели (hidden Markov model, HMM). 12.1. Отслеживание непрерывно меняющегося состояния мира Наблюдателям часто приходится делать выводы, в то время как мир непре- рывно (постепенно) меняется. Например, когда вы играете в футбол, вам нужно знать, где сейчас находится движущийся мяч (или, возможно, где он окажется в ближайшем будущем). Во многих видах спорта вам необходимо отслеживать товарищей по команде или противников. Когда вы пытаетесь найти выключатель в темной комнате, ваша собственная рука двигается, и вы оцениваете, где она сейчас (относительно выключателя). Когда вы пытае тесь понять произносимое предложение, его значение развивается по мере того, как вы собираете слуховую информацию. Когда животное добывает корм, скорость восстановления источника пищи будет уменьшаться по мере того,\nГлава 12 Вывод в меняющемся мире Как мы оцениваем текущее состояние меняющегося мира? До сих пор в этой книге мы рассматривали только состояния мира, которые не меняются. Однако реальный мир часто меняется, и наблюдателю обычно приходится оценивать состояние мира в настоящее время или в какой-то момент в будущем на основе истории наблюдений. Краткое содержание главы Мы разработаем байесовскую модель для двух обобщенных задач с изме- няющимся состоянием мира: одно – в котором состояние мира непрерывно изменяется закономерным образом, другое – в котором изменения происхо- дят в дискретные моменты времени. В первом случае порождающая модель является примером скрытой марковской модели (hidden Markov model, HMM). 12.1. Отслеживание непрерывно меняющегося состояния мира Наблюдателям часто приходится делать выводы, в то время как мир непре- рывно (постепенно) меняется. Например, когда вы играете в футбол, вам нужно знать, где сейчас находится движущийся мяч (или, возможно, где он окажется в ближайшем будущем). Во многих видах спорта вам необходимо отслеживать товарищей по команде или противников. Когда вы пытаетесь найти выключатель в темной комнате, ваша собственная рука двигается, и вы оцениваете, где она сейчас (относительно выключателя). Когда вы пытае тесь понять произносимое предложение, его значение развивается по мере того, как вы собираете слуховую информацию. Когда животное добывает корм, скорость восстановления источника пищи будет уменьшаться по мере того,\n--- Страница 304 ---\nОтслеживание непрерывно меняющегося состояния мира  303 как пищи становится меньше. В этих примерах мир меняется непрерывно и закономерно. Здесь мы рассмотрим, как такие изменения влияют на вывод, сделанный байесовским наблюдателем. 12.1.1. Этап 1: порождающая модель Чтобы упростить математические выкладки, дискретизируем время: пусть оно движется в единицах шагов, например в секундах. Обозначим время че- рез t; таким образом, t – целое число, и мы начнем его с t = 1. Порождающая модель показана на рис. 12.1. В верхней строке показано состояние мира, которое теперь меняется во времени (столбцы). Обозначим через st состоя - ние мира в момент времени t. Нижняя строка содержит измерения в разные моменты времени. Обозначим через xt измерение в момент времени t. Эта порождающая модель отличается от раздела 5.5, где наблюдатель производил последовательность измерений, но состояние мира не менялось. s0 s1 st–1 х1 xt–1s2 st х2 xt Рис. 12.1  Порождающая модель состояния мира, которое постоянно меняется во времени. Нижний индекс указывает на время В этой главе вместо слова «стимул» используется термин «состояние мира». Он предполагает физически представленное состояние мира, которое впо- следствии искажается внутренним шумом. По нашему опыту, обнаружение точки изменения – как в реальном мире, так и в лаборатории – чаще всего выполняется на данных, которые были искажены внешним шумом (шумом в мире). Таким образом, лежащее в основе мировое состояние никогда не представляется физически. Этот нюанс терминологии не отражается на урав- нениях. Относительно измерений мы делаем наше обычное предположение, что они независимы, учитывая состояния мира, и нормально распределены: (12.1) p(xi | si) = �(xi; si, σ2). (12.2) Дисперсия измерения σ2 не зависит от t . В верхнем ряду рис. 12.1 мы нарисовали стрелки только от состояния мира на одном временном шаге к состоянию мира на следующем временном шаге.\n--- Страница 305 ---\n304  Вывод в меняющемся мире Нет стрелок, соединяющих состояния, разделенные более чем одним времен- ным шагом. Таким образом, формально мы можем написать p(s) = p(s0)p(s1 | s0)p(s2 | s1)  p(st | st-1) (12.3) (12.4) Условная вероятность p(si | si-1) называется моделью переходов и состояний (state transition model). Она описывает динамику мира, а именно насколько мы ожидаем, что состояние в момент времени i будет иметь значение si, при условии что состояние в момент времени i - 1 имело значение si-1. Предпо- ложение о том, что распределение si зависит только от si-1, а не от si-2, si-3 и т. д., называется марковским свойством, и соответствующая порождающая модель для s1, s2, … sT (верхний ряд рис. 12.1) называется марковской моделью, или, точнее, марковским процессом первого порядка. Марковский процесс s1, s2, … sT определяется свойством p(st | s1, …, st-1) = p(st | st-1). (12.5) В некотором смысле марковский процесс не имеет памяти: ожидание то- го, что произойдет дальше, зависит только от текущего состояния, а не от того, как было достигнуто текущее состояние. Физические системы, под- чиняющиеся законам движения Ньютона, являются марковскими процесса- ми, поскольку эти законы можно сформулировать в виде дифференциальных уравнений. Например, чтобы предсказать траекторию мяча, нам нужно знать только текущее положение и вектор скорости, а не его прошлые положения или скорости. Теперь нам нужно сделать конкретные предположения о распределении. На протяжении всей этой главы мы будем рассматривать только простейшую из возможных моделей переходов и состояний: состояние увеличивается на фиксированную величину D от одного временного шага к другому: si = si-1 + D. Тем не менее мы допускаем шум. Мы предполагаем, что этот шум является гауссовым и имеет стандартное отклонение σs, так что p(si | si-1) = �(si; si-1 + D, σs2). (12.6) Это уравнение может, например, описать линейное движение вашей руки с постоянной скоростью, но с некоторой изменчивостью. Наконец, при- мем форму априорного распределения в момент времени 0 p(s0) в уравне- нии (12.4). Мы предполагаем гауссово распределение со средним значением μ0 и стандартным отклонением σ0: p(s0) = �(s0; μ0, σ02). (12.7) На этом этап 1 завершается. Поскольку состояния s1, s2, не наблюдаемы напрямую, а значит, «скрыты», определенная здесь порождающая модель называется скрытой марковской моделью (hidden Markov model, HMM).\n--- Страница 306 ---\nОтслеживание непрерывно меняющегося состояния мира  305 12.1.2. Этап 2: вывод Наблюдатель заинтересован в выводе текущего состояния st на основе вре- менного ряда измерений x1, x2, …, xt. Марковское свойство упрощает этот расчет. В частности, мы сможем записать рекурсивное соотношение для апостериорного распределения: апостериорное распределение по st при заданных x1, x2, …, xt может быть выражено через апостериорное распре- деление по st-1 при заданных x1, x2, …, xt–1. Теперь мы шаг за шагом строим логику процесса вывода, используя вспомогательные порождающие модели на рис. 12.2. Предположим сначала, что единственным состоянием мира является те- кущее состояние st и соответствующее измерение xt (рис. 12.2А). Состояние мира st подчиняется распределению p(st), которое наблюдатель использует как априорное. Тогда мы находимся в той же ситуации, что и в главе 3. Апо- стериорная вероятность st равна p(st | xt) µ p(xt | st)p(st). (12.8) Если бы было только одно состояние, нам просто нужно было бы исполь- зовать правило Байеса для вычисления апостериорного распределения. Теперь добавим к порождающей модели предыдущее состояние мира st-1 (рис. 12.2B). Его распределение равно p(st-1). Тогда априорное распределение st недоступно для наблюдателя и должно быть получено путем маргинали- зации по st-1: (12.9) st st–1 st–1 хt x1, …, xt–1st st хt xt(A) (B) (C) Рис. 12.2  Упрощенные порождающие модели в задаче отслеживания непрерывного изменения Это равенство отражает тот факт, что мнение наблюдателя о текущем со- стоянии st получено путем «экстраполяции» или «прогноза» предыдущего со- стояния st-1. Эта экстраполяция или прогноз фиксируется условным распре- делением вероятностей p(st | st-1). Если при переходе от одного временного шага к другому к состоянию добавляется шум, то маргинализация приводит к расширению распределения. В частности, если p(st-1) является нормальным распределением со средним значением μpost,t -1 и дисперсией σ2 post,t -1 и урав- нение (12.6) описывает переходы состояний, то p(st) также будет следовать\n--- Страница 307 ---\n306  Вывод в меняющемся мире нормальному распределению со средним значением μpost,t -1 + D и диспер- сией σ2 post,t -1 + σ2 s: распределение сдвинулось на D и стало шире. Подставляя уравнение (12.9) в уравнение (12.8), находим (12.10) В частном случае статического мира, когда st = st-1, имеем p(st | st-1) = δ(st - st-1), и уравнение (12.10) сводится к уравнению (12.8). В уравнении (12.10) мы предположили, что наблюдатель использует апри- орное распределение p(st-1). В действительности, однако, вместо априорного распределения в предыдущий момент времени у нас есть апостериорное рас - пределение p(st-1 | past), где past – это последовательность всех предыдущих измерений x1, x2, …, xt–1. Мы можем явно добавить их в порождающую модель (рис. 12.2C). Уравнение (12.10) изменяется путем обусловливания предыду - щими измерениями, и мы получаем (12.11) Это рекурсивная зависимость: апостериорное распределение состояния в момент времени t с учетом измерений в течение времени t выражается как функция апостериорных состояний в момент времени t - 1 с учетом из- мерений в течение времени t - 1. В частном случае статического мира, в котором st = st-1, имеем p(st | st-1) = δ(st - st-1). Тогда уравнение (12.11) принимает вид: p(st | xt) µ p(st | xt)p(st | x1, …, xt-1), (12.12) что эквивалентно накоплению свидетельств, которое обсуждалось в разде- ле 5.5, в частности уравнению (5.31). Если мы снова постулируем, что апостериорное распределение в момент времени t - 1 является нормальным со средним значением μpost,t -1 и дис - персией σ2 post,t -1, то «априорное» распределение в момент времени t является нормальным со средним μprior,t = μpost,t -1 + D (12.13) и дисперсией σ2 prior,t = σ2 post,t -1 + σ2 s. (12.14) Более того, апостериорное распределение p(st | x1, …, xt) в момент времени t также является нормальным со средним значением (12.15)\n--- Страница 308 ---\nОтслеживание непрерывно меняющегося состояния мира  307 и дисперсией (12.16) Упражнение 12.1. Выведите эти уравнения. Вам понадобятся уравнения (12.2) и (12.6). Эти уравнения показывают интересную комбинацию объединения из- мерения с априорным распределением (глава 3) и расширения этого рас - пределения, как обсуждалось выше. Эти «силы» противодействуют: первые сужают апостериорное распределение, вторые – расширяют. Мы видим это на рис. 12.3. В конце концов, дисперсия апостериорного распределения (и, следовательно, неопределенность) будет асимптотически приближаться к фиксированному значению (см. задачу 12.5). Апостериорное распределениеАприорное распределение Правдоподобиеt = 1 t = 2 t = 3Вероятность Стимул sот t = 1 до t = 2от t = 2 до t = 3 +D+D Рис. 12.3  Эволюция апостериорного распределения состояния мира, которое из- меняется с постоянной скоростью. Показаны априорное распределение (желтый), правдоподобие (красный) и апостериорное распределение (синий) на трех после- довательных временных шагах, t = 1, 2, 3. В каждый момент времени апостериорное распределение вычисляется точно так же, как в главе 3. Апостериорное распределе- ние становится априорным на следующем временном шаге через сдвиг на ∆ и рас - ширение согласно уравнениям (12.13) и (12.14). Использовались следующие значения параметров: µ0 = −5, σ0 = 5, ∆ = 4, σs = 1, σ = 1. Наблюдатель имеет измерения x1 = −0.24, x2 = 3.34, x3 = 8.36 Уравнения (12.15) и (12.16) определяют апостериорное распределение в момент времени t через апостериорное распределение в момент времени t - 1. Однако мы должны специально рассмотреть, что происходит при t = 1, поскольку при t - 1 у нас нет апостериорного распределения – только апри- орное, определяемое уравнением (12.7). Однако это априорное распределе- ние выступает в роли апостериорного: мы можем просто написать μpost,0 = μ0; (12.17) σ2 post,0 = σ02. (12.18)\n--- Страница 309 ---\n308  Вывод в меняющемся мире Эти уравнения определяют то, что можно назвать «начальным условием» задачи. В завершение этапа 2 напомним, что оценка непрерывного состояния ми- ра, которая минимизирует ожидаемую квадратичную ошибку, является апо- стериорным средним, которое в нашем случае дается уравнением (12.15). Хотя мы изучали наблюдателя, который делает вывод о текущем состоянии st, рассуждения можно распространить на предсказание будущего состояния. Это делается путем удаления текущего измерения xt из порождающей моде- ли и, соответственно, из уравнений (12.10) и (12.11). Полученные уравнения описывают вероятностный прогноз будущего состояния st на основе про- шлых измерений x1, …, xt-1. 12.2. Обнаружение точки изменения До сих пор мы рассматривали непрерывное изменение стимула. Однако иногда в состоянии мира происходят прерывистые изменения или «скачки». Например, ресторан может перейти к другому владельцу, в результате чего качество еды резко улучшится. Невролог может обнаружить признак эпилеп- тического заболевания на электроэнцефалограмме пациента, у которого нет никаких внешне видимых симптомов припадка. Или с вашим другом могло случиться событие в жизни, которое коренным образом изменило характер его взаимодействия с вами. Эти ситуации требуют иного подхода, чем посте- пенные изменения. Мы рассмотрим два случая: (а) наблюдатель знает, что из - менение произошло за наблюдаемый период времени, но должен определить, когда (этот раздел); (b) изменение могло независимо произойти в любой мо- мент времени в течение наблюдаемого периода времени (следующий раздел). 12.2.1. Единственная точка изменения Интуитивно понятно, что обнаружение точки изменения может быть затруд- нено, потому что шум в измерениях может создавать большие «кажущиеся изменения», которые не связаны с изменением основного состояния. Когда мы должны объявить, что наступила истинная точка изменения? Байесов- ский наблюдатель решает эту задачу оптимальным образом. Этап 1: порождающая модель. Порождающая модель показана на рис. 12.4. Мы предполагаем, что каждый момент времени между t = 1 и t = T имеет одинаковую вероятность быть точкой изменения: (12.19) Далее мы определяем, что значит иметь точку изменения в tchange : (12.20)\n--- Страница 310 ---\nОбнаружение точки изменения  309 где spre и spost фиксированы и предполагаются известными наблюдателю. Наконец, мы делаем обычное предположение, что измерения условно неза- висимы и нормально распределены (с не зависящей от времени дисперсией): (12.21) (12.22) s1 sT … х1 xTs2 х2tchange Рис. 12.4  Порождающая модель для обнаружения единствен- ной точки изменения. Вытянутая фигура в середине обозначает нелокальный эффект, вносимый точкой изменения Этап 2: вывод. Исследуемое состояние мира – это время точки изменения tchange . Следовательно, байесовский наблюдатель вычисляет апостериорное распределение tchange при заданной последовательности измерений x. Сна- чала мы применяем правило Байеса с однородным априорным распределе- нием: p(tchange | x) µ p(x | tchange )p(tchange ) (12.23) µ p(x | tchange ). (12.24) Затем выполняем маргинализацию по s : (12.25) Здесь сумма берется по всем последовательностям s. Всего имеется 2T таких последовательностей, но при заданном t допускается только одна, как указано в уравнении (12.20). Затем сумма сводится к одному члену, который принимает вид: (12.26)\n--- Страница 311 ---\n310  Вывод в меняющемся мире Его можно записать как (12.27) Упражнение 12.2. Выведите это уравнение. До сих пор мы не использовали распределение измерения. С его исполь- зованием апостериорное распределение tchange принимает вид (12.28) где Ds = spost - spre и s = spre + spost по аналогии с задачей различения в главе 7. Уравнение (12.28) имеет логичное объяснение: свидетельство наличия точки изменения в tchange тем весомее, чем больше измерений после tchange расположе- ны по той же стороне от среднего, что и spost. Однако весомость свидетельства уменьшается, если вернуться во времени достаточно далеко, чтобы охватить измерения, полученные от исходного spre. Чтобы получить фактические апосте- риорные вероятности, вычислите значение правой части уравнения (12.28) для всех значений tchange и разделите результат на их сумму (нормировка). Вывод завершается этапом получения результата. Поскольку tchange явля- ется дискретным, наиболее очевидной стратегией будет оценка MAP – выбор моды апостериорного распределения для максимизации точности. Оценка MAP сводится к максимизации Упражнение 12.3. Почему это так? Этап 3: распределение откликов. Здесь необходимо смоделировать рас - пределение откликов, но мы займемся этим при решении задачи 12.9. 12.2.2. Случайные точки изменения Мы изучали, как локализовать (во времени) единственную точку изменения во временном ряду. В реальном мире наблюдатель редко будет знать, что существует ровно одна точка изменения. Может быть, их ноль, а может быть, больше одной. Поскольку это открывает огромное пространство возможно- стей, было бы упущением не обсудить хотя бы один случай. Поэтому теперь мы рассмотрим сценарий, в котором точки изменения возникают случайным образом, а наблюдатель определяет их время. Этап 1: порождающая модель. Генеративная модель показана на рис. 12.5. Мы предполагаем, что каждая гипотеза Ct принимает значения 0 (нет изме- нения) и 1 (есть изменение), что все Ct независимы и что p(Ct = 1) = ϵ. Таким образом, точки изменения могут быть где угодно от 0 до T . Затем мы пред- полагаем, что каждый стимул st принимает значения -1 и 1. Если Ct = 0, st\n--- Страница 312 ---\nОбнаружение точки изменения  311 равен st-1, а если Ct = 1, st имеет знак, противоположный st-1. Предположим далее, что начальное состояние s0 = 1. Тогда каждому вектору C соответствует определенный вектор стимулов, который мы будем обозначать через sC. Мы делаем те же предположения об измерениях, что и раньше (см. уравнение (12.22)). С1 S1 S0 x1С2 S2 x2СT ST xT Рис. 12.5  Порождающая модель обнаружения случайных точек изменения Этап 2: вывод. Теперь у наблюдателя больше нет никаких ограничений на то, когда возникают точки изменения, и он должен рассмотреть все воз- можные временные ряды C = (C1, …, CT). Поскольку каждая гипотеза CT яв- ляется бинарной и все они независимы, пространство гипотез состоит из 2T бинарных векторов. Апостериорное распределение: p(C|x) µ p(x|C)p(C). (12.29) Здесь априорное распределение C нельзя считать равномерным, посколь- ку структура порождающей модели делает некоторые C более вероятными, чем другие. Выражаясь более точно, мы имеем: p(C)= ϵ||C||(1 - ϵ)T-||C||, (12.30) где ||С|| – общее количество значений 1 в C. Обозначение || · || называется нормой C. В этом случае мы используем так называемую норму L1, которая представляет собой просто сумму элементов. Упражнение. 12.4. Почему? Функция правдоподобия C равна �(C; x) = p(x | C) (12.31) (12.32) = p(x | s = sC), (12.33) где sC – это уникальный вектор состояния мира s, который соответствует вектору точки изменения C. Мы оставляем более подробное рассмотрение этой модели для задач 12.11, 12.12 и 12.13.\n--- Страница 313 ---\n312  Вывод в меняющемся мире 12.2.3. Более реалистичное обнаружение точки изменения В этом разделе мы сделали несколько упрощающих предположений. Мы предполагали, что точки изменения либо уникальны (одна на испытание), либо случайны (независимы во времени). Ни то, ни другое не реалистично. Во многих реальных задачах обнаружения изменений точки изменения воз- никают «время от времени», что говорит о наличии между двумя точками изменения некоторой априорной зависимости. Кроме того, обнаружение точки изменения обычно не может быть выполнено задним числом, т. е. на основе полного временного ряда наблюдений; вместо этого необходимо немедленно принять решение о том, произошло ли изменение. Этот режим называется онлайн­обнаружением точки изменения [4, 138, 206]. Наконец, изменение часто происходит не между двумя конкретными значениями со- стояния мира, а между двумя категориями. Ни один из этих аспектов не делает вывод концептуально отличным от примеров, обсуждаемых в этом разделе; однако каждый из них может привести к существенным техниче- ским сложностям. Ввиду таких сложностей и, в частности, с учетом того, насколько плохими они становятся при большом T, обнаружение точки изменения является об- ластью, в которой точные байесовские модели быстро становятся неправ- доподобными в качестве моделей человеческого поведения. Тем не менее байесовская модель может служить эталонной моделью или отправной точ- кой для построения более правдоподобных моделей. Мы обсудим это далее в главе 15. 12.3. Заключение В этой главе были рассмотрены методы, позволяющие делать выводы в по- стоянно меняющемся мире. Вы узнали следующее: вывод часто происходит в меняющемся мире. Чтобы справиться с этим, байесовскому наблюдателю нужна статистическая модель того, как ме- няется мир; если динамика мира характеризуется марковским процессом, то ре- зультирующие уравнения обновления поддаются толкованию. Вывод сводится к чередованию обновления наших убеждений с учетом из- менений мира во времени и обновлению их с учетом наблюдений; часто бывает невозможно явно найти среднее значение и дисперсию апостериорного распределения в каждый момент времени. Рекурсив- ные уравнения в сочетании с начальным условием – как правило, луч- шее, что мы можем сделать; мы нашли способ оптимального вывода для линейной модели пере- ходов и состояний и гауссова шума измерения. Этот способ также на- зывают фильтром Калмана;\n--- Страница 314 ---\nЗадачи  313 точка изменения – это момент, когда состояние мира внезапно меня- ется. Это происходит во многих реальных ситуациях; мы нашли способы определения точек изменения из временных рядов зашумленных наблюдений в двух случаях: когда есть только одна точка изменения во всем временном ряду или когда точки изменения воз- никают случайно. 12.4. Рекомендуемая литература Ryan Prescott Adams and David J. C. MacKay. Bayesian Online Changepoint Detection. arXiv preprint arXiv:0710.3742 (2007); J. Yu Angela. Adaptive Behavior: Humans Act as Bayesian Learners. Current Bio logy 17, no. 22 (2007): R977–R980. Kathryn Bonnen, Johannes Burge, Jacob Yates, Jonathan Pillow, and Lawrence K. Cormack. Continuous Psychophysics: Target ­Tracking to Measure Visual Sen­ sitivity. Journal of Vision 15, no. 3 (2015): 14. Daniel Goldreich and Jonathan Tong. Prediction, Postdiction, and Perceptual Length Contraction: A Bayesian Low­Speed Prior Captures the Cutaneous Rabbit and Related Illusions. Frontiers in Psychology 4 (2013): 221. Konrad P . Kording, Joshua B. Tenenbaum, and Reza Shadmehr. The Dynamics of Memory as a Consequence of Optimal Adaptation to a Changing Body. Nature Neuroscience 10, no. 6 (2007): 779–786. Elyse H. Norton, Luigi Acerbi, Wei Ji Ma, and Michael S. Landy. Human Online Adaptation to Changes in Prior Probability. PLoS Computational Biology 15, no. 7 (2019): e1006681. Kunlin Wei and Konrad P . Körding. Uncertainty of Feedback and State Estimation Determines the Speed of Motor Adaptation. Frontiers in Computational Neu- roscience 4 (2010): 11. Robert C. Wilson, Matthew R. Nassar, and Joshua I. Gold. Bayesian Online Lear ning of the Hazard Rate in Change­Point Problems. Neural Computation 22, no. 9 (2010): 2452–2476. Daniel M. Wolpert. Computational Approaches to Motor Control. Trends in Cog- nitive Sciences 1, no. 6 (1997): 209–216. 12.5. Задачи Задача 12.1. Мышцы нашего тела со временем меняются. Иногда они креп- нут, например после тренировки, иногда ослабевают. Как бы вы формализо- вали изменение мышц и вывод об их силе? Задача 12.2. Ситуации, когда какой-либо аспект мира претерпевает внезап- ные изменения, случаются во многих областях. Придумайте три реальных примера, не упомянутых в этой главе.\n--- Страница 315 ---\n314  Вывод в меняющемся мире Задача 12.3. Вспомните скрытую марковскую модель (HMM) в разделе 12.1. Выведите апостериорное распределение p(st | x1, …, xt), используя структуру порождающей модели (следуйте по стрелкам). Вам не нужно подставлять какие-либо конкретные распределения. Задача 12.4. Возьмем за основу НММ из раздела 12.1. Предположим, наблю- датель хочет сделать прогноз для будущего состояния st+1, учитывая измере- ния x1, …, xt (т. е. измерение в момент времени t + 1 еще не было выполнено). Получите среднее значение и дисперсию апостериорного распределения. Можно предположить, что среднее значение и дисперсия p(st | x1, …, xt-1) из- вестны. Задача 12.5. Временной ряд y1, …, yt называется асимптотическим, если он монотонно возрастает или убывает, но при очень большом t приближается к конечному значению. (a) Объясните логически, почему стандартное отклонение асимптотично. (b) Докажите, что при нашем рассмотрении HMM в разделе 12.1 дисперсия апостериорного распределения асимптотична в точке (12.34) Задача 12.6. В этой задаче мы реализуем HMM в разделе 12.1. Пусть μ0 = 0, σ0 = 1, σs = 1, σ = 2, D = 1 и t меняется от 1 до 30. (a) Сформируйте последовательность состояний мира s0, s1, …, st в соответ - ствии с распределениями в верхней строке порождающей модели. (b) Сформируйте последовательность соответствующих измерений x1, …, xt (нет измерений в момент времени 0). Теперь мы смоделируем наблюдателя, который использует последова- тельность измерений, которую вы только что сформировали, чтобы сде- лать вывод st на каждом временном шаге. (c) Создайте фильм, состоящий из тридцати кадров, в котором t-й кадр по- казывает: а) апостериорное распределение по st; b) вертикальную пунктирную черную линию в истинном st (из зада- ния (а)); c) вертикальную пунктирную синюю линию на измерении xt (из зада- ния (b)). Убедитесь, что шкалы осей остаются неизменными. Также пометьте оси. (d) Постройте на графике отдельно от фильма: a) истинное состояние мира как функцию времени (черная линия); b) измерение как функцию времени (синяя линия); c) апостериорное среднее как функцию времени (красная линия). (e) Нанесите на отдельный график апостериорное стандартное отклонение как функцию времени. Задача 12.7. Найдите внешний источник информации (например, в Вики- педии) о модели динамических систем, лежащей в основе фильтра Калмана.\n--- Страница 316 ---\nЗадачи  315 Это описание порождающей модели, которая включает в себя порождающую модель этой главы как частный случай. Здесь мы исследуем, как именно это происходит. (a) Соотнесите переменные в модели динамических систем с переменными или константами в этой главе. (b) Используя соответствия из задания (a), упростите предсказание и обно- вите уравнения, пока не получите два рекурсивных уравнения – (12.15) и (12.16). Имейте в виду, что наша оценка состояния является апостери- орным средним. Задача 12.8. Наблюдатель получает временной ряд ( -0.46, 0.83, -3.26, -0.14, -0.68, -2.31, 0.57, 1.34, 4.58, 3.77). Наблюдатель знает, что произошло одно изменение, и делает вывод в соответствии с моделью из раздела 12.2.1, где spre = -1, spost = 1 и σ = 1. Рассчитайте апостериорное распределение наблю- дателя на время точки изменения. Задача 12.9. В модели из раздела 12.2.1 предположим, что spre = -μ, spost = μ и σ = 1. Мы ожидаем, что по мере увеличения μ значения s до и после точки изменения становятся легче различимы. В данной задаче мы моделируем этот процесс. (a) Положим μ = 1. Произвольно выберем точку изменения. Точка измене- ния определяет последовательность состояний мира. Теперь сформируй- те соответствующую последовательность измерений. Затем примените правило принятия решения MAP к этой последовательности и запишите, был наблюдатель MAP прав или нет. (b) В задании (a) вы выполнили одну симуляцию методом Монте-Карло. Те- перь выполните 10 000 симуляций Монте-Карло для одного и того же μ. Правильно рассчитайте долю правильных ответов во всех симуляциях. (c) Повторите задания (a–b) для всех значений μ от 0 до 2 с шагом 0.1. По- стройте кривую зависимости доли правильных ответов от μ. Задача 12.10. В модели обнаружения одной точки изменения в разделе 12.2.1 мы предполагали, что spre и spost известны. Выведите решающее правило для случая, когда одно из них принимает значение s-, другое – значение s+, но наблюдатель не знает, какое из них какое. Другими словами, изменение мо- жет происходить от s- к s+ или наоборот. Задача 12.11. В этой задаче мы реализуем байесовскую модель для обна- ружения случайно возникающих точек изменения из раздела 12.2.2. Вы на- блюдаете временной ряд x = (-0.25, -1.66, -0.34, -0.41, -0.55, -1.88, -2.63, -0.79, 1.54, 0.85, 2.12, 1.22, - 0.85, - 0.61, - 1.14). (a) Используя модель с s0 = 1, ϵ = 0.3 и σ = 1, рассчитайте апостериорное распределение по вектору точки изменения C . (b) Сколько векторов C имеют апостериорную вероятность больше 1 %? (c) Какова MAP-оценка C ? (d) Нанесите на один график x как функцию времени в виде точек, соеди- ненных линиями, и оценку MAP для C в виде набора пунктирных верти- кальных линий в моменты времени изменения). Выглядит ли ваш вывод правдоподобным?\n--- Страница 317 ---\n316  Вывод в меняющемся мире Задача 12.12. Смоделируйте 1000 синтетических испытаний на основе моде- ли случайных точек изменения в разделе 12.2.2, с T = 10, s0 = 1, ϵ = 0.3 и σ = 1. (a) С какой периодичностью MAP-оценка C имеет то же количество точек изменения, что и истинное C ? (b) Повторите задание (а) для значений σ на сетке от 0.1 до 2 с шагом 0.1. Постройте график частоты совпадений как функцию σ . Задача 12.13. В модели из раздела 12.2.2 мы предполагали, что st может принимать значения -1 и 1 и менять знак при Ct = 1. Выведите решающее правило для случая, когда s получен из нормального распределения со стан- дартным отклонением σs и средним значением μt, может принимать значе- ния - 1 и 1 и меняет знак, когда Ct = 1.",
      "debug": {
        "start_page": 303,
        "end_page": 317
      }
    },
    {
      "name": "Глава 13. Сочетание вывода с полезностью 317",
      "content": "--- Страница 318 --- (продолжение)\nГлава 13 Сочетание вывода с полезностью Как мы принимаем оптимальные решения, когда речь идет о потенциальных выгодах и затратах? До сих пор мы моделировали перцептивное принятие решений как процесс, в котором наблюдатель вычисляет апостериорное распределение по инте- ресующей переменной состояния мира, а затем производит оценку, мак - симально близкую к истине. Однако в реальной жизни мы часто не прос то оцениваем состояния мира; вместо этого мы совершаем действия, имеющие последствия, которые, в свою очередь, приводят к вознаграждению или за- тратам. Краткое содержание главы Глава начинается с обсуждения того, как оптимальное решение зависит не только от нашего распределения вероятностей по состояниям мира, но и от наших предпочтений относительно исходов. Сначала мы рассмотрим, как принять оптимальное бинарное решение (например, взять ли зонтик?). Да- лее мы разберем, как выбрать одно из нескольких действий (например, где искать потерянные ключи?) или даже из континуума действий. Наконец, мы воспользуемся байесовской теорией принятия решений, чтобы вернуться к обманчиво простому процессу принятия решения о том, какое значение извлечь из апостериорного распределения. 13.1. Примеры задач Мы начнем главу с ряда примеров, когда полезность важнее вероятностей: решая, когда переходить дорогу, вы не только вычисляете апостериор- ное распределение скоростей встречных автомобилей, но также ком- бинируете эту информацию с полезностью, которую вы получаете от\nГлава 13 Сочетание вывода с полезностью Как мы принимаем оптимальные решения, когда речь идет о потенциальных выгодах и затратах? До сих пор мы моделировали перцептивное принятие решений как процесс, в котором наблюдатель вычисляет апостериорное распределение по инте- ресующей переменной состояния мира, а затем производит оценку, мак - симально близкую к истине. Однако в реальной жизни мы часто не прос то оцениваем состояния мира; вместо этого мы совершаем действия, имеющие последствия, которые, в свою очередь, приводят к вознаграждению или за- тратам. Краткое содержание главы Глава начинается с обсуждения того, как оптимальное решение зависит не только от нашего распределения вероятностей по состояниям мира, но и от наших предпочтений относительно исходов. Сначала мы рассмотрим, как принять оптимальное бинарное решение (например, взять ли зонтик?). Да- лее мы разберем, как выбрать одно из нескольких действий (например, где искать потерянные ключи?) или даже из континуума действий. Наконец, мы воспользуемся байесовской теорией принятия решений, чтобы вернуться к обманчиво простому процессу принятия решения о том, какое значение извлечь из апостериорного распределения. 13.1. Примеры задач Мы начнем главу с ряда примеров, когда полезность важнее вероятностей: решая, когда переходить дорогу, вы не только вычисляете апостериор- ное распределение скоростей встречных автомобилей, но также ком- бинируете эту информацию с полезностью, которую вы получаете от\n--- Страница 319 ---\n318  Сочетание вывода с полезностью экономии времени, и отрицательной полезностью, связанной с воз- можностью спровоцировать аварию или попасть в больницу; решая, следует ли приветствовать приближающегося к вам челове- ка, вы не только вычисляете апостериорное распределение по облику человека, но также выбираете между неловкостью от игнорирования знакомого и неловкостью от ошибочного обращения к незнакомцу (в некоторых странах это считается бестактным); когда вы решаете, стоит ли покупать туристическую страховку, вы принимаете в расчет не только апостериорную вероятность того, что что-то пойдет не так в вашей поездке, но также стоимость страховки и расходы, связанные с необходимостью платить из своего кармана, если вы не застрахованы; принимая решение о том, пить ли старое молоко, вы учитываете не толь- ко апостериорную вероятность того, что молоко испортилось, но и вре- мя, усилия и финансовые затраты на покупку нового молока, а также стоимость болезни после употребления молока, если оно испортилось; врачу часто приходится решать, следует ли назначить болезненное или дорогостоящее медицинское обследование, чтобы исключить малове- роятный, но очень серьезный диагноз; предположим, вы идете в туманную погоду по горной тропе с обрывом справа от вас. Опираясь на ограниченную визуальную информацию, вы можете с некоторой неопределенностью оценить расстояние до утеса. Затем вам нужно решить, где идти по тропе, выбирая из конти- нуума возможных направлений. Центр дорожки может быть наиболее комфортным для ходьбы. Отклонение от маршрута влево практически ничем не грозит; разве что путь станет более каменистым. Отклонение вправо, напротив, может стоить вам жизни. Поскольку затраты явно асимметричны, оптимальным решением будет сместить вашу пози- цию в ту сторону пути, которая находится дальше от обрыва. Эти примеры показывают, что вычисление апостериорного распределения вероятностей является лишь частью процесса принятия решений, в котором также учитываются затраты и вознаграждения. Теория учета затрат и воз- награждений на втором этапе процесса байесовского моделирования также называется байесовской теорией принятия решений. Примечание по терминологии. В каждой области науки оптимального управления, экономики и теории принятия решений в той или иной форме используются функции затрат. В экономике и теории принятия решений исследователи часто определяют функцию полезности (utility function). Эко- номическая наука традиционно делает акцент на полезности, которая служит мерой компромисса между затратами и выгодами: чем выше полезность, тем лучше для агента. В теории оптимального управления исследователи обычно определяют функцию затрат (cost) или потерь (loss). Это может быть связано с тем, что ученые стремятся минимизировать энергетические за- траты на выполнение движений или затраты на топливо для ракет. Однако в каждом случае есть положительные и отрицательные члены, вносящие вклад в функцию. В конечном итоге эти две формулировки эквивалентны, поскольку стоимость можно рассматривать как отрицательную полезность.\n--- Страница 320 ---\nВыбор между двумя действиями  319 13.2. Выбор между двумя действиями Предположим, что, собираясь выйти из дома, вы задаетесь вопросом, пойдет ли дождь. Сочетая беглую оценку облачного неба (визуальные данные) со знанием погодных условий в вашем районе, вы оцениваете апостериорную вероятность дождя в 30 %. Ваше апостериорное распределение вероятностей p(будет дождь | визуальные данные, фоновые знания) = 0.3 и p(дождя не бу - дет | визуальные данные, фоновые знания) = 0.7 отражает ваше мнение об интересующем состоянии мира, но не сообщает, как вы должны себя вести. Вам нужно принять решение: брать с собой зонт или нет? На первый взгляд может показаться, что, поскольку вы считаете, что ве- роятность предстоящего дождя меньше 0.5, вам следует просто оставить зонтик дома. Однако по размышлении становится ясно, что ваше решение о том, брать ли с собой зонт, будет основываться не только на вашей оценке вероятности дождя, но и на ценности, которую вы придаете различным по- следствиям, которые могут возникнуть в результате вашего выбора действия. Если вы решите не брать с собой зонт во время дождя, вы столкнетесь с неже- лательными последствиями промокания. С другой стороны, если вы решите взять с собой зонт, а дождя не будет, то вам может быть неудобно носить ненужный зонт. Результат может быть нежелательным, и в этом случае мы связываем его с затратами, или желательным, и в этом случае мы связыва- ем его с полезностью. На рис. 13.1А показаны затраты одного человека для четырех возможных исходов задачи о зонтике. Сухо и удобно затраты = 0 Сухо, но неудобно затраты = 5Мокро и холодно затраты = 90 Слегка влажно затраты = 10Не брать зонт Взять зонтДождь вероят. = 0.3Нет дождя вероят. = 0.7Состояние мираДействиеСухо и удобно затраты = 0 Сухо, но неудобно затраты = 10Мокро и холодно затраты = 40 Слегка влажно затраты = 20Не брать зонт Взять зонтДождь вероят. = 0.3Нет дождя вероят. = 0.7Состояние мираДействие(A) (B) Рис. 13.1  Два человека оценивают степень неприятности (затраты) четы- рех возможных исходов по 100­балльной шкале: (A) прогулка под дождем без зонта приводит к результату «мокро и холодно» стоимостью 90; прогулка под дождем с зонтом оставляет результат «слегка влажно» (стоимость 10) и т. д. Оп- тимальное решение для первого человека – взять с собой зонтик; (B) другой че- ловек определяет для себя другие затраты. Оптимальное действие для второго человека – оставить зонтик дома В рамках байесовской теории принятия решений оптимальное поведе- ние – это решение, которое минимизирует ожидаемые затраты, или, что эк - вивалентно, максимизирует ожидаемую полезность. Ожидаемые затраты –\n--- Страница 321 ---\n320  Сочетание вывода с полезностью это стоимость, связанная с каждым возможным результатом, умноженная на вероятность этого результата. Обратившись к рис. 13.1А, мы видим, что ожидаемая стоимость (estimated cost, EC) ношения зонта составляет: EC(есть зонт) = 0.3 · 10 + 0.7 · 5 = 6.5. (13.1) То есть если мы решим взять с собой зонт, у нас есть 30%-ная вероятность того, что мы понесем затраты в размере 10, и 70%-ная вероятность того, что мы понесем затраты в размере 5. Ожидаемую стоимость 6.5 можно рассмат - ривать как среднюю стоимость, которая получится, если мы решим брать зонт каждый день в течение многих дней с погодой, идентичной текущему дню. Напротив, ожидаемые затраты на отсутствие зонта составляют: EC(нет зонта) = 0.3 · 90 + 0.7 · 0 = 27. (13.2) Если мы решим не носить зонт, у нас есть 30%-ная вероятность того, что мы понесем затраты, равные 90, и 70%-ная вероятность того, что мы понесем затраты, равные 0. Ожидаемую стоимость 27 можно рассматривать как сред- нюю стоимость, которая может возникнуть, если мы решим не брать зонт каждый день в течение многих дней с погодой, идентичной текущему дню. Поскольку 6.5 < 27, чтобы минимизировать ожидаемую стоимость, нужно взять зонт; это оптимальное действие для данного человека. Важно отметить, что стоимость или полезность результата отражает лич- ные предпочтения; они по своей сути субъективны. В самом деле, два чело- века с одинаковым апостериорным распределением состояния мира могут выбрать противоположные направления действий из-за разных значений, которые они придают конкретным последствиям. Чтобы проиллюстриро- вать это положение, рассмотрим второго человека, который согласен с тем, что вероятность дождя составляет 30 %, но для которого стоимость исходов другая (рис. 13.1Б). Обратите внимание, что два человека согласны в своем ранжировании результатов от самых высоких до самых низких по стоимо- сти, но они назначают разные числовые затраты для результатов. Для этого второго человека оптимальное действие – оставить зонт дома. 13.3. Выбор между несколькими действиями Описанная нами процедура выбора одного из двух действий в равной сте- пени применима и к ситуациям, требующим выбора одного из нескольких действий. В качестве иллюстрации предположим, что, возвращаясь домой с пикника в парке с группой друзей, вы обнаруживаете, что ключи от дома на- ходятся не в правом кармане брюк, где вы обычно их держите. Основываясь\n--- Страница 322 ---\nВыбор между несколькими действиями  321 на знаниях о ваших недавних действиях, вы быстро создаете распределение вероятностей местонахождения потерянных ключей. Они могли выпасть в машине вашего друга; вы могли положить их в другой карман; или вы мог - ли потерять их в парке. В зависимости от того, где на самом деле находятся ваши ключи и где вы решили их искать, возможны девять результатов, и каж - дый из них имеет для вас определенную цену (рис. 13.2). Вам нужно принять решение: где искать в первую очередь? Оптимальным будет решение, связанное с минимальными ожидаемыми затратами. Вычисляя ожидаемую стоимость каждого действия, мы имеем: EC(искать в машине) = 0.1 · (-75) + 0.1 · 15 + 0.8 · 15 = 6; (13.3) EC(искать в карманах) = 0.1 · 11+ 0.1 · (-79) + 0.8 · 11 = 2; (13.4) EC(искать в парке) = 0.1 · 90 + 0.1 · 90 + 0.8 · 0 = 18. (13.5) Таким образом, несмотря на то что вы считаете наиболее вероятным, что ключи находятся в парке, оптимальным решением будет начать с проверки карманов. Искать в карманеИскать в паркеИскать в машинеДействиеНеудобство для друга, ключи найдены! затраты = –75Машина друга вероятность = 0.1 Неудобство для друга, результат разочаровывает затраты = 15Другой карман вероятность = 0.1 Неудобство для друга, друг разочарован затраты = 15Парк вероятность = 0.8 Легко сделать, результат разочаровывает затраты = 11Легко сделать, ключи найдены! затраты = –79Легко сделать, результат разочаровывает затраты = 11 Большое неудобство, результат разочаровывает затраты = 90Большое неудобство, результат разочаровывает затраты = 90Большое неудобство, ключи найдены! затраты = 0Состояние мира (расположение ключей) Затраты: – поиск ключей: –80 – легко сделать: 1 – неудобство для друга: 5 – неудачный результат – большое неудобство Рис. 13.2  Стоимость различных решений в задаче поиска ключей по шкале от –100 до 100 баллов. Затраты на каждый результат рассчитывались путем сложения затрат, связанных с каждой характеристикой результата. Если вы решите проверить машину, вам нужно будет позвонить своему другу и попросить его сделать это за вас, что доставляет неудобство ва- шему другу, и вам это не нравится (затраты 5); если вы решите поискать в траве в парке, где вы устроили пикник, вам придется совершить долгую поездку обратно в парк и, вероятно, потратить много времени на поиски там – это большое неудобство (стоимость 80). Тщательно проверить другой карман брюк и карманы пальто не составит труда (затраты 1) Безрезультатный поиск ключей будет разочаровывающим (затраты 10). Наконец, найти ключи будет очень полезно (затраты - 80)\n--- Страница 323 ---\n322  Сочетание вывода с полезностью Примечание 13.1 Пьяница и фонарный столб История о пьянице, потерявшем ключи по дороге из бара к своей машине, но решившем поискать их под фонарным столбом, представляет собой забавный случай неоптимального принятия ре- шений. С точки зрения байесовской теории при- нятия решений пьяница правильно приписал низкую стоимость (высокую ценность возна- граждения) результату (искать под фонарным столбом, найти ключи), потому что если его клю- чи там, он, скорее всего, легко и быстро наткнется на них при свете. Однако он не учел, что веро- ятность нахождения ключей в этом месте равна нулю, поскольку с самого начала он никогда не был рядом с фонарным столбом. Стремление ис - кать там, где это проще всего, также называют эффектом уличного фонаря (streetlight effect). Примечание 13.2 Байесовский поиск Пример, который мы рассматривали, относится к байесовскому поиску. Концепту - ально он похож на примеры зрительного поиска, которые мы обсуждали в главе 11, но в этих примерах мы не рассматривали функцию стоимости. Байесовский поиск успешно применялся для обнаружения ценных потерянных объектов; он обычно используется для поиска кораблей, потерянных в море. Известным примером яв- ляется гибель атомной подводной лодки ВМС США USS Scorpion, которая исчезла во время плавания в Атлантике в мае 1968 г. с девяносто девятью членами экипажа на борту. Консультации с экспертами ВМФ использовались для определения веро- ятностей различных сценариев, которые могли привести к затоплению подлодки, а затем было запущено компьютерное моделирование, чтобы определить предва- рительное распределение вероятностей местоположения подлодки на дне океана. Была построена поисковая сетка с присвоением априорной вероятности каждому квадрату на карте. В поисковых операциях каждый квадрат сетки может быть свя- зан с определенной стоимостью, частично основанной на сложности обнаружения субмарины, если бы она находилась в этом месте; морское дно различается по глубине, а в некоторых местах имеет узкие каньоны, что затрудняет поиск. Если выполнен поиск в квадрате сетки и подводная лодка не найдена, распределение вероятностей по карте может быть обновлено, а оптимальное место поиска пере- считано. Субмарина USS Scorpion была найдена в октябре 1968 г. на глубине около 3000 м примерно в 640 км к юго-западу от Азорских островов. Байесовский поиск также оказался полезным при поиске рейса 447 авиакомпании Air France, который погиб над Атлантическим океаном в 2009 г. Вы не найдете свои ключи, если будете искать только под фонарем\n--- Страница 324 ---\nМатематическое определение ожидаемой полезности  323 13.4. Математическое определение ожидаемой полезности Обозначения в этой главе s Состояние мира a Действие x Все наблюдения/измерения U(s, a) Полезность действия a, когда состояние мира равно s Рассмотрим агента, который обдумывает действия, пока состояние мира s неизвестно. В предыдущих главах действие было оценкой состояния мира s, но в этой главе это не обязательно; например, если s означает, хорошее ли молоко, тогда a может быть «пить» или «не пить». Термин «агент» использу - ется вместо «наблюдатель», чтобы подчеркнуть действие. Мы определяем функцию полезности U(s, a), которая является функцией как состояния s , так и действия a . Вознаграждение соответствует положительной полезности, стоимость – отрицательной. U обычно имеет вещественное значение. В предыдущих гла- вах, когда a = sˆ, U обычно является функцией разности между s и sˆ. Предположим теперь, что у наблюдателя есть апостериорное распределе- ние p(s | x) по s. Здесь x обозначает коллективный набор измерений или на- блюдений, сделанных наблюдателем; это не обязательно должно быть одно скалярное измерение, как в предыдущих главах. При этом апостериорном распределении ожидаемое значение полезности действия при заданном х, более кратко называемое ожидаемой полезностью a и х , равно если s дискретное; (13.6) если s непрерывное: (13.7) Сумма и интеграл находятся по s, поэтому результат не зависит от s. Прос тейшая версия байесовской теории принятия решений постулирует, что агент максимизирует ожидаемую полезность: он выбирает действие, которое – усредненное по всем возможным состояниям мира, взвешенным по их соответствующей апостериорной вероятности, – дает максимально возможную полезность: (13.8)\n--- Страница 325 ---\n324  Сочетание вывода с полезностью Примечание 13.3 Многоликость «принятия решений» В психологии и неврологии встречаются разные представления о том, что пред- ставляет собой область «принятия решений». Вот три из них: • принятие решений должно включать нетривиальную структуру затрат/возна- граждения. Специалисты, придерживающиеся этого понятия, обычно работают в области поведенческой экономики, нейроэкономики или в области, называе- мой «суждение и принятие решений», которая в значительной степени связана с когнитивными искажениями. Решения, изучаемые в этих дисциплинах, редко связаны с неопределенностью восприятия; например, субъекты выбирают между двумя играми/лотереями, представленными на экране. При таком понятии при- нятия решений текущая глава является первой, в которой затрагивается действи- тельное принятие решений; • принятие решений должно включать сбор свидетельств. Специалисты, при- держивающиеся этой точки зрения, обычно измеряют время реакции субъектов в перцептивных или когнитивных задачах: субъект может решить, когда реаги- ровать. Наиболее значимой в этой области является модель дрейфа­диффузии. Материал данной книги лишь косвенно связан с упомянутой областью, посколь- ку мы не рассматриваем парадигмы времени реакции. Точкой соприкосновения является накопление свидетельств, рассмотренное в разделе 5.5; • в этой книге мы используем всеобъемлющее понятие: принятие решений – это любой процесс, который сопоставляет стимулы с реакцией. Сюда входят реше- ния, основанные исключительно на восприятии, решения, основанные исключи- тельно на полезности, и все, что между ними. 13.4.1. Бинарная классификация Для начала мы разработаем схему ожидаемой полезности в простом случае: возьмем порождающую модель из главы 7 или 8, где класс C (0 или 1) должен быть выведен из измерения x. Наблюдатель сообщает о его предполагаемом классе. Таким образом, a = Cˆ, как в предыдущих главах, а функция полез- ности имеет вид U(C, Cˆ). Но вот в чем подвох: правильный ответ C = 0 дает полезность U(0, 0) = U0, а правильный ответ C = 1 дает полезность U(1, 1) = U1. Неправильный ответ не дает полезности: U(0, 1) = U(1, 0) = 0. Если бы U0 было равно U1, максимизация ожидаемой полезности сводилась бы к мак - симизации точности, для которой решением является MAP . Однако здесь мы допускаем, что U0 и U1 отличаются друг от друга. Интуитивно мы ожидаем, что если, например, U1 > U0, то мы ожидаем, что наблюдатель будет более склонен сообщать о классе 1. Тогда ожидаемая полезность действия «сооб- щить о классе 1» для заданного x равна (13.9) = U(C = 0, C ˆ = 0)p(C = 0 | x) + U(C = 1, C ˆ = 0)p(C = 1 | x) (13.10)\n--- Страница 326 ---\nМатематическое определение ожидаемой полезности  325 = U0p(C = 0 | x) + 0 · p (C = 1 | x) (13.11) = U0p(C = 0 | x). (13.12) Это просто вероятность дать правильный ответ, умноженная на возна- граждение за правильный ответ. Аналогично EU(C ˆ = 1) = U1p(C = 1 | x). Если агент максимизирует ожидаемую полезность, то он выберет Cˆ = 1, если ожи- даемая полезность этого выбора больше, чем альтернативы Cˆ = 0. Подставляя выражения для EU, мы находим, что это имеет место, когда (13.13) Левая часть представляет собой отношение ожидаемой полезности (expected-utility ratio) – обобщение апостериорного отношения. Теперь мы запишем апостериорное отношение как произведение априорного отноше- ния и LR: (13.14) Из этого уравнения мы узнаем, что полезности действуют как априорные вероятности: мы можем рассматривать произведение отношения полезно- сти и априорного отношения как модифицированное априорное отношение. Другими словами, когда априорное отношение меняется на определенный множитель, склонность агента к классу 1 изменяется таким же образом, как и в случае, когда на этот множитель изменяется отношение полезности. Различия в полезности могут возникать из-за побочных фак - торов, таких как разница в усилии, которое требуется для нажатия кнопок, связанных с отчетами C = 0 и C = 1. На практике это аргумент в пользу того, чтобы рассматривать априорное распределение в бинарной задаче в качест - ве свободного параметра, вместо того чтобы предполагать, что оно отражает экспериментальную статистику. 13.4.2. Непрерывная оценка В рассмотренных выше примерах и переменная состояния мира, и выбор действия могли принимать только ограниченное число дискретных значе- ний. Напротив, многие повседневные задачи подразумевают непрерывные состояния мира и/или выбор среди континуума возможных действий. В та- ких случаях мы больше не можем применять затраты индивидуально к каж - дому результату; нам нужно использовать функцию полезности в контину - уме возможных состояний мира или действий. Например, возвращаясь к нашему примеру с зонтиком, мы могли бы рас - смотреть более детальный сценарий, в котором состояние мира может при- нимать континуум значений, отражающих интенсивность осадков, от 0 (дож -\n--- Страница 327 ---\n326  Сочетание вывода с полезностью дя нет) до 10 (чрезвычайно сильный дождь). Это приводит к континууму возможных результатов, каждый из которых имеет определенную цену, даже если мы рассматриваем только два действия (брать или не брать зонтик). Если бы мы попытались создать таблицу результатов для представления этой ситуации, у нас было бы две строки (как на рис. 13.1), но бесконечное количество столбцов. Понятно, что здесь нужен другой подход. В таких случаях нам нужно построить функцию, отражающую затраты в непрерывном пространстве результатов для каждого действия. Возможно, цена пребывания под дождем для нас растет пропорционально количеству воды, попадающей на нас. Тогда функция полезности может быть выражена как U(s, a) = -(A + Ba)s, где s представляет интенсивность дождя, a представ- ляет наше действие (0 – взять и 1 – не брать зонт), а A и B – константы. Таким образом, независимо от того, есть у вас зонтик или нет, вы будете все больше недовольны увеличением количества осадков, но рост вашего недовольства, как функция количества осадков, больше, когда у вас нет зонтика. Поэтому заменим сумму интегралом: (13.15) Чтобы сделать задачу еще более реалистичной, мы могли бы рассмотреть не только континуум количества осадков, но и континуум возможных дей- ствий, отражающих не только наше решение взять или оставить зонт, но и скорость нашей ходьбы. Мы можем идти медленно или попытаться свести к минимуму время воздействия возможного дождя, добежав до пункта на- значения (или двигаясь с любой промежуточной скоростью). Если бы мы попытались создать таблицу результатов для представления этой ситуации (подобно рис. 13.1), у нас было бы бесконечное количество строк и столбцов. Опять же, чтобы справиться с этой ситуацией, нам нужно указать функцию затрат в пространстве результатов. Построение функции затрат для реальных задач принятия решений – трудная задача, хотя существует множество очевидных компонентов функ - ции стоимости. Мы хотим удовлетворить наши насущные потребности и же- лания, а также продвигаться к более отдаленным целям. Это означает, что получение еды, питья, крова и другие факторы будут иметь свою полезность. В то же время мы не хотим чрезмерно напрягаться и стараемся свести к ми- нимуму потребление энергии, риск повреждения нашего тела и другие не- гативные для нашего самочувствия факторы. Упражнение 13.1. Опишите функцию затрат, которая могла бы объяснить большую часть вашего поведения сегодня.\n--- Страница 328 ---\nФункции затрат «чистого восприятия»  327 13.5. Функции затрат «чистого восприятия» До этой главы мы не рассматривали явные затраты или вознаграждения, связанные с результатами. Вместо этого мы работали в чисто перцептивной области. Тем не менее затраты и вознаграждение присутствовали на заднем плане. А именно мы предположили, что система восприятия наблюдателя стремится генерировать решение (или восприятие), максимально близкое к истине. Эта цель принимает различные формы для дискретных и непре- рывных задач. Для дискретных (категориальных) задач естественной целью является максимизация доли правильных ответов – точности. Для задач не- прерывной оценки одной из возможных целей является минимизация ожи- даемой квадратичной ошибки оценки. Ранее мы упоминали, что эти цели приводят к MAP и оценке апостериорного среднего соответственно. Однако мы еще не доказали эти утверждения. Теперь мы можем исправить данное упущение, опираясь на предысторию функций полезности. В заключение мы остановимся на связи между восприятием и действием. 13.5.1. Дискретные задачи В предыдущих главах в дискретных задачах наблюдатель заботился о мак - симальной точности. Если состояние дискретного мира равно s, то это соот - ветствует функции полезности, равной 1 (или любому другому положитель- ному числу), когда sˆ = s, и 0 в противном случае. Мы можем записать такую функцию «правильной» полезности как (13.16) Она также называется функцией полезности 0­1 (или отрицательной функ - цией стоимости 0-1). Когда апостериорная вероятность равна p(s|x), ожида- емая полезность равна (13.17) = p(s = sˆ | x). (13.18) Чтобы максимизировать это количество, наблюдатель должен выбрать sˆ как значение s, которое максимизирует p(s | x) – другими словами, оценку MAP .\n--- Страница 329 ---\n328  Сочетание вывода с полезностью 13.5.2. Непрерывная оценка В задачах непрерывной оценки функция полезности 0-1 вряд ли применима. Например, в задаче оценки местоположения невозможно точно определить положение sˆ = s. Причем большие ошибки хуже мелких. Нам нужно отразить это в функции полезности. Один из разумных подходов состоит в постули- ровании того, что стоимость сообщения оценки sˆ, когда истинным стимулом является s, представляет собой квадрат разницы между ними, т. е. это квадра- тичная функция затрат. Тогда функция полезности принимает вид U(s, sˆ) = -(s - sˆ)2. (13.19) Когда апостериорная вероятность равна p(s|x), ожидаемая полезность равна (13.20) (13.21) Чтобы найти значение оценки, которая максимизирует EU, вычислим частную производную EU по sˆ (частная производная, потому что EU также зависит и от x): (13.22) (13.23) (13.24) = 2�[s | x] - 2sˆ · 1, (13.25) где �[s | x] – апостериорное среднее. Производная равна 0, когда sˆ = �[s | x], т. е. когда оценка равна апостериорному среднему. Таким образом, апосте- риорное среднее является оптимальным показанием с учетом квадратичной функции затрат. Мы уже упоминали об этом еще в разделе 3.3.6, но до сих пор не предоставили доказательства. Для разных функций полезности процедура максимизации ожидаемой полезности приведет к разным правилам принятия решений. Квадратичная функция затрат часто используется на практике, поскольку среднее значе- ние легко вычислить, а функция полезности часто аппроксимирует то, что волнует субъектов (близость к цели). Однако допустимы и другие функции затрат. Возможно, наиболее примечательной является функция стоимости абсолютной ошибки U(s, sˆ) = -|s - sˆ|. (13.26) Теперь максимизация ожидаемой полезности приводит к тому, что субъ- ект, принимающий решение, сообщает медиану, а не среднее значение апос - териорной вероятности (см. задачу 13.3).\n--- Страница 330 ---\nФункции затрат «чистого восприятия»  329 В контексте проблем оценки восприятия выбор функции полезности особенно важен, когда апостериорное распределение асимметрично. Для сим метричных одномодальных распределений, таких как гауссово распре- деление, среднее значение, медиана и мода идентичны. Однако для асим- мет ричных распределений они различаются (рис. 13.3). Более того, неко- торые распределения, будучи симметричными, являются бимодальными. В главе 10 при обсуждении причинного вывода мы столкнулись с бимодаль- ными (двухвершинными) апостериорными распределениями (рис. 10.4). Для таких распределений среднее значение и мода, как правило, различны, и ис - следования показали, что оценки людей ближе к среднему значению такого бимодального апостериорного распределения [98]. Мода Медиана СреднееМода: Медиана: Среднее: Рис. 13.3  Асимметричная плотность вероятностей; она может представлять собой апо- стериорное распределение вероятностей наблюдателя по местоположению источника звука в конкретном испытании. Три линии слева направо представляют моду, медиану и среднее значение распределения. Также приведены формальные определения среднего, медианы и моды апостериорного распределения 13.5.3. Восприятие и действие В разделе 3.9 мы обсуждали утверждения о том, что восприятия и перцеп- тивные решения (реакции в перцептивных задачах) не так уж отличаются и что их можно моделировать одним и тем же способом. С точки зрения байесовской теории принятия решений эти утверждения подразумевают, что функции затрат, которые активируются, – или предполагается, что они ак - тивируются, – когда человека просят принять решение о восприятии, очень похожи на те, которые разум использует при не подвергающемся сомнению восприятии. Эти функции затрат предположительно будут теми, которые обсуждаются в данном разделе, или очень похожими на них. Отказ от этого ограниченного набора функций затрат сделал бы задачу менее перцептив- ной и увеличил бы концептуальную дистанцию между восприятием и реак - цией. Мы видели минимальный пример в разделе 13.4.1, где разные типы правильных ответов вознаграждаются по-разному. Разделение можно еще больше увеличить, отделив набор доступных действий от вывода о состоянии\n--- Страница 331 ---\n330  Сочетание вывода с полезностью мира. Можно предположить, будет ли дождь, но принимать решение, брать ли с собой зонт. Можно сделать вывод, сколько времени потребуется машине, чтобы добраться до вас, но принимать решение, стоит ли переходить дорогу. В этих примерах восприятие является лишь стадией выбора действий. Тем не менее «естественные» функции стоимости восприятия могут продолжать действовать наряду с функциями затрат, используемыми при выборе дей- ствия. Это привело бы к представлению, в котором восприятия генерируются автоматически, даже если они не определяют действие. 13.6. Что значит принятие оптимального решения В байесовских моделях оптимальное принятие решений означает макси- мизацию ожидаемой полезности. Возникает вопрос, почему правильное апостериорное распределение является необходимым компонентом опти- мального решения. Почему наблюдатель не может использовать какое-либо другое распределение вероятностей состояния мира, вычисленное на основе наблюдений, скажем q(s | x)? Ситуация, в которой это может произойти, – это несоответствие модели, описанное в разделе 3.5, когда наблюдатель делает неверные предположения об одном или нескольких распределениях в по- рождающей модели. Полученное неверное апостериорное распределение может быть использовано для принятия решений. Использование q (s | x), отличного от p (s | x), обычно не позволяет максими- зировать ожидаемую полезность. Чтобы понять почему, предположим, что наблюдатель может следовать любой стратегии наблюдения x с действием a. Обозначим эту стратегию функцией F; следовательно, а = F(x). Тогда полез- ность, полученная в одном испытании, равна U(s, F(x)) и ожидаемая полез- ность для данного x равна (13.27) Оптимальная по Байесу стратегия – это функция F, которая для любого x максимизирует EU при истинном апостериорном распределении p(s | x). Те - перь рассмотрим величину, образованную усреднением EU по всем наблю- дениям x : (13.28) что можно также записать как общее среднее значение по x и s: (13.29) Таким образом, значение EUF представляет собой общую ожидаемую по- лезность – количество полезности, которое можно ожидать в долгосрочной\n--- Страница 332 ---\nУсложненные ситуации  331 перспективе, поскольку состояния мира и измерение x выбираются из по- рождающей модели. Так как оптимальная по Байесу стратегия максимизи- рует EUF(x) для каждого x, она также должна максимизировать общую ожи- даемую полезность. В частности, это означает, что построение F на основе альтернативного апостериорного распределения q(s | x) никогда не может дать более высокую общую ожидаемую полезность, чем использование ис - тинного апостериорного распределения p(s | x), полученного из порождаю- щей модели. Таким образом, правильное вычисление байесовского апосте- риорного распределения является сутью оптимального принятия решения. Упражнение 13.2. Как уравнение (13.29) следует из уравнения (13.28)? Связанный аргумент – это аргумент голландской книги1. Можно показать, что при правдоподобном наборе аксиом, если кто-то не использует надлежа- щее исчисление вероятностей, то есть байесовский вывод, в ситуации, в ко- торой присутствует неопределенность, то ему может быть предложен набор ставок (голландская книга), с которым человек согласится, но это в конечном итоге приведет к потере денег. Другими словами, небайесовские убеждения обходятся дорого. 13.7. Усложненные ситуации Структура ожидаемой полезности, изложенная в предыдущих разделах, яв- ляется упрощением. Здесь мы обсудим несколько распространенных ослож - нений. 13.7.1. Функции стоимости для неопределенных результатов В примерах, которые мы рассмотрели до сих пор, каждая комбинация со- стояния мира и действия детерминистически сопоставлялась с одним ре - зультатом, которому мы присвоили стоимость. В действительности, одна- ко, часто бывает так, что из определенного сочетания действия и состояния мира может быть получено множество результатов – с разной вероятностью. Например, на рис. 13.1 мы предположили, что если мы выйдем из дома без зонта (действие) и пойдет дождь (состояние мира), то мы промокнем (резуль - тат). В действительности, однако, из этой комбинации действия и состояния мира может возникнуть ряд возможных исходов, вероятностно зависящих от наличия мест на нашем пути, в которых мы могли бы укрыться от дождя. 1 В азартных играх голландская книга – это набор коэффициентов и ставок, установ- ленных букмекерской конторой, который гарантирует, что букмекерская контора получит прибыль за счет игроков независимо от исхода события, на которое игроки сделали ставку. – Прим. перев.\n--- Страница 333 ---\n332  Сочетание вывода с полезностью В таких ситуациях, учитывая конкретную комбинацию действия и состоя- ния мира, можно определить распределение вероятностей между исходами p(o | a, s) (рис. 13.4). Теперь каждый результат (o ) связан с полезностью, и лицо, принимающее оптимальное решение, выберет действие a, которое макси- мизирует ожидаемую полезность. Таким образом, формула оптимального действия (уравнение (13.8)) обобщается до (13.30) (13.31) Другими словами, агент усредняет функцию полезности как по возмож - ным результатам при заданном вами гипотетическом действии, так и по возможным состояниям мира. s x a o RСостояние мира Наблюдения Действие Исход Вознаграждение Рис. 13.4  Обобщенная основа байесовского принятия решений 13.7.2. Нелинейная зависимость между вознаграждением и полезностью Если экспериментатор дает в качестве вознаграждения человеку вдвое боль- ше денег или вдвое больше сока обезьяне, то это не означает, что их по- лезность удваивается. Одинаковый прирост объективного вознаграждения может привести к меньшему увеличению субъективной полезности при до-\n--- Страница 334 ---\nУсложненные ситуации  333 бавлении к большей базовой сумме. Это можно смоделировать, связав по- лезность U с вознаграждением R через степенной закон: U = Rα. (13.32) Это уравнение позволяет найти решение знаменитого петербургского па- радокса (примечание 13.4). Примечание 13.4 История теории полезности Джереми Бентам Даниэль Бернулли Габриэль Крамер Даниил Бернулли, а до него Габриэль Крамер работали над петербургским парадок - сом. Этот парадокс связан с простой азартной игрой: в каждом раунде подбрасыва- ется честная монета, и игра заканчивается, как только монета выпадает решкой (T). Затем испытуемый получает 2h долларов, где h – количество наблюдаемых орлов (H). Например, если наблюдаемая последовательность бросков монеты равна T, игрок выиграет 1 доллар; НТ – 2 доллара; ННТ – 4 доллара; HHHT – 8 долларов и т. д. Оказывается, возможный выигрыш в этой игре бесконечен. Ожидаемая полез- ность – это сумма всех возможных исходов выигрыша, умноженная на вероятность исхода. Представляя каждый возможный исход количеством орлов, мы имеем: (13.33) Парадокс заключается в том, что хотя ожидаемая полезность бесконечна, люди не будут платить больше нескольких долларов за участие в игре. Один из способов разрешить этот парадокс – ввести функцию полезности, отражающую убывающую предельную полезность денег: 1 001 000 долларов лишь немногим более ценны, чем 1 000 000 долларов, тогда как 1000 долларов гораздо ценнее, чем ничего. Джереми Бентам, другой мыслитель, рассматривавший идею полезности, приме- нил ее более непосредственно к удовольствиям и страданиям людей. Он использо- вал эти идеи, чтобы определить, как должно быть организовано общество, а именно путем максимизации полезности всех граждан – философии утилитаризма. Бентам считал, что все моральные и правовые нормы выводятся из этого простого принци- па с использованием методов логики и эксперимента.\n--- Страница 335 ---\n334  Сочетание вывода с полезностью 13.7.3. Искажения вероятности В теории перспектив, которая в основном касается экономических реше- ний, таких как взять 10 долларов или предпочесть 50%-ный шанс выиграть 25 долларов, иногда утверждается, что при расчете ожидаемой полезности вероятности результатов не учитываются линейно. Однако рассматриваемые вероятности затем обычно представляются в явном виде, как 50 % в примере выше. Неизвестно, взвешиваются ли апостериорные вероятности нелинейно. 13.7.4. Шум принятия решения Субъекты не всегда принимают одно и то же решение в одной и той же си- туации. В области восприятия такую поведенческую изменчивость иногда можно объяснить как следствие шума измерения, как мы это делали на про- тяжении всей книги. В решениях, основанных только на ценностях (без ком- понента восприятия), это невозможно. Поэтому часто для объяснения чело- веческого поведения вводится форма шума принятия решений: при заданном наборе действий a с соответствующими ожидаемыми полезностями EU( a) наблюдатель не всегда выбирает одно и то же a. Наиболее распространенный способ реализации шума принятия решений – постулировать, что вероят - ность выбора a пропорциональна степенной функции EU(a ): (13.34) Когда β становится очень большим, действие с наибольшей ожидаемой по- лезностью получает огромный импульс, так что его вероятность приближается к 1: это исходный случай агента, максимизирующего EU. Когда β = 0, наблюда- тель случайным образом выбирает действие с равной вероятностью, независи- мо от EU. При любом другом β агент делает что-то среднее между случайным выбором и максимизацией. По этой причине решающее правило также на- зывают правилом softmax, а параметр β также называют обратной темпера- турой (по аналогии с термодинамикой): чем ниже β, тем выше «температура» и тем шумнее система. Когда a может принимать только два значения, правило softmax упрощается до логистического отображения (см. задачу 13.3). Обратите внимание, что шум принятия решений по своей сути не является байесовской концепцией. Это общее, необязательное дополнение к любой модели, основанной на полезности, в том числе модели, в которых ожидае- мая полезность включает апостериорное распределение. 13.8. Применение Когда мы изучаем поведение человека или животного, мы часто хотим срав- нить его с оптимальным. Мы можем сравнить прогнозы байесовской теории\n--- Страница 336 ---\nПрименение  335 принятия решений с реальным поведением. Чтобы этот тип анализа был полезным, мы должны заранее знать, чего пытается достичь субъект, то есть какова его функция затрат. Имея это в виду, исследователи разработали экс - перименты, в которых стоимость задачи была относительно явной. Мы обсу - дим три примера исследований из разных областей: зрительное различение, целенаправленное движение и доверительные интервалы. В завершение раз- берем подходы к оценке неизвестной функции стоимости. 13.8.1. Зрительное различение Уайтли и Сахани [201] проверили модель ожидаемой полезности из раз- дела 13.4.1 в задаче зрительного различения (рис. 13.5А). Испытуемые со- общали, был ли один кратковременно представленный зрительный паттерн смещен влево или вправо по отношению к другому паттерну. Они получали положительные баллы за правильные ответы и штрафные баллы за непра- вильные. Важно отметить, что в большинстве блоков испытаний неправиль- ный ответ «влево» влек за собой другое наказание, чем неправильный ответ «вправо». В этой задаче байесовский наблюдатель должен вычислить и срав- нить, при заданном измерении x, ожидаемую полезность ответов «влево» и «вправо» в соответствии с уравнением (13.13). Это равносильно примене- нию решающего правила типа «сообщить ‘вправо’ если x > k», (13.35) x(A) (B) Рис. 13.5  Полезность, взаимодействующая с восприятием или действием: (A) схема эксперимента для проверки того, максимизирует ли различение сме- щения под шумом измерений ожидаемое вознаграждение. Субъекты фикси- ровали взгляд на X и нажимали клавишу, чтобы указать, смещен ли нижний участок влево или вправо относительно верхнего участка. Неправильные «ле- вые» и «правые» решения наказывались по­разному. Рисунок воспроизведен из [201]; (B) схема эксперимента для проверки того, максимизирует ли выпол- нение движений под шумом ожидаемое вознаграждение. Субъект делал быст­ рое указывающее движение, чтобы получить награды, связанные с внутренней частью зеленого круга, пытаясь избежать штрафов, связанных с внутренней частью красного круга. Рисунок воспроизведен из [183]\n--- Страница 337 ---\n336  Сочетание вывода с полезностью где k зависит как от уровня сенсорной неопределенности, так и от затрат и вознаграждений блока. Авторы обнаружили, что люди приближались к оп- тимальному поведению, хотя обратной связи в эксперименте было недо- статочно для изучения этой стратегии. Можно предположить, что сенсорная неопределенность вычисляется и гибко комбинируется с различными за- тратами и вознаграждениями. 13.8.2. Целенаправленные движения Троммерсхойзер и его коллеги использовали следующий эксперимент для изучения движения в условиях неопределенности (рис. 13.5В) [180, 182]. Субъектам было предложено сделать указательное движение путем касания экрана внутри маленького зеленого круга, избегая при этом внутренней части маленького красного круга. Если они попадали в зеленый круг, но не в красный, они получали 2.5 очка. Если они попадали в красный круг, но не в зеленый, они теряли 12.5 очка. Если они попадали в пересечение обоих кругов, они теряли 10 очков. Если они попадали за пределы обоих кругов, они получали 0 очков. Если они тратили слишком много времени, они так - же получали 0 очков, поэтому им приходилось действовать быстро. Важно отметить, что место, где объект коснулся экрана, было не совсем тем, куда он целился, потому что оно было искажено шумом движения; таким обра- зом, этот эксперимент содержал неопределенность исхода (раздел 13.7.1). В эксперименте есть момент, когда испытуемый должен стремиться мак - симизировать ожидаемое количество баллов, которое он заработает. Эта оптимальная точка будет зависеть от затрат и вознаграждения, а также от расстояния между кругами и шума их собственного двигательного аппарата (задача 13.8). Исследователи обнаружили, что люди учатся приближаться к оптимальному выбору, предписанному теорией принятия решений. В ана- логичных экспериментах использовались задачи на достижение определен- ного усилия [99, 102]. Эти эксперименты демонстрируют способность людей объединять полезность с неопределенностью результата. 13.8.3. Поощряемые доверительные интервалы В разделе 3.4.2 мы описали различные способы получения рейтинга досто- верности из апостериорного распределения p(s | x). Теперь мы рассмотрим метод поощрения значимых доверительных интервалов (рис. 13.6). Варианты этого метода использовались, например, для уверенных суждений о будущем местоположении [196], запоминаемом местоположении [212] и запоминае- мом цвете [76]. Идея проста: после сообщения точечной оценки sˆ (например, оценки PME или MAP) наблюдатель устанавливает симметричный интервал вокруг этой оценки (рис. 13.6A). Этот интервал действует как «ловушка»: если в него попадает истинное значение состояния мира, наблюдатель получает вознаграждение. Важно отметить, что величина вознаграждения являет - ся убывающей функцией размера интервала. Таким образом, испытуемый\n--- Страница 338 ---\nПрименение  337 может установить больший интервал, чтобы с большей вероятностью захва- тить s, но в случае успеха он получит меньшее вознаграждение. На рис. 13.6В показан один пример: R(L) = 100 e-0.5L, (13.36) где L – полудлина интервала. Могут быть использованы многие другие функ - циональные формы. В соответствии с уравнением (13.32) полезность равна U(L) = R(L)α. (13.37) В то время как полезность уменьшается с увеличением L, вероятность вообще получить вознаграждение, обозначаемая preward (L), возрастает. Мы можем рассчитать эту вероятность для байесовского наблюдателя: для гра- фика апостериорного распределения (рис. 13.6А) вероятность получить воз- награждение равна площади под распределением в интервале [s ˆ - L, sˆ + L] (рис. 13.6С). Тогда ожидаемая полезность: EU(L ; x) = U(L)preward (L) (13.38) (13.39) В этом выражении первый множитель монотонно убывает с ростом L, а вто - рой монотонно возрастает. В результате EU(L ) имеет максимум (рис. 13.6D), представляющий оптимальный компромисс между величиной и вероятно- стью вознаграждения. Оптимальная длина интервала служит мерой поощ- ряемой уверенности: если апостериорное распределение более узкое (на- блюдатель более уверен), ему следует установить меньшую длину интервала. Мы применим это соображение на практике в задаче 13.9. Вероятность –10 –L L L* 10 10 10 10 5 5 5 0 0 0 Предполагаемый стимул sПоловинный интервал LПоловинный интервал LПоловинный интервал L100 80 60 40 20 01.0 0.8 0.6 0.4 0.2 0.030 25 20 15 10 5 0Величина вознаграждения Величина вознаграждения Величина вознаграждения(A) (B) (C) (D) Рис. 13.6  Поощряемые доверительные интервалы: (A) пример апостериорного гауссова распределения по гипотетическому состоянию мира s. Мы предполагаем, что наблюдатель со- общает апостериорное среднее. Сегмент зеленой линии на оси x представляет собой интервал (симметричный относительно оценки), который наблюдатель устанавливает, чтобы «поймать» истинное значение s; (B) если истинное значение попало в интервал, наблюдатель получает вознаграждение в виде суммы, которая монотонно убывает с размером интервала; (C) веро- ятность успешного попадания s в интервал монотонно увеличивается с размером интервала; (D) ожидаемая полезность (здесь с α = 1) является произведением функций в (B) и (D) и по- казывает оптимальный размер интервала, обозначенный L∗\n--- Страница 339 ---\n338  Сочетание вывода с полезностью Отметим две возможные модификации этой модели: хотя идеальный максимизирующий полезность наблюдатель устано- вил бы полудлину интервала детерминистически, в эту модель можно добавить шум softmax, применяя уравнение (13.34) с a = L; мы описали задание sˆ и L с помощью отдельных процессов. Это под- разумевает разные цели на двух этапах решения, например минимиза- цию ожидаемой квадратичной ошибки для задания sˆ и максимизацию EU из уравнения (13.38) для задания L. Вместо этого наблюдатель мо- жет задать sˆ и L вместе, чтобы максимизировать EU. В зависимости от формы апостериорного распределения этот процесс может привести к различным предсказаниям (см. задачу 13.11). Наконец, можно рассмотреть модификацию эксперимента: вместо того чтобы просить испытуемого установить симметричный интервал вокруг sˆ, экспериментатор может позволить ему установить асимметричный интер- вал. С точки зрения структуры вознаграждения (но не обязательно с точки зрения поведения субъекта) это было бы эквивалентно полному отказу от точечной оценки и просьбе к субъекту установить только интервал. Конечно, апостериорное распределение должно быть более интересным, чем гауссово, чтобы эту модификацию стоило применять. 13.8.4. Вывод функций полезности Функции полезности – это гипотетические функции, которые люди должны оптимизировать. В разделе 13.8.2 мы предположили, что полезность равна заработанному вознаграждению, а в уравнении (13.32) мы постулировали степенную зависимость. Таким образом, мы предположили конкретную фор- му функции не более чем с одним параметром. Исследовательская практика сложнее. Во-первых, любое линейное преобразование полезностей оставляет неизменными прогнозы поведения. Во-вторых, что более важно, у исследо- вателей может не быть предварительных ограничений на форму функции. Эта проблема возникает во многих контекстах. Насколько лучше иметь 2 дол- лара по сравнению с 1 долларом? Насколько лучше использовать наши мыш- цы для сильного толчка в течение более короткого или более длительного времени? Экспериментально легко выяснить, что люди предпочитают боль- ше денег и меньше усилий, но это открытие не определяет форму функции. Существует несколько подходов к решению данной проблемы. Один из об - щих подходов заключается в использовании большого количества вариантов поведения и поиске, какая функция полезности может лучше всего описать измеренное поведение в соответствии со стратегией, называемой обратной теорией принятия решений [97, 135]. Обычно формула также содержит члены, которые смещают решения в сторону более простых выводимых функций полезности. Для движений это предполагаемые функции затрат, имеющие сильно нелинейные и нетривиальные формы [176]. Мы можем, учитывая некоторые предположения, экспериментально добиться прогресса в опре- делении того, какие функции затрат оптимизируются. Этот подход похож\n--- Страница 340 ---\nЗаключение  339 по духу на методы, которые выводят априорные распределения, использу - емые наблюдателем [168]. Он является довольно общим, но трудно сказать, для какого набора измеряемых моделей поведения он даст хорошие оценки лежащих в его основе функций полезности. Если у нас есть функция затрат, которая зависит более чем от одной пере- менной (например, затраченное время и энергия, затраченная на движение, вероятность пропустить цель или успеть), мы можем спросить испытуемых, какой из двух возможных результатов они предпочитают. Задавая множество таких вопросов, мы можем затем оценить, какие комбинации переменных имеют одинаковую полезность. Из многих подобных измерений и некоторых дополнительных предположений, таких как гладкость, можно потом вывести полную функцию полезности, используемую, например, в случае сил различ- ной силы и продолжительности [99]. Поскольку мы в первую очередь фокуси- руемся на выявлении пар результатов с одинаковой полезностью, опираясь на суждения о предпочтениях, можно выбрать сравнения, которые позво- ляют хорошо идентифицировать лежащие в их основе функции полезности. 13.9. Заключение В этой главе мы сформулировали задачу оптимального принятия решений в контексте максимизации полезности. Вы узнали, что: функция полезности (или затрат) является отправной точкой для опре- деления оптимального принятия решений. Полезность зависит от дей- ствия и состояния мира; ожидаемая полезность объединяет полезность с апостериорной веро- ятностью; когда действия соответствуют оценкам состояния мира, различия в воз- награждении за разные действия влияют на решения так же, как и апри- орные знания; правильное вычисление апостериорных распределений необходимо для максимизации общей ожидаемой полезности независимо от фор- мы функции полезности; в чисто перцептивных задачах апостериорное распределение может быть получено путем минимизации функции стоимости; понятие максимизации ожидаемой полезности можно обобщить, включив в него неопределенность результата, нелинейность между вознаграждением и полезностью, искажения вероятности и шум ре- шений; экспериментальные тесты показывают, что люди хорошо сочетают по- лезность с перцептивной или моторной неопределенностью при при- нятии решений; чтобы побудить наблюдателя установить значимые доверительные интервалы, вознаграждение можно связать с «заключением» стимула в интервал, при этом величина вознаграждения уменьшается с увели- чением размера интервала.\n--- Страница 341 ---\n340  Сочетание вывода с полезностью 13.10. Рекомендуемая литература Maija Honig, Wei Ji Ma, and Daryl Fougnie. Humans Incorporate Trial­to­ Trial Working Memory Uncertainty into Rewarded Decisions. Proceedings of the National Academy of Sciences 117, no. 15 (2020): 8391–8397. Konrad P . Kording, Izumu Fukunaga, Ian S. Howard, James N. Ingram, and Daniel M. Wolpert. A Neuroeconomics Approach to Inferring Utility Functions in Sensorimotor Control. PLoS Biology 2, no. 10 (2004): e330. Konrad P . Kording and Daniel M. Wolpert. The Loss Function of Sensorimotor Learning. Proceedings of the National Academy of Sciences 101, no. 26 (2004): 9839–9842. Laurence T. Maloney, Julia Trommer shäuser, and Michael S. Landy. Questions without Words: A Comparison between Decision Making under Risk and Movement Planning under Risk. Integrated Models of Cognitive Systems 29 (2007): 297– 313. Chris R. Sims. The Cost of Misremembering: Inferring the Loss Function in Visual Working Memory. Journal of Vision 15, no. 3 (2015). Lawrence D. Stone, Colleen M. Keller, Thomas M. Kratzke, and Johan P . Strump fer. Search Analysis for the Underwater Wreckage of Air France Flight 447. 14th International Conference on Information Fusion. IEEE (2011), 1–8. Emanuel Todorov. Optimality Principles in Sensorimotor Control. Nature Neu- roscience 7, no. 9 (2004): 907–915. Julia Trommershäuser, Sergei Gephstein, Laurence T. Maloney, Michael S. Landy, and Martin S. Banks Optimal Compensation for Changes in Task ­ Relevant Movement Variability. Journal of Neuroscience 25, no. 31 (2005): 7169–7178. Julia Trommershäuser, Laurence T. Maloney, and Michael S. Landy. “Statistical Decision Theory and the Selection of Rapid, Goal-Directed Movements”. Journal of the Optical Society of America A 20, no. 7 (2003): 1419–1433. Paul A. Warren, Erich W. Graf, Rebecca A. Champion, and Laurence T. Maloney. Visual Extrapolation under Risk: Human Observers Estimate and Compensate for Exogenous Uncertainty. Proceedings of the Royal Society B: Biological Sciences 279, no. 1736 (2012): 2171–2179. Louise Whiteley and Maneesh Sahani. Implicit Knowledge of Visual Uncertainty Guides Decisions with Asymmetric Outcomes. Journal of Vision 8, no. 3 (2008): 1–15. 13.11. Задачи Задача 13.1. Предположим, что вы идете в темноте по местности, где время от времени встречаются львы. Вы слышите подозрительный шум, который может указывать на присутствие льва. Решая, бежать или нет, вы применяе- те показанную ниже структуру затрат к каждому из четырех возможных результатов:\n--- Страница 342 ---\nЗадачи  341 (a) если вы считаете, что лев присутствует только с 30%-ной вероятностью, следует ли вам бежать или остаться с учетом указанной структуры за- трат? Предоставьте полный расчет; (b) насколько низкой должна быть ваша уверенность в присутствии льва, чтобы вы решили остаться, а не бежать? Предоставьте полный расчет; Физические усилия / нет вреда затраты = 2 Нет усилий / нет вреда затраты = 0Физические усилия / нет вреда затраты = 2 Нет усилий / некоторый вред затраты = 100Бежать СтоятьЕсть лев Нет льваСостояние мираДействие (c) значения, перечисленные в таблице выше, представляют собой упрощен- ное представление о проблеме. Чтобы сделать задачу более реалистичной, рассмотрим следующее. В действительности, если лев присутствует, он мо- жет поймать вас, даже если вы побежите; если вы останетесь, лев может (с некоторой вероятностью) решить не нападать на вас; если вы бежите, у вас есть некоторая вероятность пораниться, упав или столкнувшись с предме- тами (деревьями, валунами). Попробуйте изменить свое решение задачи, принимая во внимание эти реалистичные соображения. В своих расчетах вы можете использовать значения, которые вы считаете реалистичной вероятностью для каждого из этих непредвиденных обстоятельств. Задача 13.2. Вернемся к примеру поиска ваших ключей в разделе 13.3. Пред- положим, что, проверив свои карманы, вы не смогли найти ключ. Где бы вы искали дальше? Чтобы выяснить это, используйте правило Байеса, дабы обновить распределение вероятностей по местонахождению с учетом новых данных о том, что ваши ключи не в ваших карманах, а затем еще раз найдите действие, которое минимизирует ожидаемые затраты. Задача 13.3. Вспомните обсуждение функций затрат при непрерывном оценивании в разделе 13.5.2. Предположим, что мы используем в качестве функции стоимости абсолютную ошибку вместо квадрата ошибки: U(s, sˆ) = -|s - sˆ|. Докажите, что в таком случае ожидаемая полезность максимизиру - ется апостериорной медианой, а не апостериорным средним. Задача 13.4. Покажите математически, что если есть только два возможных действия, уравнение (13.34) для вероятности выбора одного действия сво- дится к логистической функции нормированной разницы ожидаемой полез- ности между этим действием и альтернативным действием. Задача 13.5. Рассмотрим вариант задачи из главы 7 – различение s+ и s-. Предположим, что априорное распределение плоское: p(s+) = p(s-) = 0.5. Да-\n--- Страница 343 ---\n342  Сочетание вывода с полезностью лее предположим, что наблюдатель производит измерение x, которое следует гауссову распределению со средним стимулом и стандартным отклонением σ. Теперь мы также учитываем различные суммы вознаграждения (полез- ности) для четырех комбинаций стимулов и сообщения субъекта о стимуле: Сообщение субъекта s+ s- Стимулs+ U++ U-+ s- U+- U-- (a) Выведите оптимальное решающее правило и выразите его через интег - ральное стандартное нормальное распределение Φstandard (раздел 7.1). (b) Выведите выражения для вероятности правильного ответа, когда истин- ный стимул равен s+ и когда он равен s-. Задача 13.6. Повторите решение задачи 13.5 для задачи бинарной классифи- кации непрерывного стимула из раздела 8.2. Обозначим классы как C = -1 и C = 1. Предположим, что существуют следующие (неправильные) распре- деления стимулов, обусловленные классами: (13.40) (13.41) где k – константа. Задача 13.7. Распространим раздел 13.5 на чисто перцептивные функции затрат. Рассмотрим переменную стимула s, которая принимает значения на окружности, например направление движения. Апостериорное распределе- ние вероятностей равно p (s|x). (a) Для оценки циклической переменной использование квадрата ошибки в качестве функции стоимости не имеет смысла. Как вы думаете, почему? Объясните на конкретном примере. (b) Функция разумной полезности – это косинус ошибки оценки U(s, sˆ) = cos(sˆ - s). Покажите, что оценка, которая максимизирует ожидаемую полезность в данном испытании, представляет собой круговое среднее апостериорного распределения, обозначаемое μpost, которое определя- ется уравнениями cos μpost = �[cos s | x]; (13.42) sin μpost = �[sin s | x]. (13.43) Задача 13.8. Эта задача относится к категории быстрого достижения цели в условиях дефицита времени в разделе 13.8.2. Предположим, что радиус\n--- Страница 344 ---\nЗадачи  343 окружностей равен 1, а расстояние между их центрами равно D. Также пред- положим, что шум движения имеет двумерное гауссово распределение со стандартным отклонением σ. Для простоты мы рассматриваем одномер- ную задачу: только целевые точки a, лежащие на бесконечной прямой, про- ходящей через центры двух окружностей. Определим точку между двумя центрами как начало координат. Таким образом, центры двух окружностей находятся в точках -D/2 и D/2. Используйте вознаграждения и затраты, ука- занные в разделе 13.8.2. (a) Выведите выражение для ожидаемой полезности попадания в точку a на этой прямой. (b) Выберите разумный диапазон значений для D и разумный диапазон значений для σ. Численно найдите оптимальную точку прицеливания как функцию как от D, так и от σ. Постройте этот оптимум как функцию D и σ, используя цветной график. (c) Поясните полученный график. Задача 13.9. Возьмем за основу раздел 13.8.3 о поощряемых доверительных интервалах. Предположим, что апостериорное распределение гауссово со стандартным отклонением σpost. Наблюдатель сообщает среднее значение апостериорного распределения как оценку стимула, а затем устанавливает симметричный интервал полудлины L вокруг этой оценки. Наблюдатель воз- награждается, если истинный стимул попадает в этот интервал с функцией полезности U (L) = max(0, 100 - 10L ). (a) Постройте функцию полезности. (b) Примите σpost = 2. Рассчитайте и постройте график вероятности полу - чения вознаграждения как функцию L . (c) Рассчитайте и постройте график ожидаемой полезности как функцию L. (d) Вычислите оптимальную полудлину интервала L∗. (e) Теперь задайте набор значений σpost от 0.1 до 5 с шагом 0.1. Повторите задание (d) для каждого значения в этом наборе. Постройте L∗ как функ - цию σpost. (f) Имеет ли эта зависимость смысл? Объясните свой ответ. (g) Является ли L∗ разумной мерой уверенности? Объясните свой ответ. Задача 13.10. В разделе 13.8.3 о поощряемых доверительных интервалах мы сосредоточились на ситуации, когда наблюдатель перед установкой довери- тельного интервала делает детерминированную точечную оценку, такую как апостериорное среднее. Из раздела 4.7 вы знаете, что некоторые байесовские модели постулируют стохастический вывод апостериорной вероятности. Здесь мы рассматриваем наблюдателя, который выбирает s из апостериор- ного распределения. В противном случае мы делаем те же предположения, что и в задаче 13.9: гауссово апостериорное распределение, симметричный интервал и функция полезности U (L) = max(0, 100 - 10L ). (a) Примите σpost = 2. Извлеките одно значение s из апостериорного распре- деления. Для этой выборки рассчитайте оптимальную половину длины интервала L∗. (b) Повторите задание (а) для 1000 выборок. Вычислите среднее значение L∗.\n--- Страница 345 ---\n344  Сочетание вывода с полезностью (c) Повторите задание (b) для значений σpost от 0.1 до 5 с шагом 0.1. Построй - те среднее значение L∗ как функцию σpost. (d) Если вы также решали задачу 13.9, то рассчитайте для каждого значения σpost в задании (c) отношение среднего L∗ при использовании выборки к L∗ при применении апостериорного среднего. Постройте график от - носительно σpost. (e) Если вы также решали задачу 13.9, то рассчитайте для каждого значения σpost в части (c) отношение средней реализованной полезности при ис - пользовании выборки к реализованной полезности при применении апостериорного среднего. Постройте график относительно σpost. Задача 13.11. Развейте идею поощряемых доверительных интервалов, из- ложенную в разделе 13.8.3. В конце этого раздела мы упомянули альтерна- тивную модель, в которой наблюдатель задает sˆ и L совместно, чтобы мак - симизировать EU из уравнения (13.38). Подумайте о ситуации, в которой это привело бы к реакции, совершенно отличной от модели, где sˆ является апо- стериорным средним, а значение L задано так, чтобы максимизировать EU. Задача 13.12. В разделе 4.5 было показано, почему оценка апостериорного среднего минимизирует общую среднеквадратичную ошибку MSE[s ˆ] в урав- нении (4.12); этот аргумент не затрагивал x напрямую. В разделе 13.5.2 мы обсуждали, почему оценка апостериорного среднего минимизирует ожидае- мую квадратичную ошибку при апостериорном распределении согласно уравнению (13.20); этот аргумент был обусловлен x. Какова математическая связь между двумя утверждениями? Задача 13.13. На самом деле сурок не ест древесину1 – он питается травами и насекомыми. Каждый день снова и снова ему приходится распределять свои ресурсы между различными возможными видами деятельности. Допус - тим, вы записали, что делает сурок, например где он ходит, как быстро, что и когда ест. Как бы вы построили нормативную модель того, почему сурок ведет себя так, а не иначе, и как бы вы проверили ее на данных? Задача 13.14. Исследования в области образования показали, что для наи- более успешного обучения студенты должны размышлять над тем, что они знают и насколько они уверены в своих знаниях. На основе таких суждений об уверенности могут быть разработаны новые формы тестирования и оце- нивания. Вот один из примеров [79]: Студентам задают вопрос, в ответе на который необходимо указать, аб- солютно ли они уверены, достаточно уверены или просто угадывают ответ. Баллы за их ответ зависят от правильности их ответа и уверенности, которую они выражают. Например, если студент заявляет, что он абсолютно уверен, то получает девять баллов за правильный ответ и ноль баллов за ошибочный. Однако если он говорит, что не уверен или просто угадывает, то получает три балла за правильный ответ и два балла за ошибочный. Студенты быстро усваивают, что тщательные размышления над своей уверенностью улучшают их оценку. 1 На английском языке сурок называется woodchuck (буквально – древогрыз). – Прим. перев.\n--- Страница 346 ---\nЗадачи  345 Предположим, что уверенность – это апостериорная вероятность того, что студент окажется прав. Обозначим ее через p . (a) При заданном p каково ожидаемое количество баллов, указывающих на то, что вы абсолютно уверены? А указывающих, что вы не уверены или просто угадываете? (b) При каком значении p студент, стремящийся к максимальному количест - ву баллов, должен сказать, что он абсолютно уверен? (c) Почему эта схема оценивания побуждает учащихся достоверно сообщать о своей уверенности? (d) Конкретные баллы, присвоенные шести возможным ответам, составляют схему оценивания. Укажите условия, которым должна соответствовать схема выставления оценок, чтобы стимулировать учащихся не только отвечать правильно, но и правдиво сообщать о своей уверенности.",
      "debug": {
        "start_page": 318,
        "end_page": 346
      }
    },
    {
      "name": "Глава 14. Нейронная функция правдоподобия 346",
      "content": "--- Страница 347 --- (продолжение)\nГлава 14 Нейронная функция правдоподобия Как добавить нейронную изменчивость в порождающую модель? Для описания сенсорного шума мы до сих пор использовали понятие измере- ния, которое имеет ту же размерность, что и переменная стимула. Однако, на- чиная с главы 3, мы знали, что это понятие было абстракцией. Биологическая реальность такова, что сенсорная информация поступает в виде потенциалов действия ( спайков, spike), запускаемых группами нейронов, работающих со- гласованно. Хотя для целей моделирования поведения в его обособленном виде такой абстракции достаточно, было бы упущением полностью игно- рировать биологическую основу. Байесовский подход остается прежним, но изменчивость нейронов заменяет шум измерений. Наше обсуждение вывода в этой главе будет ограничено нейронной активностью в качестве входных данных, а не нейронной реализацией самого вычисления вывода. Краткое содержание главы Вы пройдете ускоренный курс системной нейронауки и познакомитесь с концепциями кривых настройки, изменчивости Пуассона и кодирования популяции нейронов. Вместе эти понятия образуют порождающую модель нейронной активности при наличии стимула. Затем мы инвертируем эту порождающую модель, чтобы получить нейронную функцию правдоподо- бия, сначала для одного нейрона, а затем для популяции нейронов. Мы свя- жем нейронную функцию правдоподобия с более абстрактными функциями правдоподобия, использованными в предыдущих главах.\nГлава 14 Нейронная функция правдоподобия Как добавить нейронную изменчивость в порождающую модель? Для описания сенсорного шума мы до сих пор использовали понятие измере- ния, которое имеет ту же размерность, что и переменная стимула. Однако, на- чиная с главы 3, мы знали, что это понятие было абстракцией. Биологическая реальность такова, что сенсорная информация поступает в виде потенциалов действия ( спайков, spike), запускаемых группами нейронов, работающих со- гласованно. Хотя для целей моделирования поведения в его обособленном виде такой абстракции достаточно, было бы упущением полностью игно- рировать биологическую основу. Байесовский подход остается прежним, но изменчивость нейронов заменяет шум измерений. Наше обсуждение вывода в этой главе будет ограничено нейронной активностью в качестве входных данных, а не нейронной реализацией самого вычисления вывода. Краткое содержание главы Вы пройдете ускоренный курс системной нейронауки и познакомитесь с концепциями кривых настройки, изменчивости Пуассона и кодирования популяции нейронов. Вместе эти понятия образуют порождающую модель нейронной активности при наличии стимула. Затем мы инвертируем эту порождающую модель, чтобы получить нейронную функцию правдоподо- бия, сначала для одного нейрона, а затем для популяции нейронов. Мы свя- жем нейронную функцию правдоподобия с более абстрактными функциями правдоподобия, использованными в предыдущих главах.\n--- Страница 348 ---\nПорождающая модель активности одиночного нейрона  347 14.1. Порождающая модель активности одиночного нейрона В качестве первого этапа мы начнем с порождающей модели количества спайков одного нейрона, например количества спайков, вызванных вспыш- кой света, которая длится в течение нескольких миллисекунд. Мы описыва- ем количество спайков нейрона за заданный интервал времени, используя распределение Пуассона. Изменчивость Пуассона (рис. 14.1А) дает распре- деление по неотрицательным целым числам (включая ноль). Предположим, что предъявляется стимул s, и среднее количество спайков нейрона в ответ на этот стимул равно λ, которое не обязательно должно быть целым числом. Тогда фактическое количество спайков будет варьироваться от испытания к испытанию около λ. Для каждого возможного подсчета r мы ищем его ве- роятность. Процесс Пуассона (или, в нашем контексте, последовательность пуассоновских спайков) определяется следующим образом. Возьмем фикси- рованный интервал времени (например, 1 с) и разделим его на небольшие интервалы (например, по 1 мс каждый). Мы предполагаем, что каждый ин- тервал может содержать 0 спайков или 1 спайк и что возникновение спайка не зависит от того, случались ли спайки ранее и когда (по этой причине иногда говорят, что пуассоновский процесс «не имеет памяти»). Для про- цесса Пуассона с показателем λ вероятность наблюдения r спайков в одном испытании определяется распределением Пуассона (14.1) где «r !» обозначает факториал – произведение 1 · 2 · 3 ··· r. На рис. 14.1B–C показано распределение Пуассона для λ = 3.2 и λ = 9.7. Имейте в виду, что хотя r – целое число, λ может быть любым положительным числом. При до- статочно большом λ распределение близко к симметричному и выглядит примерно как гауссово; это не так для малых λ . Важным свойством распределения Пуассона является то, что и среднее значение, и дисперсия переменной, распределенной по Пуассону, равны па- раметру λ (рис. 14.1D). Отношение дисперсии к среднему числу спайков ней- рона называется фактором Фано; для пуассоновского нейрона фактор Фано равен 1. Эксперименты показали, что фактор Фано широко варьируется в за - висимости от области мозга и биологического вида, поэтому изменчивость Пуассона следует рассматривать как значительное упрощение реальности. Для нашей порождающей модели возбуждения нейронов нам нужно ука- зать вероятность количества спайков r как функцию стимула s. Для этого заметим, что λ является функцией стимула: это высота настроечной кривой (tuning curve, среднее количество спайков нейрона или степень возбужде- ния) при уровне стимула s. Следовательно, относительно стимула уравне- ние (14.1) может быть записано как\n--- Страница 349 ---\n348  Нейронная функция правдоподобия (14.2) Это распределение вероятностей иногда называют распределением ней- ронного шума. Испытание 1 Испытание 2 Испытание 3 Время(А) (В) (C) (D) 0.2 0.1 0.00.2 0.1 0.010 5 0 20 20 10 10 5 10 0 0 0Вероятность Вероятность Дисперсия выборки Количество спайков Количество спайков Среднее выборки Рис. 14.1  Пуассоновская изменчивость в отдельном нейроне: (А) гипотетические после- довательности спайков, вызванные в одном и том же нейроне одним и тем же стимулом, воздействовавшим три раза (испытания). В разных испытаниях различается не только время спайков, но и их количество; (В) распределение вероятностей количества спайков одного пу- ассоновского нейрона со средним количеством спайков λ = 3.2. Обратите внимание, что ось х начинается с 0, а не с 1. Поскольку распределение Пуассона дискретное, изображать его в ви­ де непрерывной кривой было бы ошибкой; (С) то же, но с λ = 9.7; (D) дисперсия пуассоновской случайной величины равна среднему значению. Чтобы проиллюстрировать это, мы равномер- но извлекли 100 значений параметра скорости из интервала [0, 10]. Для каждого значения мы смоделировали 100 спайков и рассчитали среднее значение и дисперсию. Фактор Фано не точно равен 1 из­за конечного размера выборок Примечание 14.1 Коды выборки Хотя традиционный взгляд системной нейронауки состоит в том, что нейронная активность для данного стимула является чисто стохастической, существуют и дру - гие взгляды. В одном из таких представлений – выборочном коде (sampling code) – вариабельность числа импульсов нейрона не является случайной, а соответствует выборкам из апостериорного распределения по базовой переменной состояния мира, такой как наличие простого признака в одном месте в поле зрения [53, 73, 78]. Эта точка зрения не противоречит подходу, представленному здесь, потому что, независимо от происхождения изменчивости, нейронная функция правдоподобия по-прежнему является допустимой концепцией для последующих вычислений.\n--- Страница 350 ---\nНейронная функция правдоподобия для одного нейрона  349 До сих пор мы не определяли стимул s. В простых моделях возбуждения нейронов s – переменная низкой размерности, такая как местоположение, контраст, ориентация или скорость объекта в зрительной области, а также интенсивность, местоположение источника или высота звука в слуховой об- ласти. Тогда настроечные кривые f(s) часто допускают относительно прос - тое параметрическое описание. Однако такой выбор s неизбежно приводит к радикальным упрощениям. Например, изображение ориентированного стимула также имеет контраст, размер, пространственную частоту и т. д. Все это мешающие переменные, которые необходимо учитывать при полном описании нейронной настроечной кривой. На протяжении эксперимента все или почти все мешающие параметры должны оставаться постоянными, чтобы типичный физиологический эксперимент «охватывал» пространство. Это не означает, что пространство стимулов не может быть богаче. На- пример, можно взять случайно меняющиеся изображения, в которых интен- сивность каждого пикселя независимо выбирается из распределения, или изображения природных сцен. Для прогнозирования реакции нейронов на «богатые» многомерные стимулы обычно требуется модель, в которой каж - дый нейрон чувствителен к небольшой части стимула и применяет линейный фильтр к этой части в качестве первого шага вычислений. В зрительной обла- сти перцепции такие модели также называются моделями, вычисляемыми по изображениям, поскольку входными данными является полное изображение. Такие модели тоже производят распределение p (r | s). В оставшейся части этой главы мы сосредоточимся на одномерных стиму - лах s, для которых p(r | s) допускает компактное математическое выражение. 14.2. Нейронная функция правдоподобия для одного нейрона Теперь обратимся к этапу 2 байесовской модели, где наблюдатель делает вывод о стимуле на основании определенного числа спайков r. При задан- ном r нейронное правдоподобие гипотетического значения стимула s – это вероятность того, что r спайков было вызвано этим значением s. Другими словами, мы копируем уравнение (14.2), но рассматриваем его как функцию от s, а не от r : (14.3) Мы рассмотрим два примера настроечной кривой: в форме колокола и мо- нотонную.\n--- Страница 351 ---\n350  Нейронная функция правдоподобия Примечание 14.2 Нейронные настроечные кривые Нейробиологи, начиная с исследований [29], давно проанализировали, какие сти- мулы активируют нейрон. Они записывали сигналы из первичной зрительной коры (V1) кошки, показывая ей подсвеченные ориентированные полосы. Они обнаружи- ли, что реакция нейронов V1 была систематически связана с ориентацией стимула (рис. 14.2А). Часто существовала одна ориентация, при которой нейрон срабаты- вал наиболее быстро: предпочтительная ориентация нейрона. Настроечная кривая нейрона представляет собой среднюю частоту срабатывания (количество спайков в секунду) в зависимости от ориентации. Многие зрительные нейроны имеют уни- модальную (одновершинную) кривую. 80 0 80 0 105 85 65 4555 35 1515 010 040 0 180 180 180 180 90 90 90 90 0 0 1.07 1.07 1.42 1.42 1.90 1.90 2.53 2.5390 180 270 3600 0 0Отклик (спайков/с) Нормированный отклик Отклик (спайков/с) Отклик (спайков/с)Ориентация (градусы) Направление дуновения (градусы) Ширина щели (мм) Ширина щели (мм)Ориентация (градусы) Ориентация (градусы) Ориентация (градусы)(A) (В) (C)37.5 гс 62.5 гс 87.5 гс37.5 гс 62.5 гс 87.5 гс 112.5 гс Рис. 14.2  Эмпирические настроечные кривые: (A) настроечные кривые для ориента- ции объекта в первичной зрительной коре макаки (V1). Пунктирная линия представляет скорость спонтанного срабатывания. Воспроизведено из [160]; (B) нормированные на- строечные кривые направления воздушного потока в четырех интернейронах церкаль- ной системы сверчка (термин «церкальный» относится к церкальным сенсиллам – при- даткам, покрытым маленькими волосками, которые реагируют на направление ветра). Воспроизведено из [127]; (C) настроечные кривые для реакции двух нейронов на ширину бороздки в тактильной решетке во вторичной соматосенсорной коре макаки (S2). Различ- ные кривые относятся к разным значениям тактильной силы прикосновения (выраженной в виде массы). Воспроизведено из [147]\n--- Страница 352 ---\nНейронная функция правдоподобия для одного нейрона  351 Настроечные кривые могут иметь самую разнообразную форму в зависимости от вида, области мозга и характеристики стимула (рис. 14.2). Например, в моторной коре мы обнаруживаем, что нейронные реакции влияют на направление движения руки обезьяны. Вместо узких одномодальных функций мы обычно находим очень широкие кривые. В слуховой коре частота звукового стимула влияет на скорость возбуждения нейрона в виде сложной настроечной кривой. А в гиппокампе, обла- сти мозга млекопитающих, отвечающей за накопление памяти и навигацию, су - ществует двумерное представление положения. Некоторые настроечные кривые имеют не колоколообразную, а монотонную форму (рис. 14.2C). Во всех этих слу - чаях преобразование сенсорных стимулов в активность нейронов характеризуется достаточно простыми кривыми. 14.2.1 Случай 1: колоколообразная настроечная кривая Предположим, что настроечная кривая нейрона имеет гауссову форму с по- ложением пика (предпочтительный стимул) spref = 0, шириной σtc = 10, базо- вой линией b = 1 и коэффициентом крутизны g = 5: (14.4) Эта кривая изображена на рис. 14.3В. Точка на кривой – это среднее коли- чество спайков нейрона в ответ на конкретный стимул. Несмотря на сходство с гауссовым распределением вероятностей, настроечная кривая никоим об- разом не является распределением вероятностей! В частности, она не нор- мируется. s rr = 0 r = 1 r = 2 r = 3 r = 4 r = 5 r = 6(A) (В) (C) 6 4 2 00.4 0.3 0.2 0.1 0.0Количество спайков Правдоподобие –50 –50 –25 0 25 50 0 50 Стимул s Предполагаемый стимул s Рис. 14.3  Вывод для одного нейрона: (А) порождающая модель; (B) идеализи- рованная настроечная кривая f(s) одного нейрона с предпочтительным стимулом 0; (C) функция правдоподобия от стимула для различных наблюдаемых импульсов в мозге с одним нейроном Предположим, нам говорят, что определенный нейрон произвел четыре спайка в заданный интервал времени, и спрашивают, что мы можем сказать\n--- Страница 353 ---\n352  Нейронная функция правдоподобия о стимуле. Основываясь на рис. 14.3B, мы могли бы сказать, что стимул был приблизительно равен -10 или +10, потому что тогда нейрон произвел бы ожидаемое количество спайков. Однако на рис. 14.3В показано только сред- нее количество спайков по многим испытаниям. Отклик от испытания к ис - пытанию шумный, что выражается уравнением (14.2). Следовательно, при силе стимула, скажем, 3.7 также могло быть произведено в общей сложности четыре спайка – так уж получилось, что в этом испытании нейрон произвел меньше спайков, чем в среднем. Четыре спайка могут даже указывать на то, что стимул был равен -21, хотя для этого потребуется, чтобы нейрон произ- вел гораздо больше спайков, чем его среднее количество спайков для данного стимула. Ясно, что некоторые значения стимула более вероятны, чем другие, и мы можем определить вероятность гипотетического значения стимула как вероятность наблюдения четырех спайков в ответ на это значение стимула. Запишем эти рассуждения в виде формулы, просто подставив уравнение для настроечной кривой (14.4) в уравнение (14.3), чтобы получить нейрон- ную функцию правдоподобия. Мы построили результирующую функцию для r = 4, а также для других значений r на рис. 14.3C. Эти своеобразно выгля- дящие функции сообщают нам, насколько вероятно каждое возможное зна- чение стимула на основе наблюдаемого количества спайков. Форма кривой для r = 4 подтверждает наше предположение: значения (приблизительно) +10 и -10 наиболее вероятны, 0 все еще вполне вероятно, а -30 очень мало- вероятно. Для вычисления функции правдоподобия мы использовали не только настроечную кривую (уравнение (14.4)), но и форму нейронной из- менчивости (уравнение (14.2)). Это позволяет нам сказать о стимуле больше, чем заключение о том, что + 10 и - 10 наиболее вероятны. Эти функции правдоподобия обладают несколькими интересными свой- ствами. Во-первых, функция правдоподобия может иметь совершенно раз- ную форму для разных наблюдений. Например, когда спайков не наблюдает - ся, любое из центральных значений стимулов очень маловероятно, поэтому функция правдоподобия имеет перевернутую U-образную форму. Во-вторых, в отличие от предыдущих глав, здесь мы не видим ничего близкого к гауссо- вой форме. На самом деле не существует настроечной кривой f(s), которую мы могли бы использовать для получения функции правдоподобия, являю- щейся точно гауссовой для каждого r. В-третьих, функции правдоподобия не нормированы; на самом деле площадь под каждой функцией бесконечна: «хвосты» простираются до сколь угодно больших значений. Упражнение 14.1. Используя уравнения (14.4) и (14.3), объясните почему. В общем случае функции правдоподобия не нормированы. В настоящей главе, когда мы обсуждаем нейронные модели, функция правдоподобия сти- мула никогда не нормируется автоматически. 14.2.2. Случай 2: монотонная настроечная кривая До сих пор мы рассматривали вещественную переменную, формирующую колоколообразную кривую. Теперь мы рассмотрим неотрицательную пере-\n--- Страница 354 ---\nНейронная функция правдоподобия для одного нейрона  353 менную с монотонной настроечной кривой, с которой мы столкнулись на рис. 14.2C. Примером может служить степенная кривая с базовой линией f(s) = asb + c, (14.5) где мы обеспечиваем a > 0 и c ³ 0, так что средние количества спайков гаран - тированно неотрицательны независимо от s. Этот тип настроечной кривой присущ переменным величинам, таким как длина, вес, контраст и громкость. Пример такой настроечной кривой показан на рис. 14.4А. Как в этом случае будет выглядеть правдоподобие? Предположим, мы ви- дим, что этот нейрон запускает четыре спайка. Правдоподобие отражает, на- сколько вероятно это наблюдение при различных гипотетических значени- ях s. Настроечная кривая говорит о том, что этот нейрон запускает в среднем четыре спайка при стимуле s = 6; следовательно, мы ожидаем, что вероят - ность вызвать ровно четыре спайка будет достаточно высокой для гипоте- тического значения стимула, равного 6. С другой стороны, если бы стимул был равен 0, то среднее число спайков нейрона было бы равно 0.5, поэтому очень маловероятно, что нейрон выдаст четыре спайка. Точно так же, если бы стимул был равен 100, то среднее число спайков в нейроне было бы 51, что снова сделало бы маловероятным наше наблюдение четырех спайков. Следовательно, мы ожидаем, что правдоподобие будет высоким для s = 6 и будет постепенно снижаться по мере удаления s от 6 в любом направле- нии. Поэтому мы ожидаем, что форма кривой нейронного правдоподобия будет колоколообразной. Эти рассуждения не являются специфическими для количества спайков, равного 4. В общем случае любое конкретное коли- чество спайков будет давать наивысшее правдоподобие одному значению стимула и более низкое правдоподобие для стимулов по обеим сторонам. Мы построи ли кривые правдоподобия на основе нескольких различных наблю- даемых значений r на рис. 14.4B. В соответствии с нашим предположением эти функции правдоподобия имеют форму колокола. r = 0 r = 1 r = 2 r = 3 r = 4 r = 5 r = 66 4 2 00.5 0.4 0.3 0.2 0.1 0.0 012345 05 10 15 20Среднее количество спайков Правдоподобие Стимул s Предполагаемый стимул s(A) (B) Рис. 14.4  (A) Пример монотонной настроечной кривой. Мы использовали урав- нение f(s) = asb + c, где b = 0.7 и c = 1; (B) соответствующие функции правдоподобия в предположении об изменчивости Пуассона и с наблюдениями r = 0, …, 6. Хотя настроечная кривая является монотонной, каждая функция правдоподобия имеет форму колокола\n--- Страница 355 ---\n354  Нейронная функция правдоподобия 14.3. Нейронная функция правдоподобия, основанная на популяции нейронов Нейронные функции правдоподобия, с которыми мы сталкивались до сих пор, были очень широкими, что указывает на высокую неопределенность. Но если мы вспомним, что эти функции правдоподобия были основаны на сра- батывании только одного нейрона и что этот нейрон был шумным (пуассо- новским), то удивительно, как много мы уже можем сказать о стимуле. Более того, у большинства из нас в мозгу имеется более одного нейрона, и поэтому информация, которой мы располагаем о раздражителях в мире, основана на одновременном возбуждении популяции нейронов. Теория популяционных нейронных функций правдоподобия была разработана многими нейробио- логами-теоретиками, в том числе [55, 83, 113, 146, 157]; мы приводим здесь только чрезвычайно упрощенную версию. Рассмотрим популяцию, состоящую из произвольного числа нейронов n. При заданном испытании нейроны в этой популяции будут давать набор спайков r1, …, rn, которые мы будем часто обозначать сокращенно вектором r и называть схемой активности популяции или просто активностью популя- ции. Математически r является многомерным вектором. Если бы 1000 ней- ронов избирательно реагировали на стимул s, тогда r был бы 1000-мерным вектором. Мы предполагаем, что вариабельность r между испытаниями не зависит от нейронов, обусловленных s : p(r|s) = p(r1, …, rn|s) (14.6) = p(r1|s)  p(rn|s) (14.7) (14.8) Порождающая модель показана на рис. 14.5. Предположение об условной независимости аналогично допущению, встречающемуся в объединении сигналов (рис. 5.4), накоплении данных (раздел 5.5) и обуче нии (рис. 6.1), но теперь нейроны заняли место сигналов или моментов времени. s r1 r2 rn Рис. 14.5  Порождающая модель активности популяции, состоящей из n нейронов, в ответ на стимул s\n--- Страница 356 ---\nНейронная функция правдоподобия, основанная на популяции нейронов  355 Теперь перейдем к выводу. Когда мы наблюдаем определенный паттерн активности популяции r , нейронная функция правдоподобия s имеет вид: (14.9) Каждый множитель в этом произведении можно рассматривать как функ - цию правдоподобия, основанную на подсчете спайков одного нейрона. Та- ким образом, функция правдоподобия популяции является произведением функций правдоподобия для одного нейрона. Это произведение соответству - ет произведению односигнальных правдоподобий в комбинации сигналов (уравнение (5.26)). Чтобы продвинуться дальше, мы предположим, что количество спайков каждого нейрона следует распределению Пуассона, при этом каждый ней- рон имеет свой собственный параметр скорости: для i-го нейрона параметр скорости задается i-й кривой настройки, оцененной при s, то есть fi(s). Тогда (14.10) Как следствие, когда мы наблюдаем определенный паттерн популяцион- ной активности r , нейронная функция правдоподобия s имеет вид: (14.11) Чтобы дополнительно оценить функцию правдоподобия популяции, нам нужно сделать предположения о кривых настройки. Рассмотрим пример, в котором все кривые настройки имеют колоколообразную форму. Чтобы смоделировать такую популяцию, мы заменим кривую настройки одного нейрона в уравнении (14.4) другой кривой настройки для каждого нейрона: (14.12) где gi, spref,i, σtc,i и bi – коэффициент усиления, предпочтительный стимул, ши- рина и базовая линия кривой настройки i-го нейрона соответственно. При- мер такой популяции показан на рис. 14.6А. Амплитуда, ширина и базовая линия сильно варьируются, как и в случае с реальными экспериментальными записями. (Однако тот факт, что каждая кривая настройки является членом одного и того же простого параметрического семейства функций, нереалис - тичен.) Мы смоделировали три паттерна активности в популяции из 100 незави- симых пуассоновских нейронов с настроечными кривыми, как на рис. 14.6А (предпочтительные стимулы с равным интервалом между -60 и 60), вы- званными стимулом s = 0. Эти паттерны будут соответствовать записям состояний нейронов в трех испытаниях, в которых был представлен стимул s = 0. Такой паттерн может выглядеть, как на рис. 14.6B.\n--- Страница 357 ---\n356  Нейронная функция правдоподобия Испытание 1 Испытание 2 Испытание 3Испытание 1 Испытание 2 Испытание 315 10 5 0 6 4 2 015 10 5 0Среднее количество спайков Правдоподобие Нормированное правдоподобиеСреднее количество спайков(А) (С) (D)(B) –40 –20 –20 –10 –10 0 0 10 10 20 20–100 –50 0 50 100 –20 0 20 40 Стимул s Стимул, предпочитаемый нейроном Предполагаемый стимул s Предполагаемый стимул sСр. кол­во спайков при s = 0 Одно испытание при s = 0 ×10–113 Рис. 14.6  Вывод основан на гетерогенной популяции, состоящей из 100 нейронов Пуас - сона с колоколообразными кривыми настройки: (A) кривые настройки; (B) пример модели действия; (C) три примера функции нейронного правдоподобия из этой популяции, все полу- ченные при s = 0. (D) Нормированные версии функций правдоподобия Согласно уравнению (14.9), функция правдоподобия s является произ- ведением вероятностей того, что нейрон 1 возбудит два спайка, нейрон 2 возбудит три спайка и т. д.: �(s; r) = p(r1 = 2 | s)p(r2 = 3 | s)  p(r31 = 0 | s). (14.13) Индивидуальные вероятности в правой части получаются из уравнения Пуассона – уравнения (14.10) с уравнением (14.12) для кривых настройки. Три результирующие функции правдоподобия представлены вместе на рис. 14.6C. Некоторые свойства этих функций правдоподобия заслуживают обсуж - дения. Во-первых, гладкая и структурированная форма функции правдо- подобия контрастирует с беспорядочной и явно бесструктурной моделью активности популяции на рис. 14.6Б. Во-вторых, масштаб по оси y очень мал, а именно порядка 10-113. Столь малое значение функций правдоподобия не является ошибкой: она отражает тот факт, что вероятность множественных событий (спайков во множественных нейронах) всегда меньше, чем вероят - ность любого из этих событий. С добавлением нового нейрона правдоподо- бие уменьшается. В-третьих, функции правдоподобия сильно различаются по высоте пика. В испытании 1 он в целом настолько низок, что едва заме- тен; высота составляет 8.2 · 10-115. На рис. 14.6D мы нормировали те же три функции правдоподобия для наглядности. Это позволяет хорошо разглядеть функцию правдоподобия испытания 1. Нормированная функция правдопо-\n--- Страница 358 ---\nУпрощенная модель  357 добия такая же, как апостериорное распределение, когда априорное распре- деление является однородным. Наконец, даже в большой популяции функ - ция правдоподобия может заметно отличаться от гауссовой. В общем случае функции правдоподобия могут быть асимметричными, мультимодальными или с плоской вершиной. При этом чем больше популяция (и чем выше уси- ление нейронов), тем ближе к гауссовой будет функция правдоподобия. Функция нейронного правдоподобия содержит всю информацию, которую можно объективно получить из активности популяции. Никакая дополни- тельная информация не может быть получена, и любая другая информация будет некорректной. Примечание 14.3 Представление о неопределенности или неопределенность, связанная с представлением? Нейронные функции правдоподобия иногда описываются как представление не- определенности. Однако термин «представление» в нейробиологии обычно отно- сится к состоянию мира, например «представление направления движения» или «представление идентичности лица». В этом смысле неопределенность представ- ляет собой странную вещь, поскольку является свойством убеждения наблюдателя, а не состоянием мира. Более точно можно было бы сказать, что нейронная функция правдоподобия отражает неопределенность, связанную с (однократным) представ- лением переменной состояния мира. В нейробиологии есть категория моделей, которые помещают неопределенность в первую очередь во внешний мир. В них неопределенность вычисляется исключи- тельно на основе сенсорного ввода, а не внутреннего представления или нейронной активности. Впоследствии исследователь стремится найти нейронные корреляты неопределенности, например, с помощью нейровизуализации. Этот подход явля- ется описательной моделью, а не моделью процесса, потому что он не определяет шаг за шагом, как обрабатывается сенсорная информация. Однако в таких описа- тельных моделях более оправданно говорить о представлении неопределенности. В этой книге мы не рассматриваем описательные модели этого типа, потому что серьезно относимся к идее о том, что во время логического вывода мозг использует порождающую модель своих внутренних представлений. 14.4. Упрощенная модель Хотя мы смогли рассчитать и построить функции правдоподобия популяции, мы до сих пор не получили никакого представления о том, как ее свойства зависят от нейронной активности. Чтобы получить такое интуитивное пред- ставление, нам нужны компактные выражения, поддающиеся математиче- ской интерпретации. Для этого мы сделаем дополнительные предположения. Модель в этом разделе является упрощенной, или, как иногда говорят физи- ки, «игрушечной моделью» (toy model): она отражает суть проблемы, но не является реалистичной сама по себе.\n--- Страница 359 ---\n358  Нейронная функция правдоподобия В классе настроечных кривых, определяемом уравнением (14.4), мы де- лаем дополнительное предположение, что этим кривые являются «гомоген- ными», т. е. преобразованными версиями друг друга, с предпочтительными стимулами, которые равномерно распределены по некоторому интервалу. Мы также предполагаем большое количество (высокую плотность) нейронов и нулевое начальное значение b = 0. Другими словами, мы используем урав- нение (14.12) с b = 0, gi = g и σtc,i = σtc: (14.14) Пример такой популяции изображен на рис. 14.7А. 3 2 1 00.3 0.2 0.1 0.010 8 6 4 2 0Среднее количество спайков Нормированное правдоподобие Ширина правдоподобия(A) (B) (С) –50 –10 0 50 100 0 0 50 10 Стимул s Предполагаемый стимул s Общее количество спайков в испытанииТочно ПриблизительноТеория Симуляция Рис. 14.7  Упрощенная модель с однородными кривыми настройки: (A) плотные трансля- ционно­инвариантные кривые настройки с гауссовой формой (ширина 10) и опорным значе- нием 0; (B) три примера нейронных функций правдоподобия с наложенным приближением постоянной суммы. Аппроксимация очень хорошая; (C) зависимость сенсорной неопределен- ности от нейронной популяции: ширина нейронной функции правдоподобия как функция общего количества спайков в одиночном испытании в популяции Из-за высокой плотности и гомогенности нейронов сумма кривых на- стройки по нейронам будет приблизительно независима от стимула: (14.15) Поскольку мы определили кривые настройки только в ограниченной обла- сти пространства, аппроксимация будет иметь место только в этой области; сумма упадет до нуля для значений s вне данной области. Возводя в степень обратного по знаку уравнения (14.15), получаем: (14.16) Если вас интересует только форма функции правдоподобия, как это обыч- но бывает (например, для получения оценки стимула или меры неопределен- ности), то постоянные множители, такие как не имеют значения. С учетом\n--- Страница 360 ---\nУпрощенная модель  359 этого факта и уравнения (14.16) правдоподобие из уравнения (14.11) упро- щается до (14.17) Другими словами, правдоподобие популяции приблизительно пропорцио- нально произведению кривых настройки нейронов, возведенных в степень соответствующих количеств спайков. Чем выше количество спайков нейро- на, тем больше степень и тем больше влияние кривой настройки нейрона на функцию правдоподобия. Это уже дает нам некоторое понимание. Наконец, мы подставляем уравнение (14.14) в уравнение (14.17) и упро- щаем, чтобы получить (14.18) Упражнение 14.2 (a) Проверьте вывод. (b) Почему предположение b = 0 было важным? Мы можем записать уравнение (14.18) в более простой форме: (14.19) где (14.20) (14.21) Упражнение 14.3. Этот результат не очевиден, и для его получения требуется несколько шагов. Запишите полный вывод. Есть веская причина записывать функцию правдоподобия в таком виде: в уравнении (14.19) мы узнаем форму (ненормализованной) функции Гаусса! Другими словами, если популяция независимых пуассоновских нейронов имеет однородные гауссовы кривые настройки и мы делаем аппроксима- цию постоянной суммы, то нейронная функция правдоподобия стимула яв- ляется приблизительно гауссовой. Это свойство дает ценную возможность использовать данный частный случай в качестве упрощенной модели. Мы построили нормализованные функции правдоподобия, полученные из этой популяции, используя точное уравнение, и наложили на приближенное вы- ражение – уравнение (14.19), на рис. 14.7B. Аппроксимация не отличима от полного выражения.\n--- Страница 361 ---\n360  Нейронная функция правдоподобия MLE стимула является μlikelihood : sˆML = μlikelihood . (14.22) Таким образом, при сделанных предположениях MLE представляет собой средневзвешенное значение предпочтительных стимулов нейронов в по- пуляции с весами, заданными количеством спайков нейронов. Его также называют декодером центра масс; эквивалент для круговых переменных на- зывается декодером вектора популяции (см. задачу 14.7). Дисперсия функции правдоподобия определяется уравнением (14.21). Согласно разделу 3.4.1, мы можем интерпретировать его квадратный ко- рень как (сенсорную) неопределенность, которую наблюдатель имеет от - носительно стимула. Во-первых, мы видим, что чем ýже кривая настройки (меньше σtc), тем ýже функция правдоподобия. Во-вторых, что еще более интересно, неопределенность наблюдателя зависит от общего числа спай- ков в популяции и, как таковая, меняется от испытания к испытанию, даже если физический стимул остается фиксированным. Чем выше общее количество спайков в популяции, тем ýже функция правдоподобия и ниже неопределенность (рис. 14.7C). Поскольку все нейроны вносят вклад в сумму в уравнении (14.21), мы должны думать о неопределенности как о свойстве совокупности, а не какого-то одного нейрона. Упражнение 14.4. Когда один нейрон запускает один импульс, уравнение (14.21) утверждает, что ширина вероятности равна ширине кривой настрой- ки. Объясните, почему это правильно и чем этот сценарий отличается от однонейронного мозга, обсуждавшегося в разделе 14.2. То, что мы смогли записать явные уравнения для MLE и ширины функции правдоподобия, стало возможным благодаря сделанным нами конкретным предположениям. Как правило, аналитические выражения для оценки мак - симального правдоподобия (MLE) и ширины функции правдоподобия су - ществуют редко. 14.5. Связь между концепциями абстрактной и нейронной моделей Ранее этой главе мы ввели порождающую модель нейронной активности, определили нейронную функцию правдоподобия и – в упрощенной моде- ли – нашли аналитические выражения для MLE стимула и ширины функции правдоподобия. Теперь мы сравним и сопоставим эти понятия с более абст - рактной моделью, которую мы представили в главе 3 и использовали на про- тяжении всей книги (табл. 14.1). Абстрактная модель была упрощена по сравнению с нейронной моделью несколькими способами: во-первых, функция правдоподобия всегда была гауссовой, а в нейронной модели – нет. Во-вторых, MLE была идентична на- блюдению, что стало возможным благодаря определению наблюдения как\n--- Страница 362 ---\nСвязь между концепциями абстрактной и нейронной моделей  361 измерения, расположенного в том же пространстве, что и стимул; в нейрон- ной модели наблюдение располагается в совершенно другом пространстве (пространстве n-мерных векторов неотрицательных целых чисел), чем сти- мул, а значит, и MLE. В-третьих, в абстрактной модели ширина правдоподо- бия была одинаковой от испытания к испытанию; в нейронной модели она меняется от испытания к испытанию, поскольку зависит от наблюдения. Таблица 14.1. Сравнение концепций абстрактной и нейронной моделей в байесовском моделировании задачи непрерывной оценки. Упрощенная модель рассматривается в разделе 14.4 Концепция Абстрактная модель Нейронная модель Наблюдение Скалярное измерение x. Возможные значения: вещественные числаВектор подсчетов спайков r = (r1, …, rn). Возможные значения: неотрица- тельные целые числа Его распределение p(x|s), обычно гауссово со средним s и стандартным отклонением σp(r|s) (n-мерное). В упрощенной модели: независимое пуассоновское с гауссовой кривой настройки Правдоподобие s �(s; x). Гауссово, если распределение шума гауссово�(s; r). В упрощенной модели – гауссово MLE s x В упрощенной модели: Ширина функции правдоподобияσ (фиксированная) Зависит от действий (переменная). В упрощенной модели: Распределение MLE Гауссово со средним s и стандартным откло- нением σ (такое же, как распределение шума)Аналитической формы нет. Приближается к гауссову с увеличением количества спайков Теперь нам более очевидно, что понятие измерения было абстракцией. Мозг не использует скалярные измерения в качестве входных данных для своего вывода; у него есть только нервные потенциалы действия. На самом деле мы могли бы определить измерение на основе нейронной модели (упро- щенной или нет): измерение представляет собой MLE стимула, основанно- го на нейронном наблюдении, а именно так называемого популяционного паттерна активности r. С этой точки зрения измерение x стимула s является значением s, при котором наблюдаемая нервная активность r наиболее ве- роятна. Мы можем рассматривать измерение x как «обработанную форму» нейронной активности r. Например, если стимул представляет собой ори- ентацию линии, а нейронное наблюдение представляет собой активность популяции нейронов V1, то измерение будет наилучшим предположением об ориентации, полученным из этого паттерна. Сохранится ли это соответствие статистически во многих испытаниях? В абстрактной модели для данного s измерение имеет гауссово распределе- ние со средним значением s. Но верно ли то же самое для нейронной моде- ли? Даже в упрощенной нейронной модели MLE стимула является сложной\n--- Страница 363 ---\n362  Нейронная функция правдоподобия функцией r, и ее распределение по многим испытаниям для заданного s не имеет выражения в аналитической форме. Однако в пределе, когда усиление нейрона (и, следовательно, среднее количество всплесков) очень велико, рас - пределение MLE становится неотличимым от гауссова. Это свойство известно как асимптотическая нормальность. Предел асимптоты – это такое значение MLE, при котором количество информации в популяции велико, потому что достаточное количество нейронов ответило достаточным количеством спай- ков. При тех же условиях ожидаемое значение MLE равно самому стимулу. Другими словами, MLE асимптотически несмещенная. Эти свойства дают некоторое оправдание нашему предполагаемому распределению измерений в абстрактной модели, но важно помнить, что по мере удаления от асимпто- тического режима это предполагаемое распределение становится все более грубым приближением. 14.6. Использование нейронной функции правдоподобия для вычислений До сих пор в этой главе мы определяли нейронную генеративную модель, которая соответствует этапу 1 процесса байесовского моделирования, опи- санного в главе 3. Мы описали часть этапа 2, а именно функцию правдо- подобия стимула, основанную на активности в типичной нейросенсорной популяции. Из предыдущих глав, в которых сенсорный шум играл значи- мую роль, мы знаем, что эту функцию правдоподобия можно использовать множеством способов: в сочетании с априорной информацией (глава 3), в сочетании с другими функциями правдоподобия (глава 5) или для выво- да категориальной переменной более высокого уровня (например, главы 7, 8, 10, 11). Во всех этих случаях каждая функция правдоподобия стимула является элементарным составляющим элементом, который используется для построения апостериорного распределения интересующего состояния мира. Аналогичная ситуация возникает и с нейронным правдоподобием. Если раньше мы использовали функцию правдоподобия стимула �(s; x) = p(x | s), то теперь мы можем заменить ее нейронной функцией правдоподобия, �(s; r) = p(r | s), а все остальное происходит как прежде. В качестве примера рассмотрим комбинацию нейронной функции прав- доподобия с гауссовым априорным распределением p (s): p(s) = �(s; μ, σs2). (14.23) Апостериорное распределение по s равно p(s | r) µ p(r | s)p(s). (14.24)\n--- Страница 364 ---\nИспользование нейронной функции правдоподобия для вычислений  363 В упрощенной модели из раздела 14.4 (и только в ней) вероятность про- порциональна гауссову распределению со средним значением, определяе- мым уравнением (14.20), и стандартным отклонением, определяемым урав- нением (14.21). Следовательно, мы в точности возвращаемся к примеру из главы 3, с той лишь разницей, что вместо x и σ2 подставляем выражения из табл. 14.1. Мы можем взять уравнение (3.24) для оценки апостериорного среднего из главы 3 и выполнить эти подстановки, получив (14.25) Это уравнение дает ответ на вопрос: если сенсорная входная активность равна r, а стимул извлекается из распределения p(s), как мозг получает наилучшую возможную оценку стимула? Другими словами, это нейронное байесовское отображение стимул–реакция. Задачи вывода в других главах, связанные с зашумленным измерением x, можно решить с помощью той же подстановки. Они не вносят нового понимания, поэтому мы не будем вдаваться в них. Примечание 14.4 Искусственные нейронные сети для изучения функций правдоподобия В этой главе мы начали с идеализированной и математически красивой порожда- ющей модели, которую затем использовали для получения функции правдоподо- бия. Однако реальность сложнее, и на практике может быть трудно сформулиро- вать хорошую функцию правдоподобия. В таких случаях для получения функции правдоподобия иногда имеет смысл использовать глубокое обуче ние. Мы начнем с обучающих данных (s, r), где s – стимул, а r – активность популяции. В одном из подходов мы могли бы предположить, что апостериорное распределение s является гауссовым со средним значением и дисперсией, зависящими от наблюдения. Затем мы могли бы оценить среднее значение с помощью одной сетевой функции fθ(r) и стандартное отклонение с помощью другой сетевой функции gθ(r). В качестве аль- тернативы мы могли бы выделить стимул и обучить классификационную сеть; это дало бы дискретизированное апостериорное распределение стимула [193]. В обоих подходах мы преобразовали бы апостериорную вероятность в правдоподобие, раз- делив экспериментально известное априорное распределение. Как бы то ни было, правдоподобие, найденное с помощью нейронной сети, может быть легко включе- но в дальнейшие байесовские вычисления. Такие подходы могут стать ключом к обобщению аналитически объяснимых байе- совских моделей на поведение в реальном мире. Можно представить себе функции правдоподобия, в которых релевантными входными данными являются сложные стимулы, воздействующие на людей в их повседневной жизни, а оценки относятся к сложным выборам. Однако такие подходы потеряли бы большую часть своей объ- яснительной силы по сравнению с относительно простыми моделями, на которых мы сосредоточились здесь.\n--- Страница 365 ---\n364  Нейронная функция правдоподобия 14.7. Нейронная реализация байесовских вычислений Одно дело – записать формальное выражение для апостериорного распре- деления интересующей величины, и совсем другое – указать нейронные операции, которые могут реализовать вычисление этого апостериорного распределения. К решению этой задачи можно подойти разными путями. Один из способов – создать такие операции вручную и продемонстрировать с помощью математических выкладок и моделирования, что этих операций достаточно. Этот подход, примером которого являются линейные вероят- ностные коды популяции (linear probabilistic population codes) [113], теоре - тически элегантен, но допущения, необходимые для математических рас - четов, накладывают серьезные ограничения на задачу и форму нейронной изменчивости. (Нейронная функция правдоподобия представляет собой лишь самый основной компонент этой теории.) Второй метод заключается в создании искусственной нейронной сети, которую обучают отображению, состоящему из простых структурных элементов [120, 140]. Этот подход го- раздо более гибкий, но может привести к построению сети, которую трудно интерпретировать. Помимо этих двух подходов, существует множество дру - гих сред для нейронной реализации байесовских вычислений (например, [42, 89, 107, 151, 162]). Они выходят за рамки этой книги. 14.8. Применение байесовских вычислений Хотя основное внимание в нашей книге уделяется тому, как мозг делает вы- воды на основе зашумленной сенсорной информации, существуют публи- кации о том, как экспериментатор может расшифровывать состояния мозга методом последовательных испытаний. Традиционно в этих исследованиях основное внимание уделялось точечным оценкам; однако в последние годы больше внимания стало уделяться расшифровке полных функций правдопо- добия, что связывает их с остальной частью этой главы. Функциональная магнитно-резонансная томография (фМРТ) – это метод, в котором большие мощные магниты и радиочастотные волны используют для измерения трехмерной картины оксигенации крови в голове, связанной с активностью нейронов. Мозг делят на воксели (voxel, это как пиксели, но объемные – маленькие кубики), и фМРТ фиксирует процентное изменение сигнала в каждом вокселе в ответ на предъявляемые стимулы. Очевидный интерес представляет возможность своего рода «чтения мыслей»: расшиф- ровка того, что думает или видит мозг на основе данных фМРТ. Логика здесь немного отличается от остальной части книги, поскольку модель мозга – это не модель наблюдателя. В конце концов, наблюдения человека – это\n--- Страница 366 ---\nЗаключение  365 активация нейронов, а не вокселей. Однако экспериментаторы используют порождающую модель воксельной активности для декодирования стимула. В данном случае наблюдатель является экспериментатором, а не субъектом эксперимента. Точно так же, как наблюдатель должен сначала изучить пара- метры порождающей модели своих наблюдений, чтобы сделать вывод, экс - периментатор должен изучить параметры порождающей модели воксельной активности. Для этого используют обучающие данные, в которых стимул считается известным при каждом испытании. Изучение параметров модели можно выполнить с применением оценки максимального правдоподобия, но на самом деле лучше использовать апостериорное распределение по па- раметрам, подобное апостериорному распределению по σ в разделе 6.4. За- тем, используя изученные параметры, можно декодировать апостериорное распределение по стимулу в зависимости от воксельной активности в новых испытаниях [61, 108, 185]. Расчет апостериорных распределений на основе активности нейронов для восприятия имеет близкий аналог в технической проблеме декодирова- ния в контексте интерфейсов мозг–машина. После травмы спинного мозга, которая приводит к параличу, человеку могут надеть протезы рук. Но как управлять этими протезами? Один из вариантов – использовать голосовые команды, но это весьма неудобная процедура. Другой вариант заключается в использовании движений глаз. Третий же – который на первый взгляд ка- жется достойным научно-фантастического рассказа – состоит в том, чтобы управлять протезом с помощью интерфейса мозг–машина, который считы- вает намерения пользователя по картине его нейронной активности. Запи- санные и правильно интерпретированные нейронные сигналы от моторной коры могут указывать на предполагаемые движения пользователя [189]. В та- ких сценариях интерфейса мозг–машина исследователи могут попытаться рассчитать апостериорное распределение двигательного намерения, учиты- вая зарегистрированную нейронную активность [24]. 14.9. Заключение В этой главе мы распространили определение порождающих моделей на ней - ронную активность и ввели понятия нейронных вероятностей и нейронной неопределенности. Вы узнали следующее: нейронные реакции на раздражители можно описать с помощью кри- вых настройки и модели случайной изменчивости вокруг этих кривых настройки. Простая модель представляет собой независимую пуассо- новскую изменчивость; обращая порождающую модель, определенную таким образом, мы мо- жем вычислить нейронную функцию правдоподобия стимула на осно- ве заданного паттерна активности нейронной популяции; функция нейронного правдоподобия содержит всю информацию, ко- торую можно объективно получить из активности популяции. Никакая\n--- Страница 367 ---\n366  Нейронная функция правдоподобия дополнительная информация не может быть получена, и любая другая информация будет неверной; эта нейронная функция правдоподобия, ее мода и ширина варьируются в зависимости от испытаний, даже если стимул остается неизменным; нейронная функция правдоподобия стремится к гауссовой только в пределе, при наличии большого объема информации (много нейро- нов, много спайков); нейронные функции правдоподобия можно использовать для дальней- ших байесовских вычислений так же, как мы использовали ненейрон- ные функции правдоподобия в предыдущих главах; случай независимых пуассоновских нейронов с гауссовыми кривыми настройки и аппроксимацией постоянной суммы является полезной упрощенной моделью, которая допускает интуитивно понятные ана- литические выражения как для MLE, так и для ширины функции прав- доподобия; измерение, использованное в предыдущих главах, можно приблизи- тельно отождествить с MLE стимула, полученного из нейронной по- пуляции; существует множество подходов к реализации байесовских вычисле- ний с нейронными операциями; точно так же, как мозг может вычислять нейронную функцию правдо- подобия, экспериментаторы могут вычислять функции правдоподобия на основе воксельной активности, используя оценочную порождаю- щую модель этой активности. Это позволяет расшифровывать сенсор- ную неопределенность с помощью последовательных испытаний. 14.10. Рекомендуемая литература Christopher R. Fetsch, Alexandre Pouget, Gregory C. DeAngelis, and Dora E. Angelaki. Neural Correlates of Reliability­Based Cue Weighting During Mul­ tisensory Integration. Nature Neuroscience 15, no. 1 (2012): 146–154. József Fiser, Pietro Berkes, Gergő Orbán, and Máté Lengyel. Statistically Optimal Perception and Learning: From Behavior to Neural Representations. Trends in Cognitive Sciences 14, no. 3 (2010): 119–130. Peter Földiák. The ‘Ideal Homunculus’: Statistical Inference from Neural Po­ pulation Responses. In Computation and Neural Systems, edited by Frank H. Eeckman and James M. Bower, 55–60. New York: Springer, 1993. Laura S. Geurts, James R. H. Cooke, Ruben S. van Bergen, and Janneke F. M. Jehee. Subjective Conﬁdence Reﬂects Representation of Bayesian Probability in Cortex. Nature Human Behaviour 6 (2022): 294–305. Mehrdad Jazayeri and J. Anthony Movshon. Optimal Representation of Sensory Information by Neural Populations. Nature Neuroscience 9, no. 5 (2006): 690– 696. Hsin-Hung Li, Thomas C. Sprague, Aspen H. Yoo, Wei Ji Ma, and Clayton E. Curtis. Joint Representation of Working Memory and Uncertainty in Human Cortex. Neuron 109, no. 22 (2021): 3699–3712.\n--- Страница 368 ---\nЗадачи  367 Wei Ji Ma, Jeffrey M. Beck, Peter E. Latham, and Alexandre Pouget. Bayesian Inference with Probabilistic Population Codes. Nature Neuroscience 9, no. 11 (2006): 1432–1438. Emin Orhan and Wei Ji Ma. Efficient Probabilistic Inference in Generic Neural Networks Trained with Non­Probabilistic Feedback. Nature Communications 8, no. 1 (2017): 1–14. Ryan M. Peters, Phillip Staibano, and Daniel Goldreich. Tactile Orientation Perception: An Ideal Observer Analysis of Human Psychophysical Performance in Relation to Macaque Area 3b Receptive Fields. Journal of Neurophysiology 114, no. 6 (2015): 3076–3096. Jonathan W. Pillow, Jonathon Shlens, Liam Paninski, Alexander Sher, Alan M. Litke, E. J. Chichilnisky, and Eero P . Sinomcelli. Spatio­ Temporal Corre lations and Visual Signalling in a Complete Neuronal Population. Nature 454, no. 7207 (2008): 995–999. Alexandre Pouget, Peter Dayan, and Richard Zemel. Information Processing with Population Codes. Nature Reviews Neuroscience 1, no. 2 (2000): 125–132. Terence David Sanger. Probability Density Estimation for the Interpretation of Neural Population Codes. Journal of Neurophysiology 76, no. 4 (1996): 2790–2793. Ruben S. Van Bergen, Wei Ji Ma, Michael S. Pratte, and Janneke F. M. Jehee. Sensory Uncertainty Decoded from Visual Cortex Predicts Behavior. Nature Neuroscience 18, no. 12 (2015): 1728–1730. Edgar Y. Walker, R. James Cotton, Wei Ji Ma, Andreas S. Tolias. A Neural Basis of Probabilistic Computation in Visual Cortex. Nature Neuroscience 23, no. 1 (2020): 122–129. Richard S. Zemel, Peter Dayan, and Alexandre Pouget. Probabilistic Interpretati­ on of Population Codes. Neural Computation 10, no. 2 (1998): 403–430. 14.11. Задачи Задача 14.1. Верны или ошибочны следующие утверждения? Обоснуйте свой ответ. (a) Как правило, дисперсия отклика отдельного нейрона на стимул может быть определена по значению его кривой настройки при этом значении стимула. (b) В любой популяции нейронов изменчивость активности популяции полностью известна, если известна изменчивость каждого отдельного нейрона. (c) Чем ближе стимул к предпочтительному стимулу пуассоновского нейро- на, тем ниже дисперсия ответа этого нейрона при многократном предъ- явлении стимула. (d) Если нейроны имеют похожие и одинаково расположенные кривые на- стройки, то нейронная функция правдоподобия имеет ту же ширину, что и кривая настройки. Задача 14.2. Предположим, что имеется популяция из девяти независимых нейронов Пуассона с гауссовыми кривыми настройки и предпочтительной\n--- Страница 369 ---\n368  Нейронная функция правдоподобия ориентацией от -40 до 40 с шагом 10. Параметры кривой настройки имеют значения g = 10, b = 0 и σtc = 20. Стимул s = 0 представлен этой совокупности. Какова вероятность того, что все нейроны будут молчать, т. е. не сгенерируют отклик? Задача 14.3. В упрощенной модели из раздела 14.4 мы предполагали гауссо- вы настроечные кривые с нулевым базовым уровнем (zero-baseline). (a) Что изменится в математическом представлении, если опорный уровень не равен нулю? (b) Для каждого из опорных значений 0, 0.25, 0.5 и 1 численно найдите и по- стройте десять функций правдоподобия (предположим, что истинный стимул равен нулю) и опишите, что вы наблюдаете. Задача 14.4. В упрощенной модели мы предполагали одинаковую ширину кривой настройки σtc для всех нейронов. Выведите эквивалент уравнений (14.20) и (14.21) для моды и ширины функции правдоподобия, если каждый нейрон имеет свою ширину настройки, скажем σtc,i для i -го нейрона. Задача 14.5. Предположим, что существует популяция независимых пуассо- новских нейронов, откликающихся на неотрицательный стимул s. Каждый нейрон имеет линейную кривую настройки fi(s) = ai(s). (14.26) Покажите, что для любого паттерна активности в этой популяции норми- рованная нейронная функция правдоподобия представляет собой гамма- распределение, и найдите выражения для ее параметров. Задача 14.6. Здесь мы исследуем, может ли ширина функции правдоподобия коррелировать с ошибкой MLE, другими словами, идут ли рука об руку не- определенность и ошибка. (a) Выполните симуляцию упрощенной модели для 10 000 испытаний (все при s = 0 и коэффициенте усиления 1) и создайте диаграмму рассеивания квадратичной ошибки MLE в зависимости от квадрата ширины (диспер- сии) функции правдоподобия. Что такое коэффициент корреляции? (b) Выберите несколько различных значений коэффициента усиления. За- висит ли сила корреляции от усиления? (c) Теперь разделите испытания на четыре квартиля для дисперсии функ - ции правдоподобия. Вычислите дисперсию MLE для каждой из этих че- тырех групп испытаний. Есть ли корреляция? Задача 14.7. Некоторые переменные стимула, такие как направление движе- ния, являются периодическими (направленными). Мы можем рассматривать такие переменные как принимающие значения на окружности, например от -π до π радиан. Рассмотрим лабораторный эксперимент, в котором направ- ление движения определяется по распределению фон Мизеса с круговым средним значением μs и параметром концентрации κs: p(s) µ eκscos(s -μs). (14.27)\n--- Страница 370 ---\nЗадачи  369 Предположим, что направление движения, обозначаемое s, закодировано в популяции n независимых пуассоновских нейронов. Кривая настройки i-го нейрона имеет форму фон Мизеса с усилением g, предпочтительным направлением spref,i и параметром концентрации κtc: fi(s) = geκtccos(s -spref,i). (14.28) (a) Покажите, что функция правдоподобия стимула, основанная на паттерне активности этой популяции r = (r1, …, rn), пропорциональна распределе- нию фон Мизеса �(s; r) µ eκLcos(s -μL), (14.29) и найдите выражения для cos μL, sin μL и κL, каждое через ri. MLE μL также известна как декодер вектора популяции. (b) Покажите, что апостериорное распределение также является распреде- лением фон Мизеса, и вычислите его круговое среднее. Задача 14.8. Это продолжение предыдущей задачи, в которой мы будем численно реализовывать и сравнивать два нейронных декодера периоди- ческой переменной. Это также нейронная аналогия материала, который мы рассмотрели в главе 4. (a) Получите 2000 направлений движения s из распределения стимулов с μs = 0 и κs = 4. Каждый такой стимул представляет собой одно экс - периментальное испытание. Для каждого испытания получите паттерн активности популяции в ответ на направление движения в этом испыта- нии; предположим, что g = 0.2, предпочтительные направления кратны 10° и κtc = 1. (b) Затем, снова для каждого испытания, вычислите как MLE, так и оценку апостериорного среднего (PME). Можете использовать выражения, по- лученные в предыдущей задаче. Если вы не решали эту задачу, можете выполнить расчеты численно. (Подсказка: используйте функцию арктан - генса.) (c) Вычислите квадрат синуса ошибки оценки для всех испытаний и обеих оценок. Эта величина является круговым эквивалентом квадрата ошиб- ки для переменных с вещественным значением. (d) Создайте рисунок, состоящий из сетки 2×2. В верхней левой ячейке по- кажите диаграмму рассеяния MLE в зависимости от s. В верхней правой ячейке покажите диаграмму рассеяния PME в зависимости от s. Оба гра- фика должны иметь как диапазон x, так и диапазон y от -π до π; также нарисуйте диагональ черной пунктирной линией для справки. В нижней левой ячейке должна быть показана гистограмма квадратичной синус - ной ошибки MLE. Используйте двадцать интервалов столбцов. В нижней правой ячейке покажите то же самое для PME. (e) Имеют ли MLE или PME более высокую среднеквадратичную синусную ошибку? Что лучше коррелирует с истинным стимулом? Вы ожидали такие свойства?",
      "debug": {
        "start_page": 347,
        "end_page": 370
      }
    },
    {
      "name": "Глава 15. Байесовские модели в контексте 370",
      "content": "--- Страница 371 --- (продолжение)\nГлава 15 Байесовские модели в контексте Какие возможности и ограничения присущи байесовским моделям при изучении поведения и мозга? В этой заключительной главе мы опишем, как байесовские модели восприя- тия и действия связаны с более обширными темами в области поведенческих исследований и не только. В данной книге мы также обсудим ограничения байесовских моделей и предложим перспективные направления исследова- ний в данной области. Краткое содержание главы Мы начинаем главу с предположения, что есть все основания ожидать встре- тить байесовское поведение как в течение всей жизни человека, так и в мас - штабах времени эволюции. Однако также разумно ожидать, что не всякое байесовское поведение является оптимальным и что не всякое поведение является байесовским. Мы обсудим, чем байесовские модели восприятия и действия отличаются и как соотносятся с другими классами популярных моделей в когнитивной науке и нейронауке. Затем разберем возможные ответы на критику байесовских моделей. Глава заканчивается обзором от - крытых областей исследований: сложность реального мира, обуче ние байе- совскому подходу и приблизительный вывод. 15.1. Байесовское и оптимальное поведение Обладающие мозгом живые существа, включая людей, на протяжении всей своей жизни сталкиваются с последовательными проблемами принятия ре- шений. Соответственно, мы ожидаем, что адаптивное поведение будет осно- вано на регулярном выборе решений как в течение жизни, так и в масштабах\nГлава 15 Байесовские модели в контексте Какие возможности и ограничения присущи байесовским моделям при изучении поведения и мозга? В этой заключительной главе мы опишем, как байесовские модели восприя- тия и действия связаны с более обширными темами в области поведенческих исследований и не только. В данной книге мы также обсудим ограничения байесовских моделей и предложим перспективные направления исследова- ний в данной области. Краткое содержание главы Мы начинаем главу с предположения, что есть все основания ожидать встре- тить байесовское поведение как в течение всей жизни человека, так и в мас - штабах времени эволюции. Однако также разумно ожидать, что не всякое байесовское поведение является оптимальным и что не всякое поведение является байесовским. Мы обсудим, чем байесовские модели восприятия и действия отличаются и как соотносятся с другими классами популярных моделей в когнитивной науке и нейронауке. Затем разберем возможные ответы на критику байесовских моделей. Глава заканчивается обзором от - крытых областей исследований: сложность реального мира, обуче ние байе- совскому подходу и приблизительный вывод. 15.1. Байесовское и оптимальное поведение Обладающие мозгом живые существа, включая людей, на протяжении всей своей жизни сталкиваются с последовательными проблемами принятия ре- шений. Соответственно, мы ожидаем, что адаптивное поведение будет осно- вано на регулярном выборе решений как в течение жизни, так и в масштабах\n--- Страница 372 ---\nЧрезмерно сильные заявления об оптимальности  371 времени эволюции. Среди задач принятия решений, с которыми сталкивают - ся животные в реальном мире, очень многие связаны с неопределенностью. Для этого большого класса задач оптимальные решения являются байесов- скими. Поэтому мы ожидаем, что люди и другие животные будут склонны использовать байесовские стратегии и что многие виды поведения будут иметь тенденцию соответствовать предсказаниям байесовских моделей. Использование байесовского вывода не является синонимом оптимального поведения. С одной стороны, некоторые задачи оптимизации не требуют байе- совского подхода. К таким задачам относится поиск универсального способа игры в крестики-нолики, быстрого способа сортировки списка или упаковки наибольшего количества яблок в ящик. Зачастую подобные задачи представля- ют собой искусственные примеры из области информатики, не имеющие боль- шого экологического значения для организмов. В большинстве экологически важных задач не обходится без неопределенности состояния мира. С другой стороны, субъект, принимающий решения, может быть полно- стью байесовским, но неоптимальным. Например, столкнувшись с незна- комой задачей, он может применить неверные априорные распределения или расчеты правдоподобия, что приведет к несоответствию моделей (см. раздел 3.5). В таких случаях поведение субъекта, принимающего решение, обычно будет субоптимальным по отношению к порождающей модели, ха- рактеризующей поставленную задачу. Но это поведение все равно было бы оптимальным по отношению к предполагаемой порождающей модели, осно- ванной на контрфактических данных. Байесовская модель в этом случае по- прежнему полезна. Она может служить для приписывания субоптимальности конкретному ошибочному убеждению, например относительно ошибочного априорного распределения, основанного на прошлом опыте. Более того, мо- жет случиться так, что поведение не является оптимальным по отношению к порождающей модели лабораторной задачи, но является оптимальным по отношению к порождающей модели в естественном мире. В таких случаях было бы неправильно называть поведение субоптимальным. Упражнение 15.1. Выберите три публикации, посвященные более широкому кругу байесовских моделей восприятия и действия, и выясните для себя, на какие допущения и предположения они опираются. Что считается оптималь- ным? Чем обоснована модель? 15.2. Чрезмерно сильные заявления об оптимальности Заявление о том, что поведение близко к оптимальному, часто кажется при- влекательным, потому что означает четкое, принципиальное описание по- ведения, в то время как анализ субоптимальности, как правило, является более запутанным делом. Поэтому неудивительно, что авторы исследований иногда несколько небрежно относятся к понятию оптимальности. Эта не- брежность может заключаться в добавлении специальных механизмов для\n--- Страница 373 ---\n372  Байесовские модели в контексте «сохранения» оптимальности или выполнения только качественного сравне- ния данных и модели. Байесовские модели по своей сути являются количест­ венными, и необходимо приложить все усилия для проведения подробных количественных сравнений. Особый вид сомнительного утверждения об оп- тимальности возникает, когда априорные распределения выбирают чересчур вольно, вместо того чтобы обосновать их либо естественной, либо экспери- ментальной статистикой, которую участникам эксперимента предоставили возможность изучить. Спецификация задачи должна также содержать осмысленную функцию затрат. Насколько ограничен разработчик модели в выборе функции затрат, зависит от задачи. В моделях категориальных перцептивных задач без внеш- ней структуры вознаграждения наиболее естественным выбором является функция стоимости 0-1, которая соответствует максимальной точности (раз- дел 13.5). В непрерывных перцептивных задачах для удобства часто предпо- лагается функция затрат на основе квадрата ошибки, но возможны и другие варианты. Когда существуют внешние вознаграждения, задача осложняется потенциально нелинейной зависимостью между вознаграждением и субъ- ективной полезностью (раздел 13.7.2). В наиболее сложных случаях функция затрат является неизвестной функцией нескольких переменных; например, какова цена движения вашего тела определенным образом при ловле мяча или цена непонимания кого-то в разговоре? Вообще говоря, мы считаем, что исследователи должны с осторожностью допускать гибкость любого компо- нента байесовской модели, и каждая форма гибкости должна быть тщательно обоснована. Неудивительно, что чрезмерно сильные заявления об оптимальности байе- совских моделей вызвали определенный скептицизм относительно таких утверж дений и, соответственно, байесовских моделей [22, 86]. Часто критику - ют заявления наподобие «Байесовские исследования могут сделать любое ре- шение оптимальным». Хотя такое высказывание справедливо для идеальных байесовских моделей без каких-либо ограничений, оно попросту неверно для байесовских моделей, которые в максимально возможной степени выводят вероятности, априорные распределения и функцию затрат из спецификации задачи. Многие байесовские модели имеют лишь несколько (хорошо обосно- ванных) свободных параметров. Упражнение 15.2. Знаете ли вы какие-нибудь статьи о байесовском модели- ровании, в которых, по вашему мнению, слишком много заявлений об оп- тимальности? Как повлияет на их содержание удаление этих утверждений? 15.3. Почему одни модели поведения оптимальны, а другие нет Если в одних случаях поведение близко к оптимальному, а в других нет, то чем отличаются эти ситуации? Распространено мнение, что поведение в пер-\n--- Страница 374 ---\nБайесовские модели не являются механистическими  373 цептивных и двигательных задачах склонно быть оптимальным, тогда как поведение в когнитивных задачах скорее будет субоптимальным. Аргументы в пользу субоптимальности познания обычно вращаются вокруг длинного списка когнитивных искажений и заблуждений, которые демонстрируют люди [87, 88], от заблуждения игрока до эффекта привязки, предвзятости подтверждения и эвристики доступности. На первый взгляд такое различие между восприятием и познанием кажется правдоподобным. В конце концов, системы восприятия эволюционно старше, чем человек разумный, и поэтому у них было больше времени для оптимизации. Однако при ближайшем рас - смотрении понятие оптимальности этих когнитивных эффектов во многих отношениях сильно отличается от представлений о перцептивной и двига- тельной деятельности. Например, когнитивные искажения обычно связаны с явным манипулированием вероятностями. Замена вероятностей частотами уменьшает или устраняет некоторые предубеждения; иногда большое зна- чение имеет конкретная формулировка. Кроме того, когнитивные ошибки лишь грубо измеряются на уровне индивидуума-субъекта: он возвращает либо «смещенный», либо «несмещенный» ответ, не давая представления о процессе принятия решений индивидуумом. Напротив, при использова- нии параметрических измерений в более надежных экспериментальных системах часто обнаруживается, что байесовские модели хорошо объясняют человеческое мышление и обуче ние, по крайней мере на качественном уров- не (например, [11, 34, 48, 68, 71, 134, 210]). Однако оптимальности гораздо труднее дать определение в случае когнитивных задач высокого уровня. Ин- тересный подход, заслуживающий большего внимания, состоит в том, чтобы рассмотреть задачу с когнитивной и перцептивной точек зрения и сравнить человеческое поведение между ними. Люди были близки к оптимальным решениям в задаче на быстрое перемещение, которую мы обсуждали в раз- деле 13.8.2. Однако эта задача была задумана как двигательный эквивалент лотереи, в которой люди проявляют неоптимальные формы поведения [182]. Точно так же Стефани Чен и ее коллеги обнаружили, что в задаче, в которой оптимальное поведение требовало маргинализации по категориям, люди не могли этого сделать при словесных отчетах, но справились при захвате объекта [35]. 15.4. Байесовские модели не являются механистическими Байесовские модели подвергались критике за то, что они являются моделями типа «как если бы» в том смысле, что люди могут следовать чему-то близкому к байесовскому правилу принятия решений, но байесовские математические выкладки, ведущие к этому правилу, не обязательно соответствуют реально- му положению дел в природе. В одном из вариантов этой критики обоснова- ние реальности достигается только причинно-механистическими моделями. На самом деле байесовские модели восприятия и действия представляют\n--- Страница 375 ---\n374  Байесовские модели в контексте собой функциональные, а не каузально-механистические объяснения в том смысле, что они не определяют нейронные компоненты и нейронные опе- рации, реализующие байесовские вычисления [40, 54]. При этом существует множество работ по нейронной реализации байесовских моделей (см. гла- ву 14). Здесь мы сталкиваемся с проблемой множественной реализуемости в том смысле, что одни и те же байесовские вычисления могут быть реа - лизованы нейронами множеством (и потенциально бесконечным числом) различных способов. Таким образом, байесовская модель не может делать какие-либо точные предсказания для нейробиологов, интересующихся ме- ханизмом байесовского вывода. На наш взгляд, эта проблема не специфична для байесовских моделей и затрагивает почти все модели поведения. Но есть и хорошая новость: в этой области применимо разделение масштабов. Макромасштаб (вычисление логического вывода) можно моделировать от - дельно от микромасштаба (нейронная реализация). Одно из мест, где байесовские модели могут пересекаться с механизмом, – это ресурсно-рациональные модели [109]. Это категория моделей, которые отличаются от чисто байесовских моделей тем, что учитывают стоимость представления или вычислений. В некоторых случаях эта стоимость являет - ся биологически мотивированной, например она соответствует количеству нейронов, занятых вычислением, или общей активности популяции нейро- нов. 15.5. Байесовский перенос Другой повод усомниться в байесовских моделях заключается не в нейрон- ной реализации, а во внутреннем устройстве таких моделей. В байесовской модели вероятности, априорные вероятности и функции затрат определя- ют, какое поведение будет успешным. Но имеют ли смысл эти конструкции и действительно ли они используются индивидуумом? Этот вопрос связан с вопросом во введении о том, являются ли байесовские модели моделями процессов. Для демонстрации значимости внутренних конструкций байесовской мо- дели Мэлони и Мамассян применяют так называемый байесовский перенос [121]. Идея состоит в том, что если априорное распределение, правдоподобие и стоимость имеют смысл, они должны быть гибкими и распространяться на задачи или условия. Например, если участник близок к оптимальному с комбинацией правдоподобия 1 и априорного распределения 1, а также с комбинацией правдоподобия 2 и априорного распределения 2, то тест на обобщение будет состоять из представления условия, которое требует ис - пользования правдоподобия 1 и априорного распределения 2 или правдопо- добия 2 и априорного распределения 1. В подобных обновленных условиях повторное изучение правила путем проб и ошибок будет медленным и по- требует многих попыток, тогда как рекомбинация «вычислительных моду - лей» должна быть быстрой. Конечно, возможно, что некоторые конструкции байесовских вычислений являются гибкими и обобщаемыми в отношении\n--- Страница 376 ---\nВероятностные вычисления и гибридные модели  375 вариаций только одного компонента, но не других. Ма и Джазайери предла- гают иерархию байесовской гибкости [114]. Байесовская гибкость – это основ- ной способ получения данных о степени, в которой априорные вероятности и вероятности могут комбинироваться произвольным образом. В крайнем своем проявлении подстройка правила в байесовском экспе- рименте по переносу должна быть мгновенной: изменение априорных рас - пределений и правдоподобия в каждом испытании должно давать поведе- ние, близкое к оптимальному. Другими словами, мы могли бы использовать каждое испытание как обобщающий тест других испытаний. На практике лишь немногие исследования одновременно изменяют как априорное рас - пределение, так и правдоподобие, но варьирование правдоподобия от ис - пытания к испытанию встречается очень часто. Например, в исследованиях комбинации сигналов надежность по крайней мере одного сенсорного сиг - нала может варьироваться от испытания к испытанию1. Для более строгого обобщающего тестирования следует воздерживаться от обратной связи, ос - нованной на методе проб и ошибок, чтобы сделать изучение правил мето- дом проб и ошибок практически невозможным. Первоначальное обуче ние допускается только в начале сеанса или всего эксперимента, но, возможно, даже не для всех условий правдоподобия, что вынуждает участников обоб- щать ограниченные условия обучения в сочетании с доэкспериментальным опытом взаимодействия с аналогичными стимулами. Обобщения априорных распределений или функций затрат рассматрива- ются реже, чем обобщение по правдоподобию, но некоторые исследования в этой области все же были проведены. Например, априорное направление света сверху [5] и априорная выпуклость лица [77], скорее всего, являются общими для разных задач. Ачерби и его коллеги [2] варьировали априорное распределение от испытания к испытанию, а Уайтли и Сахани [201] изменяли функцию затрат (раздел 13.8.1). Полученные результаты позволяют пред- положить, что люди способны к байесовскому переносу и в этих ситуациях. В целом мы думаем, что критика «надуманности» байесовского подхода была и будет адекватно опровергнута эмпирически, и мы не считаем ее се- рьезной проблемой для байесовской системы моделирования восприятия и действия. 15.6. Вероятностные вычисления и гибридные модели Байесовские исследования сенсорной функции правдоподобия показали, что неопределенность вычисляется мозгом и учитывается при принятии реше- 1 В главе 14 мы показали, что правдоподобие может меняться от испытания к ис - пытанию, даже если стимул остается неизменным; однако экспериментаторы не могут контролировать это изменение, поэтому им приходится прибегать к изме- нению стимула.\n--- Страница 377 ---\n376  Байесовские модели в контексте ний. Чтобы отличить этот процесс от байесовского вывода в более общем смысле, мы ранее назвали такие вычисления вероятностными вычислениями [112]. Объединив необходимость сравнения байесовских моделей с субоп- тимальными альтернативами и вероятностные вычисления, мы приходим к категории гибридных моделей, а именно моделей субоптимальных вероят- ностных вычислений . Это модели, в которых неопределенность стимула выве - дена мозгом на основе последовательных испытаний, но затем используется субоптимально в последующих вычислениях. Например, в разделе 8.6 мы вы- вели байесовское решающее правило для категоризации; это правило, одной из форм которого является уравнение (8.38), зависит от уровня сенсорной не- определенности σ. Однако возможно, что мозг учитывает неопределенность, но по другому правилу. В нескольких парадигмах визуального принятия ре- шений были обнаружены доказательства субоптимального вероятностного вычисления. Субоптимальность может быть, а может и не быть байесовской в рамках порождающей модели, отличной от той, которая продиктована по- становкой задачи, – обычно это трудно установить. Пока не установлено, что правило принятия решений является байесовским, модель следует рас - сматривать как гибридную, с байесовским интерфейсом и небайесовской вычислительной основой. В нейронных исследованиях в принципе можно было бы проследить распространение функции правдоподобия по областям мозга, участвующим в различных частях вычислений, но, насколько нам из- вестно, этого не было сделано. 15.7. Сложность реального мира Основная проблема байесовских моделей восприятия и действия состоит в их масштабировании до уровня сложных и/или реальных задач. Подавля- ющее большинство задач, изучаемых в этой книге и в соответствующей на- учной области, просты и содержат одну или несколько переменных, каждая из которых является бинарной или одномерной. Это ограничение хорошо согласуется с большинством лабораторных задач, но плохо с естественными задачами, в которых больше переменных, эти переменные имеют большую размерность и могут принимать больше значений. Мы увидели это в разделе 12.2.2, где вычисляли функцию правдоподобия для вектора точки изменения, которая может принимать значения 2T (см. также задачу 11.7). Даже при скромных значениях T это приводит к очень большому количеству гипотез, которые следует иметь в виду. Другим примером являются мешающие па- раметры верхнего уровня, которые обсуждались в главе 9. В реальной сцене для каждого объекта существует множество таких параметров: угол обзора, расстояние просмотра, освещение и т. д. Более того, сами эти мешающие параметры имеют выраженную пространственную структуру, при этом объ- екты также играют особую роль. Правильная маргинализация по всем этим параметрам, как мы это делали в наших простых примерах в главе 9, – не- простая задача. На самом деле такие задачи могут быть слишком сложными для вычислений [188].\n--- Страница 378 ---\nСложность реального мира  377 Кроме того, поведенческие выводы в реальном мире не имеют ничего общего с вариантами ответов в лабораторных задачах. Например, во время похода в лес поведение, которое нужно предсказать, состоит в том, куда че- ловек собирается двинуть ногой в следующий раз, под каким углом, в какое время и с какой силой. Другой пример поведения в реальном мире – веде- ние разговора. Там предсказываемое поведение – это не простой двоичный выбор или одномерная оценка, а естественный язык (не говоря уже о позе тела, движениях глаз и т. д.). Кроме того, функция затрат сложна и, вероятно, включает в себя несколько условий, таких как точность понимания, время, которое занимает разговор, и впечатление, которое один собеседник про- изводит на другого. Поэтому разработчики байесовских моделей даже не пытаются решать подобные задачи. Легко допустить, что если среда или задача станет слишком сложной, люди могут перестать следовать предсказаниям байесовских моделей. Од- нако именно при таких обстоятельствах даже не ясно, как сформулировать байесовскую модель, например потому, что мы не имеем представления о распределениях, задействованных в порождающей модели. Это своего рода байесовская трагедия: мы способны определить байесовские модели в сцена- риях, где они наверняка сработают, и нам трудно определить их в сценариях, где они вряд ли сработают. Примечание 15.1 Приблизительный вывод Хотя в этой книге мы в основном изучали точный байесовский вывод, существует целый мир приближенных байесовских вычислений [19]. Некоторые алгоритмы ап- проксимации проникли в когнитивную науку и нейронауку. Распространение доверия (belief propagation). В этом классе алгоритмов мы начина- ем с некоторых переменных, распространяем сообщения по системе и применяем возможные исправления. Мы часто полагаем, что соответствующие переменные имеют распределения, для которых интегралы могут быть аналитически решены или аппроксимированы. Мы также полагаем, что отношения между переменными таковы, что их можно описать как (в общем случае) разреженный граф. В некоторых видах графов распространение доверия обеспечивает эффективный способ реше- ния многих байесовских задач. Иногда он приблизительно верен, даже если допу - щения не выполняются. Вариационные байесовские методы. В случае сложных распределений вероятностей распространение убеждений не может быть полностью выражено аналитически. На самом деле распределение вероятностей каждой ненаблюдаемой переменной при наличии наблюдаемых переменных может быть сколь угодно сложным. Вариа- ционный байесовский алгоритм заключается в следующем. Мы аппроксимируем распределение вероятностей p распределением вероятностей (qθ), которое зависит от ряда параметров θ. Впоследствии мы оптимизируем параметры qθ так, чтобы qθ стало максимально похоже на реальное распределение вероятностей p. Сделать их одинаковыми, как правило, невозможно, поэтому вариационные методы обычно оптимизируют аппроксимацию, так называемую нижнюю вариационную границу (evidence lower bound, ELBo).\n--- Страница 379 ---\n378  Байесовские модели в контексте Методы цепей Маркова. Другим подходом к решению подобных задач являются методы цепей Маркова. В соответствии с этим подходом инициализируют вектор всех переменных, которые необходимо вывести, и итеративно изменяют его для создания последовательности векторов. Процесс построен таким образом, что с те- чением времени ожидаемое количество раз пребывания в определенном состоя- нии пропорционально фактической вероятности состояния с учетом наблюдаемых переменных. Эти методы распространяются на произвольные распределения веро- ятностей. Они особенно полезны, если распределение вероятностей сосредоточено в небольших областях с высокой вероятностью. Однако все доказательства коррект - ности верны лишь в пределе большого, часто астрономического, числа итераций. На практике методы цепей Маркова обычно ограничены сложностью задач, кото- рые они позволяют решить за разумное время выполнения. В когнитивной науке было высказано предположение, что люди, принимающие решения, ведут себя как цепи Маркова с ограниченным числом итераций [192]. 15.8. Как стать настоящим байесовцем В этой книге мы смоделировали мозг как систему, реализующую байесовский вывод. Но каким образом мозг обрел эту способность? В разделе 6.4 мы рас - смотрели ограниченную форму данного вопроса, описав байесовский алго- ритм для изучения параметра порождающей модели. Гораздо более широкий вопрос заключается в том, как приобретается способность точно инверти- ровать порождающую модель. Рождаемся ли мы со специализированным механизмом байесовского вывода в мозгу? Или мы узнаем из опыта, что байесовский подход обеспечивает наилучшее решение реальных проблем, связанных с неопределенностью? В области изучения восприятия и действия на эти вопросы по-прежнему нет окончательных ответов. В пользу объяс - нения, подразумевающего обуче ние, говорит тот факт, что маленькие дети более далеки от оптимальности по сравнению со взрослыми в объединении сенсорной информации с априорной [31] и в объединении мультисенсорных сигналов [69, 133, 144]. Хотя байесовские модели не являются механистическими, в целом сущест - вует тенденция рассматривать обуче ние с механистической точки зрения. Этому в значительной степени способствуют искусственные нейронные сети – обучающиеся устройства общего назначения, которые могут служить площадкой для проверки идей (примечания 6.4 и 15.2). Для задач, связанных с неопределенностью, байесовское решение всегда лучше. Решения байе- совских задач, упомянутых в этой книге (и в публикациях о научных экс - периментах), могут быть относительно легко изучены с помощью нейрон- ной сети [140]. На самом деле это открытие заставляет задуматься, почему маленькие дети еще не являются опытными байесовцами! Но байесовские задачи, изучаемые в лаборатории, обычно представляют собой чрезвычайно упрощенные версии реальных задач логического вывода. Более того, этим нейронным сетям с каждым испытанием предоставлялась обратная связь о состоянии мира. Такая обратная связь не осуществима для людей. Наиболее пессимистичный вывод из этого заключается в том, что байесовский подход\n--- Страница 380 ---\nРекомендуемая литература  379 должен быть отвергнут, потому что мир не предоставляет наблюдателям до- статочной достоверной информации для построения точных порождающих моделей, на которых можно основывать выводы [148]. Более конструктивным подходом было бы искать обучаемые механизмы общего назначения, кото- рые сочетают ограниченную обратную связь (помеченные данные) с обуче- ни ем без учителя [17, 120] и априорными знаниями [175]. Дополнительным ограничением является использование биологически правдоподобных пра- вил обучения, что обычно означает, что на обновление параметра в сети влияет только активность «близлежащих» единиц в сети; градиентный спуск в этом смысле биологически неправдоподобен. Примечание 15.2 Искусственные нейронные сети против байесовского вывода В этой книге мы использовали базовый подход, который заключается в том, чтобы определить относительно простую порождающую модель p(x | s) и инвертировать ее, получив выражение для апостериорного распределения p(s | x). Однако реаль- ный мир очень сложен, поэтому в большинстве случаев невозможно определить удовлетворительную порождающую модель для конкретной задачи. Но даже если это удалось, p(s | x) может быть очень трудно вычислить. В области искусственных нейронных сетей (ИНС) используется другой подход. Вместо того чтобы начинать с определяемой человеком порождающей модели мира, ИНС используют функцию общего назначения fθ(x) со многими свободными параметрами θ (см. примечание 6.4) для аппроксимации апостериорной вероятности. Параметры оптимизируют для получения наилучших возможных результатов. Таким образом, основное вни- мание уделяется не инвертированию порождающей модели, а поиску работающей. При этом обученная ИНС с точки зрения практических целей может быть байесов- ской (вести себя как байесовская). Другими словами, вполне возможна ситуация, когда система, которая нигде не является явно байесовской, проходит после обуче- ния все тесты на байесовское принятие решений, которые исследователь применил бы к человеку. 15.9. Рекомендуемая литература Wendy J. Adams. A Common Light ­Prior for Visual Search, Shape, and Reﬂectance Judgments. Journal of Vision 7, no. 11 (2007). John R. Anderson. The Adaptive Nature of Human Categorization. Psychological Review 98, no. 3 (1991): 409–429. John R. Anderson. The Adaptive Character of Thought. New York: Psychology Press, 2013. Jeffrey S. Bowers and Colin J. Davis. Bayesian Just ­so Stories in Psychology and Neuroscience. Psychological Bulletin 138, no. 3 (2012): 389–414. Nick Chater, Noah Goodman, Thomas L. Griffiths, Charles Kemp, Mike Oaksford, and Joshua B. Tenenbaum. The Imaginary Fundamentalists: The Unshocking Truth about Bayesian Cognitive Science. Behavioral and Brain Sciences 34, no. 4 (2011): 194–196.\n--- Страница 381 ---\n380  Байесовские модели в контексте Nick Chater and Mike Oaksford, eds. The Probabilistic Mind: Prospects for Bayesian Cognitive Science. New York: Oxford University Press, 2008. Stephanie Y. Chen, Brian H. Ross, and Gregory L. Murphy. Implicit and Explicit Processes in Category­Based Induction: Is Induction Best When We Don’t Think? Journal of Experimental Psychology: General 143, no. 1 (2014): 227–246. Jerry A. Fodor. Psychological Explanation: An Introduction to the Philosophy of Psychology. New York: Random House, 1968. Monica Gori, Michele Del Viva, Giulio Sandini, and David C. Burr. Young Children Do Not Integrate Visual and Haptic Form Information. Current Biology 18, no. 9 (2008): 694–698. Gerd Gigerenzer. On Narrow Norms and Vague Heuristics: A Reply to Kahneman and Tversky. Psychological Review 103, no. 3 (1996): 592–596. Matt Jones and Bradley C. Love. Bayesian Fundamentalism or Enlightenment? On the Explanatory Status and Theoretical Contributions of Bayesian Models of Cognition. Behavioral and Brain Sciences 34, no. 4 (2011): 169–188. Daniel Kahneman. Thinking, Fast and Slow. New York: Macmillan, 2011. Charles Kemp, Andrew Perfors, and Joshua B. Tenenbaum. Learning Over hy­ po theses with Hierarchical Bayesian Models. Developmental Science 10, no. 3 (2007): 307–321. Falk Lieder and Thomas L. Griffiths. Resource­Rational Analysis: Understanding Human Cognition as the Optimal Use of Limited Computational Resources. Beha- vioral and Brain Sciences 43, no. e1 (2020): 1–60. Wei Ji Ma. Organizing Probabilistic Models of Perception. Trends in Cognitive Sciences 16, no. 10 (2012): 511–518. Wei Ji Ma and Mehrdad Jazayeri. Neural Coding of Uncertainty and Probability. Annual Review of Neuroscience 37 (2014): 205–220. Laurence T. Maloney and Pascal Mamassian. Bayesian Decision Theory as a Model of Human Visual Perception: Testing Bayesian Transfer. Visual Neuroscience 26, no. 1 (2009): 147–155. Barbara Mellers, Ralph Hertwig, and Daniel Kahneman. Do Frequency Repre­ sentations Eliminate Conjunction Effects? An Exercise in Adversarial Collaboration. Psychological Science 12, no. 4 (2001): 269–275. Daniel J. Navarro and Amy F. Perfors. Similarity, Feature Discovery, and the Size Principle. Acta Psychologica 133, no. 3 (2010): 256–268. Dobromir Rahnev and Rachel N. Denison. Suboptimality in Perceptual Decision Making. Behavioral and Brain Sciences 41 (2018): e223. Julia Trommershäuser, Laurence T. Maloney, and Michael S. Landy. Decision Making, Movement Planning and Statistical Decision Theory. Trends in Cognitive Sciences 12, no. 8 (2008): 291–297. Edward Vul, Noah Goodman, Thomas L. Griffiths, and Joshua B. Tenenbaum. One and Done? Optimal Decisions from Very Few Samples. Cognitive Science 38, no. 4 (2014): 599–637. Louise Whiteley and Maneesh Sahani. Implicit Knowledge of Visual Uncertainty Guides Decisions with Asymmetric Outcomes. Journal of Vision 8, no. 3 (2008): 1–15. Fei Xu and Joshua B. Tenenbaum. Word Learning as Bayesian Inference. Psy- cho logical Review 114, no. 2 (2007): 245–272.\n--- Страница 382 ---\nПриложение А Обозначения Распределения вероятностей Обозначения в области теории вероятностей могут сбить с толку. В основе путаницы лежит соглашение об использовании одной и той же буквы p для любого распределения вероятностей, даже если оно связано с разными слу - чайными величинами. Чтобы различать распределения, математики исполь- зуют нижний индекс для обозначения случайной величины, которой при- надлежит распределение вероятностей. Например, в pX(x) X – это случайная величина, а x – это значение, которое она принимает. Данное обозначение используется как для функций распределения масс вероятности, так и для функций распределения плотности вероятности, хотя в некоторых статьях для функций распределения плотности используется f вместо p . К сожалению, формулы с использованием индексов быстро становятся громоздкими и запутанными. Например, «правильным» способом записи правила Байеса было бы уравнение (A.1) которое перегружено обозначениями. Поэтому мы используем собственные соглашения, за исключением главы 3 и приложения B, где применяем строгие формальные обозначения. Когда мы описываем распределение вероятностей случайных величин с помощью общих значений (в отличие от конкретных чисел), то опускаем нижний индекс, а на переменную указывает аргумент распределения. Таким образом, мы пишем p(x), p(x, y) и p(x | y) вместо pX(x), pX,Y(x, y) и pX|Y(x | y). Если значение конкретное, а не общее, то нам нужно указать и случайную переменную, и ее значение. Для дискретных случайных величин мы исполь- зуем интуитивно понятные выражения, такие как p(X = 3), p(X = 3, Y = 2) и p(X = 3 | Y = 2) вместо pX(3), pX,Y(3, 2) и pX|Y(3 | 2). В некоторых публикациях используется заглавная буква P или «Pr» для обозначения вероятности собы- тия или утверждения, например P (X = 3) или Pr(X = 3). Мы этого не делаем.\n--- Страница 383 ---\n382  Обозначения В случае непрерывных случайных величин использование такой записи проблематично. Для непрерывной случайной величины X мы не можем за- менить pX(3) на p(X = 3), поскольку pX является функцией распределения плотности вероятности, а не массы вероятности, и, следовательно, p(X = a) будет равно 0 для любого а (см. раздел B.5.4). В таких случаях мы сохраняем нижний индекс: пишем pX(3), pX,Y(3, y) и pX|Y(3 | y), где X – непрерывная, а Y – любая случайная величина. Но если конкретное значение непрерывной пере- менной является обусловленным, мы можем использовать равенство после знака вертикальной черты, например мы пишем p(x | Y = 2) вместо pX|Y(x | 2) независимо от типов X и Y. В общем, мы надеемся, что эти соглашения обеспечивают правильный баланс между удобочитаемостью и математической точностью. Математическое ожидание Теперь обсудим обозначения математического ожидания. 1. Если f(X) является функцией только одной случайной величины X, то мы используем �[f(X)] для обозначения математического ожидания или среднего значения f(X) по отношению к p(X), то есть (дискретный случай) или (непрерывный случай). 2. Если f(X, Y, …) является функцией двух или более случайных пере- менных, то мы используем нижние индексы, чтобы указать, по каким переменным находим математическое ожидание. Например, в непре- рывном случае следующие значения отличаются: (A.2) (A.3) 3. При использовании условного распределения для расчета математи- ческого ожидания мы пишем случайную переменную, которая служит условием, в нижнем индексе, а значение этой переменной – в квадрат - ных скобках. Например, в непрерывном случае (A.4) где y может быть общим или конкретным значением. Если y являет - ся общим значением, мы можем опустить нижний индекс и написать �[f(x) | y] вместо �X|Y[f(X) | y]. 4. Условные обозначения для дисперсии и стандартного отклонения та- кие же, как и для математического ожидания. Обозначим их через Var[·] и Std[·] соответственно.\n--- Страница 384 ---\nРекомендуемая литература  383 Гауссово распределение Для гауссова (нормального) распределения мы используем обозначение � с параметрами среднего значения и дисперсии: (A.5) Предостережение: многие программные пакеты в качестве параметра своего встроенного нормального распределения используют стандартное отклонение, а не дисперсию. Примечание по терминологии: несмотря на то что среднее значение, дисперсия и стандартное отклонение являются свойствами случайных ве- личин, говорить «дисперсия величины X при ее апостериорном распределе- нии» может быть неудобно. Вместо этого мы часто используем обычное бо- лее короткое выражение «дисперсия апостериорного распределения», когда в отношении переменной нет двусмысленности.\n--- Страница 385 ---\nПриложение B Основы теории вероятностей Алгебра помогает понять математику, а теория вероятностей помогает по- нять жизнь. К сожалению, теория вероятностей не является стандартным компонентом большинства учебных программ бакалавриата. Мы надеемся, что это изменится, а пока предлагаем ознакомиться с некоторыми основами теории вероятностей. Это ни в коем случае не исчерпывающее введение, которое можно найти в учебнике по теории вероятностей. Это краткий спра- вочник, который сосредоточен только на концепциях и расчетах, использу - емых в книге. B.1. Объективная и субъективная вероятности Вероятность – это степень возможности. В самом узком смысле вероятность можно определить как ожидаемую частоту исхода повторяющегося события, например вероятность того, что монета выпадет орлом, или вероятность того, что кто-то выбросит 5 на игральном кубике. Эти события могут повто- ряться произвольно большое количество раз, и можно подсчитать частоты долгосрочных результатов. Если доля подбрасываний, при которых моне- та выпадает орлом, сходится к 0.5, по мере того как число подбрасываний приближается к бесконечности, мы можем утверждать, что монета имеет вероятность выпадения орла, равную 0.5. Этот тип вероятности иногда на- зывают объективной вероятностью, и это единственный допустимый тип вероятности в соответствии со строгим частотным взглядом на вероятность. Намного более широкое и, как мы полагаем, более полезное толкование вероятности – это степень веры в возможность исхода. Иногда ее называют субъективной вероятностью. Повседневные термины «уверенность» и «не- определенность» относятся к субъективным вероятностям. Если я знаю, что у кубика шанс выпадения 5 равен 1/6, то моя уверенность в том, что выпадет\n--- Страница 386 ---\nСмысловое толкование вероятности  385 5, равна 1/6. Этот конкретный пример тривиален, потому что он основан на простом преобразовании объективной вероятности (частоты долгосроч- ных результатов) в утверждение убеждения. Однако гораздо более широкая применимость субъективного толкования становится очевидной, когда мы рассматриваем степень уверенности в результатах, которые невозможно повторить, например вероятность того, что кандидат А победит кандидата B на предстоящих выборах. Это не та вероятность, которую можно полу - чить, повторяя одно и то же событие много раз, но тем не менее мы можем иметь надежное предсказание относительно результата. В самом деле, по- вседневная жизнь изобилует примерами субъективных вероятностей, кото- рые нельзя сформулировать как частоты долгосрочных результатов: какова вероятность того, что сегодня пойдет дождь? Какова вероятность того, что мне понравится книга? Многие научные вопросы также можно сформули- ровать только в терминах субъективных, а не объективных вероятностей: какова вероятность того, что масса Сатурна находится между 1025 и 1026 кг? Какова вероятность того, что болезнь X вызвана вирусом? Различие между объективной и субъективной вероятностями имеет дол- гую историю [41] и не всегда очевидно. Например, чтобы определить веро- ятность того, что сегодня пойдет дождь, синоптик может запустить большое количество симуляций, начиная с текущего состояния атмосферы, каждая из которых использует разные экземпляры стохастических факторов в модели, и записать частоту дождливых дней в этой последовательности результатов запуска модели. Хотя результирующая вероятность является субъективной, она была получена «объективным» способом, а именно путем подсчета. Точ- но так же, если я наблюдаю на небе темные тучи и высказываю свое мнение о высокой вероятности дождя, я выражаю субъективное вероятностное суж - дение, но основываю это суждение на большом количестве ранее наблюдав- шихся, подобных (хотя и неидентичных) состояний неба. Байесовский вывод рассматривает субъективные и объективные вероят - ности одинаково: одни и те же математические отношения (правило Байеса, маргинализация и т. д.) одинаково применимы к обоим типам вероятностей. Поэтому байесовский вывод находит широкое применение. Однако байесов- ские модели восприятия основаны в основном на субъективной вероятности: существует только одно истинное состояние мира, но с точки зрения орга- низма, пытающегося сделать вывод, существует множество возможностей, и этим возможностям можно присвоить степени уверенности. B.2. Смысловое толкование вероятности Мы называем множество всех рассматриваемых возможностей простран- ством событий. Событие или гипотеза – это подмножество пространства событий. Термин «событие» обычно используется при обсуждении объек - тивной вероятности, а термин «гипотеза» – при обсуждении субъективной вероятности. Пространством событий могут быть «все возможные числа, которые я могу выбросить на кости» или «все возможные погодные условия,\n--- Страница 387 ---\n386  Основы теории вероятностей которые могут наблюдаться сегодня». Относительно первого пространства событием может быть выпадение четного числа. Относительно второго про- странства можно рассматривать, например, гипотезу «сегодня будет дождь». Вероятность события или гипотезы – это вещественное число от 0 до 1, ука- зывающее степень вероятности события или гипотезы. Событие, которое гарантированно случается, имеет вероятность 1, а невозможное событие имеет вероятность 0. Для событий вероятность можно представить как час - тоту, с которой событие происходит среди очень большого числа случайных выборок из пространства. Например, вероятность выпадения четного числа на шестигранной кости равна 0.5. Как было сказано ранее, эту вероятность можно также рассматривать как степень уверенности. К гипотезам понятие частоты обычно не применяется, но вероятность по-прежнему представля- ет собой степень уверенности; например, степень уверенности в том, что сегодня будет дождь, может быть равна 0.35. Мы обозначаем вероятность события или гипотезы X через p(X), как это принято в литературе по теории вероятностей. Например, p (монета выпадет орлом) = 0.50. B.3. Дополняющее событие Для рассматриваемого события или гипотезы дополняющим событием или гипотезой является сценарий, при котором первое событие или гипотеза не имеет места или является ложным. Например, дополняющим событием к вы- падению 1 на кубике является событие «выпадение 2, 3, 4, 5 или 6 на кубике». Если событие или гипотеза обозначены X, то их дополняющее событие или гипотеза обозначаются ¬X (читается «не X»). Здесь ¬ – символ логического отрицания. Вероятность дополняющего события или гипотезы равна 1 минус вероятность основного события или гипотезы: p(¬X) = 1 - p(X). (B.1) В некоторых ситуациях легче рассчитать вероятность дополняющего со- бытия или гипотезы, чем самого события или гипотезы. Например, если вас просят вычислить вероятность того, что сумма очков на двух игральных костях не меньше 3, проще сначала вычислить вероятность того, что сумма очков меньше 3, и вычесть ее из 1 (ответ 35). B.4. Представление в виде диаграммы Венна События и гипотезы могут быть представлены графически с помощью диа- грамм Венна (рис. B.1). (Строго говоря, площадные диаграммы, которые мы показываем в этой книге, являются диаграммами Эйлера, поскольку они\n--- Страница 388 ---\nСлучайные величины и их распределения  387 представляют только отношения, которые действительно имеют место; стро- го определенные диаграммы Венна представляют все возможные логические отношения между множествами. Однако мы следуем общепринятому согла- шению называть любые площадно-вероятностные диаграммы диаграммами Венна.) Сначала нарисуем большой прямоугольник, внутренняя часть кото- рого представляет все возможные результаты, то есть пространство выборки. Присвоим площади этого прямоугольника значение 1, представляющее сум- марную вероятность 1. Затем нарисуем внутри этого прямоугольника круг, представляющий все результаты, согласующиеся с конкретным событием или гипотезой. Все люди Студенты Рис. B.1  Пример диаграммы Венна, используемой для представления вероятностей Например, прямоугольник может представлять всех людей в определен- ной группе, а круг – всех студентов среди них. Площадь, заключенная в круг, является частью площади, заключенной в прямоугольник; эта доля представ- ляет собой вероятность события или гипотезы – в нашем примере вероят - ность того, что случайно выбранный человек является студентом. Дополняю- щее событие или гипотеза представлено точками, которые находятся внутри прямоугольника, но снаружи круга. Его площадь, деленная на площадь пря- моугольника, представляет собой вероятность того, что случайно выбранный человек в группе не является студентом. B.5. Случайные величины и их распределения Случайная величина – это переменная, значения которой не могут быть из- вестны с уверенностью. Примеры случайной величины: число, выпавшее на кубике, дата рождения человека, размер обуви случайного человека на вашей улице, время, которое требуется, чтобы добраться от дома до работы,\n--- Страница 389 ---\n388  Основы теории вероятностей количество избирателей, которые будут участвовать в предстоящих выборах, или цена акции завтра. Противоположностью случайной величине являет - ся переменная, значение которой достоверно известно. Примерами неслу - чайных величин являются количество планет между нами и Солнцем (две), количество дней в неделе (семь), отношение длины окружности к диаметру круга (π ) и расстояние между двумя соседними сантиметровыми отметками на линейке (1 см). Это не абсолютное различие. Переменные, которые кажутся неслучайны- ми, могут быть подвержены шуму производства или измерения, что делает их случайными. Например, расстояние между двумя соседними отметками на линейке может слегка различаться, поскольку машина, производящая линейки, вероятно, была запрограммирована компьютером, который уста- навливал отметки в сантиметрах. Однако числа, сгенерированные компью- тером, имеют только конечное число знаков после запятой, возможно десять. Как следствие сантиметровые отметки никогда не достигнут фемтометро- вой точности. Кроме того, краска, используемая для маркировки, не будет прилипать к поверхности одинаковым образом при изготовлении каждой линейки. Следовательно, расстояние между двумя соседними метками мож - но рассматривать как случайную величину. По причинам, подобным этим, может быть полезно рассматривать все переменные как случайные, только имеющие очень низкую неопределенность. Случайность, также называемая изменчивостью, шумом или стохастично- стью, часто является следствием недостатка знаний. Если бы перед броском кубика вам удалось каким-то образом записать точное положение, направ- ление и скорость, с которой кость покидает руку, то вы смогли бы точно смо- делировать взаимодействие кости с воздухом и столом и предсказать исход броска. Поскольку никому неизвестны значения всех этих переменных, ре- зультат броска кубика считается случайным. Существует ли истинная случай- ность – это философский вопрос, который выходит за рамки нашей книги. B.5.1. Сравнение дискретных и непрерывных случайных величин Случайные переменные можно классифицировать по значениям, которые они могут принимать. Наиболее существенное различие наблюдается между дискретными и непрерывными случайными величинами. Дискретная случайная величина принимает набор значений, которые можно посчитать, даже если их может быть бесконечно много. Примерами могут служить количество детей в семье, количество потенциалов действия, запускаемых нейроном, количество ходов в шахматной партии, возраст че- ловека, если его считать целыми годами, и идентичность произнесенного слова. Дискретная случайная величина, принимающая только два возмож - ных значения, называется бинарной. Непрерывная случайная величина принимает значения в континууме. Примерами являются длина отрезка, направление, в котором можно идти\n--- Страница 390 ---\nСлучайные величины и их распределения  389 в открытом поле, время ожидания перед красным светом, скорость автомо- биля и частота звука. Непрерывную переменную можно рассматривать как дискретную, но со значениями, поступающими с бесконечно малыми при- ращениями. Например, расстояние является непрерывной переменной, но когда его измеряют в миллиметрах, оно является дискретной переменной. В компьютерной программе истинно непрерывных переменных не сущест - вует; они всегда дискретизированы с конечной точностью. B.5.2. Суммарная вероятность всегда равна 1 Суммарная вероятность всех возможных значений случайной величины рав- на 1. Эта сумма является отражением того факта, что возможности взаимо- исключающие. Если увеличить вероятность одного значения, то вероятность хотя бы одного другого значения должна уменьшиться. B.5.3. Дискретные распределения вероятности Дискретные распределения вероятности – это функции, которые присваи- вают вероятность каждому возможному значению дискретной случайной величины. Распределение вероятности по дискретной случайной величине также называется функцией распределения массы вероятности. Дискретная случайная величина X, принимающая определенное значение x, является событием, обозначаемым p(X = x). Поскольку теперь мы варьируем x по всем его возможным значениям, мы получаем функцию от x. Это функция распре- деления массы вероятности, которую мы будем обозначать через pX(x) или через p(x), если нет двусмысленности в отношении идентичности случайной величины: pX(x) = p(X = x). (B.2) Это равенство означает, что функция распределения массы вероятности, вычисленная в точке x, равна вероятности того, что случайная величина при- мет это значение. Мы используем индекс X (верхний регистр) для обозначе- ния случайной величины, а аргумент x (нижний регистр) – для конкретного значения этой случайной величины. Термин «масса» заимствован из физики. Грубо говоря, он основан на использовании материи как метафоры возмож - ности (точки в пространстве событий). Чем больше вероятность события или гипотезы, тем больше масса «материи». Например, пусть случайная величина X – это число, выпавшее на играль- ном кубике. Его возможные значения x равны от 1 до 6. Если кубик чест - ный (шансы выпадения всех граней равны), вероятность каждого из этих значений равна 1/6, то есть p(X = x) = 1 для всех x. Это пример дискретного равномерного распределения. Если кубик «нечестный», вероятность хотя бы двух значений будет отличаться от 1/6, и распределение больше не будет однородным.\n--- Страница 391 ---\n390  Основы теории вероятностей Для дискретных случайных величин общая вероятность вычисляется пу - тем суммирования всех возможных значений и в итоге должна быть равна 1. Это свойство записывается следующим образом: (B.3) В конкретном случае, когда даны возможные значения X, мы можем по- ставить их выше и ниже знака å. Например, общая вероятность броска кости будет равна (B.4) Бинарные случайные величины являются частным случаем дискретных случайных величин. Предположим, что бинарная случайная величина X может принимать значения x1 и x2. Мы знаем, что общая вероятность рав- на 1. Следовательно, вероятность x2 равна 1 минус вероятность x1, то есть p(X = x2) = 1 - p(X = x1). B.5.4. Непрерывные распределения вероятности Какова вероятность того, что рост человека точно равен 160 см? Она равна нулю, поскольку «точно» означает, что рост указан с точностью до бесконеч- ного числа знаков после запятой. Эта проблема характерна для непрерывных случайных величин и говорит о том, что функции распределения массы ве- роятности, которые хорошо работали для дискретных распределений, долж - ны быть заменены другим понятием, чтобы учесть свойства непрерывных переменных. Предположим, нас интересует распределение вероятности роста взрослого человека (рис. B.2). В первом приближении можно рассмотреть возможные значения в интервалах с шагом 10 см: от 120 до 130 см, от 130 до 140 см и т. д. С каждым интервалом связана вероятность, и таким образом мы можем по- строить функцию распределения масс вероятностей. Однако может возник - нуть необходимость описать рост более точно, скажем в интервалах по 5 см: между 122.5 и 127.5 см и т. д. Таким образом, каждый исходный интервал заменяется двумя новыми, каждый из которых имеет в среднем половину вероятностной массы исходного интервала. Таким образом, новая функция распределения массы вероятности масштабируется примерно до половины высоты исходной (рис. B.2). Поскольку мы продолжаем уменьшать ширину интервала, чтобы повысить точность, вероятностная масса на интервал так - же продолжает уменьшаться – она может стать сколь угодно малой. Это не очень хорошо. Есть ли способ предотвратить «исчезновение» массы вероят - ности? Да, это можно сделать, разделив массу вероятности в интервале на ширину интервала. При этом функция несильно меняется по мере уменьше- ния ширины интервала – она становится только более точной. Результатом этого процесса, когда ширина ячейки приближается к нулю (зеленая кривая),\n--- Страница 392 ---\nСлучайные величины и их распределения  391 является функция плотности вероятности. Опять же, здесь просматривает - ся аналогия с физикой: если вероятность в интервале рассматривается как масса, то деление этой вероятности на ширину интервала аналогично вы- числению линейной плотности: масса на единицу длины оси x. Размер интервала = 10 Размер интервала = 5 Размер интервала = 2.50.3 0.2 0.1 0.0 0.3 0.2 0.1 0.0 0.3 0.2 0.1 0.00.03 0.02 0.01 0.00 0.03 0.02 0.01 0.00 0.03 0.02 0.01 0.00125 135 145 155 165 175 185 195 205 215 225 125 135 145 155 165 175 185 195 205 215 225 125 135 145 155 165 175 185 195 205 215 225125 135 145 155 165 175 185 195 205 215 225 125 135 145 155 165 175 185 195 205 215 225 125 135 145 155 165 175 185 195 205 215 225Функция распределения масс вероятностей Функция распределения плотности вероятностей Рис. B.2  Если случайная величина может принимать континуум значений (ось x), функция распределения массы вероятности определена только тогда, когда результаты разбиты по ин- тервалам. Значения функции будут уменьшаться с уменьшением размера интервала (левый столбец). Функции распределения плотности вероятности получаются путем деления значе- ний массы вероятности на размер интервала. Это дает значения, которые не зависят от разме- ра интервала. Процесс уменьшения размера интервалов можно продолжать до тех пор, пока они не станут бесконечно малы. Получится непрерывная функция распределения плотности вероятности, показанная зеленым цветом Сходство в обозначениях между функцией распределения массы вероят - ности для дискретных переменных и функцией распределения плотности вероятности для непрерывных переменных, которые записываются как pX(x) или p(x), вводит в заблуждение, поскольку между ними есть некоторые важ - ные концептуальные различия. Для дискретного распределения масса веро- ятности отдельной точки никогда не превышает 1, поскольку значения массы вероятности должны в сумме равняться 1. Для непрерывного распределения плотность вероятности в одной точке не имеет смысла и может принимать сколь угодно большие значения. Рассмотрим, например, равномерное рас - пределение на интервале [0, 0.01]. Оно будет иметь плотность вероятно - сти 100 в каждой точке. Только интеграл по интервалу всегда будет меньше или равен 1. Иными словами, для дискретного распределения вероятность p(X = x) – это значимое число, которое может принимать любое значение от 0 до 1, и на самом деле тождественна р(х). Для непрерывного распределения p(X = x) всегда равна 0, и имеют смысл только вероятности вида p(a £ X £ b) с произвольными числами a и b. Например, вероятность того, что вы попаде- те абсолютно точно в центр мишени бесконечно тонким дротиком, равна 0.\n--- Страница 393 ---\n392  Основы теории вероятностей Мы будем использовать такие термины, как функция распределения веро- ятности (probability distribution function, PDF), распределение вероятности или просто распределение для обозначения функции распределения массы вероятности дискретной случайной величины или функции распределения плотности вероятности непрерывной случайной величины. Так же, как и для дискретных переменных, полная вероятность всех значений непрерывной переменной равна 1. Полная вероятность для не- прерывной переменной вычисляется не как сумма, а как интеграл. Интег - рал непрерывной функции плотности вероятности, как определено выше, представляет собой ширину интервала, умноженную на значение функции в этом интервале и суммированную по всем интервалам в пределе, когда ширина интервала приближается к нулю. В алгебре существуют методы вычисления интегралов определенных функций. В этом приложении мы познакомимся с различными интегралами по функциям плотности вероят - ности, особенно потому, что они непосредственно перекликаются с выра- жениями суммы по функциям распределения массы вероятностей; однако мы не будем вычислять эти интегралы, поэтому алгебраические действия нам не понадобятся. Правило полной вероятности для непрерывной пере- менной X записывается как (B.5) По сути, dx – это ширина очень маленького интервала (приращения), а знак интеграла – растянутая буква S от слова «сумма». Наиболее важным непрерывным распределением является нормальное распределение, которое мы подробно обсудим ниже. Другим важным распре- делением является равномерное распределение. Равномерное распределение имеет постоянную плотность распределения на интервале [a , b]: (B.6) Существуют и другие распределения, широко распространенные в при- ложениях теории вероятностей. Экспоненциальное распределение задается выражением p(x) = λe-λx, где λ – константа, а x представляет собой положи- тельное вещественное число. Распределение по степенному закону задается как p(x) µ x-a, где a является константой, а x снова представляет собой по- ложительное вещественное число. B.5.5. Формальное определение функции плотности вероятности Рассмотрим непрерывную случайную величину X, такую как, например, время ожидания в очереди. Вероятность того, что значение X меньше или равно x, обозначается как p(X £ x). Это интегральная функция распределения (cumulative distribution function, CDF) X в точке x , обозначаемая PX(x):\n--- Страница 394 ---\nСлучайные величины и их распределения  393 PX(x) = p(X £ x). (B.7) По определению это монотонно возрастающая функция, принимающая значения между 0 (при приближении x к 0) и 1 (при приближении x к ¥). PDF X теперь является производной этой функции: (B.8) На рис. B.3 показан пример CDF и PDF. Для дискретных случайных величин CDF можно определить таким же образом, но это не является обязательным шагом при определении функции распределения массы вероятностей. Вероятность Интегральная вероятность –2 –2 –1 –1 0 0 1 1 2 2 Риc. B.3  Функция плотности вероятности и интегральная функция распределения Чтобы вернуться из PDF в CDF, мы интегрируем: (B.9) (Обозначение pX(y) не является опечаткой. Оно означает плотность вероят - ности случайной величины X, оцененную при значении y фиктивной пере- менной.) Физический смысл этого выражения состоит в том, что интеграл по плотности представляет собой массу. Отсюда сразу следует, что вероятность того, что X принимает значения в интервале (x1, x2), может быть получена путем интегрирования pX(x) между x1 и x2: (B.10) Из определения также следует, что B.5.6. Нормирование Функцию можно превратить в распределение вероятностей, разделив каж - дое значение на общее значение во всей области, при условии что это общее\n--- Страница 395 ---\n394  Основы теории вероятностей значение конечно. В результате распределение вероятностей будет интег - рироваться (или суммироваться) до 1. Этот процесс называется нормиро- ванием. Если общее значение во всей области бесконечно, нормирование невозможно. Упражнение B.1. Докажите, что экспоненциальное распределение p(x) = λe-λx нормировано. Упражнение B.2. Попробуйте нормировать степенное распределение p(x)x-a и найти условие для a , при соблюдении которого возможно нормирование. B.6. Среднее значение, дисперсия и математическое ожидание Для дискретной случайной величины X среднее или ожидание X равно (B.11) Дисперсия, которая является мерой разброса вокруг среднего, определя- ется как (B.12) Стандартное отклонение – это квадратный корень из дисперсии. Среднее и дисперсия являются частными случаями математического ожидания лю- бой функции случайной величины. Если мы обозначим функцию через f, то ожидание f равно (B.13) Таким образом, среднее значение является ожиданием X, а дисперсия рав- на Var[X ] = �[(x - �[X])2]. Для непрерывной случайной величины X с плотностью вероятности p(x) аналогичные выражения получаются заменой сумм интегралами: (B.14) (B.15) (B.16)\n--- Страница 396 ---\nНормальное распределение  395 Как для дискретных, так и для непрерывных переменных существует об- щее эквивалентное выражение для дисперсии, а именно разность между средним значением квадратов и квадратом среднего: Var[X ] = �[X]2 - �[X]2. (B.17) Упражнение B.3. Докажите это. B.7. Нормальное распределение B.7.1. Определение Наиболее важным непрерывным распределением в приложениях теории ве- роятностей является нормальное, или гауссово, распределение. Его функция распределения вероятностей имеет вид: (B.18) Иногда мы используем сокращенную запись p(x) = �(x; μ, σ2). Параметры μ и σ2 не имеют априорного значения (они просто обозначены как предпо- ложение), но оказываются равными среднему и дисперсии распределения соответственно (см. задачу B.9). Множитель нужен для нормирования. Стандартное нормальное распределение – это нормальное распределение со средним значением 0 и стандартным отклонением 1. B.7.2. Центральная предельная теорема Важность нормального распределения вытекает главным образом из цент­ ральной предельной теоремы. Говоря упрощенно, центральная предельная теорема утверждает, что среднее значение большого числа независимых случайных величин с идентичными распределениями вероятностей будет следовать приблизительно нормальному распределению, независимо от распределения исходных переменных. Вся мощь теоремы заключается в ее последней части: распределение исходных переменных не имеет значения. Ограничение теоремы можно смягчить, чтобы учесть независимые, но не одинаково распределенные переменные. В математических моделях восприятия центральная предельная теорема всегда присутствует на заднем плане: всякий раз, когда мы предполагаем, что шум, искажающий стимул, распределен нормально, мы, по сути, обо- сновываем это с помощью центральной предельной теоремы. Случайная величина, которая описывает шум, искажающий стимул, может быть суммой большого числа независимых шумовых процессов.\n--- Страница 397 ---\n396  Основы теории вероятностей B.7.3. Произведение двух нормальных распределений Рассмотрим произведение двух гауссовых распределений вероятностей по одной и той же случайной величине X. У одного распределения среднее μ1 и дисперсия σ2 1, а у другого среднее μ2 и дисперсия σ2 2. Перемножим эти два распределения так же, как мы умножаем обычные функции, а затем норми- руем результат (поскольку произведение не нормализуется автоматически). Каким будет итоговое распределение вероятности? Сначала запишем выра- жения для двух функций плотности вероятности: (B.19) (B.20) Умножение этих двух функций сводится к суммированию показателей степени. Мы сделаем это в первую очередь: Cумма показателей степени (B.21) (B.22) Выполним перестановку, сгруппировав все члены, содержащие x2, и все содержащие x : Cумма показателей степени (B.23) (B.24) где мы используем следующие обозначения точности: (B.25) (B.26)\n--- Страница 398 ---\nНормальное распределение  397 Кроме того, здесь и далее точками обозначены все слагаемые, не зави- сящие от x. При возведении в степень эти члены становятся постоянными множителями, не зависящими от x. Поскольку результат перемножения рас - пределений должен быть нормирован в конце вычислений, любой постоян- ный множитель, который мы добавляем или оставляем до этого момента, не имеет значения. Сумма показателей степени может быть записана как Cумма показателей степени (B.27) (B.28) Таким образом, произведение распределений в уравнении (B.20) равно (B.29) (B.30) где знак пропорциональности поглощает все множители, не зависящие от x. Мы рассматриваем результат как другое нормальное распределение, теперь со средним значением и дисперсией Упражнение B.4. Какова правильная нормировочная константа в уравнении (B.30)? B.7.4. Произведение нескольких нормальных распределений Обобщим предыдущий раздел на N нормальных распределений. Это обобще- ние используется в задаче 10.7. Рассмотрим набор из N нормальных распре- делений по одной и той же переменной x. i-е распределение имеет среднее значение μi и дисперсию σ2 i. Произведение (ненормализованное) этих рас - пределений равно (B.31)\n--- Страница 399 ---\n398  Основы теории вероятностей B.7.5. Интегральное нормальное распределение Интегральная функция гауссова распределения не является элементарной функцией (то есть функцией, построенной из композиции таких элемен- тарных функций, как степенные с любым действительным показателем, по- казательные и логарифмические, тригонометрические и обратные тригоно- метрические с использованием операций сложения, вычитания, умножения и деления). Однако у нее есть специальное название просто потому, что она часто встречается. Мы определяем интегральное распределение стандартной нормальной плотности вероятности как (B.32) Эта функция принимает значения от 0 до 1. Упражнение B.5. Покажите, что Φstandard (0) = 0.5. Упражнение B.6. Покажите, что интеграл гауссова распределения может быть выражен через интегральное нормальное распределение следующим об- разом: (B.33) Это возможно, потому что любое нормальное распределение можно сдви- нуть (на μ) и нормировать (путем деления на σ) для получения стандартного нормального распределения. Полезны два других интеграла гауссова распределения: (B.34) (B.35) B.7.6. Распределение фон Мизеса Некоторые переменные, такие как ориентация и направление движения, естественным образом имеют круговую (периодическую) область. Для таких переменных гауссово распределение не имеет смысла. Одним из решений является выбор распределения фон Мизеса (рис. B.4). Его можно рассматри- вать как круговой аналог гауссова распределения. Оно имеет два парамет - ра: круговое среднее и параметр концентрации, аналогичный обратной ве- личине дисперсии гауссова распределения. Распределение фон Мизеса по круговой переменной s с областью определения [0, 2π ), круговым средним μ и параметром концентрации κ имеет вид:\n--- Страница 400 ---\nДельта­функция  399 (B.36) Здесь I0 – модифицированная функция Бесселя первого рода порядка 0. Это так называемая специальная функция – определяемая через интеграл или бесконечный ряд. Ее точное определение здесь не важно; для нас важно лишь то, что 2π I0(κ) нормирует распределение фон Мизеса. Когда κ = 0, I0(κ) = 1, и распределение Мизеса становится равномерным на окружности. Чем выше κ, тем больше распределение фон Мизеса похоже на нормальное распреде- ление. Это показано на рис. B.4. Плотность вероятности1.0 0.8 0.6 0.4 0.2 0.0κ = 0 κ = 2 κ = 4 κ = 8 –π π –π/2 0 π/2 Рис. B.4  Примеры распределений фон Мизеса. Область определения случайной величины является периодической Упражнение B.7. Мы сказали, что μ – это круговое среднее, но как бы вы опре- делили среднее значение для круговой переменной? Упражнение B.8. Покажите аналитически, что в пределе больших κ распре- деление фон Мизеса становится нормальным распределением. (Подсказка: используйте разложение косинуса в ряд Тейлора.) Кроме того, покажите, что точность этого нормального распределения равна κ . B.8. Дельта-функция Случайная величина особого типа, с которой мы будем сталкиваться довольно часто, – это случайная величина, принимающая только одно возможное зна- чение. Существует специальное обозначение распределения вероятностей та - кой случайной величины. Если X – непрерывная случайная величина, которая всегда принимает значение X = a, то ее распределение можно записать так: p(x) = δ(x - a). (B.37) Здесь δ – дельта­функция Дирака. Она возвращает 0, если аргумент не ра- вен 0, а в ином случае возвращает бесконечность. Конечно, бесконечность\n--- Страница 401 ---\n400  Основы теории вероятностей не является числом, и поэтому дельта-функция Дирака, строго говоря, не является обычной функцией. Для нас это не имеет значения, поскольку един- ственное место, где мы будем использовать дельта-функцию, – внутри интег - рала. Там любая функция f (x) обладает следующим свойством: (B.38) где мы предполагаем, что a входит в область интегрирования. Фактически уравнение (B.38) является определяющим свойством дельты Дирака. Дель- та-функция равносильна вычислению функции f внутри интеграла в одной точке a . Нам представляется удобным использовать для дискретных переменных те же обозначения, что и для непрерывных, т. е. писать δ(x - a) вместо δxa. Тогда дискретный аналог уравнения (B.38) имеет вид (B.39) B.9. Распределение Пуассона Дискретное распределение вероятностей, которое мы используем для опи- сания нейронной активности (раздел 14.1), – это распределение Пуассона. Возможные значения случайной величины Пуассона: 0, 1, 2, 3, … (верхний предел отсутствует). Распределение Пуассона имеет свободный параметр, который мы будем называть λ. Распределение вероятностей X определяется выражением (B.40) где x! = 1 · 2 · 3  x – операция факториала. Распределение Пуассона являет - ся дискретным и определяется только целыми неотрицательными числами. Множитель e-λ действует как нормировочный. В отличие от x, λ не обязатель- но должно быть целым числом. Среднее значение и дисперсия переменной с распределением Пуассона равны λ . B.10. Выборка из распределения вероятностей В вероятностном моделировании нам часто приходится получать случайные числа в соответствии с заданным распределением вероятностей. Эти числа также называются выборками. На самом деле это отнюдь не тривиальная за-\n--- Страница 402 ---\nРаспределения, включающие несколько переменных  401 дача, но, к счастью, большинство программных пакетов имеют встроенные генераторы случайных чисел для наиболее распространенных распределе- ний. Мы можем использовать эти функции для написания пользовательского кода, извлекающего из вероятностных распределений случайные значения, которые не были запрограммированы заранее. B.11. Распределения, включающие несколько переменных Для нас может представлять интерес зависимость случайных переменных друг от друга. Эта зависимость отражена в совместных и условных распреде- лениях вероятностей, а также в правиле Байеса, формальный вывод которого мы здесь представим. Концепции, обсуждаемые в этом разделе, примени- мы как к непрерывным, так и к дискретным переменным. Таким образом, p может обозначать распределение либо массы вероятности, либо плотность вероятности. Поскольку мы рассматриваем несколько переменных одно - временно, мы обычно будем использовать нижний индекс у p, чтобы обо- значить, к какой случайной переменной(ым) принадлежит распределение вероятностей. B.11.1. Совместная вероятность Совместное распределение вероятностей случайных величин X и Y обозна- чается pX,Y(x, y), или, сокращенно, p (x, y). Это вероятность значений x и y как пары. Суммирование по x и y дает 1: (B.41) Для непрерывных переменных интеграл по обеим переменным равен 1: (B.42) Совместная вероятность симметрична: p(x, y) = p(y, x). (B.43) Если X и Y представляют события, совместная вероятность X и Y – это вероятность того, что произойдут оба события, обозначаемая как p(X, Y) или p(X ∩ Y). В форме диаграммы Венна (рис. B.5) мы представляем Y в виде второй окружности, пересекающей первую. Совместная вероятность X и Y равна площади пересечения. Она всегда меньше или равна площади каждого отдельного круга. Это свойство выражают отношения p(X, Y) £ p(X) и p(X, Y) £ p(Y). Например, вероятность того, что в данный день идет дождь и вы бу -\n--- Страница 403 ---\n402  Основы теории вероятностей дете на работе вовремя, меньше, чем вероятность того, что идет дождь. Эти отношения справедливы только для дискретных переменных. Все люди Живет один Студент Рис. B.5  Совместная вероятность событий «быть студентом» и «жить отдельно» представлена площадью пересечения, указанной стрелкой B.11.2. Маргинализация Представьте, что у вас есть кошка и собака. Вы тщательно отслеживаете, ка- кова вероятность того, что в течение дня в гостиной находится только ваша кошка, только ваша собака, ни та, ни другая или они обе. Эти вероятности по- казаны в табл. B.1, которая называется таблицей сопряженности (contingency table) и представляет вероятности совместных исходов. Предельные вероят- ности – это вероятности того, что кошка присутствует или отсутствует не- зависимо от собаки, и вероятности того, что собака присутствует или отсут - ствует независимо от кошки. Таблица B.1. Частоты сочетаний двух случайных величин Кошки нет Кошка есть Итого Собаки нет 0.40 0.05 0.45 Собака есть 0.30 0.25 0.55 Итого 0.70 0.30 1 Маргинализация – это операция получения из совместного распределения по нескольким переменным распределения по подмножеству этих перемен- ных. Например, если p(x, y) является совместным распределением X и Y, то суммирование по Y дает распределение только X : (B.44) Аналог для непрерывных переменных получается заменой суммы интег - ралом:\n--- Страница 404 ---\nРаспределения, включающие несколько переменных  403 (B.45) Это суммирование, или интегрирование, называется маргинализацией, потому что p(x) и p(y) называются маргиналами p(x, y). Если вы представляе- те (x, y) как точку в двумерном пространстве и совместное распределение обеспечивает z значений в этом пространстве, то маргиналы – это распреде- ления, полученные путем суммирования по одному из измерений (рис. B.6). Это приводит к двум одномерным распределениям, которые находятся на «краях» (margin) исходного двумерного распределения. Вероятность p(a, b)b →a → Рис. B.6  Цветной график представляет совместное распреде- ление вероятностей двух случайных величин A и B. Черные кри- вые представляют маргиналы, полученные путем суммирования совместной по одной из двух переменных B.11.3. Условная вероятность Для событий X и Y условная вероятность p(X|Y) является ответом на вопрос: «Из всех исходов, согласующихся с событием Y, какая часть также согласует - ся с событием X?» Условная вероятность события всегда находится между 0 и 1. В представлении диаграммы Венна (рис. B.7) вероятность p(X|Y) равна площади пересечения, деленной на площадь второго круга. Точно так же вероятность того, что произойдет Y, при условии что произойдет X, равна площади пересечения, деленной на площадь первого круга. Три примера условной вероятности: если вероятность того, что сегодня идет дождь и вы успеете на рабо- ту вовремя, равна 0.4, а вероятность того, что сегодня пойдет дождь,\n--- Страница 405 ---\n404  Основы теории вероятностей равна 0.5, то вероятность того, что вы придете на работу вовремя, при условии что идет дождь, равна вероятность того, что на кубике выпало 6, если выпало четное число, равна в определенной стране в каждом штате разная доля водителей такси среди населения. Вероятность p(x | y) того, что человек, случайно вы- бранный из определенного штата, является водителем такси, равна доле граждан страны, которые живут в этом штате и являются водите- лями такси p(x, y), деленной на долю людей, живущих в этом штате p(y); из таблицы сопряженности B.1 можно вычислить условные вероятно- сти. Например, вероятность присутствия кошки при наличии собаки равна 0.25 (присутствуют и кошка, и собака), деленным на 0.30 + 0.25 = 0.55 (присутствие собаки). В главе 2 мы подробно обсуждали, что p (X|Y) не равно p (Y|X). Все люди Живет один Студент Рис. B.7  Условная вероятность «живет один» при условии «является студентом» представлена площадью пересечения, деленной на площадь синего диска Если X и Y являются случайными величинами, распределение вероятно- стей X при заданном Y обозначается pX|Y(x | y) или p(x | y), если нет неоднознач - ности в отношении переменных. Знак «|» читается как «обусловленный» или «при условии что». Он определяется как вероятность пары x и y, деленная на вероятность y : (B.46) Упражнение B.9 (a) Покажите, что p (x | y) нормирована как функция от x . (b) Приведите пример, показывающий, что p(x | y) не нормируется как функция y . Теперь объединим понятие маргинализации с определением условной вероятности.\n--- Страница 406 ---\nРаспределения, включающие несколько переменных  405 Упражнение B.10. Покажите, что (B.47) Интегральная форма этого уравнения: (B.48) Уравнения (B.47) и (B.48) – это правила, которыми мы пользуемся на про- тяжении всей книги. Продолжая пример с таксистом: предположим, вас интересует вероятность того, что случайно выбранный гражданин страны является таксистом. Вы знаете долю таксистов в населении каждого штата. Вы также знаете долю всех граждан, проживающих в каждом штате. Чтобы получить ответ, вы перемножаете эти две доли для каждого штата, а затем суммируете по всем штатам. Мы можем обусловить каждую вероятность в уравнениях (B.44) и (B.47) третьей случайной величиной z (это можно сделать с любым правилом в ис - числении вероятностей). Тогда мы получаем: (B.49) (B.50) или в интегральной форме: (B.51) Мы докажем это в задаче B.5. Условные распределения не ограничиваются одной случайной величиной до и после знака вертикальной черты. Например, можно рассмотреть рас - пределение X и Y при заданных Z и W , обозначаемое как p (x, y | z, w). B.11.4. Независимость Две случайные величины X и Y называются независимыми, если их совмест - ное распределение разлагается на маргиналы, т. е. если p(x, y) = p(x)p(y) (B.52) для всех х и у. Например, вероятность того, что на кубике выпадет 6 и на монете выпадет орел, является произведением вероятностей обоих событий, взятых по отдельности. Независимость можно изобразить графически, как на рис. B.8: можно реконструировать совместное распределение путем умножения марги- налов. Понятие независимости тесно связано с понятием корреляции: две не- зависимые случайные величины также некоррелированы. Обратное неверно.\n--- Страница 407 ---\n406  Основы теории вероятностей Упражнение B.11. Почему обратное неверно? Упражнение B.12. Если X и Y независимы, что можно сказать об условных рас - пределениях p (x | y) и p (y | x)? Если X , Y и Z обозначают три случайные величины, то X и Y условно неза- висимы при заданном Z , если p(x, y | z) = p(x | z)p(y | z) (B.53) для любых значений x, y и z. Мы впервые используем понятие условной не- зависимости в главе 5. Будьте внимательны, не путайте условную независи- мость с независимостью! Все формы Шестиугольники Квадраты Рис. B.8  Диаграмма Венна, изображающая независимость двух случайных величин B.11.5. Правило Байеса Правило Байеса связывает условные вероятности p(x | y) и p(y | x) друг с дру - гом: (B.54) Здесь p(y | x) как функция x – функция правдоподобия по x, p(x) – априорное распределение по x , а p (x | y) – апостериорное распределение по x . Упражнение B.13. Прежде чем читать дальше, попробуйте доказать правило Байеса, используя уравнения из предыдущих разделов. Доказательство выглядит следующим образом. Из уравнения (B.46) мы знаем, что p(x, y) = p(x | y)p(y). Меняя местами x и y, мы также получаем p(y, x) = p(y | x)p(x). Совместная вероятность симметрична, т. е. p(x, y) = p(y, x). Из этих\n--- Страница 408 ---\nРаспределения, включающие несколько переменных  407 трех уравнений следует, что p(x | y)p(y) = p(y | x)p(x). Разделив обе части на p(y), мы получаем правило Байеса. Упражнение B.14. Докажите, что правая часть уравнения (B.54) нормирована по x . Интерпретация диаграммой Венна правила Байеса для событий X и Y за- ключается в том, что площадь перекрытия может быть рассчитана двумя способами (рис. B.9): как доля площади круга X, умноженная на площадь круга X, или как доля площади круга Y, умноженная на площадь круга Y. По - скольку результаты должны быть идентичными, это означает, что две дроби могут быть выражены друг через друга, если известно отношение площадей кругов X и Y. p(x, y) = p(y |x)p(x) p(x, y) = p(x |y)p(y) p(y) p(y)p(х) p(х) Рис. B.9  Правило Байеса получается путем записи площади пересечения двумя разными способами и приравнивания двух выражений Предположим, что 1 из 100 000 человек – профессиональный баскетболист, 1 из 100 человек – высокий и 95 % профессиональных баскетболистов – вы- сокие. Какова вероятность того, что высокий человек является профессио- нальным баскетболистом? Мы решаем эту задачу, применяя правило Байеса напрямую: если X – это «профессиональный баскетболист», а Y – «высокий рост», то p(X) = 0.00001, p(Y) = 0.01 и p(Y | Х) = 0.95. Отсюда следует, что p(X | Y) = 0.95 · 0.00001 = 0.0095, или примерно 1 из 1000. Упражнение B.15. Докажите другую форму правила Байеса: (B.55) Последний способ записать правило Байеса – использовать знак пропор- циональности (примечание 3.6): p(x | y) µ p(y | x)p(x). (B.56)\n--- Страница 409 ---\n408  Основы теории вероятностей B.12. Функции случайных величин B.12.1. Функции одной переменной: изменение переменных В этом разделе мы обсудим часто встречающуюся задачу преобразования распределения непрерывной случайной величины. Вопрос заключается в следующем. Если X – случайная величина с распределением вероятностей pX(x), а Y = f(X) – новая случайная величина, полученная путем применения функции или преобразования f к X, каково распределение Y? В этом разделе мы будем использовать индексы, такие как X в pX(x), чтобы избежать пута- ницы, поскольку существует несколько случайных переменных. Пример: X – случайная величина, имеющая равномерное распределение на [0, 1]. Y = X2 – новая случайная величина, полученная путем возведения в квадрат результатов X . Каково распределение Y ? Простой, но неправильный ответ заключается в том, что поскольку X сле - дует равномерному распределению, то и Y тоже. Простые логические рас - суждения показывают, что этот ответ неверен. Если число x лежит между 0 и 1, возведение его в квадрат всегда будет давать меньший результат. Таким образом, хотя значения Y также будут лежать между 0 и 1, более низкие значения в этом диапазоне будут иметь бóльшую плотность вероятности, чем высокие значения. На вопрос можно ответить правильно, рассмотрев интегральные функции распределения X и Y, которые мы будем обозначать PX(x) и PY(y) соответственно: PY(y) = p(Y £ y) (B.57) = p(X2 £ y) (B.58) = p(X £ y) (B.59) = PX(y). (B.60) Теперь воспользуемся тем фактом, что функция плотности вероятности является производной CDF (B.8), чтобы найти PDF y, обозначенную как pY(y): (B.61) (B.62) (Ию63) (B.64)\n--- Страница 410 ---\nФункции случайных величин  409 (B.65) (B.66) Результирующее распределение pY(y) нормировано (проверьте это) и со- ответствует нашему первоначальному предположению: плотность вероят - ности выше для более низких значений y. Мы можем проверить результат с помощью симуляции: взять множество случайных чисел из равномерного распределения между 0 и 1, возвести их в квадрат, построить нормирован- ную гистограмму квадратов с мелкими интервалами и построить поверх нее функцию Мы могли бы сформулировать ту же задачу с произвольным распределе- нием pX(x) вместо равномерного. Вычисления идентичны, за исключением последнего шага. Далее мы находим, что (B.67) Таким образом, распределение квадрата переменной является произве- дением исходного распределения, оцениваемого по значению x, которое отображается в y, pX(y), и дополнительного множителя. Дополнительный множитель, называемый якобианом, равен производной отображения от y к x. Было бы неправильно не учитывать якобиан и утверждать, что pY(y) = pX(y). Якобиан фигурирует не только в этом примере возведения в квадрат слу - чайной величины, но и в нашей первоначальной общей задаче. Предпо- ложим, что X – случайная величина с распределением вероятностей pX(x) и Y = f(X), где f – монотонно возрастающая функция. Каково распределение Y? Сначала мы определим обратную функцию f-1 как функцию от y, которая «отменяет» действие f, другими словами, f-1(f(x)) = x. Эта обратная функция определена вполне обоснованно, поскольку мы предполагали, что f моно- тонно возрастает. Заманчивым, но неправильным способом получения рас - пределения Y было бы подставить обратную функцию в p(X): pY(y) = p(X) (f-1(y)) или предположить, что операция, применяемая к распределению, такая же, как и распределение, примененное к операции, pY(y) = f(pX(y)). Правильный подход снова состоит в том, чтобы вычислить интегральное распределение Y : PY(y) = p(Y £ y) (B.68) = p(f(X) £ y) (B.69) = p(X £ f-1(y)) (B.70) = PX(f-1(y)), (B.71) и, следовательно, функция плотности вероятности y :\n--- Страница 411 ---\n410  Основы теории вероятностей (B.72) (B.73) (B.74) (B.75) До сих пор мы рассматривали монотонно возрастающую функцию f. Когда f вместо этого монотонно убывает, окончательное выражение для pY(y) при- обретает дополнительный знак минус. Упражнение B.16. Покажите это. Мы можем свести оба случая – монотонное возрастание и убывание – в од- ном уравнении: (B.76) B.12.2. Пример с яблоками В качестве последней иллюстрации процедуры замены переменных предпо- ложим, что вы собираетесь посетить яблоневый сад. Вы очень мало знаете о том, как быстро растут яблоки, или о продолжительности вегетационного периода в этом регионе, и вы не знаете, какие яблоки растут в саду. Если друг спросит вас, какого, по вашему мнению, размера будут яблоки в саду, вы можете сначала ответить, что понятия не имеете. После более обстоятельных размышлений и на основе ограниченных знаний о яблоках в целом у вас возникает убеждение в однородном априорном распределении плотности вероятности по диаметру яблок в саду от 3 до 13 см. Какова же тогда ваша первоначальная плотность распределения относительно объема яблока? Прежде чем мы получим ответ, давайте обсудим задачу. Равномерное априорное распределение относительно диаметра яблока означает, что, на- пример, вы считаете равновероятным, что диаметр яблока будет лежать как между 5 и 6 см, так и между 10 и 11 см. Если мы аппроксимируем яблоки сферами, то объем яблока равен (B.77) где r – радиус, а w = 2r – диаметр яблока. Таким образом, объемы, соответ - ствующие диаметрам 5, 6, 10 и 11 см, равны (с точностью до целого числа)\n--- Страница 412 ---\nФункции случайных величин  411 65, 113, 524 и 697 см3 соответственно. Это означает, что вы считаете равно- вероятным (точнее, с 10%-ной вероятностью), что объем яблока находится между 65 и 113 см3 (диапазон 48 см3), как и между 524 и 697 см3 (диапазон 173 см3). Таким образом, априорное распределение плотность вероятности относительно объема яблок отнюдь не плоское. Наоборот, плотность вероят - ности будет выше при меньших объемах (рис. B.10). 0.10 0.08 0.06 0.04 0.02 0.000.010 0.008 0.006 0.004 0.002 0.000Плотность вероятности (см–1) Плотность вероятности (см–3) 3456789 10 11 12 13 1150 1000 800 600 400 200 14(А) (B) Диаметр (см) Объем (см3) Рис. B.10  Замена переменных: (A) равномерное априорное распределение вероятности для диаметра яблока от 3 до 13 см (диапазон 10 см). Априорное распределение плотности вероятности представляет собой линию высотой 0.1 см–1, потому что общая площадь под плотностью должна быть равна 1. Вероятность того, что диаметр находится между 5 и 6 см, равна 0.1, как и вероятность того, что диаметр находится между 10 и 11 см (заполненные прямоугольные области); (B) априорное распределение плотности вероятности для объема яблока. Каждая закрашенная прямоугольная область – это вероятность того, что объем яблока находится в диапазоне, соответствующем диаметрам яблок, покрытым закрашенными прямо- угольниками в A. Опять же, общая площадь под плотностью равна 1, а площадь каждого за- крашенного прямоугольника равна 0.1 (обратите внимание на различия в масштабах по оси y) Чтобы получить плотность вероятности по объему, мы отмечаем, что (B.78) Для производной находим, что (B.79) Следовательно: (B.80) (B.81)\n--- Страница 413 ---\n412  Основы теории вероятностей (B.82) Это кривая, изображенная на рис. B.10B. B.12.3. Обязательство неведения Интересное последствие процедур замены переменных, проиллюстриро- ванное примером с яблоневым садом, состоит в том, что невозможно быть в неведении относительно всех нюансов задачи. Например, невозможно абсолютно ничего не знать о размере яблока в его общепринятом опреде- лении. Как вы только что видели, если мы заявляем, что ничего не знаем о диаметре яблока в том смысле, что принимаем равновероятным широкий диапазон диаметров, этим допущением мы демонстрируем определенное априорное знание объема яблока! При выполнении байесовского статис - тического анализа исследователь может постараться включить как можно меньше априорных убеждений в анализ, о котором, по его мнению, у него почти нет соответствующих фоновых знаний. Как лучше всего это сделать, если, указывая на свое незнание параметра, он тем самым демонстрирует знание о связанных параметрах? Например, если исследователь «не знает» о стандартном отклонении σ случайной величины, он может выбрать ис - пользование плоского априорного значения в очень широком диапазоне σ, но тогда он неявно задает неравномерное априорное распределение для дисперсии σ2. Поиск подходящих значений по умолчанию или эталонных априорных распределений для таких ситуаций является интересной темой в области байесовского статистического анализа. В байесовских моделях восприятия и действия это обычно не проблема, поскольку предполагает - ся, что априорное значение получено либо из экспериментальной, либо из естественной статистики. B.12.4. Математическое представление маргинализации Будет поучительно сформулировать задачу преобразования распределения случайной величины как формальную задачу маргинализации. Эти форму - лировки эквивалентны, но вторая в некоторых отношениях дает больше по- нимания. Предположим снова, что X – случайная величина с распределением вероятностей pX(x) и Y = f(X), где f – монотонно возрастающая функция. Как мы обсуждали в разделе B.8, детерминированное отображение, такое как f, может быть выражено как дельта-распределение. Здесь это распределение будет иметь вид: pY|X(y | x) = δ(y - f(x)). (B.83)\n--- Страница 414 ---\nФункции случайных величин  413 Теперь мы можем вычислить плотность вероятности в точке y, используя тождество маргинализации из уравнения (B.47): (B.84) (B.85) Другими словами, плотность вероятности y – это общая вероятность всех значений x, которые отображаются на y с помощью f. Мы можем вычислить это выражение, выполнив преобразование переменных: x = f-1(t), так что Подставляя, находим: (B.86) Теперь мы можем использовать уравнение (B.38) для вычисления интег - рала: (B.87) что совпадает с уравнением (B.75). Когда f монотонно убывает, а не возрас - тает, мы получаем тот же результат, но со знаком минус. Упражнение B.17. Откуда в этом результате берется знак минус? Преимущество этой интегральной формулы состоит в том, что первое ра- венство в уравнении (B.84) является общим и не ограничивается детермини- рованными отображениями из X в Y. Таким образом, задача преобразования случайной величины является просто частным случаем вероятностного ото- бражения из Y в X, и первое равенство в уравнении (B.84) может быть при- менено для любого условного распределения p (y | x). Второе преимущество состоит в том, что математическое ожидание любой функции g (Y) случайной величины Y = f(X) теперь легко преобразовать: (B.88) (B.89) (B.90) (B.91) = �[g(f(X))]. (B.92)\n--- Страница 415 ---\n414  Основы теории вероятностей Другими словами, комбинация pY(y)dy внутри интеграла идентична pX(x) dx, в то время как y = f(x) подставляется в другом месте интеграла. (Пределы интегрирования также могут измениться соответствующим образом.) Упражнение B.18. Используйте этот результат, чтобы показать, что среднее значение aX + b равно a �[X] + b, а его дисперсия равна a2Var[X ]. Третье преимущество формулы маргинализации состоит в том, что она напрямую обобщается на функции нескольких переменных, как будет по- казано далее. B.12.5. Функции нескольких переменных Предположим, вы бросаете две игральные кости и складываете результаты. Каково распределение вероятностей суммы? Простой подсчет дает ответ: исход 2 может быть достигнут только одним способом (1 + 1) и, следователь- но, имеет вероятность 1/36. Исход 3 может быть достигнут двумя способами (1 + 2 и 2 + 1) и, следовательно, имеет вероятность 2/36 и т. д. Это приводит к распределению вероятностей, показанному на рис. B.11. Как аналитически рассчитать это распределение? 23456789 10 11 126 5 4 3 2 1 0Частота (из 36) Исход Рис. B.11  Распределение вероятностей суммы двух бросков кости Обозначим случайные величины, соответствующие обоим броскам костей, как X и Y. Их сумма представляет собой новую случайную величину Z = X + Y. Другими словами: pZ|X,Y(z | x, y) = δ(z - x - y). (B.93) Для вычисления распределения Z, обозначаемого pZ(z), применим дис - кретный аналог уравнения (B.84):\n--- Страница 416 ---\nФункции случайных величин  415 (B.94) (B.95) (B.96) Воспользуемся теперь свойством дельта-функции, уравнением (B.39), а также тем условием, что для того, чтобы pY(y) было ненулевым, мы должны иметь 1 £ y £ 6, следовательно, 1 £ z - x £ 6 и, стало быть, z - 6 £ x £ z - 1. Тогда (B.97) (B.98) (B.99) Та же логика может быть применена к непрерывному распределению. Пусть X и Y – независимые непрерывные переменные с соответствующими PDF pX(x) и pY(y). Определим новую переменную Z = f(X, Y), где f – произ- вольная функция, и обозначим через fx-1 обратную функцию f для заданного x: Y = fx-1(Z). (Такая обратная функция не всегда существует, но в примерах этой книги она существует.) Тогда распределение Z равно (B.100) (B.101) (B.102) (B.103) где со второй строки в третью мы сделали преобразование переменных y = fx-1(t). Дельта-функцию можно рассматривать как выбор области N-мерного пространства, а именно всех точек, которые отображаются на y, а интеграл – как общую вероятность относительно p (X) в этой области.\n--- Страница 417 ---\n416  Основы теории вероятностей Упражнение B.19. Если X и Y независимы и имеют равномерное распределе- ние на [0, 1], вычислите распределение Z = X + Y. Ответ – частный случай распределения Ирвина– Холла. Упражнение B.20. Если X и Y независимы и имеют нормальное распределение, докажите, что Z = X + Y также имеет нормальное распределение. До сих пор мы вычисляли распределение суммы случайных величин. Мы также можем использовать уравнение (B.103) для вычисления распределения нелинейных комбинаций случайных величин, таких как произведение или частное. Распределение произведения (или частного) двух переменных не равно произведению (или частному) их распределений и часто сильно от - личается. Мы рассмотрим это в задаче B.14. B.13. Задачи Задача B.1. В цветнике 20 % цветов составляют тюльпаны. Из них четверть красные. Какова вероятность того, что случайно выбранным цветком в этом саду окажется тюльпан, но не красный? Хотя эта задача легко решается с по- мощью логических рассуждений, примените формулы правил вероятности. Задача B.2. Четыре игрока, сидящих вокруг стола, собираются сыграть в игру. Чтобы определить, кто начинает, один игрок бросает две кости. Сумма двух чисел определяет, кто начинает, при этом отсчет идет по часовой стрелке, начиная с бросающего кубик, которому присвоен номер 1 (таким образом, при сумме 5 или 9 начинает бросающий). Какова вероятность начать игру для каждого игрока? Задача B.3. Вы и я бросаем кубик по одному разу. (a) Какова вероятность того, что у одного из нас выпадет 6, а у другого – не- четное число? (b) Какова вероятность того, что хотя бы у одного из нас выпадет 6? (c) Какова вероятность того, что у вас выпадет больше, чем у меня? (d) Какова вероятность того, что наша сумма больше 8? Задача B.4. Вы учитесь в группе из тридцати человек. (a) Какова вероятность того, что день рождения конкретного одногруппника совпадает с вашим днем рождения? (b) Какова вероятность того, что день рождения хоть кого-то из одногрупп- ников совпадает с вашим днем рождения? (c) Какова вероятность того, что у любых двух студентов совпадают дни рождения? Задача B.5. Пусть x, y и z – дискретные случайные величины. Докажите сле- дующее: (a) условная маргинализация: (B.104)\n--- Страница 418 ---\nЗадачи  417 (b) условное правило Байеса: (B.105) Задача B.6. Мы с вами поочередно подбрасываем монету. Вы начинаете. Если у вас выпадает решка, вы выигрываете. Если у вас выпадает орел, бросок переходит ко мне. Если у меня выпадает решка, я выигрываю. Если у меня выпадает орел, бросок снова переходит к вам. Игра продолжается до тех пор, пока один из нас не выиграет. Каков ваш шанс выиграть в этой игре? Задача B.7. Программы электронной почты автоматически классифицируют электронные письма как спам или не спам на основе слов в электронном письме. Для этого они используют байесовский алгоритм, очень похожий на байесовский вывод, который мы обсуждали на примере медицинской диаг - ностики. Предположим, что 70 % всей электронной почты являются спамом. Предположим, что если электронное письмо не является спамом, вероят - ность встретить в нем слово «сделка» составляет 0.1 %. Предположим также, что если электронное письмо является спамом, вероятность встретить в нем слово «сделка» составляет 1 %. (a) Нарисуйте диаграмму генеративной модели. b) Запишите соответствующие вероятности: p(спам) = … (B.106) p(не спам) = … (B.107) p(содержит «сделка» | спам) = … (B.108) p(не содержит «сделка» | спам) = … (B.109) p(содержит «сделка» | не спам) = … (B.110) p(не содержит «сделка» | не спам) = … (B.111) (c) Какова априорная вероятность того, что случайное электронное письмо является спамом? Какова априорная вероятность того, что случайное электронное письмо не является спамом? (d) Предположим, что конкретное электронное письмо содержит слово «сделка». Какова вероятность, что это спам? Какова вероятность, что это не спам? (e) Умножьте априорную вероятность исхода «спам» на правдоподобие «спама». (f) Умножьте априорную вероятность исхода «не спам» на правдоподобие «не спама». (g) Теперь у вас есть то, что мы назвали «протопостериором». Разделите каждый из ответов на задания (e) и (f) на сумму обоих ответов. Какова апостериорная вероятность того, что это конкретное электронное пись- мо является спамом? Задача B.8. Эта задача связана с известным парадоксом Монти Холла [155]. Вы участвуете в игровом шоу. Ведущий показывает вам три двери. За одной\n--- Страница 419 ---\n418  Основы теории вероятностей из них спрятан приз. Вы выбираете одну дверь (она остается закрытой). Веду - щий, который знает, за какой дверью спрятан приз, открывает из оставшихся дверей ту, за которой нет приза. Затем ведущий дает вам возможность пере- ключить свой выбор на оставшуюся неоткрытую дверь или остаться с вашим первоначальным выбором. Выбранная вами дверь открывается, и вы полу - чаете приз, если он там есть. (a) Что нужно сделать, чтобы максимизировать вероятность получения приза? (b) Если есть N дверей и ведущий открывает m из них (где m < n - 1, какова вероятность получить приз при наилучшей стратегии? (c) Изменился бы ответ на вопрос (a), если бы ведущий не знал, в какой из двух оставшихся дверей находится приз, а в той, которую он открывает, случайно не оказалось приза? Обоснуйте свой ответ. (d) Подумайте, почему большинство людей считают, что не имеет значения, сохраните ли вы первоначальный выбор или выберете другую дверь. Задача B.9. Функция плотности вероятности гауссовой случайной величины X задается уравнением (B.18). Покажите, что среднее значение и дисперсия этой случайной величины равны μ и σ2 соответственно. Задача B.10. Если X – экспоненциально распределенная случайная вели- чина, где областью определения X является положительная вещественная числовая прямая, каковы область определения и распределение Y = eX? Задача B.11 (a) Кто-то трижды подбрасывает честную монету. Вы наблюдаете за резуль- татом одного броска, в котором выпадает орел. Какова вероятность того, что во всех трех подбрасываниях орел выпадет чаще, чем решка? (b) Кто-то подбрасывает честную монету N раз. Вы наблюдаете за результа- том одного броска, в котором выпадает орел. Какова вероятность того, что во всех N подбрасываниях орел выпадет чаще, чем решка? (Для чет - ного N интерпретируйте «больше» как «строго больше».) Задача B.12. Используйте уравнение (B.76), чтобы доказать, что если X нор- мально распределена со средним μ и дисперсией σ2, то aX + b имеет нормаль- ное распределение со средним значением a μ и дисперсией a2σ2. Задача B.13. Если X и Y являются независимыми переменными со стандарт - ным нормальным распределением, покажите, что случайная величина имеет распределение Коши, т. е. (B.112) Задача B.14. Если X и Y независимы и имеют равномерное распределение вероятностей на [0, 1] (0 £ x £ 1), покажите, что произведение случайных величин Z = XY имеет распределение pZ(z) = -log z. Убедитесь, что это рас -\n--- Страница 420 ---\nЗадачи  419 пределение нормировано, хотя плотность в точке 0 равна бесконечности. Этот пример показывает, как распределение произведения может сильно отличаться от распределения каждого из множителей. Задача B.15. Возьмем два распределения фон Мизеса для одной и той же случайной величины, одно со средним значением μ1 и параметром концент - рации κ1, другое со средним значением μ2 и параметром концентрации κ2. Покажите, что нормированное произведение этих распределений снова яв- ляется распределением фон Мизеса, и вычислите его среднее значение и па- раметр концентрации.\n--- Страница 421 ---\nПриложение C Подбор параметров и сравнение моделей В этом приложении мы описываем подбор1 модели. Это сокращенное наиме- нование процесса подбора параметров модели для наилучшего соответствия данным – корректировки неизвестных параметров модели (например, σ) таким образом, чтобы прогнозы модели как можно точнее совпадали с дан- ными. Наиболее популярной разновидностью подбора параметров является оценка максимального правдоподобия (maximum-likelihood estimation, MLE), которая заключается в том, что мы должны отдавать предпочтение пара- метрам, делающим наблюдаемые данные наиболее вероятными. Методы, упомянутые в этом приложении, не ограничиваются байесовской областью. Любая математическая модель поведения наблюдателя может быть адапти- рована к данным тем или иным способом. C.1. Что такое модель? Здесь мы понимаем этот вопрос в наиболее практическом смысле. Для байе- совской модели на этапе 3 выводится распределение отклика. Это вероят - ностное отображение пространства стимулов на пространство ответов: p(отклик субъекта | стимулы). Такое распределение характеризует не только байесовскую модель, но и любую модель поведения. Действительно, мы мо- жем определить модель поведения как распределение откликов p(отклик субъекта | стимулы). 1 Мы намеренно не используем здесь термин «обуче ние», оставляя его для публика- ций о машинном обуче нии нейросетевых моделей. – Прим. перев.\n--- Страница 422 ---\nВероятность параметра  421 C.2. Свободные параметры Модели имеют свободные параметры (или просто параметры): переменные с неизвестным значением, которые считаются постоянными на протяжении всего эксперимента. В байесовских моделях свободные параметры обычно используются для свойств или убеждений, которые могут отличаться у раз- ных субъектов. К примерам параметров относятся уровень сенсорного шума σ, с которым измеряется конкретный тип стимула, предполагаемое апри- орное среднее значение μs,assumed в разделе 3.5.1 и обратный температур- ный параметр β в уравнении (13.34). Мы будем обозначать набор свободных параметров модели через θ, а саму модель через M. Сделаем зависимость распределения откликов от параметров и модели явной, написав p(ответ субъекта | стимулы; θ, M). Точка с запятой служит для отделения переменных, которые изменяются от испытания к испытанию (реакция и стимулы), от указателя на модель и параметров модели, которые не меняются от испы- тания к испытанию. Когда однозначно ясно, о какой модели идет речь, мы опускаем M . C.3. Вероятность параметра Значения свободных параметров необходимо настроить для лучшего опи- сания данных; этот процесс называется подбором параметров, или – в ма- шинном обуче нии – обуче ни ем модели. Доминирующим методом подбора параметров является оценка максимального правдоподобия. Правдоподобие комбинации параметров θ в рамках модели M определяется как вероятность наблюдаемых данных об испытуемом с учетом стимулов, испытываемых субъектом, комбинации параметров θ и модели: �M(θ; данные) = p(отклики субъектов во всех испытаниях | стимулы во всех испытаниях; θ , M). (C.1) Другими словами, вероятность комбинации параметров высока, если мо- дель с этой комбинацией параметров, примененная к стимулам, испытыва- емым субъектом, относительно часто будет предсказывать отклик субъекта. Чтобы сделать возможным подбор параметров, ученые-бихевиористы почти всегда делают предположение об условной независимости: в данном испытании вероятность конкретного отклика субъекта зависит только от стимулов в этом испытании, комбинации параметров и модели. Она не за- висит от откликов субъекта или от стимулов в предыдущих испытаниях. На самом деле последовательные зависимости между испытаниями давно из- вестны и хорошо задокументированы в психофизике, поэтому допущение о независимости часто нарушается. Однако ослабление предположения об условной независимости требует наличия модели для последовательных за- висимостей, что выходит за рамки данной книги. Предположение об услов- ной независимости можно сформулировать как\n--- Страница 423 ---\n422  Подбор параметров и сравнение моделей �M(θ; данные) = p(отклик субъекта в испытании i | стимул в испытании i ; θ, M), (С.2) где ntrials – количество испытаний. Условная вероятность в уравнении (С.2) для общих реакций и стимулов точно соответствует предсказаниям модели; например, в байесовской модели это будет распределение ответов, получен- ное на шаге 3. Однако в уравнении (C.2) мы подставляем фактический ответ субъекта и фактические стимулы в i -м испытании. C.4. Оценка максимального правдоподобия Оценка максимального правдоподобия параметров θ означает нахождение таких значений θ, при которых �M(θ; данные) является максимальным. Это эквивалентно максимизации значения log �M(θ; данные), потому что лога- рифм является монотонно возрастающей функцией. Часто удобнее макси- мизировать логарифм правдоподобия, чем само правдоподобие. Логариф- мическое правдоподобие можно записать так: log �M( θ; данные) = log p(отклик субъекта в испытании i | стимул в испытании i ; θ, M). (С.3) Следует подчеркнуть, что параметрическое правдоподобие, максимизиро - ванное при подборе модели, концептуально подобно, но все же отличается от функций правдоподобия в байесовской модели наблюдателя, о которой в действительности идет речь в этой книге. Речь идет о параметрах моде- ли (которые неизвестны экспериментатору), а не о релевантном состоянии мира (которое обычно известно экспериментатору, но неизвестно испытуе- мому). Это различие показано на рис. C.1. В этом приложении и в практике байесовского моделирования в целом важно различать процесс принятия решения наблюдателем (этап 2) и модель наблюдателя, которую использует экспериментатор (этап 4). Отметим, что использование оценки максимального правдоподобия для подбора параметров эквивалентно байесовской оценке, при которой ис - следователь начинает с равномерного априорного распределения по па- раметрам модели. Иногда при подборе параметров модели используются неоднородные априорные значения параметров. Чтобы использовать такие априорные данные, нам необходимо иметь значимое обоснование, основан- ное на нашем понимании тех аспектов мира, которые они представляют. При подборе модели часто удается обойтись без использования априорных зна- чений, если набор данных достаточно велик (много испытаний) и имеется довольно мало параметров. Поэтому мы не будем обсуждать здесь априор- ные значения параметров.\n--- Страница 424 ---\nПодбор параметров по данным из задачи оценки  423 Точка зрения субъекта Точка зрения экспериментатора Состояние мира Состояние мира Сенсорные наблюдения Сенсорные наблюдения Байесовский или другой процесс принятия решенийБайесовский или другой процесс принятия решений Поведенческий отклик Поведенческий откликПараметры субъектаПараметры субъекта Подлежит выводу Данные (наблюдения)Мозг Мозг Рис. С.1  Различие точек зрения субъекта и экспериментатора. Субъект делает вывод о со- стоянии мира на основе сенсорных наблюдений, в то время как экспериментатор делает вы- воды о параметрах субъекта и стратегии принятия решений на основе откликов субъекта Если мы ставим перед собой цель полностью следовать байесовскому подходу при подборе модели, то должны рассчитать полные распределения вероятностей. Существует обширная литература о полностью байесовских методах подбора модели. К возможным вариантам относятся метод Монте- Карло с цепью Маркова и вариационные подходы. Когда эти детали процесса подбора важны, мы, как правило, находимся в области, где велика апостери- орная неопределенность. В этой книге мы не рассматриваем такие подходы и отсылаем заинтересованного читателя к [59, 103, 164]. В следующих разделах мы вернемся к нескольким моделям из предыду - щих глав и выполним оценку параметров модели по методу максимального правдоподобия на основе гипотетических наборов данных. В этих примерах, как и в большей части нашей книги, реакция субъекта равна его оценке на- блюдаемого состояния мира. C.5. Подбор параметров по данным из задачи оценки Первым шагом в подборе модели является определение характера ваших данных. Предположим, вы проводите оценочный эксперимент, подобный тому, что описан в главе 3, где вы получаете стимул из распределения p(s), а наблюдатель оценивает стимул. В ходе пятнадцати испытаний вы собрали следующие данные (табл. C.1):\n--- Страница 425 ---\n424  Подбор параметров и сравнение моделей Таблица С.1. Пример данных из эксперимента с непрерывной оценкой. s – стимул; sˆ – ожидание Исп. 1 23 456789 10 11 1213 1415 s 1.61 5.50 –6.78 2.59 0.96 –3.92 –1.30 1.03 10.74 8.31 –4.05 9.10 2.18 –0.19 2.14 sˆ 0.37 1.62 –1.17 1.66 1.17 –0.79 –1.14 0.76 4.31 2.86 –0.61 3.25 0.48 0.12 0.18 Эти данные показаны на рис. C.2. 10 0 –10Ожидание стимула sˆ –10 0 10 Истинный стимул s Рис. С.2  Визуализация данных из табл. C.1 C.5.1. Простая модель Даже на основе этого небольшого набора данных уже можно подобрать мо- дель. Давайте сначала рассмотрим модель, в которой наблюдатель не ис - пользует априорную оценку и просто сообщает об измерении. Тогда sˆ = x, и распределение s ˆ при заданном s равно (C.4) Это выражение полностью определяет модель. Теперь нам нужно найти параметр σ . Логарифмическое правдоподобие σ из уравнения (C.3) равно (C.5) (C.6)\n--- Страница 426 ---\nПодбор параметров по данным из задачи оценки  425 Наша цель – найти максимальное правдоподобие σ, которое мы будем обозначать через σˆ. Это значение σ, для которого логарифмическое правдо- подобие (и, следовательно, вероятность) является самым высоким. Метод 1: аналитический расчет В этом случае мы можем аналитически максимизировать логарифмическую функцию правдоподобия, приняв производную log � по σ равной 0: (C.7) (C.8) Решая уравнение для σ, мы находим оценку максимального правдоподо- бия (MLE) σ : (C.9) Поскольку sˆi - si представляет собой ошибку оценки наблюдателя в i-м ис - пытании, правая часть – это среднеквадратичная ошибка (root mean-square error, RMSE). Результат не должен удивлять: в модели, в которой оценки обычно распределяются вокруг истинного значения, как в уравнении (C.4), стандартное отклонение этого нормального распределения оценивается как эмпирическое стандартное отклонение. В приведенном выше наборе данных ответ равен σ ˆ = 3.49. Аналитический вывод минимума не имеет недостатков, но этот вариант редко доступен: приравнивание производной к 0 даст разрешимое урав- нение только в очень немногих простых случаях. Во всех других случаях логарифмическое правдоподобие в уравнении (C.3) должно быть максими- зировано численными методами. Поэтому далее мы рассмотрим методы численной оценки максимального правдоподобия. Метод 2: поиск по сетке Самый простой численный метод состоит в том, чтобы определить сетку воз- можных значений σ, например от 0.5 до 10 с шагом 0.01, и просто оценить логарифмическое правдоподобие на этой сетке. Преимущество этого метода в том, что мы можем построить логарифмическую функцию правдоподобия (или функцию правдоподобия), как мы это сделали на рис. C.3A–B. Упражнение C.1. Выбор подходящей сетки гипотетических значений зависит от здравого смысла и опыта, и, конечно же, вы всегда можете изменить ее. (a) Что произойдет, если мы включим в сетку значения меньше 0.5? (b) Почему было бы бессмысленно расширять диапазон за пределы 10?\n--- Страница 427 ---\n426  Подбор параметров и сравнение моделей На рис. C.3 мы видим, что функция правдоподобия явно не является гаус - совой, или, что то же самое, логарифмическая функция правдоподобия явно не является параболой. В моделях наблюдателя из предыдущих глав правдо- подобие непрерывной переменной обычно было гауссоым. Однако неудиви- тельно, что здесь этого быть не может: поскольку σ может принимать только положительные значения, гауссово распределение правдоподобия (прости- рающееся до отрицательной бесконечности) было бы бессмысленным. –50 –100 –150 –20010 0 –104 3 2 1 0Логарифмическое правдоподобие Правдоподобие 12345 12345 Предполагаемое значение σ Предполагаемое значение σ(А) (B) (C) ×10–18 –10 0 10Ожидание стимула sˆ Истинный стимул s Рис. С.3  Подбор простой модели в уравнении (C.4) к данным в табл. C.1: (A) логарифми- ческое правдоподобие и (B) правдоподобие σ. Пунктирные линии отмечают MLE σ, что со- ответствует σˆ = 3.49. Максимальное значение логарифмического правдоподобия составляет log �∗ = 40.0; (C) проверка модели: предсказание простой модели после подбора σ. Черные точки обозначают отдельные прогоны (симуляции). Зеленые круги: среднее значение по си- муляциям для одного и того же стимула. Сравнивая результат с рис. C.2, мы видим, что модель плохо согласуется с данными Кроме того, можно видеть, что логарифмическое правдоподобие прини- мает отрицательные значения порядка нескольких сотен. В поведенческих экспериментах значения логарифмического правдоподобия обычно нахо- дятся в области отрицательных сотен или тысяч. Это происходит потому, что функции распределения массы вероятности всегда, а функции распре- деления плотности вероятности часто принимают значения меньше 1. Это означает, что каждая отдельная вероятность меньше 1, а ее логарифм отри- цателен. Сумма в уравнении (C.3) в среднем будет пропорциональна коли- честву испытаний. Теперь мы можем просто найти значение в сетке, для которого логариф- мическое правдоподобие максимально. Поиск возвращает значение 3.49, согласующееся с нашим аналитическим расчетом. У поиска по сетке есть свои достоинства и недостатки. Достоинства: (1) прозрачность – при поиске по сетке вы точно знаете, что делаете; (2) ви- зуализация – построение кривой правдоподобия помогает убедиться, что она не имеет странной формы, и узнать ее ширину. Недостатки: (1) исследователь ограничен значениями в сетке и не может найти промежуточные значения и (2) метод становится громоздким с увеличением числа параметров. Напри- мер, 1000 значений на параметр для четырех параметров будут составлять 1012 комбинаций параметров, что займет много времени для вычисления. Эти две проблемы взаимосвязаны: выбирая более мелкую сетку для решения первой проблемы, вы усугубляете вторую проблему.\n--- Страница 428 ---\nПодбор параметров по данным из задачи оценки  427 Метод 3: алгоритм численной оптимизации Из-за вышеупомянутой комбинаторной проблемы часто возникает необ- ходимость выйти за рамки поиска по сетке. Все экосистемы для научных вычислений, включая Python, Matlab, R и Mathematica, имеют алгоритмы ми- нимизации произвольных функций. Алгоритмы оптимизации обычно явля- ются итеративными: конкретная процедура стартует с определенной началь - ной точки и выполняется многократно, пока не будет удовлетворен критерий завершения. Одни алгоритмы оптимизации основаны на той или иной фор- ме градиентного спуска, когда следующая точка выбирается в окрестностях текущей точки в направлении наибольшего уклона (см. раздел 6.4). Другие представляют собой алгоритмы глобальной оптимизации, которые не всегда выбирают ближайшую точку и иногда пытаются оценить крупномасштабную структуру оптимизируемой функции. Многие оптимизаторы, хорошо рабо- тающие в этой области, сами являются байесовскими. Мы рекомендуем вам попробовать ряд алгоритмов, чтобы убедиться: такие задачи оптимизации сложны, и выбор конкретного алгоритма может иметь значение (примеча- ние C.1). Вы можете надеяться, что нашли истинный максимум, если разные методы возвращают один и тот же результат, и со временем вы накопите большой набор инструментов для своих задач. Алгоритмы оптимизации не всегда находят глобальный оптимум. Это мо- жет быть связано с наличием «хребта» комбинаций параметров, при которых логарифмическое правдоподобие близко к максимальному логарифмическо- му правдоподобию, или из-за того, что ландшафт логарифмического правдо- подобия имеет много локальных максимумов. Распространенным способом решения этой проблемы является «многократный запуск», т. е. инициали- зация оптимизатора со множеством различных комбинаций параметров (выбранных либо случайным образом, либо систематически, но достаточно далеко друг от друга), выбор результата наилучшего запуска и использование согласованности между запусками как показателя сходимости. Проверка модели Итак, вы нашли значения параметров одним из трех способов. Распростра- ненной ошибкой при моделировании поведенческих данных является до- верие к оценкам параметров без проверки хотя бы поверхностно того, что модель действительно хорошо соответствует данным. Возможно, корень этой проблемы заключается в смешении понятий «лучшего» и «хорошего»: даже если оценка максимального правдоподобия обеспечивает наилучшую веро- ятность в контексте подбираемой модели, это самое высокое правдоподобие все же может быть низким. Подобные заблуждения влекут за собой далеко идущие последствия. Полагаться на параметры плохо подобранной модели бессмысленно: это все равно, что оценивать размер Земли, предполагая, что она плоская. Самый простой (и во многих случаях достаточный) способ проверить, хо- рошо ли подобрана модель, – это построить на одном графике исходные данные вместе с соответствующим представлением подобранной модели.\n--- Страница 429 ---\n428  Подбор параметров и сравнение моделей Один из способов сделать это в нашем текущем примере – вывести оценки стимула из нормального распределения в уравнении (C.4), используя σˆ = 3.49 вместо σ. На рис. C.3C мы сделали это для 500 повторений каждого предъ- являемого стимула. Ясно, что тренд модели сильно отличается от данных, и придется искать новую модель. Примечание С.1 Алгоритмы оптимизации При подгонке модели к поведенческим данным могут пригодиться многие алго- ритмы оптимизации параметров. Ознакомьтесь с кратким прикладным обзором: • если функция гладкая, детерминированная и в целом хорошо себя ведет, обычно предпочтительнее использовать методы на основе градиента. Примером такого подхода является популярный алгоритм Бройдена–Флетчера–Гольдфарба–Шан- но (Broyden-Fletcher-Goldfarb-Shanno, BFGS). В большинстве языков программи- рования существуют стандартные реализации BFGS (или их варианты, такие как BFGS с ограниченной памятью). С помощью этих методов мы можем предоста- вить оптимизатору аналитически рассчитанный градиент цели или автомати- чески рассчитанный автоградиент, что значительно ускоряет выполнение кода. Если градиент не указан явно, метод будет численно аппроксимировать гради- ент, используя конечные разности; • если функция сложная, приближенная или стохастическая (например, потому что мы используем выборку для оценки апостериорного распределения), часто хорошей идеей является использование эволюционной стратегии адаптации ко- вариационной матрицы (covariance matrix adaptation evolution strategy, CMA-ES). Алгоритм CMA-ES реализован на большинстве популярных языков программи- рования. CMA-ES часто проявляет себя с лучшей стороны, когда он может вы- полняться для большого количества оценок функций (десятки или сотни тысяч), что может быть не осуществимо или может занять довольно много времени, если целевая функция требует больших вычислительных затрат; • если функция довольно дорогая для оценки и, возможно, приближенная или сто- хастическая, то часто оказывается полезной байесовская оптимизация. Методы этого типа пытаются найти байесовским способом распределение целевой функ - ции, совместимое с существующими измерениями. Эти методы в настоящее вре- мя продолжают быстро развиваться, но часто предлагают эффективную оценку в случаях, когда целевые функции сложны. Отдельного упоминания заслуживает байесовский адаптивный прямой поиск (BADS, [1]), который был специально раз- работан для использования в сценариях, подобных тем, которые мы рассматри- ваем в этой книге; • наконец, если функция является многомерной и стохастической, но у вас есть градиент, то вы можете использовать методы оптимизации стохастическим гра- диентным спуском, такие как ADAM, обычно применяемые в машинном обуче- нии. Однако если размерность вашей функции невелика, перечисленные выше методы могут работать лучше. Короче говоря, подбор параметров модели не обязательно делает ее хоро- шей! Всегда необходимо убедиться, что модель хорошо соответствует дан-\n--- Страница 430 ---\nПодбор параметров по данным из задачи оценки  429 ным, прежде чем придавать какое-либо значение оценкам ее параметров. Если предсказания модели расходятся с данными, нужно задуматься о спо- собах найти более удачную модель. C.5.2. Более удачная модель Для данных на рис. C.2 мы можем легко придумать модель получше: похоже, что на оценки наблюдателя влияет априорное распределение с центром в 0. Следовательно, в данном случае мы снова подбираем полную байесовскую модель из главы 3: наблюдатель использует гауссово априорное распреде- ление со средним значением 0 и стандартным отклонением σs. Тогда рас - пределение s ˆ при заданном s из уравнения (4.5) равно (C.10) Мы не будем предполагать, что знаем σs. В таком случае модель имеет два параметра – σ и σs. Логарифмическое правдоподобие обоих параметров: (C.11) Аналитические методы в данном случае неприменимы. Следовательно, MLE σ и σs должны быть найдены численно. Мы сделаем это, используя поиск по сетке. Определим сетку для σ и σs. Выберем для обоих векторов диапазон от 0.02 до 10 на 100 шагов. Теперь мы можем перебрать оба вектора (двойной цикл for) и для каждой комбинации σ и σs в сетке оценить логарифмиче- ское правдоподобие по уравнению (C.11). После завершения этого процесса построим логарифмическое правдоподобие как функцию σ и σs; поскольку есть две независимые переменные, имеет смысл построить тепловую карту (рис. C.4A). Максимум можно найти численно: σˆ = 1.76 и σˆs = 1.24; эти значе- ния близки к тем, которые мы использовали для генерации данных, а именно 1.5 и 1 соответственно. (Из-за шума в данных мы никогда не ожидаем найти точные значения, при которых сгенерировали данные.) Значение максимума составляет log �∗ = -13.2, тогда как в простой модели оно было -40.0, что намного хуже. Чтобы проверить модель, снова построим диаграмму рассеяния оценки стимула по сравнению с истинным стимулом (рис. C.4B). Прогноз теперь намного лучше, чем на рис. C.2B. Вы можете воочию наблюдать последствия слепой веры в оценки параметров в плохо подобранных моделях: если бы мы доверились оценке параметра σˆ = 3.49 из раздела C.5, мы бы полностью от - клонились от истинного значения 1.5. Это подтверждает важность проверки модели и, если она плохо соответствует данным, отказа от нее.\n--- Страница 431 ---\n430  Подбор параметров и сравнение моделей 5 4 3 2 1–50 –100 –150 –200 –250 –3006 4 2 0 –2 –4 –6Гипотетическое σs Гипотетическое σ12345 Логарифмическое правдоподобие(A) (В) Ожидание стимула sˆ –6 –4 –2 0246 Истинный стимул s Рис. С.4  Улучшенная модель прежних данных (на рис. C.2): (A) ландшафт логарифмиче- ского правдоподобия как функция σ и σs для данных на рис. C.2A. Более красный цвет озна- чает более высокий логарифм правдоподобия. Значения логарифмического правдоподобия, меньшие –300, были установлены равными –300, поскольку в противном случае различия в логарифмическом правдоподобии вблизи пика не были бы видны. Максимум обозначен бе- лым крестиком; максимальное значение равно log �∗ = −10.2; (B) проверка модели: прогнозы модели с оценками параметров из части (A). Черные точки – отдельные симуляции. Зеленые кружки – среднее значение по симуляциям для одного и того же стимула. Эта модель визуаль- но намного больше похожа на данные, чем простая модель на рис. C.3 C.5.3. Сравнение моделей В простой модели максимальное логарифмическое правдоподобие состав- ляло примерно log �∗ = -40.0. В более сложной модели это было примерно log �∗ = -13.2. Достаточно ли велика разница в 26.8, чтобы мы могли отвер- гнуть простую модель? Более сложная модель имеет один дополнительный параметр, что делает ее более точной, но окупается ли усложнение выигры- шем в максимальном логарифмическом правдоподобии? Отсюда вытекает важный и очень распространенный вопрос: как сравнивать модели после окончания подбора параметров? Двумя общепринятыми подходами явля- ются информационный критерий Акаике (AIC) [8] и байесовский информа- ционный критерий (BIC) [159]. Оба берут максимальное логарифмическое правдоподобие log �∗ в качестве отправной точки, но затем накладывают штраф на модель за количество свободных параметров. Различие между AIC и BIC заключается в характере штрафа. В AIC количество свободных параметров вычитается из log �∗, затем результат умножается на -2. Этот множитель ничего не означает, это просто соглашение, так что для гауссовой модели главный член AIC и BIC представляет собой сумму квадратов ошибок. В BIC количество свободных параметров, умноженное на половину логариф- ма числа испытаний, вычитается из log �∗, затем результат умножается на -2. Уравнения выглядят следующим образом: AIC = -2(log �∗ - nparameters ); (C.12)\n--- Страница 432 ---\nПодбор параметров по данным из задачи оценки  431 (C.13) Затем более высокий AIC или BIC интерпретируется как свидетельство того, что модель хуже. Однако как AIC, так и BIC основаны на сильных пред- положениях о задействованных данных или моделях, и поэтому при интер- претации их результатов рекомендуется соблюдать осторожность. Возможно, AIC и BIC являются проблематичными инструментами для оценки качества подгонки в контексте моделей, представленных в этой книге. Концептуально более совершенной можно считать перекрестную проверку, поскольку она действительно отражает прогностические способности модели, естествен- ным образом исключая чрезмерно сложные варианты моделей. C.5.4. Т ест отношения правдоподобия Тест отношения правдоподобия – это метод, который немного отличается по своей природе от AIC и BIC. Этот метод может применяться, когда одна модель является подмножеством другой модели. Чтобы сравнить две такие модели, можно использовать критерий отношения правдоподобия. Если M1 – более конкретная (ограниченная, null) модель, а M2 – более общая (неогра- ниченная), показатель правдоподобия равен D = 2(log �∗ M2 - log �∗ M1). (C.14) Если бы истинной моделью была M1 (null-модель), то D приблизительно следовало бы распределению хи-квадрат с числом степеней свободы, равным разнице свободных параметров M2 и M1. P-значение теста – это вероятность того, что D в null-модели больше, чем наблюдаемое значение D. Когда одна модель не «вложена» в другую модель, использовать отношение правдопо- добия невозможно. C.5.5. Перекрестная проверка В то время как тесты AIC, BIC и отношения правдоподобия основаны на от - носительно схожем наборе идей, перекрестная проверка стоит особняком от них. Идея перекрестной проверки заключается в том, что хорошая мо- дель должна присваивать высокую вероятность незнакомым данным. При K-кратной перекрестной проверке данные делятся на K частей. Эти части должны быть статистически эквивалентны, например не может быть, чтобы все стимулы одного типа находились в одной и той же части. Затем одну часть убирают из данных и модель подгоняют к оставшимся данным. Качество подбора модели оценивают путем вычисления логарифмического правдо- подобия подобранных параметров в оставшейся части данных. Перебирая K частей, получают K значений, которые затем усредняются; результат на-\n--- Страница 433 ---\n432  Подбор параметров и сравнение моделей зывается логарифмическим правдоподобием с перекрестной проверкой. Его можно использовать непосредственно для сравнения моделей. Хорошими значения для K считаются 5 или 10; если К слишком мало, результат будет слишком сильно зависеть от случайного состава частей, а если К слишком велико, в каждой части будет слишком мало испытаний. Философия пере- крестной проверки заключается в том, что хорошая модель должна быть в со- стоянии делать точные прогнозы для незнакомых входных данных. Поэтому базовые допущения перекрестной проверки сравнительно просты. C.5.6. Сопоставление методов сравнения моделей Происхождение этих метрик выходит за рамки данной книги, но стоит отме - тить, что они имеют разные корни: AIC и перекрестная проверка предназна- чены для измерения (не)способности модели предсказывать новые данные, BIC предназначен для объяснения существующих данных – тонкое, но важ - ное концептуальное различие. На практике наиболее важное различие между AIC и BIC заключается в том, что штраф BIC за дополнительный параметр обычно больше, чем штраф AIC. Однако концептуально перекрестная про- верка проще и, возможно, требует более слабых допущений, чем AIC и BIC. Считается, что AIC может недостаточно штрафовать свободные парамет - ры, тогда как BIC может чрезмерно штрафовать их, но эти тонкие нюансы подлежат дальнейшему исследованию. Другой предмет спора заключается в том, является ли перекрестная проверка универсально предпочтительной по сравнению с использованием AIC и BIC. С практической точки зрения мы рекомендуем рассчитывать AIC, BIC, по- тенциальное отношение правдоподобия, а также перекрестную проверку ло- гарифмической вероятности и быть уверенными в результатах только в том случае, если полученные показатели качества модели согласуются между собой. Проблематично делать убедительные научные выводы, если можно легко прийти к противоположному заключению, просто изменив метрику сравнения моделей. Помимо однородности показателей, важна их величина. Насколько велика разница в показателях сравнения моделей? Конечно, любая классификация произвольна, но это не мешает авторам так или иначе постулировать свои категории. Например, Джеффрис придумал описательную классификацию, которую мы можем применить к разнице в AIC, разнице в BIC или к двукрат - ной разнице в перекрестной проверке логарифмического правдоподобия. В этой классификации разница выше 4.6 считается «сильной», между 2.3 и 4.6 – «значительной», а ниже 2.3 – «едва ли заслуживает упоминания». Такие ярлыки неявно указывают на распределение силы доказательств, обнаруженных в экспериментах в конкретной предметной области, но это распределение трудно установить, и оно зависит от предметной области. В психологии и нейрологии мы предпочитаем быть более осторожными и на- зывать различия больше примерно 10 «существенными», а больше примерно 20 – «крупными». Конечно, хорошо бы полностью освободиться от нарратив- ных ярлыков, но людям трудно без них обходиться.\n--- Страница 434 ---\nПодбор параметров по данным из задачи оценки  433 C.5.7. Восстановление параметров и восстановление модели На практике оценка параметров и сравнение моделей заметно усложняются, когда имеется несколько моделей, каждая из которых содержит несколько параметров, и вероятность параметра вычислить нелегко. Поэтому при про- граммировании так легко допустить ошибки. Важным способом отладочной проверки является создание «фальшивых наборов данных» с помощью каж - дой из рассматриваемых моделей путем имитации измерений и решений наблюдателя в соответствии с моделью, как мы делали несколько раз в пре- дыдущих главах, а затем подгонки по методу максимального правдоподобия и сравнения байесовских моделей на этих поддельных наборах данных. Для каждого набора данных подгонка модели должна возвращать значения па- раметров, близкие к тем, с которыми был создан набор данных. Для большей уверенности лучше всего создавать не один набор данных для каждой моде- ли, а несколько, с разными комбинациями параметров. Более того, в каждом наборе данных сравнение байесовских моделей должно показать истинную базовую модель как победителя. Эти проверки зависят от того, насколько хорошо различимы модели и насколько велики имитационные наборы дан- ных. Однако если вы заметили, что модель B систематически выигрывает на наборах данных, сгенерированных моделью A, значит, явно что-то не так с вашим кодом сравнения моделей. Помимо отладки, подобный процесс восстановления модели также служит для определения того, различимы ли две модели на практике. C.5.8. Ограничения сравнения моделей Разные ученые хотят найти ответы на разные вопросы, и, следовательно, сравнение моделей не дает простого правильного ответа. Для некоторых задач некоторые ученые могут искать только наиболее подходящую модель. Что касается других задач, ученые могут постараться вложить в модель опи- сания перцептивных, когнитивных или двигательных процессов. Простое добавление параметров к модели для достижения лучшего показателя при сравнении моделей не делает эту модель или эти параметры значимыми. Многие ученые при выборе компонентов модели опираются на следующие соображения: понимание экологической ниши или проблемы; предыдущие модели с этим параметром или независимые экспери - менты; нарративная гипотеза о психологическом процессе. Например, когда мы работаем над нормативными моделями, может воз- никнуть необходимость, чтобы наши модели начинались с описания задачи, возможно, неопределенности в восприятии и функции потерь. Или, если мы работаем с редукционистскими моделями, мы можем стремиться опи- сать поведение с точки зрения того, что делают нейроны. Следовательно,\n--- Страница 435 ---\n434  Подбор параметров и сравнение моделей нормативные модели должны быть представлены через переменные, опи- сывающие задачу, а редукционистские модели должны быть основаны на переменных, описывающих части нервной системы. Байесовские модели обычно имеют сильное обоснование с точки зрения нарративной гипотезы: мозг эволюционировал, чтобы выполнять опреде- ленные задачи, близкие к оптимальным, за исключением шума. Но даже в байесовских моделях можно делать произвольные, плохо обоснованные предположения только для того, чтобы подогнать модель к данным. По воз- можности следует избегать любых отступлений от строгих предпосылок. C.6. Абсолютное качество подгонки Техническое ограничение сравнения моделей заключается в том, что даже лучшая из имеющихся моделей может оставаться плохой моделью. Было бы полезно понять, насколько хороша модель в абсолютном смысле, то есть по сравнению с наилучшей возможной моделью. Наилучшей возможной моде- лью данных являются не сами данные, учитывая присущую им стохастич- ность (шум). Например, в заданном испытании вы можете предсказать, что испытуемый выберет вариант А с вероятностью p и вариант В с вероятностью 1 - p, но вы не сможете точно предсказать выбор в этом испытании. Лучшее, что вы можете сделать, – это сделать так, чтобы р соответствовало эмпири- ческой вероятности ответа А в испытаниях этого типа. Тогда логарифмическое правдоподобие такой идеальной модели, внесен- ное одним испытанием этого типа, будет равно log p, если ответ равен A, то есть с вероятностью p, и log(1 - p), если ответ равен B, то есть с вероятностью 1 - p. Объединив эти два фактора, мы видим, что ожидаемый вклад испыта- ния в логарифмическое правдоподобие этой идеальной модели будет равен log �(идеальная модель; данные) = p log p + (1 - p)log(1 - p). (C.15) Это и есть отрицательная энтропия данных. Энтропия (дискретного) рас - пределения вероятностей является мерой неопределенности: детермини- рованное отображение имеет самую низкую энтропию (0), а равномерное распределение имеет самую высокую возможную энтропию 1/N , где N – ко - личество альтернатив. В целом оказывается верным следующее соотношение между логарифми- ческим правдоподобием и отрицательной энтропией: log �(любая модель; данные) £ -Entropy(данные). (C.16) Другими словами, отрицательная энтропия данных обеспечивает верхнюю границу качества подгонки любой модели; по этой причине ее также на- зывают шумовым потолком. Чем шумнее данные, тем выше энтропия и тем ниже верхняя граница логарифмического правдоподобия любой модели. Для оценки шумового потолка можно использовать методы машинного обуче- ния [7, 64].\n--- Страница 436 ---\nПодбор параметров модели по данным в задаче различения  435 C.7. Подбор параметров модели по данным в задаче различения Мы завершаем это приложение еще двумя примерами подбора модели. Сна- чала мы рассмотрим следующие гипотетические данные из задачи различе- ния, как в главе 7, с ∆s = 1. Данные можно полностью охарактеризовать, как показано в табл. C.2. Два значения стимула встречаются одинаково часто (в данном случае по 200 раз каждое). Таблица С.2. Пример данных из задачи различения Истинный стимул Отклики наблюдателя s+Отклики наблюдателя s– s+ 138 испытаний 62 испытания s- 90 испытаний 110 испытаний C.7.1. Простая модель Наша цель снова состоит в том, чтобы подобрать параметр шума σ. В этом экс - перименте, поскольку и s, и sˆ могут принимать только два значения, прогноз модели p(sˆ | s, σ) состоит всего из четырех чисел, а именно p(sˆ = s+ | s = s+, σ), p(sˆ = s- | s = s+, σ), p(sˆ = s+ | s = s-, σ) и p(sˆ = s- | s = s-, σ). Если наблюдатель оптимален и поэтому знает, что два стимула встречаются одинаково часто, мы используем уравнения (7.20)–(7.21) для оценки этих четырех вероятно- стей как (C.17) (C.18) Единственным свободным параметром является σ. Логарифмическое правдоподобие σ согласно уравнению (C.3) равно (C.19) Испытания можно сгруппировать в четыре комбинации стимула и откли- ка. Тогда сумма в уравнении (C.19) становится суммой четырех членов: log �(σ; данные) = n++log p(sˆi = s+ | si = s+, σ) + n+-log p(sˆi = s- | si = s+, σ) (C.20) + n-+log p(sˆi = s+ | si = s-, σ) + n--log p(sˆi = s- | si = s-, σ), (C.21) где мы используем обозначения:\n--- Страница 437 ---\n436  Подбор параметров и сравнение моделей n++ n+- n-+ n--количество испытаний, где s = s+ и sˆ = s+; количество испытаний, где s = s+ и sˆ = s-; количество испытаний, где s = s- и sˆ = s+; количество испытаний, где s = s- и sˆ = s+. И логарифмическое правдоподобие, и правдоподобие представлены как функция σ на рис. C.5. Правдоподобие было получено путем возведения в степень логарифмического правдоподобия. –250 –300 –350 –400 –4504 3 2 1 0 12345 12345Логарифмическое правдоподобие Правдоподобие Гипотетическое σ Гипотетическое σ(A) (В) ×10–116 Рис. С.5  Логарифмическая функция правдоподобия и функция правдоподобия по σ, полученные из простой модели в разделе C.7.1 Для нахождения MLE σ применим аналитический метод (метод 1). Возьмем производную уравнения (C.21), приравняем ее к 0 и найдем σ . Ответ таков: (C.22) где ncorrect = n++ + n-- и nincorrect = n+- + n-+ – количество правильных и не- правильных результатов испытаний соответственно. Это выражение опре- деляется тем, что в данной конкретной модели p(sˆi = s+|si = s+, σ) и p(sˆi = s-|si = s-, σ) равны. В приведенном примере уравнение (C.22) возвращает σˆ = 1.64. Соответствующее максимальное логарифмическое правдоподобие равно log �∗ = -265.6. Как и в разделе C.5.1, важно проверить модель. Вероятность правильного ответа составляет 0.62 в соответствии с моделью, а предсказанное количест - во ответов соответствует табл. C.3. Очевидно, что эта модель не учитывает правильное неравное соотношение между s+ и s-, наблюдаемое в данных (табл. C.2). Чтобы попытаться объяснить эти неравные пропорции, мы рас - смотрим более гибкую модель.\n--- Страница 438 ---\nПодбор параметров модели по данным в задаче классификации  437 Таблица С.3. Проверка простой модели Истинный стимул Отклики модели s+Отклики модели s– s+ 124 76 s- 76 124 C.7.2. Более удачная модель? В главе 7 мы также рассмотрели более гибкую модель, в которой априорная вероятность наблюдателя не равна 0.5. Теперь модель имеет два параметра: σ и априорную вероятность. Поскольку наши данные в этом очень простом эксперименте также составляют два числа (доля правильных ответов при стимуле s+ и при стимуле s-), мы могли бы подогнать модель к двум точкам данных с помощью двух параметров. Даже не подбирая эти параметры, мы знаем, что это можно сделать идеально (см. задачу C.2). Это не означает, что каждое испытание можно точно предсказать, просто предсказанные вероят - ности откликов точно совпадают с эмпирическими вероятностями; другими словами, это «идеальная модель», как описано в разделе C.6. Согласно уравнению (C.21), логарифмическое правдоподобие снова со- стоит из четырех членов. Максимальное логарифмическое правдоподобие этой двухпараметрической модели (см. задачу C.1): (C.23) Результат составляет -261.4, что на 4.2 выше, чем в простой модели раз- дела C.7.1. Разница настолько велика, что эта модель выигрывает как в AIC, так и в BIC. Тем не менее это не особенно удачная модель. Фактически по- бедившая модель эквивалентна простому описанию каждого условия. Сле- довательно, данные в этом эксперименте слишком просты, чтобы делать на их основе убедительные выводы. C.8. Подбор параметров модели по данным в задаче классификации В главе 8 мы рассмотрели задачи бинарной классификации. Данные такой задачи богаче, чем в задаче различения в разделе C.7. Пример данных при- веден в табл. C.4.\n--- Страница 439 ---\n438  Подбор параметров и сравнение моделей Модель в этой задаче представляет собой распределение вероятностей p(Cˆ | s, θ), где θ – набор всех параметров. Таким образом, логарифмическая функция правдоподобия принимает вид: (C.24) Более подробно этот вопрос рассмотрен в задаче 8.11. В некоторых случа- ях большая сумма по испытаниям может быть разделена на сумму по уни- кальным комбинациям стимул–реакция. Затем для каждой комбинации мы должны умножить слагаемое на количество экземпляров каждой комбина- ции, аналогично уравнению (C.21). C.9. Правильный план эксперимента для байесовского моделирования Первое требование для успешного построения байесовской модели ваших поведенческих данных – правильно спланировать эксперимент. Для этого существуют хорошо известные общие рекомендации [202], а для байесовских моделей есть несколько дополнительных советов. В общем случае необходи- мо исключить как можно больше параметров, не представляющих интереса для научного исследования. Например, чтобы изучить, как люди выполня- ют различение, следует предъявлять стимулы в течение короткого времени (несколько десятков миллисекунд), чтобы избежать осложнений, связанных с движениями глаз, переключением внимания и интеграцией информации во времени – все перечисленные факторы могут повлиять на качество коди- рования (т. е. стандартное отклонение распределения шума) потенциально сложным образом. Эксперименты, связанные со временем отклика, обычно более сложны для моделирования, чем эксперименты, связанные с точно- стью и основанные на коротком времени представления стимула. Следова- тельно, если специфика вашего исследования позволяет вам организовать более точную версию того же эксперимента, это, скорее всего, сэкономит вам трудозатраты и время вычислений при моделировании. Таблица С.4. Пример данных из задачи классификации Номер испытания Стимул s Отклик субъекта s ˆ 1 4.3 1 2 1.0 1 3 2.1 1 4 0.7 1 5 4.5 1 6 3.5 1 … … …\n--- Страница 440 ---\nРекомендуемая литература  439 Точно так же нам нужно, чтобы атрибуты стимулов, которые не представля- ют интереса, не менялись от стимула к стимулу. В частности, не забудьте тща- тельно контролировать надежность или уровень шума стимулов. Например, если вы размещаете на дисплее несколько элементов, размещение их по кругу вокруг точки фиксации, а не по прямоугольной сетке, гарантирует, что эксцент - риситет (расстояние от точки фиксации) будет одинаковым, и, следовательно, точность кодирования будет по крайней мере приблизительно одинаковой, что позволяет моделировать повторяемость с помощью одного параметра. Кроме того, необходимо хорошо знать эффекты, характерные для пред- метной области, способные повлиять на точность решения задачи. Напри- мер, когда два стимула сближены, может возникнуть эффект, известный как скученность, при котором внутренние представления обоих стимулов влияют друг на друга. Если скученность не представляет первостепенного интереса, лучше свести ее к минимуму, разместив стимулы достаточно да- леко друг от друга. Что характерно для байесовского моделирования, часто полезно использовать стимулы, интересующая характеристика которых яв- ляется одномерной или, самое большее, двумерной. Например, при изуче- нии комбинации сигналов легче смоделировать вспышку и звуковой сигнал, представленные на горизонтальной линии, чем смоделировать интеграцию слуховой и визуальной информации при восприятии речи. В перцептивных экспериментах, обсуждаемых в этой книге, мы используем как можно более простые стимулы: они имеют только одно релевантное измерение, например ориентацию. Такие стимулы, как буквы, штриховые рисунки, изображения объектов или природные сцены, гораздо труднее включить в модель, по- тому что они имеют много признаков, а в некоторых случаях даже не ясно, каковы соответствующие признаки (строительные блоки восприятия). Бо- лее того, большое количество признаков приводит к большому количеству измерений, а шумовые модели с большим количеством измерений имеют еще большее количество свободных параметров. Это вовсе не означает, что изучение сложных стимулов неинтересно. Мы всего лишь указываем на не- которые ограничения байесовского моделирования. Наконец, мы рекомендуем всем, кто заинтересован в построении байе- совской модели своей задачи, написать модель и запустить ее, прежде чем начать собирать данные. Для байесовских моделей, поскольку они основаны на принципах оптимальности, это всегда возможно. Данный процесс может выявить потенциальные проблемы в плане эксперимента. C.10. Рекомендуемая литература Hirotugu Akaike. A New Look at the Statistical Model Identiﬁcation. IEEE Trans- actions on Automatic Control 19, no. 6 (1974): 716–723. Luigi Acerbi and Wei Ji Ma. Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search. In Advances in Neural Information Processing Systems, edited by Ulrike von Luxburg, Isabelle Guyon, Samy Bengio, Hann Wallach, and Rob Fergus, 1834–1844. Red Hook, NY: Curran, 2017.\n--- Страница 441 ---\n440  Подбор параметров и сравнение моделей Sylvain Arlot and Alain Celisse. A Survey of Cross­ Validation Procedures for Model Selection. Statistics Surveys 4 (2010): 40–79. David J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge: Cambridge University Press, 2003. In Jae Myung. Tutorial on Maximum Likelihood Estimation. Journal of Mathe- ma tical Psychology 47, no. 1 (2003): 90–100. Gideon Schwarz. Estimating the Dimension of a Model. Annals of Statistics 6, no. 2 (1978): 461–464. Lionel Rigoux, Klaas E. Stephan, Karl J. Friston, and Jean Daunizeau. Bayesian Model Selection for Group Studies–Revisited. Neuroimage 84 (2014): 971–985. Bas van Opheusden, Luigi Acerbi, and Wei Ji Ma. Unbiased and Efficient LogLikelihood Estimation with Inverse Binomial Sampling. PLoS Computational Biology 16, no. 12 (2020): e1008483. Scott I. Vrieze. Model Selection and Psychological Theory: A Discussion of the Differences between the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). Psychological Methods 17, no. 2 (2012): 228–243. Robert C. Wilson and Anne G. E. Collins. Ten Simple Rules for the Computational Modeling of Behavioral Data. Elife 8 (2019): e49547. C.11. Задачи Задача C.1. Выведите уравнение (C.23). Задача C.2. Уравнения (7.22)–(7.23) задают вероятности отклика в байесов- ской модели различения: (C.25) (C.26) где p+ = p(s = s+) – априорная вероятность s+. Как экспериментатор вы знаете значение ∆s . Ответы испытуемого дают вам две доли слева от знака равенства. Найдите σ и p+. Данный пример показывает, что, используя упомянутые два параметра, всегда можно точно предсказать пропорции откликов в этой задаче. Задача C.3. Эта задача касается двух моделей из раздела C.7. (a) Рассчитайте значения AIC для обеих моделей. Какая модель лучше по AIC? (b) Рассчитайте значения BIC для обеих моделей. Какая модель лучше по BIC? (c) Рассчитайте десятикратную перекрестную проверку логарифмического правдоподобия обеих моделей. Какая модель лучше с точки зрения пере - крестной проверки логарифмического правдоподобия? (d) Выполните тест отношения правдоподобия. Намного ли более сложная модель лучше, чем простая?\n--- Страница 442 ---\nБиблиография [1] Luigi Acerbi and Wei Ji Ma. Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search. Advances in Neural Information Pro- cessing Systems, edited by Ulrike von Luxburg, Isabelle Guyon, Samy Bengio, Hann Wallach, and Rob Fergus, 1834–1844. Red Hook, NY: Curran, 2017. [2] Luigi Acerbi, Sethu Vijayakumar, and Daniel M. Wolpert. On the Origins of Suboptimality in Human Probabilistic Inference. PLoS Computational Biology 10, no. 6 (2014): e1003661. [3] Daniel E. Acuña and Paul Schrater. Structure Learning in Human Sequential Decision­Making. PLoS Computational Biology 6, no. 12 (2010): e1001003. [4] Ryan Prescott Adams and David J. C. MacKay. Bayesian Online Changepoint Detection. arXiv preprint arXiv:0710.3742 (2007). [5] Wendy J. Adams. A Common Light ­Prior for Visual Search, Shape, and Re ﬂec­ tance Judgments. Journal of Vision 7, no. 11 (2007). [6] Wendy J. Adams, Erich W. Graf, and Marc O. Ernst. Experience Can Change the ‘Light ­from­above’ Prior. Nature Neuroscience 7, no. 10 (2004): 1057–1058. [7] Mayank Agrawal, Joshua C. Peterson, and Thomas L. Griffiths. Scaling up Psychology via Scientiﬁc Regret Minimization. Proceedings of the National Academy of Sciences 117, no. 16 (2020): 8825–8835. [8] Hirotugu Akaike. A New Look at the Statistical Model Identiﬁcation. IEEE Transactions on Automatic Control 19, no. 6 (1974): 716–723. [9] David Alais and David Burr. The Ventriloquist Effect Results from Near­Opti- mal Bimodal Integration. Current Biology 14, no. 3 (2004): 257–262. [10] Alhacen. Alhacen’s Theory of Visual Perception: A Critical Edition, with English Translation and Commentary, of the First Three Books of Alhacen’s De As- pectibus, the Medieval Latin Version of Ibn Al-Haytham’s Kitab Al­Manazir. Edited by A. Mark Smith. Vol. 1. Philadelphia, PA: American Philosophi- cal Society, 2001. [11] John R. Anderson. The Adaptive Nature of Human Categorization. Psychologi- cal Review 98, no. 3 (1991): 409–429. [12] John R. Anderson. The Adaptive Character of Thought. New York: Psychology Press, 2013. [13] J. Yu Angela. Adaptive Behavior: Humans Act as Bayesian Learners. Current Biology 17, no. 22 (2007): R977–R980. [14] Sylvain Arlot and Alain Celisse. A Survey of Cross­ Validation Procedures for Model Selection. Statistics Surveys 4 (2010): 40–79. [15] Bahador Bahrami et al. Optimally Interacting Minds. Science 329, no. 5995 (2010): 1081–1085. [16] F. R. S. Bayes. An Essay towards Solving a Problem in the Doctrine of Chances. Biometrika 45, no. 3–4 (1958): 296–315.\n--- Страница 443 ---\n442  Библиография [17] Suzanna Becker and Geoffrey E. Hinton. Self­Organizing Neural Network that Discovers Surfaces in Random­Dot Stereograms. Nature 355, no. 6356 (1992): 161–163. [18] Vikranth Rao Bejjanki, Meghan Clayards, David C. Knill, and Richard N. Aslin. Cue Integration in Categorical Tasks: Insights from Audio­ Visual Speech Perception. PloS One 6, no. 5 (2011): e19812. [19] Christopher M. Bishop. Pattern Recognition. Machine Learning 128, no. 9 (2006). [20] Ned Block. If Perception Is Probabilistic, Why Does It Not Seem Probabilistic? Philosophical Transactions of the Royal Society B: Biological Sciences 373, no. 1755 (2018): 20170341. [21] Kathryn Bonnen, Johannes Burge, Jacob Yates, Jonathan Pillow, and Law - rence K. Cormack. Continuous Psychophysics: Target ­Tracking to Measure Visual Sensitivity. Journal of Vision 15, no. 3 (2015): 14. [22] Jeffrey S. Bowers and Colin J. Davis. Bayesian Just ­so Stories in Psychology and Neuroscience. Psychological Bulletin 138, no. 3 (2012): 389–414. [23] David H. Brainard and William T. Freeman. Bayesian Color Constancy. Journal of the Optical Society of America 14, no. 7 (1997): 1393–1411. [24] Anthony E. Brockwell, Alex L. Rojas, and Robert E. Kass. Recursive Bayesian Decoding of Motor Cortical Signals by Particle Filtering. Journal of Neuro- physiology 91, no. 4 (2004): 1899–1907. [25] Anne-Marie Brouwer and David C. Knill. The Role of Memory in Visually Guided Reaching. Journal of Vision 7, no. 5 (2007): 6.1–12. [26] Peter Brugger and Susanne Brugger. The Easter Bunny in October: Is It Dis- guised as a Duck? Perceptual and Motor Skills 76, no. 2 (1993): 577–578. [27] Heinrich H. Bulthoff. Bayesian Decision Theory and Psychophysics. Percep- tion as Bayesian Inference, edited by David C. Knill and Whitman Richards, 123–162. Cambridge: Cambridge University Press, 1996. [28] Daniel Burdon. Pigs Float down the Dawson. Morning Bulletin, February 9, 2011. [29] Sherrington C. S. The Integrative Action of the Nervous System. C. Scribner and Sons: New York, 1906. [30] Joshua Calder-Travis and Wei Ji Ma. Explaining the Effects of Distractor Sta- tistics in Visual Search. Journal of Vision 20, no. 13 (2020): 11. [31] Claire Chambers, Taegh Sokhey, Deborah Gaebler-Spira, and Konrad Paul Kording. The Development of Bayesian Integration in Sensorimotor Estimation. Journal of Vision 18, no. 12 (2018): 8. [32] Gar Ming Chan. Bayes’ Theorem, COVID19, and Screening Tests. American Journal of Emergency Medicine 38, no. 10 (2020): 2011–2013. [33] Nick Chater, Noah Goodman, Thomas L. Griffiths, Charles Kemp, Mike Oaksford, and Joshua B. Tenenbaum. The Imaginary Fundamentalists: The Unshocking Truth about Bayesian Cognitive Science. Behavioral and Brain Sciences 34, no. 4 (2011): 194–196. [34] Nick Chater and Mike Oaksford, eds. The Probabilistic Mind: Prospects for Bayesian Cognitive Science. New York: Oxford University Press, 2008. [35] Stephanie Y. Chen, Brian H. Ross, and Gregory L. Murphy. Implicit and Explicit Processes in CategoryBased Induction: Is Induction Best When We Don’t Think? Journal of Experimental Psychology: General 143, no. 1 (2014): 227–246.\n--- Страница 444 ---\nБиблиография  443 [36] Youguo Chen, Bangwu Zhang, and Konrad P . Körding. Speed Constancy or Only Slowness: What Drives the Kappa Effect. PloS One 11, no. 4 (2016): e0154013. [37] Patricia W. Cheng. From Covariation to Causation: A Causal Power Theory. Psychological Review 104, no. 2 (1997): 367–405. [38] Anna Coenen, Bob Rehder, and Todd M. Gureckis. Strategies to Intervene on Causal Systems Are Adaptively Selected. Cognitive Psychology 79 (2015): 102–133. [39] Gloria Cooper. Red Tape Holds up New Bridge, and More Flubs from the Na- tion’s Press. New York: TarcherPerigee, 1987. [40] Carl F. Craver. Explaining the Brain: Mechanisms and the Mosaic Unity of Neu- roscience. Oxford: Clarendon Press, 2007. [41] Lorraine Daston. How Probabilities Came to Be Objective and Subjective. His- toria Mathematica 21, no. 3 (1994): 330–344. [42] Sophie Deneve. Bayesian Spiking Neurons I: Inference. Neural Computation 20, no. 1 (2008): 91–117. [43] Rachel N. Denison. Precision, Not Conﬁdence, Describes the Uncertainty of Perceptual Experience: Comment on John Morrison’s ‘Perceptual Conﬁdence.’ Analytic Philosophy 58, no. 1 (2017): 58–70. [44] Rachel N. Denison, William T. Adler, Marisa Carrasco, and Wei Ji Ma. Humans Incorporate AttentionDependent Uncertainty into Perceptual Decisions and Conﬁdence. Proceedings of the National Academy of Sciences 115, no. 43 (2018): 11090–11095. [45] Kenji Doya, Shin Ishii, Alexandre Pouget, and Rajesh P . N. Rao. Bayesian Bra in: Probabilistic Approaches to Neural Coding. Cambridge, MA: MIT Press, 2007. [46] Miguel P . Eckstein. Visual Search: A Retrospective. Journal of Vision 11, no. 5 (2011): 14. [47] Marc O. Ernst and Martin S. Banks. Humans Integrate Visual and Haptic Information in a Statistically Optimal Fashion. Nature 415, no. 6870 (2002): 429–433. [48] J. St. B. T. Evans, P . G. Brooks, and P . Pollard. Prior Beliefs and Statistical Infe­ rence. British Journal of Psychology 76, no. 4 (1985): 469–477. [49] Jacob Feldman. Bayesian Contour Integration. Perception and Psychophysics 63 (2001): 1171–1182. [50] Norman Fenton. Improve Statistics in Court. Nature 479, no. 7371 (2011): 36–37. [51] Christopher R. Fetsch, Alexandre Pouget, Gregory C. DeAngelis, and Dora E. Angelaki. Neural Correlates of Reliability­Based Cue Weighting During Multi- sensory Integration. Nature Neuroscience 15, no. 1 (2012): 146–154. [52] Brian J. Fischer and José Luis Peña. Owl’s Behavior and Neural Representa- tion Predicted by Bayesian Inference. Nature Neuroscience 14, no. 8 (2011): 1061–1066. [53] József Fiser, Pietro Berkes, Gergő Orbán, and Máté Lengyel. Statistically Optimal Perception and Learning: From Behavior to Neural Representations. Trends in Cognitive Sciences 14, no. 3 (2010): 119–130. [54] Jerry A. Fodor. Psychological Explanation: An Introduction to the Philosophy of Psychology. New York: Random House, 1968.\n--- Страница 445 ---\n444  Библиография [55] Peter Földiák. The ‘Ideal Homunculus’: Statistical Inference from Neural Popu - lation Responses. Computation and Neural Systems, edited by Frank H. Eeck - man and James M. Bower, 55–60. New York: Springer, 1993. [56] William T. Freeman. The Generic Viewpoint Assumption in a Framework for Visual Perception. Nature 368, no. 6471 (1994): 542–545. [57] Wilson S. Geisler and Jeffrey S. Perry. Contour Statistics in Natural Images: Grouping across Occlusions. Visual Neuroscience 26, no. 1 (2009): 109–121. [58] Wilson S. Geisler and Randy L. Diehl. A Bayesian Approach to the Evolu- tion of Perceptual and Cognitive Systems. Cognitive Science 27, no. 3 (2003): 379–402. [59] Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. Bayesian Data Analysis. New York: Chapman and Hall/ CRC, 1995. [60] George A. Gescheider. Psychophysics: The Fundamentals. New York: Psycho- logy Press, 2013. [61] Laura S. Geurts, James R. H. Cooke, Ruben S. van Bergen, and Janneke F. M. Jehee. Subjective Conﬁdence Reﬂects Representation of Bayesian Probability in Cortex. Nature Human Behaviour 6 (2022): 294–305. [62] Gerd Gigerenzer. On Narrow Norms and Vague Heuristics: A Reply to Kahne- man and Tversky. Psychological Review 103, no. 3 (1996): 592–596. [63] Ahna R. Girshick, Michael S. Landy, and Eero P . Simoncelli. Cardinal Rules: Visual Orientation Perception Reﬂects Knowledge of Environmental Statistics. Nature Neuroscience 14, no. 7 (2011): 926–932. [64] Joshua I. Glaser, Ari S. Benjamin, Roozbeh Farhoodi, and Konrad P . Kording. The Roles of Supervised Machine Learning in Systems Neuroscience. Progress in Neurobiology 175 (2019): 126–137. [65] Daniel Goldreich. A Bayesian Perceptual Model Replicates the Cutaneous Rab- bit and Other Tactile Spatiotemporal Illusions. PloS One 2, no. 3 (2007): e333. [66] Daniel Goldreich and Jonathan Tong. Prediction, Postdiction, and Perceptual Length Contraction: A Bayesian Low­Speed Prior Captures the Cutaneous Rab- bit and Related Illusions. Frontiers in Psychology 4 (2013): 221. [67] Daniel Goldreich and Mary A. Peterson. A Bayesian Observer Replicates Con- vexity Context Effects in Figure­ground Perception. Seeing and Perceiving 25, no. 3–4 (2012): 365–395. [68] Alison Gopnik, Clark Glymour, David M. Sobel, Laura E. Schulz, Tamar Kush- nir, and David Danks. A Theory of Causal Learning in Children: Causal Maps and Bayes Nets. Psychological Review 111, no. 1 (2004): 3–32. [69] Monica Gori, Michela Del Viva, Giulio Sandini, and David C. Burr. Young Children Do Not Integrate Visual and Haptic Form Information. Current Bio- logy 18, no. 9 (2008): 694–698. [70] David M. Green and John A. Swets. Signal Detection Theory and Psychophysics. Vol. 1. New York: Wiley, 1966. [71] Thomas L. Griffiths and Joshua B. Tenenbaum. Optimal Predictions in Every- day Cognition. Psychological Science 17, no. 9 (2006): 767–773. [72] Thomas L. Griffiths and Joshua B. Tenenbaum. Theory­Based Causal Induc - tion. Psychological Review 116, no. 4 (2009): 661–716.\n--- Страница 446 ---\nБиблиография  445 [73] Ralf M. Haefner, Pietro Berkes, and József Fiser. Perceptual Decision­Making as Probabilistic Inference by Neural Sampling. Neuron 90, no. 3 (2016): 649– 660. [74] Gary Hatﬁeld. Perception as unconscious inference. Perception and the physi- cal world: Psychological and philosophical issues in perception, Citeseer. 2002. [75] Michael J. Hautus, Neil A. Macmillan, and C. Douglas Creelman. Detection Theory: A User’s Guide. (2021). [76] Maija Honig, Wei Ji Ma, and Daryl Fougnie. Humans Incorporate Trial­to­ Trial Working Memory Uncertainty into Rewarded Decisions. Proceedings of the National Academy of Sciences 117, no. 15 (2020): 8391–8397. [77] Neil M. T. Houlsby, Ferenc Huszár, Mohammad M. Ghassemi, Gergő Orbán, Daniel M. Wolpert, and Máté Lengyel. Cognitive Tomography Reveals Complex, Task ­Independent Mental Representations. Current Biology 23, no. 21 (2013): 2169–2175. [78] Patrik O. Hoyer and Aapo Hyvärinen. Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior. Advances in Neural Information Processing Systems 15, edited by Suzanna Becker, Sebastian Thrun and Klaus Obermayer, 293–300. Cambridge, MA: MIT Press, 2002. [79] R. M. Isaacson and C. A. Was. Building a Metacognitive Curriculum: An Edu- cational Psychology to Teach Metacognition. National Teaching & Learning Forum, Vol. 19, no. 5. 2010: 1–4. [80] Robert A. Jacobs. Optimal Integration of Texture and Motion Cues to Depth. Vision Research 39, no. 21 (1999): 3621–3629. [81] William James. The Principles of Psychology. New York: Henry Holt, 1890. [82] Edwin T. Jaynes. Probability Theory: The Logic of Science. Cambridge: Cam- bridge University Press, 2003. [83] Mehrdad Jazayeri and J. Anthony Movshon. Optimal Representation of Sen- sory Information by Neural Populations. Nature Neuroscience 9, no. 5 (2006): 690–696. [84] Mehrdad Jazayeri and Michael N. Shadlen. Temporal Context Calibrates In- terval Timing. Nature Neuroscience 13, no. 8 (2010): 1020–1026. [85] Carolyn Y. Johnson, Yasmeen Abutaleb, and Joel Achenbach. CDC Study Shows Three­Fourths of People Infected in Massachusetts Coronavirus Out - break Were Vaccinated but Few Required Hospitalization. Washington Post, July 30, 2021. [86] Matt Jones and Bradley C. Love. Bayesian Fundamentalism or Enlightenment? On the Explanatory Status and Theoretical Contributions of Bayesian Models of Cognition. Behavioral and Brain Sciences 34, no. 4 (2011): 169–188. [87] Daniel Kahneman. Thinking, Fast and Slow. New York: Macmillan, 2011. [88] Daniel Kahneman, Paul Slovic, and Amos Tversky, eds. Judgment under Un- certainty: Heuristics and Biases. Cambridge: Cambridge University Press, 1982. [89] David Kappel, Stefan Habenschuss, Robert Legenstein, and Wolfgang Maass. Network Plasticity as Bayesian Inference. PLoS Computational Biology 11, no. 11 (2015): e1004485.\n--- Страница 447 ---\n446  Библиография [90] Charles Kemp, Andrew Perfors, and Joshua B. Tenenbaum. Learning Over- hypotheses with Hierarchical Bayesian Models. Developmental Science 10, no. 3 (2007): 307–321. [91] Daniel Kersten, Pascal Mamassian, and Alan Yuille. Object Perception as Bayesian Inference. Annual Review of Psychology 55 (2004): 271–304. [92] Daniel Kersten and Alan Yuille. Bayesian Models of Object Perception. Cur - rent Opinion in Neurobiology 13 (2003): 1–9. [93] David C. Knill. Mixture Models and the Probabilistic Structure of Depth Cues. Vision Research 43, no. 7 (2003): 831–854. [94] David C. Knill and Whitman Richards. Perception as Bayesian Inference. Cambridge: Cambridge University Press, 1996. [95] David C. Knill and Jeffrey A. Saunders. Do Humans Optimally Integrate Ste- reo and Texture Information for Judgments of Surface Slant? Vision Research 43, no. 24 (2003): 2539–2558. [96] Jan J. Koenderink and Andrea J. Van Doorn. The Internal Representation of Solid Shape with Respect to Vision. Biological Cybernetics 32, no. 4 (1979): 211–216. [97] Konrad Kording. Decision Theory: What ‘Should’ the Nervous System Do? Science 318, no. 5850 (2007): 606–610. [98] Konrad P . Kording, Ulrik Beierholm, Wei Ji Ma, Steven Quartz, Joshua B. Tenenbaum, and Ladan Shams. Causal Inference in Multisensory Perception. PLoS One 2, no. 9 (2007): e943. [99] Konrad P . Kording, Izumu Fukunaga, Ian S. Howard, James N. Ingram, and Daniel M. Wolpert. A Neuroeconomics Approach to Inferring Utility Functions in Sensorimotor Control. PLoS Biology 2, no. 10 (2004): e330. [100] Konrad P . Kording, Joshua B. Tenenbaum, and Reza Shadmehr. The Dynam- ics of Memory as a Consequence of Optimal Adaptation to a Changing Body. Nature Neuroscience 10, no. 6 (2007): 779–786. [101] Konrad P . Kording and Daniel M. Wolpert. Bayesian Integration in Senso- rimotor Learning. Nature 427, no. 6971 (2004): 244–247. [102] Konrad P . Kording and Daniel M. Wolpert. The Loss Function of Sensorimo- tor Learning. Proceedings of the National Academy of Sciences 101, no. 26 (2004): 9839–9842. [103] John Kruschke. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Amsterdam: Elsevier, 2014. [104] Rosa Lafer-Sousa, Katherine L. Hermann, and Bevil R. Conway. Striking In- dividual Differences in Color Perception Uncovered by ‘the Dress’ Photograph . Current Biology 25, no. 13 (2015): R545–R546. [105] Pierre-Simon Laplace. Memoir on the Probability of the Causes of Events. Statistical Science 1, no. 3 (1986): 364–378. [106] Pierre-Simon Laplace. Philosophical Essay on Probabilities. Edited and translated by Andrew I. Dale. New York: Springer Science and Business Media, 2012. [107] Tai Sing Lee and David Mumford. Hierarchical Bayesian Inference in the Visual Cortex. Journal of the Optical Society of America 20, no. 7 (2003): 1434–1448.\n--- Страница 448 ---\nБиблиография  447 [108] Hsin-Hung Li, Thomas C. Sprague, Aspen H. Yoo, Wei Ji Ma, and Clayton E. Curtis. Joint Representation of Working Memory and Uncertainty in Human Cortex. Neuron 109, no. 22 (2021): 3699–3712. [109] Falk Lieder and Thomas L. Griffiths. Resource­Rational Analysis: Under- standing Human Cognition as the Optimal Use of Limited Computational Resources. Behavioral and Brain Sciences 43, no. e1 (2020): 1–60. [110] Zili Liu, David C. Knill, and Daniel Kersten. Object Classiﬁcation for Human and Ideal Observers. Vision Research 35, no. 4 (1995): 549–568. [111] Lucretius [Titus Lucretius Carus]. The Nature of Things. Translated by A. E. Stal lings. London: Penguin, 2007. [112] Wei Ji Ma. Organizing Probabilistic Models of Perception. Trends in Cogni- tive Sciences 16, no. 10 (2012): 511–518. [113] Wei Ji Ma, Jeffrey M. Beck, Peter E. Latham, and Alexandre Pouget. Bayes- ian Inference with Probabilistic Population Codes. Nature Neuroscience 9, no. 11 (2006): 1432–1438. [114] Wei Ji Ma and Mehrdad Jazayeri. Neural Coding of Uncertainty and Prob- ability. Annual Review of Neuroscience 37 (2014): 205–220. [115] Wei Ji Ma, Vidhya Navalpakkam, Jeffrey M. Beck, Ronald van den Berg, and Alexandre Pouget. Behavior and Neural Basis of Near­Optimal Visual Search. Nature Neuroscience 14, no. 6 (2011): 783–790. [116] Wei Ji Ma, Shan Shen, Gintare Dziugaite, and Ronald van den Berg. Requiem for the Max Rule? Vision Research 116 (2015): 179–193. [117] Wei Ji Ma, Xiang Zhou, Lars A. Ross, John J. Foxe, and Lucas C. Parra. Lip­ Reading Aids Word Recognition Most in Moderate Noise: A Bayesian Explana- tion Using High ­Dimensional Feature Space. PloS One 4, no. 3 (2009): e4638. [118] David J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge: Cambridge University Press, 2003. [119] Tamas J. Madarasz, Lorenzo Diaz-Mataix, Omar Akhand, Edgar A. Ycu, Jo- seph E. LeDoux, and Joshua P . Johansen. Evaluation of Ambiguous Associa- tions in the Amygdala by Learning the Structure of the Environment. Nature Neuroscience 19, no. 7 (2016): 965–972. [120] Joseph G. Makin, Matthew R. Fellows, and Philip N. Sabes. Learning Multi- sensory Integration and Coordinate Transformation via Density Estimation. PLoS Computational Biology 9, no. 4 (2013): e1003035. [121] Laurence T. Maloney and Pascal Mamassian. Bayesian Decision Theory as a Model of Human Visual Perception: Testing Bayesian Transfer. Visual Neuroscience 26, no. 1 (2009): 147–155. [122] Laurence T. Maloney, Julia Trommershäuser, and Michael S. Landy. Ques- tions without Words: A Comparison between Decision Making under Risk and Movement Planning under Risk. Integrated Models of Cognitive Systems, edited by Wayne D. Gray, 297–313. New York: Oxford University Press, 2007. [123] Helga Mazyar, Ronald van den Berg, and Wei Ji Ma. Does Precision Decrease with Set Size? Journal of Vision 12, no. 6 (2012): 10. [124] Sharon Bertsch McGrayne. The Theory that Would Not Die. New Haven, CT: Yale University Press, 2011.\n--- Страница 449 ---\n448  Библиография [125] Harry McGurk and John MacDonald. Hearing Lips and Seeing Voices. Nature 264, no. 5588 (1976): 746–748. [126] Barbara Mellers, Ralph Hertwig, and Daniel Kahneman. Do Frequency Rep- resentations Eliminate Conjunction Effects? An Exercise in Adversarial Col- laboration. Psychological Science 12, no. 4 (2001): 269–275. [127] John P . Miller, Gwen A. Jacobs, and Frédéric E. Theunissen. Representa- tion of Sensory Information in the Cricket Cercal Sensory System. I. Response Properties of the Primary Interneurons. Journal of Neurophysiology 66, no. 5 (1991): 1680–1689. [128] John Morrison. Perceptual Conﬁdence. Analytic Philosophy 57, no. 1 (2016): 15–48. [129] Shane T. Mueller and Christoph T. Weidemann. Decision Noise: An Expla- nation for Observed Violations of Signal Detection Theory. Psychonomic Bulletin and Review 15, no. 3 (2008): 465–494. [130] Gregory L. Murphy, Stephanie Y. Chen, and Brian H. Ross. Reasoning with Uncertain Categories. Thinking and Reasoning 18, no. 1 (2012): 81–117. [131] In Jae Myung. Tutorial on Maximum Likelihood Estimation. Journal of Ma- thematical Psychology 47, no. 1 (2003): 90–100. [132] Jiri Najemnik and Wilson S. Geisler. Optimal Eye Movement Strategies in Visual Search. Nature 434, no. 7031 (2005): 387–391. [133] Marko Nardini, Peter Jones, Rachael Bedford, and Oliver Braddick. Deve­ lopment of Cue Integration in Human Navigation. Current Biology 18, no. 9 (2008): 689–693. [134] Daniel J. Navarro and Amy F. Perfors. Similarity, Feature Discovery, and the Size Principle. Acta Psychologica 133, no. 3 (2010): 256–268. [135] Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. Icml, Vol. 1. 2000:2. [136] Loren W. Nolte. An Adaptive Realization of the Optimum Receiver for a Spo- radically Recurrent Waveform in Noise (Corresp.). IEEE Transactions on Infor - mation Theory 13, no. 2 (1967): 308–311. DOI: 10.1109/TIT.1967.1053996. [137] Mohamed A. F. Noor, Robin S. Parnell, and Bruce S. Grant. A Reversible Color Polyphenism in American Peppered Moth (Biston betularia cognataria) Caterpillars. PloS One 3, no. 9 (2008): e3142. [138] Elyse H. Norton, Luigi Acerbi, Wei Ji Ma, and Michael S. Landy. Human Online Adaptation to Changes in Prior Probability. PLoS Computational Biology 15, no. 7 (2019): e1006681. [139] Pamela Licalzi O’Connell. Sweet Slips of the Ear: Mondegreens. New York Times, April 9, 1998. [140] A. Emin Orhan and Wei Ji Ma. Efficient Probabilistic Inference in Generic Neural Networks Trained with Non­Probabilistic Feedback. Nature Com- munications 8, no. 1 (2017): 1–14. [141] John Palmer, Preeti Verghese, and Misha Pavel. The Psychophysics of Visual Search. Vision Research 40, no. 10–12 (2000): 1227–1268. [142] Ryan M. Peters, Phillip Staibano, and Daniel Goldreich. Tactile Orientation Perception: An Ideal Observer Analysis of Human Psychophysical Perfor - mance in Relation to Macaque Area 3b Receptive Fields. Journal of Neuro- physiology 114, no. 6 (2015): 3076–3096.\n--- Страница 450 ---\nБиблиография  449 [143] W. Wesley Peterson, Theodore G. Birdsall, and William C. Fox. The Theory of Signal Detectability. Transactions of the IRE Professional Group on In- formation Theory 4 (1954): 171–212. [144] Karin Petrini, Alicia Remark, Louise Smith, and Marko Nardini. When Vi- sion Is Not an Option: Children’s Integration of Auditory and Haptic Informa- tion Is Suboptimal. Developmental Science 17, no. 3 (2014): 376–387. [145] Jonathan W. Pillow, Jonathon Shlens, Liam Paninski, Alexander Sher, Alan M. Litke, E. J. Chichilnisky, and Eero P . Simoncelli. Spatio­ Temporal Cor- relations and Visual Signalling in a Complete Neuronal Population. Nature 454, no. 7207 (2008): 995–999. [146] Alexandre Pouget, Peter Dayan, and Richard Zemel. Information Proces­ sing with Population Codes. Nature Reviews Neuroscience 1, no. 2 (2000): 125–132. [147] J. R. Pruett Jr., R. J. Sinclair, and H. Burton. Response Patterns in Second Somatosensory Cortex (SII) of Awake Monkeys to Passively Applied Tactile Gratings. Journal of Neurophysiology 84, no. 2 (2000): 780–797. [148] Dale Purves, William T. Wojtach, and R. Beau Lotto. Understanding Vision in Wholly Empirical Terms. Proceedings of the National Academy of Sciences 108, Supplement 3 (2011): 15588–15595. [149] Ahmad T. Qamar, R. James Cotton, Ryan G. George, and Wei Ji Ma. Trial­ to­Trial, Uncertainty­Based Adjustment of Decision Boundaries in Visual Categorization. Proceedings of the National Academy of Sciences 110, no. 50 (2013): 20332–20337. [150] Dobromir Rahnev and Rachel N. Denison. Suboptimality in Perceptual Deci- sion Making. Behavioral and Brain Sciences 41 (2018): e223. [151] Rajesh P . N. Rao. Bayesian Computation in Recurrent Neural Circuits. Neural Computation 16, no. 1 (2004): 1–38. [152] Lionel Rigoux, Klaas E. Stephan, Karl J. Friston, and Jean Daunizeau. Bayes- ian Model Selection for Group Studies Revisited. Neuroimage 84 (2014): 971–985. [153] David L. Roberts and Andrew R. Solow. Flightless Birds: When Did the Dodo Become Extinct? Nature 426, no. 6964 (2003): 245. [154] Ruth Rosenholtz. Visual Search for Orientation among Heterogeneous Dis- tractors: Experimental Results and Implications for Signal­Detection Theory Models of Search. Journal of Experimental Psychology: Human Perception and Performance 27, no. 4 (2001): 985–999. [155] Jason Rosenhouse. The Monty Hall Problem: The Remarkable Story of Math’s Most Contentious Brainteaser. Oxford: Oxford University Press, 2009. [156] Jenny R. Saffran, Richard N. Aslin, and Elissa L. Newport. Statistical Lear­ ning by 8­Month­Old Infants. Science 274, no. 5294 (1996): 1926–1928. [157] Terence David Sanger. Probability Density Estimation for the Interpretation of Neural Population Codes. Journal of Neurophysiology 76, no. 4 (1996): 2790–2793. [158] Yoshiyuki Sato, Taro Toyoizumi, and Kazuyuki Aihara. Bayesian Inference Explains Perception of Unity and Ventriloquism Aftereffect: Identiﬁcation of Common Sources of Audiovisual Stimuli. Neural Computation 19, no. 12 (2007): 3335–3355.\n--- Страница 451 ---\n450  Библиография [159] Gideon Schwarz. Estimating the Dimension of a Model. Annals of Statistics 6, no. 2 (1978): 461–464. [160] Robert Shapley, Michael Hawken, and Dario L. Ringach. Dynamics of Orien- tation Selectivity in the Primary Visual Cortex and the Importance of Cortical Inhibition. Neuron 38, no. 5 (2003): 689–699. [161] Shan Shen and Wei Ji Ma. A Detailed Comparison of Optimality and Simpli­ city in Perceptual Decision Making. Psychological Review 123, no. 4 (2016): 452–480. [162] Lei Shi, Thomas L. Griffiths, Naomi H. Feldman, and Adam N. Sanborn. Exemp lar Models as a Mechanism for Performing Bayesian Inference. Psy - chonomic Bulletin and Review 17, no. 4 (2010): 443–464. [163] Chris R. Sims. The Cost of Misremembering: Inferring the Loss Function in Visual Working Memory. Journal of Vision 15, no. 3 (2015). [164] Devinderjit Sivia and John Skilling. Data Analysis: A Bayesian Tutorial. Oxford: Oxford University Press, 2006. [165] Russell Smith. Milk Drinkers Turn to Powder and Other Punishing Headlines. Globe and Mail, September 23, 2009. [166] Charles Spence. Noise and Its Impact on the Perception of Food and Drink. Flavour 3, no. 1 (2014): 1–17. [167] Stephen M. Stigler. Who Discovered Bayes’ Theorem? American Statistician 37, no. 4a (1983): 290–296. [168] Alan A. Stocker and Eero P . Simoncelli. Noise Characteristics and Prior Ex - pectations in Human Visual Speed Perception. Nature Neuroscience 9, no. 4 (2006): 578–585. [169] J. V. Stone, I. S. Kerrigan, and J. Porrill. Where Is the Light? Bayesian Per- ceptual Priors for Lighting Direction. Proceedings of the Royal Society B: Biological Sciences 276, no. 1663 (2009): 1797–1804. [170] James V. Stone. Vision and Brain: How We Perceive the World. Cambridge, MA: MIT Press, 2012. [171] James V. Stone. Bayes’ Rule: A Tutorial Introduction to Bayesian Analysis. n.p.: Sebtel, 2013. [172] Lawrence D. Stone, Colleen M. Keller, Thomas M. Kratzke, and Johan P . Strumpfer. Search Analysis for the Underwater Wreckage of Air France Flight 447. 14th International Conference on Information Fusion, IEEE (2011), 1–8. [173] Leland S. Stone and Peter Thompson. Human Speed Perception Is Contrast Dependent. Vision Research 32, no. 8 (1992): 1535–1549. [174] Joshua B. Tenenbaum. Bayesian Modeling of Human Concept Learning. Ad - vances in Neural Information Processing Systems 11, edited by Michael S. Kearns, Sara A. Solla and David A. Cohn, 59–68. Cambridge, MA: MIT Press, 1998. [175] Joshua B. Tenenbaum, Charles Kemp, Thomas L. Griffiths, and Noah S. Goodman. How to Grow a Mind: Statistics, Structure, and Abstraction. Sci- ence 331, no. 6022 (2011): 1279–1285. [176] Emanuel Todorov. Optimality Principles in Sensorimotor Control. Nature Neuroscience 7, no. 9 (2004): 907–915.\n--- Страница 452 ---\nБиблиография  451 [177] Dave Tompkins. How to Wreck a Nice Beach: The Vocoder from World War II to Hip­Hop, the Machine Speaks. Melville House, 2011. [178] Jonathan Tong, Vy Ngo, and Daniel Goldreich. Tactile Length Contraction as Bayesian Inference. Journal of Neurophysiology 116, no. 2 (2016): 369–379. [179] Michel Treisman. Motion Sickness: An Evolutionary Hypothesis. Science 197, no. 4302 (1977): 493–495. [180] Julia Trommershäuser, Sergei Gepshtein, Laurence T. Maloney, Michael S. Landy, and Martin S. Banks. Optimal Compensation for Changes in Task ­ Relevant Movement Variability. Journal of Neuroscience 25, no. 31 (2005): 7169–7178. [181] Julia Trommershäuser, Konrad P . Körding, and Michael S. Landy. Sensory Cue Integration. Oxford: Oxford University Press, 2011. [182] Julia Trommershäuser, Laurence T. Maloney, and Michael S. Landy. Sta- tistical Decision Theory and the Selection of Rapid, Goal­Directed Move- ments. Journal of the Optical Society of America A 20, no. 7 (2003): 1419–1433. [183] Julia Trommershäuser, Laurence T. Maloney, and Michael S. Landy. Deci- sion Making, Movement Planning and Statistical Decision Theory. Trends in Cognitive Sciences 12, no. 8 (2008): 291–297. [184] Robert J. van Beers, Anne C. Sittig, and Jan J. van der Gon Denier. How Hu- mans Combine Simultaneous Proprioceptive and Visual Position Information. Experimental Brain Research 111, no. 2 (1996): 253–261. [185] Ruben S. van Bergen, Wei Ji Ma, Michael S. Pratte, and Janneke F. M. Jehee. Sensory Uncertainty Decoded from Visual Cortex Predicts Behavior. Nature Neuroscience 18, no. 12 (2015): 1728–1730. [186] Ronald van den Berg, Michael Vogel, Krešimir Josić, and Wei Ji Ma. Optimal Inference of Sameness. Proceedings of the National Academy of Sciences 109, no. 8 (2012): 3178–3183. [187] Bas van Opheusden, Luigi Acerbi, and Wei Ji Ma. Unbiased and Efficient Log­ Likelihood Estimation with Inverse Binomial Sampling. PLoS Computational Biology 16, no. 12 (2020): e1008483. [188] Iris Van Rooij. The Tractable Cognition Thesis. Cognitive science 32, no. 6 (2008): 939–984. [189] Meel Velliste, Sagi Perel, M. Chance Spalding, Andrew S. Whitford, and Andrew B. Schwartz. Cortical Control of a Prosthetic Arm for Self ­Feeding. Nature 453, no. 7198 (2008): 1098–1101. [190] Hermann von Helmholtz, Treatise on Physiological Optics. Vol. 3, The Per - ceptions of Vision, edited by James P . C. Southall. Rochester, NY: Optical Society of America, 1925. [191] Scott I. Vrieze. Model Selection and Psychological Theory: A Discussion of the Differences between the Akaike Information Criterion (AIC) and the Baye­ sian Information Criterion (BIC). Psychological Methods 17, no. 2 (2012): 228–243. [192] Edward Vul, Noah Goodman, Thomas L. Griffiths, and Joshua B. Tenen- baum. One and Done? Optimal Decisions from Very Few Samples. Cognitive Science 38, no. 4 (2014): 599–637.\n--- Страница 453 ---\n452  Библиография [193] Edgar Y. Walker, R. James Cotton, Wei Ji Ma, and Andreas S. Tolias. A Neural Basis of Probabilistic Computation in Visual Cortex. Nature Neuroscience 23, no. 1 (2020): 122–129. [194] Mark T. Wallace, G. E. Roberson, W. David Hairston, B. E. Stein, J. W. Vau- ghan, and J. A. Schirillo. Unifying Multisensory Signals across Time and Space. Experimental Brain Research 158, no. 2 (2004): 252–258. [195] Pascal Wallisch. Illumination Assumptions Account for Individual Differences in the Perceptual Interpretation of a Profoundly Ambiguous Stimulus in the Color Domain. ‘The Dress.’ Journal of Vision 17, no. 4 (2017): 5. [196] Paul A. Warren, Erich W. Graf, Rebecca A. Champion, and Laurence T. Maloney. Visual Extrapolation under Risk: Human Observers Estimate and Compensate for Exogenous Uncertainty. Proceedings of the Royal Society B: Biological Sciences 279, no. 1736 (2012): 2171–2179. [197] Kunlin Wei and Konrad P . Körding. Uncertainty of Feedback and State Esti- mation Determines the Speed of Motor Adaptation. Frontiers in Computa- tional Neuroscience 4 (2010): 11. [198] Xue-Xin Wei and Alan A. Stocker. A Bayesian Observer Model Constrained by Efficient Coding Can Explain ‘ Anti­Bayesian’ Percepts. Nature Neuroscience 18, no. 10 (2015): 1509–1517. [199] Yair Weiss, Eero P . Simoncelli, and Edward H. Adelson. Motion Illusions as Optimal Percepts. Nature Neuroscience 5, no. 6 (2002): 598–604. [200] Max Wertheimer, Gestalt theory. A Source Book of Gestalt Psychology, edi- ted by Willis D. Ellis, 1–11. London: Kegan Paul, Trench, Trubner, 1938. [201] Louise Whiteley and Maneesh Sahani. Implicit Knowledge of Visual Uncer- tainty Guides Decisions with Asymmetric Outcomes. Journal of Vision 8, no. 3 (2008): 1–15. [202] Frederick A. A. Kingdom and Nicolaas Prins. Psychophysics: A Practical Introduction. 2nd ed. London: Academic Press, 2016. [203] Felix A. Wichmann and N. Jeremy Hill. The Psychometric Function: I. Fit - ting, Sampling, and Goodness of Fit. Perception and Psychophysics 63, no. 8 (2001): 1293–1313. [204] Thomas D. Wickens. Elementary Signal Detection Theory. Oxford: Oxford University Press, 2001. [205] Robert C. Wilson and Anne G. E. Collins. Ten Simple Rules for the Compu- tational Modeling of Behavioral Data. Elife 8 (2019): e49547. [206] Robert C. Wilson, Matthew R. Nassar, and Joshua I. Gold. Bayesian Online Learning of the Hazard Rate in Change­Point Problems. Neural Computation 22, no. 9 (2010): 2452–2476. [207] Daniel M. Wolpert. Computational Approaches to Motor Control. Trends in Cognitive Sciences 1, no. 6 (1997): 209–216. [208] Sylvia Wright. The Death of Lady Mondegreen. Harper’s Magazine 209, no. 1254 (1954): 48–51. [209] Ting Xiang, Terry Lohrenz, and P . Read Montague. Computational Sub- strates of Norms and Their Violations during Social Exchange. Journal of Neuroscience 33, no. 3 (2013): 1099–1108. [210] Fei Xu and Joshua B. Tenenbaum. Word Learning as Bayesian Inference. Psychological Review 114, no. 2 (2007): 245–272.\n--- Страница 454 ---\nБиблиография  453 [211] Scott Cheng-Hsin Yang, Mate Lengyel, and Daniel M. Wolpert. Active Sen­ sing in the Categorization of Visual Patterns. Elife 5 (2016): e12215. [212] Aspen H. Yoo, Zuzanna Klyszejko, Clayton E. Curtis, and Wei Ji Ma. Stra- tegic Allocation of Working Memory Resource. Scientiﬁc Reports 8, no. 1 (2018): 1–8. [213] Richard S. Zemel, Peter Dayan, and Alexandre Pouget. Probabilistic In - terpretation of Population Codes. Neural Computation 10, no. 2 (1998): 403–430. [214] Yanli Zhou, Luigi Acerbi, and Wei Ji Ma. The Role of Sensory Uncertainty in Simple Contourition. PLoS Computational Biology 16, no. 11 (2020): e1006308.\n--- Страница 455 ---\nПредметный указатель Q Q-обучение, 173 А Агент, 323 Активность популяции, 354 Альтернатива, 189 Амплитудная переменная, 104 Апостериорное отношение, 191 логарифмическое, 191 Априорность, 55 Асимптотическая нормальность, 362 Б Базовая частотность. См. Встречаемость Байесовская гибкость, 375 Байесовский перенос, 374 Бета-распределение, 167 Бета-функция, 167 Бинарная классификация, 214 Бинарное решение, 188 В Вероятность апостериорная, 35, 56 априорная, 30 объективная, 384 предельная, 402 субъективная, 384 условная, 26, 54 Воксель, 364Восприятие, 25 Встречаемость, 54 Вывод байесовский, 20 вероятностный, 20 оптимальный, 21 причинный, 257 Г Гамма-распределение, 175 обратное, 177 Гамма-функция, 176 Гетероскедастичность, 103 Гипотеза, 27, 385 Голландская книга, 331 Д Декодер вектора популяции, 369 центра масс, 360 Дельта-функция, 246 Дирака, 399 Диагностический тест специфичность, 209 чувствительность, 209 Диаграмма Венна, 386 Дисконтирование источника света, 249 З Задача обнаружения, 189 различения, 189\n--- Страница 456 ---\nПредметный указатель  455 типа «да-нет», 189 Закон Вебера–Фехнера, 104 Законы гештальта, 271 И Измерение, 81 достоверность, 85 точность, 85 Иллюзия одновременного контраста, 249 Понцо, 245 Интенсивность источника света, 247 Испытание, 79 Исходные убеждения, 20 К Кривая настроечная, 347 отражения, 248 Критерий принятия решения, 192 Л Логарифмическое отношение правдоподобия, 193 М Маргинализация, 169, 214, 402 Марковское свойство, 304 Математическое ожидание, 394 условное, 116 Метод постоянных стимулов, 217 Многослойный персептрон, 182 Множественная реализуемость, 374 Модель внутренняя, 55 графовая, 53 дрейфа-диффузии, 324 марковская, 304 скрытая, 304 несоответствие, 101 нормативная, 21 обучение, 421 описательная, 21 переходов и состояний, 304 порождающая, 20, 53, 79процессная, 21 скрытая марковская, 302 Н Наблюдение, 25 Нейронная сеть однослойная, 182 Неопосредованное мышление, 24 Нижняя вариационная граница, 377 Нормативное моделирование, 131 Норма L1, 311 Нормировочная константа, 84 О Обратная температура, 334 Обучающие данные, 183 Объединение сигналов, 40 Ожидаемая полезность, 323 Окклюзия, 31 Онлайн-обнаружение, 312 Отвлекающий фактор, 281 Отклик, 116 Относительное смещение, 134 Отношение ожидаемой полезности, 325 Отражательная способность, 247 Отрицательная энтропия, 434 Оценка апостериорного среднего, 96 максимальная апостериорная, 59 максимального правдоподобия, 90, 420 смещение, 121 Ошибка предсказания вознаграждения, 173 прокурора, 60 среднеквадратичная, 121 Ощущение, 25 П Переменная мешающая, 219 номинальная, 126 ординальная, 126 решающая, 192\n--- Страница 457 ---\n456  Предметный указатель Положительная прогностическая ценность, 209 Потенциал действия, 346 Правдоподобие, 29, 55 Правило последовательности Лапласа, 169 Рескорлы–Вагнера, 172 решающее, 192 Принцип гештальта, 66 общего вида, 77 Пространство выборки, 387 событий, 385 Протопостериор, 56 Процесс Бернулли, 166 Психометрическая кривая, 215 Психофозика, 79 Р Рабочая характеристика приемника, 203 Разделение масштабов, 374 Раздражитель, 140 Различимость, 206 Распределение апостериорное, 35 прогностическое, 169 априорное, 30, 87 неудовлетворительное, 103 биномиальное, 170 Гаусса (нормальное), 83 Ирвина–Холла, 416 логнормальное, 242 нейронного шума, 348 нормальное, 392 логарифмическое, 104 отклика, 115, 420 оценки, 115 параметр масштаба, 176 формы, 176 по степенному закону, 392 Пуассона, 347, 400 равномерное, 392совместное, 401 стандартное нормальное, 84, 395 фон Мизеса, 398 экспоненциальное, 392 Распространение доверия, 377 С Свободные параметры, 421 Сенсорный вход. См. Наблюдение Сенсорный сигнал. См. Раздражитель Скорость обучения, 173 Случайная величина, 387 дискретная, 388 непрерывная, 388 Событие, 385 Состояние мира гипотетическое, 27 Т Таблица сопряженности, 402 Теория перспектив, 187 принятия решений обратная, 338 Точка субьективного равенства, 227 Точность, 93 У Уверенность, 200 Ф Фактор Фано, 347 Функция затрат, 318 полезности, 318 правдоподобия, 29 двумерная, 246 мгновенная, 167 элементарная, 146 распределения массы вероятности, 389 специальная, 399 спрямленная линейная, 182 убеждения (доверия), 97 целевая, 68\n--- Страница 458 ---\nПредметный указатель  457 Ц Центральная предельная теорема, 84, 395 Ч Численное интегрирование, 261 Ш Шумовой потолок, 434Шум принятия решений, 334 Э Эффективность, 21 Эффект уличного фонаря, 322 Я Якобиан, 104, 409\n--- Страница 459 ---\nКниги издательства «ДМК ПРЕСС » можно купить оптом и в розницу в книготорговой компании «Галактика» (представляет интересы издательств «ДМК ПРЕСС », «СОЛОН ПРЕСС», «КТК Галактика»). Адрес: г. Москва, пр. Андропова, 38, оф. 10; тел.: (499) 782-38-89, электронная почта: books@alians-kniga.ru. При оформлении заказа следует указать адрес (полностью), по которому должны быть высланы книги; фамилию, имя и отчество получателя. Желательно также указать свой телефон и электронный адрес. Эти книги вы можете заказать и в интернет-магазине: http://www.galaktika-dmk.com/. Вэй Цзи Ма, Конрад Кёрдинг, Дэниел Голдрайх Байесовские модели восприятия и действия Главный редактор Мовчан Д. А. dmkpress@gmail.com Зам. главного редактора Сенченкова Е. А. Перевод Яценков В. С. Корректор Синяева Г. И. Верстка Чаннова А. А. Дизайн обложки Мовчан А. Г. Гарнитура PT Serif. Печать цифровая. Усл. печ. л. 37,21. Тираж 100 экз. Веб-сайт издательства: www.dmkpress.com",
      "debug": {
        "start_page": 371,
        "end_page": 460
      }
    }
  ]
}
{
  "title": "Математика в машинном обучении [2024] Дайзенрот, Фейзал, Он",
  "chapters": [
    {
      "name": "Глава 1. Введение и мотивация 28",
      "content": "--- Страница 30 --- (продолжение)\nГлава 1. Введение и мотивация Модель, как правило, используется для описания процесса генерации данных, подобных тем, что содержатся в имеющемся датасете. Следовательно, хорошие модели также можно трактовать как упрощенные версии реального (неизвест - ного) процесса генерации данных, схватывающие особенности, которые важны для моделирования данных и извлечения из них скрытых закономерностей. В дальнейшем хорошая модель может применяться для прогнозирования того, что произойдет в реальных условиях, без постановки соответствующих реальных экспериментов. Теперь мы подходим к апогею нашей темы: собственно обучению в рамках МО. Допустим, у нас есть датасет и подходящая модель. Обучение модели означает использование доступных данных для оптимизации некоторых параметров модели с учетом функции полезности, оценивающей, насколько хорошо модель прогнозирует обучающие данные. Большинство методов обучения можно по - нимать как попытки взобраться на вершину холма. В данной аналогии вершина холма соответствует максимуму некоторого показателя желаемой производи - тельности. Но на практике мы заинтересованы, чтобы модель хорошо работала на тех данных, которых ей еще не показывали. Хорошая производительность модели на уже показанных ей (обучающих) данных может означать лишь то, что мы нашли хороший способ запоминания этих данных. Однако может ока - заться, что модель плохо обобщает заранее не известные данные, а на практике зачастую приходится задействовать нашу модель МО в таких ситуациях, с ко- торыми она ранее не сталкивалась. Итак, обобщим основные концепции машинного обучения, о которых пойдет речь в этой книге: zМы представляем данные в виде векторов. zМы выбираем подходящую модель, исходя из вероятностной или оптими - зационной точки зрения. zМы учимся на доступных данных, используя методы численной оптимизации, и стремимся добиться того, чтобы модель хорошо работала на данных, не применявшихся для ее обучения. 1.2. ДВА СПОСОБА ЧИТАТЬ ЭТУ КНИГУ Можно рассмотреть две стратегии, помогающие понять математику в рамках машинного обучения: zВосходящая : опираясь на основополагающие концепции, изучаем все более продвинутые. Такой подход зачастую предпочтителен в точных науках, на - пример в математике. Преимущество такой стратегии в том, что читатель в любой момент может обратиться к концепциям, изученным ранее. К со-\nГлава 1. Введение и мотивация Модель, как правило, используется для описания процесса генерации данных, подобных тем, что содержатся в имеющемся датасете. Следовательно, хорошие модели также можно трактовать как упрощенные версии реального (неизвест - ного) процесса генерации данных, схватывающие особенности, которые важны для моделирования данных и извлечения из них скрытых закономерностей. В дальнейшем хорошая модель может применяться для прогнозирования того, что произойдет в реальных условиях, без постановки соответствующих реальных экспериментов. Теперь мы подходим к апогею нашей темы: собственно обучению в рамках МО. Допустим, у нас есть датасет и подходящая модель. Обучение модели означает использование доступных данных для оптимизации некоторых параметров модели с учетом функции полезности, оценивающей, насколько хорошо модель прогнозирует обучающие данные. Большинство методов обучения можно по - нимать как попытки взобраться на вершину холма. В данной аналогии вершина холма соответствует максимуму некоторого показателя желаемой производи - тельности. Но на практике мы заинтересованы, чтобы модель хорошо работала на тех данных, которых ей еще не показывали. Хорошая производительность модели на уже показанных ей (обучающих) данных может означать лишь то, что мы нашли хороший способ запоминания этих данных. Однако может ока - заться, что модель плохо обобщает заранее не известные данные, а на практике зачастую приходится задействовать нашу модель МО в таких ситуациях, с ко- торыми она ранее не сталкивалась. Итак, обобщим основные концепции машинного обучения, о которых пойдет речь в этой книге: zМы представляем данные в виде векторов. zМы выбираем подходящую модель, исходя из вероятностной или оптими - зационной точки зрения. zМы учимся на доступных данных, используя методы численной оптимизации, и стремимся добиться того, чтобы модель хорошо работала на данных, не применявшихся для ее обучения. 1.2. ДВА СПОСОБА ЧИТАТЬ ЭТУ КНИГУ Можно рассмотреть две стратегии, помогающие понять математику в рамках машинного обучения: zВосходящая : опираясь на основополагающие концепции, изучаем все более продвинутые. Такой подход зачастую предпочтителен в точных науках, на - пример в математике. Преимущество такой стратегии в том, что читатель в любой момент может обратиться к концепциям, изученным ранее. К со-\n--- Страница 31 ---\n1.2. Два способа читать эту книгу 31 жалению, для практика основополагающие концепции не столь интересны сами по себе, и из-за отсутствия мотивации их изучать большинство опре - делений таких базовых концепций быстро забываются. zzНисходящая : конкретизация, сведение практических потребностей к более базовым требованиям. Такой целеориентированный подход хорош тем, что читателю в любой момент ясно, зачем нужно прорабатывать конкретную концепцию, и к нужным знаниям лежит ясный путь. Недостаток такой стра - тегии заключается в том, что основы таких знаний могут получиться шатки - ми, и читателю приходится запоминать набор слов, понять которые у него нет никакой возможности. Мы решили написать эту книгу в виде системы модулей, чтобы отделить базо - вые (математические) концепции от прикладных, и текст можно было читать обоими вышеупомянутыми способами. Книга разделена на две части. В части I излагаются математические основы, а в части II концепции из части I применя - ются для решения набора фундаментальных задач машинного обучения, по - казанных на рис. 1.1: это регрессия, снижение размерности, оценка плотности и классификация. Последующие главы в части I в основном базируются на предыдущих, но при необходимости можно пропустить главу, прочитать следу - ющую, а затем вернуться к пропущенной, если потребуется. Главы в части II связаны очень слабо, и их можно читать в любом порядке. В обоих частях кни - ги расставлено множество отсылок на предыдущие и последующие главы, они связывают математические концепции с алгоритмами машинного обучения. Разумеется, найдутся и другие способы чтения этой книги, кроме двух выше­ упомянутых . Большинству читателей подойдет комбинация восходящего и нисходящего подхода. В некоторых случаях потребуется наработать базовые математические навыки, прежде чем подступаться к сложным концепциям, но также можно выбирать темы, отталкиваясь от возможностей применения ма - шинного обучения. Часть I — о математике Четыре столпа машинного обучения, рассматриваемые в этой книге (рис 1.1), требуют основательного математического базиса, и этот базис изложен в части I. Мы представляем числовые данные в векторном виде, а таблицу таких данных — как матрицу. Изучение векторов и матриц называется линейной алгеброй, с ней мы познакомимся в главе 2. Там же матрица описывается как совокупность векторов. Имея два вектора, представляющих два реальных объекта, мы собираемся делать утверждения об их сходстве. Идея в том, что два схожих вектора должны давать\n--- Страница 32 ---\n32 Глава 1. Введение и мотивация схожие выводы после обработки нашим алгоритмом МО (предиктором). Чтобы формализовать идею сходства векторов, необходимо ввести операции, прини - мающие два вектора в качестве ввода и возвращающие числовое значение, от - ражающее их сходство. Создание сходства и расстояний играет центральную роль в аналитической геометрии , рассматриваемой в главе 3. /uni041A/uni043B/uni0430/uni0441/uni0441/uni0438/uni0444/uni0438/uni043A/uni0430/uni0446/uni0438/uni044F/uni041E/uni0446/uni0435/uni043D/uni043A /uni0430 /uni043F/uni043B/uni043E/uni0442/uni043D/uni043E/uni0441/uni0442/uni0438/uni0420/uni0435/uni0433/uni0440/uni0435/uni0441/uni0441/uni0438/uni044F /uni0423/uni043C/uni0435/uni043D/uni044C/uni0448/uni0435/uni043D/uni0438/uni0435 /uni0440/uni0430/uni0437/uni043C/uni0435/uni0440/uni043D/uni043E/uni0441 /uni0442/uni0438/uni041C/uni0430/uni0448/uni0438/uni043D/uni043D/uni043E/uni0435 /uni043E/uni0431/uni0443/uni0447/uni0435/uni043D/uni0438/uni0435 /uni0412/uni0435/uni043A/uni0442/uni043E/uni0440/uni043D/uni044B/uni0439 /uni0430/uni043D/uni0430/uni043B/uni0438 /uni0437 /uni041E/uni043F/uni0442/uni0438/uni043C/uni0438/uni0437/uni0430/uni0446/uni0438/uni044F /uni0410/uni043D/uni0430/uni043B/uni0438/uni0442/uni0438/uni0447/uni0435/uni0441/uni043A/uni0430/uni044F /uni0433 /uni0435/uni043E/uni043C/uni0435/uni0442/uni0440/uni0438/uni044F /uni0420/uni0430/uni0437/uni043B/uni043E/uni0436/uni0435/uni043D/uni0438/uni0435 /uni043C/uni0430/uni0442/uni0440/uni0438/uni0446 /uni041B/uni0438/uni043D/uni0435/uni0439/uni043D/uni0430/uni044F /uni0430/uni043B/uni0433 /uni0435/uni0431/uni0440/uni0430 /uni0412/uni0435/uni0440/uni043E/uni044F/uni0442 /uni043D/uni043E/uni0441/uni0442/uni044C /uni0438 /uni0440/uni0430/uni0441/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni0438/uni044F Рис. 1.1. Основания и четыре столпа машинного обучения В главе 4 вводятся некоторые фундаментальные концепции, касающиеся матриц и их разложения . Некоторые операции над матрицами исключительно полезны в МО и обеспечивают интуитивно понятную интерпретацию данных, а также более эффективное обучение. Часто данные трактуются как зашумленные наблюдения некого истинного базового сигнала. Мы надеемся, что, применив МО, сможем вычленить сигнал из шума. Для этого нам нужен язык, на котором можно было бы количественно выразить, что такое «шум». Часто нам также хотелось бы иметь предикторы, которые позволили бы выразить некоторую неопределенность, например чтобы количественно охарактеризовать степень нашей уверенности в спрогнозиро - ванном значении в конкретной точке тестовых данных. Квантификацией (ко- личественной оценкой) неопределенности занимается теория вероятности, ей посвящена глава 6. Для обучения моделей обычно подыскиваются параметры, доводящие до мак - симума некоторую меру производительности. Многие оптимизационные при -\n--- Страница 33 ---\n1.2. Два способа читать эту книгу 33 емы т ребуют понимания концепции градиента, который указывает, в каком направлении искать решение. Глава 5 посвящена векторному анализу и по дробно описывает концепцию градиентов, которыми мы затем воспользуемся в главе 7; в ней же мы поговорим об оптимизации для поиска максимумов и минимумов функций. Часть II — о машинном обучении Во второй части книги вы познакомитесь с четырьмя столпами машинного обу­ чения , показанными на рис. 1.1. Мы проиллюстрируем, как математические концепции, объясненные в первой части книги, служат основаниями каждого из столпов. В широком смысле, главы упорядочены от простого к сложному. В главе 8 мы освежим в памяти три компонента МО (данные, модели, оценка параметров), рассмотрев их с математической точки зрения. Кроме того, мы дадим некоторые рекомендации о том, как подбираются экспериментальные установки, помогающие перестраховаться от чрезмерно оптимистичной оценки систем МО. Как вы помните, наша цель — построить предиктор, хорошо рабо - тающий на ранее не известных данных. В главе 9 мы подробно рассмотрим линейную р егрессию и зададимся целью найти такие функции, которые сопоставляют входные данных x ∈ D с соот- ветствующими наблюдаемыми значениями функции y ∈ , которые мы сможем интерпретировать как метки соответствующих входных данных. Мы обсудим классическую подгонку модели (оценку параметров) с применением методов максимального правдоподобия и максимальной апостериорной оценки, а также байесовскую линейную регрессию, где будем исключать параметры путем ин - тегрирования, а не оптимизировать их. В главе 10 основное внимание уделяется снижению размерности , второму стол - пу с рис. 1.1; для этого воспользуемся анализом главных компонент. Ключевая цель снижения размерности — найти компактное представление (с малым ко - личеством измерений) для данных, описываемых большим количеством из - мерений x ∈ D; такое компактное представление зачастую легче анализировать, нежели исходные данные. В отличие от регрессии, снижение размерности за - висит только от моделирования данных — нет никаких меток, которые были бы связаны с точкой данных x. В главе 11 мы перейдем к нашему третьему столпу: оценке плотности . Назна - чение оценки плотности — найти вероятностное распределение, описывающее заданный датасет. Изучая эту тему, мы сосредоточимся на моделях гауссовых смесей и обсудим итеративную схему нахождения параметров такой модели. Как и в случае со снижением размерности, здесь нет никаких меток для точек данных x ∈ D. Однако мы не ищем такое представление данных, которое об -\n--- Страница 34 ---\n34 Глава 1. Введение и мотивация ладало бы низкой размерностью. Нас скорее интересует плотностная модель, которая описывает эти данные. Глава 12, завершающая книгу, содержит углубленное обсуждение четвертого столпа: классификации . Примерно как и при работе с регрессией (глава 9), у нас есть входные значения x и соответствующие им метки y. Однако, в отличие от регрессии, при которой метки имеют вещественные значения, метки при клас - сификации являются целочисленными, поэтому обращаться с ними нужно особенно осторожно. 1.3. УПРАЖНЕНИЯ И ОБРАТНАЯ СВЯЗЬ Мы приводим некоторые упражнения в части I, и для выполнения большинства из них достаточно ручки и бумаги. К главе II мы подготовили руководства по программированию (это блокноты Jupyter ), которые помогут вам исследовать некоторые свойства алгоритмов машинного обучения, рассматриваемых в этой книге. Мы высоко ценим участие издательства Cambridge University Press , активно поддерживающего нас в нашем стремлении к демократизации образования. Издательство выложило эту книгу для свободного скачивания по адресу https://mml-book.com , где также находятся решения упражнений, списки найденных ошибок и допол - нительные материалы. Сообщать об ошибках и оставлять отзывы можно по вышеприведенной ссылке.\n--- Страница 35 ---\n2 Линейная алгебра При формализации интуитивно понятных концепций принято подбирать набор объектов (символов) и давать набор правил по обращению с этими объектами. Такая наука называется алгеброй . Линейная алгебра — это наука о векторах и определенных правилах операций над ними. Векторы, которые многие помнят из школьного курса, называются «геометрическими» и обычно обозначаются стрелочкой над буквой, например, и . В этой книге речь пойдет о более обоб - щенном представлении векторов, и вектор будет обозначаться жирной латинской буквой, например x и y. В принципе, векторы — это объекты особого рода, которые можно складывать друг с другом и умножать на скаляры, чтобы получить новый объект того же рода. С точки зрения абстрактной математики, любой объект, обладающий двумя этими свойствами, может считаться вектором. Вот несколько примеров таких векторных объектов: 1. Геометрические векторы. Векторы такого рода изучаются в старших классах, в курсах математики и физики. Геометрические векторы — см. рис. 2.1( а) — это направленные отрезки, которые можно чертить (как минимум в двух измерениях). Два геометрических вектора , можно сложить, так что их суммой будет третий геометрический вектор. Более того, умножение на скаляр, , λ ∈ , также даст геометрический вектор. Фактически это исходный вектор, умноженный на λ. Следовательно, геометрические векто - ры — это примеры воплощения концепции вектора, с которой мы познако - мились выше. Интерпретируя векторы как геометрические сущности, мы можем интуитивно судить об их направлении и величине, а также рассуждать о математических операциях над ними. 2. Многочлены — это тоже векторы; см. рис. 2.1( b): два многочлена можно сложить друг с другом, получив в результате третий многочлен; также их можно умножать на скаляр λ ∈ , и в результате тоже получится многочлен.\n--- Страница 36 ---\n36",
      "debug": {
        "start_page": 30,
        "end_page": 36
      }
    },
    {
      "name": "Глава 2. Линейная алгебра 35",
      "content": "--- Страница 36 --- (продолжение)\nГлава 2. Линейная алгебра Следовательно, многочлены — это образцы векторов (пусть и довольно не - обычные). Обратите внимание на то, что многочлены очень отличаются от геометрических векторов. Тогда как геометрический вектор — это конкретный «рисунок», многочлен — это абстрактная концепция. Однако и те, и другие являются векторами в вышеизложенном смысле. 3. Аудиосигналы — это векторы. Аудиосигнал можно представить как после - довательность чисел. Также можно складывать аудиосигналы друг с другом, и их суммой будет новый аудиосигнал. Если умножить аудиосигнал, также получится аудиосигнал. Следовательно, аудиосигналы также являются свое- образными векторами. 4. Элементы n (кортежи из n вещественных чисел) — это векторы. n более абстрактны, чем многочлены, и именно этой концепции уделяется особое внимание в данной книге. Так, (2.1) — это пример тройки чисел. Покомпонентное сложение двух векторов a, b ∈ n дает еще один вектор: a + b = с ∈ n. Более того, при умножении a ∈ n на λ ∈  получается умноженный вектор λa ∈ n. Рассматривать векторы как элементы n удобно еще и потому, что в таком представлении они услов - но соответствуют массивам вещественных чисел с точки зрения компьютера1. Во многих языках программирования поддерживаются операции над мас - сивами, благодаря чему удобно реализовывать алгоритмы, связанные с вы- полнением операций над векторами. →x→y→x + y→ –2 0 2 x–6–4–2024y Рис. 2.1. Векторы разных типов. Векторы порой удивительны, и к ним относятся как (a) геометрические векторы, так и ( b) многочлены 1 Тщательно проверьте, на самом ли деле операции над массивами тождественны опе - рациям над векторами, если реализовать их на компьютере.\nГлава 2. Линейная алгебра Следовательно, многочлены — это образцы векторов (пусть и довольно не - обычные). Обратите внимание на то, что многочлены очень отличаются от геометрических векторов. Тогда как геометрический вектор — это конкретный «рисунок», многочлен — это абстрактная концепция. Однако и те, и другие являются векторами в вышеизложенном смысле. 3. Аудиосигналы — это векторы. Аудиосигнал можно представить как после - довательность чисел. Также можно складывать аудиосигналы друг с другом, и их суммой будет новый аудиосигнал. Если умножить аудиосигнал, также получится аудиосигнал. Следовательно, аудиосигналы также являются свое- образными векторами. 4. Элементы n (кортежи из n вещественных чисел) — это векторы. n более абстрактны, чем многочлены, и именно этой концепции уделяется особое внимание в данной книге. Так, (2.1) — это пример тройки чисел. Покомпонентное сложение двух векторов a, b ∈ n дает еще один вектор: a + b = с ∈ n. Более того, при умножении a ∈ n на λ ∈  получается умноженный вектор λa ∈ n. Рассматривать векторы как элементы n удобно еще и потому, что в таком представлении они услов - но соответствуют массивам вещественных чисел с точки зрения компьютера1. Во многих языках программирования поддерживаются операции над мас - сивами, благодаря чему удобно реализовывать алгоритмы, связанные с вы- полнением операций над векторами. →x→y→x + y→ –2 0 2 x–6–4–2024y Рис. 2.1. Векторы разных типов. Векторы порой удивительны, и к ним относятся как (a) геометрические векторы, так и ( b) многочлены 1 Тщательно проверьте, на самом ли деле операции над массивами тождественны опе - рациям над векторами, если реализовать их на компьютере.\n--- Страница 37 ---\n37 Линейная алгебра В линейной алгебре особое внимание уделяется сходству между двумя этими векторными концепциями. Такие векторы можно складывать друг с другом и умножать на скаляры. Мы сосредоточимся преимущественно на векторах из n, так как большинство алгоритмов линейной алгебры формулируются в n. В главе 8 будет показано, что данные часто трактуются как векторы в n. В этой книге мы сосредоточимся на изучении конечномерных векторных пространств, где существует однозначное соответствие между вектором любого рода и n. Когда это будет удобно, мы будем использовать аналогии из области геометри - ческих векторов, рассуждая об алгоритмах, основанных на массивах. Одной из важнейших идей в математике является замыкание. Вот вопрос: ка - ково множество всех результатов, которые могут быть получены при выполне - нии предлагаемых мной операций? В случае векторов формулировка такова: какое множество векторов можно получить, взяв за основу небольшой набор векторов, а затем складывая и нормируя их? В результате получится векторное пространство (раздел 2.4). На концепции векторного пространства и его свой - ствах во многом базируется машинное обучение. Концепции, введенные в этой главе, обобщены на рис. 2.2. /uni0412/uni0435/uni043A/uni0442/uni043E/uni0440 /uni0413/uni043B/uni0430/uni0432/uni0430 5. /uni0412/uni0435/uni043A/uni0442/uni043E/uni0440/uni043D/uni044B/uni0439 /uni0430/uni043D/uni0430/uni043B/uni0438 /uni0437 /uni0413/uni0440/uni0443/uni043F/uni043F/uni0430 /uni041E/uni0431/uni0440/uni0430/uni0442/uni043D/uni044B/uni0435 /uni043C/uni0430/uni0442/uni0440/uni0438/uni0446/uni044B/uni0413/uni0430/uni0443/uni0441/uni0441/uni043E/uni0432/uni043E /uni0438/uni0441/uni043A/uni043B/uni044E/uni0447/uni0435/uni043D/uni0438 /uni0435 /uni0413/uni043B/uni0430/uni0432/uni0430 10. /uni0421/uni043D/uni0438/uni0436/uni0435/uni043D/uni0438/uni0435 /uni0440/uni0430/uni0437/uni043C/uni0435/uni0440/uni043D/uni043E /uni0441/uni0442/uni0438/uni0413/uni043B/uni0430/uni0432/uni0430 12. /uni041A/uni043B/uni0430/uni0441/uni0441/uni0438/uni0444/uni0438/uni043A/uni0430/uni0446/uni0438/uni044F/uni0413/uni043B/uni0430/uni0432/uni0430 3. /uni0410/uni043D/uni0430/uni043B/uni0438/uni0442/uni0438/uni0447/uni0435/uni0441/uni043A/uni0430/uni044F /uni0433 /uni0435/uni043E/uni043C/uni0435/uni0442/uni0440/uni0438/uni044F/uni0441/uni043B/uni0430/uni0433/uni0430/uni0435/uni0442/uni0437/uni0430/uni043C/uni044B/uni043A/uni0430/uni043D/uni0438/uni0435 /uni0410/uni0431/uni0435/uni043B/uni0435/uni0432 /uni0430 /uni0441 +/uni043F/uni0440/uni0435/uni0434 /uni0441/uni0442/uni0430/uni0432/uni043B/uni044F/uni0435/uni0442 /uni043F/uni0440/uni0435/uni0434/uni0441/uni0442/uni0430/uni0432/uni043B/uni044F/uni0435/uni0442 /uni0440/uni0435/uni0448/uni0430/uni0435/uni043C /uni043E/uni0440/uni0435/uni0448/uni0430/uni0435 /uni0442/uni0441/uni0432/uni043E/uni0439/uni0441/uni0442/uni0432/uni043E/uni043C/uni0430/uni043A/uni0441/uni0438/uni043C/uni0430/uni043B/uni044C/uni043D/uni043E/uni0435 /uni043C/uni043D/uni043E/uni0436/uni0435 /uni0441/uni0442/uni0432/uni043E/uni041C/uni0430/uni0442/uni0440/uni0438/uni0446/uni0430 /uni0412/uni0435/uni043A/uni0442/uni043E/uni0440/uni043D/uni043E/uni0435 /uni043F/uni0440/uni043E/uni0441 /uni0442/uni0440/uni0430/uni043D/uni0441 /uni0442/uni0432/uni043E/uni041B/uni0438/uni043D/uni0435/uni0439/uni043D/uni0430/uni044F /uni043D/uni0435/uni0437/uni0430/uni0432/uni0438/uni0441/uni0438/uni043C/uni043E/uni0441 /uni0442/uni044C /uni041E/uni0441/uni043D/uni043E/uni0432/uni0430/uni043D/uni0438/uni0435/uni0421/uni0438/uni0441/uni0442/uni0435/uni043C/uni0430 /uni043B/uni0438/uni043D/uni0435/uni0439/uni043D/uni044B/uni0445 /uni0443/uni0440/uni0430/uni0432/uni043D/uni0435/uni043D/uni0438/uni0439/uni041B/uni0438/uni043D/uni0435/uni0439/uni043D/uni043E/uni0435//uni0430/uni0444/uni0444/uni0438/uni043D/uni043D/uni043E/uni0435 /uni043E/uni0442/uni043E/uni0431/uni0440/uni0430/uni0436/uni0435/uni043D/uni0438 /uni0435 Рис. 2.2. Ассоциативная карта концепций, вводимых в этой главе, с указанием, в каких еще частях книги они фигурируют Эта глава большей частью основана на конспектах и книгах Drumm and Weil (2001), Gilbert Strang (2003), Hogben (2013), Liesen and Mehrmann (2015), а так-\n--- Страница 38 ---\n38 Глава 2. Линейная алгебра же на серии Павла Гринфельда (Pavel Grinfeld) «Линейная алгебра» ( http://tinyurl. com/nahclwm ). Другие отличные ресурсы — это курс Гилберта Стрэнга по линей - ной алгебре (MIT) ( http://tinyurl.com/29p5q8j ) и серия «Линейная алгебра» с 3Blue1Brown ( https://tinyurl.com/h5g4kps ). Линейная алгебра играет важную роль в машинном обучении и в математике в целом. Концепции, включенные в эту главу, далее раскрываются в главе 3 с захватом геометрии. В главе 5 мы поговорим о векторном анализе, где важны хорошо усвоенные знания об операциях над матрицами. В главе 10 мы будем пользоваться проекциями (с ними вы познакомитесь в разделе 3.8) для сниже - ния размерности с применением анализа главных компонент. В главе 9 мы по - говорим о линейной регрессии, где линейная алгебра играет центральную роль в решении задач наименьших квадратов. 2.1. СИСТЕМЫ ЛИНЕЙНЫХ УРАВНЕНИЙ Системы линейных уравнений играют центральную роль в линейной алгебре. Многие задачи можно сформулировать в виде систем линейных уравнений, а линейная алгебра предоставляет инструментарий для их решения. Пример 2.1 Компания производит линейку продуктов N1, , Nn, для которых требу - ются ресурсы R1, , Rm. На производство единицы продукта Nj требуется aij единиц ресурса Ri, где i = 1, , m, а j = 1, , n. Цель — подобрать оптимальный производственный план, то есть сплани - ровать, сколько единиц xj продукта Nj должно быть произведено, если доступно всего bi единиц ресурса Ri и, в идеале, неизрасходованных ре - сурсов не остается. Если мы произведем x1, , xn единиц соответствующего продукта, то нам понадобится всего (2.2) единиц ресурса Ri. Следовательно, оптимальный производственный план (x1, , xn) ∈ n должен удовлетворять следующей системе уравнений: , (2.3) где aij ∈  и bi ∈ n.\n--- Страница 39 ---\n2.1. Системы линейных уравнений 39 Уравнение (2.3) — это обобщенная форма системы линейных уравнений , а x1, , xn — это неизвестные данной системы. Каждый n-кортеж ( x1, , xn) ∈ n, удов - летворяющий (2.3), является решением системы линейных уравнений. Пример 2.2 Система линейных уравнений (2.4) не имеет решения . Сложив два первых уравнения, имеем 2 x1 + 3x3 = 5, что противоречит третьему уравнению (3). Рассмотрим систему линейных уравнений . (2.5) Из первого и третьего уравнения следует, что x1 = 1. Из (1) + (2) имеем, что 2 x1 + 3x3 = 5, то есть x3 = 1. Затем из (3) получаем, что x2 = 1. Следова - тельно, (1, 1, 1) — это единственно возможное, единственное решение (чтобы убедиться, что (1, 1, 1) является решением, подставьте его в урав- нение). В качестве третьего примера рассмотрим: . (2.6) Поскольку (1) + (2) = (3), третье уравнение можно опустить (оно избы - точно). Из (1) и (2) следует, что 2 x1 = 5 – 3 x3 и 2x2 = 1 + x3. Мы определя - ем x3 = a ∈ n как свободную переменную, такую что любая тройка , a ∈  (2.7) является решением системы линейных уравнений, то есть мы получаем множество, в котором содержится бесконечное количество решений.\n--- Страница 40 ---\n40 Глава 2. Линейная алгебра В целом, в системах линейных уравнений, значения которых являются веще - ственными числами, имеется либо ноль, либо строго одно, либо бесконечно много решений. При помощи линейной регрессии (глава 9) решается такая версия примера 2.1, которую невозможно решить при помощи системы линей - ных уравнений. ПРИМЕЧАНИЕ В системе линейных уравнений с двумя переменными x1, x2 каждое линейное уравнение определяет прямую линию на плоскости x1x2. Поскольку решение системы линейных уравнений должно удовлетворять всем решениям этой системы одновременно, данное решение является пере - сечением этих линий. Данное пересечение может представлять собой линию (если все линейные уравнения описывают одну и ту же прямую), или точку, или быть пустым (когда линии параллельны). На рис. 2.3 проиллюстрирова - на система уравнений , (2.8) где пространство решений — это точка ( x1, x2) = (1, ). Аналогично, при трех переменных каждое линейное уравнение определяет плоскость в трехмерном пространстве. При пересечении этих плоскостей, то есть в ситуации, удовлет - воряющей одновременно всем линейным уравнениям, можно получить решение, которое будет представлять собой плоскость, или прямую, или точку, или будет пустым (когда плоскости не пересекаются).  x1x2 4x1 + 4x2 = 5 2x1 /dollar.g00DC 4x2 = 1 Рис. 2.3. Пространство решений двух линейных уравнений с двумя переменными поддается геометрической интерпретации как пересечение двух прямых. Каждому линейному уравнению соответствует прямая Для систематического подхода к решению систем линейных уравнений введем удобную компактную нотацию. Соберем коэффициенты aij в векторы, а век-\n--- Страница 41 ---\n2.2. Матрицы 41 торы — в матрицы. Иными словами, запишем систему из (2.3) в следующей форме: (2.9) . (2.10) Далее подробнее рассмотрим эти матрицы и определим правила их вычисления. Вернемся к решению линейных уравнений в разделе 2.3. 2.2. МАТРИЦЫ Матрицам отводится центральная роль в линейной алгебре. Их можно исполь - зовать для компактного представления систем линейных уравнений, но также они представляют линейные функции (линейные отображения), как будет по - казано ниже, в разделе 2.7. Прежде чем перейти к обсуждению некоторых из этих интересных тем, давайте определим, что такое матрица и какие виды опе - раций можно производить над матрицами. Подробнее о свойствах матриц мы поговорим в главе 4. Определение 2.1 (матрица). При m, n ∈  матрица A (m, n), значениями которой являются вещественные числа, представляет собой m · n-кортеж элементов aij, i = 1, , m, j = 1, , n, упорядоченный в соответствии с прямоугольной структурой, в которой m строк и n столбцов: , aij ∈  (2.11) Принято называть (1, n)-матрицы строками , а (m, 1)-матрицы столбцами . Такие особые матрицы также называются векторами строк/столбцов . m × n — это множество всех ( m, n)-матриц, чьи значения являются вещественны - ми числами. A ∈ m × n можно эквивалентно представить как a ∈ mn, уложив все n столбцов матрицы в длинный вектор (рис. 2.4).\n--- Страница 42 ---\n42 Глава 2. Линейная алгебра A ∈ R 4×2a ∈ R 8 изменение формы матрицы Рис. 2.4. Объединив столбцы матрицы A, можно представить ее как длинный вектор a 2.2.1. Сложение и перемножение матриц Сумма двух матриц1 A ∈ m × n, B ∈ m × n определяется как их покомпонентная сумма (2.12) Для матриц A ∈ m×n, B ∈ n×k элементы cij произведения С = AB ∈ m × k вычис - ляются2 как i = 1, …, m, j = 1, …, k. (2.13) Это означает, что для вычисления элемента cij мы умножаем элементы i-й стро - ки A на j-й столбец B и суммируем их. Ниже, в разделе 3.2, мы назовем такой результат скалярным произведением соответствующих строки и столбца3. В слу- чаях, когда нам потребуется явно указать, что мы выполняем умножение, мы обозначим его при помощи нотации A · B (то есть покажем « ·»). ПРИМЕЧАНИЕ Матрицы поддаются перемножению, лишь если совпадают их «соседние» размерности. Например, матрицу A вида n × k можно умножить на матрицу B вида k × m, но только слева: . (2.14) Произведение BA не определено, если m ≠ n, так как соседние размерности не совпадают.  1 Обратите внимание на размер матриц. 2 C = np.einsum('il, lj', A, B) . 3 В A n столбцов, а в B n строк, так что можно вычислить ailblj для l = 1, , n. Обычно скалярное произведение двух векторов a и b обозначается как или < a, b>.\n--- Страница 43 ---\n2.2. Матрицы 43 ПРИМЕЧАНИЕ Умножение матриц не определяется как покомпонентная операция над элементами матрицы, то есть cij ≠ aijbij (даже если размер AB был подобран правильно). Подобное покомпонентное перемножение часто встреча - ется в языках программирования, когда мы перемножаем друг с другом (много - мерные) массивы, и называется произведением Адамара .  Пример 2.3 Для мы получаем (2.15) (2.16) На этом примере уже можем убедиться, что перемножение матриц не - коммутативно, то есть AB ≠ BA; см. также рис. 2.5 в качестве иллюстрации. Рис. 2.5. Даже если оба варианта умножения матриц AB и BA определены, размерности результатов могут отличаться Определение 2.2 (единичная матрица). В n × n определяем единичную матрицу (2.17) как матрицу n × n, содержащую 1 по диагонали и 0 во всех других направлениях.\n--- Страница 44 ---\n44 Глава 2. Линейная алгебра Теперь, когда мы определили перемножение матриц, сложение матриц и разо- брались, что такое единичная матрица, рассмотрим некоторые свойства матриц: zАссоциативность : (2.18) zДистрибутивность : (2.19 a) (2.19 b) zУмножение на единичную матрицу: (2.20) Обратите внимание: Im ≠ In для m ≠ n. 2.2.2. Обращение и транспонирование Определение 2.3 (обращение). Рассмотрим квадратную матрицу1 A ∈ n × n. Пусть матрица B ∈ n × n такова, что AB = In = BA. B называется обратной A и обо- значается A–1. К сожалению, не у каждой матрицы A есть обратная A–1. Если такая обратная матрица существует, то A называется регулярной/обратимой/невырожденной матрицей, а в противном случае — вырожденной/необратимой . Если обратная матрица существует, то она единственна. В разделе 2.3 мы обсудим общий спо - соб расчета обратной матрицы путем решения системы линейных уравнений. ПРИМЕЧАНИЕ Рассмотрим матрицу (2.21) Если умножить A на , (2.22) получим (2.23) 1 В квадратной матрице одинаковое количество строк и столбцов.\n--- Страница 45 ---\n2.2. Матрицы 45 Следовательно, , (2.24) тогда и только тогда, когда a11a22 − a12a21 ≠ 0. В разделе 4.1 мы увидим, что a11a22 – – a12a21 является определителем (детерминантом) матрицы 2 × 2. В дальнейшем можно использовать детерминант матрицы, чтобы проверить, является ли она обратимой.  Пример 2.4 (обратная матрица) Матрицы (2.25) являются обратными друг другу, поскольку AB = I = BA. Определение 2.4 (транспонирование). Для A ∈ m × n матрица B ∈ n × m с bij = aij называется транспозицией A. Это записывается как B = AТ. В общем случае AТ можно получить, записав столбцы A как строки AТ. Ниже показаны некоторые важные свойства обращения и транспозиции1: (2.26) (2.27) (2.28) (2.29) (2.30) (2.31) 1 Главная диагональ (иногда именуемая «основной диагональю») матрицы A — это со - вокупность элементов Aij, где i = j. Скалярное представление (2.28) записывается как .\n--- Страница 46 ---\n46 Глава 2. Линейная алгебра Определение 2.5 (симметричная матрица). Матрица A ∈ n × n является симме ­ тричной , если A = AТ. Обратите внимание: лишь ( n, n)-матрицы могут быть симметричны. Обычно (n, n)-матрицы также именуются квадратными , поскольку число строк и столб - цов в них одинаково. Более того, если A обратима, то и AТ обратима, а также (A–1)Т = (AТ)–1 =: A–Т. ПРИМЕЧАНИЕ Сумма симметричных матриц A, B ∈ n × n всегда симметрич - на. Однако хотя их произведение обычно и является определенным, оно, как правило, несимметрично: (2.32)  2.2.3. Умножение на скаляр Рассмотрим, что происходит с матрицами, умножаемыми на скаляр λ ∈ . Пред - положим, что A ∈ m × n и λ ∈ . Тогда λA = K, Kij = λaij. Практически на λ умно - жается каждый компонент A. Для λ ψ ∈  верно следующее: zАссоциативность : (λψ)С = λ(ψС), С ∈ m × n; λ(BС) = (λB)С = B(λС) = (BC)λ, B ∈ m × n, C ∈ n × k. Обратите внимание: это позволяет нам менять скалярные значения местами. (λС)T = С TλT = С Tλ = λС T, так как λ = λT для всех λ ∈ . zДистрибутивность : (λ + ψ)C = λC + ψC, С ∈ m × n; λ(B + C) = λB + λC, B, С ∈ m × n. Пример 2.5 (дистрибутивность) Если определить (2.33) то для любых λ, ψ ∈ , имеем\n--- Страница 47 ---\n2.3. Решение систем линейных уравнений 47 (2.34 a) (2.34 b) 2.2.4. Компактное представление системы уравнений Если рассмотреть систему линейных уравнений (2.35) и воспользоваться правилами перемножения матриц, то данную систему урав - нений можно записать в более компактной форме как (2.36) Обратите внимание: на x1 умножается первый столбец, на x2 — второй и на x3 — третий. Как правило, систему линейных уравнений можно компактно представить в ма- тричной форме как Ax = b, см. (2.3), а произведение Ax — это (линейная) комби - нация столбцов A. Мы подробнее обсудим линейные комбинации в разделе 2.5. 2.3. РЕШЕНИЕ СИСТЕМ ЛИНЕЙНЫХ УРАВНЕНИЙ В (2.3) мы в общем виде познакомили вас с системой уравнений, то есть (2.37), где aij ∈  и bi ∈  — это известные константы, а xj — неизвестные, i = 1, , m, j = 1, , n. Как мы уже убедились, матрицы могут использоваться для компактной записи систем линейных уравнений, поэтому можем записать: Ax = b (2.10). Кроме того, мы определили базовые операции с матрицами, в частности научи -\n--- Страница 48 ---\n48 Глава 2. Линейная алгебра лись их складывать и перемножать. Далее мы сосредоточимся на решении систем линейных уравнений и дадим алгоритм для нахождения обратной матрицы. 2.3.1. Частное и общее решение Прежде чем перейти к рассмотрению общего случая решения систем линейных уравнений, обсудим пример. Рассмотрим систему уравнений: (2.38) В этой системе два уравнения и четыре неизвестных. Следовательно, как пра - вило, в таком случае должно быть бесконечно много решений. Данная система уравнений особенно проста по форме, в ней первые два столбца состоят из 1 и 0. Как вы помните, нас интересуют скаляры x1, , x4, такие что , где мы определяем ci как i-й столбец матрицы, а b — как правую часть (2.38). Решение задачи (2.38) можно найти сразу, 42 раза взяв первый столбец и 8 раз второй столбец, так что (2.39) Следовательно, решение — [42, 8, 0, 0]T. Это решение называется частным или специальным . Однако это не единственное решение системы линейных уравне - ний. Чтобы охватить все решения, нужно проявить креативность и нетривиаль - ным образом генерировать 0, беря за основу столбцы матрицы. Если прибавить 0 к нашему специальному решению, то оно не изменится. Для этого выразим третий столбец, воспользовавшись двумя первыми столбцами (которые имеют очень простую форму): (2.40) так, чтобы 0 = 8c1 + 2c2 – 1c3 + 0c4 и (x1, x2, x3, x4) = (8, 2, −1, 0). Фактически при любом умножении данного решения на λ1 ∈  получается 0-вектор, то есть (2.41)\n--- Страница 49 ---\n2.3. Решение систем линейных уравнений 49 Следуя той же логике рассуждений, выразим четвертый столбец матрицы из (2.38) при помощи двух первых столбцов и сгенерируем еще еще одно нетри - виальное выражение для 0 вида (2.42) для любой λ2 ∈ . Собрав все вместе, мы получим полное множество решений для системы уравнений из (2.38), которое называется общим решением и пред- ставляет собой множество (2.43) ПРИ МЕЧАНИЕ Общий подход, которого мы придерживались, включает три шага: 1. Найти частное решение для Ax = b. 2. Найти все решения для Ax = 0. 3. Скомбинировать решения из шагов 1 и 2, чтобы найти общее решение. Ни частное, ни общее решение не уникальны.  Решить систему линейных уравнений из предыдущего примера было легко, так как матрица из примера (2.38) обладает именно такой удобной формой, которая позволяет нам тривиально найти частное и общее решение. Однако общие си - стемы уравнений не столь просты по форме. К счастью, существует конструк - тивный алгоритмический способ преобразовать любую систему уравнений к такой особенно простой форме: это гауссово исключение. Ключевой состав - ляющей гауссова исключения являются элементарные преобразования систем линейных уравнений, переводящие систему уравнений именно в такую простую форму. Далее к такой простой форме применимы те три шага, которые уже рас - сматривались в контексте примера (2.38). 2.3.2. Элементарные преобразования Ключевым фактором решения систем линейных уравнений являются элемен ­ тарные преобразования , при которых множество решений не меняется, но сама система уравнений преобр азуется в более простую форму:\n--- Страница 50 ---\n50 Глава 2. Линейная алгебра zПерестановка двух уравнений (строк в матрице, представляющей систему уравнений). zУмножение уравнения (строки) на постоянную λ ∈  \\ {0}. zСложение двух уравнений (строк). Пример 2.6 Для a ∈  найдем все решения следующей системы уравнений: (2.44) Начнем с преобразования этой системы уравнений в компактную матрич - ную нотацию Ax = b. Мы более не будем явно упоминать переменные x и построим расширенную матрицу (вида [ A| b]). где вертикальная линия служит для отделения левой части от правой (2.44)1. Символ ⇝ будет использован для указания преобразования рас - ширенной матрицы с применением элементарных преобразований. Поменяв местами строки 1 и 3, получим Теперь, применив указанные преобразования (то есть четырежды вычтя строку 1 из строки 2), получим 1 Расширенная матрица [ A | b] компактно представляет систему линейных уравнений Ax = b.\n--- Страница 51 ---\n2.3. Решение систем линейных уравнений 51 ⇝ ⇝ ⇝ ⇝ . Расширенная матрица в данном случае приведена к удобному ступенча ­ тому виду (REF). Снова обратив эту компактную нотацию к явной нота - ции с искомыми переменными, получим . (2.45) Данная система может быть решена только для a = −1. Частное решение — (2.46) Общее решение , охватывающее весь набор возможных решений, — (2.47)\n--- Страница 52 ---\n52 Глава 2. Линейная алгебра В дальнейшем мы подробно опишем конструктивный способ получения част - ного и общего решения для системы линейных уравнений. ПРИМЕЧАНИЕ Направляющий коэффициент строки (первое ненулевое значение слева) называется «ведущим» (pivot) и всегда находится строго спра - ва от ведущего значения вышестоящего ряда. Следовательно, любая система уравнений, записанная в ступенчатом виде, структурно напоминает лестницу.  Определение 2.6 (ступенчатая форма). Матрица имеет ступенчатую форму , если zвсе ее строки, содержащие только нули, расположены в самом низу матрицы; соответственно, все строки, в которых содержится как минимум один не - нулевой элемент, расположены выше строк, содержащих только нули; zzпри рассмотрении лишь ненулевых строк первое ненулевое значение слева (также именуемое ведущим , или ведущим коэффициентом1) всегда находит - ся строго справа от ведущего значения, расположенного на ряд выше. ПРИМЕЧАНИЕ Переменные, соответствующие ведущим коэффициентам при записи в ступенчатой форме, называются базисными , а другие переменные — свободными . Например, в (2.45) x1, x3, x4 — это базисные переменные, а x2, x5 — свободные.  ПРИМЕЧАНИЕ Ступенчатая форма облегчает нам жизнь при нахождении частного решения. Для этого мы выражаем правую часть уравнения при помо - щи ведущих столбцов, так что b = , где pi, i = 1, , P, это ведущие столб - цы. λi проще всего определить, если начать с самого правого ведущего столбца и двигаться влево. В предыдущем примере мы пытались найти λ1, λ2, λ3, так чтобы (2.48) Отсюда мы относительно легко находим, что λ3 = 1, λ2 = –1, λ1 = 2. Собирая все вместе, следует не забывать и о неведущих столбцах, для каждого из которых подразумевается и ставится коэффициент 0. Следовательно, получаем частное решение x = [2, 0, −1, 1, 0]T.  1 В других книгах иногда встречается требование, что ведущий коэффициент должен быть равен 1.\n--- Страница 53 ---\n2.3. Решение систем линейных уравнений 53 ПРИМЕЧАНИЕ Система уравнений находится в приведенном ступенчатом виде (также именуемом « каноническим »), если zона находится в ступенчатом виде; zвсе ведущие коэффициенты равны 1; zведущий коэффициент является единственным ненулевым в своем столбце.  Такой приведенный ступенчатый вид будет важен ниже, в разделе 2.3.3, по - скольку позволяет легко определить общее решение для системы линейных уравнений. ПРИМЕЧАНИЕ Г ауссово исключение — это алгоритм, выполняющий элемен - тарное преобразование, для того чтобы придать системе линейных уравнений приведенный ступенчатый вид.  Пример 2.7 (приведенный ступенчатый вид) Убедимся, что следующая матрица находится в приведенном ступенчатом виде (ведущие коэффициенты выделены жирным ): (2.49) Ключевая идея относительно нахождения решений Ax = 0 — рассмотреть неведущие столбцы , которые нам понадобятся, чтобы выразить (линей - ную) комбинацию ведущих столбцов. В приведенном ступенчатом виде это относительно просто, и мы выражаем неведущие столбцы в терминах сумм и кратных тех ведущих столбцов, что расположены левее них. Вто - рой столбец равен первому, умноженному втрое (можно игнорировать ведущие столбцы правее второго столбца). Следовательно, чтобы полу - чить 0, нужно вычесть второй столбец из первого, умноженного втрое. Далее рассмотрим пятый столбец, который является вторым из неведу - щих. Пятый столбец можно выразить как умноженный втрое первый ведущий столбец, умноженный вдевятеро второй ведущий столбец и ум- ноженный на –4 третий ведущий столбец. Необходимо отслеживать все индексы ведущих столбцов и преобразовать имеющееся в умноженный втрое первый столбец, умноженный на 0 второй столбец (не являющийся ведущим), умноженный вдевятеро третий столбец (который является у нас вторым ведущим) и умноженный на –4 четвертый столбец (который является третьим ведущим). Далее нам понадобится вычесть пятый стол -\n--- Страница 54 ---\n54 Глава 2. Линейная алгебра бец, чтобы получить 0. В конце концов, мы все равно решаем однородную систему уравнений. Резюмируя, отметим, что все решения Ax = 0, x ∈ 5 даются (2.50) 2.3.3. Прием с –1 Далее расскажем о практическом приеме, помогающем считывать решения x однородной системы линейных уравнений Ax = 0, где A ∈ k × n, x ∈ n. Для начала предположим, что A находится в приведенном ступенчатом виде, без единого ряда, в котором содержались бы только нули, то есть (2.51) где * может быть любым вещественным числом, с тем ограничением, что первая ненулевая запись в строке должна быть равна 1, а все остальные записи в соот- ветствующем столбце должны быть равны 0. Столбцы j1, , jk с ведущими (вы - деленными жирным ) — это стандартные единичные векторы e1, , ek ∈ k. Эта матрица расширяется до n × n-матрицы путем добавления к ней n – k строк в форме , (2.52) так что диагональ расширенной матрицы содержит 1 или –1. Тогда столбцы , содержащие –1 в качестве ведущего, являются решениями однородной системы уравнений Ax = 0. Если быть точным, эти столбцы образуют базис (раздел 2.6.1) пространства решений Ax = 0, который мы далее будем называть ядром матри ­ цы, или нулевым пространством (раздел 2.7.3).\n--- Страница 55 ---\n2.3. Решение систем линейных уравнений 55 Пример 2.8 (прием с –1) Вернемся к матрице из примера (2.49), которая уже находится в ступен - чатом виде: (2.53) Сейчас мы расширим эту матрицу до матрицы 5 × 5, добавив строки, как в примере (2.52), там где по диагонали отсутствуют ведущие коэффици - енты, и получим (2.54) Из этой формы мы можем сразу же считывать решения для Ax = 0, взяв те столбцы , которые содержат –1 по диагонали: (2.55) что идентично решению (2.50), на которое мы «вышли». Расчет обратной матрицы Чтобы вычислить матрицу A–1, обратную A ∈ n × n, необходимо найти матрицу X, удовлетворяющую AX = In. Тогда X = A–1. Можно записать это как множество одновременных линейных уравнений AX = In, где мы находим решение для . Мы используем нотацию расширенных матриц для представле - ния данного множества систем линейных уравнений и получаем (2.56)\n--- Страница 56 ---\n56 Глава 2. Линейная алгебра Таким образом, если привести расширенную систему уравнений к приведенно - му ступенчатому виду, то можно получить обратное от правой части системы уравнений. Следовательно, определение обратной матрицы эквивалентно ре - шению системы линейных уравнений. Пример 2.9 (вычисление обратной матрицы методом гауссова исключения) Чтобы определить матрицу, обратную , (2.57) запишем расширенную матрицу и методом гауссова исключения преобразуем ее в приведенный ступен - чатый вид , так что искомая обратная окажется в ее правой части. (2.58) Можно убедиться, что (2.58) действительно обратная, выполнив умно - жение AA–1 и убедившись, что в итоге получается I4.\n--- Страница 57 ---\n2.3. Решение систем линейных уравнений 57 2.3.4. Алгоритмы для решения системы линейных уравнений Далее кратко обсудим подходы к решению системы линейных уравнений вида Ax = b. Предположим, что решение существует. Если бы решения не существо - вало, нам следовало бы прибегнуть к приближенным решениям, которые мы не рассматриваем в этой главе. Один из способов решения приближенной задачи — это линейная регрессия, о которой мы подробно поговорим в главе 9. В частных случаях мы можем быть в состоянии определить обратное A–1, такое что решение Ax = b будет выражаться как A–1b. Однако это возможно лишь в случае, если матрица A является квадратной и при этом необратима, что бы - вает нечасто. В противном случае, при довольно общих допущениях (согласно которым A должна иметь линейно независимые столбцы) можно использовать преобразование , (2.59) а также псевдообратную матрицу Мура — Пенроуза (ATA)–1AT для определения решения, удовлетворяющего Ax = b, которое также соответствует минимально - му решению наименьших квадратов. Недостаток этого подхода в том, что он требует множества вычислений для получения матрично-матричного произ - ведения, а также расчета матрицы, обратной ATA. Более того, по причинам, связанным с числовой точностью, обычно не рекомендуется вычислять обратную или псевдообратную. Поэтому далее мы кратко обсудим альтернативные под - ходы к решению систем линейных уравнений. Метод гауссова исключения играет важную роль при вычислении детерминан - тов (раздел 4.1), проверке, является ли множество векторов линейно независи - мым (раздел 2.5), вычислении обратной матрицы (раздел 2.6.2) и определении базиса векторного пространства (раздел 2.6.1). Гауссово исключение — интуи - тивно понятный и конструктивный способ решения системы линейных урав - нений с тысячами переменных. Однако для систем с миллионами переменных этот метод непрактичен, поскольку необходимое количество арифметических операций возрастает как третья степень от количества уравнений. На практике системы, содержащие много линейных уравнений, решаются кос - венно, при помощи либо стационарных итерационных методов, таких как метод Ричардсона, метод Якоби, метод Гаусса — Зейделя, и метода последовательной верхней релаксации, либо при помощи методов подпространств Крылова, таких как сопряженные градиенты, обобщенные минимальные невязки или бисопря - женные градиенты. Подробнее эти темы рассмотрены в книгах Stoer and Burlirsch (2002), Strang (2003) и Liesen and Mehrmann (2015).\n--- Страница 58 ---\n58 Глава 2. Линейная алгебра Пусть является решением Ax = b. Ключевая идея этих итерационных методов заключается в выполнении итераций вида (2.60) для подходящих C и d, чтобы на каждой итерации уменьшать остаточную ошиб - ку на каждой итерации, так чтобы решение сходилось к . В раз- деле 3.1 мы введем нормы || ⋅ ||, которые позволят нам вычислять сходство между векторами. 2.4. ВЕКТОРНЫЕ ПРОСТРАНСТВА Выше мы рассматривали системы линейных уравнений и способы их решений (раздел 2.3). Мы убедились, что системы линейных уравнений можно компактно представлять при помощи матрично-векторной нотации (2.10). Далее мы по- дробнее рассмотрим векторные пространства — так называется структуриро - ванное пространство, в котором располагаются векторы. В начале этой главы мы упрощенно охарактеризовали векторы как объекты, поддающиеся сложению друг с другом и умножению на скаляр, причем они остаются объектами одного и того же типа. Теперь мы готовы это формализовать, и для начала введем концепцию группы, представляющей собой множество элементов и операцию, определенную для этих элементов, благодаря чему не - которая структура данного множества остается неприкосновенной. 2.4.1. Группы Группы играют важную роль в информатике. Они не только закладывают осно - ву для операций над множествами, но и активно используются в криптографии, теории кода и графике. Определение 2.7 (группа). Рассмотрим множество  и операцию ⊗:  ×  → , определенную для . Тогда G := (, ⊗) называется группой , если удовлетворяет следующим условиям: 1. Замыкание  при ⊗: ∀x, y ∈  : x ⊗ y ∈ . 2. Ассоциативность : ∀x, y, z ∈  : (x ⊗ y) ⊗ z = x ⊗ (y ⊗ z). 3. Нейтральный элемент : ∃e ∈  ∀x ∈  : x ⊗ e = x и e ⊗ x = x. 4. Обратный элемент : ∀x ∈  ∃y ∈  : x ⊗ y = e и y ⊗ x = e. Часто пишут x–1, обо- значая элемент, обратный x.\n--- Страница 59 ---\n2.4. Векторные пространства 59 ПРИМЕЧАНИЕ Обратный элемент определяется относительно операции ⊗ и не обязательно означает .  Если к тому же ∀x, y ∈  : x ⊗ y = y ⊗ x, то G = (, ⊗) является абелевой группой (коммутативной). Пример 2.10 (группы) Рассмотрим несколько примеров на множества и связанные с ними опе - рации и разберемся, являются ли эти множества группами: • (, +) это группа. • (0, +) это не группа: хотя ( 0, +) и содержит нейтральный элемент (0), обратные элементы в ней отсутствуют1. • (, ·) это не группа; хотя ( , ·) и содержит нейтральный элемент (1), обратные элементы для любых z ∈ , z ≠ ±1 отсутствуют. • (, ·) это не группа, поскольку у 0 не существует обратного элемента. • ( \\ {0}, ·) это абелева группа. • (n, +), (n, +), n ∈  это абелева группа, если + определен покомпо - нентно, то есть (2.61) Тогда ( x1, ···, xn)–1 := (−x1, ···, −xn) — это обратный элемент, а e = (0, ···, 0) — нейтральный элемент. • (m × n, +), множество m × n матриц — это абелева группа (с покомпо - нентным сложением, как определено в (2.61)). Давайте подробнее остановимся на ( n × n, ·), то есть множестве n × n матриц с умножением матриц, как определено в (2.13). • Замыкание и ассоциативность прямо следуют из определения умно - жения матриц. • Нейтральный элемент: единичная матрица In — это нейтральный эле - мент относительно умножения матриц « ·» в (n × n, + ·). • Обратный элемент: если обратная существует ( A обычная), то A–1 является обратным элементом A ∈ n × n, и именно в этом случае ( n × n, + ·) является группой, называется при этом полной линейной группой . 1 .\n--- Страница 60 ---\n60 Глава 2. Линейная алгебра Определение 2.8 (полная линейная группа). Множество обычных (необрати - мых) матриц является группой A ∈ n×n относительно умножения матриц так, как оно определено в (2.13), и такая группа называется полной линейной группой GL(n, ). Однако, поскольку умножение матриц некоммутативно, эта группа не является абелевой. 2.4.2. Векторные пространства Обсуждая группы, мы рассматривали множества  и внутренние операции над , то есть отображения  ×  → , которые выполняются только над элемента - ми . В дальнейшем мы рассмотрим множества, которые, кроме внутренней операции +, также содержат внешнюю операцию ·, умножение вектора x ∈  на скаляр λ ∈ . Можно трактовать внутреннюю операцию как форму сложения, а внешнюю операцию как форму умножения. Обратите внимание: внешние/ внутренние операции никак не связаны с внешними/внутренними произведе - ниями. Определение 2.9 (векторное пространство). Векторное пространство с мно- жеством вещественных значений V = (, +,·) — это множество  с двумя опера - циями + :  ×  →  (2.62) · :  ×  → , (2.63) где 1. (, +) — это абелева группа. 2. Дистрибутивность: a. ∀λ ∈ , x, y ∈  : λ · (x + y) = λ · x + λ · y. b. ∀λ, ψ ∈ , x ∈  : (λ + ψ) · x = λ · x + ψ · x. 3. Ассоциативность (внешняя операция): ∀λ, ψ ∈ , x ∈  : λ · (ψ · x) = (λψ) · x. 4. Нейтральный элемент относительно внешней операции: ∀x ∈  : 1 · x = x. Элементы x ∈ V называются векторами . Нейтральный элемент ( , +) — это нулевой вектор 0 = [0, , 0]T, а внутренняя операция + называется сложением векторов . Элементы ψ ∈  называются скалярами , а внешняя операция · — это умножение на скаляр . Обратите внимание: скалярное произведение — это иная сущность, и о ней мы поговорим в разделе 3.2. ПРИМЕЧАНИЕ «Векторное умножение» ab, a, b ∈ n, не определено. Теоре - тически можно было бы определить поэлементное умножение, такое что с = ab при сj = ajbj. Такое «умножение матриц» распространено во многих языках про -\n--- Страница 61 ---\n2.4. Векторные пространства 61 граммирования, но с математической точки зрения не слишком целесообразно, если пользоваться стандартными правилами перемножения матриц. Трактуя векторы как матрицы n × 1 (так обычно и делается), мы можем пользоваться перемножением матриц в том виде, как оно определено в (2.13). Но тогда раз - мерности векторов не будут совпадать. Лишь следующие варианты умножения для векторов определены: abT ∈ n × n (внешнее произведение ), aTb ∈  (внутрен - нее/скалярное произведение).  Пример 2.11 (векторные пространства) Рассмотрим некоторые важные примеры: •  = n, n ∈  — это векторное пространство, операции в котором опре - деляются следующим образом: Сложение: x + y = (x1, , xn)+(y1, , yn) = (x1+y1, , xn+yn) для всех x, y ∈ n. Умножение на скаляр: λx = λ(x1, , xn) = (λx1, , λxn) для всех λ ∈ , x ∈ n. •  = m × n, т, n ∈  — это векторное пространство, для которого верно следующее: Сложение: A + B = определяется поэлементно для всех A, B ∈ . Умножение на скаляр: λA = , как определено в разделе 2.2. Не забывайте, что m × n эквивалентно mn. •  = , со стандартно определенным сложением комплексных чисел. ПРИМЕЧАНИЕ В дальнейшем мы будем обозначать векторное пространство (, +, ·) через V, где + и · — это стандартное сложение векторов и умножение на скаляр. Кроме того, для упрощения нотации мы будем обозначать векторы  как x ∈ V. \n--- Страница 62 ---\n62 Глава 2. Линейная алгебра ПРИМЕЧАНИЕ Векторные пространства n, n × 1, 1 × n отличаются только способом записи векторов. В дальнейшем мы не будем делать разницы между n и n × 1, что позволит нам записывать n-кортежи как вектор­столбец . (2.64) Таким образом упрощается нотация, описывающая операции над векторными пространствами. Однако мы различаем n × 1 и 1 × n (векторы-строки), чтобы из - бежать путаницы с перемножением матриц. По умолчанию мы обозначаем вектор-столбец через x, а вектор-строку через xT, это транспозиция x.  2.4.3. Векторные подпространства Далее мы познакомимся с векторными подпространствами. Интуитивно по - нятно, что это множества, содержащиеся в исходном векторном пространстве и обладающие таким свойством: при выполнении операций векторного про - странства над элементами из этого подпространства мы никогда не покидаем это подпространство. В данном смысле векторные подпространства «замкнуты». Векторные подпространства — это ключевая идея в машинном обучении. На - пример, в главе 10 продемонстрировано, как использовать векторные подпро - странства для снижения размерности. Определение 2.10 (векторное подпространство). Пусть V = (, +,·) это век - торное пространство, и  ⊆ ,  ≠ ∅.Тогда U = (, +, ·) называется векторным подпространством V (или линейным подпространством ), если U — это век - торное пространство с операциями векторного пространства + и ·, ограничен - ное  ×  и  × . При помощи записи U ⊆ V мы обозначаем подпространство U от V. Если и  ⊆ , а V — векторное пространство, то естественно, что U наследует некоторые свойства напрямую от V, поскольку они соблюдаются для всех x ∈  и, в частности, для всех x ∈  ⊆ . Сюда входят свойства абелевых групп, дис - трибутивность, ассоциативность и нейтральный элемент. Чтобы определить, является ли (, +, ·) подпространством V, мы должны показать, что: 1.  ≠ ∅, в частности 0 ∈ . 2. Замыкание U: a. Относительно внешней операции: ∀λ ∈  ∀x ∈  : λx ∈ . b. Относительно внутренней операции: ∀x, y ∈  : x + y ∈ .\n--- Страница 63 ---\n2.5. Линейная независимость 63 Пример 2.12 (векторные подпространства) Рассмотрим несколько примеров: • Для любого векторного пространства V тривиальными подпростран - ствами являются само V и {0}. • Только пример D на рис. 2.6 является подпространством 2 (с обыч - ными внутренними и внешними операциями). В A и C свойство за - мыкания нарушается; B не содержит 0. • Множество решений однородной системы уравнений Ax = 0 с n неиз - вестных x = [x1, , xn]T является подпространством n. • Решение неоднородной системы линейных уравнений Ax = b, b ≠ 0, не является подпространством n. • Пересечение любого количества подпространств само является под - пространством. 0 0 0 0AB CD Рис. 2.6. Не все подмножества 2 являются подпространствами. В A и C свойство замыкания нарушается; B не содержит 0. Только D является подпространством ПРИМЕЧАНИЕ Каждое подпространство U ⊆ (n, +,·) является пространством решений однородной системы однородных линейных уравнений Ax = 0 для x ∈ n.  2.5. ЛИНЕЙНАЯ НЕЗАВИСИМОСТЬ Далее подробно рассмотрим, что можно проделывать с векторами (элементами векторного пространства). В частности, векторы можно складывать друг с дру- гом и умножать их на скаляры. Свойство замыкания гарантирует, что в итоге у нас получится другой вектор в том же векторном пространстве. Можно найти такое множество векторов, которое позволит представить любой вектор в век- торном пространстве, складывая и умножая интересующие нас векторы. Это множество векторов называется базис , мы подробнее обсудим его в разделе 2.6.1.\n--- Страница 64 ---\n64 Глава 2. Линейная алгебра До того нам понадобится познакомиться с концепциями линейных комбинаций и линейной независимости. Определение 2.11 (линейная комбинация). Рассмотрим векторное простран - ство V и конечное множество векторов x1, , xk ∈ V. Тогда любое v ∈ V вида (2.65) при λ1, , λk ∈  является линейной комбинацией векторов x1, , xk. 0-вектор всегда можно записать как линейную комбинацию k векторов x1, , xk, поскольку 0 = всегда верно. В дальнейшем нас будут интересовать не - тривиальные линейные комбинации множества векторов для представления 0, то есть линейные комбинации векторов x1, , xk, где не все коэффициенты λi в (2.65) равны 0. Определение 2.12 (линейная (не)зависимость). Рассмотрим векторное про - странство V с k ∈  и x1, , xk ∈ V. Если это нетривиальная линейная комбинация, такая что 0 = , при как минимум одном λi ≠ 0, то векторы x1, , xk линей ­ но зависимы . Если существует только тривиальное решение, то есть λ1 = … = = λk = 0, то векторы x1, , xk линейно независимы . Линейная независимость — одна из важнейших концепций в линейной алгебре. Интуитивно понятно, что множество линейно независимых векторов состоит из векторов, не имеющих избыточности. То есть если удалить любые из этих векторов из множества, то какая-то информация будет утрачена. В следующих разделах мы точнее формализуем это интуитивное представление. Пример 2.13 (линейно зависимые векторы) Чтобы прояснить концепцию линейной зависимости, приведем пример из географии. Житель Найроби (Кения), рассказывая, где находится город Кигали (Руанда), может сказать: «Чтобы добраться до Кигали, сначала нужно проехать на 506 километров к северо-западу в Кампалу (Уганда), а оттуда 374 километра на юго-запад». Этой информации до - статочно, чтобы описать географическое местонахождение Кигали, так как систему географических координат можно считать двумерным век - торным пространством (игнорируя высоту и кривизну земной поверх - ности). Еще кениец может добавить: «Это примерно в 751 километре к западу отсюда». Хотя это утверждение и верно, при наличии вышепри - веденной информации оно не является необходимым, чтобы найти Ки - гали (в качестве иллюстрации см. рис. 2.7). В этом примере вектор «506 км\n--- Страница 65 ---\n2.5. Линейная независимость 65 к северо-западу» и вектор «374 км к юго-западу» линейно независимы. Это означает, что юго-западный вектор нельзя описать в терминах северо- западного вектора и наоборот. Однако третий вектор «751 км к западу» является линейной комбинацией двух векторов, поэтому из-за него мно - жество векторов становится линейно зависимым. Эквивалентно, имея «751 км к западу» и «374 км к юго-западу» можно линейно скомбиниро - вать, чтобы получить «506 км к северо-западу». 751 км к западу Кигали506 км к северо-западу 374 км к юго-западу374 км к юго-западуКампала Найроби Рис. 2.7. Географический пример (с грубыми приближениями к сторонам света), иллюстрирующий линейно зависимые векторы в двумерном пространстве (на плоскости) ПРИМЕЧАНИЕ Следующие свойства пригодятся, если необходимо выяснить, являются ли векторы линейно независимыми. zk-векторы являются либо линейно зависимыми, либо линейно независимы - ми. Третьего не дано. zЕсли как минимум один из векторов x1, , xk равен 0, то они линейно зави - симы. То же верно, если два вектора идентичны. zВекторы { x1, , xk : xi ≠ 0, i = 1, , k}, k ≥ 2 линейно зависимы тогда и только тогда, если (как минимум) один из них является линейной комбинацией других. В частности, если один вектор является кратным другому вектору, то есть xi = λxj, λ ∈ , то множество { x1, , xk : xi ≠ 0, i = 1, , k} является ли - нейно зависимым. zНа практике, чтобы проверить, являются ли векторы x1, , xk ∈ V линейно независимыми, удобно использовать гауссово исключение. Запишите все\n--- Страница 66 ---\n66 Глава 2. Линейная алгебра векторы как столбцы матрицы A и выполняйте гауссово исключение до тех пор, пока матрица не примет ступенчатый вид (приведенный ступенчатый вид здесь не является необходимым): yВедущие столбцы указывают векторы, линейно независимые от векторов, расположенных слева от них. Обратите внимание: при построении мат- рицы соблюдается порядок векторов. yНеведущие столбцы можно выразить как линейные комбинации ведущих столбцов, расположенных слева от них. Например, ступенчатый вид (2.66) позволяет заключить, что первый и третий столбец являются ведущими. Второй столбец является неведущим, поскольку равен первому, умножен - ному на три.  Все векторы-столбцы являются линейно независимыми тогда и только тогда, когда все столбцы являются ведущими. При наличии хотя бы одного неведу - щего столбца столбцы (и, следовательно, соответствующие им векторы) явля - ются линейно зависимыми. Пример 2.14 Рассмотрим 4 с (2.67) Чтобы проверить, являются ли они линейно зависимыми, воспользуемся обычным подходом и решим (2.68) для λ1 … λ3. Запишем векторы xi, i = 1, 2, 3 как столбцы матрицы и будем применять элементарные строковые операции, до тех пор пока не выявим ведущие столбцы:\n--- Страница 67 ---\n2.5. Линейная независимость 67 (2.69) Здесь каждый из столбцов матрицы является ведущим. Следовательно, нетривиального решения нет, и нам потребуется λ1 = 0, λ2 = 0, λ3 = 0, чтобы решить систему уравнений. Следовательно, векторы x1, x2, x3 линейно независимы. ПРИМЕЧАНИЕ Рассмотрим векторное пространство V с k линейно незави - симыми векторами b1, , bk и m линейных комбинаций (2.70) Определяя B = [b1, , bk] как матрицу, чьи столбцы являются линейно незави - симыми векторами b1, , bk, можно записать: (2.71) в более компактной форме. Мы хотим проверить, являются ли x1, , xm линейно независимыми. Для этого воспользуемся обычным подходом и проверим, в самом ли деле . При помощи (2.71) получим (2.72) Таким образом, { x1, , xm} линейно независимы тогда и только тогда, когда век - торы-столбцы { λ1, , λm} линейно независимы.  ПРИМЕЧАНИЕ В векторном пространстве V, m линейные комбинации k векторов x1, , xk линейно зависимы, если m > k. \n--- Страница 68 ---\n68 Глава 2. Линейная алгебра Пример 2.15 Рассмотрим множество линейно независимых векторов b1, b2, b3, b4 ∈ n и (2.73) Являются ли векторы x1, , x4 ∈ n линейно независимыми? Чтобы от - ветить на этот вопрос, проверим, являются ли векторы-столбцы (2.74) линейно независимыми. Приведенный ступенчатый вид соответствующей линейной системы уравнений с матрицей коэффициентов (2.75) дается как (2.76) Как видим, соответствующая система линейных уравнений имеет нетри - виальное решение: последний столбец не является ведущим, а x4 = = –7x1 – 15x2 – 18x3. Следовательно, x1, , x4 линейно зависимы, посколь - ку x4 может быть выражено как линейная комбинация x1, , x3. 2.6. БАЗИС И РАНГ В векторном пространстве V нас особенно интересуют множества векторов , таких что любой вектор v ∈ V можно получить линейной комбинацией векторов из . Эти векторы являются особенными, и далее мы их охарактеризуем.\n--- Страница 69 ---\n2.6. Базис и ранг 69 2.6.1. Генерация множества и базиса Определение 2.13 (генерация множества и оболочки). Рассмотрим векторное пространство V = (, +,·) и множество векторов  = {x1, , xk} ⊆ . Если каждый вектор v ∈  можно выразить как линейную комбинацию x1, , xk, то  называ - ется порождающим множеством V . Множество всех линейных комбинаций векторов в  называется оболочкой . Если  охватывает векторное простран - ство V, то мы пишем V = span [] или V = span [ x1, , xk]. Порождающие множества — это множества векторов, охватывающие (под)про - странства векторов, то есть каждый вектор можно представить, как линейную комбинацию векторов из порождающего множества. Теперь углубимся в детали и опишем минимальное порождающее множество, охватывающее (под)про - странство векторов. Определение 2.14 (базис). Рассмотрим векторное пространство V = (, +,·) и  ⊆ . Порождающее множество  от V называется минимальным , если не существует меньшего множества ⊆  ⊆ , которое охватывает V. Каждое линейно независимое порождающее множество V является минимальным и на- зывается базисом1 V. Пусть V = (, +, ·) векторное пространство и  ⊆ ,  ≠ ∅.Тогда следующие ут - верждения эквивалентны: z это базис V. z это минимальное порождающее множество. z это максимальное линейно независимое множество векторов в V, то есть при добавлении любого другого вектора в это множество оно станет линей - но зависимым. zКаждый вектор x ∈ V является линейной комбинацией векторов из , и каж- дая линейная комбинация уникальна, то есть при (2.77) и λi, ψi ∈ , bi ∈  следует, что λi = ψi, i = 1, , k. 1 Базис — это минимальное порождающее множество и максимальное линейно неза - висимое множество векторов.\n--- Страница 70 ---\n70 Глава 2. Линейная алгебра Пример 2.16 • В 3 канонический/стандартный базис — это  (2.78) • Другие базисы в 3 — это 1 2 (2.79) • Множество  (2.80) является линейно независимым, но не порождающим (и не базисным) множеством 4: например, вектор [1, 0, 0, 0]T невозможно получить ли - нейной комбинацией элементов в . ПРИМЕЧАНИЕ Каждое векторное пространство V обладает базисом . Как понятно из предыдущих примеров, у векторного пространства V может быть много базисов, то есть базис не является уникальным. Однако в каждом из ба - зисов содержится одинаковое количество элементов — базисных векторов .  Мы рассматриваем только конечномерные векторные пространства V. В таком случае размерность пространства V — это количество базисных векторов в V, что записывается как dim ( V)1. Если U ⊆ V является подпространством V, то dim (U) ≤ dim ( V), а dim ( U) = dim ( V) тогда и только тогда, когда U = V. Интуи - тивно размерность векторного пространства можно представить как количество независимых направлений в данном векторном пространстве. 1 Размерность векторного пространства соответствует количеству его базисных век - торов.\n--- Страница 71 ---\n2.6. Базис и ранг 71 ПРИМЕЧАНИЕ Размерность векторного пространства не обязательно равна количеству элементов в векторе. Например, векторное пространство V = является одномерным, хотя в его базисном векторе два элемента.  ПРИМЕЧАНИЕ Найти базис подпространства U = span [ x1, , xm] ⊆ n можно, выполнив следующие шаги: 1. Записать векторы оболочки как столбцы матрицы A. 2. Определить ступенчатый вид матрицы A. 3. Векторы оболочки, связанные с ведущими столбцами, будут базисом U.  Пример 2.17 (определение базиса) Для векторного подпространства U ⊆ 5, чьей оболочкой являются век - торы , , , , (2.81) мы хотим узнать, какие из векторов x1, , x4 являются базисом U. Для этого нужно проверить, являются ли x1, , x4 линейно независимыми. Следовательно, мы должны решить уравнение (2.82) что даст нам однородную систему уравнений с матрицей (2.83) Воспользовавшись базовыми правилами преобразования для систем линейных уравнений, получим ступенчатый вид\n--- Страница 72 ---\n72 Глава 2. Линейная алгебра Поскольку по ведущим столбцам понятно, какие множества векторов являются линейно независимыми, мы заключаем по ступенчатому виду, что x1, x2, x4 линейно независимы (поскольку система линейных уравне - ний λ1x1 + λ2x2 + λ4x4 = 0 решаема только при λ1 = λ2 = λ4 = 0). Следователь - но, {x1, x2, x4} является базисом U. 2.6.2. Ранг Количество линейно независимых столбцов матрицы A ∈ m×n равно количеству линейно независимых строк и называется рангом A, обозначается rk( A). ПРИМЕЧАНИЕ У ранга матрицы есть ряд важных свойств: zrk(A) = rk(AT), то есть столбцовый ранг равен строчному. zСтолбцы A ∈ m × n охватывают подпространство U ⊆ m с dim ( U) = rk(A). Далее мы будем называть это подпространство образом или диапазоном . Базис U можно найти, применив гауссово исключение к A, чтобы выявить ведущие столбцы. zСтроки A ∈ m × n охватывают подпространство W ⊆ n с dim ( W) = rk(A). Базис W можно найти, применив гауссово исключение к AT. zДля всех A ∈ n × n соблюдается правило, что A регулярная (необратимая) тогда и только тогда, когда rk( A) = n. zДля всех A ∈ m × n и b ∈ m линейная система уравнений Ax = b может быть решена тогда и только тогда, когда rk( A) = rk(A | b), где A | b обозначает рас - ширенную систему. zДля A ∈ m × n подпространство решений Ax = 0 обладает размерностью n – rk(A). Далее мы будем называть это подпространство ядром или нулевым пространством . zМатрица A ∈ m×n имеет полный ранг , если ее ранг равен максимальному воз - можному рангу матрицы с той же размерностью. Таким образом, ранг матри - цы равен наименьшему количеству строк и столбцов, то есть rk( A) = min (m, n). Ранг матрицы называется неполным , если полным рангом она не обладает. \n--- Страница 73 ---\n2.7. Линейные отображения 73 Пример 2.18 (ранг) У A два линейно независимых столбца и две линейно независимые стро - ки, так что rk( A) = 2. Для определения ранга используем гауссово исключение: (2.84) Здесь мы видим, что количество линейно независимых строк и столбцов равно 2, поэтому rk( A) = 2. 2.7. ЛИНЕЙНЫЕ ОТОБРАЖЕНИЯ Далее мы изучим отображения на векторные пространства, позволяющие сохра - нять их структуру. Это позволит нам определить концепцию координат. В начале этой главы было сказано, что векторы — это объекты, которые можно складывать друг с другом и умножать на скаляр, а получаемый в результате объект все равно является вектором. Мы хотим сохранить это свойство и при применении ото - бражений. Рассмотрим два вещественных векторных пространства V, W. Ото - бражение Φ : V → W сохраняет структуру векторного пространства, если Φ (x + y) = Φ(x) + Φ(y) (2.85) Φ (λx) = λΦ(x) (2.86) для всех x, y ∈ V и λ ∈ . Данные отношения можно резюмировать в виде сле - дующего определения: Определение 2.15 (линейное отображение). Для векторных пространств V, W отображение Φ: V → W называется линейным отображением (или гомоморфиз ­ мом векторного пространства / линейным преобразованием ), если ∀ x, y ∈ V ∀λ, ψ ∈  : Φ(λx + ψy) = λΦ(x) + ψΦ(y). (2.87)\n--- Страница 74 ---\n74 Глава 2. Линейная алгебра Оказывается, что линейные отображения можно представлять как матрицы (раздел 2.7.1). Также напоминаем, что можно собрать множество векторов как столбцы матрицы. При работе с матрицами необходимо учитывать, что именно представляет матрица: линейное отображение или совокупность векторов. По- дробнее о линейных отображениях мы поговорим в главе 4. Прежде чем про - должить эту тему, кратко познакомимся со специальными отображениями. Определение 2.16 (инъективные, сюръективные, биективные отображения). Рассмотрим отображение Φ :  → , где ,  могут быть произвольными мно - жествами. Тогда Φ называется zинъективным , если ∀x, y ∈  : Φ(x) = Φ (y) x = y; zсюръективным , если Φ: () = ; zбиективным, если оно одновременно является инъективным и сюръективным. Если Φ сюръективно, то каждый элемент  «достижим» из  при помощи Φ. Биективное Φ можно «обратить», то есть существует отображение Ψ :  → , такое что Ψ ◦ Φ (x) = x. Такое отображение Ψ называется обратным Φ и обычно обозначается Φ–1. Опираясь на такие определения, введем следующие специальные случаи линей - ных отображений между векторными пространствами V и W: zИзоморфизм : Φ : V → W линейное и биективное. zЭндоморфизм : Φ : V → V линейное. zАвтоморфизм : Φ : V → V линейное и биективное. Мы определяем idV : V → V, x x как тождественное отображение или тож­ дественный автоморфизм в V. Пример 2.19 (гомоморфизм) Отображение Φ : = 2 →  Φ(x) = x + ix2 является гомоморфизмом: ; (2.88)\n--- Страница 75 ---\n2.7. Линейные отображения 75 Это же объясняет, почему комплексные числа представимы в 2 в качестве кортежей: существует биективное линейное отображение, которое пре - образует поэлементное сложение кортежей в 2 в множество комплексных чисел с соответствующим сложением. Обратите внимание: мы показали линейность, но не биективность. Теорема 2.17 (теорема 3.59 в Axler (2015)). Конечномерные векторные про ­ странства V и W являются изоморфными тогда и только тогда, если dim( V) = = dim( W). Согласно теореме 2.17, существует линейное биективное отображение между двумя векторными пространствами одной и той же размерности. Интуитивно из этого следует, что векторные пространства одинаковой размерности — это вещи одного и того же рода, поскольку их можно преобразовывать друг в друга без каких-либо потерь. Кроме того, теорема 2.17 позволяет трактовать m × n (векторное пространство из m × n матриц) и mn (векторное пространство из векторов длиной mn) так же, как если бы их размерности были равны mn и существовало линейное биектив - ное отображение, преобразующее одно в другое. ПРИМЕЧАНИЕ Рассмотрим векторные пространства V, W, X. Тогда: zДля линейных отображений Φ : V → W и Ψ : W → X линейное отображение Ψ ◦ Φ : V → X также линейное. zЕсли Φ : V → W является изоморфизмом, то и Φ–1 W → V является изомор - физмом. zЕсли Φ : V → W, Ψ : V → W линейны, то Φ + Ψ и λΦ, λ ∈ , также линейны.  2.7.1. Матричное представление линейных отображений Любое n-мерное векторное пространство изоморфно n (теорема 2.17). Рассмо - трим базис { b1, , bn} n-мерного векторного пространства V. В дальнейшем будет важен порядок базисных векторов. Следовательно, запишем B = (b1, , bn) (2.89) и назовем этот n-кортеж упорядоченным базисом V . ПРИМЕЧАНИЕ Мы добрались до того места, где нотация становится не - сколько заковыристой. Поэтому обобщим здесь некоторые замечания о ней.\n--- Страница 76 ---\n76 Глава 2. Линейная алгебра B = (b1, , bn) это упорядоченный базис,  = {b1, , bn}это (неупорядоченный) базис, а B = [b1, , bn] это матрица, чьими столбцами являются векторы b1, , bn.  Определение 2.18 (координаты). Рассмотрим векторное пространство V и упо- рядоченный базис B = (b1, , bn) из V. Для любого x ∈ V мы получаем уникальное представление (линейную комбинацию) x = α1b1 + + αnbn (2.90) x относительно B. Тогда α1, , αn являются координатами x относительно B, а вектор (2.91) — это координатный вектор / координатное представление x относительно упорядоченного базиса B. Базис фактически определяет координатную систему. Мы знакомы с системой декартовых координат в двух измерениях, которая охватывается каноническими базисными векторами e1, e2. В этой координатной системе вектор x ∈ 2 имеет представление, сообщающее нам, как линейно скомбинировать e1 и e2, чтобы получить x. Однако любой базис 2 определяет действительную координатную систему, и тот же вектор x, что показан выше, может иметь иное координатное представление при базисе ( b1, b2). На рис. 2.8 координаты x относительно стан - дартного базиса ( e1, e2) равны [2, 2]T. Однако относительно базиса ( b1, b2) тот xx e2 e1b2 b1 Рис. 2.8. Две различные координатные системы, определенные двумя множествами базисных векторов. У вектора x разные координатные векторы, зависящие от выбранной координатной системы\n--- Страница 77 ---\n2.7. Линейные отображения 77 же вектор x имеет представление [1,09, 0,72]T, то есть x = 1,09b1 + 0,72 b2. В следу - ющих разделах будет показано, как получить такое представление. Пример 2.20 Рассмотрим геометрический вектор x ∈ 2 с координатами [2, 3]T отно - сительно стандартного базиса ( e1, e2) от 2. Таким образом, можно записать x = 2e1 + 3e2. Однако мы не обязаны брать стандартный базис для пред - ставления вектора. Если мы воспользуемся базисными векторами b1 = = [1, –1]T, b2 = [1, 1]T, то получим координаты T, позволяющие пред - ставить тот же самый вектор относительно ( b1, b2) (рис. 2.9). e2 e1b2 b1 Рис. 2.9. Различные координатные представления вектора x, зависящие от выбранного базиса ПРИМЕЧАНИЕ Для n-мерного векторного пространства V и упорядоченного базиса B от V отображение Φ : n → V, Φ(ei) = bi, i = 1 , n, линейно (а по теоре - ме 2.17 изоморфно), где ( e1, , en) — это стандартный базис n.  Теперь мы готовы непосредственно связать матрицы и линейные отображения между конечномерными векторными пространствами. Определение 2.19 (матрица перехода). Рассмотрим векторные простран - ства V, W с соответствующими (упорядоченными) базисами B = (b1, , bn) и C = (c1, , cm). Кроме того, рассмотрим линейное отображение Φ : V → W. Для j ∈ 1, , n — (2.92) это уникальное представление Φ(bj) относительно C. Далее мы называем m×n- матрицу AΦ, чьи элементы даются как AΦ(i, j) = αij, (2.93) матрицей перехода от Φ (относительно упорядоченных базисов B от V и C от W).\n--- Страница 78 ---\n78 Глава 2. Линейная алгебра Координаты Φ(bj) относительно упорядоченного базиса C от W — это j-й стол - бец AΦ. Рассмотрим (конечномерные) векторные пространства V, W с упоря- доченными базисами B, C и линейное отображение Φ : V → W с матрицей перехода AΦ. Если — координатный вектор x ∈ V относительно B, а — координатный вектор y = Φ(x), ∈ W относительно C, то (2.94) Это означает, что матрица перехода может использоваться для отображения координат относительно упорядоченного базиса в V на координаты относитель - но упорядоченного базиса W. Пример 2.21 (матрица перехода) Рассмотрим гомоморфизм Φ : V → W и упорядоченные базисы B = (b1, , b3) и C = (c1, , c4) от W. При (2.95) матрица перехода AΦ относительно B и C удовлетворяет Φ(bk) = для k = 1, …, 3 и дается как (2.96) где αj, j = 1, 2, 3 — это координатные векторы Φ(bj) относительно C. Пример 2.22 (линейное преобразование векторов) Рассмотрим три линейных преобразования множества векторов в 2 с матрицами перехода (2.97)\n--- Страница 79 ---\n2.7. Линейные отображения 79 На рис. 2.10 приведено три примера линейных преобразований множества векторов. На рис. 2.10( a) показано 400 векторов из 2, каждый из которых представлен точкой с соответствующими координатами ( x1, x2). Векторы расположены квадратом. При использовании матрицы A1 в (2.97), чтобы линейно преобразовать каждый из этих векторов, мы получим повернутый квадрат, как на рис. 2.10( b). Если мы применим линейное отображение, представленное A2, то получим прямоугольник как на рис. 2.10( c), где каждая координата x1 будет растянута вдвое. На рис. 2.10( d) показан ис - ходный квадрат, тот же, что и на рис. 2.10( a), но линейно преобразованный с использованием A3, что является комбинацией отражения, поворота и растягивания. (a) Исходные данные (b) Поворот на 45° (c) Растягивание по горизонтальной оси(d) Общее линейное отображение Рис. 2.10. Три примера линейного преобразования векторов, представленных в виде точек в ( a): (b) — поворот на 45 °, (с) растягивание в горизонтальных координатах в два раза и ( d) комбинация отражения, поворота и растягивания 2.7.2. Изменение базиса Далее мы подробнее рассмотрим, как меняются матрицы перехода для линей - ного отображения Φ : V → W при изменении базисов в V и W. Рассмотрим два упорядоченных базиса (2.98) из V и два упорядоченных базиса (2.99) из W. Кроме того, AΦ ∈ m×n это матрица перехода линейного отображения Φ : V → W относительно базисов B и C, а Φ ∈ m×n это соответствующее ото - бражение перехода относительно и . Далее мы исследуем, как связаны A и , то есть как (и можно ли) преобразовать AΦ в Φ, если мы решим перейти от базисов B, C к , .\n--- Страница 80 ---\n80 Глава 2. Линейная алгебра ПРИМЕЧАНИЕ Мы фактически получаем различные координатные пред - ставления тождественного отображения idV. В контексте рис. 2.9 это означало бы отобразить координаты относительно ( e1, e2) на координаты относительно (b1, b2), не меняя вектор x. Изменив базис и, соответственно, представление векторов, можно получить относительно этого нового базиса матрицу перехода в особенно простой форме, что располагает к прямолинейным вычислениям.  Пример 2.23 (изменение базиса) Рассмотрим матрицу перехода (2.100) относительно канонического базиса в 2. Если определить новый базис , (2.101) то получится диагональная матрица перехода (2.102) относительно B, работать с которой проще, чем с A. В дальнейшем мы рассмотрим отображения, преобразующие координатные векторы, построенные относительно одного базиса, в координатные векторы, построенные относительно другого базиса. Сначала постулируем наш основной результат, а затем дадим ему объяснение. Теорема 2.20 (изменение базиса). Для линейного отображения Φ : V → W упо ­ рядоченные базисы (2.103) от V и (2.104) от W, а также матрицы перехода AΦ от Φ относительно B и C, соответству ­ ющая матрица перехода Φ относительно базисов и , дается как . (2.105)\n--- Страница 81 ---\n2.7. Линейные отображения 81 Здесь S ∈ n × n это матрица перехода idV, отображающая координаты относи ­ тельно на координаты относительно B , а T ∈ m × m это матрица перехода idW, отображающая координаты относительно на координаты относительно С . Доказательство . Согласно Drumm and Weil (2001), можно записать векторы нового базиса от V как линейную комбинацию базисных векторов B, так что (2.106) Аналогично, мы записываем новые базисные векторы от W как линейную комбинацию базисных векторов C, что дает нам (2.107) Мы определяем S = ((sij)) ∈ n × n как матрицу перехода, отображающую коорди - наты относительно на координаты относительно B, и T = ((tlk)) ∈ m × m как матрицу перехода, отображающую координаты относительно на координаты относительно С. В частности, j-й столбец S — это координатное представление относительно B, а k-й столбец T — это координатное представление относи - тельно С. Обратите внимание: как S, так и T являются регулярными. Далее рассмотрим с двух точек зрения. Сначала, применив отображение Φ, получим, что для всех j ∈ 1, , n, (2.108) где впервые выражаем новые базисные векторы ∈ W как линейные комби - нации базисных векторов cl ∈ W, а затем меняем порядок суммирования. B ˜B˜CC ΨBB~ΦCBΦ ΦCB~~ΞCC~Векторные пространства Упорядоченные базисы T SAΦ ˜AΦW V B ˜B˜CC ΨBB~ΦCBΦ ΦCB~~ΞCC = ~ΞCC~–1T–1SAΦ ˜AΦW V Рис. 2.11. Для гомоморфизма Φ: V → W и упорядоченных базисов B, от V и С, от W можно выразить отображение относительно , эквивалентно как композицию гомоморфизмов относительно базисов, указанных в нижних индексах.\n--- Страница 82 ---\n82 Глава 2. Линейная алгебра В альтернативном случае, выражая ∈ V как линейные комбинации bj ∈ V, получаем (2.109 a) (2.109 b) здесь мы воспользовались линейностью Φ. Из сравнения (2.108) и (2.109 b) следует, что для всех j ∈ 1, , n и l ∈ 1, , m верно (2.110) и, следовательно (2.111), так что (2.112) что является доказательством теоремы 2.20.  Теорема 2.20 демонстрирует, что при изменении базиса в V (B меняется на ) и W (С меняется на ) матрица перехода AΦ линейного отображения Φ : V → W заменяется эквивалентной матрицей Φ, такой что (2.113) Это отношение продемонстрировано на рис. 2.11. Рассмотрим гомоморфизм Φ : V → W и упорядоченные базисы B, от V и С, от W. Отображение ΦCB является примером Φ и отображает базисные векторы B на линейные комби - нации базисных векторов С. Предположим, что нам известна матрица пере - хода AΦ от ΦCB относительно упорядоченных базисов B, С. Осуществляя из - менение базиса B на в V и C на в W, мы можем определить соответствующую матрицу перехода Φ следующим образом: сначала находим матричное представление линейного отображения : V → V, отображающее координаты относительно нового базиса на «уникальные» координаты, от - кладываемые относительно «старого» базиса B (в V). Затем используем ма - трицу перехода AΦ от ΦCB: V → W, чтобы отобразить координаты относительно C на координаты относительно . Следовательно, мы выражаем линейное\n--- Страница 83 ---\n2.7. Линейные отображения 83 отображение как составление линейных отображений, включающих «ста - рый» базис: (2.114) Конкретно, мы используем = idV и = idW, то есть тождественные ото - бражения, отображающие векторы на сами эти векторы, но относительно иного базиса. Определение 2.21 (эквивалентность). Две матрицы A, ∈ m × n эквивалентны , если существуют регулярные матрицы S ∈ n × n и T ∈ m × m, такие что = T–1AS. Определение 2.22 (подобие). Две матрицы A, ∈ n × n подобны , если существу - ет регулярная матрица S ∈ n × n с = S–1AS. ПРИМЕЧАНИЕ Подобные матрицы всегда эквивалентны. Но эквивалентные матрицы не обязательно подобны.  ПРИМЕЧАНИЕ Рассмотрим векторные пространства V, W, X. Из примечания к теореме 2.17 нам уже известно, что для отображений Φ: V → W и Ψ: W → X отображение Ψ ◦ Φ: V → X также линейно. Имея матрицы перехода AΦ и AΨ со- ответствующих отображений, мы уже знаем, что общая матрица перехода будет иметь вид AΨ ◦ Φ = AΨ AΦ.  В свете этого примечания можно рассмотреть изменения базиса с точки зрения составления линейных отображений: zAΦ это матрица перехода линейного отображения ΦCB : V → W относительно базисов B, C. z Φ это матрица перехода линейного отображения : V → W относительно базисов , . zS это матрица перехода линейного отображения : V → V (автоморфизм), представляющая в терминах B. Как правило, Ψ = idV это тождественное отображение в V. zT это матрица перехода линейного отображения : W → W (автоморфизм), представляющая в терминах C. Как правило, = idW это тождественное отображение в W. Если (нестрого) записать переход лишь в терминах базисов, то AΦ: B → C, , S : → B, T : → C и ; (2.115) (2.116)\n--- Страница 84 ---\n84 Глава 2. Линейная алгебра Обратите внимание, что порядок выполнения в (2.116) — справа налево, так как векторы умножаются в правой части и, соответственно, T –1 (AΦ(Sx)) = . Пример 2.24 (изменение базиса) Рассмотрим линейное отображение Φ : 3 → 4, чья матрица перехода (2.117) относительно стандартных базисов , (2.118) Мы ищем матрицу перехода от Φ относительно новых базисов , (2.119) Тогда , , (2.120) где i-й столбец S — это координатное представление относительно базисных векторов B. Поскольку B это стандартный базис, найти коорди - натное представление легко. Для общего базиса B нам потребуется решить систему линейных уравнений, чтобы найти λi, такое что , j = 1, …, 3. Аналогично, j-й столбец T — это координатное представление относительно базисных векторов C.\n--- Страница 85 ---\n2.7. Линейные отображения 85 Следовательно, получаем (2.121 a) (2.121 b) В главе 4 мы сможем воспользоваться концепцией изменения базиса, чтобы найти базис, относительно которого матрица перехода эндоморфизма имеет особенно простую (диагональную) форму. В главе 10 мы рассмотрим задачу сжатия данных и найдем удобный базис, на который сможем спроецировать данные, минимизировав при этом потери при сжатии. 2.7.3. Образ и ядро Образ и ядро линейного отображения — это векторные подпространства, об - ладающие некоторыми важными свойствами. Далее мы охарактеризуем их более тщательно. Определение 2.23 (образ и ядро) Для Φ : V → W определим ядро/нулевое пространство (2.122) и образ /диапазон (2.123) Также будем называть V и W областью определения и областью значений Φ со- ответственно. Интуитивно понятно, что ядро — это множество векторов в v ∈ V, отобража - емое Φ на нейтральный элемент 0W ∈ W. Образ — это множество векторов w ∈ W, «достижимое» для Φ из любого вектора в V. Это проиллюстрировано на рис. 2.12.\n--- Страница 86 ---\n86 Глава 2. Линейная алгебра ker(Φ) Im(Φ) 0V0WΦ : V → W V W Рис. 2.12. Ядро и образ линейного отображения Φ : V → W ПРИМЕЧАНИЕ Рассмотрим линейное отображение Φ : V → W, где V, W — векторные пространства. zВсегда верно, что Φ(0V) = 0W, и, следовательно, 0V ∈ ker( Φ). В частности, нулевое пространство не бывает пустым. zIm(Φ) ⊆ W — это подпространство W, а ker( Φ) ⊆ V — это подпростран - ство V. zΦ инъективно (один к одному) тогда и только тогда, когда ker( Φ) = {0}.  ПРИМЕЧАНИЕ Рассмотрим A ∈ m × n и линейное отображение Φ : n → m, x Ax. zДля A = [a1, , an], где ai — столбцы A, получим (2.124 a) (2.124 b) то есть образ — это оболочка столбцов A, также называемая пространством столбцов . Следовательно, пространство столбцов (образ) — это подпростран - ство m, где m — это «высота» матрицы. zrk(A) = dim(Im( Φ)). zЯдро / нулевое пространство — это общее решение для однородной системы уравнений Ax = 0, охватывающее все возможные линейные комбинации элементов n, дающие 0 ∈ m.\n--- Страница 87 ---\n2.7. Линейные отображения 87 zЯдро — это подпространство n, где n — это «ширина» матрицы. zСуть ядра заключается в описании отношения между столбцами, и его мож - но использовать, чтобы определить, можно ли (и как) выразить столбец в качестве линейной комбинации других столбцов.  Пример 2.25 (образ и ядро линейного отображения) Отображение , (2.125 a) (2.125 b) линейное. Чтобы определить Im( Φ), можно взять оболочку столбцов матрицы перехода и получить (2.126) Чтобы вычислить ядро (нулевое пространство) Φ, необходимо решить Ax = 0, то есть однородную систему уравнений. Для этого воспользуемся гауссовым исключением и преобразуем A в приведенный ступенчатый вид: (2.127) Эта матрица находится в приведенном ступенчатом виде, поэтому можно воспользоваться приемом с минус единицей, чтобы вычислить базис ядра (раздел 2.3.3). В качестве альтернативы можно выразить неведущие столбцы (3 и 4) как линейные комбинации ведущих столбцов (1 и 2). Третий столбец a3 эквивалентен произведению второго столбца a2 на . Следовательно, 0 = a3 + a2. Аналогично можно убедиться, что a4 = a1 –\n--- Страница 88 ---\n88 Глава 2. Линейная алгебра – a2 и, следовательно, 0 = a1 – a2 – a4. В итоге это дает нам следующее ядро (нулевое пространство): (2.128) Теорема 2.24 (теорема о ранге и дефекте). Для векторных пространств V, W и линейного отображения Φ : V → W верно, что dim(ker( Φ)) + dim(Im( Φ)) = dim( V). (2.129) Теорема о ранге и дефекте также именуется фундаментальной теоремой линей ­ ных отображений (Axler, 2015, теорема 3.22). Нижеизложенное прямо следует из теоремы 2.24: zЕсли dim(Im( Φ)) < dim( V), то ker( Φ) нетривиально, то есть ядро содержит более чем 0V и dim(ker( Φ)) ≥ 1. zЕсли AΦ — это матрица перехода Φ относительно упорядоченного базиса и dim(Im( Φ)) < dim( V), то система линейных уравнений AΦx = 0 имеет бес - конечно много решений. zЕсли dim( V) = dim( W), то соблюдается следующая тройная эквивалентность: – Φ инъективно; – Φ сюръективно; – Φ биективно. Так как Im( Φ) ⊆ W. 2.8. АФФИННЫЕ ПРОСТРАНСТВА Далее мы подробнее рассмотрим такие пространства, которые расположены с отступом от начала координат, то есть уже не являющиеся векторными под - пространствами. Кроме того, мы кратко обсудим свойства отображений между такими аффинными пространствами, напоминающие линейные отображения. ПРИМЕЧАНИЕ В литературе по машинному обучению различие между ли - нейными и аффинными иногда проводится не очень четко, поэтому можно\n--- Страница 89 ---\n2.8. Аффинные пространства 89 найти ссылки, где аффинные пространства/отображения называются линей - ными пространствами/отображениями.  2.8.1. Аффинные подпространства Определение 2.25 (аффинное подпространство). Пусть V — это векторное пространство, x0 ∈ V и U ⊆ V это подпространство. Тогда подмножество L = x0 + U := {x0 + u : u ∈ U} = (2.130 a) = {v ∈ V | ∃u ∈ U : v = x0 + u} ⊆ V (2.130 b) называется аффинным подпространством или линейным многообразием от V. U называется направлением или пространством направлений , а x0 называется точкой поддержки . В главе 12 мы будем называть такое подпространство гипер ­ плоскостью . Обратите внимание: определение аффинного подпространства исключает 0, если x0 U. Примерами аффинных подпространств являются точки, прямые и плоскости в 3, которые не обязательно проходят через начало координат. ПРИМЕЧАНИЕ Рассмотрим два аффинных подпространства L = x0 + U и векторного пространства V. Тогда L ⊆ тогда и только тогда, когда и . Аффинные подпространства часто описываются параметрами : рассмотрим k-мерное аффинное пространство L = x0 + U от V. Если ( b1, , bk) — это упорядо - ченный базис U, то любой элемент x ∈ L можно описать как x = x0 + λ1b1 + + λkbk, (2.131) где λ1, , λk ∈ . Такое представление называется параметрическим уравнением L с векторами направления b1, , bk и параметрами λ1, , λk.  Пример 2.26 (аффинные подпространства) • Одномерные аффинные подпространства называются линиями и могут быть записаны как y = x0 + λx1, где λ ∈ , где U = span[ x1] ∈ n — это одномерное подпространство n. Это означает, что линия определяет - ся точкой поддержки x0 и вектором x1, задающим направление. См. рис. 2.13 в качестве иллюстрации.\n--- Страница 90 ---\n90 Глава 2. Линейная алгебра • Двумерные аффинные подпространства n называются плоскостями . Параметрическое уравнение для плоскостей записывается как y = = x0 + λ1x1 + λ2x2, где λ1, λ2 ∈  и U = [x1, x2] ⊆ n. Это означает, что плоскость определяется точкой поддержки x0 и двумя линейно неза - висимыми векторами x1,x2, которые охватывают пространство направ - лений. • В n (n – 1)-мерные аффинные подпространства называются гипер ­ плоскостями , и им соответствует параметрическое уравнение y = = x0 + , где x1, , xn–1 образуют базис ( n – 1)-мерного подпро - странства U от n. Это означает, что гиперплоскость определяется точкой поддержки и (n – 1) линейно независимыми векторами x1, , xn–1, охватывающими пространство направлений. В 2 линия также является гиперплоскостью. В 3 плоскость также является гиперпло - скостью. 0x0 uyL = x 0 + λu Рис. 2.13. Векторы y на линии лежат в аффинном подпространстве L с точкой поддержки x 0 и направлением u ПРИМЕЧАНИЕ Для A ∈ m × n и b ∈ m решением системы уравнений Ax = b является либо пустое множество, либо аффинное подпространство n с раз- мерностью n – rk( A). В частности, решение линейного уравнения λ1x1 + + … λnxn = b, где ( λ1 …, λn) ≠ (0, …, 0), является гиперплоскостью в n. В n любое k-мерное аффинное подпространство является решением линейной неоднородной системы уравнений Ax = b, где A ∈ m × n, b ∈ m и rk(A) = n – k. Как вы помните, для однородных систем уравнений Ax = 0 решением было векторное подпространство, которое также можно считать особым аффинным пространством с точкой поддержки x0 = 0.  2.8.2. Аффинные отображения Подобно линейным отображениям между векторными пространствами, о чем мы говорили в разделе 2.7, можно определить аффинные отображения между двумя аффинными пространствами. Линейные и аффинные отображения тесно\n--- Страница 91 ---\n2.9. Дополнительное чтение 91 связаны. Следовательно, многие свойства, уже известные нам по линейным отображениям, например что отображение, составленное из двух линейных отображений, само является линейным отображением, также соблюдаются и для аффинных отображений. Определение 2.26 (аффинное отображение). Для двух векторных про - странств V, W линейного отображения Φ: V → W и a ∈ W, отображение ϕ : V → W (2.132) x a + Φ(x) (2.133) является аффинным отображением с V на W. Вектор a называется вектором трансляции ϕ. zКаждое аффинное отображение ϕ : V → W также является композицией линейного отображения Φ : V → W и трансляции τ: W → W в W, так что ϕ = τ ◦ Φ. Отображения Φ и τ определяются уникально. zКомпозиция ϕ′ ◦ ϕ аффинных отображений ϕ : V → W, ϕ′ : W → X является аффинной. zГеометрическая структура у аффинных отображений сохраняется инвари - антной. Они также сохраняют размерность и параллелизм. 2.9. ДОПОЛНИТЕЛЬНОЕ ЧТЕНИЕ Есть множество ресурсов для изучения линейной алгебры, в том числе учебни - ки Стрэнга (Strang, 2003), Голана (Golan, 2007), Экслера (Axler, 2015), а также Лизена и Мерманна (Liesen and Mehrmann, 2015). Также есть несколько онлай - новых ресурсов, которые мы упомянули во введении к этой главе. Здесь мы рассказали только о гауссовом исключении, но существует и много других способов решения систем линейных уравнений, и мы рекомендуем почитать многочисленные книги по линейной алгебре, в частности Stoer and Burlirsch (2002), Golub and Van Loan (2012) и Horn and Johnson (2013), где есть углублен - ное изучение этих тем. В этой книге различаются темы, относящиеся к линейной алгебре (например, векторы, матрицы, линейная независимость, базис), и темы, связанные с гео- метрией векторного пространства. В главе 3 мы познакомимся с векторным произведением, а далее — с нормой. При помощи этих концепций мы сможем определять углы, длины и расстояния, которые понадобятся нам для ортого - нальных проекций. Проекции играют ключевую роль во многих алгоритмах машинного обучения, в частности для линейной регрессии и анализа главных компонент. Эти темы будут рассмотрены в главах 9 и 10 соответственно.\n--- Страница 92 ---\n92 Глава 2. Линейная алгебра УПРАЖНЕНИЯ 2.1. Рассмотрим (  \\ {−1}, ∗), где a ∗ b := ab + a + b, a, b ∈ \\{−1}. (2.134) а. Покажите, что (  \\ {−1}, ∗) принадлежит абелевой группе. b. Решите 3 ∗ x ∗ x = 15 в абелевой группе (  \\ {−1}, ∗), где ∗ определено по (2.134). 2.2. Пусть n принадлежит  \\ {0}. Пусть k, x принадлежат . Определим класс сравнений целого числа k как множество Определим /n (иногда обозначаемое как n) как множество всех классов сравнения по модулю n. Евклидово деление означает, что это множество явля - ется конечным множеством, содержащим n элементов: . Для всех , ∈ n определим . a. Покажите, что ( n, ⊗) является группой. Является ли данная группа абе - левой? b. Определим другую операцию ⊗ для всех и в n как , (2.135) где a × b представляет собой обычное умножение в . Пусть n = 5. Нарисуйте таблицу умножения элементов 5\\{ } под ⊗, то есть вычислите произведение для всех и в 5\\{ }. Таким образом, покажем, что 5\\{ } замкнуто относительно ⊗ и содержит нейтральный элемент для ⊗. Отобразите обратное для всех элементов 5\\ { } под ⊗. Сделайте вывод, что 5\\{ } — абелева группа. c. Покажите, что ( 5\\{ }, ⊗) не является группой.\n--- Страница 93 ---\nУпражнения 93 d. Напомним, что теорема Безу утверждает, что два целых числа а и b взаимно просты (то есть gcd(a, b) = 1) тогда и только тогда, когда суще - ствуют два целых числа u и v, такие что au + bv = 1. Покажите, что (5\\{ }, ⊗) является группой тогда и только тогда, когда n ∈ \\{ } про - стое число. 2.3. Рассмотрим множество  матриц 3 × 3, определенное следующим образом:  . (2.136) Определим · как стандартное матричное умножение. Является ли (, ·) группой? Если да, то абелевой ли? Обоснуйте свой ответ. 2.4. Если возможно, вычислите следующие матричные произведения: a . b . c . d . e .\n--- Страница 94 ---\n94 Глава 2. Линейная алгебра 2.5. Найдите множество S всех решений по x следующих неоднородных линей - ных систем Ax = b, где A и b определены следующим образом: a. ; b. . 2.6. Используя метод исключения Гаусса, найдите все решения системы неодно - родных уравнений Ax = b с: . 2.7. Найдите все решения в системы уравнений Ax = 12x, где: и 2.8. Если возможно, выполните инвертирование следующих матриц: a. ; b. .\n--- Страница 95 ---\nУпражнения 95 2.9. Какие из следующих множеств являются подпространствами 3? a. A = {(λ, λ + μ3, λ − μ3) | λ, μ ∈ }. b. B = {(λ2,−λ2, 0) | λ ∈ }. c. Пусть γ содержится в . C = {(ξ1, ξ2, ξ3) ∈ 3 | ξ1 − 2ξ2 + 3ξ3 = γ}. d. D = {(ξ1, ξ2, ξ3) ∈ 3 | ξ2 ∈ }. 2.10. Являются ли следующие множества векторов линейно независимыми? a. ; b. . 2.11. Запишите как линейную комбинацию . 2.12. Рассмотрим два подпространства 4: Определите базис U1 ∩ U2.\n--- Страница 96 ---\n96 Глава 2. Линейная алгебра 2.13. Рассмотрим два подпространства U1 и U2, где U1 — пространство решений однородной системы уравнений A1x = 0 и U2 — пространство решений однород - ной системы уравнений A2x = 0 с a. Определите размер U1, U2. b. Определите базисы U1 и U2. c. Определите базис U1 ∩ U2. 2.14. Рассмотрим два подпространства U1 и U2, где U1 охватывает столбцы A1, а U2 охватывает столбцы A2 с a. Определите размер U1, U2. b. Определите базисы U1 и U2. c. Определите базис U1 ∩ U2. 2.15. Пусть F = {(x, y, z) ∈ 3 | x + y − z = 0} и G = {(a − b, a + b, a − 3b) | a, b ∈ }. a. Покажите, что F и G — подпространства в 3. b. Вычислите F ∩ G, не прибегая к базисному вектору. c. Найдите один базис для F и один для G, вычислите F ∩ G, используя ранее найденные базисные векторы, и проверьте свой результат с помощью предыдущего вопроса. 2.16. Являются ли следующие отображения линейными? a. Пусть a, b ∈ . где обозначает множество интегрируемых функций на [ a, b].\n--- Страница 97 ---\nУпражнения 97 b. где для k ≥ 1, C k обозначает набор k раз непрерывно дифференцируемых функций, а C 0 обозначает набор непрерывных функций. c. d. e. Пусть θ находится в [0, 2 π]. 2.17. Рассмотрим линейное отображение zНайдите матрицу преобразования AΦ. zОпределите rk( AΦ). zВычислите ядро и отображение Φ. Чем являются dim(ker( Φ)) и dim(Im( Φ))? 2.18. Пусть E — векторное пространство. Пусть f и g — два автоморфизма E, такие что f ◦ g = idE (то есть f ◦ g — тождественное отображение idE). Докажите, что ker( f) = ker(g ◦ f), Im ( g) = Im (g ◦ f) и ker( f) ∩ Im(g) = {0E}. 2.19. Рассмотрим эндоморфизм Φ : 3 → 3, матрица преобразования которого (относительно стандартного базиса в 3):\n--- Страница 98 ---\n98 Глава 2. Линейная алгебра a. Определите ker( Φ) и Im( Φ). b. Определите матрицу преобразований относительно базиса то есть выполните изменение базиса к новому базису B. 2.20. Рассмотрим , , , — четыре вектора 2, выраженные в стандартном базисе 2 как: , и определим два упорядоченных базиса B = ( , ) и = ( , ) из 2. a. Покажите, что B и являются двумя базисами 2 и нарисуйте эти базис - ные векторы. b. Вычислите матрицу P1, которая выполняет изменение базиса с на B. c. Рассмотрим c1, c2, c3, три вектора из 3, определенные в стандартном ба - зисе  как: . Определим C = (с1, c2, c3). 1. Покажите, что C является базисом 3, например с помощью детерми - нантов (раздел 4.1). 2. Пусть = ( , , ) — стандартный базис 3. Определите матри - цу P2, которая выполняет изменение базиса с C на . d. Рассмотрим гомоморфизм Φ : 2 → 3, такой что\n--- Страница 99 ---\nУпражнения 99 где B = (b1, b2) и C = (c1, c2, c3) — упорядоченные базисы 2 и 3 соответ ственно. Определите матрицу преобразования AΦ матрицы Φ относительно упорядо - ченных базисов B и C. e. Определите , матрицу преобразования Ф относительно базисов и . f. Рассмотрим вектор x ∈ 2, координаты которого в — [2, 3]T. Другими словами, x = 2 + 3 . 1. Вычислите координаты x в B. 2. Исходя из этого, вычислите координаты Ф( x), выраженные в С. 3. Затем запишите Ф( x) исходя из , , . 4. Используйте представление x в и матрицу , чтобы получить этот результат напрямую.\n--- Страница 100 ---\n3 Аналитическая геометрия В главе 2 мы изучали векторы, векторные пространства и линейные отобра - жения в общем, но абстрактном виде. В данной главе добавим к этим поняти - ям геометрическую интерпретацию и интуицию. В частности, рассмотрим геометрические векторы, научимся вычислять их длины, расстояния и углы между двумя векторами. Чтобы можно было это делать, определим на вектор - ном пространстве внутреннее произведение, которое придает ему геометри - ческие свойства. Внутренние произведения и соответствующие им нормы и метрики заключают в себе интуитивные представления о подобии и рас- стояниях, которыми мы воспользуемся в главе 12 при разработке машины опорных векторов. Затем нам понадобятся понятия длины вектора и угла между векторами, чтобы разобраться с ортогональными проекциями, которые будут в центре обсуждения анализа главных компонент в главе 10, а также регрессии и оценки максимального правдоподобия в главе 9. На рис. 3.1 по - казано, как связаны концепции из этой главы друг с другом и с материалом из других глав книги. 3.1. НОРМЫ Рассуждая о геометрических векторах, то есть о направленных отрезках, на - чинающихся в начале координат, мы интуитивно понимаем, что длина вектора — это расстояние от начала координат до «конца» этого отрезка. Далее мы будем обсуждать феномен длины вектора, пользуясь понятием нормы. Определение 3.1 (норма). Норма векторного пространства V — это функция (3.1) , (3.2)\n--- Страница 101 ---\n3.1. Нормы 101 /uni0412/uni043D/uni0443/uni0442/uni0440/uni0435/uni043D/uni043D/uni0435/uni0435 /uni043F/uni0440/uni043E/uni0438/uni0437/uni0432/uni0435/uni0434/uni0435/uni043D/uni0438/uni0435 /uni041D/uni043E/uni0440/uni043C/uni0430 /uni0414/uni043B/uni0438/uni043D/uni044B/uni041E/uni0440/uni0442/uni043E/uni0433/uni043E/uni043D/uni0430/uni043B/uni044C/uni043D/uni0430/uni044F /uni043F/uni0440/uni043E/uni0435/uni043A/uni0446/uni0438/uni044F/uni0423/uni0433/uni043B/uni044B /uni041F/uni043E/uni0432/uni043E/uni0440/uni043E /uni0442/uni044B /uni0413/uni043B/uni0430/uni0432/uni0430 4. /uni041C/uni0430/uni0442/uni0440/uni0438/uni0447/uni043D/uni044B/uni0435 /uni0440/uni0430/uni0437/uni043B/uni043E/uni0436/uni0435/uni043D/uni0438/uni044F/uni0413/uni043B/uni0430/uni0432/uni0430 10. /uni0421/uni043D/uni0438/uni0436/uni0435/uni043D/uni0438/uni0435 /uni0440/uni0430/uni0437/uni043C/uni0435/uni0440/uni043D/uni043E/uni0441 /uni0442/uni0438/uni0413/uni043B/uni0430/uni0432/uni0430 9. /uni0420/uni0435/uni0433/uni0440/uni0435/uni0441/uni0441/uni0438 /uni044F/uni0413/uni043B/uni0430/uni0432/uni0430 12. /uni041A/uni043B/uni0430/uni0441/uni0441/uni0438/uni0444/uni0438/uni043A/uni0430/uni0446/uni0438/uni044F/uni0438/uni043D/uni0434/uni0443/uni0446/uni0438/uni0440/uni0443/uni0435/uni0442 Рис. 3.1. Ассоциативная карта концепций, вводимых в этой главе, и их связи с материалом из других частей книги где каждому вектору x присваивается значение длины || x || ∈ , такое что для всех λ ∈  и x, y ∈ V соблюдается следующее: zАбсолютная однородность : || λx || = |λ| || x ||. zНеравенство треугольника : || x + y|| ≤ || x || + || y ||. zПоложительная определенность : || x || ≥ 0 и || x || = 0 x = 0. a b c /g100 a + b Рис. 3.2. Неравенство треугольника В геометрических терминах неравенство треугольника постулирует, что для любого треугольника сумма длин любых двух его сторон должна быть более или равна длине третьей стороны; см. рис. 3.2 в качестве иллюстрации. Определе - ние 3.1 дано в терминах общего векторного пространства V (раздел 2.4), но в этой книге рассматривается только конечномерное векторное пространство n. Пом -\n--- Страница 102 ---\n102",
      "debug": {
        "start_page": 36,
        "end_page": 102
      }
    },
    {
      "name": "Глава 3. Аналитическая геометрия 100",
      "content": "--- Страница 102 --- (продолжение)\nГлава 3. Аналитическая геометрия ните, что для вектора x ∈ n элементы вектора обозначаются с использованием нижнего индекса, то есть xi — это i-й элемент вектора x. Пример 3.1 (манхэттенская норма) Манхэттенская норма (манхэттенское расстояние ) для n определяется для x ∈ n как (3.3) где | · | — абсолютное значение. В левой панели на рис. 3.3 показаны все векторы x ∈ 2 при || x ||1 = 1. Манхэттенская норма также называется нормой . 111 1|| x ||1 = 1| | x ||2 = 1 Рис. 3.3. Для различных норм линиями ограничены множества векторов с нормой 1. Слева: манхэттенская норма; справа: евклидово расстояние Пример 3.2 (евклидова норма) Евклидова норма для x ∈ n определяется как (3.4) и позволяет вычислить евклидово расстояние от x до начала координат. В правой части рис. 3.3 показаны все векторы x ∈ 2 при || x ||2 = 1. Евкли - дова норма также называется нормой . ПРИМЕЧАНИЕ По умолчанию в этой книге используется евклидова нор - ма (3.4), если не указано иное.  3.2. ВНУТРЕННИЕ ПРОИЗВЕДЕНИЯ При помощи внутренних произведений (внутренним называется произведение одного вектора на другой) удобно ввести интуитивно понятные геометрические\nГлава 3. Аналитическая геометрия ните, что для вектора x ∈ n элементы вектора обозначаются с использованием нижнего индекса, то есть xi — это i-й элемент вектора x. Пример 3.1 (манхэттенская норма) Манхэттенская норма (манхэттенское расстояние ) для n определяется для x ∈ n как (3.3) где | · | — абсолютное значение. В левой панели на рис. 3.3 показаны все векторы x ∈ 2 при || x ||1 = 1. Манхэттенская норма также называется нормой . 111 1|| x ||1 = 1| | x ||2 = 1 Рис. 3.3. Для различных норм линиями ограничены множества векторов с нормой 1. Слева: манхэттенская норма; справа: евклидово расстояние Пример 3.2 (евклидова норма) Евклидова норма для x ∈ n определяется как (3.4) и позволяет вычислить евклидово расстояние от x до начала координат. В правой части рис. 3.3 показаны все векторы x ∈ 2 при || x ||2 = 1. Евкли - дова норма также называется нормой . ПРИМЕЧАНИЕ По умолчанию в этой книге используется евклидова нор - ма (3.4), если не указано иное.  3.2. ВНУТРЕННИЕ ПРОИЗВЕДЕНИЯ При помощи внутренних произведений (внутренним называется произведение одного вектора на другой) удобно ввести интуитивно понятные геометрические\n--- Страница 103 ---\n3.2. Внутренние произведения 103 концепции, в частности длину вектора и угол или расстояние между двумя векторами. Главное назначение внутренних произведений — определять, орто - гональны ли векторы друг другу. 3.2.1. Скалярное произведение Возможно, вам уже известно об особом типе внутреннего произведения, ска­ лярном произведении n, которое записывается как (3.5) Такой частный случай внутреннего произведения мы будем называть в этой книге скалярным. Но концепция внутренних произведений более общая, и вну- тренние произведения обладают специфическими свойствами, о которых мы сейчас расскажем. 3.2.2. Общие внутренние произведения Вспомните линейное отображение из раздела 2.7, где было показано, как можно переупорядочить отображение при сложении и умножении на скаляр. Билиней ­ ное отображение Ω — это отображение с двумя аргументами, и в каждом аргу - менте оно линейно, то есть при рассмотрении векторного пространства V для всех x, y, z ∈ V, λ, ψ ∈  верно, что Ω(λx + ψy, z) = λΩ(x, z) + ψΩ(y, z); (3.6) Ω(x, λy + ψz) = λΩ(x, y) + ψΩ(x, z). (3.7) Здесь в (3.6) утверждается, что Ω линейно в первом аргументе, а в (3.7) утверж - дается, что Ω линейно во втором аргументе (см. также (2.87)). Определение 3.2. Пусть V — векторное пространство, а Ω : V × V →  билиней - ное отображение, которое принимает два вектора и отображает их на веществен - ное число. Тогда zΩ называется симметричным , если Ω(x, y) = Ω(y, x) для всех x, y ∈ V, то есть порядок следования аргументов не важен. zΩ называется положительно определенным , если ∀ x ∈ V \\ {0} : Ω(x, x) > 0, Ω(0, 0) = 0. (3.8) Определение 3.3. Пусть V — векторное пространство, а Ω : V × V →  билиней - ное отображение, которое принимает два вектора и отображает их на веществен - ное число. Тогда:\n--- Страница 104 ---\n104 Глава 3. Аналитическая геометрия zПоложительно определенное, симметричное билинейное отображение Ω : V × V →  называется внутренним произведением V . Как правило, пишут <x, y>, а не Ω(x, y). zzПара ( V, <·, ·>) называется предгильбертовым пространством или (веще - ственным) векторным пространством с внутренним произведением . Если воспользоваться скалярным произведением, определенным в (3.5), то (V, < , ·>) называется евклидовым векторным пространством . Такие пространства в этой книге будут называться предгильбертовыми про - странствами. Пример 3.3 (внутреннее произведение, не являющееся скалярным) Рассмотрим V = 2. Если определить , (3.9) то < ·, ·> является внутренним произведением, но отличается от скаляр - ного произведения. Докажите это в качестве упражнения. 3.2.3. Симметричные положительно определенные матрицы Симметричные положительно определенные матрицы играют важную роль в машинном обучении. Они определяются при помощи внутреннего произве - дения. В разделе 4.3 мы вернемся к симметричным положительно определенным матрицам в контексте разложения матриц. Идея симметричных положительно полуопределенных матриц играет ключевую роль при определении ядер (раз - дел 12.4). Рассмотрим n-мерное векторное пространство V с внутренним произведением <·, ·> : V × V →  (см. определение 3.3) и упорядоченным базисом B = (b1, , bn) от V. Как вы помните из раздела 2.6.1, любые векторы x, y ∈ V можно записать как линейную комбинацию базисных векторов, такую что x = и y = для подходящих ψiλj ∈ . Поскольку внутреннее произведе - ние билинейно, для всех x, y ∈ V верно, что (3.10) где Aij : = <bi, bj>, а — это координаты x и y относительно базиса B. Это под - разумевает, что внутреннее произведение < ·, ·> уникально определяется через A.\n--- Страница 105 ---\n3.2. Внутренние произведения 105 Симметрия внутреннего произведения также означает, что A симметрично. Кроме того, положительная определенность внутреннего произведения подразу- мевает, что ∀ x ∈ V \\ {0} : xTAx > 0. (3.11) Определение 3.4 (симметричная, положительно определенная матрица). Симметричная матрица A ∈ n × n, удовлетворяющая (3.11), называется сим­ метричной, положительно определенной или просто положительно определенной . Если для (3.11) верно только ≥, то A называется симметричной, положительно полуопределенной . Пример 3.4 (симметричные, положительно определенные матрицы) Рассмотрим матрицы (3.12) A1 является положительно определенной, поскольку она симметрична и (3.13 a) (3.13 b) для всех x ∈ V \\ {0}. Напротив, A2 симметрична, но не положительно определенна, так как может быть меньше 0, напр. для x = [2, –3]T. Если A ∈ n × n симметрична, положительно определенна, то (3.14) определяет внутреннее произведение относительно упорядоченного базиса B, где и — координатные представления x, y ∈ V относительно B. Теорема 3.5. Для вещественнозначного, конечномерного векторного простран ­ ства V и упорядоченного базиса B от V верно, что < ·, ·> : V × V →  является внутренним произведением тогда и только тогда, если существует симметрич ­ ная, положительно определенная матрица A ∈ n × n с . (3.15)\n--- Страница 106 ---\n106 Глава 3. Аналитическая геометрия Следующие свойства соблюдаются, если A ∈ n × n является симметричной и по- ложительно определенной: zНулевое пространство (ядро) A состоит лишь из 0, поскольку xT Ax > 0 для всех x ≠ 0. Это подразумевает, что Ax ≠ 0 и x ≠ 0. zzДиагональные элементы aii от A положительны, поскольку aii = Aei > 0, где ei это i-й вектор стандартного базиса в n. 3.3. ДЛИНЫ И РАССТОЯНИЯ В разделе 3.1 мы уже обсуждали нормы, которыми можно пользоваться для вычисления длины вектора. Внутренние произведения и нормы тесно связаны в том смысле, что любое внутреннее произведение вводит норму (3.16) естественным образом, так что длины векторов можно вычислять при помощи внутреннего произведения. Но не всякая норма вводится внутренним произ - ведением. Манхэттенская норма (3.3) — такая, у которой нет соответствующего внутреннего произведения. В дальнейшем мы сосредоточимся на нормах, вво - димых внутренними произведениями, и познакомимся с геометрическими концепциями, в частности с длинами, расстояниями и углами. ПРИМЕЧАНИЕ Для векторного пространства внутреннего произведения (V,<·, ·>) индуцированная норма || · || удовлетворяет неравенству Коши — Швар ца1 (3.17)  Пример 3.5 (длины векторов с использованием внутренних произведений) В геометрии нас часто интересуют длины векторов. Теперь для их вы - числения мы можем воспользоваться внутренним произведением (3.16). Предположим, x = [1, 1]T ∈ 2. Если воспользоваться скалярным произ - ведением как внутренним, то, исходя из (3.16), получим (3.18) 1 В отечественной литературе часто называют неравенством Коши — Буняковского — Шварца. — Примеч. науч. ред.\n--- Страница 107 ---\n3.3. Длины и расстояния 107 в качестве длины x. Теперь выберем иное внутреннее произведение: (3.19) Если вычислить норму вектора, то это внутреннее произведение будет возвращать меньшие значения, чем скалярное произведение, в случае если у x1 и x2 одинаковый знак (а x1x2 > 0); в противном случае оно будет воз - вращать б ольшие значения, чем скалярное произведение. С таким внут- ренним произведением имеем: (3.20) такое что x «короче» с этим внутренним произведением, чем со скалярным произведением. Определение 3.6 (расстояние и метрика). Рассмотрим пространство внутрен - него произведения ( V,<·, ·>). Тогда (3.21) называется расстоянием между x и y для x, y ∈ V. Если использовать скалярное произведение как внутреннее, то это расстояние называется евклидовым рас ­ стоянием . Отображение (3.22) (3.23) называется метрикой . ПРИМЕЧАНИЕ Подобно длине вектора, расстояние между векторами не требует внутреннего произведения; достаточно нормы. Если у вас есть норма, индуцированная внутренним произведением, то расстояние может отличаться в зависимости от выбранного внутреннего произведения.  Метрика d удовлетворяет следующим условиям: 1. d является положительно определенной, то есть d(x, y) ≥ 0 для всех x, y ∈ V и d(x, y) = 0 x = y. 2. d симметрично , то есть d(x, y) = d(y, x) для всех x, y ∈ V. 3. Неравенство треугольников : d(x, z) ≤ d(x, y) для всех x, y, z ∈ V.\n--- Страница 108 ---\n108 Глава 3. Аналитическая геометрия ПРИМЕЧАНИЕ На первый взгляд, списки свойств внутренних произведений и метрик выглядят очень похоже. Однако если сравнить определение 3.3 с опре- делением 3.6, то можно заменить, что < x, y> и d(x, y) действуют в противопо - ложных направлениях. Очень схожие x и y будут давать большое значение для внутреннего произведения и малое значение для метрики.  3.4. УГЛЫ И ОРТОГОНАЛЬНОСТЬ Внутренние произведения позволяют не только определять длины векторов и расстояние между двумя векторами, но и схватывают геометрию векторного пространства, определяя угол ω между двумя векторами. Мы воспользуемся неравенством Коши — Шварца (3.17) для определения углов ω в пространствах внутренних произведений между двумя векторами x, y, и это определение со - впадает с интуитивным пониманием 2 и 3. Предположим, что x ≠ 0, y ≠ 0. Тогда (3.24) Следовательно, существует уникальный ω ∈ [0, π], показанный на рис. 3.4, с (3.25) 0 π/2 π ω–101cos(ω) Рис. 3.4. В области [0, π] f(ω) = cos(ω) возвращает число в интервале [−1, 1] Число ω — это угол между векторами x и y. Интуитивно понятно, что угол между двумя векторами позволяет судить, насколько схоже они ориентированы. Например, воспользовавшись скалярным произведением, найдем, что угол между x и y = 4x (то есть y — это x с некоторым множителем) равен 0. Пример 3.6 (угол между векторами) Вычислим угол между x = [1, 1]T ∈ 2 и y = [1, 2]T ∈ 2; см. рис. 3.5, где мы используем внутреннее произведение как скалярное. Затем получаем (3.26)\n--- Страница 109 ---\n3.4. Углы и ортогональность 109 и угол между двумя векторами равен , что со - ответствует примерно 18 °. y x 1 01 ω Рис. 3.5. Угол ω между двумя векторами x и y вычисляется при помощи внутреннего произведения Определение 3.7 (ортогональность). Два вектора x и y ортогональны тогда и только тогда, когда < x, y> = 0, и мы пишем . Если к тому же || x || = 1 = = || y ||, то есть векторы являются единичными, то x и y ортонормированны . Из этого определения следует, что 0-вектор ортогонален любому вектору в век- торном пространстве. ПРИМЕЧАНИЕ Ортогональность — это обобщение концепции перпендику - лярности, применимой к билинейным формам, которые не обязательно дают скалярное произведение. В нашем контексте с геометрической точки зрения можно считать, что ортогональные векторы расположены под прямым углом относительно конкретного внутреннего произведения.  Пример 3.7 (ортогональные векторы) y –1 1 01 ωx Рис. 3.6. Угол ω между двумя векторами x и y может меняться в зависимости от внутреннего произведения Рассмотрим два вектора x = [1, 1]T, y = [–1, 1]T ∈ 2; см. рис. 3.6. Нас ин - тересует угол ω между ними, который мы найдем, исходя из двух внут- ренних произведений. Используя скалярное произведение как внутрен -\n--- Страница 110 ---\n110 Глава 3. Аналитическая геометрия нее, получим угол ω между x и y, равный 90 °, такой что . Однако если выбрать внутреннее произведение (3.27) то получим, что угол ω между x и y вычисляется как (3.28) и x и y не ортогональны. Следовательно, векторы, ортогональные отно - сительно одного внутреннего произведения, не обязательно должны быть ортогональны относительно другого внутреннего произведения. Определение 3.8 (ортогональная матрица). Квадратная матрица A ∈ n × n яв- ляется ортогональной матрицей тогда и только тогда, когда ее столбцы орто - нормированны, так что AAT = I = ATA, (3.29) что подразумевает A–1 = AT, (3.30) то есть обратная матрица получается простым транспонированием матрицы. Преобразования ортогональных матриц1 представляют особенный случай, по - скольку длина вектора x не меняется, если преобразовывать его с применением ортогональной матрицы A. Для скалярного произведения получаем (3.31) Более того, угол между любыми двумя векторами x, y, измеренный по их внутреннему произведению, также не изменится, если преобразовать оба эти вектора с применением ортогональной матрицы A. Взяв скалярное произ - ведение в качестве внутреннего, получим, что угол образов Ax и Ay дается как 1 Принято называть такие матрицы ортогональными, но точнее было бы называть их ортонормированными. При преобразованиях ортогональных матриц сохраняются расстояния и углы.\n--- Страница 111 ---\n3.5. Ортонормированный базис 111 (3.32) то есть в точности равен углу между x и y. Это означает, что у ортогональных матриц A, где AT = A–1, сохраняются как углы, так и расстояния. Оказывается, что ортогональные матрицы определяют преобразования, являющиеся пово - ротами (с возможностью переворота). В разделе 3.9 мы подробнее поговорим о поворотах. 3.5. ОРТОНОРМИРОВАННЫЙ БАЗИС В разделе 2.6.1 мы охарактеризовали свойства базисных векторов и нашли, что в n-мерном векторном пространстве нам требуется n базисных векторов, то есть n векторов, которые линейно независимы. В разделах 3.3 и 3.4 мы пользовались внутренними произведениями для вычисления длины векторов и угла между векторами. Далее мы обсудим особый случай, в котором базисные векторы ортогональны друг другу и где длина каждого базисного вектора равна 1. Такой базис мы будем называть ортонормированным. Дадим ему более строгое определение. Определение 3.9 (ортонормированный базис). Рассмотрим n-мерное векторное пространство V и базис { b1, , bn} от V. Если для (3.33) (3.34) для всех i, j = 1, , n, то базис называется ортонормированным базисом (ОНБ) (orthonormal basis, ONB). Если соблюдается лишь (3.3), то базис называется ортогональным . Обратите внимание: (3.34) предполагает, что длина/норма каждого базисного вектора равна 1. Как вы помните из раздела 2.6.1, можно использовать гауссово исключение, чтобы найти базис векторного пространства, ограниченного множеством век - торов. Допустим, у нас есть множество неортогональных ненорми - рованных базисных векторов. Мы сцепляем их в матрицу и при- меняем гауссово исключение к расширенной матрице (раздел 2.3.2) для получения ортонормированного базиса. Такой конструктивный подход, позволяющий итеративно выстроить ортонормированный базис { b1 … bn}, на- зывается « процесс Грама — Шмидта » (Str ang 2003).\n--- Страница 112 ---\n112 Глава 3. Аналитическая геометрия Пример 3.8 (ортонормированный базис) Канонический/стандартный базис евклидова векторного пространства n является ортонормированным базисом, внутреннее произведение кото - рого — это скалярное произведение векторов. В 2 векторы (3.35) образуют ортонормированный базис, поскольку и . Мы воспользуемся концепцией ортонормированного базиса в главах 10 и 12, где обсудим машины опорных векторов и анализ главных компонент. 3.6. ОРТОГОНАЛЬНОЕ ДОПОЛНЕНИЕ Определив ортогональность, давайте теперь рассмотрим векторные простран - ства, ортогональные друг другу. Эта концепция сыграет важную роль в главе 10, где мы обсудим линейное снижение размерности с геометрической точки зрения. Рассмотрим D-мерное векторное пространство V и M-мерное подпространство, такое что U ⊆ V. Тогда его ортогональное дополнение будет ( D – M)-мерным подпространством от V и будет содержать все векторы из V, которые ортого - нальны каждому из векторов U. Кроме того, U ∩ = {0}, поэтому каждый из векторов x ∈ V поддается уникальному разложению в (3.36) где (b1 … bM) это базис U, а — это базис . Следовательно, ортогональное дополнение также может применяться для опи - сания плоскости U (двумерного подпространства) в трехмерном векторном пространстве. Точнее говоря, вектор w с || w || = 1, ортогональный плоскости U, является базисным вектором . Такая структура проиллюстрирована на рис. 3.7. Все векторы, ортогональные w, должны (по определению) лежать в плоскости U. Вектор w называется нормальным вектором U. В принципе, ортогональные дополнения могут использоваться для описания гиперплоскостей в n-мерном векторном и аффинном пространствах.\n--- Страница 113 ---\n3.7. Внутреннее произведение функций 113 e3 e2 e1w U Рис. 3.7. Плоскость U в трехмерном векторном пространстве можно описать ее нормальным вектором, который охватывает ее ортогональ- ное дополнение 3.7. ВНУТРЕННЕЕ ПРОИЗВЕДЕНИЕ ФУНКЦИЙ До сих пор мы рассматривали свойства внутренних произведений при вычис - лении длин, углов и расстояний. Мы уделяли особое внимание внутренним произведениям конечномерных векторов. Далее мы рассмотрим пример вну - тренних произведений векторов иного типа: поговорим о внутренних произ - ведениях функций. Внутренние произведения, которые мы обсуждали до сих пор, определялись для векторов с конечным числом включений. Можно считать вектор x ∈ n функцией с n значений функции. Концепцию внутреннего произведения мож - но обобщить до векторов с бесконечным числом включений (счетно бесконеч - ных), а также до функций с непрерывными значениями (бессчетно бесконечных). Затем, просуммировав отдельные компоненты векторов — см., в частности, пример 3.5, — получаем интеграл. Внутреннее произведение двух функций u:  →  и v:  →  можно определить как определенный интеграл (3.37) для нижнего и верхнего предела a, b < ∞ соответственно. Как и в случае с обыч - ным внутренним произведением, можно определять нормы и ортогональность, ориентируясь на внутреннее произведение. Если (3.37) результирует в 0, то функции u и v ортогональны. Чтобы добиться математической точности преды - дущего внутреннего произведения, нужно уделить внимание мерам и опреде - лению интегралов, что приведет нас к определению гильбертова пространства. В дальнейшем, в отличие от внутренних произведений конечномерных векторов, внутренние произведения функций могут расходиться (иметь бесконечное значение). Все это требует вдаваться в тонкие детали вещественного и функцио- нального анализа, но мы не рассматриваем данных тем в этой книге.\n--- Страница 114 ---\n114 Глава 3. Аналитическая геометрия Пример 3.9 (внутреннее произведение функций) Если взять u = sin(x) и v = cos(x), то получится подынтегральная функция f(x) = u(x)v(x) от (3.37), показанная на рис. 3.8. Мы видим, что эта функ - ция нечетная, то есть f(−x) = −f(x). Следовательно, интеграл с пределами a = −π, b = π от этого произведения равен 0. Следовательно, функции sin и cos ортогональны. –2,5 0,0 2,5 x–0,50,00,5sin(x)cos(x) Рис. 3.8. f(x) = sin(x)cos( x) ПРИМЕЧАНИЕ Также верно, что набор функций (3.38) ортогонален, если интегрировать от −π до π, то есть в любой паре функций обе функции ортогональны друг другу. Набор функций в (3.38) охватывает большое подпространство функций, являющихся четными и периодическими на отрезке [−π, π), и проецирование функций на это подпространство — фундаментальная идея, на которой основаны ряды Фурье.  В разделе 6.4.6 мы рассмотрим второй тип необычных внутренних произведений: внутреннее произведение случайных переменных. 3.8. ОРТОГОНАЛЬНЫЕ ПРОЕКЦИИ Проекции — это важный класс линейных преобразований (наряду с поворотами и отражениями). Они играют важную роль в графике, теории кодирования, статистике и машинном обучении. В машинном обучении часто приходится иметь дело с данными, обладающими высокой размерностью. Такие данные зачастую бывает сложно анализировать или визуализировать. Однако много - мерным данным весьма часто присуще следующее свойство: основная часть информации содержится всего в нескольких измерениях, а большинство других измерений несущественны при описании ключевых свойств данных. При сжа - тии и визуализации данных, которые обладают большим количеством измерений, часть информации теряется. Чтобы минимизировать такие потери, приходящи -\n--- Страница 115 ---\n3.8. Ортогональные проекции 115 еся на сжатие, в идеале нужно найти наиболее информативные измерения данных. Как говорилось в главе 1, данные можно представлять в векторном виде, и в этой главе мы обсудим некоторые инструменты, принципиально важные для сжатия данных. Точнее, мы рассмотрим, как можно проецировать исходные данные с высокой размерностью на признаковое1 пространство с меньшей раз - мерностью, чтобы извлечь больше информации обо всем множестве данных и извлечь из него важные паттерны. Например, в алгоритмах машинного обу - чения, таких как анализ главных компонент (PCA), рассмотренный в Pearson (1901) и Hotelling (1933), а также при работе с глубокими нейронными сетями (например, с глубокими автоэнкодерами) активно используется идея снижения размерности. Далее в этой главе мы сосредоточимся на ортогональных проек - циях, которыми воспользуемся в главе 10 для линейного снижения размерности и в главе 12 — для классификации. Даже линейная регрессия, обсуждаемая в главе 9, поддается интерпретации при помощи ортогональных проекций. Для заданного пространства с малой размерностью ортогональные проекции данных, обладающих высокой размерностью, сохраняют максимум информации, на - сколько это возможно, а также минимизируют разницу/погрешность между исходными данными и соответствующей им проекцией. Такая ортогональная проекция проиллюстрирована на рис. 3.9. Прежде чем мы подробно обсудим, как получать такие проекции, давайте определим, что же представляет собой проекция. –4 –2 0 2 4 x1–2–1012x2 Рис. 3.9. Ортогональная проекция (светлые точки) двумерного множества данных (темные точки) на одномерном подпространстве (прямая линия) Определение 3.10 (проекция). Пусть V — векторное пространство, а U ⊆ V — подпространство V. Линейное отображение π : V → U называется проекцией , если π2 = π ◦ π = π. 1 Термином «признак» часто обозначается представление данных.\n--- Страница 116 ---\n116 Глава 3. Аналитическая геометрия Поскольку линейные отображения можно выражать при помощи матриц пре - образований (см. раздел 2.7), предыдущее определение в равной степени при - менимо и к особому классу матриц преобразований — проекционным матри ­ цам Pπ. Такая матрица обладает свойством = Pπ. В дальнейшем мы будем выводить ортогональные проекции векторов в про- странстве внутренних произведений ( n <·, ·>) на подпространства. Начнем с одномерных подпространств, которые также называются линиями (прямыми ). Если не указано иное, мы считаем < x, y> = xTy внутренним произведением. 3.8.1. Проекция на одномерные подпространства (прямые) Допустим, нам дана прямая (одномерное подпространство), проходящая через начало координат с базисным вектором b ∈ n. Прямая — это одномерное под - пространство U ⊆ n, чьей оболочкой является b. Если спроецировать x ∈ n на U, то нужно найти вектор πU(x) ∈ U, ближайшему к x. Воспользовавшись гео - метрическими аргументами, охарактеризуем некоторые свойства проекции πU(x) (рис. 3.10( a) возьмем в качестве иллюстрации): zПроекция πU(x) является ближайшей к x, где «ближайшая» подразумевает, что расстояние || x – πU(x) || является минимальным. Следовательно, отрезок πU(x) – x от πU(x) до x ортогонален U и, следовательно, базисному вектору b от U. Условие ортогональности дает < πU(x) – x, b> = 0, поскольку углы между векторами определяются по внутреннему произведению. zzПроекция πU(x) от x на U должна быть элементом U и, следовательно, крат - ной базисному вектору b, который охватывает U. Следовательно, πU(x) = λb для некоторого λ ∈ 1. На следующих трех этапах определяем координату λ, проекцию πU(x) ∈ U, и матрицу проекции Pπ, отображающую любое на x ∈ n на U: 1. Находим координату λ. Условие ортогональности дает (3.39) Теперь можно воспользоваться билинейностью внутреннего произведения и получить (3.40) 1 В таком случае λ является координатой πU(x) относительно b.\n--- Страница 117 ---\n3.8. Ортогональные проекции 117 b bx x ω cosωsinω ωπU(x) ( а) (b) Рис. 3.10. Примеры проекций на одномерные подпространства В последнем шаге мы воспользовались тем фактом, что внутренние произ - ведения симметричны1. Если мы выберем < ·, ·> в качестве скалярного про - изведения, то получим (3.41) Если || b || = 1, то координата λ проекции будет описываться выражени - ем bTx. 2. Находим точку проекции πU(x) ∈ U. Поскольку πU(x) = λb, мы, воспользо - вавшись (3.40), сразу же получаем, что (3.42) где последнее равенство сохраняется только для скалярного произведения. Мы также можем вычислить длину πU(x) при помощи определения 3.1 как (3.43) Следовательно, наша проекция имеет длину | λ |, умноженного на длину b. Это также интуитивно подсказывает, что λ является координатой πU(x) от- носительно базисного вектора b, охватывающего одномерное подпростран - ство U. 1 С общим внутренним произведением получаем λ = <x, b>, если || b || = 1.\n--- Страница 118 ---\n118 Глава 3. Аналитическая геометрия Если мы воспользуемся скалярным произведением как внутренним, то по- лучим (3.44) Здесь ω — это угол между x и b. Это уравнение должно быть знакомо вам из тригонометрии: если || x || = 1, то x располагается на единичной окружности. Отсюда следует, что проекция на горизонтальную ось1, охватываемую b, со- ставляет ровно cos ω, а длина соответствующего вектора πU(x) = | cos ω |. Это проиллюстрировано на рис. 3.10( b). 3. Нахождение проекционной матрицы Pπ. Мы знаем, что проекция является линейным отображением (см. определение 3.10). Следовательно, существу - ет проекционная матрица Pπ, такая что πU(x) = Pπx. При использовании скалярного произведения в качестве внутреннего и (3.45) мы сразу видим, что (3.46) Отметим, что bbT (и, следовательно, Pπ) — это симметричная матрица2 (с ран- гом 1), а || b ||2 = <b, b> — это скаляр. Проекционная матрица Pπ проецирует любой вектор x ∈ n на линию, проходя - щую через начало координат в направлении b (или, что эквивалентно, подпро - странство U охватывается b). ПРИМЕЧАНИЕ Проекция πU(x) ∈ n все равно является n-мерным вектором, а не скаляром. Однако координаты n нам больше не требуются для представле - ния проекции, достаточно одной координаты, если мы хотим выразить ее от - носительно базисного вектора b, являющегося оболочкой подпространства U : λ.  1 Горизонтальная ось является одномерным подпространством. 2 Проекционные матрицы всегда симметричны.\n--- Страница 119 ---\n3.8. Ортогональные проекции 119 Пример 3.10 (проекция на прямую) Найдем проекционную матрицу Pπ на прямую, проходящую через на - чало координат, с оболочкой b = [1 2 2]T. b — это направление и базис одномерного подпространства (линии, проходящей через начало коор - динат). Опираясь на (3.46), получаем (3.47) Выберем конкретный x и посмотрим, находится ли он в подпространстве, охватываемом b. Для x = [1 1 1]T проекция равна (3.48) Обратите внимание: применение Pπ к πU(x) ничего не меняет, то есть PππU(x) = πU(x). Это ожидаемо, поскольку, согласно определению (3.10), мы знаем, что проекционная матрица Pπ удовлетворяет = Pπx для всех x. ПРИМЕЧАНИЕ По результатам главы 4 мы сможем показать, что πU(x) яв- ляется собственным вектором Pπ, а соответствующее собственное значение равно 1.  3.8.2. Проекция на общие подпространства В дальнейшем рассмотрим ортогональные проекции векторов x ∈ n на про - странства меньшей размерности U ⊆ n с dim(U) = m ≥ 1. Это проиллюстриро - вано на рис. 3.11. Предположим, что b1 … bm — упорядоченный базис U. Любая проекция πU(x) на U обязательно является элементом U. Следовательно, они могут быть пред - ставлены как линейные комбинации базисных векторов b1 …, bm от U, такие что πU(x) = .\n--- Страница 120 ---\n120 Глава 3. Аналитическая геометрия 0 b1b2U πU(x)x – πU(x)x Рис. 3.11. Проекция на двумерное подпространство U с базисом b1, b2. Проекция πU(x) от x ∈ 3 на U может быть выражена как линейная комбинация b1, b2, и вектор смещения x – πU(x) ортогонален как b1, так и b2 Как и в одномерном случае, мы выполняем трехэтапную процедуру, чтобы най - ти проекцию πU(x) и проекционную матрицу Pπ. 1. Находим координаты λ1, , λm проекции (относительно базиса U), такие что линейная комбинация (3.49) (3.50) является ближайшей к x ∈ n. Как и в одномерном случае, «ближайшая» означает «минимальное расстояние», что подразумевает следующее: вектор, соединяющий πU(x) ∈ U и x ∈ n, должен быть ортогонален всем базисным векторам U. Следовательно, мы получаем m одновременных условий (беря скалярное произведение в качестве внутреннего). (3.51) , (3.52) что, при πU(x) = Bλ, можно записать как (3.53) , (3.54)\n--- Страница 121 ---\n3.8. Ортогональные проекции 121 так что мы получим однородную систему линейных уравнений (3.55) (3.56) Последнее выражение называется нормальным уравнением . Поскольку b1, , bm являются базисом U и, следовательно, они линейно независимы, BTB ∈ m×m регулярна и может быть обращена. Таким образом, мы можем решить ее с коэффициентами/координатами (3.57) Матрица ( BTB)–1 BT также называется псевдообратной B, которую можно вычислять для неквадратных матриц B. При этом требуется лишь, чтобы BTB была положительно определенной, а ситуация такова, если у B полный ранг. При применении на практике (например, с линейной регрессией) в BTB часто добавляется «поправочный член» ∈ I, обеспечивающий повышенную чис - ленную устойчивость и положительную определенность. Этот «край» мож - но тщательно вычислить при помощи байесовского инференса. Подробнее об этом рассказано в главе 9. 2. Находим проекцию πU(x) ∈ U. Мы уже установили, что πU(x) = Bλ. Следова - тельно, при (3.57) (3.58) 3. Находим проекционную матрицу Pπ. Из (3.58) видно, что проекционная матрица, решающая Pπ = πU (x), должна быть (3.59) ПРИМЕЧАНИЕ Решение для проецирования на общие подпространства включает одномерный случай как специальный: если dim(U) = 1, то BTB ∈  это скаляр, и можно переписать проекционную матрицу Pπ = B(BTB)–1BT как , которая как раз является проекционной матрицей, приведенной в (3.46). \n--- Страница 122 ---\n122 Глава 3. Аналитическая геометрия Пример 3.11 (проекция на двумерное подпространство) Для подпространства и найдем координаты λ от x в терминах подпространства U, проекционной точки πU(x) и проекционной матрицы Pπ. Сначала обратим внимание, что порождающее множество U является базисом (линейная независимость) и запишем базисные векторы U в мат- рицу . Далее вычислим матрицу BTB и вектор BTx как (3.60) Затем решим нормальное уравнение BTBλ = BTx, чтобы найти λ: (3.61) В-четвертых, проекцию πU(x) от x на U, то есть на пространство столб - цов B, можно напрямую вычислить при помощи (3.62) Соответствующая проекционная ошибка1 — это норма разностного векто - ра между исходным вектором и его проекцией на U, то есть (3.63) В-пятых, проекционная матрица (для любого x ∈ 3) дается как (3.64) 1 Проекционную ошибку также называют ошибкой восстановления.\n--- Страница 123 ---\n3.8. Ортогональные проекции 123 Чтобы проверить результаты, можно ( a) проверить, ортогонален ли век - тор смещения πU(x) – x всем базисным векторам U и (b) убедиться, что Pπ = (определение 3.10). ПРИМЕЧАНИЕ Проекции πU (x) все равно являются векторами в n, хотя и находятся в m-мерном пространстве U ⊆ n. Правда, чтобы представить спро - ецированный вектор, нам нужны только m-координаты λ1 …, λm относительно базисных векторов b1 …, bm от U.  ПРИМЕЧАНИЕ В векторных пространствах с общими внутренними произ - ведениями требуется внимательно действовать при вычислении углов и рас- стояний, которые определяются при помощи внутреннего произведения.  Проекции позволяют рассматривать такие ситуации, в которых у нас есть си - стема Ax = b, не имеющая решения1. Как вы помните, это означает, что b не входит в оболочку A, то есть вектор b не относится к подпространству, охваты - ваемому столбцами A. Учитывая, что линейное уравнение не может быть реше - но точно, можно найти для него приближенное решение . Идея в том, чтобы найти в подпространстве, охватываемом столбцами A, такой вектор, который будет ближайшим к b, то есть вычислить ортогональную проекцию b на подпро - странство, охватываемое столбцами A. Такая проблема часто возникает на практике и решается методом наименьших квадратов (здесь предполагается, что скалярное произведение является внутренним) переопределенной системы уравнений. Эта проблема подробнее рассматривается в разделе 9.4. Использо - вание ошибок реконструкции (3.63) — один из способов вывода анализа главных компонент (раздел 10.3). ПРИМЕЧАНИЕ Мы только что рассмотрели проекции вектора x на подпро - странство U с базисными векторами { b1 … bk}. Если базис является ОНБ (орто - нормированным), то есть (3.33) и (3.34) удовлетворяются, то проекционное уравнение значительно упрощается до , (3.65) поскольку BTB = I, с координатами λ = BTx. (3.66) Таким образом, нам больше не приходится рассчитывать обратную от (3.58), и мы экономим время на вычисления.  1 Для систем линейных уравнений, не имеющих решения, при помощи проекций мож - но находить приблизительные решения.\n--- Страница 124 ---\n124 Глава 3. Аналитическая геометрия 3.8.3. Ортогонализация Грама — Шмидта Проекции лежат в основе метода Грама — Шмидта, позволяющего конструктив - но преобразовать любой базис ( b1 … bn) n-мерного векторного пространства V в ортогональный/ортонормированный базис ( u1 … un) от V. Такой базис всегда существует (Liesen and Mehrmann, 2015), и span[ b1 … bn] = span[ u1 … un]. Метод ортогонализации Грама — Шмидта итеративно собирает ортогональный базис (u1, …, un) из любого базиса ( b1, …, bn) от V следующим образом: u1 := b1; (3.67) uk := bk − πspan[u1, , uk–1](bk), k = 2, , n. (3.68) В (3.68) k-й базисный вектор bk проецируется на подпространство, охватывае - мое первыми k – 1 построенными ортогональными векторами u1 … uk–1 (раз- дел 3.8.2.) Эта проекция затем вычитается из bk и дает вектор uk, ортогональный (k – 1)-мерному подпространству, охватываемому u1, …, uk–1. Если повторить эту процедуру для всех n базисных векторов b1, …, bn, то получается ортогональ - ный базис ( u1, …, un) от V. Если нормировать uk, то получим ОНБ, где || uk || = 1 для k = 1, , n. 0 0 0 (c) Ортогональные базисные векторы u1 и u2 = b2 – πspan[u1](b2) (a) Исходныенеортогональные векторы b 1, b2(b) Первый новый базисный вектор u1 = b1 и проекция b2 на подпространство, охватываемое u1πspan[u1](b2) πspan[u1](b2)b2b2b2 b1u1u1u2 Рис. 3.12. Ортогонализация Грама — Шмидта. ( a) Неортогональный базис ( b1, b2) от 2. (b) Первый построенный базисный вектор u1 и ортогональная проекция b2 на span[ u1]. (c) Ортогональный базис ( u1, u2) от 2 Пример 3.12 (ортогонализация Грама — Шмидта) Рассмотрим базис ( b1, b2) от 2, где (3.69);\n--- Страница 125 ---\n3.8. Ортогональные проекции 125 см. также рис. 3.12( a). Воспользовавшись методом Грама — Шмидта, мы строим ортогональный базис ( u1, u2) от 2 следующим образом (считая, что скалярное произведение эквивалентно внутреннему про - изведению): (3.70), (3.71) Эти шаги проиллюстрированы на рис. 3.12( b) и (c). Мы сразу видим, что u1 и u2 ортогональны, то есть что = 0. 3.8.4. Проекция на аффинные подпространства До сих пор мы обсуждали, как спроецировать вектор на пространство с меньшим количеством измерений U. В дальнейшем будет показано, как спроецировать вектор на аффинное подпространство. Рассмотрим ситуацию, показанную на рис. 3.13( a). Имеем аффинное подпро - странство L = x0 + U, где b1, b2 — это базисные векторы U. Чтобы определить ортогональную проекцию πL(x) от x на L, переформулируем задачу в ту форму, в которой умеем ее решать: как проекцию на векторное подпространство. Чтобы прийти к этому, вычтем опорную точку x0 из x и из L, так чтобы L – x0 = U в точ- ности равнялось векторному подпространству U. Теперь мы можем воспользо - ваться ортогональными проекциями на подпространство, которые обсуждались в разделе 3.8.2, и получить проекцию πU (x – x0), показанную на рис. 3.13( b). Теперь эту проекцию можно транслировать обратно в L, сложив ее с x0, чтобы получить ортогональную проекцию на аффинное пространство L как πL(x) = x0 + πU(x − x0), (3.72) где πU(·) — ортогональная проекция на подпространство U, то есть пространство направлений от L; см. рис. 3.13( с). Из рис. 3.13 также очевидно, что расстояние x от аффинного пространства L идентично расстоянию x – x0 от U, то есть d (x, L) = || x − πL(x) || = || x − (x0 + πU (x − x0)) || (3.73 a) = d(x − x0, πU (x − x0)). (3.73 b)\n--- Страница 126 ---\n126 Глава 3. Аналитическая геометрия В разделе 12.1 мы воспользуемся проекциями на аффинное подпространство для вывода концепции разделяющей гиперплоскости. /g83U(x /dollar.g00DC x0)b2 U = L /dollar.g00DC x0 0 b1x /dollar.g00DC x0 L 0b2 b1x0x L /g83L(x) b2 0 b1x0x (c) /b.g007B/percent.g00BAƒ/quotedbl.g006D/exclam.g00AF/equal.g00C8/question.g009D/yright…/comma.g00D2/yright /percent.g00BA/C.g00B9/percent.g00BA/exclam.g00AF…/percent.g00BA /L.g00AE /two.g0088/percent.g00BA/.notdef.g0107/asterisk.g007D/comma.g00D2 /.notdef.g0105/.notdef.g00E3/space.g00AB /C.g00B9/percent.g00BA/.notdef.g00E3/three.g0082/.notdef.g0107/yright…/comma.g00D2/space.g00AB /equal.g00C8/hyphen.g00C1/hyphen.g00C1/comma.g00D2……/percent.g00BA/L.g00AE /C.g00B9/exclam.g00AF/percent.g00BA/yright/asterisk.g007D/.notdef.g0106/comma.g00D2/comma.g00D2 /g83L(a) /h.g006A“/period.g00B2/percent.g00BA/.notdef.g0105…/percent.g00BA/yright /C.g00B9/percent.g00BA/.notdef.g00E3/percent.g00BA›/yright…/comma.g00D2/yright (b) /q.g0076/quotedbl.g006D/yright/.notdef.g0105/yright…/comma.g00D2/yright ƒ/equal.g00C8/.notdef.g0105/equal.g00C8/.notdef.g0107/comma.g00D2 /asterisk.g007D /C.g00B9/exclam.g00AF/percent.g00BA/yright/asterisk.g007D/.notdef.g0106/comma.g00D2/comma.g00D2 /g83U …/equal.g00C8 /quotedbl.g006D/yright/asterisk.g007D/two.g0088/percent.g00BA/exclam.g00AF…/percent.g00BA/yright /C.g00B9/percent.g00BA/.notdef.g0105/C.g00B9/exclam.g00AF/percent.g00BA“/two.g0088/exclam.g00AF/equal.g00C8…“/two.g0088/quotedbl.g006D/percent.g00BA Рис. 3.13. Проекция на аффинное пространство. ( a) Исходное положение. (b) Положение, сдвинутое на – x0, так чтобы x – x0 можно было спроецировать на пространство направлений U. (c) Проекция транслируется обратно на x0 + πU (x – x0), что дает нам окончательную ортогональную проекцию πL(x) 3.9. ПОВОРОТЫ Сохранение длин и углов, о чем мы говорили в разделе 3.4, — это две характе - ристики линейных отображений при работе с ортогональными матрицами пре - образований. Далее мы подробнее рассмотрим конкретные ортогональные ма - трицы преобразований, описывающие повороты. Поворот — это линейное отображение (точнее, автоморфизм евклидова вектор - ного пространства), поворачивающий плоскость на угол θ относительно начала координат. То есть начало координат — это фиксированная точка. По общепри - нятому соглашению, для положительного угла с θ > 0 поворот производится против часовой стрелки. На рис. 3.14 показан пример, чья матрица преобразо - вания (3.74) К важным областям, в которых применяются повороты, относятся, в частности, компьютерная графика и робототехника. Например, в робототехнике часто\n--- Страница 127 ---\n3.9. Повороты 127 требуется знать, как поворачивать сочленения манипулятора, чтобы робот мог правильно подхватить и поставить предмет; см. рис. 3.15. Исходная Повернутая на 112,5° Рис. 3.14. При повороте объект в плоскости поворачивается относительно начала координат. Если угол поворота положительный, то поворот осуществляется против часовой стрелки Рис. 3.15. Манипулятору робота необходимо поворачивать сочленения, чтобы подхватывать предметы или правильно их расставлять. Рисунок взят из работы Deisenroth et al. (2015) 3.9.1. Повороты в 2 Рассмотрим стандартный базис от 2, определяющий стан - дартную систему координат в 2. Мы собираемся повернуть эту систему коор - динат на угол θ, как показано на рис. 3.16. Обратите внимание: после поворота векторы остались линейно независимыми и поэтому являются базисом 2. Таким образом, при повороте выполняется изменение базиса.\n--- Страница 128 ---\n128 Глава 3. Аналитическая геометрия θθΦ(e2) = [–sin θ, cos θ]T Φ(e1) = [cos θ, sin θ]T –sin θsin θ e1e2 cos θcos θ Рис. 3.16. Поворот стандартного базиса в 2 на угол θ Повороты Φ являются линейными отображениями, поэтому их можно выразить при помощи матрицы поворота R(θ). Тригонометрия (см. рис. 3.16) позволяет определить координаты осей после поворота (образ Φ) относительно стандарт - ного базиса в 2. Получаем (3.75) Соответственно, матрица поворота, осуществляющая изменение базиса в коор- динаты после поворота R(θ), дается как (3.76) 3.9.2. Повороты в 3 В отличие от случая с 2, в 3 можно поворачивать любую двумерную плоскость по одномерной оси. Чтобы указать общую матрицу поворота, проще всего опи - сать, как предполагается повернуть образы стандартного базиса ( e1, e2, e3), а за- тем убедиться, что эти образы Re1, Re2, Re3 ортонормальны друг другу. Можно получить общую матрицу поворота R, скомбинировав образы стандартного базиса. Чтобы получить понятный угол поворота, нужно определить, что такое «против часовой стрелки» при работе более чем в двух измерениях. Принято считать, что поворот «против часовой стрелки» (в плоскости) относится к повороту по оси «лицом вперед, от кончика к началу координат». Следовательно, в 3 воз- можны три варианта поворота (в плоскости) по трем стандартным базисным векторам (рис. 3.17).\n--- Страница 129 ---\n3.9. Повороты 129 θe1e2e3 Рис. 3.17. Поворот вектора (серого) в 3 на угол θ по оси e3. Повернутый вектор показан светлой стрелкой zПоворот по оси e1. (3.77) Здесь координата e1 является фиксированной, а поворот против часовой стрелки выполняется в плоскости e2, e3. zПоворот по оси e2. (3.78) Если повернуть плоскость e1, e3 по оси e2, то мы смотрим на ось e2 с ее «кон - чика» по направлению к началу координат. zПоворот по оси e3. (3.79) Это проиллюстрировано на рис. 3.17. 3.9.3. Поворот в n измерениях Обобщение поворотов от 2D- и 3D- до n-мерных евклидовых векторных про - странств можно интуитивно описать как фиксацию n – 2 измерений и огра-\n--- Страница 130 ---\n130 Глава 3. Аналитическая геометрия ничение вращения двумерной плоскостью в n-мерном пространстве. Как и в трехмерном случае, можно вращать любую плоскость (двумерное подпро - странство n). Определение 3.11 (поворот Гивенса). Пусть V — это n-мерное евклидово век - торное пространство, а Φ : V → V это автоморфизм с матрицей преобразования (3.80) для 1 ≤ i < j ≤ n и θ ∈ . Тогда Ri,j(θ) называется поворотом Гивенса . В сущности, Ri,j(θ) — это единичная матрица In с (3.81) В двух измерениях (то есть n = 2) мы получаем (3.76) в качестве частного случая. 3.9.4. Свойства поворотов У поворотов обнаруживается ряд полезных свойств, которые можно вывести, если трактовать повороты как ортогональные матрицы (определение 3.8): zПри поворотах сохраняются расстояния, например || x − y || = || Rθ(x) – Rθ(y) ||. Иными словами, после такого преобразования, как поворот, расстояния между любыми двумя точками остаются неизменными. zПри поворотах сохраняются углы, то есть угол между Rθx и Rθy равен углу между x и y. zПовороты в трех (или более) измерениях обычно некоммутативны. Следо - вательно, важен порядок, в котором применяются повороты, даже если это повороты вокруг одной и той же точки. Коммутативны только повороты векторов в двух измерениях, такие что R(ϕ)R(θ) = R(θ) R(ϕ) для всех ϕ, θ ∈ [0, 2π). Они образуют абелеву группу (при умножении), только если по - ворачиваются вокруг одной и той же точки (например, начала координат). 3.10. ДОПОЛНИТЕЛЬНОЕ ЧТЕНИЕ В этой главе был дан краткий обзор некоторых важнейших концепций анали - тической геометрии, которыми мы воспользуемся в следующих главах книги. Для более широкого и углубленного обзора некоторых из представленных нами\n--- Страница 131 ---\nУпражнения 131 концепций отсылаем вас к следующим отличным книгам: Axler (2015) и Boyd and Vandenberghe (2018). Внутренние произведения позволяют определять конкретные базисы векторных (под)пространств, где каждый вектор ортогонален всем остальным (ортогональ - ные базисы), используя при этом метод Грама — Шмидта. Эти базисы важны при оптимизации и в числовых алгоритмах для решения систем линейных уравнений. Например, методы подпространства Крылова, такие как сопряжен - ные градиенты или обобщенный метод минимальных невязок (GMRES), ми - нимизируют остаточные ошибки, ортогональные друг другу (Stoer and Burlirsch, 2002). В машинном обучении внутренние произведения важны в контексте методов ядра (Sch ölkopf and Smola, 2002). Методы ядра опираются на тот факт, что мно - гие линейные алгоритмы можно выразить чисто на уровне вычислений вну - тренних произведений. Затем «прием с ядром» позволяет неявно вычислить эти внутренние произведения в (потенциально бесконечномерном) признаковом пространстве, даже не обладая явной информацией об этом признаковом про - странстве. Так обеспечивается «нелинеаризация» многих алгоритмов, приме - няемых в машинном обучении, в частности ядерного анализа главных компонент (Sch ölkopf et al., 1997) для снижения размерности. Гауссовы процессы (Rasmussen and Williams, 2006) также попадают в категорию методов ядра, являющихся стандартом качества в современной вероятностной регрессии (подгонка кривых под точки данных). Идея ядер более подробно исследована в главе 12. Проекции часто используются в компьютерной графике, например для генера - ции теней. При оптимизации ортогональные проекции часто используются для (итеративной) минимизации остаточных ошибок. Это также находит примене - ние в машинном обучении, например при линейной регрессии, когда требуется найти (линейную) функцию, минимизирующую остаточные ошибки, то есть длины ортогональных проекций данных на линейную функцию (Bishop, 2006). Эта тема будет подробнее рассмотрена в главе 9. При анализе главных компонент (Pearson, 1901; Hotelling, 1933) также используются проекции для сокращения размерности данных с исходно высокой размерностью. Об этом мы подробнее поговорим в главе 10. УПРАЖНЕНИЯ 3.1. Покажите, что определено для всех x = [x1, x2]T ∈ 2 и y = [y1, y2]T ∈ 2 на : = x1y2 – (x1y2 + x2y1) + 2( x2y2) и является скалярным произведением.\n--- Страница 132 ---\n132 Глава 3. Аналитическая геометрия 3.2. Рассмотрим 2 с , определенным для всех x и y в 2 как Является ли скалярным произведением? 3.3. Вычислите расстояние между , используя a. <x, y> := xTy. b. <x, y> := xTAy, . 3.4. Вычислите угол между , используя a. <x, y> := xTy. b. <x, y> := xTBy, . 3.5. Рассмотрим евклидово векторное пространство 5 со скалярным произ - ведением. Подпространство U ⊆ 5 и x ∈ 5 задаются формулами . a. Определите ортогональную проекцию πU(x) точки x на U. b. Определите расстояние d(x, U).\n--- Страница 133 ---\nУпражнения 133 3.6. Рассмотрим 3 с внутренним произведением <x, y> := xT y. Помимо всего, мы определим e1, e2, e3 как стандартный/канонический базис в 3. a. Определите ортогональную проекцию πU(e2) точки e2 на U = span[ e1, e3]. Подсказка: ортогональность определяется скалярным произведением. b. Вычислите расстояние d(e2, U). c. Изобразите сценарий: стандартные базисные векторы и πU (e2). 3.7. Пусть V — векторное пространство π, эндоморфизм V. a. Докажите, что π является проекцией тогда и только тогда, когда idV – π является проекцией, где idV — тождественный эндоморфизм на V. b. Предположим теперь, что π является проекцией. Вычислите Im (idV – π) и ker (idV – π) как функцию от Im( π) и ker( π). 3.8. Используя метод Грама — Шмидта, превратите базис B = (b1, b2) двумерно - го подпространства U ⊆ 3 в ОНБ C = (c1, c2) пространства U, где: 3.9. Пусть n ∈ * и пусть x1, , xn > 0 будут n положительным действительными числами, такими что x1 + ··· + xn = 1. Воспользуйтесь неравенством Коши — Шварца и покажите, что: a. . b. . Подсказка: подумайте о скалярном произведении на n. Затем выберите конкретные векторы x, y ∈ n и примените неравенство Коши — Шварца. 3.10. Поверните векторы на 30 ° .\n--- Страница 134 ---\n4 Матричные разложения В главах 2 и 3 мы изучили способы работы с векторами и измерения векторов, проекции векторов и линейные отображения. Отображения и преобразования векторов можно удобно описать как операции, выполняемые с помощью матриц. Более того, данные часто также представлены в матричной форме, например где строки матрицы представляют разных людей, а столбцы описывают различные характеристики людей, такие как вес, рост и социально-экономический статус. В этой главе мы рассмотрим три аспекта работы с матрицами: как суммировать матрицы, как раскладывать матрицы и как эти разложения можно использовать для аппроксимации матриц. Сначала рассмотрим методы, которые позволяют описывать матрицы всего не - сколькими числами, характеризующими общие свойства матриц. Мы сделаем это в разделах, посвященных детерминантам (раздел 4.1) и собственным значе - ниям (раздел 4.2), для важного частного случая квадратных матриц. Эти харак - теристические числа имеют важные математические последствия и позволяют быстро понять, какими полезными свойствами обладает матрица. Отсюда мы перейдем к методам разложения матриц: аналогией разложения матриц явля - ется разложение чисел, например разложение 21 на простые числа 7 · 3. По этой причине разложение матриц также часто называют факторизацией матриц . Разложения матриц используются для описания матрицы посредством другого представления с использованием множителей интерпретируемых матриц. Сначала мы рассмотрим операцию извлечения квадратного корня для симме - тричных положительно определенных матриц — разложение Холецкого (раз - дел 4.3). Отсюда мы рассмотрим два связанных метода факторизации матриц в канонические формы. Первый известен как матричная диагонализация (раз - дел 4.4), которая позволяет нам представить линейное отображение с помощью матрицы диагонального преобразования, если мы выберем подходящий базис.\n--- Страница 135 ---\n135 Матричные разложения Второй метод, сингулярное разложение (раздел 4.5), расширяет эту факториза - цию на неквадратные матрицы и считается одним из фундаментальных понятий в линейной алгебре. Эти разложения полезны, поскольку матрицы, представ - ляющие числовые данные, часто очень большие и их трудно анализировать. Мы завершим главу систематическим обзором типов матриц и характерных свойств, которые их различают, в форме матричной таксономии (раздел 4.7). Методы, которые мы рассмотрим в этой главе, станут важными как в последу - ющих математических главах, таких как глава 6, так и в прикладных главах, таких как уменьшение размерности в главе 10 или оценка плотности в главе 11. Общая структура этой главы изображена в диаграмме связей на рис. 4.1. /uni0414/uni0435/uni0442/uni0435/uni0440/uni043C/uni0438/uni043D/uni0430/uni043D/uni0442 /uni041E/uni0431/uni0440/uni0430/uni0442/uni0438/uni043C/uni043E/uni0441/uni0442/uni044C/uni0420/uni0430/uni0437/uni043B/uni043E/uni0436/uni0435/uni043D/uni0438/uni0435 /uni0425/uni043E/uni043B/uni0435/uni0446/uni043A/uni043E/uni0433/uni043E /uni0421/uni043E/uni0431/uni0441/uni0442/uni0432/uni0435/uni043D/uni043D/uni044B/uni0435 /uni0437/uni043D/uni0430/uni0447/uni0435/uni043D/uni0438/uni044F /uni0421/uni043E/uni0431/uni0441/uni0442/uni0432/uni0435/uni043D/uni043D/uni044B/uni0435 /uni0432/uni0435/uni043A/uni0442/uni043E/uni0440/uni044B/uni041E/uni0440/uni0442/uni043E/uni0433/uni043E/uni043D/uni0430/uni043B/uni044C/uni043D/uni0430/uni044F /uni043C/uni0430/uni0442/uni0440/uni0438/uni0446/uni0430 /uni0414/uni0438/uni0430/uni0433/uni043E/uni043D/uni0430/uni043B/uni0438/uni0437/uni0430/uni0446/uni0438/uni044F SVD/uni0413/uni043B/uni0430/uni0432/uni0430 6. /uni0412/uni0435/uni0440/uni043E/uni044F/uni0442/uni043D/uni043E/uni0441/uni0442/uni044C /uni0438 /uni0440/uni0430/uni0441/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni0438/uni044F /uni0413/uni043B/uni0430/uni0432/uni0430 10. /uni0421/uni043D/uni0438/uni0436/uni0435/uni043D/uni0438/uni0435 /uni0440/uni0430/uni0437/uni043C/uni0435/uni0440/uni043D/uni043E/uni0441/uni0442/uni0438/uni043F/uni0440/uni043E/uni0432/uni0435/uni0440/uni044F/uni0435/uni0442 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432/uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432 /uni043E/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni044F/uni0435/uni0442 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432/uni0444/uni043E/uni0440/uni043C/uni0438/uni0440/uni0443/uni0435/uni0442 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432/uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432 Рис. 4.1. Ассоциативная карта концепций, представленных в этой главе, а также их использования в других частях книги\n--- Страница 136 ---\n136",
      "debug": {
        "start_page": 102,
        "end_page": 136
      }
    },
    {
      "name": "Глава 4. Матричные разложения 134",
      "content": "--- Страница 136 --- (продолжение)\nГлава 4. Матричные разложения 4.1. ДЕТЕРМИНАНТ И СЛЕД Детерминанты играют важную роль в линейной алгебре. Детерминант — это математический объект при анализе и решении систем линейных уравнений. Детерминанты определены только для квадратных матриц A ∈ n × n, то есть матриц с одинаковым числом строк и столбцов. В этой книге мы пишем детер - минант как det( A) или иногда как | A |1, так что (4.1) Детерминант квадратной матрицы A ∈ n × n это функция, которая отображает A на действительное число. Прежде чем дать определение детерминанта для общих матриц размера n × n, давайте рассмотрим несколько примеров и определим детерминанты для некоторых специальных матриц. Пример 4.1 (проверка матрицы не обратимость) Начнем с исследования, обратима ли квадратная матрица A (раздел 2.2.2). В самых узких случаях мы уже знаем, когда матрица обратима. Если A — матрица 1 × 1, то есть это скалярное число, то A = a ⇒ A–1 = . Таким образом, a = 1 выполняется тогда и только тогда, когда a ≠ 0. Для матриц 2 × 2 по определению обращения (определение 2.3) мы знаем, что AA–1 = I. Тогда с учетом (2.24) обратная к A есть (4.2) Следовательно, A обратима тогда и только тогда, когда a11a22 – a12a21 ≠ 0. (4.3) Эта величина является детерминантом A ∈ 2×2, то есть (4.4) 1 Обозначение детерминанта | A | не следует путать с абсолютным значением.\nГлава 4. Матричные разложения 4.1. ДЕТЕРМИНАНТ И СЛЕД Детерминанты играют важную роль в линейной алгебре. Детерминант — это математический объект при анализе и решении систем линейных уравнений. Детерминанты определены только для квадратных матриц A ∈ n × n, то есть матриц с одинаковым числом строк и столбцов. В этой книге мы пишем детер - минант как det( A) или иногда как | A |1, так что (4.1) Детерминант квадратной матрицы A ∈ n × n это функция, которая отображает A на действительное число. Прежде чем дать определение детерминанта для общих матриц размера n × n, давайте рассмотрим несколько примеров и определим детерминанты для некоторых специальных матриц. Пример 4.1 (проверка матрицы не обратимость) Начнем с исследования, обратима ли квадратная матрица A (раздел 2.2.2). В самых узких случаях мы уже знаем, когда матрица обратима. Если A — матрица 1 × 1, то есть это скалярное число, то A = a ⇒ A–1 = . Таким образом, a = 1 выполняется тогда и только тогда, когда a ≠ 0. Для матриц 2 × 2 по определению обращения (определение 2.3) мы знаем, что AA–1 = I. Тогда с учетом (2.24) обратная к A есть (4.2) Следовательно, A обратима тогда и только тогда, когда a11a22 – a12a21 ≠ 0. (4.3) Эта величина является детерминантом A ∈ 2×2, то есть (4.4) 1 Обозначение детерминанта | A | не следует путать с абсолютным значением.\n--- Страница 137 ---\n4.1. Детерминант и след 137 Пример 4.1 уже указывает на связь между детерминантами и существованием обратных матриц. Следующая теорема утверждает тот же результат для матриц размера n × n. Теорема 4.1. Для любой квадратной матрицы A ∈ n × n верно, что A обратима тогда и только тогда, когда det(A) ≠ 0. Существуют явные (в замкнутой форме) выражения для детерминантов малых матриц через элементы матрицы. Для n = 1 det(A) = det(a11) = a11. (4.5) Для n = 2 (4.6) что рассматривалось в предыдущем примере. Для n = 3 (известно как правило Сарруса) (4.7) Чтобы запомнить условия умножения в правиле Сарруса, попробуйте проследить за элементами тройных произведений в матрице. Квадратная матрица T называется верхнетреугольной матрицей , если Tij = 0 для i > j, то есть матрица равна нулю ниже своей диагонали. Аналогичным образом определяется нижнетреугольная матрица с нулями над ее диагональю. Для треугольной матрицы T ∈ n × n детерминант является произведением диагональ - ных элементов, то есть (4.8) Пример 4.2 (детерминанты как мера объема) Понятие детерминанта естественно, когда мы рассматриваем его как ото - бражение набора из n векторов, охватывающих объект в n. Оказывается, детерминант det( A) представляет собой ориентированный объем (объем со знаком) n-мер ного параллелепипеда, образованного столбцами матри - цы A.\n--- Страница 138 ---\n138 Глава 4. Матричные разложения При n = 2 столбцы матрицы образуют параллелограмм; см. рис. 4.2. По мере уменьшения угла между векторами уменьшается и площадь парал - лелограмма. Рассмотрим два вектора b, g, которые образуют столбцы матрицы A = [b, g]. Тогда модуль детерминанта A — это площадь парал - лелограмма с вершинами 0, b, g, b + g. В частности, если b, g линейно зависимы, так что b = λg для некоторого λ ∈ , они больше не образуют двумерный параллелограмм. Следовательно, соответствующая площадь равна 0. Напротив, если b, g линейно независимы и кратны каноническим базисным векторам e1, e2, то их можно записать как и , и детерминант . b g Рис. 4.2. Площадь параллелограмма (заштрихованная область), натянутого на векторы b и g, равна | det([b, g]) | b gr Рис. 4.3. Объем параллелепипеда (заштрихованный объем), натянутого на векторы r, b, g, равен | det([r, b, g ]) | Знак детерминанта указывает ориентацию остовных векторов b, g относи - тельно стандартного базиса ( e1, e2). На нашем рисунке изменение порядка на g, b меняет местами столбцы A и меняет ориентацию заштрихованной области. Получаем знакомую формулу: площадь = высота × длина. Это верно и для более высоких размерностей. В 3 мы рассматриваем три век - тора r, b, g ∈ 3, охватывающие ребра параллелепипеда, то есть твердое тело с гранями, которые являются параллелограммами (рис. 4.3). Абсолютное значение детерминанта матрицы 3 × 3 [r, b, g] — это объем твердого тела. Таким образом, детерминант действует как функция, которая измеряет объем со знаком, образованный векторами — столбцами матрицы. Рассмотрим три линейно независимых вектора r, g, b ∈ 3, заданных как (4.9)\n--- Страница 139 ---\n4.1. Детерминант и след 139 Запись этих векторов как столбцов матрицы (4.10) позволяет нам вычислить объем как V = | det(A)| = 186. (4.11) Вычисление детерминанта матрицы размера n × n требует общего алгоритма для решения случаев для n > 3, которые будут рассмотрены ниже. Теорема 4.2 ниже сводит проблему вычисления детерминанта матрицы размера n × n к вычислению детерминанта матриц размера ( n – 1) × (n – 1). Таким образом, рекурсивно при - меняя разложение Лапласа (теорема 4.2), можно вычислить детерминанты матриц размера n × n, в конечном итоге вычислив детерминанты матриц 2 × 2. Теорема 4.2 (разложение Лапласа). Рассмотрим матрицу A ∈ n × n. Затем, для всех j = 1, , n: 1. Разложение по столбцу j (4.12) 2. Разложение по строке j (4.13) Здесь Ak,j ∈ (n–1)×(n–1) — это подматрица матрицы A, полученная при удалении строки k и столбца j 1. Пример 4.3 (разложение Лапласа) Вычислим детерминант , (4.14) 1 det(Ak, j) называется минором, а (–1)k + j det(Ak, j) — кофактором.\n--- Страница 140 ---\n140 Глава 4. Матричные разложения используя разложение Лапласа по первой строке. Применяя (4.13), полу - чим (4.15) Воспользуемся (4.6) для вычисления детерминантов всех матриц 2 × 2 и получим det(A) = 1(1 − 0) − 2(3 − 0) + 3(0 − 0) = −5. (4.16) Для полной ясности мы можем сравнить этот результат с вычислением детерминанта по правилу Сарруса (4.7): det(A) = 1 · 1 · 1 + 3 · 0 · 3 + 0 · 2 · 2 – 0 · 1 · 3 – – 1 · 0 · 2 – 3 · 2 · 1 = 1 − 6 = −5. (4.17) Для A ∈ n × n детерминант проявляет следующие свойства: zДетерминант произведения матриц равен произведению соответствующих детерминантов, det( AB) = det(A)det( B). zДетерминанты инвариантны к транспонированию, то есть det ( A) = det ( AT). zЕсли A регулярная (обратимая), то det( A–1) = . zПодобные матрицы (определение 2.22) обладают одним и тем же детерми - нантом. Следовательно, для линейного отображения Φ : V → V все матрицы преобразования AΦ отображения Φ имеют один и тот же детерминант. Таким образом, детерминант инвариантен к выбору базиса линейного отображения. zДобавление столбца/строки к другому столбцу/строке не изменяет det( A). zУмножение столбца/строки с λ ∈  умножает det( A) на λ. В частности, det(λA) = λn det(A). zzПерестановка двух строк/столбцов меняет знак det( A). Благодаря последним трем свойствам исключение Гаусса может быть исполь - зовано (раздел 2.1) для вычисления det( A) путем приведения A к приведенному ступенчатому виду. Можно остановить метод исключения Гаусса, когда A име- ет треугольную форму, где все элементы ниже диагонали равны 0. Напомним\n--- Страница 141 ---\n4.1. Детерминант и след 141 из (4.8), что детерминант треугольной матрицы равен произведению ее диаго - нальных элементов. Теорема 4.3. Квадратная матрица A ∈ n × n имеет детерминант det(A) ≠ 0 тогда и только тогда, когда rk(A) = n. Другими словами, A обратима тогда и только тогда, когда она имеет полный ранг. Когда математические вычисления выполнялись в основном вручную, вычис - ление детерминанта считалось важным способом анализа обратимости матрицы. Однако современные подходы к машинному обучению используют прямые численные методы, которые вытеснили явное вычисление детерминанта. На - пример, в главе 2 мы узнали, что обратные матрицы могут быть вычислены методом исключения Гаусса. Таким образом, метод исключения Гаусса может использоваться для вычисления детерминанта матрицы. Детерминанты будут играть важную теоретическую роль в следующих разделах, особенно когда мы узнаем о собственных значениях и собственных векторах (раздел 4.2) через характеристический полином. Определение 4.4. След квадратной матрицы A ∈ n × n определяется как (4.18) то есть след — это сумма диагональных элементов A. След удовлетворяет следующим свойствам: ztr(A + B) = tr(A) + tr( B) для A, B ∈ n × n. ztr(αA) = αtr(A), α ∈  для A ∈ n × n. ztr(In) = n. zztr(AB) = tr(BA) для A ∈ n × k, B ∈ k × n. Можно показать, что существует единственная функция, удовлетворяющая этим четырем свойствам вместе — след (Gohberg et al., 2012). След произведения матриц обладает и более общими свойствами. В частности, след инвариантен относительно циклических перестановок, то есть tr(AKL) = tr(KLA) (4.19) для матриц A ∈ a × k, K ∈ k × l, L ∈ l × a. Это свойство распространяется на произведения произвольного числа матриц. Как частный случай (4.19) следует, что для двух векторов x, y ∈ n tr(xyT) = tr(yTx) = yTx ∈ . (4.20)\n--- Страница 142 ---\n142 Глава 4. Матричные разложения Для линейного отображения Φ : V → V, где V — векторное пространство, мы определяем след этого отображения, используя след матричного представле - ния Φ. Для данного базиса V мы можем описать Φ с помощью матрицы преоб - разования A. Тогда след матрицы Φ является следом A. Для другого базиса V верно, что соответствующая матрица преобразования B матрицы Φ может быть получена заменой базиса на S–1AS для подходящего S (см. раздел 2.7.2). Для соответствующего следа Φ это означает (4.21) Следовательно, хотя матричные представления линейных отображений зависят от базиса, след линейного отображения Φ от базиса независим. В этом разделе мы рассмотрели детерминанты и следы как функции, характеризующие ква - дратную матрицу. Объединив понятия детерминантов и следов, теперь мы можем определить важное уравнение, описывающее матрицу A в терминах многочлена (полинома), которое мы будем широко использовать в следующих разделах. Определение 4.5 (характеристический многочлен). Для λ ∈  и квадратной матрицы A ∈ n × n (4.22 a) (4.22 b) c0, , cn–1 ∈ , является характеристическим многочленом A. В частности, c0 = det(A), (4.23) cn–1 = (−1)n–1tr(A). (4.24) Характеристический полином (4.22 a) позволит нам вычислить собственные значения и собственные векторы, которые будут рассмотрены в следующем разделе. 4.2. СОБСТВЕННЫЕ ЗНАЧЕНИЯ И СОБСТВЕННЫЕ ВЕКТОРЫ Теперь мы познакомимся с новым способом характеристики матрицы и связан - ного с ней линейного отображения. Напомним из раздела 2.7.1, что каждое линейное отображение имеет уникальную матрицу преобразования с упорядо - ченным базисом. Мы можем интерпретировать линейные отображения и свя- занные с ними матрицы преобразования, выполнив «собственный» анализ. Как мы увидим, собственные значения линейного отображения расскажут нам, как\n--- Страница 143 ---\n4.2. Собственные значения и собственные векторы 14 3 специальный набор векторов, собственные векторы, преобразуется линейным отображением. Определение 4.6. Пусть A ∈ n × n квадратная матрица. Тогда λ ∈  — собствен ­ ное значение оператора A, а x ∈ n \\ {0} — соответствующий собственный вектор оператора A, если Ax = λx. (4.25) Мы называем (4.25) уравнением для собственных значений. ПРИМЕЧАНИЕ В литературе и программном обеспечении по линейной ал - гебре часто применяется соглашение, согласно которому собственные значения сортируются в порядке убывания, так что наибольшее собственное значение и связанный с ним собственный вектор называют первым собственным значе - нием и связанным с ним собственным вектором, а вторые по величине называ - ются вторым собственным значением и связанным с ним собственным вектором и т. д. Однако в учебниках и публикациях порядок упорядочения может отли - чаться или вообще отсутствовать. Мы не хотим предполагать упорядочение в этой книге, если это не указано явно.  Следующие утверждения эквивалентны: zλ — собственное значение оператора A ∈ n × n. zСуществует x ∈ n \\ {0} с Ax = λx или, что то же самое, ( A − λIn)x = 0, что решается нетривиально, то есть x ≠ 0. zrk(A – λIn) < n. zdet(A – λIn) = 0. Определение 4.7 (коллинеарность и сонаправленность). Два вектора, указы - вающие в одном направлении, называются сонаправленными . Два вектора кол­ линеарны , если они указывают в одном или противоположном направлении. ПРИМЕЧАНИЕ Если x — собственный вектор матрицы A, связанный с соб- ственным значением λ, то для любого c ∈  \\ {0} верно, что cx — собственный вектор матрицы A с тем же собственным значением, поскольку A (cx) = cAx = cλx = λ(cx). (4.26) Таким образом, все векторы, коллинеарные x, также являются собственными векторами A.  Теорема 4.8. λ ∈  является собственным значением A ∈ n × n тогда и только тогда, когда λ является корнем характеристического многочлена pA(λ) опера ­ тора A.\n--- Страница 144 ---\n144 Глава 4. Матричные разложения Определение 4.9. Пусть квадратная матрица A имеет собственное значение λi. Алгебраическая кратность λi — это количество вхождений этого корня в харак - теристический многочлен. Определение 4.10 (собственное подпространство и собственный спектр). Для A ∈ n×n множество всех собственных векторов A, связанных с собственным значением λ, охватывает подпространство в n, которое называется собственным подпространством A относительно λ и обозначается Eλ. Набор всех собственных значений оператора A называется собственным спектром или просто спектром оператора A. Если λ является собственным значением оператора A ∈ n × n, то соответствующее собственное подпространство Eλ является пространством решений однородной системы линейных уравнений ( A − λI)x = 0. Геометрически собственный вектор, соответствующий ненулевому собственному значению, указывает в направлении, которое растягивается линейным отображением. Собственное значение — это коэффициент, на который оно растягивается. Если собственное значение от - рицательное, направление растяжения меняется. Пример 4.4 (случай единичной матрицы) Единичная матрица I ∈ Rn×n имеет характеристический многочлен pI(λ) = = det(I – λI) = (1 – λI)n = 0, который имеет только одно собственное зна - чение λ = 1, встречающееся n раз. Более того, Ix = λx = 1x выполняется для всех векторов x ∈ n \\ {0}. Из-за этого единственное собственное подпространство E1 единичной матрицы охватывает n измерений, и все n стандартных базисных векторов n являются собственными векторами I. К полезным свойствам собственных значений и собственных векторов относят - ся следующие: zМатрица A и ее транспонированная матрица AT имеют одинаковые собствен - ные значения, но не обязательно одинаковые собственные векторы. zСобственное подпространство Eλ является нулевым пространством A – λI, поскольку (4.27 a) (4.27 b) zПодобные матрицы (см. определение 2.22) имеют одинаковые собственные значения. Следовательно, линейное отображение Φ имеет собственные зна - чения, которые не зависят от выбора базиса его матрицы преобразования.\n--- Страница 145 ---\n4.2. Собственные значения и собственные векторы 14 5 Это делает собственные значения, вместе с детерминантом и следом, ключе - выми характеристическими параметрами линейного отображения, посколь - ку все они инвариантны при изменении базиса. zСимметричные положительно определенные матрицы всегда имеют поло - жительные действительные собственные значения. Пример 4.5 (вычисление собственных значений, собственных векторов и собственных подпространств) Найдем собственные значения и собственные векторы матрицы 2 × 2 (4.28) Шаг 1: характеристический многочлен. Из нашего определения собствен - ного вектора x ≠ 0 и собственного значения λ матрицы A найдется такой вектор, что Ax = λx, то есть ( A – λI) x = 0. Поскольку x ≠ 0, для этого требуется, чтобы ядро (нулевое пространство) матрицы A – λI содержало больше элементов, чем просто 0. Это означает, что A – λI необратима, и, следовательно, det( A – λI) = 0. Следовательно, нам нужно вычислить корни характеристического многочлена (4.22 a), чтобы найти собственные значения. Шаг 2: собственные значения. Характеристический многочлен равен pA(λ) = det(A − λI) = (4.29 a) (4.29 b) (4.29 c) Факторизуем характеристический многочлен и получаем p (λ) = (4 − λ)(3 − λ) − 2 · 1 = 10 − 7λ + λ2 = (2 − λ)(5 − λ), (4.30) что дает корни λ1 = 2 и λ2 = 5. Шаг 3: собственные векторы и собственные подпространства. Мы на - ходим собственные векторы, соответствующие этим собственным значе - ниям, рассматривая векторы x, такие что (4.31)\n--- Страница 146 ---\n146 Глава 4. Матричные разложения При λ = 5 получаем (4.32) Решаем эту однородную систему и получаем пространство решений . (4.33) Это собственное подпространство одномерно, поскольку имеет единствен - ный базисный вектор. Аналогично находим собственный вектор при λ = 2, решая однородную систему уравнений . (4.34) Это означает, что любой вектор , где x2 = –x1, например , — собственный вектор с собственным значением 2. Соответствующее соб - ственное подпространство имеет вид (4.35) Два собственных подпространства E5 и E2 в примере 4.5 одномерны, поскольку каждое из них натянуто на один вектор. Однако в других случаях у нас может быть несколько идентичных собственных значений (см. определение 4.9), а соб- ственное подпространство может иметь более одного измерения. Определение 4.11. Пусть λi — собственное значение квадратной матрицы A. Тогда геометрическая кратность λi — это количество линейно независимых собственных векторов, связанных с λi. Другими словами, это размерность собственного подпространства, натянутого на собственные векторы, связанные с λi. ПРИМЕЧАНИЕ Геометрическая кратность конкретного собственного значе - ния должна быть не меньше единицы, потому что каждое собственное значение имеет как минимум один связанный собственный вектор. Геометрическая крат - ность собственного значения не может превышать его алгебраическую кратность, но может быть меньше. \n--- Страница 147 ---\n4.2. Собственные значения и собственные векторы 147 Пример 4.6 Матрица имеет два повторяющихся собственных значения λ1 = λ2 = 2 и алгебраическую кратность 2. Однако собственное значение имеет только один отдельный единичный собственный вектор и, следовательно, геометрическую кратность 1. 4.2.1. Графическая интуиция в двух измерениях Давайте разберемся с примерами для детерминантов, собственных векторов и собственных значений, используя различные линейные отображения. На рис. 4.4 изображены пять матриц преобразования A1, , A5 и их влияние на ква - дратную сетку точек с центром в начале координат: z . Направление двух собственных векторов соответствует канони - ческим базисным векторам в 2, то есть двум координатным осям. Верти - кальная ось растягивается в 2 раза (собственное значение λ1 = 2), а горизон - тальная ось сжимается в раза (собственное значение λ2 = ). Отображение сохраняет площадь1 (det( A1) = 1 = 2 · ). z соответствует отображению сдвига, то есть сдвигает точки вдоль горизонтальной оси вправо, если они находятся на положительной полови - не вертикальной оси, и влево, если наоборот. Это отображение сохраняет площадь (det( A2) = 1). Собственное значение λ1 = 1 = λ2 повторяется, а соб- ственные векторы коллинеарны (нарисованы здесь для выделения в двух противоположных направлениях). Это означает, что отображение действует только в одном направлении (горизонтальная ось). z . Матрица A3 вращает точки на про- тив часовой стрелки и имеет только комплексные собственные значения, отражая, что отображение является вращением (следовательно, собственные 1 В геометрии свойство сохранения площади этого типа сдвига параллельно оси также известно как принцип Кавальери (Katz, 2004).\n--- Страница 148 ---\n148 Глава 4. Матричные разложения векторы не рисуются). Вращение должно сохранять объем, поэтому детер - минант равен 1. Подробнее о поворотах см. в разделе 3.9. λ1 = 2,0 λ2 = 0,5 det(A) = 1,0 λ1 = 1,0 λ2 = 1,0 det(A) = 1,0 λ1 = 0,0 λ2 = 2,0 det(A) = 0,0 λ1 = 0,5 λ2 = 1,5 det(A) = 0,75λ1 = (0,87 – 0,5j) λ2 = (0,87 + 0,5j) det(A) = 1,0 Рис. 4.4. Детерминанты и собственные подпространства. Обзор пяти линейных отображений и связанных с ними матриц преобразования Ai ∈ 2 × 2, проецирующих 400 точек x ∈ 2 (левый столбец) на целевые точки Aix (правый столбец). В центральном столбце изображен первый собственный вектор, растянутый на соответствующее собственное значение λ1, и второй собственный вектор — на собственное значение λ2. Каждая строка показывает действие одной из пяти матриц Ai на стандартный базис\n--- Страница 149 ---\n4.2. Собственные значения и собственные векторы 149 z представляет собой отображение в стандартном базисе, которое сворачивает двумерную область в одно измерение. Поскольку одно собствен - ное значение равно 0, пространство в направлении собственного вектора, соответствующего λ1 = 0, сжимается, в то время как ортогональный собствен - ный вектор растягивает пространство в λ2 = 2 раза. Следовательно, площадь изображения равна 0. z — отображение сдвига и растяжения, сжимающее пространство на 25%, поскольку | det(A5) | = . Он растягивает пространство вдоль соб - ственного вектора λ2 в 1,5 раза и сжимает его вдоль ортогонального собствен - ного вектора в 0,5 раза. Пример 4.7 (собственный спектр биологической нейронной сети) Методы анализа и изучения сетевых данных являются важным компо - нентом методов машинного обучения. Ключ к пониманию сетей — это связь между сетевыми узлами, особенно если два узла подключены друг к другу или нет. В приложениях для обработки данных часто бывает по - лезно изучить матрицу, которая фиксирует эти данные о подключении. Мы строим матрицу связности/смежности A ∈ 277 × 277 полной нейронной сети червя C.Elegans . Каждая строка/столбец представляет один из 277 нейронов мозга этого червя. Матрица связности A имеет значение aij = 1, если нейрон i общается с нейроном j через синапс, и aij = 0 в про- тивном случае. Матрица связности не является симметричной, что озна - чает, что собственные значения не могут быть действительными. Поэтому мы вычисляем симметризованную версию матрицы связности как Asym := A + AT. Эта новая матрица Asym показана на рис. 4.5( а) и имеет не - нулевое значение aij тогда и только тогда, когда два нейрона соединены (белые пиксели), независимо от направления соединения. На рис. 4.5( b) мы показываем соответствующий собственный спектр Asym. По горизон - тальной оси отложены порядковые номера собственных значений, отсо - ртированные по убыванию. Вертикальная ось показывает соответствую - щее собственное значение. S-образная форма этого собственного спектра типична для многих биологических нейронных сетей. Поиск основных механизмов, отвечающих за это, — тема актуальных исследований в ней - робиологии.\n--- Страница 150 ---\n150 Глава 4. Матричные разложения 050100150200250 /h.g006A…/.notdef.g0105/yright/asterisk.g007D“ …/yright/L.g00AE/exclam.g00AF/percent.g00BA…/equal.g00C80 50 100 150 200 250/h.g006A…/.notdef.g0105/yright/asterisk.g007D“ …/yright/L.g00AE/exclam.g00AF/percent.g00BA…/equal.g00C8 (/equal.g00C8) /.notdef.g00E4/equal.g00C8/two.g0088/exclam.g00AF/comma.g00D2/.notdef.g0106/equal.g00C8 “/quotedbl.g006D/space.g00ABƒ…/percent.g00BA“/two.g0088/comma.g00D20 100 200 /h.g006A…/.notdef.g0105/yright/asterisk.g007D“ “/percent.g00BA/K.g00AD“/two.g0088/quotedbl.g006D/yright……/percent.g00BA/.notdef.g0104/percent.g00BA ƒ…/equal.g00C8/.notdef.g0107/yright…/comma.g00D2/space.g00AB/dollar.g00DC10/dollar.g00DC50510152025/q.g0076/percent.g00BA/K.g00AD“/two.g0088/quotedbl.g006D/yright……/percent.g00BA/yright ƒ…/equal.g00C8/.notdef.g0107/yright…/comma.g00D2/yright (/K.g00AD) “/percent.g00BA/K.g00AD“/two.g0088/quotedbl.g006D/yright……/slash.g00A9/L.g00AE “/C.g00B9/yright/asterisk.g007D/two.g0088/exclam.g00AF Рис. 4.5. Нейронная сеть Caenorhabditis elegans (Kaiser and Hilgetag, 2006): (а) симметричная матрица связности; ( b) собственный спектр Теорема 4.12. Собственные векторы x1, , xn матрицы A ∈ n × n с n различными собственными значениями λ1, , λn линейно независимы. Эта теорема утверждает, что собственные векторы матрицы с n различными собственными значениями образуют базис n. Определение 4.13. Квадратная матрица A ∈ n × n является дефектной , если она имеет менее n линейно независимых собственных векторов. Дефектная матри - ца A ∈ n × n не обязательно требует n различных собственных значений, но требует, чтобы собственные векторы составляли базис n. Из рассмотрения собственных подпространств дефектной матрицы следует, что сумма размер - ностей собственных подпространств меньше n. В частности, дефектная матрица имеет по крайней мере одно собственное значение λi с алгебраической кратностью m > 1 и геометрической кратностью меньше m. ПРИМЕЧАНИЕ Дефектная матрица не может иметь n различных собственных значений, так как разные собственные значения имеют линейно независимые собственные векторы (теорема 4.12).  Теорема 4.14. Для матрицы A ∈ m × n мы всегда можем получить симметричную положительно полуопределенную матрицу S ∈ n × n, задав S := ATA. (4.36) ПРИМЕЧАНИЕ Если rk( A) = n, то S := ATA симметрично, положительно опре - делено. \n--- Страница 151 ---\n4.2. Собственные значения и собственные векторы 151 Понимание того, почему выполняется теорема 4.14, помогает понять, как мы можем использовать симметризованные матрицы: для симметрии требуется, чтобы S = S T, и, вставив (4.36), мы получаем S = ATA = AT(AT)T = (ATA)T = S T. Более того, положительная полуопределенность (раздел 3.2.3) требует, чтобы xTSx ≥ 0, и, подставляя (4.36), получаем xTSx = xTATAx = (xTAT)(Ax) ≥ (Ax)T(Ax) ≥ 0, по- тому что скалярное произведение вычисляет сумму квадратов (которые сами по себе неотрицательны). Теорема 4.15 (спектральная теорема). Если A ∈ n × n симметрично, существу ­ ет ортонормированный базис соответствующего векторного пространства V, состоящий из собственных векторов A, и каждое собственное значение веще ­ ственно. Прямым следствием спектральной теоремы является то, что существует соб - ственное разложение симметричной матрицы A (с действительными собствен - ными значениями), и что мы можем найти ОНБ собственных векторов, так что A = PDP T, где D диагональ, а столбцы P содержат собственные векторы. Пример 4.8 Рассмотрим матрицу (4.37) Характеристический многочлен A равен pA(λ) = −(λ − 1)2(λ − 7), (4.38) так что мы получаем собственные значения λ1 = 1 и λ2 = 7, где λ1 — повто - ряющееся собственное значение. Следуя нашей стандартной процедуре вычисления собственных векторов, мы получаем собственные подпро - странства (4.39) Мы видим, что x3 ортогонален как x1, так и x2. Однако поскольку , они не ортогональны. Спектральная теорема (теорема 4.15) утверждает, что существует ортогональный базис, но тот, который у нас есть, не орто - гонален. Однако мы можем построить его. Чтобы построить такой базис,\n--- Страница 152 ---\n152 Глава 4. Матричные разложения мы используем тот факт, что x1, x2 — собственные векторы, связанные с одним и тем же собственным значением λ. Следовательно, для любых α, β ∈  выполняется A (αx1 + βx2) = Ax1α + Ax2β = λ(αx1 + βx2), (4.40) то есть любая линейная комбинация x1 и x2 также является собственным вектором A, связанным с λ. Алгоритм Грама — Шмидта (раздел 3.8.3) — это метод итеративного построения ортогонального/ортонормированно - го базиса из набора базисных векторов с использованием таких линейных комбинаций. Следовательно, даже если x1 и x2 не ортогональны, мы можем применить алгоритм Грама — Шмидта и найти собственные векторы, связанные с λ1 = 1, которые ортогональны друг другу (и x3). В нашем при - мере мы получим (4.41) которые ортогональны друг другу, ортогональны x3 и собственным век - торам матрицы A, связанным с λ1 = 1. Прежде чем мы закончим рассмотрение собственных значений и собственных векторов, полезно связать эти характеристики матрицы с понятиями детерми - нанта и следа. Теорема 4.16. Детерминант матрицы A ∈ n × n — это произведение ее собствен ­ ных значений, то есть (4.42) где λi — (возможно, повторяющиеся) собственные значения A. Теорема 4.17. След матрицы A ∈ n × n — это сумма ее собственных значений, то есть (4.43) где λi — (возможно, повторяющиеся) собственные значения A. Дадим наглядную интерпретацию этим двум теоремам. Рассмотрим матрицу A ∈ 2×2, которая обладает двумя линейно независимыми собственными векто -\n--- Страница 153 ---\n4.2. Собственные значения и собственные векторы 153 рами x1, x2. В этом примере мы предполагаем, что ( x1, x2) являются ОНБ 2, так что они ортогональны и площадь квадрата, который они охватывают, равна 1 см (рис. 4.6). Из раздела 4.1 мы знаем, что детерминант вычисляет изменение пло - щади единичного квадрата при преобразовании A. В этом примере мы можем вычислить изменение площади явно: отображение собственных векторов с по- мощью A дает нам векторы v1 = Ax1 = λ1x1 и v2 = Ax2 = λ2x2, то есть новые векторы vi являются масштабированными версиями собственных векторов xi, а коэффи - циенты масштабирования — соответствующие собственные значения λi. v1, v2 по-прежнему ортогональны, а площадь прямоугольника, который они охваты - вают, равна | λ1λ2 |. Учитывая, что x1, x2 (в нашем примере) ортонормированы, мы можем напрямую вычислить длину периметра единичного квадрата как 2(1 + 1). Отображение собственных векторов с помощью A создает прямоугольник с пе- риметром 2(| λ1 | + | λ2 |). Следовательно, сумма абсолютных значений собственных значений говорит нам, как изменяется периметр единичного квадрата под дей - ствием матрицы преобразования A. A x1x2 v1v2 Рис. 4.6. Геометрическая интерпретация собственных значений. Собственные векторы матрицы A растягиваются на соответствующие собственные значения. Площадь единичного квадрата изменится на | λ1λ2|, длина окружности изменится в 2 раза (| λ1 | + | λ2 |) Пример 4.9 (Google's PageRank — веб-страницы как собственные векторы) Google использует собственный вектор, соответствующий максимально - му собственному значению матрицы A, чтобы определить ранг страницы для поиска. Идея алгоритма PageRank, разработанного в Стэнфордском университете Ларри Пейджем и Сергеем Брином в 1996 году, заключалась в том, что важность любой веб-страницы можно приблизительно оценить по важности страниц, которые ссылаются на нее. Для этого они записы - вают все веб-сайты в виде огромного ориентированного графа, который показывает, какие страницы на какие ссылаются. PageRank вычисляет вес (важность) xi ≥ 0 веб-сайта ai путем подсчета количества страниц, указы - вающих на ai. Более того, PageRank учитывает важность веб-сайтов, ко - торые ссылаются на ai. Затем навигационное поведение пользователя\n--- Страница 154 ---\n154 Глава 4. Матричные разложения моделируется с помощью матрицы переходов A этого графика, которая сообщает нам, с какой вероятностью (кликом) кто-то окажется на другом веб-сайте. Матрица A обладает тем свойством, что для любого начально - го вектора ранга/важности x веб-сайта последовательность x, Ax, A2x, сходится к вектору x*. Этот вектор называется PageRank и удовлетворяет Ax* = x*, то есть это собственный вектор A (с соответствующим собствен - ным значением 1). После нормализации x*, такой что || x* || = 1, мы можем интерпретировать элементы как вероятности. Более подробную инфор - мацию и различные точки зрения на PageRank можно найти в исходном техническом отчете (Page et al., 1999). 4.3. РАЗЛОЖЕНИЕ ХОЛЕЦКОГО Есть много способов факторизовать специальные типы матриц, с которыми мы часто сталкиваемся в машинном обучении. В положительных действительных числах есть операция извлечения квадратного корня, которая дает разложение числа на идентичные компоненты, например 9 = 3 · 3. При работе с матрицами нужно тщательно следить за тем, чтобы операция квадратного корня вычислялась для положительных величин. Для симметричных положительно определенных матриц (см. раздел 3.2.3) мы можем выбирать из нескольких эквивалентных операций извлечения квадратного корня. Разложение Холецкого / факторизация Холецкого выполняет операцию над симметричными положительно определен - ными матрицами, эквивалентную квадратному корню. Теорема 4.18 (разложение Холецкого). Симметричная положительно опреде ­ ленная матрица A может быть разложена на множители в произведение A = LLT, где L — нижнетреугольная матрица с положительными диагональными элемен ­ тами: (4.44) L называется фактором Холецкого для A, и L единственна. Пример 4.10 (факторизация Холецкого) Рассмотрим симметричную положительно определенную матрицу A ∈ 3 × 3. Нас интересует нахождение ее факторизации Холецкого A = LLT, то есть\n--- Страница 155 ---\n4.3. Разложение Холецкого 155 (4.45) Умножение правой части дает (4.46) Сравнение левой части (4.45) и правой части (4.46) показывает, что в диагональных элементах lii имеется простая закономерность: (4.47) Аналогично для элементов ниже диагонали ( lij, где i > j) также существу - ет повторяющаяся закономерность: (4.48) Таким образом, мы построили разложение Холецкого для любой симме - тричной положительно определенной матрицы 3 × 3. Ключевая реализа - ция состоит в том, что мы можем вычислить в обратном порядке, какими должны быть компоненты lij для L, учитывая значения aij для A и ранее вычисленные значения lij. Разложение Холецкого — важный инструмент для выполнения вычислений, лежащих в основе машинного обучения. Здесь требуется часто иметь дело с симметричными положительно определенными матрицами. Например, кова - риационная матрица многомерной гауссовой переменной (раздел 6.5) симме - трична, положительно определена. Разложение Холецкого этой ковариационной матрицы позволяет нам генерировать выборки из распределения Гаусса. Она также позволяет нам выполнять линейное преобразование случайных величин, которое активно используется при вычислении градиентов в глубоких стоха - стических моделях, таких как вариационный автокодировщик (Jimenez Rezende et al., 2014; Kingma and Welling, 2014). Разложение Холецкого также позволяет очень эффективно вычислять детерминанты. Учитывая разложение Холецкого A = LLT, мы знаем, что det( A) = det(L) det( L) = det(LT)2. Поскольку L — тре- угольная матрица, детерминант — это просто произведение ее диагональных элементов, так что . Таким образом, многие пакеты для алгебраи -\n--- Страница 156 ---\n156 Глава 4. Матричные разложения ческих вычислений используют разложение Холецкого, чтобы сделать вычис - ления более эффективными. 4.4. СОБСТВЕННОЕ РАЗЛОЖЕНИЕ И ДИАГОНАЛИЗАЦИЯ Диагональная матрица — это матрица, все недиагональные элементы которой равны нулю. То есть она имеет вид (4.49) Такая форма матриц позволяет быстро вычислять детерминанты, степени и обратные величины. Детерминант — это произведение диагональных эле - ментов матрицы, степень матрицы Dk задается каждым диагональным элемен - том, возведенным в степень k, а обратная величина D–1 — это величина, об - ратная произведению диагональных элементов матрицы, если все они не равны нулю. В этом разделе мы обсудим, как преобразовать матрицы в диагональную форму. Это важное применение изменения базиса, которое мы обсуждали в разделе 2.7.2, и собственных значений из раздела 4.2. Напомним, что две матрицы A, D подобны (определение 2.22), если существует обратимая матрица P, такая что D = P –1AP. Более конкретно, мы рассмотрим матрицы A, которые похожи на диагональные матрицы D, которые содержат собственные значения матрицы A на диагонали. Определение 4.19 (диагонализуемость). Матрица A ∈ n × n диагонализируема, если она подобна диагональной матрице, то есть если существует обратимая матрица P ∈ n × n, такая что D = P –1AP. Далее мы увидим, что диагонализация матрицы A ∈ n × n — это способ выразить то же линейное отображение, но в другом базисе (раздел 2.6.1), который ока - жется базисом, состоящим из собственных векторов группы A. Пусть A ∈ n × n, пусть λ1, , λn — набор скаляров, и пусть p1, , pn — множество векторов в n. Определим P := [p1, , pn], и пусть D ∈ n×n — диагональная матрица с диагональ - ными элементами λ1, , λn. Тогда мы можем показать, что AP = PD (4.50) тогда и только тогда, когда λ1, , λn — собственные значения A и p1, , pn — соот - ветствующие собственные векторы матрицы A.\n--- Страница 157 ---\n4.4. Собственное разложение и диагонализация 157 Мы видим, что это утверждение верно, потому что AP = A[p1, , pn] = [Ap1, , Apn], (4.51) (4.52) Таким образом, из (4.50) следует, что (4.53) (4.54) Следовательно, столбцы матрицы P должны быть собственными векторами матрицы A. Наше определение диагонализации требует, чтобы P ∈ n × n был обратимым, то есть P имел полный ранг (теорема 4.3). Для этого необходимо иметь n линей - но независимых собственных векторов p1, , pn, то есть pi образуют базис n. Теорема 4.20 (собственное разложение). Квадратную матрицу A ∈ n × n мож­ но разложить на A = PDP –1, (4.55) где P ∈ n × n, а D — диагональная матрица, диагональные элементы которой являются собственными значениями матрицы A, тогда и только тогда, когда собственные векторы матрицы A образуют базис n. Из теоремы 4.20 следует, что только недефектные матрицы могут быть диаго - нализованы и что столбцы P — это n собственных векторов A. Для симметричных матриц мы можем получить еще более интересные результаты при разложении по собственным значениям. Теорема 4.21. Симметричную матрицу S ∈ n × n всегда можно диагонализовать. Теорема 4.21 непосредственно следует из спектральной теоремы 4.15. Более того, спектральная теорема утверждает, что мы можем найти ОНБ собственных век - торов n. Это делает P ортогональной матрицей, так что D = P TAP. ПРИМЕЧАНИЕ Нормальная форма матрицы Жордана предлагает разложение, которое работает для дефектных матриц (Lang, 1987), но выходит за рамки этой книги. \n--- Страница 158 ---\n158 Глава 4. Матричные разложения 4.4.1. Геометрическая интуиция для собственного разложения Мы можем интерпретировать собственное разложение матрицы следующим образом (см. также рис. 4.7): пусть A будет матрицей преобразования линейно - го отображения относительно стандартного базиса. P –1 выполняет замену стан - дартного базиса на собственный. Это идентифицирует собственные векторы pi (светлые стрелки на рис. 4.7) на стандартных базисных векторах ei. Затем диа - гональ D масштабирует векторы вдоль этих осей на собственные значения λi. Наконец, P преобразует эти масштабированные векторы обратно в стандартные/ канонические координаты, в результате чего получается λipi. A P /dollar.g00DC1 DP /g792e2/g792p2 /g791e1/g791p1p2 p1 e2 e1 Рис. 4.7. Наглядная иллюстрация собственного разложения как последовательных преобразований. От левого верхнего к левому нижнему изображению: P –1 выполняет изменение базиса (здесь нарисовано в 2 и изображено как операция, подобная вращению), отображая собственные векторы в стандартный базис. От левого нижнего к правому нижнему: D выполняет масштабирование по перенаправленным ортогональным собственным векторам, изображенным здесь кругом, растянутым до эллипса. От правого нижнего к правому верхнему: P отменяет базовое изменение (изображенное как обратное вращение) и восстанавливает исходную систему координат Пример 4.11 (собственное разложение) Вычислим собственное разложение . Шаг 1: вычислим собственные значения и собственные векторы. Харак - теристический многочлен A равен\n--- Страница 159 ---\n4.4. Собственное разложение и диагонализация 159 (4.56 a) = (2 – λ)2 – 1 = λ2 – 4λ + 3 = (λ – 3)( λ – 1). (4.56 b) Следовательно, собственные значения матрицы A равны λ1 = 1 и λ2 = 3 (корни характеристического полинома), а соответствующие (нормиро - ванные) собственные векторы получаются с помощью (4.57) Это дает (4.58) Шаг 2: проверим наличие. Собственные векторы p1, p2 составляют ба - зис 2. Следовательно, A можно диагонализовать. Шаг 3: построим матрицу P для диагонализации A. Соберем собственные векторы матрицы A в P, так чтобы (4.59) Затем получим (4.60) Аналогичным образом получаем (с учетом того, что P –1 = P T, поскольку собственные векторы p1 и p2 в этом примере образуют ОНБ): (4.61) zДиагональные матрицы D можно эффективно возвести в степень. Следова - тельно, мы можем найти степень матрицы для матрицы A ∈ n × n с помощью разложения по собственным значениям (если оно существует), так что Ak = (PDP –1)k = PDkP –1. (4.62)\n--- Страница 160 ---\n160 Глава 4. Матричные разложения Вычисление Dk эффективно, потому что мы применяем эту операцию индиви - дуально к любому диагональному элементу. zПредположим, что собственное разложение A = PDP –1 существует. Тогда det(A) = det(PDP –1) = det(P) det( D) det( P –1) = (4.63 a) (4.63 b) позволяет эффективно вычислять детерминант A. Для разложения по собственным значениям требуются квадратные матрицы. Было бы полезно выполнить разложение матрицы общего вида. В следующем разделе мы представим метод разложения матриц общего вида — разложение по сингулярным числам. 4.5. РАЗЛОЖЕНИЕ ПО СИНГУЛЯРНЫМ ЗНАЧЕНИЯМ Сингулярное разложение (singular value decomposition, SVD) матрицы — это центральный метод разложения матриц в линейной алгебре. Его называют «фундаментальной теоремой линейной алгебры» (Strang, 1993), потому что его можно применить ко всем матрицам, а не только к квадратным, и оно существу - ет всегда. Более того, как мы исследуем ниже, SVD матрицы A, которая пред - ставляет линейное отображение Φ : V → W, количественно определяет измене - ние базовой геометрии этих двух векторных пространств. Мы рекомендуем работы Калмана (1996) и Роя и Банерджи (2014) для более глубокого обзора математики SVD. Теорема 4.22 (теорема SVD). Пусть Am × n — прямоугольная матрица ранга r ∈ [0, min ( m, n)]. SVD оператора A представляет собой разложение вида (4.64) с ортогональной матрицей U ∈ m × m с векторами­столбцами ui, i = 1, , m, и ор­ тогональной матрицей V ∈ n × n с векторами­столбцами vj, j = 1, , n. Более того, Σ является матрицей размера m × n с Σii = σi ≥ 0 и Σij = 0, i ≠ j. Диагональные элементы σi, i = 1, , r, вектора Σ называются сингулярными зна ­ чениями , ui — левосингулярными векторами , а vj — правосингулярными вектора ­ ми. По соглашению, особые значения упорядочены, то есть σ1 ≥ σ2 ≥ σr ≥ 0.\n--- Страница 161 ---\n4.5. Разложение по сингулярным значениям 161 Матрица сингулярных значений Σ уникальна, но требует некоторого внимания. Заметим, что Σ ∈ m×n прямоугольна. В частности, Σ имеет тот же размер, что и A. Это означает, что Σ имеет диагональную подматрицу, которая содержит сингулярные значения и требует дополнительного заполнения нулями. В част- ности, если m > n, то матрица Σ имеет диагональную структуру до строки n и затем состоит из 0T векторов-строк от n + 1 до m ниже, так что (4.65) Если m < n, матрица Σ имеет диагональную структуру до столбца m и столбцов, состоящих из 0, от m + 1 до n: (4.66) ПРИМЕЧАНИЕ SVD существует для любой матрицы A ∈ m × n.  4.5.1. Геометрические интуиции для SVD SVD предлагает геометрическую интуицию для описания матрицы преобразо - вания A. Далее мы обсудим SVD как последовательные линейные преобразо - вания, выполняемые на базисах. В примере 4.12 мы затем применим матрицы преобразования SVD к набору векторов в 2, что позволит нам более четко визуализировать эффект каждого преобразования. SVD матрицы можно интерпретировать как разложение соответствующего линейного отображения (вспомните раздел 2.7.1) Φ : n → m на три операции (рис. 4.8). Интуиция SVD следует внешне схожей структуре с нашей интуици - ей собственного разложения (см. рис. 4.7): в общих чертах, SVD выполняет изменение базиса через V T, за которым следует масштабирование и увеличение (или уменьшение) размерности через матрицу сингулярных значений Σ. На- конец, оно выполняет вторую смену базиса через U. SVD влечет за собой ряд важных деталей и оговорок, поэтому мы рассмотрим нашу интуицию более подробно.\n--- Страница 162 ---\n162 Глава 4. Матричные разложения A V T ΣUσ2u2 σ1u1 σ2e2e2 σ1e1e1V2 V1 Рис. 4.8. Иллюстрация к представле- нию SVD матрицы A ∈ 3×2 в виде последовательных преобразований. От верхнего левого изображения к нижнему левому: V T выполняет базисное изменение в 2. От нижнего левого к нижнему правому: Σ масшта- бируется и отображается от 2 до 3. Эллипс в правом нижнем углу находится в  3. Третье измерение ортогонально поверхности эллипти-ческого диска. От нижнего правого к верхнему правому: U выполняет базисное изменение в  3 Предположим, нам дана матрица преобразования линейного отображения Φ : n → m относительно стандартных базисов B и C n и m соответственно1. Кроме того, предположим, что есть второй базис в n и в m. Тогда: 1. Матрица V выполняет замену базиса в области n от (представленного векторами v1 и v2 в верхнем левом углу рис. 4.8) на стандартный базис B. V T = V –1 выполняет изменение базиса с B на . Векторы теперь выровнены по каноническому основанию в левом нижнем углу рис. 4.8. 2. Изменив систему координат на , Σ масштабирует новые координаты син - гулярными значениями σi (и добавляет или удаляет измерения), то есть Σ является матрицей преобразования Φ относительно и , представленной векторами, растянутыми и лежащими в плоскости e1-e2, которая теперь встроена в третье измерение в правом нижнем углу рис. 4.8. 3. U выполняет замену базиса в области m с на канонический базис m, пред - ставленный поворотом векторов из плоскости e1-e2. Это показано в правом верхнем углу рис. 4.8. SVD выражает изменение базиса как в домене, так и в кодомене. Это контра - стирует с собственным разложением, которое работает в том же векторном пространстве, где применяется то же изменение базиса, а затем отменяется. Что делает SVD особенным, так это то, что эти два разных базиса одновременно связаны матрицей сингулярных значений Σ. 1 Полезно освежить в памяти изменения базиса (раздел 2.7.2), ортогональные матрицы (определение 3.8) и ортонормированные базисы (раздел 3.5).\n--- Страница 163 ---\n4.5. Разложение по сингулярным значениям 163 Пример 4.12 (векторы и SVD) Рассмотрим отображение квадратной сетки векторов  ∈ 2, которые помещаются в прямоугольник размером 2 × 2 с центром в начале коорди - нат. Используя стандартный базис, сопоставим эти векторы с помощью (4.67 a) (4.67 b) x11,5 1,51,0 1,00,5 0,50,0 0,0–0,5 –0,5–1,0 –1,0–1,5 –1,5 x11,5 1,0 0,5 0,0 –0,5 –1,0 –1,5x2 1,5 1,0 0,50,0 –0,5 –1,0 –1,5x2x 1x2 –1,5–0,50,51,5x3 –1,0–0,50,00,51,0 x 1–1,5–0,50,51,5x2 –1,5–0,50,51,5x3 0 –1,5–0,5 0,5 1,5 Рис. 4.9. SVD и отображение векторов (представленных точками). Части рисунка расположены в том же порядке против часовой стрелки, что и на рис. 4.8\n--- Страница 164 ---\n164 Глава 4. Матричные разложения Мы начинаем с набора векторов  (светлые и темные точки; см. верхнее левое изображение на рис. 4.9), расположенных в виде сетки. Затем мы применяем V T ∈ 2×2, который вращает . Повернутые векторы показаны на левом нижнем изображении рис. 4.9. Теперь мы отображаем эти век - торы, используя матрицу сингулярных значений Σ, в область 3 (см. нижнюю правую панель рис. 4.9). Обратите внимание, что все векторы лежат в плоскости x1-x2. Третья координата всегда равна 0. Векторы в пло- скости x1-x2 растянуты на сингулярные значения. Прямое отображение векторов  посредством A в область 3 равно пре - образованию  посредством UΣV T, где U выполняет поворот внутри об - ласти 3, так что отображаемые векторы больше не ограничиваются плоскостью x1-x2; они все еще находятся в плоскости, как показано на правой верхней панели рис. 4.9. 4.5.2. Построение SVD Далее мы обсудим, почему существует SVD, и подробно покажем, как его вы - числить. SVD общей матрицы имеет некоторое сходство с собственным раз - ложением квадратной матрицы. ПРИМЕЧАНИЕ Сравните собственное разложение симметричной положи - тельно определенной матрицы S = S T = PDP T (4.68) с соответствующим SVD S = UΣVT. (4.69) Если мы установим U = P = V, D = Σ, (4.70) мы видим, что SVD симметричных положительно определенных матриц явля - ется их собственным разложением.  Далее мы исследуем, почему теорема 4.22 верна и как строится SVD. Вычисле - ние SVD для A ∈ m × n эквивалентно нахождению двух наборов ортонормиро - ванных базисов U = (u1, , um) и V = (v1, , vn) области m и области n соответ - ственно. По этим упорядоченным базисам построим матрицы U и V.\n--- Страница 165 ---\n4.5. Разложение по сингулярным значениям 165 Наш план состоит в том, чтобы начать с построения ортонормированного на - бора правосингулярных векторов v1, , vn ∈ n. Затем мы строим ортонормиро - ванное множество левосингулярных векторов u1, , un ∈ m. После этого мы свяжем эти два множества и потребуем, чтобы ортогональность vi сохранялась при преобразовании A. Это важно, потому что мы знаем, что изображения Avi образуют набор ортогональных векторов. Затем мы нормализуем эти изобра - жения с помощью скалярных множителей, которые окажутся сингулярными значениями. Начнем с построения правосингулярных векторов. Спектральная теорема (тео- рема 4.15) говорит нам, что симметричная матрица обладает ОНБ собственных векторов, что также означает, что она может быть диагонализована. Более того, по теореме 4.14 мы всегда можем построить симметричную положительно полу - определенную матрицу ATA ∈ n × n из любой прямоугольной матрицы A ∈ m × n. Таким образом, мы всегда можем диагонализовать ATA и получить (4.71) где P — ортогональная матрица, составленная из ортонормированного соб - ственного базиса. λi ≥ 0 являются собственными значениями оператора ATA. Предположим, что SVD оператора A существует, и подставим (4.64) в (4.71). Это дает ATA = (UΣV T)T(UΣV T) = VΣ TU TUΣV T, (4.72) где U, V — ортогональные матрицы. Следовательно, при U TU = I получаем (4.73) Сравнивая теперь (4.71) и (4.73), отождествляем VT = PT (4.74) (4.75) Следовательно, собственные векторы матрицы ATA, составляющие P, являются правыми сингулярными векторами V матрицы A (4.74). Собственные значения ATA — это квадраты сингулярных значений Σ (4.75).\n--- Страница 166 ---\n166 Глава 4. Матричные разложения Чтобы получить левосингулярные векторы U, проделаем аналогичную про - цедуру. Начнем с вычисления SVD симметричной матрицы ATA ∈ m×m (вместо предыдущей ATA ∈ n × n). SVD А дает AAT = (UΣV T)(UΣV T)T = UΣV TVΣTU T = (4.76 a) (4.76 b) Спектральная теорема говорит нам, что AAT = SDST можно диагонализовать, и мы можем найти ОНБ собственных векторов AAT, которые собраны в S. Ор- тонормированные собственные векторы AAT являются левосингулярными векторами U и образуют ортонормированный базис в кодомене SVD. Остается вопрос о структуре матрицы Σ. Поскольку AAT и ATA имеют одинако - вые ненулевые собственные значения (см. c. 144), ненулевые элементы Σ-матриц в SVD для обоих случаев должны быть одинаковыми. Наконец, объединим вместе все, что мы уже сделали. У нас есть ортонормиро - ванный набор правосингулярных векторов в V. Чтобы завершить построение SVD, мы соединяем их с ортонормированными векторами U. Для достижения этой цели мы используем тот факт, что изображения vi под A также должны быть ортогональными. Мы можем показать это, используя результаты из раздела 3.4. Мы требуем, чтобы скалярное произведение между Avi и Avj было равно 0 для i ≠ j. Для любых двух ортогональных собственных векторов vi, vj, i ≠ j, справед - ливо (4.77) Для случая m ≥ r справедливо, что { Av1, , Avr} является базисом r-мерного под - пространства в m. Для завершения построения SVD нам потребуются ортонормированные лево - сингулярные векторы: нормализуем образы правосингулярных векторов Avi и получаем (4.78) где последнее равенство было получено из (4.75) и (4.76 b). Отсюда вытекает: собственные значения AAT таковы, что . Следовательно, собственные векторы ATA, которые, как мы знаем, являются правосингулярными векторами vi, и их нормализованные образы относительно A, левосингулярные векторы ui,\n--- Страница 167 ---\n4.5. Разложение по сингулярным значениям 167 образуют два самосогласованных ОНБ, которые связаны посредством матрицы сингулярных значений Σ. Перепишем (4.78), чтобы получить сингулярное уравнение Avi = σiui, i = 1, , r. (4.79) Это уравнение очень похоже на уравнение для собственных значений (4.25), но векторы в левой и правой частях не совпадают. При n > m (4.79) выполняется только для i ≤ m, а (4.79) ничего не говорит о ui при i > m. Однако мы знаем по построению, что они ортонормированы. Наобо - рот, для m > n (4.79) выполняется только для i ≤ n. Для i > n у нас Avi = 0, и мы знаем, что vi образуют ортонормированное множество. Это означает, что SVD также предоставляет ортонормированный базис ядра (нулевое пространство) A, набор векторов x с Ax = 0 (см. раздел 2.7.3). Более того, объединение vi как столбцов V и ui как столбцов U дает AV = UΣ, (4.80) где Σ имеет ту же размерность, что и A, и диагональную структуру для строк 1, , r. Следовательно, умножение справа на V T дает A = UΣV T, которая являет - ся SVD A. Пример 4.13 (расчет SVD) Найдем сингулярное разложение (4.81) SVD требует от нас вычисления правосингулярных векторов vj, сингу - лярных значений σk и левосингулярных векторов ui. Шаг 1: правосингулярные векторы как собственный базис ATA. Начнем с вычисления (4.82) Мы вычисляем сингулярные значения и правосингулярные векторы vj с помощью разложения по собственным значениям матрицы ATA, которое задается как\n--- Страница 168 ---\n168 Глава 4. Матричные разложения (4.83) и мы получаем правосингулярные векторы как столбцы P, так что (4.84) Шаг 2: матрица сингулярных чисел. Так как сингулярные значения σi являются квадратными корнями из собственных значений оператора ATA, мы получаем их прямо из D. По- скольку rk( A) = 2, есть только два ненулевых сингулярных значения: σ1 = и σ2 = 1. Матрица сингулярных значений должна быть того же размера, что и A, и мы получаем (4.85) Шаг 3: левосингулярные векторы как нормализованное изображение правосингулярных векторов. Находим левосингулярные векторы, вычисляя образ правосингулярных векторов под A и нормализуя их, разделив на соответствующее сингуляр - ное значение. Получаем: (4.86)\n--- Страница 169 ---\n4.5. Разложение по сингулярным значениям 169 (4.87) (4.88) Обратите внимание, что на компьютере подход, проиллюстрированный здесь, плохо реализуется, и SVD A обычно вычисляется без использования разложения на собственные значения ATA. 4.5.3. Разложение на собственные значения и разложение на сингулярные значения Рассмотрим собственное разложение A = PDP –1 и SVD A = UΣV T и вспомним основные положения предыдущих разделов. zSVD всегда существует для любой матрицы m × n. Собственное разложение определено только для квадратных матриц n × n и существует только в том случае, если мы можем найти базис из собственных векторов n. zВекторы в матрице собственного разложения P не обязательно ортогональ - ны, то есть смена базиса не является простым поворотом и масштабирова - нием. А вот векторы в матрицах U и V в SVD ортонормированы, поэтому они соответствуют вращению. zИ собственное разложение, и SVD представляют собой композиции трех линейных отображений: 1. Изменение базиса в домене. 2. Независимое масштабирование каждого нового базисного вектора и ото- бражение от домена к домену. 3. Изменение базиса в кодомене. Ключевое различие между собственным разложением и SVD состоит в том, что в SVD домен и кодомен могут быть векторными пространствами разных размеров. zВ SVD лево- и правосингулярные векторные матрицы U и V обычно не ин - версны друг другу (они выполняют замену базиса в разных векторных про -\n--- Страница 170 ---\n170 Глава 4. Матричные разложения странствах). В собственном разложении матрицы замены базиса P и P–1 обратны друг другу. zВ SVD все элементы диагональной матрицы Σ являются действительными и неотрицательными, что обычно неверно для диагональной матрицы в соб- ственном разложении. zSVD и собственное разложение тесно связаны своими проекциями: yЛевосингулярные векторы A являются собственными векторами AAT. yПравосингулярные векторы A являются собственными векторами ATA. yНенулевые сингулярные значения A являются квадратными корнями ненулевых собственных значений матрицы AAT и равны ненулевым соб - ственным значениям матрицы ATA. zДля симметричных матриц A ∈ n×n разложение по собственным значениям и SVD — одно и то же, что следует из спектральной теоремы 4.15. Пример 4.14 (определение структуры рейтингов фильмов и зрителей) Давайте добавим практическую интерпретацию SVD, проанализировав данные о людях и их любимых фильмах. Рассмотрим трех зрителей (Али, Беатрикс и Чандру), оценивающих четыре разных фильма («Звездные войны», «Бегущий по лезвию», «Амели», «Деликатесы»). Их рейтинги представляют собой значения от 0 (наихудшее) до 5 (наилучшее) и за- кодированы в матрице данных A ∈ 4 × 3, как показано на рис. 4.10. Каждая строка представляет фильм, каждый столбец — пользователя. Таким об - разом, векторами-столбцами рейтингов фильмов, по одному для каждого зрителя, являются xAli, xBeatrix, xChandra. Разлагая A на множители с помощью SVD, можно проследить закономер - ности в том, как эти зрители оценивают фильмы, а также понять, суще - ствует ли структура, позволяющая увидеть, кому какие фильмы нравят - ся. Примененяя SVD к нашей матрице данных A, мы делаем ряд предположений: 1. Все зрители оценивают фильмы последовательно, используя одно и то же линейное отображение. 2. В рейтингах нет ошибок и шумов. 3. Мы интерпретируем левосингулярные векторы ui как стереотипные фильмы, а правосингулярные векторы vj как стереотипных зрителей. Затем мы делаем допущение, что предпочтения любого зрителя в отно- шении фильма могут быть выражены как линейная комбинация vj. Ана-\n--- Страница 171 ---\n4.5. Разложение по сингулярным значениям 171 логичным образом «предпочитаемость» любого фильма может быть вы - ражена как линейная комбинация ui. Следовательно, вектор в области SVD можно интерпретировать как зрителя в «пространстве» стереотип - ных зрителей, а вектор в кодомене SVD, соответственно, как фильм в «пространстве» стереотипных фильмов1. Давайте проверим SVD нашей кино-пользовательской матрицы. Первый левосингулярный вектор u1 имеет большие абсолютные значения для двух научно-фантастических фильмов и большое первое сингулярное значение (темная заливка на рис. 4.10). Таким образом, это группирует тип пользователей с опреде - ленным набором фильмов (тема научной фантастики). Точно так же первое правое единственное число v1 показывает большие абсолютные значения для Али и Беатрикс, которые высоко оценивают научно-фанта - стические фильмы (светлая заливка на рис. 4.10). Это говорит о том, что v1 отражает тип любителя научной фантастики. 541 550 005 104 /grave.g006B/.notdef.g00E3/comma.g00D2 /a.g0072/yright/equal.g00C8/two.g0088/exclam.g00AF/comma.g00D2/asterisk.g007D“ /parenright.g00A0/equal.g00C8…/.notdef.g0105/exclam.g00AF/equal.g00C8 /g.g007E/quotedbl.g006D/yrightƒ/.notdef.g0105…/slash.g00A9/yright /quotedbl.g006D/percent.g00BA/L.g00AE…/slash.g00A9 /a.g0072/yright/.notdef.g0104/three.g0082/question.g009D/comma.g00D2/L.g00AE /C.g00B9/percent.g00BA /.notdef.g00E3/yrightƒ/quotedbl.g006D/comma.g00D2/.notdef.g010A /grave.g006B/.notdef.g00E4/yright/.notdef.g00E3/comma.g00D2 /d.g0069/yright/.notdef.g00E3/comma.g00D2/asterisk.g007D/equal.g00C8/two.g0088/yright“/slash.g00A9=/dollar.g00DC0,6710 /dollar.g00DC0,7197 /dollar.g00DC0,0939 /dollar.g00DC0,1515/dollar.g00DC0,5774 0,4619 /dollar.g00DC0,3464/dollar.g00DC0,57740,4647 /dollar.g00DC0,4759 /dollar.g00DC0,5268 0,52930,0236 0,0254 /dollar.g00DC0,7705/dollar.g00DC0,6030 9,6438 0 0 000 0,7056 00 6,3639 00 /dollar.g00DC0,7367 0,08520,6708/dollar.g00DC0,1811 /dollar.g00DC0,9807/dollar.g00DC0,7043/dollar.g00DC0,6515 0,1762 /dollar.g00DC0,7379 Рис. 4.10. Оценки трех человек для четырех фильмов и их SVD-разложение Точно так же u2 отражает тему фильмов французского арт-хауса, а v2 указывает на то, что Чандру можно считать почти идеализированной любительницей таких фильмов. Идеализированный любитель научной фантастики — пурист и не смотрит ничего, кроме научно-фантастических 1 Эти два «пространства» будут значимыми для соответствующего зрителя и фильма, только если данные содержат достаточное разнообразие зрителей и фильмов.\n--- Страница 172 ---\n172 Глава 4. Матричные разложения фильмов, поэтому этот зритель v1 дает нулевую оценку всему, кроме те - матики научной фантастики, — эта логика подразумевает диагональную подструктуру для матрицы сингулярных значений Σ. Таким образом, конкретный фильм представлен тем, как он (линейно) разлагается на стереотипные фильмы. Точно так же конкретного зрителя можно пред - ставить через тематическое распределение тех фильмов, что ему нравят - ся (здесь также применяется линейная комбинация). Стоит кратко обсудить терминологию и условные обозначения SVD, поскольку в литературе используются разные версии. С математической точки зрения эти отличия инвариантны, но, вообще говоря, могут приводить к некоторой пута - нице. zДля удобства в обозначениях и абстракции мы используем обозначение SVD, где SVD описывается как имеющее две квадратные лево- и правосингулярные векторные матрицы, но неквадратную матрицу сингулярных значений. Наше определение (4.64) для SVD иногда называют полным SVD . zНекоторые авторы определяют SVD несколько иначе и сосредоточиваются на квадратных сингулярных матрицах. Тогда для A ∈ m × n и m ≥ n, (4.89) Иногда эту формулировку называют уменьшенным SVD (например, Datta, 2010) или SVD (например, Press et al., 2007). Этот альтернативный формат меняет только способ построения матриц, но оставляет неизменной мате - матическую структуру SVD. Удобство этой альтернативной формулировки состоит в том, что Σ диагональна, как в разложении на собственные зна - чения. zВ разделе 4.6 мы узнаем о методах аппроксимации матриц с использованием SVD, который также называют усеченным SVD. zМожно определить SVD матрицы A ранга r так, чтобы U была матрицей раз - мера m × r, Σ — диагональной матрицей r × r, а V — матрицей размера r × n. Эта конструкция очень похожа на сформулированное нами определение и гарантирует, что диагональная матрица Σ содержит вдоль диагонали толь - ко ненулевые элементы. Основное удобство этой альтернативной записи заключается в том, что Σ диагональна, как в разложении по собственным значениям. zzОграничение, состоящее в том, что SVD для A применяется только к матри - цам m × n с m > n, практически не требуется. Когда m < n, SVD-разложение\n--- Страница 173 ---\n4.6. Матричное приближение 173 даст Σ с б ольшим количеством нулевых столбцов, чем строк и, следователь - но, сингулярными значениями σm + 1, , σn, равными 0. SVD используется во множестве приложений машинного обучения, от метода наименьших квадратов при аппроксимации кривой до решения систем линейных уравнений. Эти приложения используют различные важные свойства SVD, его связь с рангом матрицы и его способность аппроксимировать матрицы задан - ного ранга матрицами более низкого ранга. Преимущество замены матрицы на ее SVD состоит в том, что вычисления становятся более устойчивыми к ошиб - кам численного округления. Как мы увидим в следующем разделе, способность SVD систематически аппроксимировать матрицы с помощью «более простых» матриц открывает возможности для приложений машинного обучения, начиная от уменьшения размерности и тематического моделирования до сжатия и кла- стеризации данных. 4.6. МАТРИЧНОЕ ПРИБЛИЖЕНИЕ Мы рассматривали SVD как способ факторизовать A = UΣV T ∈ m × n в произ - ведение трех матриц, где U ∈ m × m и V ∈ n × n ортогональны, а Σ содержит сингулярные значения на своей главной диагонали. Вместо того чтобы вы - полнять полную факторизацию SVD, мы теперь исследуем, как SVD позво - ляет нам представить матрицу A как сумму более простых (низкоранговых) матриц Ai, которая поддается схеме аппроксимации матрицы, которая дешев - ле в вычислении, чем полное SVD. Построим матрицу Ai ∈ m × n ранга 1 сле - дующим образом: (4.90) который образован внешним произведением i-го ортогонального вектора-столб - ца U и V. На рис. 4.11 показано изображение Стоунхенджа, которое может быть представлено матрицей A ∈ 1432 × 1910 и некоторыми внешними произведениями Ai, как определено в (4.90). Матрица A ∈ m × n ранга r может быть записана как сумма матриц Ai ранга 1, так что (4.91) где матрицы внешнего произведения Ai взвешиваются i-м сингулярным значе - нием σi. Мы можем видеть, почему выполняется (4.91): диагональная структу - ра матрицы сингулярных значений Σ умножает только совпадающие лево- и правосингулярные векторы и масштабирует их на соответствующее сингулярное значение σi. Все члены Σij обращаются в нуль при i ≠ j, посколь -\n--- Страница 174 ---\n174 Глава 4. Матричные разложения ку Σ — диагональная матрица. Любые члены i > r обращаются в нуль, поскольку соответствующие сингулярные значения равны 0. В (4.90) мы ввели матрицы Ai ранга 1. Мы просуммировали r отдельных матриц ранга 1, чтобы получить ма - трицу A ранга r; см. (4.91). Если сумма не проходит по всем матрицам Ai, i = 1, , r, но только до промежуточного значения k < r, мы получаем приближение ранга k (4.92) группы A с . На рис. 4.12 показаны аппроксимации низкого ран - га исходного изображения A Стоунхенджа. Форма скал становится все более заметной и отчетливо узнаваемой в приближении 5-го ранга. В то время как исходное изображение требует 1432 · 1910 = 2 735 120 чисел, приближение ранга 5 требует, чтобы мы сохраняли только пять сингулярных значений и пять левых и правых векторов (1432- и 1910-размерный каждый), всего 5 · (1432 + + 1910 + 1) = 16 715 чисел — чуть больше 0,6% от оригинала. Чтобы измерить разницу (ошибку) между A и его приближением ранга k, нам понадобится понятие нормы. В разделе 3.1 мы уже применяли векторные нормы, чтобы из - мерить длину вектора. По аналогии мы также можем определить нормы матриц. (a) /h.g006A“/period.g00B2/percent.g00BA/.notdef.g0105…/percent.g00BA/yright /comma.g00D2ƒ/percent.g00BA/K.g00AD/exclam.g00AF/equal.g00C8›/yright…/comma.g00D2/yright A (b) A1, /g861 /g124 228, 052 (d) A3, /g863 /g124 26, 125 (e) A4, /g864 /g124 20, 232 (f) A5, /g865 /g124 15, 436(c) A2, /g862 /g124 40, 647 Рис. 4.11. Обработка изображений с помощью SVD. ( a) Исходное изображение в градациях серого представляет собой матрицу размером 1432 × 1910 значений от 0 (черный) до 1 (белый). ( b)–(f) Матрицы A1, , A5 и соответствующие им особые значения σ1, , σ5. Сеточноподобная структура каждой матрицы ранга 1 накладывается внешним произведением лево- и правосингулярных векторов\n--- Страница 175 ---\n4.6. Матричное приближение 175 (a) /h.g006A“/period.g00B2/percent.g00BA/.notdef.g0105…/percent.g00BA/yright /comma.g00D2ƒ/percent.g00BA/K.g00AD/exclam.g00AF/equal.g00C8›/yright…/comma.g00D2/yright A (b) /o.g0081/exclam.g00AF/comma.g00D2/K.g00AD/.notdef.g00E3/comma.g00D2›/yright…/comma.g00D2/yright /exclam.g00AF/equal.g00C8…/.notdef.g0104/equal.g00C8 1 A(1) (c) /o.g0081/exclam.g00AF/comma.g00D2/K.g00AD/.notdef.g00E3/comma.g00D2›/yright…/comma.g00D2/yright /exclam.g00AF/equal.g00C8…/.notdef.g0104/equal.g00C8 2 A (2) (e) /o.g0081/exclam.g00AF/comma.g00D2/K.g00AD/.notdef.g00E3/comma.g00D2›/yright…/comma.g00D2/yright /exclam.g00AF/equal.g00C8…/.notdef.g0104/equal.g00C8 4 A(1) (d) /o.g0081/exclam.g00AF/comma.g00D2/K.g00AD/.notdef.g00E3/comma.g00D2›/yright…/comma.g00D2/yright /exclam.g00AF/equal.g00C8…/.notdef.g0104/equal.g00C8 3 A(1) (f) /o.g0081/exclam.g00AF/comma.g00D2/K.g00AD/.notdef.g00E3/comma.g00D2›/yright…/comma.g00D2/yright /exclam.g00AF/equal.g00C8…/.notdef.g0104/equal.g00C8 5 A(2) Рис. 4.12. Реконструкция изображения с помощью SVD. ( а) Исходное изображение. (b)–(f) Восстановление изображения с использованием приближения низкого ранга SVD, где приближение ранга k задается как Определение 4.23 (спектральная норма матрицы). Для x ∈ n \\ {0} спектраль ­ ная норма матрицы A ∈ m×n определяется как (4.93) Введем обозначение нижнего индекса в матричной норме (левая часть) анало - гично евклидовой норме для векторов (правая часть), у которой есть нижний индекс 2. Спектральная норма (4.93) определяет, насколько длинным может стать любой вектор x при умножении на A. Теорема 4.24. Спектральная норма матрицы A — это наибольшее сингулярное значение σ1. Мы оставляем доказательство этой теоремы в качестве упражнения. Теорема 4.25 (теорема Эккарта — Юнга (Eckart and Y oung, 1936)). Рассмотрим матрицу m × n ранга r, и пусть B ∈ m × n — матрица ранга k . Для любого k ≤ r с выполняется (4.94) (4.95)\n--- Страница 176 ---\n176 Глава 4. Матричные разложения Теорема Эккарта — Юнга явно указывает, сколько ошибок мы вносим, аппрок - симируя A с использованием приближения ранга k. Мы можем интерпретировать приближение ранга k, полученное с помощью SVD, как проекцию матрицы A полного ранга на пространство меньшей размерности матриц ранга не более k. Из всех возможных проекций SVD минимизирует ошибку (относительно спек - тральной нормы) между A и любым приближением ранга k. Можно повторно отследить некоторые шаги, чтобы понять, почему выраже - ние (4.95) верно. Заметим, что разность между представляет собой матрицу, содержащую сумму оставшихся матриц ранга 1: (4.96) По теореме 4.24 сразу получаем σk+1 как спектральную норму разностной мат- рицы. Рассмотрим подробнее (4.94). Если предположить, что существует другая матрица B с rk( B) ≤ k, такая что (4.97) то существует не менее чем ( n – k)-мерное нулевое пространство Z ⊆ n, такое что из x ∈ Z следует, что Bx = 0. Тогда следует, что (4.98) и, используя вариант неравенства Коши — Шварца (3.17), охватывающий нор - мы матриц, получаем (4.99) Однако существует ( k + 1)-мерное подпространство, где || Ax ||2 ≥ σk+1 || x ||2, кото - рое натянуто на правосингулярные векторы vj, j ≤ k + 1 множества A. Суммиро - вание размерностей этих двух пространств дает число больше n, так как в обоих пространствах должен быть ненулевой вектор. Это противоречит теореме о ранге недействительности (теорема 2.24) из раздела 2.7.3. Из теоремы Эккарта — Юнга следует, что мы можем использовать SVD для уменьшения матрицы A ранга r до матрицы ранга k принципиальным опти - мальным (в смысле спектральной нормы) способом. Мы можем интерпретиро - вать аппроксимацию A матрицей ранга k как форму сжатия с потерями. Следо - вательно, низкоранговая аппроксимация матрицы появляется во многих приложениях машинного обучения, например при обработке изображений, фильтрации шума и регуляризации некорректно поставленных задач. Кроме того, она играет ключевую роль в уменьшении размерности и анализе главных компонентов, как мы увидим в главе 10.\n--- Страница 177 ---\n4.6. Матричное приближение 177 Пример 4.15 (поиск структуры в рейтингах фильмов и потребителях (продолжение)) Возвращаясь к нашему примеру рейтингов фильмов, теперь мы можем применить концепцию аппроксимации низкого ранга для аппроксимации исходной матрицы данных. Напомним, что наша первая сингулярная величина отражает понятия темы научной фантастики в фильмах и лю- бителя научной фантастики. Таким образом, используя только первый сингулярный член в разложении ранга 1 матрицы рейтингов фильмов, мы получаем прогнозируемые рейтинги (4.100 a) . (4.100 b) Это первое приближение ранга A1 информативно: мы видим, что Али и Беатрикс любят научно-фантастические фильмы, такие как «Звезд - ные войны» и «Бегущий по лезвию» (записи имеют значения > 4), но не знаем, какие рейтинги дает другим фильмам Чандра. Это неудиви - тельно, поскольку в первое сингулярное значение не входят фильмы, которые понравились бы Чандре. Второе сингулярное значение дает нам лучшее приближение ранга 1 для любителей этой тематики филь - мов: (4.101 a) (4.101 b)\n--- Страница 178 ---\n178 Глава 4. Матричные разложения Во втором приближении ранга 1 A2 мы получаем рейтинги и тематики фильмов, которые любит Чандра, но сюда не попадают научно-фантасти - ческие фильмы. Поэтому следует рассмотреть приближение ранга 2 , где мы объединяем первые два приближения ранга 1: (4.102) подобна исходной таблице рейтингов фильмов (4.103) следовательно, мы можем игнорировать вклад A3. Это можно интерпре - тировать так, что в таблице нет данных по третьей категории тематики фильмов / любителей фильмов. Это также означает, что пространство тематики фильмов / любители фильмов в нашем примере — двухмерное, и содержит только тематику и любителей научной фантастики и фран - цузского арт-хауса. 4.7. МАТРИЧНАЯ ФИЛОГЕНИЯ В главах 2 и 3 мы рассмотрели основы линейной алгебры и аналитической геометрии. В этой главе изучили фундаментальные характеристики матриц и линейных отображений. На рис. 4.13 изображено филогенетическое1 дере - во отношений между различными типами матриц (черные стрелки указыва - ют «является подмножеством») и операциями, которые мы можем выполнять с ними. Рассмотрим все вещественные матрицы A ∈ n × m. Для неквадратных матриц (где n ≠ m) SVD существует всегда, как мы видели в этой главе. Если сосредоточиться на квадратных матрицах A ∈ n × n, то по их детерминанту можно определить, существует ли у квадратной матрицы обратная матрица, то есть принадлежит ли она к классу обратимых матриц. Если квадратная матрица размера n × n имеет n линейно независимых собственных векторов, 1 Слово «филогенетический» описывает, как мы фиксируем отношения между отдель - ными людьми или группами, и происходит от греческих слов «племя» и «источник».\n--- Страница 179 ---\n4.7. Матричная филогения 179 то матрица не является дефектной и существует собственное разложение (теорема 4.12). Мы знаем, что повторяющиеся собственные значения могут привести к появлению дефектных матриц, которые не могут быть диагонали - зованы. /uni0412/uni0435/uni0449/uni0435/uni0441/uni0442/uni0432/uni0435/uni043D/uni043D/uni044B/uni0435 /uni043C/uni0430/uni0442/uni0440/uni0438/uni0446/uni044B ∃/uni041F/uni0441/uni0435/uni0432/uni0434/uni043E/uni043E/uni0431/uni0440/uni0430/uni0442/uni043D/uni044B/uni0435 ∃SVD /uni041A/uni0432/uni0430/uni0434/uni0440/uni0430/uni0442/uni043D/uni0430/uni044F ∃/uni0414/uni0435/uni0442/uni0435/uni0440/uni043C/uni0438/uni043D/uni0430/uni043D/uni0442 ∃/uni0421/uni043B/uni0435/uni0434/uni041D/uni0435/uni043A/uni0432/uni0430/uni0434/uni0440/uni0430/uni0442/uni043D/uni0430/uni044F /uni0414/uni0435/uni0444/uni0435/uni043A/uni0442/uni043D/uni0430/uni044F/uni0421/uni0438/uni043D/uni0433/uni0443/uni043B/uni044F/uni0440/uni043D/uni0430/uni044F /uni041D/uni0435/uni0434/uni0435/uni0444/uni0435/uni043A/uni0442/uni043D/uni0430/uni044F (/uni0434/uni0438/uni0430/uni0433/uni043E/uni043D/uni0430/uni043B/uni0438/uni0437/uni0438/uni0440/uni0443/uni0435/uni043C/uni0430/uni044F) /uni041D/uni043E/uni0440/uni043C/uni0430/uni043B/uni044C/uni043D/uni0430/uni044F /uni041D/uni0435 /uni043D/uni043E/uni0440/uni043C/uni0430/uni043B/uni044C/uni043D/uni0430/uni044F /uni0421/uni0438/uni043C/uni043C/uni0435/uni0442/uni0440/uni0438/uni0447/uni043D/uni0430/uni044F. /uni0421/uni043E/uni0431/uni0441/uni0442/uni0432/uni0435/uni043D/uni043D/uni044B/uni0435 /uni0437/uni043D/uni0430/uni0447/uni0435/uni043D/uni0438/uni044F ∈R /uni041F/uni043E/uni043B/uni043E/uni0436/uni0438/uni0442/uni0435/uni043B/uni044C/uni043D/uni043E /uni043E/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni043D/uni0430/uni044F. /uni0421/uni043E/uni0431/uni0441/uni0442/uni0432/uni0435/uni043D/uni043D/uni044B/uni0435 /uni0437/uni043D/uni0430/uni0447/uni0435/uni043D/uni0438/uni044F /uni0425/uni043E/uni043B/uni0435/uni0446/uni043A/uni043E/uni0433/uni043E > 0/uni0414/uni0438/uni0430/uni0433/uni043E/uni043D/uni0430/uni043B/uni044C/uni043D/uni0430/uni044F /uni0415/uni0434/uni0438/uni043D/uni0438/uni0447/uni043D/uni0430/uni044F /uni043C/uni0430/uni0442/uni0440/uni0438/uni0446/uni0430∃/uni041E/uni0431/uni0440/uni0430/uni0442/uni043D/uni0430/uni044F /uni043C/uni0430/uni0442/uni0440/uni0438/uni0446/uni0430 /uni0420/uni0435/uni0433/uni0443/uni043B/uni044F/uni0440/uni043D/uni0430/uni044F (/uni043E/uni0431/uni0440/uni0430/uni0442/uni0438/uni043C/uni0430/uni044F) /uni041E/uni0440/uni0442/uni043E/uni0433/uni043E/uni043D/uni0430/uni043B/uni044C/uni043D/uni0430/uni044F /uni0412/uni0440/uni0430/uni0449/uni0435/uni043D/uni0438/uni0435/uni041D/uni0435/uni0442 /uni0431/uni0430/uni0437/uni0438/uni0441/uni0430 /uni0438/uni0437 /uni0441/uni043E/uni0431/uni0441/uni0442/uni0432/uni0435/uni043D/uni043D/uni044B/uni0445 /uni0432/uni0435/uni043A/uni0442/uni043E/uni0440/uni043E/uni0432 /uni0411/uni0430/uni0437/uni0438/uni0441 /uni0438/uni0437 /uni0441/uni043E/uni0431/uni0441/uni0442/uni0432/uni0435/uni043D/uni043D/uni044B/uni0445 /uni0432/uni0435/uni043A/uni0442/uni043E/uni0440/uni043E/uni0432 /uni0421/uni0442/uni043E/uni043B/uni0431/uni0446/uni044B — /uni043E/uni0440/uni0442/uni043E/uni0433/uni043E/uni043D/uni0430/uni043B/uni044C/uni043D/uni044B/uni0435 /uni0441/uni043E/uni0431/uni0441/uni0442/uni0432/uni0435/uni043D/uni043D/uni044B/uni0435 /uni0432/uni0435/uni043A/uni0442/uni043E/uni0440/uni044Bdet =0det= 0Rn × nRn × m /grave.g006BŠ/grave.g006B = /grave.g006B/grave.g006B Š = I/grave.g006BŠ/grave.g006B = /grave.g006B/grave.g006BŠ /grave.g006BŠ/grave.g006B = /grave.g006B/grave.g006BŠ Рис. 4.13. Функциональная филогения матриц, встречающихся в машинном обучении Неособые и недефектные матрицы — это не одно и то же. Например, матрица вращения будет обратимой (детерминант отличен от нуля), но не диагонализу - емой в действительных числах (не гарантируется, что собственные значения будут действительными числами). Давайте подробнее рассмотрим недефектные квадратные матрицы размера n × n. A является нормальной, если выполняется условие ATA = AAT. Более того, если выполняется более жесткое условие ATA = AAT = I, то A называется ортогональной (см. определение 3.8). Множество ортогональных матриц яв -\n--- Страница 180 ---\n180 Глава 4. Матричные разложения ляется подмножеством регулярных (обратимых) матриц и удовлетворяет условию AT = A–1. Нормальные матрицы имеют часто встречающееся подмножество симметричных матриц S ∈ n × n, которые удовлетворяют условию S = S T. Симметричные матри - цы имеют только действительные собственные значения. Подмножество сим - метричных матриц состоит из положительно определенных матриц P, которые удовлетворяют условию xTPx > 0 для всех x ∈ n \\ {0}. В этом случае существу - ет единственное разложение Холецкого (теорема 4.18). Положительно опреде - ленные матрицы имеют только положительные собственные значения и всегда обратимы (то есть имеют ненулевой детерминант). Другое подмножество симметричных матриц состоит из диагональных матриц D. Диагональные матрицы замкнуты при умножении и сложении, но не обязатель - но образуют группу (это происходит только в том случае, если все диагональные элементы не равны нулю, так что матрица обратима). Специальная диагональ - ная матрица — это единичная матрица I. 4.8. ДОПОЛНИТЕЛЬНОЕ ЧТЕНИЕ Большая часть этой главы посвящена математическому аппарату и его связям с методами сопоставления (маппинга) данных. Эти методы лежат в основе про - граммных решений машинного обучения. Характеристика матриц с использо - ванием детерминантов, собственных спектров и собственных подпространств создает возможности и условия для категоризации и анализа матриц. Это рас - пространяется на все формы представления маппинга данных, а также на оцен - ку численной устойчивости вычислительных операций с такими матрицами (Press et al., 2007). Детерминанты — это фундаментальные инструменты для обращения матриц и вычисления собственных значений «вручную». Однако почти во всех слу - чаях численные вычисления с применением метода Гаусса более эффективны, чем детерминанты (Press et al., 2007). Тем не менее, детерминанты остаются мощной теоретической концепцией, например для получения интуитивного понимания ориентации базиса на основе знака детерминанты. Собственные векторы могут использоваться для выполнения базисных изменений для пре - образования данных в координаты значимых ортогональных векторов при - знаков. Точно так же методы разложения матриц, такие как разложение Хо - лецкого, часто возникают при вычислении или моделировании случайных событий (Rubinstein and Kroese, 2016). Таким образом, разложение Холецко - го позволяет нам использовать прием репараметризации , когда мы хотим выполнить непрерывное дифференцирование по случайным величинам, на -\n--- Страница 181 ---\n4.8. Дополнительное чтение 181 пример в вариационных автоэнкодерах (Jimenez Rezende et al., 2014; Kingma and Ba, 2014). Фундаментальная ценность разложения матрицы на основе собственных век - торов заключается в том, что оно позволяет извлекать значимую и интерпрети - руемую информацию, которая характеризует линейные отображения. Следова - тельно, собственное разложение лежит в основе общего класса алгоритмов машинного обучения, называемых спектральными методами , которые выпол - няют собственное разложение положительно определенного ядра. Эти методы спектральной декомпозиции включают классические подходы к статистическо - му анализу данных, такие как: zАнализ главных компонент (principal component analysis — PCA (Pearson, 1901), см. также главу 10), в котором ищется низкоразмерное подпростран - ство, которое объясняет большую часть изменчивости данных. zДискриминантный анализ Фишера , целью которого является определение разделяющей гиперплоскости для классификации данных (Mika et al., 1999). zzМногомерное масштабирование (multidimensional scaling — MDS) (Carroll and Chang, 1970). Вычислительная эффективность этих методов обычно достигается за счет на - хождения наилучшего приближения ранга k к симметричной положительно полуопределенной матрице. Более современные примеры спектральных методов имеют различное происхождение, но каждый из них требует вычисления соб - ственных векторов и собственных значений положительно определенного ядра. Сюда входят Isomap (Tenenbaum et al., 2000), лапласовские собственные карты (Belkin and Niyogi, 2003), собственные карты Гессе (Donoho and Grimes, 2003) и спектральная кластеризация (Shi and Malik, 2000). Основные вычисления для них обычно подкрепляются методами аппроксимации матриц низкого ран - га (Belabbas and Wolfe, 2009), с которыми мы встречались при изучении SVD. SVD позволяет извлекать информацию того же типа, что и собственное раз - ложение. Однако SVD более широко применяется к неквадратным матрицам и таблицам данных. Эти методы матричной факторизации становятся актуаль - ными всякий раз, когда мы хотим идентифицировать неоднородность данных и планируем выполнить сжатие данных путем аппроксимации, например вместо хранения значений n × m сохраняя k(n + m) значений. Другой вариант — когда хотим выполнить предварительную обработку данных, например для декорре - ляции переменных-предикторов матрицы плана (Ormoneit et al., 2001). SVD работает с матрицами, которые можно интерпретировать как прямоугольные массивы с двумя индексами (строками и столбцами). Структуры, подобные матрицам, но в многомерном пространстве, называются тензорами. На самом\n--- Страница 182 ---\n182 Глава 4. Матричные разложения деле SVD — это частный случай более общего семейства декомпозиций, которые работают с такими тензорами (Kolda and Bader, 2009). Примеры SVD-подобных операций и низкоранговых аппроксимаций тензоров — разложение Таккера (Tucker, 1966) или CP­разложение (Carroll and Chang, 1970). Приближение низкого ранга SVD часто используется в машинном обучении в силу его вычислительной эффективности. Это связано с тем, что оно умень - шает объем памяти и операций с ненулевым умножением, которые необходимо выполнять с потенциально очень большими матрицами данных (Trefethen and Bau III, 1997). Более того, аппроксимации низкого ранга используются для работы с матрицами, которые могут содержать пропущенные значения, а также в целях сжатия с потерями и уменьшения размерности (Moonen and De Moor, 1995; Markovsky, 2011). УПРАЖНЕНИЯ 4.1. Вычислите детерминант, используя разложение Лапласа (возьмите первую строку) и правило Сарруса для 4.2. Эффективно вычислите следующий детерминант: 4.3. Вычислите собственные подпространства 4.4. Вычислите все собственные подпространства\n--- Страница 183 ---\nУпражнения 183 4.5. Диагонализуемость матрицы не связана с ее обратимостью. Определите для следующих четырех матриц, являются ли они диагонализуемыми и/или обра - тимыми. 4.6. Вычислите собственные подпространства следующих матриц преобразова - ния. Являются ли они диагонализуемыми? a. b. 4.7. Диагонализируемы ли следующие матрицы? Если да, определите их диа - гональную форму и базис, относительно которого матрицы преобразования являются диагональными. Если нет, укажите причины, по которым они не под - даются диагонализации. a. . b. c. d.\n--- Страница 184 ---\n184 Глава 4. Матричные разложения 4.8. Вычислите сингулярное разложение матрицы: 4.9. Вычислите сингулярное разложение 4.10. Найдите наилучшее приближение ранга 1. 4.11. Покажите, что для любого A ∈ m × n матрицы ATA и AAT имеют одинаковые ненулевые собственные значения. 4.12. Покажите, что при x ≠ 0 верна теорема 4.24, то есть покажите, что где σ1 — наибольшее сингулярное значение A ∈ m × n.\n--- Страница 185 ---\n5 Векторный анализ Многие алгоритмы машинного обучения оптимизируют целевую функцию от - носительно параметров модели, от которых зависит то, насколько хорошо модель описывает данные. Нахождение подходящих параметров можно сформулировать как задачу оптимизации (разделы 8.2 и 8.3). Примерами являются, в частности, (i) линейная регрессия (глава 9), используемая для задач приближения кри - вой, — в ней мы подбираем параметры (вес а) так, чтобы максимизировать правдоподобие; (ii) нейронные сети-автокодировщики, применяемые для по - нижения размерности и сжатия данных, — параметрами будут веса и смещения на каждом из уровней, и мы минимизируем ошибку восстановления повторным применением цепного правила; и (iii) смешанные гауссовы модели (глава 11), используемые для моделирования распределений данных, — в них мы оптими - зируем параметры (среднее и дисперсию) каждого из компонентов смеси, чтобы максимизировать правдоподобие модели. На рис. 5.1 показаны некоторые из /dollar.g00DC4 /dollar.g00DC2 0 2 4 x/dollar.g00DC4/dollar.g00DC2024y/n.g007C/K.g00AD/three.g0082/.notdef.g0107/equal.g00C8/.notdef.g010A/question.g009D/comma.g00D2/yright /.notdef.g0105/equal.g00C8……/slash.g00A9/yright MLE /dollar.g00DC10 /dollar.g00DC5 0 5 10 x1/dollar.g00DC10/dollar.g00DC50510x2 Рис. 5.1. Векторный анализ играет ключевую роль при ( a) регрессии (приближение кривой) и ( b) оценке плотностей (моделирование распределения данных). (a) Задача регрессии: найти такие параметры, чтобы кривая хорошо соответствовала наблюдениям (обозначенным крестиками). ( b) Оценка плотностей для смешанного гауссова распределения. Необходимо найти такие средние и дисперсии, которые соответствуют наблюдаемым данным (обозначенным точками)\n--- Страница 186 ---\n186",
      "debug": {
        "start_page": 136,
        "end_page": 186
      }
    },
    {
      "name": "Глава 5. Векторный анализ 185",
      "content": "--- Страница 186 --- (продолжение)\nГлава 5. Векторный анализ этих задач, которые мы обычно решаем с помощью алгоритмов оптимизации, основанных на вычислении градиента (раздел 7.1). Рисунок 5.2 дает представ - ление о том, как темы этой главы соотносятся между собой и с остальными главами книги. /uni0420/uni0430/uni0437/uni043D/uni043E/uni0441 /uni0442/uni043D/uni043E/uni0435 /uni043E/uni0442/uni043D/uni043E/uni0448/uni0435/uni043D/uni0438 /uni0435 /uni0427/uni0430/uni0441/uni0442/uni043D/uni044B/uni0435 /uni043F/uni0440/uni043E/uni0438/uni0437/uni0432/uni043E/uni0434/uni043D/uni044B/uni0435 /uni041C/uni0430/uni0442/uni0440/uni0438/uni0446/uni0430 /uni042F/uni043A/uni043E/uni0431/uni0438. /uni041C/uni0430/uni0442/uni0440/uni0438/uni0446/uni0430 /uni0413 /uni0435/uni0441/uni0441/uni0435 /uni0420/uni044F/uni0434 /uni0422/uni0435/uni0439/uni043B/uni043E/uni0440/uni0430/uni0413/uni043B/uni0430/uni0432/uni0430 7. /uni041E/uni043F/uni0442/uni0438/uni043C/uni0438/uni0437/uni0430/uni0446/uni0438 /uni044F /uni0413/uni043B/uni0430/uni0432/uni0430 6. /uni0412/uni0435/uni0440/uni043E/uni044F/uni0442 /uni043D/uni043E/uni0441/uni0442/uni044C/uni0413/uni043B/uni0430/uni0432/uni0430 9. /uni0420/uni0435/uni0433/uni0440/uni0435/uni0441/uni0441/uni0438/uni044F /uni0413/uni043B/uni0430/uni0432/uni0430 10. /uni041F/uni043E/uni043D/uni0438/uni0436/uni0435/uni043D/uni0438/uni0435 /uni0440/uni0430/uni0437/uni043C/uni0435/uni0440/uni043D/uni043E /uni0441/uni0442/uni0438 /uni0413/uni043B/uni0430/uni0432/uni0430 11. /uni041E/uni0446/uni0435/uni043D/uni043A/uni0430 /uni043F/uni043B /uni043E/uni0442/uni043D/uni043E/uni0441/uni0442/uni0438 /uni0440/uni0430/uni0441/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni0438/uni044F /uni0413/uni043B/uni0430/uni0432/uni0430 12. /uni041A/uni043B/uni0430/uni0441/uni0441/uni0438/uni0444/uni0438/uni043A/uni0430/uni0446/uni0438/uni044F /uni043E/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni044F/uni0435/uni0442 /uni0441/uni043E/uni0431/uni0440/uni0430/uni043D/uni043E /uni0432 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432/uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432/uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432/uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432/uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432 Рис. 5.2. Ассоциативная карта концепций, вводимых в этой главе, с указанием, в каких еще частях книги они фигурируют Основным в данной главе является понятие функции. Функция f — это прави - ло, связывающее две величины: — в нашей книге переменную x ∈ D и значение функции f(x) (которое мы считаем, если не оговорено иное, вещественным). Здесь D называется областью значений f, а множество всех значений f(x) — об- разом f. В разделе 2.7.3 гораздо более подробно обсуждаются линейные функции. При задании функции часто пишут f : D → ; (5.1а) , (5.1b)\nГлава 5. Векторный анализ этих задач, которые мы обычно решаем с помощью алгоритмов оптимизации, основанных на вычислении градиента (раздел 7.1). Рисунок 5.2 дает представ - ление о том, как темы этой главы соотносятся между собой и с остальными главами книги. /uni0420/uni0430/uni0437/uni043D/uni043E/uni0441 /uni0442/uni043D/uni043E/uni0435 /uni043E/uni0442/uni043D/uni043E/uni0448/uni0435/uni043D/uni0438 /uni0435 /uni0427/uni0430/uni0441/uni0442/uni043D/uni044B/uni0435 /uni043F/uni0440/uni043E/uni0438/uni0437/uni0432/uni043E/uni0434/uni043D/uni044B/uni0435 /uni041C/uni0430/uni0442/uni0440/uni0438/uni0446/uni0430 /uni042F/uni043A/uni043E/uni0431/uni0438. /uni041C/uni0430/uni0442/uni0440/uni0438/uni0446/uni0430 /uni0413 /uni0435/uni0441/uni0441/uni0435 /uni0420/uni044F/uni0434 /uni0422/uni0435/uni0439/uni043B/uni043E/uni0440/uni0430/uni0413/uni043B/uni0430/uni0432/uni0430 7. /uni041E/uni043F/uni0442/uni0438/uni043C/uni0438/uni0437/uni0430/uni0446/uni0438 /uni044F /uni0413/uni043B/uni0430/uni0432/uni0430 6. /uni0412/uni0435/uni0440/uni043E/uni044F/uni0442 /uni043D/uni043E/uni0441/uni0442/uni044C/uni0413/uni043B/uni0430/uni0432/uni0430 9. /uni0420/uni0435/uni0433/uni0440/uni0435/uni0441/uni0441/uni0438/uni044F /uni0413/uni043B/uni0430/uni0432/uni0430 10. /uni041F/uni043E/uni043D/uni0438/uni0436/uni0435/uni043D/uni0438/uni0435 /uni0440/uni0430/uni0437/uni043C/uni0435/uni0440/uni043D/uni043E /uni0441/uni0442/uni0438 /uni0413/uni043B/uni0430/uni0432/uni0430 11. /uni041E/uni0446/uni0435/uni043D/uni043A/uni0430 /uni043F/uni043B /uni043E/uni0442/uni043D/uni043E/uni0441/uni0442/uni0438 /uni0440/uni0430/uni0441/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni0438/uni044F /uni0413/uni043B/uni0430/uni0432/uni0430 12. /uni041A/uni043B/uni0430/uni0441/uni0441/uni0438/uni0444/uni0438/uni043A/uni0430/uni0446/uni0438/uni044F /uni043E/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni044F/uni0435/uni0442 /uni0441/uni043E/uni0431/uni0440/uni0430/uni043D/uni043E /uni0432 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432/uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432/uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432/uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432 /uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432/uni0438/uni0441/uni043F/uni043E/uni043B/uni044C/uni0437/uni0443/uni0435/uni0442/uni0441/uni044F /uni0432 Рис. 5.2. Ассоциативная карта концепций, вводимых в этой главе, с указанием, в каких еще частях книги они фигурируют Основным в данной главе является понятие функции. Функция f — это прави - ло, связывающее две величины: — в нашей книге переменную x ∈ D и значение функции f(x) (которое мы считаем, если не оговорено иное, вещественным). Здесь D называется областью значений f, а множество всех значений f(x) — об- разом f. В разделе 2.7.3 гораздо более подробно обсуждаются линейные функции. При задании функции часто пишут f : D → ; (5.1а) , (5.1b)\n--- Страница 187 ---\n5.1. Дифференцирование функций одной переменной 187 где (5.1 a) означает, что f — отображение из D в , а (5.1b) описывает правило, по которому входу (аргументу) x сопоставляется значение f(x). Функция f со- поставляет каждому x ровно одно значение f(x). Пример 5.1 Вспомним частный случай скалярного произведения — стандартное ска - лярное произведение (раздел 3.2.2). В представленных обозначениях f(x) = xTx, x ∈ 2 будет выглядеть как f : D → ; (5.2а) . (5.2b) В этой главе мы обсудим, как вычислять градиенты функций, что зачастую играет ключевую роль в процессе обучения моделей, так как направление гра - диента — это то направление, в котором функция быстрее всего возрастает. Поэтому векторный анализ является одним из главных математических инстру - ментов машинного обучения. В данной книге мы будем предполагать, что все функции дифференцируемы. С некоторыми техническими дополнениями, ко - торых мы касаться не будем, многие подходы можно обобщить до субдиффе - ренцируемых функций (непрерывных, но в некоторых точках не дифференци - руемых). Обобщение для случая некоторых ограничений на функцию мы рассмотрим в главе 7. 5.1. ДИФФЕРЕНЦИРОВАНИЕ ФУНКЦИЙ ОДНОЙ ПЕРЕМЕННОЙ В этом разделе мы кратко повторим дифференцирование функций одной пере - менной, знакомое, быть может, из старшей школы. Начнем с разностного от - ношения для функции одной переменной y = f(x), x, y ∈ , а затем определим производную. Определение 5.1 (разностное отношение). Разностное отношение (5.3) — это тангенс угла наклона секущей, проходящей через две точки на графике f. На рис. 5.3 это точки с x-координатами x0 и x0 + δx. Если f — линейная функция, то разностное отношение — это средний тангенс угла наклона секущей между x0 и x0 + δx. В пределе, если f дифференцируема,\n--- Страница 188 ---\n188 Глава 5. Векторный анализ при δx → 0 получится тангенс угла наклона касательной к f в точке x. Он и будет значением производной f в точке x. δy δx xf(x0 + δx) f(x0)f(x)y Рис. 5.3. Средний наклон графика функции f между точками x0 и x0 + δx равен наклону секущей, проходящей через f(x0) и f(x0 + δx), и задается формулой δy/δx Определение 5.2 (производная). Для h > 0 производная f в точке x — это предел (5.4) и секущая на рис. 5.3 превращается в касательную. Пример 5.2 (производная многочлена) Пусть нужно вычислить производную многочлена f(x) = xn, n ∈ . Воз- можно, вы уже знаете, что ответом будет nxn–1, но давайте выведем его из определения производной как предела разностных отношений. По фор - муле (5.4) получаем (5.5a) (5.5b) (5.5c)\n--- Страница 189 ---\n5.1. Дифференцирование функций одной переменной 189 Заметим, что . Когда суммирование начинается с 1, слагае- мое xn уходит, и мы получаем (5.6a) (5.6b) (5.6c) (5.6d) 5.1.1. Ряд Тейлора Ряд Тейлора — это представление функции f в виде бесконечной суммы. Члены этой суммы определены через производные f в точке x0. Определение 5.3 (многочлен Тейлора). Многочлен Тейлора степени n функ - ции f в точке x0 определяется как (5.7) где fk(x0) — k-я производная функции f в точке x0 (мы предполагаем, что она существует), а — коэффициенты многочлена1. Определение 5.4 (ряд Тейлора). Для гладкой функции f ∈ C ∞ (f ∈ C ∞ означает, что f непрерывно дифференцируема бесконечно много раз ), f :  → , ряд Тейлора функции f в точке x0 определяется как (5.8) 1 Мы определяем t0:= 1 для всех t ∈ .\n--- Страница 190 ---\n190 Глава 5. Векторный анализ Для x0 = 0 получаем частный случай ряда Тейлора — ряд Маклорена. Если f(x) = = T∞C(x), f называют аналитической функцией. ПРИМЕЧАНИЕ Многочлен Тейлора степени n — это аппроксимация функ - ции f, вообще говоря, не полиномиальной. Его значения близки к значениям f в окрестности точки x0. Однако если f — полиномиальная функция степени k ≤ n, то многочлен Тейлора степени n совпадает с ней, поскольку все производные f(i), i > k, обращаются в нуль.  Пример 5.3 (многочлен Тейлора) Рассмотрим полиномиальную функцию f (x) = x4 (5.9) и построим многочлен Тейлора T6 в точке x0 = 1. Сначала вычислим ко - эффициенты f(k)(1) для k = 0, …, 6: f (1) = 1; (5.10) ; (5.11) ; (5.12) f(3)(1) = 24; (5.13) f(4)(1) = 24; (5.14) f(5)(1) = 0; (5.15) f(6)(1) = 0. (5.16) Таким образом, искомый многочлен Тейлора равен (5.17 a) = 1 + 4( x – 1) + 6( x – 1)2 + 4(x – 1)3 + (x – 1)4 + 0. (5.17 b) Раскрывая скобки и приводя подобные слагаемые, получим T6(x) = (1 − 4 + 6 − 4 + 1)+ x(4 − 12 + 12 − 4) + + x2(6 − 12 + 6) + x3(4 − 4) + x4 = (5.18 a) = x4 = f(x), (5.18 b) то есть в точности исходную функцию.\n--- Страница 191 ---\n5.1. Дифференцирование функций одной переменной 191 Пример 5.4 (ряд Тейлора) Рассмотрим функцию с рис. 5.4, заданную формулой f (x) = sin(x) + cos( x) ∈  ∞. (5.19) Найдем разложение f в ряд Тейлора в точке x0 = 0 (то есть разложение Маклорена). Производные выглядят так: f (0) = sin(0) + cos(0) = 1; (5.20) ; (5.21) ; (5.22) f(3)(0) = – cos(0) + sin(0) = –1; (5.23) (5.24) Видим закономерность: так как sin(0) = 0, то коэффициенты нашего ряда Тейлора — это только ±1, повторяющиеся по два раза, и f(k+4)(0) = = f(k)(0). Таким образом, разложение f в ряд Тейлора в точке x0 = 0 вы - глядит как (5.25 a) (5.25 b) (5.25 c) (5.25 d) (5.25 e) — последнее равенство получается из разложений в степенные ряды (5.26) (5.27)\n--- Страница 192 ---\n192 Глава 5. Векторный анализ На рис. 5.4 показаны многочлены Тейлора Tn для n = 0, 1, 5, 10. –4 –2 00 22 44 x–2 yf T0 T1 T5 T10 Рис. 5.4. Многочлены Тейлора. Исходная функция f(x) = sin(x) + + cos(x) (сплошная черная линия) приближается многочле- ном Тейлора (пунктирная линия) в окрестности точки x 0 = 0. Многочлены Тейлора более высоких порядков приближают f точнее и на большем участке. T 10 близка к f на [−4, 4] ПРИМЕЧАНИЕ Ряд Тейлора — частный случай степенного ряда , (5.28) где ak — коэффициенты (имеющие в определении 5.4 специальный вид), c — константа.  5.1.2. Правила дифференцирования В этом разделе мы сформулируем основные правила дифференцирования. Про - изводную функции f мы обозначаем . Производная произведения: . (5.29) Производная частного: . (5.30) Производная суммы: . (5.31) Цепное правило . (5.32) (производная композиции): Здесь обозначает композицию функций .\n--- Страница 193 ---\n5.2. Частные производные и градиенты 193 Пример 5.5 (цепное правило) Используя цепное правило, вычислим производную функции h(x) = = (2x + 1)4. Так как h (x) = (2x + 1)4 = g(f(x)), (5.33) f (x) = 2x + 1, (5.34) g (f) = f4, (5.35) и производные f и g (5.36) (5.37) получаем производную h: (5.38) (мы применили правило (5.32) и подставили в определение функ - ции f). 5.2. ЧАСТНЫЕ ПРОИЗВОДНЫЕ И ГРАДИЕНТЫ В разделе 5.1 мы обсуждали дифференцирование функций одной скалярной пере - менной x ∈ . В данном разделе мы рассмотрим общий случай, в котором f зависит от одной или нескольких переменных x ∈ n, например f(x) = f(x1, x2). Обобщени - ем производной для функций нескольких переменных является градиент. Мы фиксируем значения всех переменных, кроме одной, относительно которой и вычисляем производную. Набор таких частных производных по всем пере - менным и будет называться градиентом. Определение 5.5 (частная производная). Для функции f : n → , , x ∈ n от n переменных x1, , xn частные производные определяются как (5.39)\n--- Страница 194 ---\n194 Глава 5. Векторный анализ Объединим их в вектор-строку (5.40) где n — количество переменных, а 1 — размерность области значений f. Здесь мы рассматриваем вектор-столбец x = [x1, , xn]T. Вектор-строка из (5.40) на - зывается градиентом или матрицей Якоби функции (якобианом) f и обобщает понятие производной из раздела 5.1. ПРИМЕЧАНИЕ Это определение якобиана является частным случаем обще - го определения якобиана для векторнозначных функций — как набора частных производных. К этому понятию мы вернемся в разделе 5.3.  Пример 5.6 (Вычисление частных производных по цепному правилу) Для f(x, y) = (x + 2y3)2 частные производные равны (5.41) (5.42) — мы используем для их вычисления цепное правило (5.32)1. ПРИМЕЧАНИЕ Нередко в литературе градиент определяется как вектор- столбец, в соответствии с наиболее общепринятым способом записи векторов. Мы определяем градиент как вектор-строку по двум причинам. Во-первых, это делает более логичным обобщение градиента на векторнозначные функции f : n → m (тогда он становится матрицей). Во-вторых, тогда мы можем при - менять многомерное цепное правило вне зависимости от размерности градиен - та. Оба эти соображения мы обсудим в разделе 5.3.  Пример 5.7 (градиент) Для функции частные производные (то есть производные f относительно x1 и x2) равны ; (5.43) 1 Можно использовать правила для скалярного дифференцирования: каждая частная производная — это производная относительно скаляра.\n--- Страница 195 ---\n5.2. Частные производные и градиенты 195 (5.44) и градиент соответственно равен (5.45) 5.2.1. Основные правила взятия частных производных В случае функции нескольких переменных, когда x ∈ n, по-прежнему приме - нимы основные правила дифференцирования (для произведения, для суммы, для композиции), известные нам со школы. Однако при вычислении производ- ных относительно x ∈ n необходимо быть внимательными: градиенты теперь являются векторами или матрицами, а умножение матриц некоммутативно (раздел 2.2.1), то есть порядок множителей важен. Вот как обобщаются правила вычисления производных произведения, суммы и композиции1: Производная произведения: . (5.46) Производная суммы: . (5.47) Цепное правило (производная . (5.48) композиции): Рассмотрим цепное правило подробнее. Правило (5.48) несколько напоминает правило умножения матриц, где для существования произведения размеры матриц должны быть согласованы (раздел 2.2.1). Свойства цепного правила похожи: проходя по нему слева направо, мы встречаем ∂f в «знаменателе» первого сомножителя и в «числителе» второго2. Умножение определено: ∂f «со- кращается», остается ∂g / ∂x. 1 Производная произведения: ( fg)′ = f ′g + fg ′, производная суммы: ( f + g)′ = f ′ + g′, про- изводная композиции: ( g(f ))′ = g′ (f )f ′. 2 На самом деле такая аналогия некорректна: производная — это не дробь.\n--- Страница 196 ---\n196 Глава 5. Векторный анализ 5.2.2. Цепное правило Рассмотрим функцию f : 2 →  от двух переменных x1, x2, которые, в свою очередь, являются функциями от t: x1(t), x2(t). Чтобы найти градиент f относи - тельно t, мы применяем цепное правило (5.48) для функций нескольких пере - менных: (5.49) где d обозначает градиент, а ∂ — частную производную. Пример 5.8 Рассмотрим функцию , где x1 = sin(t) и x2 = cos(t). Тогда (5.50 a) (5.50 b) (5.50 c) — производная f относительно t. Если f(x1, x2) — функция от x1 и x2, где x1(s, t) и x2(s, t) — функции двух пере - менных s и t, по цепному правилу вычисляем производные ; (5.51) , (5.52) и градиент получается умножением матриц (5.53)\n--- Страница 197 ---\n5.3. Градиенты векторнозначных функций 197 Этот краткий способ записи цепного правила как умножения матриц имеет смысл только в том случае, когда градиент записывается в виде строки. Иначе для согласованности размеров матриц нам пришлось бы транспонировать гра - диент. Пока градиент является вектором или матрицей, это просто, но когда градиент становится тензором (что мы обсудим в дальнейшем), транспониро - вание становится нетривиальной задачей. ПРИМЕЧАНИЕ Определение частной производной как предела соответству - ющего разностного отношения (5.39) можно использовать для проверки, пра - вильно ли реализовано нахождение градиентов в компьютерной программе. С помощью конечных разностей можно протестировать процедуру нахождения градиентов: взять маленькое h (например, h = 10–4) и сравнить приближение конечными разностями (5.39) с нашей аналитической реализацией градиента. Если ошибка мала, наша реализация градиента, скорее всего, верна. «Малой» ошибкой можно считать, например , где dhi — приближение конечными разностями, а dfi — аналитические значения частной производной относительно переменной xi.  5.3. ГРАДИЕНТЫ ВЕКТОРНОЗНАЧНЫХ ФУНКЦИЙ До сих пор мы обсуждали частные производные и градиенты функций f : n → , принимающих вещественные значения. В данном разделе мы обобщим понятие градиента на векторнозначные функции (векторные поля) f : n → m, где и m > 1. Функции f : n → m и вектору x = [x1, , xn]T ∈ n соответствует вектор значений функции (5.54) Запись векторнозначной функции в таком виде позволяет нам рассматривать векторнозначную функцию f : n → m как вектор [ f1, , fm]T из функций fi : n → , принимающих вещественные значения. Каждая из функций fi диф- ференцируется по правилам из раздела 5.2. Таким образом, частная производная векторнозначной функции f : n → m от- носительно xi ∈ , i = 1, …, n, задается формулой\n--- Страница 198 ---\n198 Глава 5. Векторный анализ (5.55) По формуле (5.40) градиент функции f относительно вектора — вектор-строка из частных производных. В формуле (5.55) каждая частная производная ∂f / ∂xi является вектором-столбцом. Собрав вместе все эти частные производные, получим градиент f : n → m относительно x ∈ n: (5.56a) (5.56b) Определение 5.6 (матрица Якоби). Набор всех частных производных первого порядка векторнозначной функции f : n → m называется матрицей Якоби. Матрица Якоби J — это матрица размера m × n, определяемая как (5.57) (5.58) (5.59)\n--- Страница 199 ---\n5.3. Градиенты векторнозначных функций 199 В частном случае (5.58) функции f : n → 1, отображающей вектор x ∈ n в ска- ляр (например, ), по формуле (5.58) получаем вектор-строку (ма - трицу размера 1 × n), см. (5.40). ПРИМЕЧАНИЕ В нашей книге мы используем способ записи производной векторнозначной функции f, при котором компоненты f соответствуют строкам, а компоненты x — столбцам. Существует и другой способ, при котором мы полу - чаем транспонированную матрицу по отношению к описанной.  В разделе 6.7 мы увидим, как использовать матрицу Якоби в методе замены переменной для распределения вероятностей. Ее детерминант (якобиан) задает коэффициент, на который придется домножать при замене переменной. Из раздела 4.1 мы узнали, что с помощью детерминанта можно вычислить пло - щадь параллелограмма. Если b1 = [1,0]T, b2 = [0,1]T — стороны единичного ква - драта (слева на рис. 5.5), площадь этого квадрата равна (5.60) Для параллелограмма c1 = [–2,1]T, c2 = [1,1]T (справа на рис. 5.5) площадь равна абсолютной величине детерминанта (раздел 4.1): (5.61) то есть его площадь в три раза больше площади единичного квадрата. Этот множитель можно найти, рассмотрев отображение, переводящее единичный квадрат в параллелограмм. На языке линейной алгебры, мы переходим от бази - са (b1, b2) к (c1, c2). Замена переменных линейна, и абсолютная величина детер - минанта отображения дает в точности искомый множитель. f(·) b2 b1c2c1 Рис. 5.5. Якобиан f можно использовать для нахождения/отыскания, во сколько раз площадь фигуры справа больше, чем фигуры слева\n--- Страница 200 ---\n200 Глава 5. Векторный анализ Рассмо трим два подхода к поиску такого отображения. Первый подход исполь - зует его линейность, так что мы можем применять инструменты из главы 2. Вто - рой, основанный на частных производных, использует материал из данной главы. Подход 1. Подходя с точки зрения линейной алгебры, мы для начала отметим, что {b1, b2} и {c1, c2} — базисы 2 (раздел 2.6.1). Фактически мы осуществляем замену базиса ( b1, b2) на ( c1, c2), и нужно найти матрицу перехода. Результаты раздела 2.7.2 позволяют нам найти матрицу замены базиса: (5.62) где Jb1 = c1 и Jb2 = c2. Абсолютная величина детерминанта J, дающая искомый множитель, равна |det( J)| = 3, то есть площадь параллелограмма со сторонами (c1, c2) втрое больше, чем квадрата со сторонами ( b1, b2). Подход 2. Описанный ранее подход работает для линейных преобразований, для нелинейных же преобразований (которые понадобятся нам в разделе 6.7) будем следовать более общему методу, использующему частные производные. Рассмотрим функцию f : 2 → 2, осуществляющую замену переменных. В на- шем примере f сопоставляет координатам вектора x ∈ 2 в базисе ( b1, b2) его координаты в базисе. Нам хочется описать это отображение так, чтобы можно было вычислить изменение площади (или объема) при его применении. Для этого надо выяснить, как меняется f(x) при малом изменении x. На этот вопрос легко ответить, используя матрицу Якоби . Так как можно записать y1 = −2x1 + x2; (5.63) y2 = x1 + x2, (5.64) получаем функциональную зависимость между x и y, что позволяет нам вы - числить частные производные (5.65) и записать матрицу Якоби (5.66) Матрица Якоби описывает искомое преобразование координат. Оно является точным, если преобразование координат линейно (как в нашем случае), и (5.66) дает матрицу перехода от базиса к базису (5.62). Если же преобразование коор -\n--- Страница 201 ---\n5.3. Градиенты векторнозначных функций 201 динат нелинейно, матрица Якоби локально приближает его линейным преобра - зованием. Абсолютная величина якобиана |det( J)| будет множителем, на который изменится площадь или объем при замене координат. В нашем примере | det(J) | = 3. Якобиан и замены переменных будут нужны нам в разделе 6.7 для работы со случайными величинами и распределениями вероятностей. Эти замены чрез - вычайно важны для машинного обучения, а именно для глубоких нейросетей при проведении репараметризации. В данной главе мы познакомились с производными функций. Рисунок 5.6 ил - люстрирует их размерности. x f(x) /g119f /g119x Рис. 5.6. Размерности частных производных Если f :  → , то градиент — это просто скаляр (на рисунке в левом верхнем углу). Для f : D →  градиентом будет вектор-строка (матрица размера 1 × D), для f :  → E — вектор-столбец (матрица размера E × 1). Наконец, если f : D → E, градиент будет матрицей размера E × D. Пример 5.9 (градиент векторнозначной функции) Пусть нам даны f(x) = Ax, f(x) ∈ M, A ∈ M×N, x ∈ N. Чтобы вычислить градиент d f / dx, сначала определим его размерность. Мы знаем, что f : N → M, значит d f / dx ∈ M × N. Затем найдем частные производные функции f относительно каждой из переменных xj: (5.67) Запишем частные производные в виде матрицы Якоби и получим градиент (5.68)\n--- Страница 202 ---\n202 Глава 5. Векторный анализ Пример 5.10 (цепное правило) Рассмотрим функцию h :  → , h(t) = ( )(t), где f : 2 → ; (5.69) g :  → 2; (5.70) (5.71) (5.72) и найдем градиент f относительно t. Так как f : 2 →  и g :  → 2, то (5.73) Получаем искомый градиент, применяя цепное правило: (5.74 a) (5.74 b) (5.74 c) где x1 = t cost и x2 = t sint (5.72). Пример 5.11 (градиенты квадратичной функции потерь в линейной модели) Рассмотрим линейную модель1 y = Φθ, (5.75) где θ ∈ D — вектор параметров, Φ ∈ N × D — признаки, а y ∈ N — соот - ветствующие наблюдения. Определим функции 1 Эту модель мы рассмотрим подробнее в главе 9, где нам понадобятся производные функции потерь относительно параметров.\n--- Страница 203 ---\n5.3. Градиенты векторнозначных функций 203 (5.76) e(θ) = y – Φθ. (5.77) Мы хотим найти и будем использовать для этого цепное правило. L называется квадратичной функцией потерь. Сначала определим размерность градиента: (5.78) Цепное правило позволяет нам вычислить его по формуле (5.79) где d-й компонент равен (5.80) Мы знаем, что || e||2 = eTe (раздел 3.2), тогда (5.81) Далее получаем (5.82) и искомая производная равна (5.83) ПРИМЕЧАНИЕ Тот же результат можно было получить и без применения цепного правила, сразу рассмотрев функцию L2(θ) := ||y – Φθ||2 = (y − Φθ)T(y − Φθ). (5.84) Этот подход удобен для простых функций вроде L2, но плохо подходит для сложных композиций. \n--- Страница 204 ---\n204 Глава 5. Векторный анализ 5.4. ГРАДИЕНТЫ МАТРИЦ Нам встретятся ситуации, в которых понадобится вычислять градиенты матриц относительно векторов или других матриц, получая в итоге тензоры. Такой тензор можно воспринимать как многомерный массив частных производных. Например, чтобы вычислить градиент m × n-матрицы A относительно p × q-матрицы B, матрица Якоби будет размера ( m × n) × (p × q), то есть окажет - ся четырехмерным тензором J, элементы которого равны Jijkl = ∂Aij / ∂Bkl. Матрицы соответствуют линейным отображениям. Можно воспользоваться изоморфизмом векторных пространств (то есть обратимым линейным ото - бражением между ними) — пространства m × n-матриц m × n и пространства mn-векторов mn. Таким образом, наши матрицы превратятся в векторы длин mn и pq соответственно. Градиентом, таким образом, окажется матрица раз - мера mn × pq. На рис. 5.7 показаны оба подхода. На практике часто разумно превратить матрицу в вектор1 и дальше работать с его матрицей Якоби. Цеп - ное правило (5.48) тогда сводится к умножению матриц, в то время как при работе с тензорами Якоби понадобится внимательно следить за размер - ностями. Пример 5.12 (градиенты векторов относительно матриц) Рассмотрим такой пример: f = Ax, f ∈ M, A ∈ M×N, x ∈ N. (5.85) Пусть мы хотим найти градиент d f/dA. Начнем с выяснения его размер - ности: (5.86) По определению, градиент будет набором частных производных (5.87) 1 Матрицу можно превратить в вектор, записав ее столбцы друг под другом (вектори - зация матрицы).\n--- Страница 205 ---\n5.4. Градиенты матриц 205 Чтобы вычислить эти частные производные, вспомним формулу умно - жения матрицы на вектор: (5.88) Тогда частные производные равны (5.89) Это позволит нам вычислить частные производные fi относительно стро - ки A (5.90) (5.91) здесь важно следить за размерностью! Так как fi принимают значения в , а каждая строка матрицы A имеет размер 1 × N, частной производной fi относительно строки A будет тензор размера 1 × 1 × N. Соберем вместе частные производные из (5.91) и получим искомый гра - диент (5.87): (5.92)\n--- Страница 206 ---\n206 Глава 5. Векторный анализ x1 x2 x3A ∈ R4×2 /g119A /g119x3∈ R4×2 /g119A /g119x2∈ R4×2 /g119A /g119x1∈ R4×2dA dx∈ R4×2×3x ∈ R3 4 23/parenright.g00A0/equal.g00C8“/two.g0088…/slash.g00A9/yright /C.g00B9/exclam.g00AF/percent.g00BA/comma.g00D2ƒ/quotedbl.g006D/percent.g00BA/.notdef.g0105…/slash.g00A9/yright: /percent.g00BA/K.g00AD/A.g009E/yright/.notdef.g0105/comma.g00D2- …/comma.g00D2/two.g0088/.notdef.g0109 Рис. 5.7. Вычисление градиента матрицы относительно вектора. Мы хотим вычислить градиент A ∈  4 × 2 относительно вектора x ∈ 3. Мы знаем, что градиент . Используем два эквивалентных подхода: (a) собираем частные производные в тензор Якоби; (b) преобразуем матрицу в вектор, ищем матрицу Якоби, преобразуем ее в тензор (a) Подход 1: вычисляем частные производные ∂A/∂x1, ∂A/∂x2, ∂A/∂x3, каждая из которых будет матрицей 4 × 2, и объединяем их в тензор 4 × 2 × 3 /comma.g00D2ƒ/.notdef.g00E4/yright…/comma.g00D2/two.g0088/.notdef.g0109 /exclam.g00AF/equal.g00C8ƒ/.notdef.g00E4/yright/exclam.g00AF…/percent.g00BA“/two.g0088/.notdef.g0109/comma.g00D2ƒ/.notdef.g00E4/yright…/comma.g00D2/two.g0088/.notdef.g0109 /exclam.g00AF/equal.g00C8ƒ/.notdef.g00E4/yright/exclam.g00AF…/percent.g00BA“/two.g0088/.notdef.g0109/.notdef.g0104/exclam.g00AF/equal.g00C8/.notdef.g0105/comma.g00D2/yright…/two.g0088x1 x2 x3A ∈ R4×2 A ∈ R4×2A ∈ R8 ~ ∈ R8×3dA~ dxdA dx∈ R4×2× 3x ∈ R3 (b) Подход 2: превращаем A ∈ 4 × 2 в вектор Ã ∈ 8, вычисляем градиент ∂Ã/∂x и делаем из него тензор\n--- Страница 207 ---\n5.4. Градиенты матриц 207 Пример 5.13 (градиент матрицы относительно матрицы) Рассмотрим матрицу R ∈ M×N и f : M×N → N×N, где f (R) = RTR =: K ∈ N × N, (5.93) и будем искать градиент dK / dR. Приступая к этой непростой задаче, запишем, что мы знаем. Градиент имеет размерность (5.94) то есть является тензором. Далее, (5.95) для p, q = 1, , N, где Kpq — это ( p, q)-й элемент K = f (R). Обозначим i-й столбец R через ri и заметим, что каждый элемент K — скалярное произ - ведение двух столбцов R, то есть (5.96) Посчитав частную производную , получим (5.97) (5.98) Из формулы (5.94) знаем, что размерность искомого градиента — (N × N) × (M × N), а каждый элемент этого тензора равен ∂pqij из (5.98), где p, q, j = 1, , N и i = 1, , M.\n--- Страница 208 ---\n208 Глава 5. Векторный анализ 5.5. ПОЛЕЗНЫЕ ТОЖДЕСТВА ДЛЯ ВЫЧИСЛЕНИЯ ГРАДИЕНТОВ В этом разделе мы перечислим некоторые полезные тождества, часто требую - щиеся в машинном обучении (Petersen and Pedersen, 2012). Здесь tr( ·) — след матрицы (см. определение 4.4), det( ·) — ее детерминант (раздел 4.1) и f(X)–1 — обратная функция к f(X) (если она существует). ; (5.99) ; (5.100) ; (5.101) ; (5.102) ; (5.103) ; (5.104) ; (5.105) ; (5.106) ; (5.107) . (5.108) для симметричной W ПРИМЕЧАНИЕ В нашей книге мы обсуждали транспонирование и след толь - ко для матриц. Однако, как мы видим, производные могут быть тензорами большей размерности, так что обычные определения этих операций здесь не годятся. В этом случае след D × D × E × F тензора будет E × F-матрицей. Это частный случай тензорного сжатия. Аналогично при «транспонировании» тен -\n--- Страница 209 ---\n5.6. Обратное распространение ошибки и автоматическое дифференцирование 209 зора мы меняем местами две первых размерности. В формулах (5.99)–(5.102), если нам нужно вычислять производные векторнозначных функций f(·) отно - сительно матриц (не превращая их в векторы, как в разделе 5.4), нам нужны тензоры.  5.6. ОБРАТНОЕ РАСПРОСТРАНЕНИЕ ОШИБКИ И АВТОМАТИЧЕСКОЕ ДИФФЕРЕНЦИРОВАНИЕ Во многих задачах машинного обучения мы ищем подходящие параметры мо - дели методом градиентного спуска (раздел 7.1), основанным на вычислении градиента целевой функции относительно параметров модели. Для заданной целевой функции вычислить этот градиент можно аналитическими методами, применяя цепное правило (раздел 5.2.2). Мы уже видели пример в разделе 5.3, когда рассматривали градиент квадратичной ошибки относительно параметров линейной регрессионной модели. Рассмотрим функцию (5.109) Применяя цепное правило и пользуясь линейностью дифференцирования, получим (5.110) Записывать градиент в таком явном виде не всегда разумно — выражение для производной будет слишком длинным. На практике из-за этого нахождение формулы для градиента может быть значительно сложнее, чем вычисление функции, что требует лишних ресурсов. Для глубоких нейронных сетей эффек - тивным методом вычисления градиента функции потерь относительно парамет- ров модели является алгоритм обратного распространения ошибки1 (Kelley, 1960; Bryson, 1961; Dreyfus, 1962; Rumelhart et al., 1986). 1 Подробное обсуждение алгоритма обратного распространения ошибки и цепного пра - вила можно найти в блоге Тима Виеры https://tinyurl.com/ycfm2yrw .\n--- Страница 210 ---\n210 Глава 5. Векторный анализ 5.6.1. Градиенты в глубоких нейронных сетях Глубокое обучение — область, где цепное правило используется на каждом шагу, поскольку в нем значение функции y вычисляется как композиция большого количества функций (5.111) где x — входные переменные (например, изображения), y — выходы (например, метки классов), и каждая из функций fi, i = 1, …, K, имеет свои собственные па - раметры. В нейронных сетях с большим числом уровней на i-м уровне находят - ся функции fi(xi–1) = σ(Ai–1 + bi–1). Здесь xi–1 — выход i – 1-го слоя, а σ — функция активации1, например логистический сигмоид , tanh или ReLU. Для обу- чения таких моделей нам нужен градиент функции потерь L относительно всех параметров модели Aj, bj, j = 1, …, K. Это потребует вычисления градиента L от- носительно входов каждого из слоев. Например, если x — входы, y — выходы, а структура сети задается формулами f0 := x; (5.112) fi := σi(Ai–1 fi–1 + bi–1), i = 1, , K, (5.113) (см. рис. 5.8), нам нужно найти такие Aj, bj, j = 1, … K, которые минимизируют квадратичную функцию потерь L (θ) = || y − fK(θ, x) ||2, (5.114) где θ = {A0, b0, , AK–1, bK–1}. f1fK–1fK L x A0, b0A1, b1AK–2, bK–2AK–1, bK–1 Рис. 5.8. Прямой проход по глубокой нейронной сети, где ошибка L — функция входов x и параметров Ai, bi 1 Чтобы упростить обозначения, мы считаем функции активации в каждом слое одина - ковыми.\n--- Страница 211 ---\n5.6. Обратное распространение ошибки и автоматическое дифференцирование 211 f1fK–1fK L x A0, b0A1, b1AK–2, bK–2AK–1, bK–1 Рис. 5.9. Обратный проход по глубокой нейронной сети для вычисления градиентов функции потерь Чтобы найти градиент относительно набора параметров θ, нужны частные про - изводные функции L относительно параметров θj = {Aj, bj} для каждого уровня j = 0, , K − 1. Мы находим их по цепному правилу1: ; (5.115) ; (5.116) ; (5.117) . (5.118) Если мы нашли частные производные ∂L/∂θi+1,можно переиспользовать резуль - таты предыдущих вычислений для нахождения ∂L/∂θi. Дополнительные слага - емые, которые нужно вычислить, обведены в рамку. На рис. 5.9 показано, как градиент движется обратно по сети. 5.6.2. Автоматическое дифференцирование На самом деле, метод обратного распространения ошибки является частным случаем более общего численного метода, называемого автоматическим диф ­ ференцированием . Автоматическое дифференцирование можно считать набором приемов, позволяющих найти по возможности точное численное значение (а не символьную формулу) градиента или функции, применяя цепное правило. При автоматическом дифференцировании последовательно применяются элемен - тарные арифметические операции (такие как сложение и умножение) и элемен - тарные функции (например, sin, cos, exp, log). Применяя к этим операциям 1 Более подробно градиенты в нейронных сетях обсуждаются в лекциях Дж. Домке: https://tinyurl.com/yalcxgtv .\n--- Страница 212 ---\n212 Глава 5. Векторный анализ цепное правило, можно автоматически вычислять градиенты весьма сложных функций. Автоматическое дифференцирование1 реализуется в общем виде и может применяться как в прямом, так и в обратном порядке. В Baydin et al. (2018) дан замечательный обзор применений автоматического дифференциро - вания в машинном обучении. На рис. 5.10 изображен граф, показывающий, как мы приходим от входов x к выходам y через промежуточные значения a, b. Чтобы найти производную dy/dx, применим цепное правило и получим (5.119) x a b y Рис. 5.10. Граф потока данных с входом x, выходом y и промежуточными значениями a, b Интуитивно понятно, что прямой и обратный порядок вычислений различают - ся порядком проведения умножения2. Благодаря ассоциативности умножения матриц можно выбирать между и (5.120) (5.121) Равенство (5.120) соответствует обратному порядку , так как градиенты рас - пространяются по графу противоположно потоку данных. Равенство (5.121) описывает прямой порядок , при котором градиенты движутся по графу слева направо вместе с данными. Далее мы сосредоточимся на автоматическом дифференцировании в обратном порядке, то есть на алгоритме обратного распространения ошибок. Для нейрон - ных сетей, где размерность входа зачастую во много раз больше размерности выхода, обратный режим вычислительно много проще прямого. Начнем с показательного примера. 1 Автоматическое дифференцирование отличается от символьного дифференцирова - ния и обычных численных методов нахождения градиента (например, через конечные разности). 2 В общем случае матрица Якоби может быть вектором или, напротив, тензором Якоби.\n--- Страница 213 ---\n5.6. Обратное распространение ошибки и автоматическое дифференцирование 213 Пример 5.14 Рассмотрим функцию (5.122) из (5.109). Если нам придется реализовывать f программно, мы сможем ускорить вычисления, сохраняя промежуточные значения : a = x2; (5.123) b = exp( a); (5.124) c = a + b; (5.125) ; (5.126) e = cos(c); (5.127) f = d + e. (5.128) Идея здесь такая же, как при использовании цепного правила. Заметим, что эти вычисления требуют меньше операций, чем реализация функ - ции f(x) по определению (5.109). Соответствующий граф вычислений на рис. 5.11 показывает поток данных и вычисления, благодаря которым мы получаем значения f. Этот набор равенств, включающий промежуточные значения, можно рас - смотреть как граф вычислений. Такое представление часто используется в нейросетевых библиотеках. Производные промежуточных переменных относительно их входов можно вычислить напрямую по определениям производных элементарных функций. Получаем ; (5.129) ; (5.130) ; (5.131) ; (5.132) ; (5.133) (5.134)\n--- Страница 214 ---\n214 Глава 5. Векторный анализ Рассмотрев граф вычислений с рис. 5.11, можно вычислить ∂f/∂x, двига - ясь от выхода назад: ; (5.135) ; (5.136) ; (5.137) (5.138) Заметим, что для вычисления ∂f / ∂x мы неявно применяли цепное пра - вило. Подставляя производные элементарных функций, получаем ; (5.139) ; (5.140) ; (5.141) (5.142) Принимая каждую из производных за новую переменную, видим, что вычисление производных имеет ту же сложность, что и всей функции. Это довольно контринтуитивно, так как формула для производной ∂f/∂x (5.110) выглядит существенно сложнее, чем для функции f(x) (5.109). x aexp(·) (·)2b + c· d e+ f cos(·) Рис. 5.11. Граф вычислений с входом x, выходом f и промежуточными значениями a, b, c, d, e\n--- Страница 215 ---\n5.7. Производные высших порядков 215 Автоматическое дифференцирование формализует пример 5.14. Пусть x1, …, xd — входы функции, xd+1, …, xD–1 — промежуточные переменные, а xD — выход. Тогда граф вычислений задается как для i = d + 1, , , (5.143) где gi(·) — элементарные функции, а — родительские узлы в графе для переменных xi. Вычислить функцию, заданную таким образом, можно пошаго - во по цепному правилу. Так как по определению f = xD, то (5.144) Применяем для других переменных xi цепное правило: (5.145) где Pa( xj) — множество родительских узлов для xj в графе вычислений. Уравне - ние (5.143) задает прямое распространение функции, а (5.145) — обратное рас - пространение градиента по графу вычислений. При обучении нейронных сетей ошибка предсказания выхода распространяется обратно. Автоматическое дифференцирование1 применимо, когда функцию можно пред - ставить в виде графа вычислений, в котором элементарные функции диффе - ренцируемы. На самом деле, функцией можно считать и компьютерную про - грамму. Однако не все программы можно автоматически дифференцировать, например может не быть элементарных дифференцируемых функций. Такие конструкции программирования, как циклы и условные операторы, также тре - буют особого внимания. 5.7. ПРОИЗВОДНЫЕ ВЫСШИХ ПОРЯДКОВ До сих пор мы обсуждали градиенты, то есть производные первого порядка. Иногда нам нужны производные высших порядков, например при использова - нии метода оптимизации Ньютона — второго порядка (Nocedal and Wright, 2006). В разделе 5.1.1 мы обсуждали приближение функций многочленами с помощью рядов Тейлора. То же самое можно сделать и в случае нескольких переменных. Начнем, однако, с обозначений. 1 При автоматическом дифференцировании в обратном порядке необходимо строить дерево парсинга.\n--- Страница 216 ---\n216 Глава 5. Векторный анализ Рассмотрим функцию f : 2 →  от двух переменных x и y. Для частных произ - водных и градиентов мы будем использовать следующие обозначения: 1. — вторая частная производная f относительно x. 2. — n-я частная производная f относительно x. 3. — частная производная, полученная дифференцированием f сначала относительно x, а затем относительно y. 4. — частная производная, полученная дифференцированием f сначала относительно y, а затем относительно x. Матрицей Гессе называется набор всех частных производных второго порядка. Если f (x, y) дважды непрерывно дифференцируема, то (5.146) т. е. порядок взятия производных не важен, и соответствующая матрица Гессе (5.147) симметрична. Матрица Гессе обозначается . Вообще говоря, для f : n →  матрица Гессе имеет размер n × n. Матрица Гессе характеризует вы - пуклость функции в окрестности ( x, y). ПРИМЕЧАНИЕ Если f : n → m — векторное поле, мат рица Гессе будет тен - зором размера ( m × n × n).  5.8. ЛИНЕАРИЗАЦИЯ И РЯДЫ ТЕЙЛОРА ДЛЯ НЕСКОЛЬКИХ ПЕРЕМЕННЫХ Градиент ∇f функции f часто используется для локально линейного приближе - ния f в окрестности x0: f (x) ≈ f(x0) + ( ∇xf)(x0)(x − x0). (5.148) Здесь — значение градиента функции f относительно x в точке x0.\n--- Страница 217 ---\n5.8. Линеаризация и ряды Тейлора для нескольких переменных 217 На рис. 5.12 изображено линейное приближение функции f в точке x0. –4 4 –2–11 –2 2 00 xf(x)f(x) f(x0) f(x0) + f'(x0)(x – x0) Рис. 5.12. Линейное приближение функции. Исходная функция f аппроксимируется линейной в точке x 0 = −2 использованием ряда Тейлора Исходная функция аппроксимируется прямой линией. Это приближение ло - кально довольно точно, но чем дальше мы удаляемся от x0, тем оно хуже. Равен - ство (5.148) является частным случаем разложения в ряд Тейлора для несколь - ких переменных, в котором мы рассматриваем только первые два слагаемых. Сейчас мы рассмотрим более общий случай, позволяющий достичь лучшего приближения. Определение 5.7 (ряд Тейлора от нескольких переменных). Рассмотрим функ - цию f : D → ; (5.149) (5.150) гладкую в точке x0. Если определить разностный вектор δ = x – x0, ряд Тейлора f в точке x0 определяется как (5.151) где — значение k-й полной производной функции f относительно x в точке x0. Определение 5.8 (многочлен Тейлора). Многочлен Тейлора степени n функции f в точке x0 состоит из первых n + 1 слагаемых ряда (5.151), то есть равен (5.152)\n--- Страница 218 ---\n218 Глава 5. Векторный анализ В формулах (5.151) и (5.152) мы использовали не вполне аккуратное обозначе - ние δk, не определявшееся для векторов x ∈ D, D > 1, k > 1. Заметим, что как , так и δk — тензоры порядка k, то есть k-мерные массивы1. Тензор порядка k равен k-й тензорной степени вектора x ∈ . Тензорное произведе - ние обозначается ⊗. Например, ; (5.153) (5.154) На рис. 5.13 изображены два таких тензорных произведения. В общем случае ряд Тейлора будет состоять из слагаемых , (5.155) где содержит многочлены степени k. (a) Дан вектор δ. Внешнее произведение δ2 := δ ⊗ δ = δδT будет матрицей (b) Внешнее произведение δ3 := δ ⊗ δ ⊗ δ будет тензором третьего порядка («трехмерной матрицей»), то есть массивом с тремя индексами Рис. 5.13. Тензорные произведения. В тензорном произведении векторов каждый множитель увеличивает размерность на 1. ( a) Тензорное произведение двух векторов будет матрицей. ( b) Тензорное произведение трех векторов будет тензором порядка 3 1 Вектор может быть реализован как одномерный массив, а матрица — как двумерный.\n--- Страница 219 ---\n5.8. Линеаризация и ряды Тейлора для нескольких переменных 219 Теперь, определив ряды Тейлора для векторных полей, давайте выпишем в яв- ном виде несколько первых слагаемых ряда Тейлора для k = 0, , 3 и δ := x − x0: ; (5.156) ; (5.157) ; (5.158) ; (5.159) . (5.160) Здесь H(x0) — значение матрицы Гессе функции f в точке ( x0). Пример 5.15 (разложение в ряд Тейлора функции двух переменных) Рассмотрим функцию f (x, y) = x2 + 2xy + y3. (5.161) Мы хотим найти разложение f в ряд Тейлора в точке ( x0, y0) = (1, 2). Сна - чала поймем, что мы ожидаем получить. Функция, заданная форму - лой (5.161), — многочлен степени 3. Мы ищем разложение в ряд Тейлора, которое само является линейной комбинацией многочленов. Чтобы полу - чился многочлен степени 3, ряд Тейлора не должен содержать члены четвертой степени и выше. Таким образом, чтобы точно представить (5.161), в (5.151) достаточно найти первые четыре слагаемых. Начнем искать разложение в ряд Тейлора с вычисления свободного чле - на и первых производных: f (1, 2) = 13; (5.162) ; (5.163) (5.164)\n--- Страница 220 ---\n220 Глава 5. Векторный анализ Таким образом получаем , (5.165) так что (5.166) Заметим, что содержит только линейные слагаемые, то есть многочлены степени 1. Частные производные второго порядка равны ; (5.167) ; (5.168) ; (5.169) (5.170) Объединим их в матрицу Гессе: (5.171) Следовательно, (5.172) Таким образом, следующий член ряда Тейлора равен (5.173a) (5.173b)\n--- Страница 221 ---\n5.8. Линеаризация и ряды Тейлора для нескольких переменных 221 (5.173c) Видим, что содержит только квадратичные слагаемые, то есть многочлены степени 2. Производные третьего порядка равны (5.174) (5.175) (5.176) Так как все вторые производные из (5.171), кроме одной, являются кон - стантами, только одна из производных третьего порядка будет ненулевой: (5.177) Производные более высоких порядков, как и смешанные производные порядка 3 (например, ), обращаются в нуль, так что (5.178) и . (5.179) Мы нашли все кубические слагаемые. В итоге разложением f в ряд Тей - лора в точке ( x0, y0) = (1, 2) будет (5.180 a)\n--- Страница 222 ---\n222 Глава 5. Векторный анализ (5.180 b) (5.180 c) В этом примере ряд Тейлора в точности равен многочлену из (5.161), то есть (5.180 c) совпадает с (5.161). Здесь такой итог неудивителен, посколь - ку исходной функцией был многочлен степени 3, и мы в (5.180 c) выра - жали его как линейную комбинацию констант и многочленов степеней 1, 2 и 3. 5.9. ДЛЯ ДАЛЬНЕЙШЕГО ЧТЕНИЯ Больше материала про дифференцирование матриц вместе с кратким изложе - нием необходимых сведений из линейной алгебры можно найти в книге Magnus and Neudecker (2007). Автоматическое дифференцирование также является темой с длинной историей, и заинтересованного читателя мы отсылаем к Griewank and Walther (2003, 2008) и Elliott (2009), а также их спискам лите - ратуры. В машинном обучении и других областях нередко требуется вычислять мате - матическое ожидание, имеющее вид интеграла (5.181) Даже при «хорошей» (например, гауссовой) функции p(x), в общем случае этот интеграл не берется. Один из способов поиска приближенного решения ис - пользует разложение функции f в ряд Тейлора. Пусть p(x) = (μ, Σ) — плотность гауссова распределения. Приближение f многочленом Тейлора степени 1 в окрестности точки μ локально приближает нелинейную функцию к линейной. Для линейных функций, если p(x) — плотность гауссового распределения, можно найти среднее и дисперсию (раздел 6.5). Это свойство играет важную роль в расширенном фильтре Калмана (Maybeck, 1979) для оценки состояния\n--- Страница 223 ---\nУпражнения 223 нелинейных динамических систем (модель пространства состояний). Другие детерминированные способы приблизить интеграл из (5.181) — это «преобра - зование без запаха» (unscented transform) (Julier and Uhlmann, 1997), не требу - ющее использования градиентов, и аппроксимация Лапласа (MacKay, 2003; Bishop, 2006; Murphy, 2012), использующая разложение в ряд Тейлора до ква - дратичных слагаемых (а значит, матрицу Гессе) для локального гауссова при - ближения p(x) в окрестности ее моды. УПРАЖНЕНИЯ 5.1. Вычислите производную для 5.2. Вычислите производную логистической сигмоидной функции 5.3. Вычислите производную функции где μ, σ ∈  — константы. 5.4. Вычислите многочлены Тейлора Tn, n = 0, …, 5 из f(x) = sin(x) + cos( x) в точ- ке x0 = 0. 5.5. Рассмотрим следующие функции: a. Каковы размеры ? b. Вычислите якобианы. 5.6. Продифференцируйте f по t и g по X, где где tr обозначает след.\n--- Страница 224 ---\n224 Глава 5. Векторный анализ 5.7. Вычислите производные d f/dx следующих функций, используя цепное правило. Укажите размеры каждой частной производной. Подробно опишите свои действия. a. f(z) = log(1 + z), z = xTx, x ∈ D. b. f(z) = sin(z), z = Ax + b, A ∈ E×D, x ∈ D, b ∈ E, где sin( ·) применяется к каждому элементу z. 5.8. Вычислите производные df/dx следующих функций. Подробно опишите свои действия. a. Используйте цепное правило. Укажите размеры каждой частной произ - водной. где x, μ ∈ D, S ∈ D×D. b. f(x) = tr(xxT + σ2I), x ∈ D. Здесь tr( A) — след A, то есть сумма диагональных элементов Aii. Подсказ - ка: явно выпишите внешний продукт. c. Используйте цепное правило. Укажите размеры каждой частной произ - водной. Явно вычислять произведение частных производных не нужно. f = tanh(z) ∈ M; z = Ax + b, x ∈ N, A ∈ M×N, b ∈ M. Здесь tanh применяется к каждому компоненту z. 5.9. Определим g(z, v) := log p(x, z) − log q(z, v); z := t(ε, v) для дифференцируемых функций p, q, t. Используя цепное правило, вычислите градиент\n--- Страница 225 ---\n6 Вероятность и распределения Неформально говоря, теория вероятностей изучает неопределенность. Вероят - ность можно рассматривать как долю случаев, когда происходит данное событие, или как меру нашей уверенности в событии. Как упоминалось в главе 1, нам часто приходится оценивать неопределенность как в данных, так и в модели машинного обучения и ее прогнозах. Оценка неопределенности требует понятия случайной величины — функции, сопоставляющей исходам случайного экспе - римента интересующие нас свойства. Со случайной величиной связана функция, измеряющая вероятность конкретного исхода (или множества исходов); ее называют распределением вероятностей . Распределения вероятностей будут «кирпичиками», которые мы будем исполь - зовать в темах вероятностного моделирования (раздел 8.4), графических моде - лей (раздел 8.5) и выбора модели (раздел 8.6). В следующем разделе мы опре - делим три составляющие, задающие вероятностное пространство (пространство исходов, события и вероятности событий), и их связь с понятием случайной величины. Мы сознательно выбрали не вполне формальный способ изложения, так как б ольшая строгость помешала бы увидеть суть вводимых понятий. На рис. 6.1 показаны основные понятия данной главы. 6.1. ПОСТРОЕНИЕ ВЕРОЯТНОСТНОГО ПРОСТРАНСТВА Цель теории вероятностей — построить математическую модель, позволяющую описывать случайные исходы экспериментов. Например, при подбрасывании монеты мы не можем предсказать результат, но, сделав большое число бросков, можем наблюдать закономерности. Используя математическую модель вероят - ности, мы хотим автоматически делать выводы — в этом смысле теория вероят - ностей обобщает логику (Jaynes, 2003).\n--- Страница 226 ---\n226",
      "debug": {
        "start_page": 186,
        "end_page": 226
      }
    },
    {
      "name": "Глава 6. Вероятность и распределения 225",
      "content": "--- Страница 226 --- (продолжение)\nГлава 6. Вероятность и распределения /uni0422/uni0435/uni043E/uni0440/uni0435/uni043C/uni0430 /uni0411/uni0430/uni0439/uni0435/uni0441 /uni0430 /uni041E/uni0431/uni043E/uni0431/uni0449/uni0430/uni044E/uni0449/uni0430/uni044F /uni0441 /uni0442/uni0430/uni0442/uni0438/uni0441/uni0442/uni0438/uni043A/uni0430/uni0421/uni0440/uni0435/uni0434/uni043D/uni0435/uni0435 /uni0414/uni0438/uni0441/uni043F/uni0435/uni0440/uni0441/uni0438/uni044F /uni041F/uni0440/uni0435/uni043E/uni0431/uni0440/uni0430/uni0437/uni043E/uni0432/uni0430/uni043D/uni0438/uni044F /uni041D/uni0435/uni0437/uni0430/uni0432/uni0438/uni0441/uni0438/uni043C/uni043E/uni0441 /uni0442/uni044C /uni0421/uni043A/uni0430/uni043B/uni044F/uni0440/uni043D/uni043E/uni0435 /uni043F/uni0440/uni043E/uni0438/uni0437/uni0432/uni0435/uni0434/uni0435/uni043D/uni0438/uni0435/uni0411/uni0435/uni0442/uni0430- /uni0440/uni0430/uni0441/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni0438/uni0435/uni0414/uni043E/uni0441/uni0442/uni0430/uni0442/uni043E/uni0447/uni043D/uni044B/uni0435 /uni0441 /uni0442/uni0430/uni0442/uni0438/uni0441/uni0442/uni0438/uni043A/uni0438 /uni042D/uni043A/uni0441/uni043F/uni043E/uni043D/uni0435/uni043D/uni0446/uni0438/uni0430/uni043B/uni044C/uni043D/uni043E/uni0435 /uni0441/uni0435/uni043C/uni0435/uni0439/uni0441 /uni0442/uni0432/uni043E/uni0413/uni043B/uni0430/uni0432/uni0430 9. /uni0420/uni0435/uni0433/uni0440/uni0435/uni0441/uni0441/uni0438/uni044F /uni0413/uni043B/uni0430/uni0432/uni0430 10. /uni041F/uni043E/uni043D/uni0438/uni0436/uni0435/uni043D/uni0438/uni0435 /uni0440/uni0430/uni0437/uni043C/uni0435/uni0440/uni043D/uni043E /uni0441/uni0442/uni0438 /uni0413/uni043B/uni0430/uni0432/uni0430 11. /uni041E/uni0446/uni0435/uni043D/uni043A/uni0430 /uni043F/uni043B /uni043E/uni0442/uni043D/uni043E/uni0441/uni0442/uni0438/uni0421/uni0432/uni043E/uni0439/uni0441 /uni0442/uni0432/uni043E /uni041F/uni043E/uni0434/uni043E/uni0431/uni0438/uni0435/uni041F/uni0440/uni0438/uni043C/uni0435 /uni0440 /uni041F/uni0440/uni0438/uni043C/uni0435/uni0440/uni0421/uni043E/uni043F/uni0440/uni044F/uni0436/uni0435/uni043D/uni043D/uni043E /uni0441/uni0442/uni044C /uni0421/uni0432/uni043E/uni0439/uni0441 /uni0442/uni0432/uni043E /uni041A/uni043E/uni043D/uni0435/uni0447/uni043D/uni043E/uni0441 /uni0442/uni044C/uni0413/uni0430/uni0443/uni0441/uni0441/uni043E/uni0432/uni043E /uni0440/uni0430/uni0441/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni0438/uni0435/uni041F/uni0440/uni0430/uni0432/uni0438/uni043B/uni043E /uni0441/uni0443/uni043C/uni043C/uni044B/uni041F/uni0440/uni0430/uni0432/uni0438/uni043B/uni043E /uni043F/uni0440/uni043E/uni0438/uni0437/uni0432/uni0435/uni0434/uni0435/uni043D/uni0438 /uni044F /uni0421/uni043B/uni0443/uni0447/uni0430/uni0439/uni043D/uni044B/uni0435 /uni0432/uni0435/uni043B/uni0438/uni0447/uni0438/uni043D/uni044B /uni0438 /uni0440/uni0430/uni0441/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni0438/uni044F /uni0420/uni0430/uni0441/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni0438/uni0435 /uni0411/uni0435/uni0440/uni043D/uni0443/uni043B/uni043B/uni0438 Рис. 6.1. Ассоциативная карта концепций, вводимых в этой главе, с указанием, в каких еще частях книги они фигурируют 6.1.1. Философские вопросы При построении систем автоматического логического вывода классическая булева логика не позволяет нам выразить некоторые вполне разумные рассуж - дения. Рассмотрим такой сценарий. Мы обнаруживаем, что при ложности A реже случается B. Такой вывод нельзя сделать в классической логике. Если теперь мы узнаем, что B истинно, то A станет менее правдоподобным. Мы каж - дый день рассуждаем подобным образом. Например, пусть мы ждем подругу. Возможны три варианта развития событий: H1 — она придет вовремя, H2 — она застрянет в пробке и H3 — ее похитят инопланетяне. Заметив, что она опазды - вает, мы логически отметем H1. Мы также посчитаем H2 более правдоподобной версией, хотя строго говоря законы логики нас к этому не вынуждают. Мы мо- жем не сбрасывать со счетов и H3, но будем считать ее крайне малоправдопо - добной. Как мы приходим к тому, что H2 наиболее разумна? С такой точки зрения, теорию вероятностей можно рассматривать как обобщение булевой логики. В машинном обучении она часто применяется при построении систем\nГлава 6. Вероятность и распределения /uni0422/uni0435/uni043E/uni0440/uni0435/uni043C/uni0430 /uni0411/uni0430/uni0439/uni0435/uni0441 /uni0430 /uni041E/uni0431/uni043E/uni0431/uni0449/uni0430/uni044E/uni0449/uni0430/uni044F /uni0441 /uni0442/uni0430/uni0442/uni0438/uni0441/uni0442/uni0438/uni043A/uni0430/uni0421/uni0440/uni0435/uni0434/uni043D/uni0435/uni0435 /uni0414/uni0438/uni0441/uni043F/uni0435/uni0440/uni0441/uni0438/uni044F /uni041F/uni0440/uni0435/uni043E/uni0431/uni0440/uni0430/uni0437/uni043E/uni0432/uni0430/uni043D/uni0438/uni044F /uni041D/uni0435/uni0437/uni0430/uni0432/uni0438/uni0441/uni0438/uni043C/uni043E/uni0441 /uni0442/uni044C /uni0421/uni043A/uni0430/uni043B/uni044F/uni0440/uni043D/uni043E/uni0435 /uni043F/uni0440/uni043E/uni0438/uni0437/uni0432/uni0435/uni0434/uni0435/uni043D/uni0438/uni0435/uni0411/uni0435/uni0442/uni0430- /uni0440/uni0430/uni0441/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni0438/uni0435/uni0414/uni043E/uni0441/uni0442/uni0430/uni0442/uni043E/uni0447/uni043D/uni044B/uni0435 /uni0441 /uni0442/uni0430/uni0442/uni0438/uni0441/uni0442/uni0438/uni043A/uni0438 /uni042D/uni043A/uni0441/uni043F/uni043E/uni043D/uni0435/uni043D/uni0446/uni0438/uni0430/uni043B/uni044C/uni043D/uni043E/uni0435 /uni0441/uni0435/uni043C/uni0435/uni0439/uni0441 /uni0442/uni0432/uni043E/uni0413/uni043B/uni0430/uni0432/uni0430 9. /uni0420/uni0435/uni0433/uni0440/uni0435/uni0441/uni0441/uni0438/uni044F /uni0413/uni043B/uni0430/uni0432/uni0430 10. /uni041F/uni043E/uni043D/uni0438/uni0436/uni0435/uni043D/uni0438/uni0435 /uni0440/uni0430/uni0437/uni043C/uni0435/uni0440/uni043D/uni043E /uni0441/uni0442/uni0438 /uni0413/uni043B/uni0430/uni0432/uni0430 11. /uni041E/uni0446/uni0435/uni043D/uni043A/uni0430 /uni043F/uni043B /uni043E/uni0442/uni043D/uni043E/uni0441/uni0442/uni0438/uni0421/uni0432/uni043E/uni0439/uni0441 /uni0442/uni0432/uni043E /uni041F/uni043E/uni0434/uni043E/uni0431/uni0438/uni0435/uni041F/uni0440/uni0438/uni043C/uni0435 /uni0440 /uni041F/uni0440/uni0438/uni043C/uni0435/uni0440/uni0421/uni043E/uni043F/uni0440/uni044F/uni0436/uni0435/uni043D/uni043D/uni043E /uni0441/uni0442/uni044C /uni0421/uni0432/uni043E/uni0439/uni0441 /uni0442/uni0432/uni043E /uni041A/uni043E/uni043D/uni0435/uni0447/uni043D/uni043E/uni0441 /uni0442/uni044C/uni0413/uni0430/uni0443/uni0441/uni0441/uni043E/uni0432/uni043E /uni0440/uni0430/uni0441/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni0438/uni0435/uni041F/uni0440/uni0430/uni0432/uni0438/uni043B/uni043E /uni0441/uni0443/uni043C/uni043C/uni044B/uni041F/uni0440/uni0430/uni0432/uni0438/uni043B/uni043E /uni043F/uni0440/uni043E/uni0438/uni0437/uni0432/uni0435/uni0434/uni0435/uni043D/uni0438 /uni044F /uni0421/uni043B/uni0443/uni0447/uni0430/uni0439/uni043D/uni044B/uni0435 /uni0432/uni0435/uni043B/uni0438/uni0447/uni0438/uni043D/uni044B /uni0438 /uni0440/uni0430/uni0441/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni0438/uni044F /uni0420/uni0430/uni0441/uni043F/uni0440/uni0435/uni0434/uni0435/uni043B/uni0435/uni043D/uni0438/uni0435 /uni0411/uni0435/uni0440/uni043D/uni0443/uni043B/uni043B/uni0438 Рис. 6.1. Ассоциативная карта концепций, вводимых в этой главе, с указанием, в каких еще частях книги они фигурируют 6.1.1. Философские вопросы При построении систем автоматического логического вывода классическая булева логика не позволяет нам выразить некоторые вполне разумные рассуж - дения. Рассмотрим такой сценарий. Мы обнаруживаем, что при ложности A реже случается B. Такой вывод нельзя сделать в классической логике. Если теперь мы узнаем, что B истинно, то A станет менее правдоподобным. Мы каж - дый день рассуждаем подобным образом. Например, пусть мы ждем подругу. Возможны три варианта развития событий: H1 — она придет вовремя, H2 — она застрянет в пробке и H3 — ее похитят инопланетяне. Заметив, что она опазды - вает, мы логически отметем H1. Мы также посчитаем H2 более правдоподобной версией, хотя строго говоря законы логики нас к этому не вынуждают. Мы мо- жем не сбрасывать со счетов и H3, но будем считать ее крайне малоправдопо - добной. Как мы приходим к тому, что H2 наиболее разумна? С такой точки зрения, теорию вероятностей можно рассматривать как обобщение булевой логики. В машинном обучении она часто применяется при построении систем\n--- Страница 227 ---\n6.1. Построение вероятностного пространства 227 автоматического логического вывода. Информацию об ее применении в систе - мах логического вывода можно найти в Pearl (1988). Философские основания теории вероятностей и ее связь с нашими предполо - жениями об истинности изучались Коксом (Jaynes, 2003)1. Можно посмотреть на это и так: если мы подведем строгие основания под здравый смысл, в итоге мы построим понятие вероятности. E. T. Jaynes (1922–1998) перечислил три критерия, которые должны выполняться для любых рассуждений о вероятностях: 1. Вероятности должны представляться вещественными числами. 2. Эти числа должны быть согласованы со здравым смыслом. 3. Итоговые рассуждения должны быть корректными в трех смыслах: (a) Непротиворечивость. При вычислении одной и той же вероятности разными способами должно получаться одно и то же значение. (b) Полнота. Необходимо учесть все имеющиеся данные. (c) Воспроизводимость. Если в двух задачах у нас имеется одна и та же информация, вероятности также должны совпадать. По теореме Кокса — Джейнса, этих свойств достаточно для задания универсаль - ных математических правил, касающихся вероятности p, с точностью до произ - вольного монотонного преобразования. Это и будут основные правила теории вероятностей. ПРИМЕЧАНИЕ В машинном обучении и статистике существуют две основные интерпретации понятия вероятности: байесовская и частотная (Bishop, 2006; Efron and Hastie, 2016). Байесовская интерпретация вероятности — это степень нашей уверенности в некотором событии, ее также называют «субъективной вероятностью». При частотной интерпретации подсчитывается доля случаев, когда происходит интересующее нас событие, от общего количества экспери - ментов. Вероятность события определяется как предельное значение этой доли при стремящемся к бесконечности числе экспериментов.  Часто тексты, посвященные вероятностным моделям в машинном обучении, используют нестрогие обозначения и жаргон, порой затрудняющие понимание. Эта книга не будет исключением. Несколько разных понятий будут скрываться под названием «распределение вероятностей», и читателю часто придется по - нимать значение по контексту. Один из способов разобраться — подумать, мо - делируем ли мы разбиение на категории (дискретную случайную величину) или что-то, изменяющееся непрерывно (непрерывную случайную величину). 1 «Чтобы рассуждать разумно, необходимо расширить дискретные понятия истины и лжи до непрерывных вероятностей» (Jaynes, 2003).\n--- Страница 228 ---\n228 Глава 6. Вероятность и распределения Вопросы, встающие перед нами в машинном обучении, часто тесно связаны с ответом на этот вопрос. 6.1.2. Вероятность и случайные величины При обсуждении вероятности часто путают три разных понятия. Первое из них — понятие вероятностного пространства, позволяющее нам численно вы - разить вероятность. Обычно мы работаем с ним не напрямую, а через посредство случайных величин (второе понятие), которые «переносят» вероятность на более удобное пространство, зачастую числовое. Третье понятие — распределе - ние вероятностей, закон, описывающий поведение случайной величины. Первые два понятия мы введем в данном разделе, а третье — в разделе 6.2. Современная теория вероятностей основана на наборе аксиом, предложенных Колмогоровым (Grinstead and Snell, 1997; Jaynes, 2003), вводящим понятия пространства элементарных исходов, пространства событий и вероятностной меры. Вероятностное пространство описывает некоторый реальный процесс (который мы будем называть экспериментом) со случайным результатом. Пространство элементарных исходов Ω Пространство элементарных исходов — это множество всех возможных ис - ходов эксперимента, обычно обозначаемое как Ω. Например, для двух по - следовательных бросков монеты пространство элементарных исходов — это {оо, рр, ор, ро}, где «о» — «орел», а «р» — «решка». Пространство событий  Пространство событий — это пространство возможных результатов экс - перимента. Подмножество A в пространстве элементарных исходов Ω при- надлежит пространству событий , если после эксперимента мы можем определить, принадлежит ли исход ω ∈ Ω подмножеству A. Пространство событий  состоит из подмножеств Ω, и для дискретных распределений (раздел 6.2.1) обычно совпадает с множеством всех подмножеств в Ω. Вероятность P С каждым событием A ∈  мы свяжем число P(A), измеряющее частоту со - бытия или нашу уверенность в нем. P(A) называют вероятностью A. Вероятность любого события обязана лежать в интервале [0, 1], а сумма вероят - ностей всех исходов в Ω должна составлять 1, то есть P(Ω) = 1. Введя понятие вероятностного пространства ( Ω, , P), мы хотим применить его к явлениям реального мира. В машинном обучении мы зачастую не ука -\n--- Страница 229 ---\n6.1. Построение вероятностного пространства 229 зываем вероятностное пространство явно, а вместо этого говорим о вероят - ностях значений интересующей нас величины, множество которых обознача - ем за . В этой книге мы будем называть  фазовым пространством , а его элементы — состояниями. Введем функцию X : Ω → , принимающую элемент Ω (элементарный исход) и возвращающую значение x интересующей нас вели - чины, принадлежащее . Такое отображение из Ω в  называют случайной ве ­ личиной1. Например, при подбрасывании двух монет и подсчете количества орлов случайная величина X сопоставляет элементарным исходам одно из трех значений: X(оо) = 2, X(ор) = 1, X(ро) = 1 и X(рр) = 0. В этом случае  = {0, 1, 2}, и нас интересуют вероятности элементов множества . Когда пространство элементарных исходов Ω и фазовое пространство  конечны, функцию, зада - ющую случайную величину, можно представить в виде таблицы. С каждым подмножеством S ⊆  мы связываем вероятность PX(S) ∈ [0, 1], с которой про - исходит событие «значение X попадает в S». Пример 6.1 иллюстрирует вве - денные сейчас термины. ПРИМЕЧАНИЕ Пространство элементарных исходов Ω, к сожалению, вы - ступает в различных источниках под разными названиями. Его нередко назы - вают «пространством состояний» (Jacod and Protter, 2004), хотя этот термин часто относится и к состояниям динамической системы (Hasselblatt and Katok, 2003). Ω также порой выступает под именем «пространства исходов», «про - странства возможностей» и «пространства событий».  Пример 6.1 Мы предполагаем, что читатель уже знаком с вычислением вероятностей пересечения и объединения событий. Более аккуратное введение в теорию вероятностей, с большим количеством примеров, можно найти в главе 2 Walpole et al. (2011). Рассмотрим статистический эксперимент — игру, в которой мы вытаски - ваем из мешка две монеты (с возвращением). В мешке есть американские монеты (обозначаемые как $) и британские (обозначаемые как £), так что для двух вытаскиваний есть четыре возможных исхода. Таким образом, пространство элементарных исходов Ω состоит из ($, $), ($, £), (£, $), ( £,£). Допустим, что в мешке такое соотношение двух видов монет, что вероят - ность вытащить доллар равна 0,32. 1 Термин «случайная величина» вносит много путаницы — поскольку это не величина и она не случайна. На самом деле это функция. 2 Этот игрушечный пример по сути аналогичен примеру с броском нечестной монетки.\n--- Страница 230 ---\n230 Глава 6. Вероятность и распределения Мы хотим узнать, сколько раз вытащим доллар. Обозначим через X слу- чайную величину, отображающую исход из Ω в элемент , обозначающий число вытащенных долларов. Посмотрев на пространство элементарных исходов, мы понимаем, что можем получить 1, 2 или 3 доллара, так что  = {0, 1, 2}. Случайная вели - чина X (функция) может быть описана с помощью такой таблицы: X (($, $)) = 2; (6.1) X (($, £)) = 1; (6.2) X ((£, $)) = 1; (6.3) X ((£, £)) = 0. (6.4) Так как перед тем, как вытащить вторую монету, мы возвращаем первую в мешок, то два вытаскивания независимы (что мы обсудим в разде - ле 6.4.5). Заметим, что двум элементарным исходам соответствует одно и то же событие из , когда вытащен только один доллар. Таким образом, распределение вероятностей (раздел 6.2.1) для случайной величины X задано формулами P (X = 2) = P(($, $)) = = P($) · P($) = = 0,3 · 0,3 = 0,09; (6.5) P (X = 1) = P(($,£) ∪ (£,$)) = = P(($,£)) + P((£, $)) = = 0,3 · (1 − 0,3) + (1 − 0,3) · 0,3 = 0,42; (6.6) P (X = 0) = P((£,£)) = = P(£) · P(£) = = (1 – 0,3) · (1 – 0,3) = 0,49. (6.7) В предшествующих рассуждениях мы отождествляли две вероятности — веро - ятность определенного значения X и вероятность соответствующих исходов из Ω. Так, в формуле (6.7) мы говорим, что P(X = 0) = P((£, £)). Рассмотрим случайную величину X : Ω →  и подмножество S ⊆  (например, состоящее только из одного элемента , как выпадение одного орла при подбрасывании двух монет). Пусть X–1(S) — прообраз S относительно X, то есть множество\n--- Страница 231 ---\n6.1. Построение вероятностного пространства 231 элементов Ω, которые переходят в элементы S при действии X; {ω ∈ Ω : X(ω) ∈ S}. Один из способов думать о таком переносе вероятностей с событий из Ω на значения X — рассматривать вероятность прообраза S (Jacod and Protter, 2004). Для S ⊆  это можно записать так: PX(S) = P(X ∈ S) = P(X–1(S)) = P({ω ∈ Ω : X(ω) ∈ S}). (6.8) В левой части формулы (6.8) стоит вероятность множества значений случайной величины (например, того, что вытащен ровно один доллар), которые нас инте - ресуют. Случайная величина X сопоставляет исходам из Ω значения из , так что в правой части (6.8) мы видим вероятность множества исходов (из Ω), об- ладающих нужным свойством (в данном случае, $ £ и £$). Мы говорим, что за - дано распределение PX случайной величины X, которое определяется соответ - ствием между событиями и значением случайной величины. Иными словами, функция PX (или, что то же самое, P ◦ X–1) задает закон распределения случайной величины X. ПРИМЕЧАНИЕ Фазовое пространство, то есть область значений  случайной величины X, характеризует тип случайной величины. Когда  конечно или счетно, случайная величина называется дискретной (раздел 6.2.1). Непрерывные случайные величины (раздел 6.2.2) мы рассматриваем только для  =  или  = D.  6.1.3. Статистика Теорию вероятностей и статистику часто упоминают вместе, но они касаются двух разных типов неопределенности. Их можно противопоставить по типу рассматриваемых задач. В теории вероятности мы строим модель некоторого процесса, выражаем имеющуюся неопределенность с помощью случайной ве - личины и пытаемся вывести, что произойдет. В статистике мы уже знаем, что произошло, и пытаемся с помощью утверждений теории вероятностей понять свойства процесса. В этом смысле машинное обучение близко к статистике в своей цели построить модель, адекватно описывающую процесс, породивший данные. С помощью теории вероятностей мы можем подобрать наилучшую модель для имеющихся данных. Другая важная черта машинного обучения — наша заинтересованность в обоб- щении (см. главу 8). Это означает, что нам на самом деле важны результаты нашей системы на объектах, которые мы еще не видели. Анализ будущих ре - зультатов основан на теории вероятностей и статистике, в основном выходящих за пределы содержания этой главы. Заинтересованному читателю советуем обратиться к книгам Boucheron et al. (2013) и Shalev-Shwartz and Ben-David (2014). Мы еще поговорим о статистике в главе 8.\n--- Страница 232 ---\n232 Глава 6. Вероятность и распределения 6.2. ДИСКРЕТНЫЕ И НЕПРЕРЫВНЫЕ РАСПРЕДЕЛЕНИЯ Сосредоточимся на способах описать вероятности событий, введенных в гла- ве 6.1. В зависимости от того, дискретно или непрерывно фазовое пространство, естественно описывать распределения по-разному. Когда фазовое пространство  дискретно, можно найти вероятность, что случайная величина X принимает конкретное значение x ∈ , то есть P(X = x). Выражение P(X = x) для дискретной случайной величины X известно как распределение вероятности. Когда фазовое пространство  непрерывно, например это вещественная прямая , естественнее говорить о вероятностях того, что случайная величина X попадает в некоторый интервал, то есть P(a ≤ X ≤ b) для a < b. Нам удобно говорить о вероятности, что случайная величина X не превосходит некоторого x, то есть P(X ≤ x). Выражение P(X ≤ x) для непрерывной случайной величины X известно как функция рас ­ пределения . Непрерывные случайные величины мы обсудим в разделе 6.2.2. Терминологию и различия дискретных и непрерывных случайных величин мы обсудим в разделе 6.2.3. ПРИМЕЧАНИЕ Термин « одномерное распределение » мы будем применять, говоря о распределениях одной случайной величины (значения которой обо - значены обычной буквой x). Распределения более чем одной случайной вели - чины мы будем называть многомерными и рассматривать вектор случайных величин (обозначая состояния жирным x).  6.2.1. Дискретные вероятности Когда фазовое пространство дискретно, можно представлять себе распределение многомерной случайной величины как многомерный массив чисел. На рис. 6.2 представлен пример. Фазовое пространство совместной вероятности представ - ляет собой декартово произведение фазовых пространств для каждой из слу - чайных величин. Совместную вероятность мы определяем как вероятность того, что обе случайные величины принимают заданные значения: (6.9) где nij — количество исходов, соответствующих значениям xi и yj, а N — общее число исходов. Совместная вероятность есть вероятность пересечения событий, то есть P(X = xi, Y = yj) = P(X = xi ∩ Y = yj). На рис. 6.2 показано дискретное рас­ пределение . Для двух случайных величин X и Y вероятность того, что X = x и Y = y, записывается как p(x, y) и называется совместной вероятностью. Такая запись обусловлена тем, что можно думать о ней как о функции, принимающей на вход значения x и y и возвращающей вещественное число. Частная (маргинальная) вероятность того, что X принимает значение x безотносительно значения Y\n--- Страница 233 ---\n6.2. Дискретные и непрерывные распределения 233 записывается как p(x). Чтобы показать, что случайной величине X соответству - ет распределение p(x), мы пишем X ∼ p(x). Если мы рассматриваем только ис - ходы, при которых X = x, доля исходов, при которых Y = y (условная вероятность ), обозначается как p(y | x). Xnijrj x1y1 x2y2 x3y3 x4x5ci Y Рис. 6.2. Изображение распределения для двух дискретных случайных величин X и Y. Диаграмма (с изменениями) взята из Bishop (2006) Пример 6.2 Рассмотрим две случайные величины X и Y, где X имеет пять возможных значений, а Y — три возможных значения, как показано на рис. 6.2. Коли - чество исходов, при которых X = xi и Y = yj, мы обозначим как nij, а общее количество возможных исходов как N. ci — сумма элементов i-го столбца, то есть . Аналогично, rj — это суммы по строкам, то есть . С помощью этих обозначений можно полностью описать рас - пределение X и Y. Частное распределение каждой из случайных величин выражается как сумма по строке или столбцу: (6.10) и (6.11) где ci и rj соответствуют i-му столбцу и j-й строке таблицы вероятностей. Мы договорились, что для дискретной случайной величины с конечным числом возможных значений сумма вероятностей всех значений равна единице: и (6.12)\n--- Страница 234 ---\n234 Глава 6. Вероятность и распределения Условная вероятность — это доля исходов из данной клетки от общего количества исходов в строке или столбце. Например, условная вероят - ность Y при заданном X равна (6.13) а условная вероятность X при заданном Y (6.14) В машинном обучении дискретные распределения используют при моделиро - вании категориальных признаков , то есть принимающих значения в конечном неупорядоченном множестве. Это могут быть признаки, подающиеся на вход алгоритму (например, образование при предсказании заработной платы) или метки классов (скажем, буквы при распознавании рукописных текстов). Дис - кретные распределения также часто используют для построения вероятностных моделей, комбинирующих конечное число непрерывных распределений (гла - ва 11). 6.2.2. Непрерывные вероятности В этом разделе мы рассматриваем вещественнозначные случайные величины, то есть фазовыми пространствами будут интервалы на вещественной оси. В дан- ной книге мы делаем вид, что можем выполнять операции над вещественными случайными величинами так же, как в дискретном пространстве с конечным числом значений. Однако это упрощение неверно для двух ситуаций: повторе - ния операции бесконечное число раз и случайного выбора точки на интервале. Первая ситуация возникает при обсуждении обобщающей способности (глава 8). Вторая — при рассмотрении непрерывных (например, гауссовых) распределений (раздел 6.5). Однако для наших целей допустимо подобное отступление от ма - тематической строгости ради более быстрого введения в тему. ПРИМЕЧАНИЕ В непрерывных вероятностных пространствах возникают два дополнительных контринтуитивных нюанса. Во-первых, множество всех под - множеств (которое было пространством событий  в разделе 6.1) не обладает столь хорошими свойствами. Чтобы множества из  «хорошо себя вели» при операциях дополнения, объединения и пересечения, необходимо брать не все подмножества. Во-вторых, количество элементов множества (которое в дис- кретных пространствах можно получить простым подсчетом элементов) оказы -\n--- Страница 235 ---\n6.2. Дискретные и непрерывные распределения 235 вается не столь простым понятием. «Размер» множества будет выражаться понятием меры . Например, мощность дискретных множеств, длина интервала в  и объем области в d — это меры. Множества, «хорошо себя ведущие» от - носительно операций над ним и вдобавок имеющие топологическую структуру, называются борелевской σ-алгеброй . Бетанкур подробно, но не увязая в ненуж - ных деталях, пишет о построении вероятностных пространств на основе теории множеств, см. https://tinyurl.com/yb3t6mfd . Для более строгого изложения мы рекомендуем Billingsley (1995) и Jacod and Protter (2004). В нашей книге мы рассматриваем вещественнозначные случайные величины с соответствующей борелевской σ-алгеброй. Мы рассматриваем случайные величины из D как векторы из вещественных случайных величин.  Определение 6.1 (плотность вероятности). Функция f : D →  называется плотностью вероятности (probability density function, pdf), если 1. ∀x ∈ D : . 2. Она интегрируема и (6.15) Для распределений дискретной случайной величины аналогом интеграла (6.15) является сумма (6.12). Заметим, что плотностью вероятности является любая неотрицательная функ - ция f, интеграл от которой равен 1. С этой функцией мы связываем случайную величину X: (6.16) где a, b ∈ , и x ∈  — значения непрерывной случайной величины X. Значения x ∈ D определяются аналогично, как вектор из x ∈ . Зависимость (6.16) на - зывают законом распределения случайной величины X. ПРИМЕЧАНИЕ В отличие от дискретных случайных величин, вероятность P(X = x) того, что непрерывная случайная величина X принимает конкретное значение, равна нулю. Представьте, что в формуле (6.16) указан интервал с a = b.  Определение 6.2 (функция распределения). Функция распределения (cumulative distribution function, cdf) многомерной вещественнозначной случайной вели - чины X со значениями x ∈ D задается формулой , (6.17)\n--- Страница 236 ---\n236 Глава 6. Вероятность и распределения где X = [X1, , XD]T, x = [x1, , xD]T, и в правой части стоят вероятности того, что Xi принимает значение, не превосходящее xi. Функцию распределения также можно представить как интеграл от плотности распределения1 f(x): (6.18) ПРИМЕЧАНИЕ Повторим, что, говоря о распределении, мы фактически имеем в виду два разных понятия. Первое — это плотность вероятности (обо - значаемая f(x)), неотрицательная функция с интегралом 1. Второе — закон распределения случайной величины X, то есть связь X с плотностью f(x).  В основном в этой книге мы будем использовать обозначения f(x) и FX(x) для плотности распределения и функции распределения. В разделе 6.7 нам при - дется быть аккуратнее с этими понятиями. 6.2.3. Различия дискретных и непрерывных распределений Из раздела 6.1.2 мы знаем, что вероятности неотрицательны и в сумме по всем исходам дают единицу. Для дискретных случайных величин (6.12) это означает, что вероятность любого значения лежит в интервале [0, 1]. Однако для непре - рывных случайных величин нормализация (6.15) не подразумевает, что значе - ние плотности во всех точках не превосходит 1. Это показано на рис. 6.3 на примере равномерного распределения для дискретной и непрерывной случайных величин. (a) Дискретное распределение p(x) (b) Непрерывное распределениеx2 21 10 02,0 1,5 1,0 0,5 0,0P(Z = z)2,0 1,51,0 0,5 0,0 –1 –1 z Рис. 6.3. Примеры ( a) дискретного и ( b) непрерывного равномерного распределения. Подробности приведены в примере 6.3 1 Для некоторых функций распределения не существует соответствующей плотности.\n--- Страница 237 ---\n6.2. Дискретные и непрерывные распределения 237 Пример 6.3 Рассмотрим два примера равномерных распределений, в которых каждое значение равновероятно. Этот пример показывает некоторые различия между дискретными и непрерывными случайными величинами. Пусть Z — дискретная равномерно распределенная случайная величина с тремя возможными значениями { z = −1,1, z = 0,3, z = 1,5}1. Распределение можно представить как таблицу значений вероятности Можно также представить его графически (рис. 6.4( a)), отмечая значения на оси x, а на оси y —вероятность каждого из значений. Мы специально удлинили ось y на рис. 6.4( a), чтобы она была такой же, как на рис. 6.4( b). Пусть X — непрерывная случайная величина, принимающая значения на интервале 0,9 ≤ X ≤ 1,6, как показано на рис. 6.4( b). Заметим, что плотность может подниматься выше 1. Однако должно выполняться условие (6.19) ПРИМЕЧАНИЕ С дискретными распределениями связана еще одна допол - нительная сложность. Значения z1, , zd в общем случае не упорядочены, то есть их нельзя сравнивать, например z1 = красный , z2 = зеленый , z3 = синий . Однако во многих задачах машинного обучения дискретные значения будут числовыми, например z1 = −1,1, z2 = 0,3, z3 = 1,5, когда можно заметить, что z1 < z2 < z3. Число - вые дискретные значения особенно полезны, поскольку нас часто интересует математическое ожидание (раздел 6.4.1) случайных величин. К сожалению, обозначения и терминология литературы по машинному обучению не показывают разницы между пространством элементарных исходов Ω, фазо - вым пространством  и случайной величиной X. Для x из множества возможных значений случайной величины X, то есть x ∈ , p (x) обозначает вероятность, что X принимает значение x2. Для дискретной случайной величины та же самая вероятность записывается как P (X = x) и известна как распределение случайной 1 Конкретные значения не важны. Мы выбрали их так, чтобы подчеркнуть неважность порядка значений. 2 Мы считаем исход x аргументом для вероятности p(x).\n--- Страница 238 ---\n238 Глава 6. Вероятность и распределения величины. Для непрерывной случайной величины p (x) называют плотностью вероятности. Чтобы еще больше запутать дело, P (X ≤ x) называют функцией распределения. В данной главе как одномерные, так и многомерные случайные величины мы будем обозначать буквой X, а их значения — соответственно x и x. Терминологию мы собрали в табл. 6.1. ПРИМЕЧАНИЕ Выражение «распределение вероятностей» мы будем ис - пользовать не только для дискретного распределения, но и для плотности ве - роятности в непрерывном случае, хотя в строгом смысле это некорректно. Как и в основной массе литературы по машинному обучению, мы опираемся на контекст при использовании слов «распределение вероятностей» в разных смыслах.  Таблица 6.1. Терминология для распределений вероятностей Тип «Вероятность точки» «Вероятность интервала» Дискретная P(X = x) РаспределениеНе применимо Непрерывная p(x) Плотность вероятностиP(X ≤ x) Функция распределения 6.3. ПРАВИЛО СУММЫ, ПРАВИЛО ПРОИЗВЕДЕНИЯ И ТЕОРЕМА БАЙЕСА Теорию вероятностей мы можем считать расширением логики. Как уже говори - лось в разделе 6.1.1, все правила теории вероятностей следуют из выполнения некоторых естественных требований (Jaynes, 2003, глава 2). Вероятностное моделирование (раздел 8.4) является основой для построения моделей машин - ного обучения. Когда известны распределения вероятностей (раздел 6.2), соот - ветствующие неопределенностям в данных и в задаче, остается применять лишь два фундаментальных правила: правило суммы и правило произведения. Вспомним формулу (6.9), в ней p(x, y) — совместное распределение двух слу - чайных величин x и y. Соответствующие частные распределения обозначим как p(x) и p(y), а p(y | x) — условное распределение y при заданном x. Зная опреде - ления частной и условной вероятности для дискретных и непрерывных случай - ных величин из раздела 6.2, мы можем сформулировать два фундаментальных правила теории вероятностей1. 1 Эти два правила естественным образом возникают из требований, которые мы обсуж - дали в разделе 6.1.1.\n--- Страница 239 ---\n6.3. Правило суммы, правило произведения и теорема Байеса 239 Первое из них, правило суммы , утверждает, что (6.20) где  — фазовое пространство случайной величины Y. Это означает, что мы суммируем (или интегрируем) по всем возможным значениям Y. Правило сум - мы также известно как правило маргинализации , так как связывает совместное и частное (маргинальное) распределения. В общем случае, когда рассматрива - ется совместное распределение более чем одной случайной величины, правило суммы можно применять к любому подмножеству случайных величин и полу- чать их частное распределение. А именно, если x = [x1 , xD]T, мы получаем частное распределение (6.21) повторным применением правила суммы, складывая все случайные величины, кроме xi, или интегрируя по ним (значок \\ i означает «все, кроме i»). ПРИМЕЧАНИЕ Многие вычислительные задачи вероятностного моделиро - вания обусловлены применением правила суммы. Когда случайных величин или возможных значений дискретной случайной величины много, правило суммы сводится к вычислению многомерной суммы или интеграла, что вычис - лительно трудно (то есть нет известного полиномиального алгоритма, дающего точный результат).  Второе правило, известное как правило произведения , связывает совместное распределение с условным распределением по формуле p(x, y) = p(y | x)p(x). (6.22) Правило произведения можно истолковать так: любое совместное распределе - ние двух случайных величин можно разложить в произведение двух других распределений. Этими двумя сомножителями будут частное распределение первой случайной величины p(x) и условное распределение второй случайной величины при заданной первой p(y | x). Так как порядок записи случайных ве - личин в p(x, y) произволен, из правила произведения также следует, что p(x, y) = = p(x | y)p(y). Если быть точными, (6.22) относится к распределениям дискрет - ных случайных величин. Для непрерывных случайных величин правило про - изведения записывается через плотности вероятности (раздел 6.2.3). В машинном обучении и байесовской статистике нас часто интересует вывод значений ненаблюдаемых (скрытых) случайных величин, когда мы наблюдаем\n--- Страница 240 ---\n240 Глава 6. Вероятность и распределения другие случайные величины. Предположим, что у нас есть некоторое предвари - тельное (априорное) знание p(x) о скрытой случайной величине x и некоторая зависимость p(y | x) между x и второй случайной величиной y, которую мы можем наблюдать. Наблюдая y, мы можем с использованием теоремы Байеса делать вы - воды об x при условии наблюдаемых значений y. Теорема (правило) Байеса (6.23) напрямую следует из правила произведения (6.20), так как p (x, y) = p(x | y)p(y) (6.24) и p(x, y) = p(y | x)p(x), (6.25) так что (6.26) В формуле (6.23) p(x) — априорное распределение , характеризующее наше субъ - ективное исходное знание о ненаблюдаемой (латентной) случайной величине x до получения каких-либо данных. Мы можем выбрать любое осмысленное значение априорного распределения, но важно убедиться, что плотность вероят - ности (или вероятность) не равна нулю на всех возможных значениях x, даже очень редких. Правдоподобие p(y | x) описывает, как связаны x и y, и в случае дискретного распределения это вероятность данных y при известном значении скрытой переменной x. Заметим, что правдоподобие не будет распределением случайной величины x, а только y. Мы говорим о p(y | x) как о правдоподобии x при за - данном y или вероятности y при заданном x, но не о правдоподобии y (MacKay, 2003). В байесовской статистике мы ищем апостериорное распределение p(x | y), так как оно в точности выражает интересующую нас величину, то есть то, что мы знаем об x, узнав y. Выражение X[p(y | x)] (6.27)\n--- Страница 241 ---\n6.4. Обобщающие статистики и независимость 241 называется маргинальным правдоподобием . В правой части (6.27) стоит операция взятия математического ожидания, определенная в разделе 6.4.1. По определе - нию, маргинальное правдоподобие — это интеграл от выражения (6.23) по скрытой переменной x. Следовательно, маргинальное правдоподобие не зависит от x, и оно гарантирует, что апостериорная вероятность p(x | y) нормализована. Маргинальное правдоподобие также можно истолковать как ожидание правдо - подобия относительно априорного распределения p(x). Кроме нормализации апостериорного распределения, маргинальное правдоподобие также играет важную роль в байесовском выборе модели, как мы обсудим в разделе 8.6. Из-за интеграла в выражении (8.44) его зачастую непросто вычислить. Теорема Байеса (6.23) позволяет нам «обратить» зависимость y от x, заданную функцией правдоподобия. Поэтому ее иногда называют теоремой об обращении вероятностей. Мы еще будем обсуждать теорему Байеса в разделе 8.4. ПРИМЕЧАНИЕ В байесовской статистике нас интересует апостериорное распределение как учитывающее всю доступную информацию от априорного распределения и имеющихся данных. Вместо того чтобы продолжать работать с самим апостериорным распределением, можно сосредоточиться на некоторой его статистике, например его максимуме, что мы обсудим в разделе 8.3. Однако при фокусировке на некоторой статистике распределения теряется часть ин - формации. Например, апостериорное распределение можно использовать в си- стемах принятия решений. И знать его целиком может быть крайне полезно для принятия решений, устойчивых к ошибкам в данных. Например, в контексте моделей обучения с подкреплением Deisenroth et al. (2015) показывают, что знание апостериорного распределения подходящих функций перехода ведет к очень быстрому (эффективному в использовании данных) обучению, тогда как использование только максимума апостериорного распределения приводит к ошибкам. Таким образом, знание апостериорного распределения целиком может быть очень полезно. В главе 9 мы продолжим обсуждение этой темы в применении к линейной регрессии.  6.4. ОБОБЩАЮЩИЕ СТАТИСТИКИ И НЕЗАВИСИМОСТЬ Нас часто интересует некоторое обобщение набора случайных величин или сравнение двух случайных величин. Статистика случайной величины — детер - минированная функция этой величины. Обобщающие статистики распределе - ния помогают увидеть поведение случайной величины и обобщенно охаракте - ризовать распределение. Мы определим среднее и дисперсию, две известнейшие статистики. Затем мы обсудим два способа сравнивать пары случайных величин:\n--- Страница 242 ---\n242 Глава 6. Вероятность и распределения первый — определить, что они независимы; второй — найти их скалярное про - изведение. 6.4.1. Среднее и дисперсия Среднее и дисперсия часто используются для описания свойств распределений вероятностей (как ожидаемое значение и мера разброса). В разделе 6.6 мы увидим, что существует важное семейство распределений (экспоненциальные распределения), для которых эти статистики несут всю возможную инфор - мацию. Понятие математического ожидания является центральным для машинного обучения, и все основные понятия самой теории вероятностей можно вывести из его свойств (Whittle, 2000). Определение 6.3 (математическое ожидание). Математическое ожидание для функции g :  →  от одной непрерывной случайной величины X ∼ p(x) задает - ся формулой X[g(x)] = (6.28) Аналогично, математическое ожидание функции g от дискретной случайной величины X ∼ p(x) задается формулой X[g(x)] = (6.29) где  — множество возможных значений (фазовое пространство) случайной величины X. В этом разделе мы рассматриваем дискретные случайные величины, принима - ющие числовые значения (как можно заметить, функция g принимает веще - ственные аргументы). ПРИМЕЧАНИЕ Мы рассматриваем многомерные случайные величины X как векторы из конечного числа одномерных: [ X1, , Xn]T. Для многомерных случай - ных величин мы определяем математическое ожидание покомпонентно: X[g(x)] = , (6.30) где индекс XD показывает, что мы берем математическое ожидание относитель - но d-й компоненты вектора x. \n--- Страница 243 ---\n6.4. Обобщающие статистики и независимость 243 В определении 6.3 X представляет собой оператор взятия интеграла по плот - ности вероятности (для непрерывных распределений) или суммы по всем значениям (для дискретных распределений). Определение среднего значения (определение 6.4) является частным случаем математического ожидания от функции случайной величины, где в качестве g берется тождественная функция. Определение 6.4 (среднее значение). Среднее значение случайной величины X со значениями x ∈ D (математическое ожидание этой величины) определяется как X[x] = , (6.31) где (6.32) для d = 1, , D, где индекс d указывает на компоненту вектора x. Интеграл и сум- ма берутся по всем значениям  фазового пространства случайной величины X. В случае размерности 1 интуиция подсказывает еще два способа определить типичное значение величины, кроме среднего, — медиану и моду. Медиана — это «серединный» элемент среди отсортированных значений случайной величины, то есть 50% значений больше медианы, а 50% меньше медианы. Обобщить эту идею на непрерывные случайные величины можно, определив медиану как точку, в которой функция распределения (определение 6.2) равна 0,5. Для асим - метричных или имеющих длинные хвосты распределений медиана оценивает типичное значение лучше (ближе к человеческому пониманию типичности), чем среднее значение. Кроме того, медиана устойчивее к выбросам, чем среднее. Обобщить медиану на многомерные распределения — непростая задача, так как отсутствует очевидный способ сортировки значений (Hallin et al., 2010; Kong and Mizera, 2012). Мода — это наиболее часто встречающееся значение. Для дискретной случайной величины мода определяется как значение x, встречаю - щееся наибольшее количество раз. Для непрерывной случайной величины мода определяется как значение, соответствующее максимуму плотности p(x). У функции плотности p(x) может быть более одной моды, и, более того, у мно- гомерных распределений может быть очень много мод. Таким образом, нахож - дение всех мод распределения может быть весьма сложным вычислительно.\n--- Страница 244 ---\n244 Глава 6. Вероятность и распределения Пример 6.4 Рассмотрим два двумерных распределения, изображенных на рис. 6.4: p (x) = 0,4 + 0,6 (6.33) Среднее Моды Медиана Рис. 6.4. Среднее, моды и медиана двумерного набора данных, а также частные плотности В разделе 6.5 мы определим гауссово распределение  (μ, σ2). Также на рисунке показаны его частные распределения для каждой из компонент. Заметим, что это распределение бимодально (имеет две моды), но одно из частных распределений унимодально (имеет одну моду). Горизонталь - ное бимодальное одномерное распределение показывает, что среднее и медиана могут отличаться. Хотя и хочется определить двумерную ме - диану как пару медиан компонент, отсутствие порядка на точках в дву- мерном пространстве осложняет ситуацию. Говоря, что мы не можем ввести порядок, мы имеем в виду, что существует более одного способа определить отношение <, такое, что\n--- Страница 245 ---\n6.4. Обобщающие статистики и независимость 245 ПРИМЕЧАНИЕ Математическое ожидание (определение 6.3) является ли - нейным оператором. Например, для вещественнозначной функции f(x) = = ag(x) + bh(x), где a, b ∈  и x ∈ D, имеем X[f(x)] = (6.34 a) (6.34 b) (6.34 c) = aX[g(x)] + bX[h(x)]. (6.34 d)  Иногда мы хотим описать связь между двумя случайными величинами. Кова - риация характеризует, насколько сильно они зависят друг от друга. Определение 6.5 (ковариация (одномерная)). Ковариация между двумя одно - мерными случайными величинами X, Y ∈  равна ожиданию произведения их отклонений от средних, то есть CovX,Y [x, y] := X,Y [(x – X[x])(y – Y[y])]. (6.35) ПРИМЕЧАНИЕ Когда ясно, какой случайной величине соответствует мате - матическое ожидание или ковариация, индекс часто опускается (например, X[x] записывается как [x]).  По линейности математического ожидания выражение из определения 6.5 можно переписать как матожидание произведения минус произведение матожи- даний, то есть Cov [x,y] = [xy] – [x][y]. (6.36) Ковариация случайной величины с собой Cov[ x, x] называется дисперсией и обо- значается X[x]. Квадратный корень из дисперсии называется стандартным отклонением и часто обозначается σ(x). Понятие ковариации можно обобщить на многомерные случайные величины1. Определение 6.6 (ковариация (многомерная)). Если X, Y — две многомерные случайные величины со значениями x ∈ D и y ∈ E соответственно, ковариация X и Y определяется как Cov [ x, y] = [xyT] – [x][y]T = Cov [ y, x]T ∈ D×E. (6.37) 1 Терминология: ковариацию многомерной случайной величины Cov[ x, y] часто назы - вают кросс-ковариацией, а ковариацией — Cov[ x, x].\n--- Страница 246 ---\n246 Глава 6. Вероятность и распределения Определение 6.6 можно применить и для случая, когда в роли обоих аргументов выступает одна и та же случайная величина, и результат характеризует разброс ее значений. Для многомерной случайной величины дисперсия описывает связь между ее отдельными компонентами. Определение 6.7 (дисперсия). Дисперсия случайной величины X со значения - ми x ∈ D и средним μ ∈ D определяется как X[x] = CovX [x, x] = (6.38 a) = X[(x – μ)(x – μ)T] = X[xxT] – X[x]X[x]T = (6.38 b) (6.38 c) Матрица размера D × D в формуле (6.38 c) называется ковариационной матрицей многомерной случайной величины X. Ковариационная матрица симметрична и положительно определена, она несет информацию о разбросе в данных. На ее диагонали стоят дисперсии частных распределений (6.39) где «\\i» означает «все компоненты, кроме i». Элементы вне диагонали — это кросс­ковариации Cov[ xi, xj] для i, j = 1, , D, i ≠ j. ПРИМЕЧАНИЕ В нашей книге мы для упрощения восприятия предполагаем, что ковариационная матрица всегда положительно определена, не рассматривая граничные случаи с положительно полуопределенной (неполного ранга) кова - риационной матрицей.  Когда мы хотим сравнить ковариации между различными парами случайных величин, оказывается, что на значение ковариации влияет дисперсия каждой из случайных величин. Нормализованная ковариация называется корреляцией . Определение 6.8 (корреляция). Корреляция между двумя случайными вели - чинами X, Y задается формулой (6.40) Матрица корреляции — это ковариационная матрица стандартизированных случайных величин x/σ(x). Иными словами, каждая случайная величина в кор- реляционной матрице делится на свое стандартное отклонение (квадратный корень из дисперсии).\n--- Страница 247 ---\n6.4. Обобщающие статистики и независимость 247 Ковариация (и корреляция) показывают, насколько зависят друг от друга две случайные величины (рис. 6.5). Положительная корреляция corr[ x, y] означает, что при росте x ожидается, что вырастет и y. Отрицательная корреляция пока - зывает, что при увеличении x, y будет уменьшаться. xy x00 5 /dollar.g00DC5/dollar.g00DC2246 y 0 /dollar.g00DC2246 05 /dollar.g00DC5 (a) /percent.g00BA/two.g0088/exclam.g00AF/comma.g00D2/.notdef.g0106/equal.g00C8/two.g0088/yright/.notdef.g00E3/.notdef.g0109…/equal.g00C8/space.g00AB /asterisk.g007D/percent.g00BA/exclam.g00AF/exclam.g00AF/yright/.notdef.g00E3/space.g00AB/.notdef.g0106/comma.g00D2/space.g00AB x /comma.g00D2 y ( b) /C.g00B9/percent.g00BA/.notdef.g00E3/percent.g00BA›/comma.g00D2/two.g0088/yright/.notdef.g00E3/.notdef.g0109…/equal.g00C8/space.g00AB /asterisk.g007D/percent.g00BA/exclam.g00AF/exclam.g00AF/yright/.notdef.g00E3/space.g00AB/.notdef.g0106/comma.g00D2/space.g00AB x /comma.g00D2 y Рис. 6.5. Двумерные датасеты с идентичными средними значениями и дисперсией вдоль обеих осей, но с разными ковариациями 6.4.2. Эмпирические среднее и дисперсия Определения из раздела 6.4.1 часто также называют средним и дисперсией гене ­ ральной совокупности , так как они являются истинными статистиками по всей генеральной совокупности. В машинном обучении нам приходится обучать модели на эмпирических данных. Рассмотрим случайную величину X. Чтобы прийти от статистики по генеральной совокупности к эмпирической, надо про - делать два шага. Сначала мы пользуемся конечностью выборки (размера N), чтобы построить эмпирическую статистику как функцию от конечного числа одинаково распределенных случайных величин X1, , XN. Затем применяем эту эмпирическую статистику к данным, то есть реализациям x1, , xN наших слу - чайных величин. В частности, для среднего (определение 6.4) по выборке можно получить оцен - ку, называемую эмпирическим , или выборочным, средним . То же верно и для выборочной дисперсии. Определение 6.9 (выборочные среднее и ковариация). Вектор выборочного среднего состоит из средних арифметических наблюдений для каждой случайной величины и определяется как (6.41) где xn ∈ D.\n--- Страница 248 ---\n248 Глава 6. Вероятность и распределения Аналогично, выборочная ковариационная матрица — это матрица размера D × D. (6.42) Чтобы вычислить статистики для конкретной выборки, мы берем наблюдения x1, , xN и подставляем в формулы (6.41) и (6.42). Выборочные ковариационные матрицы симметричны и положительно полуопределены (раздел 3.2.3)1. 6.4.3. Три формулы дисперсии Сосредоточимся теперь на изучении одной случайной величины X и с помощью представленных выше эмпирических формул выведем три возможные формулы для дисперсии. Аналогичные рассуждения будут верны и для дисперсии гене - ральной совокупности, разве что придется повозиться с интегралами2. Обычное определение дисперсии, следующее из определения ковариации (определе - ние 6.5), — это математическое ожидание квадрата отклонения X от ее матожи - дания μ, то есть X[x] := X[(x – μ)2]. (6.43) Математическое ожидание из (6.43) и среднее μ = X(x) вычисляются по фор - мулам (6.32), в зависимости от дискретности либо непрерывности X. Дисперсия по формуле (6.43) является средним для новой случайной величины Z := (X − μ)2. Оценивая по выборке дисперсию с помощью формулы (6.43), мы должны дваж - ды пройти по всей выборке: сначала вычислить среднее μ по формуле (6.41), а затем, зная оценку , вычислить выборочную дисперсию. Оказывается, пере - ставляя слагаемые, мы можем избежать двух проходов. Формулу (6.43) можно преобразовать в такую формулу: X[x] := X[x2] – (X[x])2. (6.44) Выражение из (6.44) можно запомнить как «среднее от квадратов минус ква - драт среднего». Его можно вычислить за один проход по выборке, так как мы можем одновременно прибавлять xi (для вычисления среднего) и , где xi — i-е наблюдение. К сожалению, такая реализация может быть вычислительно неустойчивой3. Версия (6.44) может пригодиться в машинном обучении, на - 1 В нашей книге мы используем выборочную ковариацию, являющуюся смещенной оценкой. Несмещенная оценка ковариации имеет в знаменателе N – 1 вместо N. 2 Доказательство оставлено в качестве упражнения. 3 Если оба слагаемых в (6.44) большие и примерно равные, вычисления с плавающей запятой могут привести к потере точности.\n--- Страница 249 ---\n6.4. Обобщающие статистики и независимость 249 пример при нахождении разложения на компоненты смещения и разброса (Bishop, 2006). Третий способ — думать о дисперсии как о сумме попарных разностей между всеми парами наблюдений. Рассмотрим выборку x1, , xN реализаций случайной величины X и вычислим квадраты разностей между xi и xj. Раскрывая скобки, можно заметить, что сумма N 2 попарных разностей даст выборочную дисперсию: (6.45) Можно заметить, что (6.45) — это удвоенное выражение (6.44). Это означает, что сумму N 2 попарных разностей можно представить как сумму N отклонений от среднего. Геометрически это означает, что между попарными расстояниями и расстояниями от центра множества точек есть эквивалентность. С вычисли - тельной точки зрения это означает, что, вычисляя среднее ( N слагаемых), а затем дисперсию (также N слагаемых), можно получить выражение из левой ча - сти (6.45), содержащее N 2 слагаемых. 6.4.4. Суммы и преобразования случайных величин Иногда нам нужно моделировать явления, которые не описываются стандарт - ными распределениями (некоторые из которых мы введем в разделах 6.5 и 6.6), так что приходится производить некоторые преобразования (например, скла - дывать случайные величины). Рассмотрим две случайные величины X, Y со значениями x, y ∈ D. Тогда  [x + y] = [x] + [y]; (6.46)  [x − y] = [x] − [y]; (6.47)  [x + y] = [x] + [y] + Cov[x, y] + Cov[y, x]; (6.48)  [x − y] = [x] + [y] − Cov[ x, y] − Cov[ y, x]. (6.49) При аффинных преобразованиях среднее и дисперсия обладают некоторыми полезными свойствами. Рассмотрим случайную величину X со средним μ и ко- вариационной матрицей Σ и аффинное преобразование y = Ax + b значения x. Тогда y является случайной величиной со средним и ковариационной матрицей, заданными формулами Y [y] = X[Ax + b] = AX[x] + b = Aμ + b; (6.50) Y [y] = X[Ax + b] = X[Ax] = AX[x]AT = AΣAT (6.51)\n--- Страница 250 ---\n250 Глава 6. Вероятность и распределения соответственно. Далее, Cov[x, y] = [x(Ax + b)T] − [x][Ax + b]T = (6.52 a) = [x]bT + [xxT]AT − μbT − μμTAT = (6.52 b) = μbT− μbT + ([xxT] – μμT)AT = (6.52 c) (6.52 d) где Σ = [xxT] − μμT — ковариация X. 6.4.5. Статистическая независимость Определение 6.10 (независимость). Две случайные величины X, Y называются статистически независимыми , если и только если p(x, y) = p(x)p(y). (6.53) Неформально говоря, две случайные величины X и Y независимы, если значе - ние y не дает никакой дополнительной информации об x (и наоборот). Если X, Y статистически независимы, то zp(y | x) = p(y); zp(x | y) = p(x); zX,Y [x + y] = X[x] + Y[y]; zCovX,Y [x, y] = 0. Обратное к последнему утверждению неверно, то есть две случайные величины могут не быть статистически независимыми, но иметь ковариацию 0. Чтобы понять причину, вспомним, что ковариация характеризует только линейную зависимость. Таким образом, случайные величины с нелинейной зависимостью могут иметь нулевую ковариацию. Пример 6.5 Рассмотрим случайную величину X с нулевым средним ( X[x] = 0), и пусть также X[x3] = 0. Положим y = x2 (таким образом, Y зависит от X) и найдем ковариацию (6.36) между X и Y. Однако Cov[x, y] = [xy] − [x][y] = [x3] = 0. (6.54)\n--- Страница 251 ---\n6.4. Обобщающие статистики и независимость 251 В машинном обучении часто встречаются задачи, которые можно моделировать с использованием независимых одинаково распределенных (i.i.d.) случайных величин X1, , XN. Для более чем двух случайных величин термин «независи - мость» (определение 6.10) обычно относится к ситуации, когда все подмножества независимы (см. Pollard (2002, глава 4) и Jacod and Protter (2004, глава 3)). Понятие «одинаково распределенные» означает, что все эти случайные величи - ны отвечают одному и тому же распределению. Другим полезным для машинного обучения понятием будет условная незави - симость. Определение 6.11 (условная независимость). Две случайные величины X и Y условно независимы при условии Z, если и только если p (x, y | z) = p(x | z)p(y | z) для всех z ∈ , (6.55) где  — множество значений случайной величины Z. Чтобы обозначить, что X и Y условно независимы при условии Z, мы пишем X ⫫ Y | Z. В определении 6.11 требуется выполнение равенства (6.55) для каждого из значений z. Можно истолковать (6.55) так: «при заданном z распределение x и y раскладывается на множители». Независимость можно рассмотреть как частный случай условной независимости: X ⫫ Y | ∅. Используя правило произведения (6.22), можно переписать левую часть (6.55) и получить p (x, y | z) = p(x | y, z)p(y | z). (6.56) Сравнивая правую часть (6.55) с (6.56), замечаем, что p(y | z) появляется в обе- их, так что p(x | y, z) = p(x | z). (6.57) Равенство (6.57) дает нам альтернативное определение условной независимости, то есть X ⫫ Y | Z. Такую запись можно понять как «если мы знаем z, знание y не дает нам информации об x». 6.4.6. Скалярные произведения случайных величин Вспомним определение скалярного произведения из раздела 3.2. В этой главе мы введем скалярное произведение двух случайных величин. Если у нас есть две не коррелирующие случайные величины X, Y, то  [x + y] = [x] + [y]. (6.58)\n--- Страница 252 ---\n252 Глава 6. Вероятность и распределения Так как единица измерения дисперсии — это квадрат единицы, в которой из - меряется случайная величина, выражение очень напоминает теорему Пифагора для прямоугольного треугольника c2 = a2 + b2. В дальнейшем мы увидим, как проинтерпретировать геометрически соотноше - ние (6.58) для дисперсий коррелирующих случайных величин. Случайные ве - личины можно рассматривать как элементы векторного пространства и, опре - делив скалярные произведения, изучать геометрические свойства скалярных величин (Eaton, 2007). Определим скалярное произведение случайных вели - чин1 X и Y с нулевыми средними как . (6.59) Заметим, что ковариация симметрична, положительно определена и линейна по каждому из аргументов. Нормой («длиной») случайной величины будет , (6.60) то есть ее стандартное отклонение2. Чем «длиннее» случайная величина, тем больше ее разброс (тем менее мы уверены в ее значении), а случайная величина с длиной 0 — это константа. Посмотрим на угол θ между двумя случайными величинами X и Y: (6.61) Это просто корреляция (определение 6.8) между двумя случайными величина - ми. Это означает, что можно думать о корреляции как об угле между двумя рассматриваемыми геометрически случайными величинами. Из определения 3.7 мы знаем, что X ⊥ Y . В нашем случае это означает, что X и Y орто - гональны тогда и только тогда, когда Cov[ x, y] = 0, то есть они не коррелируют. Эта связь показана на рис. 6.6. 1 Так же можно определить скалярное произведение и для многомерных величин. 2 Cov[x, x] = 0 ⇐⇒ x = 0, Cov[ αx + z, y] = α Cov[x, y] + Cov[ z, y] при α ∈ .\n--- Страница 253 ---\n6.4. Обобщающие статистики и независимость 253 ac bvar[x + y] = var[x] + var[y] var[y]var[x] Рис. 6.6. Геометрия случайных величин. Если случайные величины X и Y не коррелируют, их можно рассматривать как ортого-нальные векторы и приме-нять теорему Пифагора ПРИМЕЧАНИЕ Хотя и очень хочется для сравнения распределений вероят - ностей использовать евклидово расстояние (построенное на основе введенного ранее определения скалярного произведения), но это не лучший способ опре - делить расстояние между распределениями. Вспомним, что вероятности (или плотность распределения) положительны и должны в сумме давать 1. Эти ограничения означают, что распределения «живут» на так называемом про - странстве статистического многообразия. Изучение этого пространства рас - пределений вероятностей называют информационной геометрией. Часто рас - стоянием между распределениями считают расхождение Кульбака — Лейблера, обобщающее те расстояния, которые задают свойства статистического много - образия. Подобно тому как евклидово расстояние является частным случаем метрики (раздел 3.3), расхождение Кульбака — Лейблера является частным случаем двух более общих классов: расхождения Брегмана и f-расхождения. Тема расхождений выходит за пределы содержания нашей книги, так что за подробностями отсылаем читателя к недавно вышедшей книге Amari (2016), одного из основоположников информационной геометрии. \n--- Страница 254 ---\n254 Глава 6. Вероятность и распределения 6.5. ГАУССОВО РАСПРЕДЕЛЕНИЕ Гауссово распределение — самое изученное распределение вероятностей для непрерывных случайных величин1. Его также называют нормальным распреде ­ лением . Оно столь важно из-за своих удобных для вычислений свойств, которые мы обсудим в дальнейшем. В частности, мы будем использовать его при опре - делении правдоподобия и априорных вероятностей в задаче линейной регрессии (глава 9), а для оценки плотности рассматривать смесь гауссовых распределений (глава 11). Гауссово распределение важно и для других областей машинного обучения, например для глубокого обучения, изучения гауссовых процессов и вариаци - онного вывода. Оно широко используется и в других прикладных областях, таких как обработка сигналов (например, фильтр Калмана), управлении (на - пример, линейно-квадратичный регулятор) и статистике (проверка гипотез). Для одномерной случайной величины плотность гауссова распределения за - дается формулой (6.62) Многомерное гауссово распределение полностью определяется вектором средних μ и ковариационной матрицей Σ по формуле (6.63) где x ∈ D. Мы записываем p(x) =  (x | μ, Σ) или X ∼  (μ, Σ). На рис. 6.7 изо - бражено гауссово распределение от двух переменных и соответствующий кон - турный график. На рис. 6.8 изображены гауссовы распределения — одномерное и двумерное — вместе с соответствующими выборками. Частный случай гаус - сова распределения с нулевым средним и единичной дисперсией известен как стандартное нормальное распределение . Гауссианы широко применяются в статистическом оценивании и в машинном обучении, так как дают явные выражения для частных и условных распреде - лений. В главе 9 мы будем активно использовать эти явные выражения при - менительно к линейной регрессии. Преимуществом моделирования с исполь - зованием гауссовых распределений является то, что зачастую не нужны преобразования переменных (раздел 6.7). Так как гауссово распределение 1 Гауссово распределение естественным образом возникает при рассмотрении сумм не - зависимых одинаково распределенных случайных величин. Это утверждение извест - но как центральная предельная теорема (Grinstead and Snell, 1997).\n--- Страница 255 ---\n6.5. Гауссово распределение 255 полностью задается своими средним и дисперсией, нам часто достаточно при - менить преобразования к среднему и ковариации исходной величины. x10 1–1 –5,0–2,50,02,55,07,50,20 0,15 0,10 0,05 p(x1, x2) 0,00 x2 Рис. 6.7. Гауссово распределение от двух случайных величин x1 и x2 /dollar.g00DC2,5 /dollar.g00DC1 1 2,5 7,5 0,0 0 /dollar.g00DC5,0/dollar.g00DC4/dollar.g00DC20 0,000,050,100,150,20 2468 5,02/g86 (a) /n.g007C/.notdef.g0105…/percent.g00BA/.notdef.g00E4/yright/exclam.g00AF…/percent.g00BA/yright /.notdef.g0104/equal.g00C8/three.g0082““/percent.g00BA/quotedbl.g006D/percent.g00BA /exclam.g00AF/equal.g00C8“/C.g00B9/exclam.g00AF/yright/.notdef.g0105/yright/.notdef.g00E3/yright…/comma.g00D2/yright: /asterisk.g007D/exclam.g00AF/yright“/two.g0088/comma.g00D2/asterisk.g007D/percent.g00BA/.notdef.g00E4 /percent.g00BA/two.g0088/.notdef.g00E4/yright/.notdef.g0107/yright…/percent.g00BA “/exclam.g00AF/yright/.notdef.g0105…/yright/yright, /.notdef.g0104/percent.g00BA/exclam.g00AF/comma.g00D2ƒ/percent.g00BA…/two.g0088/equal.g00C8/.notdef.g00E3/.notdef.g0109…/percent.g00BA/L.g00AE /.notdef.g00E3/comma.g00D2…/comma.g00D2/yright/L.g00AE /numbersign.g00DB /exclam.g00AF/equal.g00C8ƒ/K.g00AD/exclam.g00AF/percent.g00BA“x1x2 xp(x)/q.g0076/exclam.g00AF/yright/.notdef.g0105…/yright/yright /braceright.g0077/.notdef.g00E3/yright/.notdef.g00E4/yright…/two.g0088/q.g0076/exclam.g00AF/yright/.notdef.g0105…/yright/yright /braceright.g0077/.notdef.g00E3/yright/.notdef.g00E4/yright…/two.g0088 (b) /d.g0069/quotedbl.g006D/three.g0082/.notdef.g00E4/yright/exclam.g00AF…/percent.g00BA/yright /.notdef.g0104/equal.g00C8/three.g0082““/percent.g00BA/quotedbl.g006D/percent.g00BA /exclam.g00AF/equal.g00C8“/C.g00B9/exclam.g00AF/yright/.notdef.g0105/yright/.notdef.g00E3/yright…/comma.g00D2/yright: /asterisk.g007D/exclam.g00AF/yright“/two.g0088/comma.g00D2/asterisk.g007D/percent.g00BA/.notdef.g00E4 /percent.g00BA/two.g0088/.notdef.g00E4/yright/.notdef.g0107/yright…/percent.g00BA “/exclam.g00AF/yright/.notdef.g0105…/yright/yright, /percent.g00BA/quotedbl.g006D/equal.g00C8/.notdef.g00E3/equal.g00C8/.notdef.g00E4/comma.g00D2 /numbersign.g00DB /.notdef.g00E3/comma.g00D2…/comma.g00D2/comma.g00D2 /exclam.g00AF/equal.g00C8/quotedbl.g006D…/percent.g00BA/L.g00AE /C.g00B9/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/comma.g00D2 Рис. 6.8. Гауссовы распределения и выборка из 100 элементов: (a) одномерный случай; ( b) двумерный случай 6.5.1. Частные и условные распределения — тоже гауссианы В этом разделе мы рассмотрим переход к частным и условным распределениям для многомерных случайных величин. Если при первом прочтении материал кажется слишком сложным, советуем рассмотреть вариант с двумя случайными\n--- Страница 256 ---\n256 Глава 6. Вероятность и распределения величинами. Пусть X и Y — две многомерные случайные величины, возможно, с разными распределениями. Чтобы увидеть эффект от применения правила суммы и перехода к условным вероятностям, запишем гауссово распределение для пар [ x, y]T в явном виде: , (6.64) где Σxx = Cov[ x, x] и Σyy = Cov[ y, y] — частные ковариационные матрицы x и y соответственно, а Σxy = Cov[ x, y] — матрица кросс-ковариации между x и y. Условное распределение p(x | y) также гауссово (показано на рис. 6.9( c)) и за- дается формулами (вывод которых можно увидеть в разделе 2.3 книги Bishop, 2006) p (x | y) =  (μx | y, Σx | y); (6.65) ; (6.66) (6.67) Заметим, что при вычислении среднего в (6.66), значение y нам известно. ПРИМЕЧАНИЕ Условное гауссово распределение часто появляется в задачах, где нас интересует апостериорная вероятность: zФильтр Калмана (Kalman, 1960), один из главных алгоритмов оценки со - стояний при обработке сигналов, всего лишь вычисляет гауссовы условные вероятности совместных распределений (Deisenroth and Ohlsson, 2011; S ärkkä, 2013). zГауссовы процессы (Rasmussen and Williams, 2006) реализуют на практике распределения на функциях. В гауссовом процессе мы предполагаем со - вместную гауссовость случайных величин. Переходя к условным вероят - ностям, мы можем найти апостериорное распределение на функциях. zСкрытые линейные гауссовы модели (Roweis and Ghahramani, 1999; Murphy, 2012), включая вероятностный анализ главных компонент (probabilistic principal component analysis, PPCA) (Tipping and Bishop, 1999). Мы подроб - нее обсудим PPCA в разделе 10.7.  Частное распределение p(x) для гауссова распределения p(x, y) (6.64) само будет гауссовым, может быть вычислено по правилу суммы (6.20) и задается формулой (6.68)\n--- Страница 257 ---\n6.5. Гауссово распределение 257 Аналогичный результат верен для p(y). Неформально говоря, мы смотрим на совместное распределение (6.64) и игнорируем все, что нас не интересует (про - изводя интегрирование). Иллюстрацией служит рис. 6.9( b). Пример 6.6 Рассмотрим двумерное гауссово распределение (показанное на рис. 6.9): (6.69) (a) Двумерное гауссово распределение (b) Частное распределение (c) У словное распределение–1 1 –1,5 –1,5 –1,0 –1,0 –0,5 –0,5 1,5 1,5 0,5 0,5 1,0 1,0 0,0 0,00–4–20 0,00,20,40,60,81,2 1,0 0,00,20,40,62468 2σx1 x1x1x2 = –1x2 p(x1) Среднее 2σp(x1 | x2=–1) Среднее Рис. 6.9. (a) Двумерное гауссово распределение. ( b) Частное распределение для совместного гауссова распределения — гауссово. ( c) Условное распределение также гауссово\n--- Страница 258 ---\n258 Глава 6. Вероятность и распределения Можно вычислить параметры одномерного гауссова распределения при условии x2 = −1, применяя (6.66) и (6.67) для среднего и дисперсии соот - ветственно. Мы получаем μx1 | x2=–1 = 0+(−1) · 0,2 · (−1 − 2) = 0,6 (6.70) и (6.71) Таким образом, условное гауссово распределение выглядит как p (x1 | x2 = −1) =  (0,6, 0,1). (6.72) Частное распределение p(x1), напротив, можно получить по форму - ле (6.68), использующей значения среднего и дисперсии для x1: p (x1) =  (0, 0,3). (6.73) 6.5.2. Произведение гауссовых плотностей При построении модели линейной регрессии (глава 9) нам необходимо вычис - лить гауссово правдоподобие. Возможно, мы также хотим взять гауссово апри - орное распределение (раздел 9.3). Для вычисления апостериорного распре- деления мы применяем теорему Байеса, так что приходится умножать правдоподобие на априорную вероятность. Нам надо найти произведение двух гауссовых плотностей. Перемножив  (x | a, A) и  (x | b, B), получим (с точно - стью до умножения на c ∈ ) гауссово распределение c (x | c, C)1, где C = (A–1 + B–1)–1; (6.74) c = C(A–1a + B–1b); (6.75) (6.76) Константу c также можно записать в виде гауссовой плотности от a или b с ко- вариационной матрицей A + B, то есть c =  (a | b, A + B) =  (b | a, A + B). ПРИМЕЧАНИЕ Для удобства обозначений мы иногда будем использовать функцио нальную запись  (x | m, S) даже в том случае, когда x не является случайной величиной. Мы только что поступили именно так, записав c =  (a | b, A + B) =  (b | a, A + B). (6.77) 1 Доказательство оставлено в качестве упражнения.\n--- Страница 259 ---\n6.5. Гауссово распределение 259 В этой формуле ни a, ни b не являются случайными величинами, но зато такая запись компактнее, чем (6.76).  6.5.3. Суммы и линейные преобразования Если X, Y — независимые гауссовы случайные величины (то есть совместное распределение задается как p(x, y) = p(x)p(y), где p(x) =  (x | μx, Σx) и p(y) = =  (x | μy, Σy), то x + y — также гауссова случайная величина и задается фор- мулой p (x + y) =  (μx + μy, Σx + Σy). (6.78) Зная, что p(x + y) — гауссова, мы можем сразу вычислить среднее и ковариаци - онную матрицу по формулам (6.46)–(6.49). Это свойство будет важным, когда мы будем рассматривать независимые одинаково распределенные шумы, дей - ствующие на случайные величины, — например, в задаче линейной регрессии (глава 9). Пример 6.7 Так как математическое ожидание является линейной операцией, можно рассмотреть взвешенную сумму независимых гауссовых случайных ве - личин: p (ax + by) =  (aμx + bμy, a2Σx + b2Σy). (6.79) ПРИМЕЧАНИЕ В главе 11 нам понадобится взвешенная сумма гауссовых плотностей. Это не то же самое, что взвешенная сумма гауссовых случайных величин.  В теореме 6.12 случайная величина x принадлежит распределению, плотность которого является смесью плотностей p1(x) и p2(x), с весом первого α. Эту тео - рему можно обобщить на случай многомерных случайных величин, так как для них также выполняется свойство линейности математического ожидания. Од - нако возведение в квадрат надо будет заменить на xxT. Теорема 6.12. Рассмотрим смесь двух одномерных гауссовых плотностей p (x) = αp1(x) + (1 − α)p2(x), (6.80) где константа 0 < α < 1 — вес смеси, а p1(x) и p2(x) — одномерные гауссовы плот ­ ности распределения (6.62) с различными параметрами, то есть .\n--- Страница 260 ---\n260 Глава 6. Вероятность и распределения Тогда среднее для смешанной плотности p(x) равно взвешенной сумме средних:  [x] = αμ1 + (1 − α)μ2. (6.81) Дисперсия для смешанной плотности p (x) задается формулой (6.82) Доказательство . Среднее для смешанной плотности p(x) равно взвешенной сумме средних двух случайных величин. Применим определение среднего (определение 6.4), и подставим в формулу смеси (6.80), получив (6.83 a) (6.83 b) (6.83 c) (6.83 d) Для вычисления дисперсии мы можем использовать формулу (6.44), требующую найти матожидание квадрата случайной величины. Будем использовать опре - деление матожидания функции (в данном случае квадрата) от случайной вели - чины (определение 6.3): (6.84 a) (6.84 b) (6.84 c) (6.84 d) где в последнем равенстве мы снова пользовались формулой (6.44), по которой σ2 = [x2] − μ2. Перенесем μ2 в левую часть и получим, что математическое ожи - дание квадрата случайной величины равно сумме квадрата ее матожидания и ее дисперсии. Таким образом, вычитаем (6.83 d) из (6.84 d) и получаем дисперсию  [x] = [x2] − ([x])2 = (6.85 a)\n--- Страница 261 ---\n6.5. Гауссово распределение 261 (6.85 b) (6.85 c)  ПРИМЕЧАНИЕ Вычисления выше верны для любой плотности, но так как гауссова плотность однозначно задается своими средним и дисперсией, плот - ность смеси можно записать в явном виде.  Для смешанной плотности отдельные компоненты можно рассмотреть как ус - ловные распределения. Уравнение (6.85 c) является примером формулы для условной дисперсии, известной также как закон полной дисперсии , которая в общем случае утверждает, что для двух случайных величин X и Y выполняет - ся равенство X[x] = Y[X[x | y]] + Y[X[x | y]], то есть (полная) дисперсия X равна матожиданию условной дисперсии плюс дисперсии условного среднего. В примере 6.17 мы рассматривали стандартную двумерную гауссову случайную величину X и применяли к ней линейное преобразование Ax. Результатом яв - лялась гауссова случайная величина со средним, равным нулю, и ковариацией AAT. Заметим, что прибавление константного вектора изменит среднее распре - деления, но не его дисперсию, то есть случайная величина x + μ гауссова со средним μ и единичной ковариационной матрицей. Таким образом, при любом линейном или аффинном преобразовании гауссова случайная величина оста - ется гауссовой. Рассмотрим гауссову случайную величину X ∼  (μ, Σ). Для заданной матрицы A подходящего размера пусть Y — случайная величина, полученная преобразова - нием y = Ax. Среднее для y мы можем вычислить, пользуясь линейностью матожи дания (6.50):  [y] = [Ax] = A[x] = Aμ. (6.86) Аналогично, дисперсию y можно найти по формуле (6.51):  [y] = [Ax] = A[x]AT = AΣAT. (6.87) Таким образом, распределение случайной величины y задается как p (y) =  (y | Aμ, AΣAT). (6.88) Рассмотрим теперь обратное преобразование: пусть мы знаем, что среднее слу - чайной величины задано линейным преобразованием другой случайной вели -\n--- Страница 262 ---\n262 Глава 6. Вероятность и распределения чины. Пусть для матрицы A ∈ M×N полного ранга, где M ≥ N, y ∈ M — гауссова случайная величина со средним Ax, то есть p (y) =  (y | Ax, Σ). (6.89) Как выглядит соответствующее распределение p(x)? Если матрица A обратима, мы можем записать x = A–1y и применить преобразование из предыдущего па - раграфа. Однако в общем случае A не обратима, и мы воспользуемся подходом, похожим на введение псевдообратных матриц (3.57). А именно, мы домножим обе части на AT, а затем найдем обратную к симметричной положительно опре - деленной ATA и получим . (6.90) Так как x получена из y линейным преобразованием, получаем p (x) =  (x | ATA)–1ATy, (ATA)–1ATΣA(ATA)–1). (6.91) 6.5.4. Семплирование из многомерного гауссова распределения Мы не будем углубляться в тонкости компьютерного семплирования, а за- интересованный читатель может обратиться к книге Gentle (2004). В случае многомерного гауссова распределения процесс состоит из трех этапов: снача - ла с помощью псевдослучайного генератора получить элемент равномерного распределения на интервале [0,1]; затем применить нелинейное преобразова - ние, например преобразование Бокса — Мюллера (Devroye, 1986) и получить элемент из одномерного гауссова распределения, и, наконец, объединить ре - зультаты в вектор из многомерного стандартного нормального распределения  (0, I). В общем случае для многомерного распределения Гаусса (когда среднее может быть отлично от нуля, а ковариационная матрица — от единичной) будем ис - пользовать свойства линейных преобразований гауссовых случайных величин. Пусть мы хотим сгенерировать выборку xi, i = 1, , n, из многомерного гауссова распределения со средним μ и ковариационной матрицей Σ. Мы сгенерируем выборку из многомерного стандартного нормального распределения  (0, I). Чтобы получить выборку из распределения  (μ, Σ), применим свойства линей - ных преобразований гауссовых случайных величин. Если x ∼  (0, I), то y = Ax + μ, где AAT = Σ, будет гауссовой случайной величиной со средним μ\n--- Страница 263 ---\n6.6. Сопряженность и экспоненциальное семейство распределений 263 и ковариационной матрицей Σ. Удобно, например, использовать A из разложения Холецкого (раздел 4.3) ковариационной матрицы Σ = AAT. Преимуществом такого выбора будет то, что A из разложения Холецкого треугольна, что упро - щает вычисления1. 6.6. СОПРЯЖЕННОСТЬ И ЭКСПОНЕНЦИАЛЬНОЕ СЕМЕЙСТВО РАСПРЕДЕЛЕНИЙ Многие «именные» распределения вероятностей из учебников статистики были придуманы для моделирования конкретных типов явлений. Например, в раз- деле 6.5 мы встретились с гауссовым распределением. Распределения порой сложным образом связаны друг с другом (Leemis and McQueston, 2008). Для начинающего выбор подходящего распределения может быть чрезвычайно трудной задачей. Кроме того, многие распределения были придуманы во вре - мена, когда вычисления делались вручную. Естественно задаться вопросом, какие понятия и идеи полезны в эру компьютеров (Efron and Hastie, 2016). В предыдущем разделе мы увидели, что многие вычисления удобно производить с гауссовыми распределениями. Здесь стоит напомнить наши пожелания к рас- пределениям, используемым в машинном обучении: 1. При применении таких утверждений, как теорема Байеса, должно выпол - няться «свойство замкнутости» — применение некоторой операции должно давать распределение того же типа. 2. При получении большего количества данных нам не должны требоваться дополнительные параметры для описания распределения. 3. Так как нам интересно обучение на основе данных, мы хотим, чтобы хорошо работала оценка параметров. Класс распределений, называемый экспоненциальным семейством , обеспечива - ет сочетание широкой применимости с удобством для вычислений и статисти - ческого вывода. Перед тем как мы познакомимся с экспоненциальным семей - ством, однако, рассмотрим еще три «именных» распределения вероятностей: распределение Бернулли (пример 6.8), биномиальное распределение (пример 6.9) и бета-распределение (пример 6.10). 1 Чтобы вычислить разложение Холецкого, необходимо, чтобы матрица была симмет- рична и положительно определена. Ковариационные матрицы обладают этими свой - ствами.\n--- Страница 264 ---\n264 Глава 6. Вероятность и распределения Пример 6.8 Распределение Бернулли — это распределение случайной величины X, принимающей значения x ∈ {0, 1}. Оно задается одним параметром μ ∈ [0, 1] — вероятностью того, что X = 1. Формулы для распределения Бернулли Ber(μ): p (x | μ) = μx(1 − μ)1–x, x ∈ {0, 1}; (6.92)  [x] = μ; (6.93)  [x] = μ(1 − μ), (6.94) где [x] и [x] — среднее и дисперсия случайной величины X. Примером применения распределения Бернулли может служить моделирование бросания монетки (когда нас интересует, например, вероятность выпадения орла). ПРИМЕЧАНИЕ Запись распределения Бернулли, в которой булевы случай - ные величины представлены как 0 и 1 и стоят в показателе степени, часто ис - пользуется в книгах по машинному обучению. Аналогичная запись использу - ется для мультиномиального распределения.  Пример 6.9 (биномиальное распределение) Биномиальное распределение является обобщением распределения Бер - нулли, но значения случайной величины уже будут целыми (рис. 6.10). В частности, биномиальным распределением можно описать вероятность m случаев с X = 1 среди реализаций N бернуллиевской случайной вели - чины с p(X = 1) = μ ∈ [0, 1]. Биномиальное распределение Bin( N, μ) за- дается формулами (6.95)  [m] = Nμ, (6.96)  [m] = Nμ(1 − μ), (6.97) где [m] и [m] — соответственно среднее и дисперсия m. Биномиальное распределение является обобщением распределения Бернулли, фазовым пространством для которого является множество целых чисел (оно по - казано на рис. 6.10).\n--- Страница 265 ---\n6.6. Сопряженность и экспоненциальное семейство распределений 265 0,0 0,0 5,0 2,5 10,0 7,5 15,0 12,50,10,20,3/g80 = 0,1 /g80 = 0,4 /g80 = 0,75p(m) /parenright.g00A0/comma.g00D2“/.notdef.g00E3/percent.g00BA m ƒ…/equal.g00C8/.notdef.g0107/yright…/comma.g00D2/L.g00AE x = 1 /quotedbl.g006D N = 15 /period.g00AA/asterisk.g007D“/C.g00B9/yright/exclam.g00AF/comma.g00D2/.notdef.g00E4/yright…/two.g0088/equal.g00C8/period.g00B2 Рис. 6.10. Примеры биномиальных распределений для μ ∈ {0,1, 0,4, 0,75} и N = 15 Пример использования биномиального распределения: пусть мы хотим описать вероятность получить m орлов в эксперименте из N подбрасываний монеты, если вероятность выпадения орла при одном броске равна μ. Пример 6.10 (бета-распределение) Иногда нам надо моделировать непрерывную случайную величину со значениями на конечном интервале. Бета­распределение — это распределение непрерывной случайной ве - личины μ ∈ [0, 1], которая зачастую используется для нахождения ве - роятности бинарного события (например, она может быть параметром распределения Бернулли). Само бета-распределение Beta( α, β) (пока - зано на рис. 6.11) задается двумя параметрами α > 0, β > 0 и определя - ется как ; (6.98) , (6.99) где Γ(·) — гамма-функция, определяемая как (6.100) (6.101)\n--- Страница 266 ---\n266 Глава 6. Вероятность и распределения /g800,0 0,2 0,4 0,6 0,8 1,00246810 /g68 = 0,5 = /g69 /g68 = 1 = /g69 /g68 = 2, /g69 = 0,3 /g68 = 4, /g69 = 10 /g68 = 5, /g69 = 1p(/g80 | /g68, /g69) Рис. 6.11. Примеры бета-распределе- ний для различ- ных α и β Заметим, что гамма-функции в знаменателе (6.98) нормируют бета-рас - пределение. Неформально говоря, α сдвигает распределение к 1, а β — к 0. Частными случа - ями (Murphy, 2012) являются: zПри α = 1 = β получаем равномерное распределение  [0, 1]. zПри α, β < 1 получаем бимодальное распределение с пиками 0 и 1. zПри α, β > 1 распределение унимодально. zПри α, β > 1 и α = β распределение унимодально, симметрично и центриро - вано на [0, 1], то есть среднее совпадает с модой и равно 1/2. ПРИМЕЧАНИЕ Именные распределения — это целый зоопарк с многооб - разными связями между собой (Leemis and McQueston, 2008). Стоит иметь в виду, что каждое из именных распределений создавалось под определенную задачу, но может иметь и другие приложения. Знание, для каких задач было определено то или иное распределение, часто помогает на практике. Описанные три распределения мы ввели для того, чтобы на примерах продемонстрировать понятия сопряженности (раздел 6.6.1) и экспоненциальных семейств (раз - дел 6.6.3).  6.6.1. Сопряженность Согласно теореме Байеса (6.23), апостериорная вероятность пропорциональна произведению априорной вероятности на правдоподобие. Введение априорно - го распределения сложно по двум причинам. Во-первых, оно должно отражать наши знания о задаче, до того как мы получим данные. Описать их не всегда легко. Во-вторых, апостериорное распределение часто невозможно найти ана -\n--- Страница 267 ---\n6.6. Сопряженность и экспоненциальное семейство распределений 267 литически. Однако существуют удобные для вычислений априорные распреде - ления — сопряженные. Определение 6.13 (сопряженное априорное распределение). Априорное рас - пределение является сопряженным к функции правдоподобия, если апостери - орное распределение имеет тот же тип, что и априорное. Сопряженность особенно удобна, поскольку мы можем найти апостериорное распределение, просто пересчитав параметры априорного. ПРИМЕЧАНИЕ С геометрической точки зрения на распределение вероят - ностей при переходе от правдоподобия к сопряженному распределению сохра - няются расстояния (Agarwal and Daum é III, 2010).  В качестве конкретного примера сопряженного априорного распределения рас - смотрим в примере 6.11 биномиальное распределение дискретной случайной величины и бета-распределение непрерывной случайной величины. Пример 6.11 (бета-биномиальная сопряженность) Рассмотрим биномиальную случайную величину x ∼ Bin( N, μ), где (6.102) — вероятность выпадения x орлов при N бросках монеты ( μ — вероятность выпадения орла). Берем в качеств априорного распределения параметра μ бета-распределение, то есть μ ∼ Beta( α, β), где (6.103) Пусть мы наблюдаем значение x = h, то есть за N бросков выпало h орлов. Апостериорное распределение μ будет выглядеть как p (μ | x = h, N, α, β) ∝ p(x | N, μ)p(μ | α, β) (6.104 a) ∝ μh(1 − μ)(N–h)μα–1(1 − μ)β–1 (6.104 b) = μh+α–1(1 − μ)(N–h)+β–1 (6.104 c) ∝ Beta( h + α, N − h + β), (6.104 d) то есть оно, как и априорное, будет бета-распределением. Таким образом, априорное бета-распределение для параметра μ является сопряженным к биномиальной функции правдоподобия.\n--- Страница 268 ---\n268 Глава 6. Вероятность и распределения В следующем примере мы придем к похожему выводу — покажем, что бета-рас - пределение является сопряженным априорным распределением для распреде - ления Бернулли. Пример 6.12 (сопряженность бета — Бернулли) Пусть x ∈ {0, 1} берется из распределения Бернулли с θ ∈ [0, 1], то есть p(x = 1 | θ) = θ. То же самое можно записать как p(x | θ) = θx(1 − θ)1–x. Пусть θ взята из бета-распределения с параметрами α, β, то есть p(θ | α, β) ∝ θα–1 (1 − θ)β–1. Перемножая бета-распределение и распределение Бернулли, получим p (θ | x, α, β) = p(x | θ)p(θ | α, β) (6.105 a) ∝ θx(1 − θ)1–xθα–1(1 − θ)β–1 (6.105 b) = θα+x–1(1 − θ)β+(1– x)–1 (6.105 c) ∝ p(θ | α + x, β + (1 − x)). (6.105 d) В последней строчке стоит бета-распределение с параметрами ( α + x, β + (1 − x)). В табл. 6.2 приведены примеры сопряженных априорных вероятностей для параметров некоторых стандартных распределений, используемых в вероят - ностном моделировании. Такие распределения, как мультиномиальное, обратное гамма-распределение, обратное распределение Уишарта, встречаются в любом учебнике статистики и описаны, например, в Bishop (2006). Таблица 6.2. Примеры сопряженных априорных распределений для распространенных функций правдоподобия ПравдоподобиеСопряженное априорное распределениеАпостериорное распределение Бернулли Бета Бета Биномиальное Бета Бета Гауссово Гауссово / обратное гамма-распределениеГауссово / обратное гамма-распределение Гауссово Гауссово / обратное распределение УишартаГауссово / обратное распределение Уишарта Мультиномиальное Дирихле Дирихле\n--- Страница 269 ---\n6.6. Сопряженность и экспоненциальное семейство распределений 269 Бета-распределение является сопряженным априорным распределением для параметра μ как для биномиального, так и бернуллиевского правдоподобия. Ког - да функция правдоподобия гауссова, можно выбрать гауссово априорное рас - пределение для среднего. Гауссово распределение указано в таблице дважды, для одномерного и многомерного случая. В одномерном случае сопряженным апри - орным распределением для дисперсии является обратное гамма-распределение. В многомерном случае сопряженным априорным распределением для ковариа - ционной матрицы будет обратное распределение Уишарта. Распределение Ди - рихле является сопряженным априорным распределением для мультиномиальной функции правдоподобия1. Подробнее об этом можно почитать в Bishop (2006). 6.6.2. Достаточные статистики Вспомним, что статистикой случайной величины называется детерминистиче - ская функция от нее. Например, пусть x = [x1, , xN]T — вектор одномерных случайных гауссовых величин, то есть xn ∼  (μ, σ2). Тогда выборочное среднее будет статистикой. Сэр Рональд Фишер — автор понятия достаточной статистики. Идея состоит в том, что некоторые статистики содер - жат всю информацию о распределении, которую можно получить из данных. Иными словами, достаточные статистики содержат всю необходимую инфор - мацию, чтобы делать выводы о генеральной совокупности; их достаточно для описания распределения. Пусть дано множество распределений, параметризованное θ, и X — случайная величина с распределением p(x | θ0) для неизвестного θ0. Вектор статистик ϕ(x) называется достаточной статистикой для θ0, если он содержит всю возможную информацию о θ0. Говоря более строго, вероятность x при условии θ можно раз - ложить на множитель, не зависящий от θ, и множитель, зависящий от θ только через ϕ(x). Формально это понятие описывается теоремой факторизации Фи - шера — Неймана, которую мы приводим под номером 6.14 без доказательства. Теорема 6.14 (Фишера — Неймана). [Теорема 6.5 в Lehmann and Casella (1998)] Пусть плотность вероятности X задается функцией p(x | θ). Тогда статисти ­ ка ϕ(x) будет достаточной для θ, если и только если p(x | θ) может быть запи ­ сана в виде p (x | θ) = h(x)gθ(ϕ(x)), (6.106) 1 Априорное гамма-распределение является сопряженным к обратной дисперсии для одномерного гауссова правдоподобия. Априорное распределение Уишарта является сопряженным к обратной для матрицы ковариации многомерного гауссова правдопо - добия.\n--- Страница 270 ---\n270 Глава 6. Вероятность и распределения где h(x) — распределение, не зависящее от θ, и вся зависимость от θ содержит ­ ся в функции gθ от ϕ(x). Если p(x | θ) не зависит от θ, ясно, что ϕ(x) будет достаточной статистикой при любой функции ϕ. Интересней случай, когда p(x | θ) зависит только от ϕ(x), а не от самого x. В этом случае ϕ(x) — достаточная статистика для θ. В машинном обучении мы имеем дело с конечными выборками. В случае про - стых распределений (например, бернуллиевского из примера 6.8) мы можем оценить параметры распределения по небольшой выборке. Рассмотрим обратную задачу: пусть у нас имеется набор данных (выборка из неизвестного распреде - ления), какое распределение наилучшим образом его описывает? Естественно задаться вопросом, понадобится ли нам большее количество параметров θ при увеличении количества данных? В общем случае — да, и этой темой занимается непараметрическая статистика (Wasserman, 2007). Можно, напротив, искать класс распределений, достаточные статистики для которых конечномерны (то есть количество необходимых для их описания параметров не может неогра - ниченно расти). Ответом будет экспоненциальное семейство распределений, которое мы изучим в следующем разделе. 6.6.3. Экспоненциальное семейство распределений Рассматривать распределения дискретной или непрерывной случайной вели - чины мы можем на трех уровнях абстракции. На первом, наименее абстрактном, мы имеем дело с конкретным именным распределением с фиксированными параметрами, например одномерным гауссовым  (0, 1) с нулевым средним и единичной дисперсией. В машинном обучении часто применяется второй уровень абстракции, когда мы фиксируем вид распределения (одномерное гаус сово) и находим значения параметров по данным. Например, мы предпо - лагаем, что наше распределение — одномерное гауссово  (μ, σ2) с неизвест - ными средним μ и дисперсией σ2, и методом максимального правдоподобия находим параметры ( μ, σ2). Пример мы увидим, рассматривая линейную ре - грессию в главе 9. Третьим уровнем абстракции является рассмотрение се - мейств распределений — в нашей книге мы познакомимся с экспоненциальным семейством. Одномерное гауссово распределение принадлежит этому семей - ству, как и многие другие широко используемые статистические модели (в том числе все «именные» из табл. 6.2). Все они могут быть описаны вместе (Brown, 1986). ПРИМЕЧАНИЕ Краткая историческая справка. Как и многие другие идеи в математике и естественных науках, экспоненциальные семейства были одно -\n--- Страница 271 ---\n6.6. Сопряженность и экспоненциальное семейство распределений 271 временно открыты несколькими исследователями. В 1935–193 6 годах Эдвин Питман в Тасмании, Жорж Дармуа в Париже и Бернард Купман в Нью-Йорке независимо друг от друга показали, что экспоненциальные семейства распреде - лений — единственные обладающие конечномерными достаточными статисти - ками при независимой генерации элементов выборки (Lehmann and Casella, 1998).  Экспоненциальным семейством называется семейство распределений вероят - ностей с параметром θ ∈ D вида , (6.107) где ϕ(x) — векторная достаточная статистика. В общем случае в формуле (6.107) можно использовать любое скалярное произведение. Мы будем использовать стандартное скалярное произведение . Заметим, что вид экс - поненциального семейства является частным случаем выражения gθ(ϕ(x)) из теоремы Фишера — Неймана (теорема 6.14). Множитель h(x) можно внести под знак скалярного произведения, прибавив log h(x) к вектору достаточной статистики ϕ(x) и задав значение соответству - ющего параметра θ0 = 1. Слагаемое A(θ) — это нормировочная константа, обес- печивающая то, что сумма или интеграл от распределения равны 1. Можно получить общее представление об экспоненциальных семействах, игнорируя эти два слагаемых и рассматривая экспоненциальные семейства вида p (x | θ) ∝ exp(θTϕ(x)). (6.108) При такой параметризации θ называются естественными параметрами . На первый взгляд кажется, что экспоненциальные семейства — это неинтересное преобразование скалярного произведения. Однако то, что вся информация о данных будет содержаться в ϕ(x), крайне полезно при построении модели и в вычислениях. Пример 6.13 (гауссианы как экспоненциальное семейство) Рассмотрим одномерное гауссово распределение  (μ, σ2). Пусть Тогда по определению экспоненциального семейства p (x | θ) ∝ exp( θ1x + θ2x2). (6.109)\n--- Страница 272 ---\n272 Глава 6. Вероятность и распределения Взяв (6.110) и подставив в (6.109), получим (6.111) Таким образом, одномерное гауссово распределение принадлежит экспо - ненциальному семейству с достаточной статистикой и есте- ственными параметрами (6.110). Пример 6.14 (распределения Бернулли как экспоненциальное семейство) Вспомним распределение Бернулли из примера 6.8: p (x | μ) = μx(1 − μ)1–x, x ∈ {0, 1}. (6.112) Можно записать его как экспоненциальное семейство p(x | μ) = exp [log( μx(1 − μ)1–x)] = (6.113 a) = exp [ x log μ + (1 − x) log(1 − μ)] = (6.113 b) = exp [ x log μ − x log(1 − μ) + log(1 − μ)] = (6.113 c) (6.113 d) В последней строке (6.113 d) можно узнать формулу для экспоненциаль - ных семейств (6.107), заметив, что h(x) = 1; (6.114) ; (6.115) ϕ (x) = x; (6.116) A (θ) = −log(1 − μ) = log(1+exp( θ)). (6.117)\n--- Страница 273 ---\n6.6. Сопряженность и экспоненциальное семейство распределений 273 Зависимость между θ и μ обратима, так что (6.118) Соотношение (6.118) используется для получения второго из равенств в (6.117). ПРИМЕЧАНИЕ Зависимость между параметром μ исходного распределения Бернулли и естественным параметром θ известна как сигмоид , или логистическая функция. Заметим, что μ ∈ (0, 1), а θ ∈ , так что сигмоид «сжимает» веществен - ную ось в интервал (0, 1). Это свойство важно для машинного обучения и ис- пользуется, например, в логистической регрессии (Bishop, 2006, раздел 4.3.2) и в качестве нелинейной функции активации в нейронных сетях (Goodfellow et al., 2016, глава 6).  Часто неясно, как найти параметрический вид сопряженного к заданному рас - пределения (например, к распределениям из табл. 6.2). Экспоненциальные се - мейства позволяют искать сопряженные пары распределений. Рассмотрим слу- чайную величину X, принадлежащую экспоненциальному семейству (6.107): . (6.119) У любого элемента экспоненциального семейства имеется сопряженное апри - орное распределение (Brown, 1986) , (6.120) где имеет размерность dim( θ) + 1. Достаточными статистиками для со - пряженного распределения будут . Зная общий вид сопряженных рас - пределений для экспоненциальных семейств, мы можем найти сопряженные для конкретных распределений. Пример 6.15 Вспомним запись распределения Бернулли в виде экспоненциального семейства (6.113d): (6.121)\n--- Страница 274 ---\n274 Глава 6. Вероятность и распределения Канонический вид сопряженного распределения: (6.122) где мы определили γ := [α, β + α]T и hc(μ) := μ/(1 − μ). Равенство (6.122) тогда упрощается до p (μ | α, β) = exp[( α − 1) log μ + (β − 1) log(1 − μ) − Ac(α, β)]. (6.123) Не в виде экспоненциального семейства это выглядит как p (μ | α, β) ∝ μα–1(1 − μ)β–1, (6.124) и мы узнаем бета-распределение (6.98). В примере 6.12 мы предположи - ли, что бета-распределение является сопряженным априорным распре - делением к распределению Бернулли, и показали, что это действительно так. В этом примере мы пришли к бета-распределению, записав канони - ческий вид (как экспоненцального семейства) априорного сопряженного распределения к распределению Бернулли. Как упоминалось в предыдущем разделе, главной мотивацией к использованию экспоненциальных семейств является наличие у них конечномерных достаточ - ных статистик. Кроме того, легко записать сопряженное распределение, также принадлежащее экспоненциальному семейству. С точки зрения статистическо - го вывода, оценка максимального правдоподобия хорошо себя ведет, так как оценки достаточных статистик по выборке являются оптимальными оценками этих статистик на генеральной совокупности (вспомним о среднем и ковариации для гауссова распределения). Так как функция логарифма правдоподобия во - гнута, можно применить эффективные методы оптимизации (глава 7). 6.7. ЗАМЕНА ПЕРЕМЕННЫХ / ОБРАТНОЕ ПРЕОБРАЗОВАНИЕ Может показаться, что известных распределений очень много, но на самом деле набор «именных» распределений довольно ограничен. Поэтому часто полезно понимать, каково распределение случайной величины после некоторого преоб - разования. Например, пусть X — случайная величина из одномерного нормаль - ного распределения  (0, 1). Как распределена X2? Другой пример, часто встре - чающийся в машинном обучении: пусть X1 и X2 — одномерные стандартные нормальные распределения. Как распределена ( X1 + X2)/2?\n--- Страница 275 ---\n6.7. Замена переменных / Обратное преобразование 275 Одним из способов найти распределение ( X1 + X2)/2 будет вычислить среднее и дисперсию X1 и X2, а затем работать с ними. Как мы увидели в разделе 6.4.4, мы можем вычислить среднее и дисперсию случайных величин, получающихся при аффинных преобразованиях. Однако мы не всегда можем описать полу - чившееся распределение как функцию. Более того, порой нас интересуют не - линейные преобразования случайных величин, для которых не всегда есть явные формулы. ПРИМЕЧАНИЕ В этом разделе мы говорим о случайных величинах и при- нимаемых ими значениях. Поэтому напомним, что для обозначения случайных величин мы используем заглавные буквы X, Y, а для принимаемых ими значений в фазовом пространстве  — строчные буквы x, y. Мы явно записываем распре - деление дискретной случайной величины X как P(X = x). Для непрерывной случайной величины X (раздел 6.2.2) мы обозначаем плотность распределения как f(x), а функцию распределения — как FX(x).  Рассмотрим два подхода к нахождению распределений случайных величин по - сле преобразований: использующий определение функции распределения и метод замены переменной, использующий цепное правило (раздел 5.2.2). Метод замены переменной широко применяется, так как дает готовый «рецепт», как получить распределение после преобразования. Мы объясним оба подхода на примере одномерных случайных величин, а для многомерных сформулиру - ем только результаты1. Преобразования дискретных случайных величин можно описать напрямую. Пусть X — дискретная случайная величина с распределением P(X = x) (раз - дел 6.2.1), а U(x) — обратимая функция. Рассмотрим преобразованную случай - ную величину Y := U (X) с распределением P(Y = y). Тогда P (Y = y) = P(U(X) = y) интересующее нас преобразование (6.125 a) = P(X = U–1(y)) обратное к нему, (6.125 b) и можно заметить, что x = U–1(y). Следовательно, для дискретных случайных величин преобразование работает на отдельных значениях (соответственно меняются и вероятности). 6.7.1. Метод функций распределения Метод функций распределения отсылает нас к базовым понятиям и использует определение функции распределения как FX(x) = P(X ≤ x) и то, что ее производ- 1 Производящие функции моментов также могут использоваться при изучении преоб - разований случайных величин (Casella and Berger, 2002, глава 2).\n--- Страница 276 ---\n276 Глава 6. Вероятность и распределения ная является плотностью распределения f(x) (Wasserman, 2004, глава 2). Для случайной величины X и функции U найдем плотность распределения случай - ной величины Y := U (X). 1. Найдем функцию распределения: FY(y) = P(Y ≤ y). (6.126) 2. Продифференцируем FY(y) и получим плотность распределения f (y): (6.127) Мы должны также помнить, что при преобразовании может измениться область значений случайной величины. Пример 6.16 Пусть X — непрерывная случайная величина с плотностью вероятности 0 ≤ x ≤ 1, f (x) = 3x2 (6.128) на интервале 0 ≤ x ≤ 1. Найдем плотность вероятности Y = X2. f — возрастающая функция от x, так что значения y лежат в интервале [0, 1]. Получаем FY(y) = P(Y ≤ y) определение функции распределения (6.129 a) = P(X2 ≤ y) преобразование (6.129 b) обратное преобразование (6.129 c) определение функции распределения (6.129 d) функция распределения как определенный интеграл (6.129 e) результат интегрирования (6.129 f) (6.129 g) Таким образом, функция распределения Y (6.130)\n--- Страница 277 ---\n6.7. Замена переменных / Обратное преобразование 277 на интервале 0 ≤ y ≤ 1. Чтобы получить плотность распределения, мы дифференцируем функцию распределения: (6.131) на интервале 0 ≤ y ≤ 1. В примере 6.16 мы рассматривали строго монотонную возрастающую функцию f(x) = 3x2. Поэтому мы могли найти обратную функцию. В общем случае мы требуем, чтобы интересующая нас функция y = U(x) имела обратную 1 x = U–1(y). Можно рассмотреть функцию распределения FX(x) случайной величины X и взять ее в качестве U(x). Это приведет нас к следующей теореме. Теорема 6.15 [Теорема 2.1.10 в Casella and Berger (2002)]. Пусть X — непрерыв ­ ная случайная величина со строго монотонной функцией распределения FX(x). Тогда случайная величина Y, определенная как Y := FX(x), (6.132) имеет равномерное распределение. Теорема 6.15 известна как интегральное преобразование вероятностей и ис- пользуется для создания алгоритмов семплинга (преобразуя результат семплин - га из равномерного распределения) (Bishop, 2006). Алгоритм сначала порожда - ет элемент равномерного распределения, а потом преобразует его с помощью обратной к функции распределения (если она существует), чтобы получить элемент из желаемого распределения. Интегральное преобразование вероят - ности также используется для проверки гипотез о происхождении выборки из данного распределения (Lehmann and Romano, 2005). Идея о принадлежности результата применения функции распределения равномерному распределению также лежит в основе теории копул (Nelsen, 2006). 6.7.2. Замена переменных Метод функций распределения, описанный в разделе 6.7.1, основан на базовых идеях: определении функции распределения, свойствах обратных функций, дифференцирования и интегрирования. Он использует два соображения: 1. Мы можем преобразовать функцию распределения Y в функцию распреде - ления X. 1 Функции, имеющие обратные, называются биективными (раздел 2.7).\n--- Страница 278 ---\n278 Глава 6. Вероятность и распределения 2. Мы можем продифференцировать функцию распределения и получить плот - ность распределения. Проведем доказательство пошагово. Это поможет нам понять более общий метод замены переменных, вводимый в теореме 6.16. ПРИМЕЧАНИЕ Название происходит от метода замены переменной при взятии интегралов1. Для функций одной переменной она описывается формулой где u = g(x). (6.133) Вывод этой формулы основан на правиле дифференцирования компози - ции (5.32) и применении (дважды) основной теоремы анализа. Основная тео - рема анализа формализует тот факт, что дифференцирование и интегрирование «обратны» друг другу. Неформально ее можно объяснить, рассмотрев маленькие изменения (дифференциалы) уравнения u = g(x), то есть Δu = . При под - становке u = g(x) выражение под интегралом в правой части (6.133) превраща - ется в f(g(x)). Поверив, что d u можно приблизить как d u ≈ Δu = , и что dx ≈ Δx, получим (6.133).  Рассмотрим одномерную случайную величину X и обратимую функцию U, дающую нам новую случайную величину Y = U(X). Пусть случайная величина X принимает значения x ∈ [a, b]. По определению функции распределения FY (y) = P(Y ≤ y). (6.134) Нас интересует функция U от случайной величины P (Y ≤ y) = P(U(X) ≤ y), (6.135) где мы предполагаем, что U обратима. Обратимая функция на интервале долж - на либо строго возрастать, либо строго убывать. Если U строго возрастает, об - ратная к ней U–1 также строго возрастает. Применяя обратную функцию U–1 к аргументу в P(U(X) ≤ y), получим P (U(X) ≤ y) = P(U–1(U(X)) ≤ U–1(y)) = P(X ≤ U–1(y)). (6.136) Справа в (6.136) стоит функция распределения X. Вспомним ее определение через плотность распределения (6.137) 1 Метод замены переменной в теории вероятностей основан на замене переменных из математического анализа (Tandra, 2014).\n--- Страница 279 ---\n6.7. Замена переменных / Обратное преобразование 279 Теперь мы выразили функцию распределения Y через x: (6.138) Чтобы найти плотность распределения, продифференцируем (6.138) по y: (6.139) Заметим, что в правой части мы интегрируем по x, а нужен интеграл по y (по- скольку дифференцировать будем по y). Используя (6.133), получаем замену , где x = U–1(y). (6.140) Применяя формулу (6.140) к правой части (6.139), приходим к (6.141) Теперь вспомним, что дифференцирование — линейный оператор. Чтобы не забыть, что fx(U–1(y)) — функция от x, а не от y, используем индекс. Применим основную теорему анализа и получим (6.142) Вспомним наше предположение, что U строго возрастает. Для убывающих функций при тех же рассуждениях мы получим знак «минус». Поэтому, чтобы выражение имело одинаковый вид как для возрастающих, так и для убываю - щих U, будем использовать абсолютное значение дифференциала: (6.143) Такой способ действий называется методом замены переменной . Множитель показывает, как меняется единичный объем при применении U (см. определение якобиана в разделе 5.3). ПРИМЕЧАНИЕ По сравнению с дискретным случаем в (6.125b), у нас по - является дополнительный множитель . Непрерывный случай требу - ет большей аккуратности, поскольку P(Y = y) = 0 для всех y, и плотность вероят - ности f(y) не описывается как вероятность некоторого касающегося y события. \n--- Страница 280 ---\n280 Глава 6. Вероятность и распределения До сих пор в этом разделе мы изучали замену переменных в одномерном случае. Случай многомерной случайной величины аналогичен, но сложнее, из-за того что для многомерных функций вместо абсолютного значения производной надо использовать якобиан. Вспомним формулу (5.58): матрица Якоби — это матри - ца частных производных, и отличие ее определителя от нуля показывает, что она обратима. В разделе 4.1 обсуждалось, что детерминант в формуле появля - ется из-за того, что дифференциалы (считаем их кубами) матрица Якоби пре - вращает в параллелепипеды. Подведем итог всему сказанному выше в следую - щей теореме, дающей нам алгоритм для многомерной замены переменных. Теорема 6.16. [Теорема 17.2 в Billingsley (1995)] Пусть f(x) — плотность рас ­ пределения многомерной непрерывной случайной величины X. Если векторнознач ­ ная функция y = U(x) дифференцируема и обратима для всех x из области определения, то для соответствующих значений y плотность вероятности случайной величины Y = U(X) задается формулой (6.144) На первый взгляд эта теорема выглядит пугающе, однако ее основная идея со - стоит в том, что замена переменных для многомерных случайных величин аналогична этой процедуре для одномерных. Сначала мы находим обратное преобразование, подставляем его в функцию плотности, потом вычисляем яко - биан и перемножаем результаты. В примере ниже рассмотрен случай двумерной случайной величины. Пример 6.17 Рассмотрим двумерную случайную величину X со значениями и плотностью распределения (6.145) Используем метод замены переменной из теоремы 6.16, чтобы выяснить эффект от применения к случайной величине линейного преобразования (раздел 2.7). Возьмем матрицу A ∈ 2×2: (6.146) Мы хотим найти плотность вероятности для преобразованной случайной величины Y, принимающей значения y = Ax. При замене переменных нам требуется обратное преобразование, задающее x как функцию от y. Так\n--- Страница 281 ---\n6.7. Замена переменных / Обратное преобразование 281 как мы имеем дело с линейными преобразованиями, обратное преобра - зование задается обратной матрицей (раздел 2.2.2). Для матриц размера 2 × 2 можно явно записать обратную: (6.147) Заметим, что ad – bc — это детерминант (раздел 4.1) матрицы A. Соот - ветствующая плотность распределения задается формулой (6.148) Частная производная от произведения матрицы на вектор относительно вектора — это сама матрица (раздел 5.5), и, следовательно, . (6.149) Вспомним (раздел 4.1), что детерминант обратной матрицы обратен к де- терминанту исходной, так что якобиан равен (6.150) Теперь мы можем применить формулу замены переменных из теоре - мы 6.16, перемножив (6.148) и (6.150) и получив (6.151 a) (6.151 b) Хотя в примере 6.17 используется двумерная случайная величина, что позво - ляет нам легко вычислить обратную матрицу, соотношение выше выполняется и для больших размерностей. ПРИМЕЧАНИЕ В разделе 6.5 мы видели, что плотность f(x) в (6.148) — стан - дартная гауссова, а f(y) после преобразования — гауссова плотность с ковари - ационной матрицей Σ = AAT.  Идеи этой главы будут использоваться в разделе 8.4 для описания вероятност - ного моделирования и в разделе 8.5 при введении графического языка. Их при - менение напрямую в машинном обучении будет рассмотрено в главах 9 и 11.\n--- Страница 282 ---\n282 Глава 6. Вероятность и распределения 6.8. ДЛЯ ДАЛЬНЕЙШЕГО ЧТЕНИЯ Изложение в этой главе довольно сжато. Более постепенное введение в тему, подходящее для самообразования, есть в книгах Grinstead and Snell (1997) и Walpole et al. (2011). Читатели, заинтересованные в более философских аспек - тах теории вероятностей, могут обратиться к Hacking (2001), тогда как подход Downey (2014) больше ориентирован на применение в программировании. Обзор экспоненциальных семейств можно найти в Barndorff-Nielsen (2014). В главе 8 мы больше узнаем о том, как использовать распределения вероятностей в задачах машинного обучения. Недавний всплеск интереса к нейронным сетям привлек внимание и к вероятностным моделям. Например, идея нормализации потоков (Jimenez Rezende and Mohamed, 2015) основана на замене переменных при преобразовании случайных величин. Обзор методов вариационного выво - да в применении к нейронным сетям дан в главах 16–20 книги Goodfellow et al. (2016). Мы обошли большую часть трудностей, касающихся непрерывных случайных величин, так как не касались вопросов теории меры (Billingsley, 1995; Pollard, 2002) и приняли без объяснений существование вещественных чисел, способы задания их подмножеств и определения вероятностей на этих подмножествах. Между тем эти подробности важны, например при определении условной ве - роятности p(y | x) для непрерывных случайных величин x, y (Proschan and Presnell, 1998). Не слишком подробная запись скрывает тот факт, что мы рас - сматриваем ситуацию X = x (нулевое множество). Далее, мы рассматриваем плотность вероятности для y. Более строгой записью было бы y[f(y) | σ(x)], где мы рассматриваем матожидание относительно y тестовой функции f при усло - вии σ-алгебры, заданной x. Читатели с более высоким уровнем подготовки, интересующиеся теорией вероятностей, имеют широкий выбор: Jaynes, 2003; MacKay, 2003; Jacod and Protter, 2004; Grimmett and Welsh, 2014, в том числе среди очень продвинутых текстов — Shiryayev, 1984; Lehmann and Casella, 1998; Dudley, 2002; Bickel and Doksum, 2006; Çinlar, 2011. Альтернативный подход к теории вероятностей — начать с понятия математического ожидания и дви- гаться «назад» к свойствам вероятностного пространства (Whittle, 2000). Так как машинное обучение позволяет нам моделиовать более нетривиальные рас - пределения на более сложных типах данных, разработчик вероятностных моде - лей обучения должен разбираться в этих тонкостях. К текстам о машинном обучении, фокусирующимся на верояностном моделировании, относятся MacKay (2003); Bishop (2006); Rasmussen and Williams (2006); Barber (2012); Murphy (2012).\n--- Страница 283 ---\nУпражнения 283 УПРАЖНЕНИЯ 6.1. Рассмотрим следующее двумерное распределение p(x, y) двух дискретных случайных величин X и Y. Вычислите: a. Маргинальные распределения p(x) и p(y). b. Условные распределения p(x | Y = y1) и p(y | X = x3). 6.2. Рассмотрим смесь двух гауссовых распределений (показанную на рис. 6.4): a. Вычислите предельные распределения для каждого измерения. b. Вычислите среднее значение, моду и медианное значение для каждого предельного распределения. c. Вычислите среднее значение и моду для двумерного распределения. 6.3. Вы написали компьютерную программу, которая иногда компилируется, а иногда нет (код не меняется). Вы решаете смоделировать кажущуюся стоха - стичность (успех или отсутствие успеха) x компилятора, используя распреде - ление Бернулли с параметром μ: p(x | μ) = μx(1 − μ)1–x, x ∈ {0, 1}. Выберите сопряженное априорное распределение для вероятности Бернулли и вычислите апостериорное распределение p(μ | x1, , xN). 6.4. Пусть есть две сумки. В первой сумке четыре манго и два яблока; во второй сумке четыре манго и четыре яблока. Также есть несимметричная монета, которая показывает «орел» с вероятно - стью 0,6 и «решку» с вероятностью 0,4. Если на монете изображен «орел», на - угад выбираем фрукт из мешка 1; в противном случае наугад выбираем фрукт из мешка 2.\n--- Страница 284 ---\n284 Глава 6. Вероятность и распределения Ваш друг подбрасывает монету (вы не видите результат), наугад выбирает фрукт из соответствующей сумки и преподносит вам манго. Какова вероятность того, что манго было взято из мешка 2? Подсказка : используйте теорему Байеса. 6.5. Рассмотрим модель временных рядов где w, v — независимые одинаково распределенные переменные гауссова шума. Далее, предположим, что p(x0) =  (μ0, ∑0). a. Каков вид p(x0, x1, , xT)? Обоснуйте свой ответ (явно вычислять совмест - ное распределение не нужно). b. Предположим, что p(xt | y1, , yt) =  (μt, Σt). 1. Вычислите p(xt+1 | y1, , yt). 2. Вычислите p(xt+1, yt+1 | y1, , yt). 3. В момент времени t + 1 наблюдается значение . Вычислите условное распределение p(xt+1 | y1, , yt+1). 6.6. Докажите связь в (6.44), которая связывает стандартное определение дис - персии с выражением необработанной оценки для дисперсии. 6.7. Докажите связь в (6.45), которая связывает попарную разницу между примерами в наборе данных с выражением необработанной оценки для дис - персии. 6.8. Выразите распределение Бернулли в форме естественного параметра экс - поненциального семейства, см. (6.107). 6.9. Выразите биномиальное распределение в виде экспоненциального семей - ного распределения. Выразите также бета-распределение в виде экспоненци - ального семейного распределения. Покажите, что продукты бета-распределения и биномиального распределения также принадлежат к экспоненциальному семейству. 6.10. Выведите связь из раздела 6.5.2 двумя способами: a. Завершив квадрат. b. Выражая гауссиан в форме семейства экспоненциальных распределений. Произведение двух гауссианов  (x | a, A)  (x | b, B) является ненормали - зованным гауссовым распределением c (x | c, C) с\n--- Страница 285 ---\nУпражнения 285 C = (A–1 + B–1)–1; c = C(A–1a +B–1b); Обратите внимание, что нормализующая константа c сама по себе может считаться (нормализованным) распределением Гаусса либо в a, либо в b с «завышенной» ковариационной матрицей A + B, то есть  (a | b, A + B) = =  (b | a, A + B). 6.11. Повторяющиеся ожидания Рассмотрим две случайные величины x, y с совместным распределением p(x, y). Покажите, что X[x] = Y[X[x | y]]. Здесь X[x | y] обозначает математическое ожидание x при условном распреде - лении p(x | y). 6.12. Манипулирование гауссовыми случайными величинами Рассмотрим гауссову случайную величину x ∼  (x | μx, Σx), где x ∈ D. Кроме того, есть y = Ax + b + w, где y ∈ E, A ∈ E×D, b ∈ E и w ∼  (w | 0, Q) является независимым гауссовым шумом. «Независимый» означает, что x и w — независимые случайные величи - ны, а Q диагональна. a. Запишите вероятность p (y | x). b. Распределение гауссово. Вычислите среднее μy и ковариацию Σy. Получите подробный результат. c. Случайная величина y преобразуется в соответствии с отображением из - мерений z = Cy + v, где z ∈ F, C ∈ F×E, и v ∼  (v | 0, R) является независимым гауссовым (измерительным) шумом. yЗапишите p(z | y). yВычислите p(z), то есть среднее значение µz и ковариацию Σz. Полу - чите подробный результат.\n--- Страница 286 ---\n286 Глава 6. Вероятность и распределения d. Теперь измеряется значение . Вычислите апостериорное распределение p (x | ). Подсказка : эта апостериорная функция также является гауссовой, то есть нуж - но определить только ее среднее значение и матрицу ковариации. Начните с явного вычисления совместного гауссова p(x, y). Также требуются вычисления кросс-ковариаций Covx,y[x, y] и Covy,x[y, x]. Затем примените правила гауссовой обусловленности. 6.13. Интегральное преобразование вероятности Учитывая непрерывную случайную величину x, с помощью кумулятивной функции распределения Fx(x) покажите, что случайная величина y = Fx(x) равномерно распределена.\n--- Страница 287 ---\n7 Непрерывная оптимизация Поскольку алгоритмы машинного обучения реализуются на компьютере, их математические основы выражаются как численные оптимизационные методы. В этой главе описаны базовые численные методы для обучения моделей МО. Зачастую обучение модели МО сводится к нахождению хорошего набора пара - метров. Что такое «хорошо» — зависит от целевой функции или вероятностной модели, примеры которых мы увидим во второй части этой книги. При наличии целевой функции находим наилучшее значение при помощи алгоритмов опти - мизации. В этой главе рассматриваются две основные ветви непрерывной оптимизации (рис. 7.1): неограниченная и ограниченная1. Далее мы предположим, что наша целевая функция является дифференцируемой (глава 5), поскольку нам доступен градиент в каждой точке пространства, что помогает нам найти оптимальное значение. По существующему соглашению, большинство целевых функций в машинном обучении требуется минимизировать, то есть наилучшим значением считается минимальное. Интуитивно нахождение наилучшего значения подобно нахождению минимумов функции, тогда как градиент соответствует движению вверх. Идея в том, чтобы двигаться вниз (по направлению, противоположному градиенту) и надеяться, что удастся найти самую глубокую точку. Для неограни - ченной оптимизации только эта концепция нам и требуется, но при проектиро - вании есть несколько вариантов выбора, которые мы обсудим в разделе 7.1. Для ограниченной оптимизации потребуется ввести другие концепции, чтобы спра - виться с ограничениями (раздел 7.2). В этой главе мы познакомимся с особым классом задач (задачи выпуклой оптимизации в разделе 7.3), в рамках которых сможем высказывать суждения о достижении глобального оптимума. 1 Поскольку мы рассматриваем данные и модели в D, здесь мы имеем дело именно с за- дачами непрерывной оптимизации, которая противопоставляется комбинаторной опти - мизации; задачи последней предназначены для работы с дискретными переменными.\n--- Страница 288 ---\n288",
      "debug": {
        "start_page": 226,
        "end_page": 288
      }
    },
    {
      "name": "Глава 7. Непрерывная оптимизация 287",
      "content": "--- Страница 288 --- (продолжение)\nГлава 7. Непрерывная оптимизация /uni041D/uni0435/uni043F/uni0440/uni0435/uni0440/uni044B/uni0432/uni043D/uni0430/uni044F /uni043E/uni043F/uni0442/uni0438/uni043C/uni0438/uni0437/uni0430/uni0446/uni0438/uni044F /uni041D/uni0435/uni043E/uni0433/uni0440/uni0430/uni043D/uni0438/uni0447/uni0435/uni043D/uni043D/uni0430/uni044F /uni043E/uni043F/uni0442/uni0438/uni043C/uni0438/uni0437/uni0430/uni0446/uni0438/uni044F /uni041E/uni0433/uni0440/uni0430/uni043D/uni0438/uni0447/uni0435/uni043D/uni043D/uni0430/uni044F /uni043E/uni043F/uni0442/uni0438/uni043C/uni0438/uni0437/uni0430/uni0446/uni0438/uni044F/uni0413/uni0440/uni0430/uni0434/uni0438/uni0435/uni043D/uni0442/uni043D/uni044B/uni0439 /uni0441/uni043F/uni0443/uni0441 /uni043A/uni041F/uni043E/uni0448/uni0430/uni0433/uni043E /uni0432/uni043E /uni0418/uni043C/uni043F/uni0443/uni043B/uni044C/uni0441 /uni0421/uni0442/uni043E/uni0445/uni0430/uni0441/uni0442/uni0438/uni0447/uni0435/uni0441/uni043A/uni0438/uni0439 /uni0433/uni0440/uni0430/uni0434/uni0438/uni0435/uni043D/uni0442/uni043D/uni044B/uni0439 /uni0441/uni043F/uni0443/uni0441/uni043A /uni041C/uni043D/uni043E/uni0436/uni0438 /uni0442/uni0435/uni043B/uni0438 /uni041B/uni0430/uni0433/uni0440/uni0430/uni043D/uni0436 /uni0430 /uni0412/uni044B/uni043F/uni0443/uni043A /uni043B/uni0430/uni044F /uni043E/uni043F/uni0442/uni0438/uni043C/uni0438/uni0437/uni0430/uni0446/uni0438/uni044F /uni0438 /uni0434/uni0443/uni0430/uni043B/uni044C/uni043D/uni043E/uni0441 /uni0442/uni044C /uni0412/uni044B/uni043F/uni0443/uni043A /uni043B/uni043E/uni0435 /uni0441/uni043E/uni043F/uni0440/uni044F/uni0436/uni0435/uni043D/uni043D/uni043E/uni0435/uni041B/uni0438/uni043D/uni0435/uni0439/uni043D/uni043E/uni0435 /uni043F/uni0440/uni043E/uni0433/uni0440/uni0430/uni043C/uni043C/uni0438/uni0440/uni043E/uni0432/uni0430/uni043D/uni0438 /uni0435 /uni041A/uni0432/uni0430/uni0434/uni0440/uni0430/uni0442/uni0438/uni0447/uni043D/uni043E /uni0435 /uni043F/uni0440/uni043E/uni0433/uni0440/uni0430/uni043C/uni043C/uni0438/uni0440/uni043E/uni0432/uni0430/uni043D/uni0438 /uni0435/uni0413/uni043B/uni0430/uni0432/uni0430 10. /uni0421/uni043D/uni0438/uni0436/uni0435/uni043D/uni0438/uni0435 /uni0440/uni0430/uni0437/uni043C/uni0435/uni0440/uni043D/uni043E/uni0441 /uni0442/uni0438 /uni0413/uni043B/uni0430/uni0432/uni0430 11. /uni041E/uni0446/uni0435/uni043D/uni043A/uni0430 /uni043F/uni043B /uni043E/uni0442/uni043D/uni043E/uni0441/uni0442/uni0438 /uni0413/uni043B/uni0430/uni0432/uni0430 12. /uni041A/uni043B/uni0430/uni0441/uni0441/uni0438/uni0444/uni0438/uni043A/uni0430/uni0446/uni0438/uni044F/uni0412/uni044B/uni043F/uni0443/uni043A /uni043B/uni044B/uni0435 Рис. 7.1. Ассоциативная карта концепций, связанных с оптимизацией, так, как они представлены в этой главе. Две основных идеи из этой карты — градиентный спуск и выпуклая оптимизация Рассмотрим функцию на рис. 7.2. У этой функции есть глобальный минимум в точке около x = −4,5, значение целевой функции для которого равно около −47. Поскольку эта функция гладкая, градиенты могут пригодиться при на - хождении минимума и указывать, куда следует сделать шаг в его поисках — влево или вправо. При этом предполагается, что мы в правильной впадине функции, поскольку там есть еще один локальный минимум в районе x = 0,7.\nГлава 7. Непрерывная оптимизация /uni041D/uni0435/uni043F/uni0440/uni0435/uni0440/uni044B/uni0432/uni043D/uni0430/uni044F /uni043E/uni043F/uni0442/uni0438/uni043C/uni0438/uni0437/uni0430/uni0446/uni0438/uni044F /uni041D/uni0435/uni043E/uni0433/uni0440/uni0430/uni043D/uni0438/uni0447/uni0435/uni043D/uni043D/uni0430/uni044F /uni043E/uni043F/uni0442/uni0438/uni043C/uni0438/uni0437/uni0430/uni0446/uni0438/uni044F /uni041E/uni0433/uni0440/uni0430/uni043D/uni0438/uni0447/uni0435/uni043D/uni043D/uni0430/uni044F /uni043E/uni043F/uni0442/uni0438/uni043C/uni0438/uni0437/uni0430/uni0446/uni0438/uni044F/uni0413/uni0440/uni0430/uni0434/uni0438/uni0435/uni043D/uni0442/uni043D/uni044B/uni0439 /uni0441/uni043F/uni0443/uni0441 /uni043A/uni041F/uni043E/uni0448/uni0430/uni0433/uni043E /uni0432/uni043E /uni0418/uni043C/uni043F/uni0443/uni043B/uni044C/uni0441 /uni0421/uni0442/uni043E/uni0445/uni0430/uni0441/uni0442/uni0438/uni0447/uni0435/uni0441/uni043A/uni0438/uni0439 /uni0433/uni0440/uni0430/uni0434/uni0438/uni0435/uni043D/uni0442/uni043D/uni044B/uni0439 /uni0441/uni043F/uni0443/uni0441/uni043A /uni041C/uni043D/uni043E/uni0436/uni0438 /uni0442/uni0435/uni043B/uni0438 /uni041B/uni0430/uni0433/uni0440/uni0430/uni043D/uni0436 /uni0430 /uni0412/uni044B/uni043F/uni0443/uni043A /uni043B/uni0430/uni044F /uni043E/uni043F/uni0442/uni0438/uni043C/uni0438/uni0437/uni0430/uni0446/uni0438/uni044F /uni0438 /uni0434/uni0443/uni0430/uni043B/uni044C/uni043D/uni043E/uni0441 /uni0442/uni044C /uni0412/uni044B/uni043F/uni0443/uni043A /uni043B/uni043E/uni0435 /uni0441/uni043E/uni043F/uni0440/uni044F/uni0436/uni0435/uni043D/uni043D/uni043E/uni0435/uni041B/uni0438/uni043D/uni0435/uni0439/uni043D/uni043E/uni0435 /uni043F/uni0440/uni043E/uni0433/uni0440/uni0430/uni043C/uni043C/uni0438/uni0440/uni043E/uni0432/uni0430/uni043D/uni0438 /uni0435 /uni041A/uni0432/uni0430/uni0434/uni0440/uni0430/uni0442/uni0438/uni0447/uni043D/uni043E /uni0435 /uni043F/uni0440/uni043E/uni0433/uni0440/uni0430/uni043C/uni043C/uni0438/uni0440/uni043E/uni0432/uni0430/uni043D/uni0438 /uni0435/uni0413/uni043B/uni0430/uni0432/uni0430 10. /uni0421/uni043D/uni0438/uni0436/uni0435/uni043D/uni0438/uni0435 /uni0440/uni0430/uni0437/uni043C/uni0435/uni0440/uni043D/uni043E/uni0441 /uni0442/uni0438 /uni0413/uni043B/uni0430/uni0432/uni0430 11. /uni041E/uni0446/uni0435/uni043D/uni043A/uni0430 /uni043F/uni043B /uni043E/uni0442/uni043D/uni043E/uni0441/uni0442/uni0438 /uni0413/uni043B/uni0430/uni0432/uni0430 12. /uni041A/uni043B/uni0430/uni0441/uni0441/uni0438/uni0444/uni0438/uni043A/uni0430/uni0446/uni0438/uni044F/uni0412/uni044B/uni043F/uni0443/uni043A /uni043B/uni044B/uni0435 Рис. 7.1. Ассоциативная карта концепций, связанных с оптимизацией, так, как они представлены в этой главе. Две основных идеи из этой карты — градиентный спуск и выпуклая оптимизация Рассмотрим функцию на рис. 7.2. У этой функции есть глобальный минимум в точке около x = −4,5, значение целевой функции для которого равно около −47. Поскольку эта функция гладкая, градиенты могут пригодиться при на - хождении минимума и указывать, куда следует сделать шаг в его поисках — влево или вправо. При этом предполагается, что мы в правильной впадине функции, поскольку там есть еще один локальный минимум в районе x = 0,7.\n--- Страница 289 ---\n289 Непрерывная оптимизация Как вы помните, можно найти решения функции для всех ее стационарных точек1, рассчитав ее производную и установив ее равной 0. Для (7.1) имеем соответствующий градиент вида . (7.2)Цель Значение параметра2 1 0 –1 –2 –3 –4 –5 –6–6060 –4040 –2020 0x4 + 7x3 + 5x2 – 17x + 3 Рис. 7.2. Пример целевой функции. Градиенты обозначены стрелками, а глобальный минимум ограничен прерывистой линией Поскольку это кубическое уравнение, у него, как правило, три решения при равенстве нулю. В данном примере два из них — это минимумы, а одно — мак - симум (около x = −1,4). Чтобы проверить, является ли стационарная точка минимумом или максимумом, необходимо повторно взять производную и про- верить, какова вторая производная в стационарной точке — положительная или отрицательная. В нашем случае вторая производная равна (7.3) 1 Стационарные точки – это вещественные корни производной, то есть точки с нулевым градиентом.\n--- Страница 290 ---\n290 Глава 7. Непрерывная оптимизация Подставив оцененные на глазок значения x = −4,5, −1,4, 0,7, мы, как и ожидалось, увидим, что средняя точка соответствует максимуму , а две другие стационарные точки — минимумам. Обратите внимание: в дискуссии выше мы избегали аналитического подхода к нахождению значений x, хотя такой подход можно применить с многочленами низшего порядка, такими как предыдущий. Как правило, мы не в состоянии найти аналитическое решение и поэтому должны начинать с какого-то значения, скажем, x0 = −10, и следовать градиенту1. Градиент указывает, что мы должны двигаться вправо, но не конкретизирует, как далеко. Более того, если бы мы начали с правого края (например, x0 = 0), то градиент привел бы нас не к тому минимуму. На рис. 7.2 проиллюстрировано, что для x > −1 градиент направлен к минимуму (в правой части рисунка), у которого большее целевое значение. В разделе 7.3 мы познакомимся с классом выпуклых функций, для которых не характерна такая хитрая зависимость от начальной точки алгоритма. Для вы - пуклых функций все локальные минимумы равны глобальному минимуму. Оказывается, что многие целевые функции в области машинного обучения проектируются так, чтобы они получались выпуклыми, и пример этого будет показан в главе 12. До сих пор речь в этой главе шла об одномерной функции, при работе с которой мы могли наглядно представить идеи градиентов, направлений спуска и опти- мальных значений. В оставшейся части главы мы будем развивать те же идеи в более высоких измерениях. К сожалению, визуализировать концепции мы можем только в одномерном виде, но некоторые концепции не обобщаются не - посредственно до высших измерений, поэтому читать этот материал нужно внимательно. 7.1. ОПТИМИЗАЦИЯ С ИСПОЛЬЗОВАНИЕМ ГРАДИЕНТНОГО СПУСКА Рассмотрим, как найти решение для минимума вещественнозначной функции , (7.4) где f : d →  — это целевая функция, в которой заключена стоящая перед нами задача машинного обучения. Мы предполагаем, что функция f дифференциру - ема, и мы неспособны аналитически найти решение в замкнутой форме. 1 Согласно теореме Абеля — Руффини, как правило, не существует алгебраического решения для многочленов степени 5 или выше (Abel, 1826).\n--- Страница 291 ---\n7.1. Оптимизация с использованием градиентного спуска 291 Градиентный спуск — это оптимизационный алгоритм первого порядка. Чтобы найти локальный минимум функции при помощи градиентного спуска, нужно делать шаги, пропорциональные отрицательному значению, равному по модулю градиенту функции в данной точке. Как вы помните из раздела 5.1, градиент указывает направление самого крутого подъема1. Еще одно интуитивное по - нятие, которое полезно здесь учесть, это набор линий, вдоль которых функция сохраняет определенное значение ( f(x) = c для некоторого значения c ∈ ); эти линии также называются контурными. Мы хотим оптимизировать точки гра - диента в направлении, ортогональном контурным линиям. Рассмотрим функции со многими переменными. Представим поверхность (описываемую функцией f(x)), когда «шар» стартует в конкретной точке x0. Когда шар трогается с места, он начинает катиться вниз в направлении самого крутого спуска. При градиентном спуске используется тот факт, что f(x0) сни - жается быстрее всего при переходе от x0 в направлении отрицательного гради - ента −((∇f)(x0))T от f в x0. В этой книге предполагается, что функции диффе - ренцируемы, и отсылаем читателя к более общим характеристикам, изложенным в разделе 7.4. Тогда, если x1 = x0 − γ((∇f)(x0))T (7.5) для небольшого пошагового γ ≥ 0, то для f(x1) ≤ f(x0). Обратите внимание, что здесь мы используем транспонирование для градиента, поскольку в противном случае измерения не складываются. Данное наблюдение позволяет нам определить простой алгоритм для градиент - ного спуска: если мы хотим найти локальный оптимум f(x*) от функции f : n → , , мы начинаем с исходной гипотезы x0 о параметрах, которые хотим оптимизировать, а затем перебираем в соответствии с xi+1 = xi − γi((∇f)(xi))T. (7.6) Для подходящего пошагового γi, последовательность f(x0) ≥ f(x1) ≥ … сходится к локальному минимуму. Пример 7.1 Рассмотрим квадратичную функцию в двух измерениях (7.7) 1 Для градиентов используется то же соглашение, что действует для векторов.\n--- Страница 292 ---\n292 Глава 7. Непрерывная оптимизация с градиентом (7.8) Начиная с исходного местоположения x0 = [−3,−1]Т, мы последовательно применяем (7.6) для получения последовательности оценок, которые сходятся у минимального значения (проиллюстрировано на рис. 7.3). Видим (как на рисунке, так и подставив x0 в (7.8) со значением γ = 0,085), что градиент в x0 указывает на северо-восток, приводя к x1 = [−1,98, 1,21]Т. Повторив этот аргумент, получим x2 = [−1,32, −0,42]Т и так далее. ПРИМЕЧАНИЕ Поблизости от минимума градиентный спуск может быть относительно медленным: его асимптотический темп сходимости уступает многим другим методам. Взяв в качестве аналогии мяч, скатывающийся по склону холма, отметим, что если катится по узкой длинной канавке, то задача поставлена некорректно (Trefethen and Bau III, 1997). В случае некорректно поставленных выпуклых задач градиентный спуск все сильнее принимает фор - му зигзага, поскольку градиент направлен практически перпендикулярно крат - чайшему направлению к точке минимума (рис. 7.3).  x10020406080 0 22 4 –2–2–11 –4x2 Рис. 7.3. Градиентный спуск на двумерной квадратичной поверхности (показанной в виде тепловой карты). Описание см. в примере 7.1\n--- Страница 293 ---\n7.1. Оптимизация с использованием градиентного спуска 293 7.1.1. Размер шага Как упоминалось выше, при градиентном спуске важно правильно выбрать размер шага1. Если шаг слишком мал, то градиентный спуск может получиться медленным. Если выбранный размер шага слишком велик, то градиентный спуск может «зашкалить», не сойтись и даже разойтись. В следующем разделе мы по - говорим об использовании импульса. Этот метод сглаживает эрратические явления при обновлениях градиента и гасит осцилляции. Адаптивные градиентные методы позволяют перемасштабировать размер шага на каждой итерации, в зависимости от локальных свойств. Здесь есть две про - стые эвристики (Toissant, 2012): zКогда после шага градиента значение функции увеличивается, это означает, что выбранный размер шага был слишком велик. Отмените шаг и уменьши - те размер шага. zzКогда после шага значение функции уменьшается, это означает, что шаг мог бы быть и побольше. Попробуйте увеличить размер шага. Хотя «отмена» шага кажется пустой тратой ресурсов, использование такой эв - ристики гарантирует монотонную сходимость. Пример 7.2 (решение системы линейных уравнений) При решении линейных уравнений вида Ax = b, на практике мы прибли - зительно решаем Ax − b = 0, находя , минимизирующий квадратичную ошибку || Ax – b ||2 = (Ax − b)T(Ax − b) (7.9) при использовании евклидовой нормы. Градиент из (7.9) относительно x равен ∇x = 2(Ax − b)TA. (7.10) Этот градиент можно использовать напрямую в алгоритме градиентного спуска. Правда, в данном прикладном случае оказывается, что аналити - ческое решение существует, и его можно найти, установив градиент в ноль. Подробнее о решении задач со среднеквадратичной ошибкой мы погово - рим в главе 9. 1 Размер шага также называется скоростью обучения.\n--- Страница 294 ---\n294 Глава 7. Непрерывная оптимизация ПРИМЕЧАНИЕ При применении в ходе решения системы линейных уравне - ний Ax = b градиентный спуск может сходиться медленно. Скорость сходимости градиент ного спуска зависит от числа обусловленности , которое является отношением максимального единичного значения к минимальному (раздел 4.5) у A. В сущности, число обусловленности дает отношение наиболее искривленного варианта направления к наименее искривленному, что наглядно соответствует следующей картине: некорректно поставленные задачи похожи на узкие длинные канавки: графики очень искривлены в одном направлении, но плоские в другом. Вместо того чтобы непосредственно решать Ax = b, можно вместо этого решить P–1(Ax – b) = 0, где P называется предобусловливатель . Цель — спроектировать P–1, так чтобы у P–1A число обусловленности было луч - ше, но в то же время чтобы P–1 было легко вычислять. Подробнее о градиентном спуске, предобусловливании и сходимости можно почитать в Boyd and Van- denberghe (2004, глава 9).  7.1.2. Градиентный спуск с импульсом Как показано на рис. 7.3, сходимость градиентного спуска может происходить очень медленно, если кривизна оптимизационной поверхности такова, что некоторые области на ней плохо шкалируются. Кривизна такова, что шаги градиентного спуска перекидываются между стенками долины, и приближе - ние к оптимуму происходит малыми шагами. Предлагаемая коррекция, по - зволяющая улучшить сходимость, — вложить в градиентный спуск некоторую память. Градиентный спуск с импульсом1 (Rumelhart et al., 1986) — это метод, при ко - тором дополнительный член используется в алгоритме для запоминания того, что произошло на предыдущей итерации. Такая память гасит осцилляции и сглаживает обновления градиента. Продолжая аналогию с шариком, можно сказать, что импульс в данной модели эмулирует тяжелый шар, который слож - но сбить с курса. Идея реализовать обновление градиента с привлечением па - мяти связана с использованием скользящего среднего. Метод, основанный на использовании импульса, запоминает обновление Δxi на каждой итерации i и определяет следующее обновление как линейную комбинацию актуального и предыдущего градиентов xi+1 = xi − γi((∇f)(xi))T + αΔxi; (7.11) Δxi = xi − xi–1 = αΔxi–1 − γi–1((∇f)(xi–1))T, (7.12) 1 Goh (2017) написал понятный пост о градиентном спуске с импульсом.\n--- Страница 295 ---\n7.1. Оптимизация с использованием градиентного спуска 295 где α ∈ [0, 1]. Иногда градиент будет известен нам лишь приблизительно. В та- ких случаях член импульса полезен, так как выравнивает различные зашумлен - ные оценки градиента. Один очень дельный способ получить приблизительный градиент — использовать стохастическое приближение, о котором мы поговорим далее. 7.1.3. Стохастический градиентный спуск На вычисление градиента может потребоваться значительное время. Однако часто удается найти «дешевое» приближение градиента. Приближение гради - ента так или иначе полезно, поскольку оно указывает примерно в том же на - правлении, что и истинный градиент. Стохастический градиентный спуск (stochastic gradient descent, SGD) — это стохастическое приближение метода градиентного спуска для минимизации целевой функции, которая записывается как сумма дифференцируемых функ - ций. Термин «стохастический» в данном случае означает признание того, что точный градиент нам не известен, но мы знаем несколько зашумленное при - ближение к нему. Ограничивая вероятностное распределение приблизительных градиентов, мы все равно можем теоретически гарантировать сходимость SGD. В машинном обучении, имея n = 1, , N точек данных, мы часто рассматриваем целевые функции, являющиеся суммой потерь Ln, вызванных каждым приме - ром n. В математической нотации это выражается как (7.13) где θ — вектор интересующих нас параметров, то есть мы хотим найти θ, мини - мизирующий L. Пример, связанный с регрессией (глава 9), — отрицательное логарифмическое правдоподобие, выражаемое как сумма логарифмических правдоподобий отдельных примеров, так что (7.14) где xn ∈ D — это учебные входные данные, yn — цели обучения, а θ — параметры регрессионной модели. Стандартный градиентный спуск в том виде, как он представлен выше, — это метод «пакетной» оптимизации, то есть проводимой с применением всего учеб - ного множества, путем обновления вектора параметров согласно (7.15)\n--- Страница 296 ---\n296 Глава 7. Непрерывная оптимизация для подходящего параметра ширины шага γi. Для получения результирующего градиента суммы могут потребоваться затратные расчеты градиентов от всех отдельных функций Ln. Когда учебное множество колоссально и/или существу - ют простые формулы, расчет сумм градиентов становится очень затратной операцией. Рассмотрим член в (7.15). Здесь мы можем сократить объем вычислений, взяв сумму меньшего множества Ln. В отличие от пакетного градиентного спуска, использующего Ln для n = 1, , N, мы случайным образом выберем подмножество Ln для градиентного спуска с мини-пакетами. В при- мере, доведенном до крайности, мы случайным образом выбираем всего одно значение Ln для оценки градиента. Ключевая причина, по которой действи - тельно разум но брать подмножество данных, понятна, если учесть, что для сходимости градиентного спуска требуется лишь обеспечить, чтобы этот градиент был неискаженной оценкой истинного градиента. Фактически, член в (7.15) — это эмпирическая оценка ожидаемого значения гра - диента (раздел 6.4.1). Если размер мини-батча сравнительно велик, то оценить градиент можно сравнительно точно, уменьшив вариативность при обновле - нии параметров. Более того, крупные мини-батчи более выгодны благодаря сильной оптимизации матричных операций в векторизованных реализациях стоимости и градиента. Уменьшение вариативности приводит к более ста - бильной сходимости, но каждая операция расчета градиента будет обходить - ся дороже. Напротив, мелкие мини-батчи поддаются сравнительно быстрой оценке. Если следить, чтобы мини-батчи оставались компактными, то зашумленность нашей оценки градиента позволит нам избавиться от некоторых плохих локальных оптимумов, с которыми в противном случае мы могли бы застрять. В машин - ном обучении методы оптимизации используются для обучения путем мини - мизации целевой функции для учебных данных, но стратегическая цель — улучшить обобщающую способность и производительность (глава 8). Поскольку цель машинного обучения не обязательно заключается в точной оценке минимума целевой функции, широко применяются приблизительные градиенты с использованием мини-батчей. Стохастический градиентный спуск очень эффективен при решении крупномасштабных задач машинного обуче - ния (Bottou et al., 2018), например при обучении глубоких нейронных сетей на миллионах изображений (Dean et al., 2012), работе с моделями топиков (Hoffman et al., 2013), обучении с подкреплением (Mnih et al., 2015) либо при обучении крупномасштабных моделей гауссовых процессов (Hensman et al., 2013; Gal et al., 2014).\n--- Страница 297 ---\n7.2. Ограниченная оптимизация и множители Лагранжа 297 7.2. ОГРАНИЧЕННАЯ ОПТИМИЗАЦИЯ И МНОЖИТЕЛИ ЛАГРАНЖА В предыдущем разделе мы решали задачу для минимума функции (7.16) где f : D → . В этом разделе поработаем с дополнительными ограничениями. То есть для вещественнозначной функции gi : D →  при i = 1, , m мы рассмотрим задачу ограниченной оптимизации (рис. 7.4) (7.17) x1x2 –3–3 –2–2 –1–1 00 11 22 33 Рис. 7.4. Иллюстрация выпуклой оптимизации. Задача без ограничений (обозначенная контурными линиями) имеет минимум справа (обозначен кружком). Ограничения, обозначенные рамкой ( −1 ≤ x ≤ 1 и −1 ≤ y ≤ 1), требуют, чтобы целевое решение находилось в пределах рамки, таким образом в результате получаем целевое значение, обозначенное звездочкой\n--- Страница 298 ---\n298 Глава 7. Непрерывная оптимизация Стоит отметить, что функции f и gi могут быть в общем случае невыпуклыми, но в следующем разделе мы рассмотрим выпуклый случай. Один очевидный, но не очень практичный способ преобразовать ограниченную задачу (7.17) в неограниченную — использовать индикаторную функцию (7.18) где 1(z) — это бесконечная ступенчатая функция (7.19) В результате, если ограничение не удовлетворяется, получаем бесконечный штраф и, соответственно, все то же решение. Но бесконечная ступенчатая функ - ция столь же сложно поддается оптимизации. Чтобы справиться с этой трудно - стью, можно прибегнуть к множителям Лагранжа . Идея множителей Лагранжа в том, чтобы заменить ступенчатую функцию линейной. С задачей (7.17) мы ассоциируем лагранжиан , вводя множители Лагранжа λi ≥ 0, соответствующие каждому отдельному ограничению в неравенстве (Boydand Vandenberghe, 2004, глава 4), так что (7.20 a) = f(x) + λTg(x), (7.20 b) где в последней строке мы сцепили все ограничения gi(x) в вектор g(x), а все множители Лагранжа — в вектор λ ∈ m. Теперь познакомимся с идеей лагранжевой двойственности . В принципе, при оптимизации идея двойственности заключается в том, чтобы преобразовать задачу оптимизации, содержащую одно множество переменных x (прямые пере - менные), в другую задачу оптимизации, содержащую другое множество пере - менных λ (двойственные переменные). Мы обсудим два разных подхода к двой- ственности; в этом разделе поговорим о лагранжевой двойственности, а в разделе 7.3.3 речь пойдет о двойственности Лежандра — Фенхеля. Определение 7.1 . Задача в (7.17) (7.21)\n--- Страница 299 ---\n7.2. Ограниченная оптимизация и множители Лагранжа 299 известна под названием « прямая задача », и в ней мы оперируем прямыми пере - менными x. Связанная с ней двойственная задача Лагранжа задается как (7.22), где λ — это двойственные переменные, и ПРИМЕЧАНИЕ Обсуждая определение 7.1, мы используем две концепции, которые также интересны сами по себе (Boyd and Vandenberghe, 2004). Первая — это неравенство минимакса , согласно которому для любой функции с двумя аргументами ϕ(x, y) максимин меньше минимакса, то есть (7.23) Это неравенство можно доказать, рассмотрев неравенство для всех x, y (7.24) Обратите внимание: взяв максимум от y в левой части (7.24), неравенство со - храняется, поскольку оно верно для всех y. Аналогично, можно взять минимум от x в правой части (7.24), получив таким образом (7.23). Вторая концепция — это слабая двойственность , использующая (7.23), чтобы продемонстрировать, что прямые значения всегда больше двойственных или равны им. Это подробнее описано в (7.27).  Напомню, что разница между J(x) в (7.18) и лагранжианом в (7.20b) заключа - ется в том, что мы ослабили индикаторную функцию, сделав ее линейной. Следовательно, при λ ≥ 0 лагранжиан является нижним пределом J(x). Таким образом, максимум относительно λ равен (7.25) Напомним, что исходная проблема сводилась к минимизации J(x), (7.26) Из неравенства минимакса (7.23) следует, что при перемене порядка минималь - ных и максимальных значений результирующее значение уменьшается. (7.27)\n--- Страница 300 ---\n300 Глава 7. Непрерывная оптимизация Это явление также называется слабая двойственность . Обратите внимание: внутренняя составляющая правой части является двойственной целевой функ - цией , определенной далее. По сравнению с исходной задачей оптимизации, в которой есть ограничения, — это задача оптимизации без ограничений для заданного зна - чения λ. Если решить легко, то легко решить и всю задачу. При - чина в том, что внешняя задача (максимизация для λ) дает максимум для множества аффинных функций и, следовательно, это вогнутая функция, пусть и f(·), и gi (·) могут быть невогнутыми. Максимум вогнутой функции эффек - тивно поддается вычислению. Предположив, что f(·) и gi (·) являются дифференцируемыми, решаем задачу двойственности Лагранжа относительно x, устанавливая при этом дифферен - циал в 0 и решая задачу для целевого значения. Два конкретных примера мы обсудим в разделах 7.3.1 и 7.3.2, где f(·) и gi (·) являются выпуклыми. ПРИМЕЧАНИЕ Рассмотрим выражение (7.17) с дополнительными ограни - чениями равенства (7.28) Можно смоделировать ограничения равенства, заменив их двумя ограничени - ями неравенства. То есть для каждого ограничения равенства hj(x) = 0 справед - ливо, что его можно равноценно заменить двумя ограничениями hj(x) ≤ 0 и hj(x) ≥ 0. Оказывается, что получающиеся в результате множители Лагранжа не имеют ограничений. Следовательно, мы ограничиваем множители Лагранжа так, чтобы ограничения из (7.28) оставались неотрицательными, и множители Лагранжа, соответству - ющие ограничениям равенства, мы оставляем неограниченными.  7.3. ВЫПУКЛАЯ ОПТИМИЗАЦИЯ Сосредоточимся на особенно полезном классе задач оптимизации, где мы можем гарантировать глобальную оптимальность. Если f(·) — выпуклая функ - ция, и ограничения g(·) и h(·) задают выпуклые множества, задача называется задачей выпуклой оптимизации . При такой постановке задачи существует сильная двойственность : оптимальное решение двойственной задачи совпада -\n--- Страница 301 ---\n7.3. Выпуклая оптимизация 301 ет с оптимальным решением исходной задачи. Различие между выпуклыми функциями и выпуклыми множествами часто обходится стороной в литера - туре по машинному обучению, однако обычно можно понять значение этого слова из контекста. Определение 7.2. Множество  называется выпуклым множеством , если для любых x, y ∈  и любого скаляра θ из интервала [0, 1] выполняется θx + (1 − θ)y ∈ . (7.29) Выпуклые множества — это такие множества, что отрезок, соединяющий любые две точки множества, лежит в нем целиком. Выпуклое и невыпуклое множество показаны на рис.7.5 и 7.6 соответственно. Рис. 7.5. Пример выпуклого множестваРис. 7.6. Пример невыпуклого множества Выпуклые функции — это такие функции, что отрезок, соединяющий любые две точки на их графике, лежит над графиком. На рис. 7.7 показана выпуклая функция. 0 1 –1 2 –2 3 –3 x010203040yy = 3x2 – 5x + 2 Рис.7.7. Пример выпуклой функции\n--- Страница 302 ---\n302 Глава 7. Непрерывная оптимизация Определение 7.3. Пусть функция f : D →  определена на выпуклом множестве. Функция f выпукла , если для любых x, y из области определения f и любого скаляра θ из интервала [0, 1] f (θx + (1 − θ)y) ≤ θf(x) + (1 − θ)f(y). (7.30) ПРИМЕЧАНИЕ Вогнутая функция — противоположное к выпуклой понятие.  Ограничения, заданные функциями g(·) и h(·) в (7.28), можно представить как некоторые множества. Еще одно соотношение между выпуклыми функциями и выпуклыми множествами мы получим из рассмотрения множества, «запол - няющего» график выпуклой функции. Выпуклая функция похожа на чашку, и мы представляем, как наполняем ее водой. Получившееся множество, назы - ваемое надграфиком выпуклой функции, является выпуклым. Если функция f : n →  дифференцируема, выпуклость можно описать в тер- минах ее градиента ∇xf(x) (раздел 5.2). Функция f(x) выпукла, если и только если для любых двух точек x, y выполняется неравенство f (y) ≥ f(x) + ∇xf(x)T(y − x). (7.31) Если, кроме того, мы знаем, что f(x) дважды дифференцируема, то есть для всех точек области определения существует матрица Гессе (5.147), то f(x) будет вы - пуклой в том и только том случае, когда положительно полуопределена (Boyd and Vandenberghe, 2004). Пример 7.3 Отрицательная энтропия f(x) = x log2x выпукла для x > 0. График этой функции изображен на рис. 7.8, и мы видим, что она выпукла. В качестве иллюстрации к предыдущим определениям выпуклости, проведем вы - числения для точек x = 2 и x = 4. Заметим, что для доказательства выпук- лости f(x) мы должны были бы проверить это свойство для всех x ∈ . Вспомним определение 7.3. Рассмотрим точку, лежащую ровно посере - дине между нашими двумя точками (то есть θ = 0,5); тогда в левой части стоит f(0,5 · 2 + 0,5 · 4) = 3 log23 ≈ 4,75. Правая часть равна 0,5(2log22) + + 0,5(4log24) = 1 + 4 = 5. Таким образом, условие из определения выпол - нено.\n--- Страница 303 ---\n7.3. Выпуклая оптимизация 303 Так как f(x) дифференцируема, можно вместо этого использовать усло - вие (7.31). Вычислив производную f(x), получаем (7.32) Взяв для проверки те же самые точки x = 2 и x = 4, видим в левой ча - сти (7.31) f(4) = 8. Правая часть равна (7.33 a) (7.33 b) 0 1 2 3 xf(x) 4 50510x log2x /asterisk.g007D/equal.g00C8“/equal.g00C8/two.g0088/yright/.notdef.g00E3/.notdef.g0109…/equal.g00C8/space.g00AB /quotedbl.g006D /two.g0088/percent.g00BA/.notdef.g0107/asterisk.g007D/yright x = 2 Рис. 7.8. Функция отрицательной энтропии (выпуклая) и касательная к ней в точке x = 2 Можно проверить выпуклость функции или множества по определению. На практике мы часто используем для этого то, что некоторые операции сохра - няют выпуклость. По сути, это идея замкнутости, введенная в главе 2 для век - торных пространств (хотя детали и сильно разнятся). Пример 7.4 Взвешенная сумма выпуклых функций с неотрицательными коэффици - ентами выпукла. Заметим, что для выпуклой функции f и неотрицатель - ного скаляра α функция αf будет выпуклой. Это легко увидеть, домножив\n--- Страница 304 ---\n304 Глава 7. Непрерывная оптимизация на α обе части неравенства в определении 7.3 и учитывая, что умножение на неотрицательное число не меняет знак неравенства. Если функции f1 и f2 выпуклы, то по определению f1(θx + (1 − θ)y) ≤ θf1(x) + (1 − θ)f1(y); (7.34) f2(θx + (1 − θ)y) ≤ θf2(x) + (1 − θ)f2(y). (7.35) Складывая левые и правые части соответственно, получим f1(θx + (1 − θ)y) + f2(θx + (1 − θ)y) ≤ ≤ θf1(x) + (1 − θ)f1(y) + θf2(x) + (1 − θ)f2(y), (7.36) где правую часть можно переписать как θ (f1(x) + f2(x)) + (1 − θ)(f1(y) + f2(y)), (7.37) и мы доказали, что сумма выпуклых функций выпукла. Объединяя два доказанных нами факта, видим, что αf1(x) + βf2(x) вы- пукла при α, β ≥ 0. Аналогичным образом можно обобщить это свойство замкнутости на взвешенные суммы более чем двух выпуклых функций с неотрицательными коэффициентами. ПРИМЕЧАНИЕ Неравенство (7.30) иногда называют неравенством Йенсена . На самом деле, целый класс неравенств, касающихся взвешенных сумм выпуклых функций с неотрицательными коэффициентами, называют неравенствами Йенсена.  В заключение сформулируем определение: задача оптимизации с ограничени - ями называется задачей выпуклой оптимизации , если (7.38) где f(x) и все gi(x) — выпуклые функции и все hj(x) = 0 — выпуклые множества. В дальнейшем мы опишем два глубоко изученных и широко применимых клас - са задач выпуклой оптимизации.\n--- Страница 305 ---\n7.3. Выпуклая оптимизация 305 7.3.1. Линейное программирование Рассмотрим частный случай задачи оптимизации, в котором все функции ли - нейны, то есть (7.39) где A ∈ m×d и b ∈ m. Такие задачи известны как задачи линейного программи ­ рования1. В нашей задаче d неизвестных и m линейных ограничений. Лагранжи - ан задается формулой L(x, λ) = cTx + λT(Ax – b), (7.40) где λ ∈ m — вектор неотрицательных множителей Лагранжа. Сгруппировав слагаемые, содержащие x, получим L (x, λ) = (c +ATλ)Tx – λTb. (7.41) Возьмем производную L(x, λ) по x и приравняем ее к нулю: c +ATλ = 0. (7.42) Таким образом, двойственный лагранжиан равен D(λ) = −λTb. Вспомним, что мы хотим максимизировать D(λ). Кроме ограничения, состоящего в равенстве L(x, λ) нулю, мы также имеем ограничение λ ≥ 0, и в итоге приходим к следую - щей двойственной задаче оптимизации2: (7.43) Это снова задача линейного программирования, но уже от m переменных. Мы можем выбирать, решать ли исходную задачу (7.39) или двойственную (7.43) в зависимости от того, m или d больше. Напомним, что d — число переменных, а m — число ограничений в исходной задаче линейного программирования. 1 Линейное программирование — один из самых часто применяемых на практике под - ходов. 2 Принято, что исходной задачей является задача минимизации, а двойственной — за- дача максимизации.\n--- Страница 306 ---\n306 Глава 7. Непрерывная оптимизация Пример 7.5 (задача линейного программирования) Рассмотрим задачу с двумя переменными: (7.44) Она изображена на рис. 7.9. Целевая функция линейна, так что линии ее равных значений — прямые. Стандартная запись ограничений содержит - ся в легенде. Оптимальное значение должно лежать в допустимой области (закрашена) и отмечено звездочкой. x10 02 24 46 68 810 10 12 14 16x22x2 ≤ 33 – 2x1 4x2 ≥ 2x1 – 8 x2 ≥ 1x2 ≤ 2x1 – 5 x2 ≤ 8 Рис. 7.9. Задача линейного программирования. Оптимальное значение при данных ограничениях показано звездочкой\n--- Страница 307 ---\n7.3. Выпуклая оптимизация 307 7.3.2. Квадратичное программирование Рассмотрим случай выпуклой квадратичной целевой функции и афффинных ограничений, то есть (7.45) где A ∈ m×d, b ∈ m и c ∈ d. Симметричная квадратная матрица Q ∈ d×d по- ложительно определена, так что целевая функция выпукла. Такие задачи из - вестны как задачи квадратичной оптимизации. Заметим, что в нашей задаче d переменных и m линейных ограничений. Пример 7.6 (квадратичное программирование) Рассмотрим задачу квадратичного программирования (7.46) (7.47) с двумя неизвестными. Задача показана также на рис. 7.4. Целевая функция квадратична, с симметричной положительно определенной матрицей Q. Линии равных значений имеют форму эллипсов. Оптимальное значение должно лежать в допустимой области (закрашена) и отмечено звездочкой. Лагранжиан задается формулой , (7.48 a) (7.48 b) (мы снова перегруппировали слагаемые). Возьмем производную L(x, λ) по x и приравняем ее нулю: Qx + (c + ATλ) = 0. (7.49)\n--- Страница 308 ---\n308 Глава 7. Непрерывная оптимизация Если Q обратима, получим x = −Q–1(c + ATλ). (7.50) Подставив (7.50) в исходный лагранжиан L(x, λ), получим двойственный ла - гранжиан (7.51) В итоге, двойственная задача оптимизации выглядит как (7.52) С приложениями квадратичной оптимизации к машинному обучению мы по - знакомимся в главе 12. 7.3.3. Преобразование Лежандра — Фенхеля и выпуклое сопряжение Вернемся к идее двойственности из раздела 7.2, пока не рассматривая ограни - чения. Важным фактом о выпуклых множествах является то, что их можно описать с помощью опорных гиперплоскостей. Гиперплоскость называется опорной для выпуклого множества, если она пересекается с этим множеством, и все множество лежит по одну сторону от нее. Напомним, что мы можем «за - полнить» график выпуклой функции, чтобы получить надграфик, и она будет выпуклым множеством. Таким образом, мы можем описать и выпуклые функции через их опорные плоскости. Далее заметим, что опорная гиперплоскость пере - секается с графиком только в одной точке, и на самом деле является касательной к нему. Вспомним теперь, что тангенс угла наклона касательной к графику функции f(x) в точке x0 равен значению градиента в этой точке. Таким образом, так как выпуклые множества описываются с помощью опорных плоскостей, выпуклые функции описываются своей функцией градиента. Преобразование Лежандра1 формализует эту идею. Начнем с наиболее общего определения, к сожалению, не слишком понятного интуитивно, а затем рассмотрим частные случаи, чтобы связать определение с выработанным ранее интуитивным пониманием. Преобразование Лежандра — Фенхеля (слово «преобразование» здесь имеет тот же смысл, что и, например, 1 Студенты-физики часто знакомятся с преобразованием Лежандра как с преобразова - нием в классической механике, связывающим лагранжиан и гамильтониан.\n--- Страница 309 ---\n7.3. Выпуклая оптимизация 309 в словосочетании «преобразование Фурье») сопоставляет выпуклой дифферен - цируемой функции f(x) функцию, зависящую от касательных s(x) = ∇xf(x). Стоит подчеркнуть, что мы преобразуем именно функцию f( · ), а не перемен - ную x или значение в точке x. Преобразование Лежандра — Фенхеля также из - вестно как выпуклое сопряжение (причины чему мы скоро увидим) и тесно связано с двойственностью (Hiriart-Urruty and Lemar échal, 2001, глава 5). Определение 7.4. Выпукло сопряженная функция к функции f : D →  — это функция f*, заданная формулой (7.53) Заметим, что такое определение выпуклого сопряжения не требует от функции f быть ни выпуклой, ни дифференцируемой. В определении 7.4 мы использовали произвольное скалярное произведение (раздел 3.2), но далее в этом разделе мы для простоты рассматриваем стандартное скалярное произведение конечномер - ных векторов (< s, x> = sTx). Чтобы понять геометрический смысл определения 7.4, рассмотрим «хорошую» одномерную выпуклую дифференцируемую функцию1, например f(x) = x2. За- метим, что так как мы рассматриваем одномерную задачу, гиперплоскости пред - ставляют собой прямые. Рассмотрим прямую y = sx + c. Мы знаем, что выпуклые функции можно описать с помощью опорных плоскостей, так что давайте по - пробуем описать нашу f(x) с помощью опорных прямых. Зафиксируем градиент прямой s ∈  и для каждой точки ( x0, f(x0)) на графике f найдем минимальное значение c, такое что прямая проходит через ( x0, f(x0)). Заметим, что это мини - мальное значение c соответствует ситуации, когда прямая с фиксированным s является касательной к графику f(x) = x2. Прямая, проходящая через точку ( x0, f(x0)) с градиентом s, задается формулой y − f(x0) = s(x − x0). (7.54) y-координаты точки на этой прямой задаются формулой −sс + f(x0). Тогда ми - нимальное значение c, при котором y = sx + c пересекает график f, равно (7.55) Выпукло сопряженная к данной функции по определению — выражение, противоположное к данному. Эти рассуждения не использовали выпуклость и дифференцируемость нашей функции (а также ее зависимость от только 1 Эти рассуждения проще понять, если следить за ними по рисунку.\n--- Страница 310 ---\n310 Глава 7. Непрерывная оптимизация одной переменной), так что будут верны и для f : D → , невыпуклых и не- дифференцируемых 1. ПРИМЕЧАНИЕ Выпуклые дифференцируемые функции, такие как наш при - мер f(x) = x2, — это «хороший» частный случай, где нет необходимости брать супремум и существует взаимно однозначное соответствие между функцией и ее преобразованием Лежандра. Выведем это из определений и простейших свойств. Для выпуклой дифференцируемой функции запишем уравнение каса - тельной в точке f(x) = x2: (7.56) Напомним, что мы хотели описать выпуклую функцию f(x) через ее градиент ∇xf(x), и что s = ∇xf(x0). Выразим −c: −c = sx0 − f(x0). (7.57) Заметим, что −c зависит от x0 и, следовательно, от s, поэтому мы можем рассма - тривать ее как функцию от s, которую мы обозначим f *(s) := sx0 − f(x0). (7.58) Сравнивая (7.58) с определением 7.4, видим, что (7.58) — частный случай опре - деления (где нам не нужен супремум).  Сопряженная функция обладает удобными нам свойствами, например для вы - пуклых функций повторное применение преобразования Лежандра возвраща - ет нас к исходной функции. Аналогично тому, что наклон f(x) равен s, наклон f *(s) равен x. Два следующих примера показывают частые применения выпук - ло сопряженных функций в машинном обучении. Пример 7.7 (выпуклое сопряжение) Покажем применение выпуклого сопряжения на примере квадратичной функции , (7.59) заданной положительно определенной матрицей K ∈ n × n. Обозначим переменную в исходной задаче как y ∈ n, а в двойственной — как α ∈ n. 1 Классическое преобразование Лежандра определено для выпуклых дифференцируе - мых функций на D.\n--- Страница 311 ---\n7.3. Выпуклая оптимизация 311 Применив определение 7.4, получим функцию (7.60) Так как эта функция дифференцируема, мы можем найти максимум, взяв производную по y и приравняв ее нулю. . (7.61) Таким образом, когда градиент равен нулю, y = (1/λ)Kα. Подставим то выражение в (7.60) и получим (7.62) Пример 7.8 В машинном обучении мы часто встречаемся с суммами функций, напри - мер целевая функция на обучающей выборке включает сумму потерь на каждом ее элементе. Сейчас мы найдем выпукло сопряженную функцию к сумме потерь , где . Этот пример также покажет при - менение сопряжения к многомерному случаю. Пусть (t) . Тогда (7.63 a) по определению скалярного произведения (7.63 b) (7.63 с) по определению сопряжения. (7.63 d) Вспомним, что в разделе 7.2 мы получили двойственную задачу оптимизации с помощью множителей Лагранжа. Далее, для задач выпуклой оптимизации имеется сильная двойственность, то есть решения исходной и двойственной задач равны. Описанное сейчас преобразование Лежандра — Фенхеля также можно использовать для получения двойственной задачи. Более того, если\n--- Страница 312 ---\n312 Глава 7. Непрерывная оптимизация функция выпукла и дифференцируема, супремум будет единственным. Чтобы подробнее изучить связь двух подходов, рассмотрим задачу выпуклой оптими - зации с линейными ограничениями. Пример 7.9 Пусть f(y) и g(x) — выпуклые функции, A — вещественная матрица под - ходящей размерности, такая что Ax = y. Тогда (7.64) Вводим множитель Лагранжа u для ограничений Ax = y, (7.65 a) (7.65 b) где на последнем шаге мы можем поменять местами max и min, благодаря тому что f(y) и g(x) выпуклы. Перегруппировкой слагаемых получаем (7.66 a) (7.66 b) . (7.66 c) Вспомнив определение 7.4 выпуклого сопряжения и симметричность скалярного произведения1, получим (7.67 a) (7.67 b) Таким образом, мы показали, что (7.68) Сопряжение по Лежандру — Фенхелю оказывается весьма полезным для задач машинного обучения, которые могут быть выражены в виде задач выпуклой 1 В общем случае для скалярного произведения AT заменяется на A*.\n--- Страница 313 ---\n7.4. Для дальнейшего чтения 313 оптимизации. В частности, для выпуклых функций потерь, применяющихся по отдельности к каждому объекту, сопряженная функция потерь позволяет перей- ти к двойственной задаче. 7.4. ДЛЯ ДАЛЬНЕЙШЕГО ЧТЕНИЯ Непрерывная оптимизация является активно развивающейся областью, и мы не претендуем на полное изложение недавних достижений. У градиентного спуска есть два основных недостатка, каждому из которых по - священо некоторое количество литературы. Первая проблема состоит в том, что градиентный спуск — алгоритм первого порядка и не использует информацию о кривизне поверхности. В «оврагах» направление градиента перпендикулярно нужному. Идею моментов можно обобщить и получить ускоренные методы (Nesterov, 2018). Метод сопряженных градиентов избегает проблем обычного градиентного спуска, учитывая предшествующие направления (Shewchuk, 1994). Методы второго порядка, такие как метод Ньютона, используют матрицу Гессе для получения информации о кривизне. Рассмотрение кривизны поверхности приводит к разнообразным методам выбора шага и идеям, аналогичным момен - там (Goh, 2017; Bottou et al., 2018). Квазиньютоновы методы, такие как L-BFGS, пытаются более эффективно с вычислительной точки зрения приближать мат- рицу Гессе (Nocedal and Wright, 2006). Недавно возник интерес к другому вы - бору метрик для вычисления направления градиента, приведя к таким подходам, как зеркальный спуск (Beck and Teboulle, 2003) и естественный градиент (Toussaint, 2012). Второй проблемой является работа с недифференцируемыми функциями. Градиентные методы не всегда применимы, когда у функции имеются особен - ности. В этих случаях можно использовать субградиентные методы (Shor, 1985). Чтобы узнать больше об алгоритмах оптимизации недифференцируемых функций, советуем читать книгу Bertsekas (1999). Имеется огромный массив литературы о разных подходах к численному решению задач непрерывной оптимизации, в том числе об алгоритмах для задач с ограничениями. Хорошей отправной точкой могут служить книги Luenberger (1969) и Bonnans et al. (2006). Недавно обзор по непрерывной оптимизации был представлен в Bubeck (2015). В современных приложениях машинного обучения размер входных данных часто не позволяет провести градиентный спуск по батчам, поэтому сейчас стандартным выбором для больших объемов данных является стохастический градиентный спуск. К недавним обзорам литературы относятся Hazan (2015) и Bottou et al. (2018).\n--- Страница 314 ---\n314 Глава 7. Непрерывная оптимизация По теме двойственности и выпуклой оптимизации советуем книгу Boyd and Vandenberghe (2004), а также прилагающиеся к ней лекции и слайды в интер - нете. Более математический подход можно найти в Bertsekas (2009), а недавно вышла книга Nesterov (2018), автор которой — один из самых выдающихся исследователей в области оптимизации. Выпуклая оптимизация основана на выпуклом анализе, и заинтересованному в математических основах читателю советуем обратиться к Rockafellar (1970), Hiriart-Urruty and Lemar échal (2001), и Borwein and Lewis (2006). В данных книгах освещается и тема преобразований Лежандра — Фенхеля, но более подходящее для начинающего изложение пред - ставлено в Zia et al. (2009). Роль преобразований Лежандра — Фенхеля в ана- лизе алгоритмов выпуклой оптимизации рассмотрена в Polyak (2016)1. УПРАЖНЕНИЯ 7.1. Рассмотрим одномерную функцию f(x) = x3 + 6x2 − 3x − 5. Найдите ее стационарные точки и укажите, являются ли они максимальной, минимальной или седловой точками. 7.2. Рассмотрим уравнение обновления для стохастического градиентного спу - ска (7.15). Запишите обновление, если мы используем размер мини-пакета, равный единице. 7.3. Верны ли следующие утверждения или нет: а. Пересечение любых двух выпуклых множеств выпукло. b. Объединение любых двух выпуклых множеств выпукло. c. Разница одного выпуклого множества A и второго выпуклого множества B есть выпуклое множество. 7.4. Верны ли следующие утверждения или нет: a. Сумма любых двух выпуклых функций выпукла. b. Разность любых двух выпуклых функций выпукла. c. Произведение любых двух выпуклых функций выпукло. d. Максимум любых двух выпуклых функций выпуклый. 1 Блог Хьюго Гонсалвеса (Hugo Gon çalves) — также полезный ресурс для ознакомления с преобразованиями Лежандра — Фенхеля: https://tinyurl.com/ydaal7hj .\n--- Страница 315 ---\nУпражнения 315 7.5. Выразите следующую задачу оптимизации в виде стандартной линейной программы в матричной форме с учетом ограничений ξ ≥ 0, x0 ≤ 0 и x1 ≤ 3. 7.6. Рассмотрим линейную программу, показанную на рис. 7.9. . Выведите двойственную линейную программу, используя двойственность Ла - гранжа. 7.7. Рассмотрим квадратичную программу, показанную на рис. 7.4, Выведите двойственную квадратичную программу, используя двойственность Лагранжа. 7.8. Рассмотрим следующую задачу выпуклой оптимизации Выведите двойственный лагранжиан, введя множитель Лагранжа λ. 7.9. Рассмотрим отрицательную энтропию x ∈ D,\n--- Страница 316 ---\n316 Глава 7. Непрерывная оптимизация Получите выпуклую сопряженную функцию f*(s), предполагая стандартное скалярное произведение. Подсказка : градиент соответствующей функции установите на ноль. 7.10. Рассмотрим функцию где A строго положительно определена, что означает, что она обратима. Вывести выпуклое сопряжение f(x). Подсказка : градиент соответствующей функции установите на ноль. 7.11. Кусочно-линейная функция потерь (которая является потерей, использу - емой методом опорных векторов) определяется как L(α) = max{0, 1 − α}. Если мы заинтересованы в применении градиентных методов, таких как L-BFGS, и не хотим прибегать к субградиентным методам, нам необходимо сгладить из - лом в кусочно-линейной функции потерь. Вычислите выпуклое сопряжение кусочно-линейной функции потерь L*(β), где β — двойственная переменная. Добавьте два проксимальных члена и вычислите сопряжение полученной функ - ции где γ — заданный гиперпараметр.\n--- Страница 317 ---\nЧАСТЬ II Главные задачи машинного обучения\n--- Страница 318 ---\n8 О сочетании модели и данных В первой части книги мы познакомились с математикой, лежащей в основе многих методов машинного обучения. Надеемся, что читатель получил базовые представления о том математическом аппарате, которым мы будем пользовать - ся при описании и обсуждении МО. Во второй части книги мы познакомимся с четырьмя столпами машинного обучения: zрегрессия (глава 9); zснижение размерности (глава 10); zоценка плотности (глава 11); zzклассификация (глава 12). Основная цель этой части книги — проиллюстрировать, как математические концепции, показанные в первой части, могут использоваться при проектиро - вании алгоритмов МО, которые, в свою очередь, применяются для решения задач в пределах заданной предметной области. Мы не собираемся рассказы - вать о продвинутых концепциях МО, а, напротив, предлагаем практические методы, которые помогут читателю воспользоваться знаниями, приобретен - ными в первой части книги. Также эти главы проложат путь к более обширной литературе по машинному обучению для тех читателей, что уже знакомы с математикой. 8.1. ДАННЫЕ, МОДЕЛИ И ОБУЧЕНИЕ Здесь стоит приостановиться и задуматься о том, задачи какого рода должен решать алгоритм машинного обучения. Как говорилось в главе 1, в системе МО есть три основные составляющие: данные, модели и обучение. Основной вопрос МО: «Что мы понимаем под хорошими моделями?» В данном значении слово\n--- Страница 319 ---\n8.1. Данные, модели и обучение 319 «модель » имеет много тонкостей, и мы неоднократно обратимся к ним в рамках этой главы. Также не вполне очевидно, как объективно оценить понятие «хоро - ший». Один из руководящих принципов МО — в том, что хорошие модели должны успешно работать на еще не известных данных. Для этого требуется определить ряд метрик производительности, например точность (accuracy) или отдаленность от истинного значения, а также найти способы достижения хоро - ших результатов в соответствии с этими метриками. В этой главе рассмотрен ряд необходимых составляющих математического и статистического аппарата, которые обычно используются при рассуждении о моделях машинного обучения. Обсудив их, мы кратко очертим современные наилучшие практики обучения модели, с опорой на которые предиктор должен показывать хорошие результа - ты на еще не известных данных. Как упоминалось в главе 1, выражение «алгоритм машинного обучения» упо - требляется в контексте двух основных операций: обучение и прогнозирование. Мы опишем их в данной главе, а также расскажем, как выбирать модель. В раз- деле 8.2 мы познакомимся с фреймворком для минимизации эмпирического риска, в разделе 8.3 объясним принцип максимального правдоподобия, а в раз- деле 8.4 поговорим о вероятностных моделях. В разделе 8.5 мы кратко затронем графический язык для описания вероятностных моделей и, наконец, в разделе 8.6 поговорим о выборе модели. В оставшейся части этого раздела раскроем основ - ные понятия МО: данные, модели и обучение. 8.1.1. Данные как векторы Предполагается, что данные могут считываться компьютером и адекватно представляться в числовом формате. Также мы исходим из того, что данные хранятся в табличном виде (рис. 8.1), где каждая строка представляет конкрет - ный экземпляр или пример, а каждый столбец — конкретный признак1. В по- следние годы МО применяется на данных самого разного типа, и не всегда очевидно, как представить некоторые из них в табличном числовом формате. Среди таких сложных примеров — геномные последовательности, текстовое и графическое содержимое веб-страницы, графики из социальных сетей. Мы не затрагиваем важные и нетривиальные вопросы выявления хороших при - знаков. Многие из этих задач зависят от опыта в предметной области и требу - ют тщательного проектирования; в последние годы они обобщаются под термином «наука о данных» (data science) (Stray, 2016; Adhikari and DeNero, 2018). 1 Предполагается, что данные должны быть в аккуратном формате (Wickham, 2014; Codd, 1990).\n--- Страница 320 ---\n320",
      "debug": {
        "start_page": 288,
        "end_page": 320
      }
    },
    {
      "name": "Глава 8. О сочетании модели и данных 318",
      "content": "--- Страница 320 --- (продолжение)\nГлава 8. О сочетании модели и данных 0 10 20 30 40 50 60 70 80 x0255075100125150y? Рис. 8.1. Простой пример данных для линейной регрессии. Обучающие данные в парах ( xn, yn) из двух крайних правых столбцов в табл. 8.2. Нас интересует зарплата человека в возрасте 60 лет ( x = 60), показанная в виде вертикальной пунктирной линии красного цвета. Эти данные не входят в обучающий датасет Хотя данные и представлены у нас в табличном формате, все равно остаются варианты, какое именно числовое представление для них выбрать. Например, в табл. 8.1 столбец «пол» (категориальная переменная) может быть преобра - зован в числа 0 (для «М») и 1 (для «Ж»). Другой вариант — выразить содер - жимое этого столбца при помощи чисел +1 и –1 соответственно (как показано в табл. 8.2). Кроме того, зачастую важно использовать при создании представ - ления данные, относящиеся к предметной области: например, знать, что ака - демические степени присваиваются в порядке «бакалавр — магистр — PhD» или что предоставленный почтовый код — это не просто последовательность символов, а обозначение одного из районов Лондона. В табл. 8.2 находятся данные из табл. 8.1, преобразованные в числовой формат. Каждый почтовый код представлен в виде двух числовых значений: широты и долготы. Любые числовые данные, которые потенциально поддаются непосредственному счи - тыванию алгоритмом МО, нужно внимательно рассмотреть на предмет единиц измерения, масштаба и ограничений. Без дополнительной информации при - дется масштабировать все столбцы датасета таким образом, чтобы их эмпири - ческое среднее было равно 0, а эмпирическая дисперсия — 1. В контексте этой книги мы предполагаем, что эксперт уже правильно преобразовал данные, то есть что каждый ввод xn является D-мерным вектором вещественных чисел, которые называются признаками , атрибутами или ковариатами . Будем счи - тать, что датасет представлен в такой форме, как в табл. 8.2. Обратите внима - ние: в новом числовом представлении мы отбросили столбец «Имя» из табл. 8.1. Это желательно по двум причинам: (1) мы не предполагаем, что идентифика - тор (Имя) будет информативен в контексте задачи машинного обучения,\nГлава 8. О сочетании модели и данных 0 10 20 30 40 50 60 70 80 x0255075100125150y? Рис. 8.1. Простой пример данных для линейной регрессии. Обучающие данные в парах ( xn, yn) из двух крайних правых столбцов в табл. 8.2. Нас интересует зарплата человека в возрасте 60 лет ( x = 60), показанная в виде вертикальной пунктирной линии красного цвета. Эти данные не входят в обучающий датасет Хотя данные и представлены у нас в табличном формате, все равно остаются варианты, какое именно числовое представление для них выбрать. Например, в табл. 8.1 столбец «пол» (категориальная переменная) может быть преобра - зован в числа 0 (для «М») и 1 (для «Ж»). Другой вариант — выразить содер - жимое этого столбца при помощи чисел +1 и –1 соответственно (как показано в табл. 8.2). Кроме того, зачастую важно использовать при создании представ - ления данные, относящиеся к предметной области: например, знать, что ака - демические степени присваиваются в порядке «бакалавр — магистр — PhD» или что предоставленный почтовый код — это не просто последовательность символов, а обозначение одного из районов Лондона. В табл. 8.2 находятся данные из табл. 8.1, преобразованные в числовой формат. Каждый почтовый код представлен в виде двух числовых значений: широты и долготы. Любые числовые данные, которые потенциально поддаются непосредственному счи - тыванию алгоритмом МО, нужно внимательно рассмотреть на предмет единиц измерения, масштаба и ограничений. Без дополнительной информации при - дется масштабировать все столбцы датасета таким образом, чтобы их эмпири - ческое среднее было равно 0, а эмпирическая дисперсия — 1. В контексте этой книги мы предполагаем, что эксперт уже правильно преобразовал данные, то есть что каждый ввод xn является D-мерным вектором вещественных чисел, которые называются признаками , атрибутами или ковариатами . Будем счи - тать, что датасет представлен в такой форме, как в табл. 8.2. Обратите внима - ние: в новом числовом представлении мы отбросили столбец «Имя» из табл. 8.1. Это желательно по двум причинам: (1) мы не предполагаем, что идентифика - тор (Имя) будет информативен в контексте задачи машинного обучения,\n--- Страница 321 ---\n8.1. Данные, модели и обучение 321 и (2), возможно, мы захотим анонимизировать данные, чтобы обеспечить приватность информации о сотрудниках. Таблица 8.1. Пример данных из фиктивной базы данных кадрового отдела, представленных не в числовом формате Имя Пол Степень Почтовый код Возраст Ежегодная зарплата Адитья М Магистр W21BG 36 89 563 Боб М PhD EC1A1BA 47 123 543 Хлои Ж Бакалавр экономикиSW1A1BH 26 23 989 Дайсукэ М Бакалавр SE207AT 68 138 769 Элизабет Ж MБA SE10AA 33 113 888 Таблица 8.2. Пример данных из фиктивной базы данных кадрового отдела (см. табл. 8.1), преобразованных в числовой формат ID пола Степень Широта (в десятичных градусах)Долгота (в десятичных градусах)Возраст Ежегодная зар - плата (в тысячах) –1 2 51,5073 0.1290 36 89,563 –1 3 51,5074 0.1275 47 123,543 +1 1 51,5071 0.1278 26 23,989 –1 1 51,5075 0.1281 68 138,769 +1 2 51,5074 0.1278 33 113,888 В этой части книги мы обозначим через N количество примеров во множестве данных, а индексировать примеры будем строчной буквой n = 1, , N. Предпо - лагается, что дан набор числовых данных в виде массива векторов (табл. 8.2). Каждая строка соответствует конкретному отдельному xn, часто именуемому в машинном обучении примером или точкой данных . Нижний индекс n означа - ет, что речь идет об n-м примере из общего количества N примеров в наборе данных. В каждом столбце представлен конкретный признак интересующего нас примера, и мы индексируем признаки как d = 1, , D. Напоминаем, что дан - ные представляются в векторном виде, и это означает, что каждый пример (каждая точка данных) является D-мерным вектором. Ориентация таблицы традиционная, принятая в сообществе специалистов по базам данных, но в не-\n--- Страница 322 ---\n322 Глава 8. О сочетании модели и данных которых алгоритмах МО (например, из главы 10) удобнее представлять при - меры в виде векторов-столбцов. Рассмотрим задачу прогнозирования годовой зарплаты в зависимости от воз - раста сотрудника, основываясь на данных из табл. 8.2. Это пример задачи на обучение с учителем, где у нас есть метка yn (зарплата), связанная с каждым из примеров xn (возраст). Метку, такую как yn, могут называть по-разному, в част- ности целевой или зависимой переменной, а также аннотацией. Датасет запи - сывается как набор пар «пример — метка» {( x1, y1), , ( xn, yn), , ( xN, yN)}. Табли - ца примеров { x1, …, xN} часто конкатенируется и записывается как X ∈ N×D. На рис. 8.1 показан набор данных, составленный из двух крайних правых столбцов табл. 8.2, где x = возраст и y = зарплата. Мы воспользуемся концепциями, представленными в первой части книги, для формализации задач МО таким образом, как это сделано в предыдущем абзаце. Представляя данные в виде векторов xn, можно пользоваться концепциями линейной алгебры (описанными в главе 2). Во многих алгоритмах МО требу - ется дополнительно иметь возможность сравнивать два вектора. Как будет по - казано в главах 9 и 12, при вычислении сходства или расстояния между двумя примерами можно формализовать интуитивную догадку о том, что у примеров со схожими признаками должны быть схожие метки. Для сравнения двух век - торов требуется построить геометрию (подробнее в главе 3), что позволит нам оптимизировать результирующую задачу обучения, воспользовавшись приема - ми из главы 7. Поскольку у нас есть векторные представления данных, их можно обработать так, чтобы найти потенциально более качественные представления. Мы обсудим два способа сделать это: поиск для исходного вектора признаков приближений более низкой размерности, а также использование нелинейных комбинаций исходного вектора признаков более высокой размерности, чем оригинал. В гла- ве 10 рассмотрим пример приближения с малой размерностью для исходного пространства данных путем нахождения главных компонент. Поиск главных компонент тесно связаны с концепциями собственного значения и сингулярно - го разложения, с которыми мы познакомились в главе 4. Для представления в высокой размерности мы рассмотрим явную признаковую карту ϕ(·), позво - ляющую представлять значения xn при помощи варианта с более высокой раз - мерностью ϕ(xn). Основная причина, по которой целесообразно прибегать к представлениям с более высокой размерностью, в том, что можно составлять новые признаки как нелинейные комбинации исходных признаков, что, в свою очередь, упрощает задачу обучения. Мы поговорим о признаковой карте в раз- деле 9.2, и в разделе 12.4 покажем, как эта карта признаков ведет к ядру. В по- следние годы методы глубокого обучения (Goodfellow et al., 2016) показали\n--- Страница 323 ---\n8.1. Данные, модели и обучение 323 многообещающие результаты в плане использования данных для самостоятель - ного извлечения новых полезных признаков и позволили добиться больших успехов в таких областях, как компьютерное зрение, распознавание речи и об- работка естественного языка. В этой части книги мы не будем затрагивать нейронные сети, но напоминаем читателю, что в разделе 5.6 дается математиче - ское описание обратного распространения ошибки — ключевой концепции в обучении нейронных сетей. 8.1.2. Модели как функции Имея данные в подходящем векторном представлении, мы можем пустить их в дело, сформировав с их помощью предиктивную функцию (так называемый предиктор ). В главе 1 у нас еще не было языка для точного определения моделей. Но теперь, воспользовавшись концепциями из первой части книги, мы можем объяснить, что такое «модель». В этой книге представлено два основных под - хода: предиктор как функция и предиктор как вероятностная модель. Первый подход мы опишем здесь, а второй — в следующем подразделе. Предиктор — это функция, которая, получая на ввод конкретный пример (в на- шем случае — вектор признаков), дает вывод. Пока давайте рассмотрим случай, в котором вывод — единственное число, то есть речь идет о вещественночислен - ном скалярном выводе. Это можно записать как f : D → , (8.1) где входной вектор x является D-мерным (имеет D признаков), а затем к нему применяется функция f (записывается как f(x)), которая возвращает веществен - ное число. На рис. 8.2 проиллюстрирована возможная функция, позволяющая рассчитывать прогнозы для входных значений x. В этой книге мы не рассматриваем общего случая для всех функций, для чего нам потребовался бы функциональный анализ. Вместо этого обсудим частный случай для линейных функций f (x) = θTx + θ0 (8.2) для неизвестных θ и θ0. Такое ограничение означает, что материала глав 2 и 3 достаточно, чтобы точно сформулировать понятие предиктора для не вероят - ностного (в противовес вероятностному, описанному ниже) представления о машинном обучении. Линейные функции обеспечивают хороший баланс между универсальностью задач, решаемых с их помощью, и объемом математи - ческого бэкграунда, необходимого для работы с ними.\n--- Страница 324 ---\n324 Глава 8. О сочетании модели и данных 0 10 20 30 40 50 60 70 80 x0255075100125150y Рис. 8.2. Пример функции (черная сплошная диагональная прямая) и ее прогноз при x = 60, то есть f(60) = 100 8.1.3. Модели как вероятностные распределения Зачастую считается, что данные — это зашумленные проявления некого базо - вого наблюдаемого эффекта. Мы надеемся, что, применив машинное обучение, сможем выделить сигнал из шума. Для этого нам требуется язык, который по - зволил бы количественно выразить эффект шума. Также мы обычно хотим иметь предикторы, при помощи которых выражается неопределенность того или иного рода, чтобы квантифицировать нашу уверенность в ценности прогноза или конкретной точки тестовых данных. Как было показано в главе 6, теория вероятностей обеспечивает язык для квантификации неопределенности. На рис. 8.3 прогностическая неопределенность функции показана в виде гаус - сова распределения. Мы будем трактовать предиктор не как отдельную функцию, а как вероятност - ную модель, то есть модель, описывающую распределение возможных функций. В этой книге мы ограничимся рассмотрением частного случая распределений с конечномерными параметрами; это позволит нам описывать вероятностные модели без привлечения стохастических процессов и случайных мер. В данном частном случае можно считать вероятностные модели мультивариантными вероятностными распределениями, которые уже подходят для построения об - ширного класса моделей. В разделе 8.4 мы расскажем, как использовать концепции из теории вероятностей (глава 6) для определения моделей МО, а в разделе 8.5 познакомим вас с гра- фическим языком для компактного описания вероятностных моделей.\n--- Страница 325 ---\n8.1. Данные, модели и обучение 325 0 10 20 30 40 50 60 70 80 x0255075100125150y Рис. 8.3. Пример функции (черная сплошная диагональная прямая) и ее прогностическая неопределенность при x = 60 (показано как гауссиан) 8.1.4. Обучение — это нахождение параметров Цель обучения — найти модель и соответствующие ей параметры таким образом, чтобы результирующий предиктор хорошо работал на неизвестных данных. Существует три концептуально разных алгоритмических фазы, рассматриваемые при обсуждении МО: 1. Прогноз, или инференс. 2. Обучение, или оценка параметров. 3. Настройка гиперпараметров, или выбор модели. Фаза прогноза имеет место, когда мы используем обученный предиктор на еще не известных тестовых данных. Иными словами, параметры и модель уже выбраны и зафиксированы, и предиктор применяется к новым векторам, представляющим новые входные точки данных. Как было показано в главе 1 и в разделе 8.1.3, в этой книге рассматриваются две школы машинного обу - чения, разница между которыми в том, чем является предиктор — функцией или вероятностной моделью. Когда у нас есть вероятностная модель (рас - сматриваемая далее в разделе 8.4), то фаза прогноза называется инференсом (inference). ПРИМЕЧАНИЕ К сожалению, не существует общепринятых наименований для различных фаз алгоритмов. Слово «inference» иногда еще означает оценку параметров вероятностной модели, а также может означать прогнозирование для не вероятностных моделей. \n--- Страница 326 ---\n326 Глава 8. О сочетании модели и данных На этапе обучения или оценки параметров мы корректируем нашу предиктив - ную модель на основании обучающих данных, чтобы найти для них хорошие предикторы. Существует две основные стратегии, позволяющие это сделать: первая — нахождение наилучшего предиктора на основе некоторого качествен - ного параметра (иногда этот процесс называется нахождением точечной оценки) и вторая — при помощи байесовского инференса. Нахождение точечной оценки может применяться с предикторами обоих типов, но для байесовского инферен - са требуются вероятностные модели. Для не вероятностной модели мы придерживаемся принципа минимизации эм ­ пирического риска , который будет описан в разделе 8.2. Минимизация эмпириче - ского риска непосредственно подводит нас к задаче оптимизации для нахождения хороших параметров. В статистических моделях используется принцип макси ­ мальной вероятности , позволяющий найти хороший набор параметров (раз - дел 8.3). Дополнительно мы можем смоделировать неопределенность параметров при помощи вероятностной модели, которую подробнее рассмотрим в разделе 8.4. Мы используем численные методы для нахождения параметров, которые «под - ходят» к данным, и большинство методов обучения можно рассматривать как методы поиска экстремума: ищется максимум для целевого параметра, например максимального правдоподобия. Для применения таких подходов, аналогичных поиску экстремума, используются градиенты, описанные в главе 5, и внедряют - ся подходы с числовой оптимизацией1, описанные в главе 7. Как упоминалось в главе 1, мы заинтересованы так обучить модель на основе имеющихся данных, чтобы в будущем она хорошо работала на новых. Недо - статочно взять и подтянуть модель для хорошей работы на обучающем датасе - те; предиктор должен хорошо справляться с теми данными, которых ранее не видел. Симуляция поведения нашего предиктора на новых, будущих данных достигается при помощи кросс­валидации2 (раздел 8.2.4). Как будет показано в этой главе, для достижения поставленной цели (хорошая работа на неизвест - ных данных) необходимо балансировать между хорошей подгонкой модели под обучающие данные и поиском «простых» объяснений феномена. Такой компро - мисс достигается при помощи регуляризации (раздел 8.2.3) или путем добав - ления априорного значения (раздел 8.3.2). В философии такой подход не счи - тается ни индукцией, ни дедукцией, а называется абдукцией . Согласно Стэнфордской философской энциклопедии, абдукция — это процесс инферен - са наилучшего объяснения (Douven, 2017). 1 Принято считать, что целью оптимизации является минимизация целевого параметра. Поэтому в задачах машинного обучения при определении целей зачастую ставится дополнительный знак «минус». 2 Иначе говоря, перекрестной проверке. — Примеч. ред.\n--- Страница 327 ---\n8.2. Минимизация эмпирического риска 327 Часто приходится принимать при моделировании высокоуровневые решения о структуре предиктора, например сколько компонентов использовать или какой класс вероятностных распределений рассмотреть. Выбор количества компонен - тов — это пример гиперпараметра , и этот выбор может значительно повлиять на производительность модели. Проблема выбора одной из нескольких моделей называется выбором модели (model selection), о чем мы поговорим в разделе 8.6. В случае невероятностных моделей выбор зачастую делается при помощи вло­ женной кросс­валидации , которая описана в разделе 8.6.1, или при подборе ги - перпараметров для нашей модели. ПРИМЕЧАНИЕ Различение параметров и гиперпараметров является в не- которой степени произвольным, и в основном определяется разграничением между тем, что поддается численной оптимизации, и тем, что требует исполь - зования поисковых приемов. Другой подход к трактовке такой разницы — рас - сматривать параметры как явные показатели вероятностной модели, а гиперпа - раметры (параметры более высокого уровня) считать факторами, которые управляют распределением этих явных параметров.  В следующих разделах мы рассмотрим три разновидности машинного обучения: минимизацию эмпирического риска (раздел 8.2), принцип максимального прав - доподобия (раздел 8.3) и вероятностное моделирование (раздел 8.4). 8.2. МИНИМИЗАЦИЯ ЭМПИРИЧЕСКОГО РИСКА Вооружившись всей необходимой математикой, мы в состоянии объяснить, что такое «учиться» с точки зрения машины. «Обучение» в составе машинного обу- чения сводится к оценке параметров на основании обучающего набора данных. В этом разделе будет рассмотрен случай, в котором предиктор является функ - цией, а случай вероятностных моделей будет рассмотрен в разделе 8.3. Мы опишем идею минимизации эмпирического риска, которая изначально была популяризована при рассмотрении метода опорных векторов (о ней рассказа - но в главе 12). Однако общие принципы этой идеи широко применимы и по- зволяют поставить вопрос о сути обучения, не прибегая при этом к явному конструированию вероятностных моделей. Существует четыре основных ва - рианта проектирования, которые мы подробно рассмотрим в следующих под - разделах: Раздел 8.2.1. Какое множество функций мы разрешаем принимать предиктору? Раздел 8.2.2. Как мы измеряем эффективность работы предиктора на обучаю - щих данных?\n--- Страница 328 ---\n328 Глава 8. О сочетании модели и данных Раздел 8.2.3. Как конструировать предикторы на основании только обучающих данных, но таким образом, чтобы эти предикторы хорошо работали на новых тестовых данных? Раздел 8.2.4. Какова процедура поиска в пространстве моделей? 8.2.1. Гипотеза класса функций Допустим, нам дано N примеров xn ∈ D и соответствующие скалярные метки yn ∈ D. Мы рассмотрим случай обучения с учителем, где получим пары ( x1, y1), , (xN, yN). Имея эти данные, мы хотели бы оценить предиктор f(·, θ) : D → , параметризованный θ. Мы надеемся, что удастся найти хороший параметр θ*, такой, что он позволит нам хорошо подогнать данные, чтобы f (xn, θ*) ≈ yn для всех n = 1, , N. (8.3) В этом разделе мы используем нотацию = f(xn,θ*), чтобы представить вывод предиктора. ПРИМЕЧАНИЕ Для удобства представления мы опишем минимизацию эмпири ческого риска в терминах обучения с учителем (где у нас есть метки). Так становится проще определить класс гипотез и функцию потерь. В машинном обучении также распространена практика, при которой выбирается параметри - зованный класс функций, например аффинных функций.  Пример 8.1 Введем задачу регрессионного анализа методом наименьших квадратов, чтобы проиллюстрировать минимизацию эмпирического риска. Более подробно о регрессии рассказано в главе 9. Когда метка yn является веще - ственнозначной, для предикторов в качестве класса функций часто вы - бирают множество аффинных функций1. Мы выберем более компактную нотацию для аффинной функции, подцепив дополнительный единичный признак x(0) = 1 к xn, то есть = . Соответственно, вектор параметров θ = [θ0, θ1, θ2, …, θD]T, благодаря чему можно записать предиктор как линейную функцию f (xn, θ) = θTxn. (8.4) 1 В машинном обучении афинные функции, как правило, называют линейными.\n--- Страница 329 ---\n8.2. Минимизация эмпирического риска 329 Этот линейный предиктор эквивалентен аффинной модели (8.5) Обратите внимание: предиктор принимает вектор признаков, представ - ляющий одиночный пример xn как ввод и выдающий вещественночис - ленный вывод, то есть f : D+1 → . На предыдущих рисунках в этой главе предиктор обозначался прямой линией, что означает: мы полагали его аффинной функцией. Возможно, мы захотим рассмотреть в качестве предикторов нелинейные функции вместо линейных. Недавние достижения в области нейронных сетей обеспечивают эффективное вычисление более сложных классов нелинейных функций. Имея класс функций, мы хотим поискать хороший предиктор. Теперь переходим ко второму компоненту минимизации эмпирического риска: измерим, насколь - ко хорошо предиктор подходит под учебные данные. 8.2.2. Функция потерь для обучения Рассмотрим метку yn для конкретного примера и соответствующий ей прогноз , который мы делаем на основе xn. Чтобы определить, что такое «хорошо под - страиваться к данным», необходимо определить функцию потерь , которая принимает в качестве ввода метку базовой истины и прогноз, а в ответ выдает неотрицательное число (именуемое «потерей»), отражающее, какова величина ошибки, допущенной нами при этом конкретном прогнозе. Наша цель при нахождении хорошего вектора параметров θ* — минимизировать среднюю потерю1 на множестве из N учебных примеров. В машинном обучении часто делается допущение, что множество примеров (x1, y1), , ( xN, yN) независимо и одинаково распределено . Слово «независимо» (раздел 6.4.5) означает, что две точки данных ( xi, yi) и (xj, yj) статистически не зависят друг от друга, то есть что их эмпирическое среднее — хорошая оценка математического ожидания (раздел 6.4.1). Так подразумевается, что эмпирическое среднее потери можно использовать с обучающими данными. Для заданного обучающего датасета {(x1, y1), , ( xN, yN)} вводится нотация матрицы-примера X: = [x1,…, xN]T ∈ NxD и вектор меток y: = [y1,…, yN]T ∈ N. 1 Вместо термина «потеря» часто используется слово «ошибка».\n--- Страница 330 ---\n330 Глава 8. О сочетании модели и данных При использовании этой матричной нотации средняя потеря определяется как (8.6) где = f(xn, θ*). Уравнение (8.6) называется эмпирическим риском и зависит от трех аргументов: предиктора f и данных X, y. Общая стратегия для обучения таким способом называется « минимизация эмпирического риска ». Пример 8.2 (потеря наименьших квадратов) Продолжая пример с регрессией наименьших квадратов, укажем, что из - меряем цену ошибки при обучении при помощи квадратичной потери . Мы хотим минимизировать эмпирический риск (8.6), представляющий собой среднее потерь в пересчете на все данные: (8.7) где мы подставили предиктор = f(xn, θ). Воспользовавшись выбранным нами линейным предиктором f(xn, θ) = θTxn, получаем оптимизированную задачу (8.8) Это уравнение можно эквивалентно выразить в матричной форме: (8.9) Такая ситуация известна под названием « задача наименьших квадратов ». Для нее существует аналитическое решение в замкнутой форме, связан - ное с решением нормальных уравнений, — об этом речь пойдет в раз- деле 9.2. Нас не интересует предиктор, который бы хорошо работал только на обучающих данных. Вместо этого нам нужен предиктор, который хорошо работает (демон - стрирует низкий риск) на ранее не известных тестовых данных. Выражаясь более формально, нам нужно найти предиктор f (с фиксированными парамет- рами), который минимизирует ожидаемый риск (8.10)\n--- Страница 331 ---\n8.2. Минимизация эмпирического риска 331 где y это метка, а f(x) — прогноз на основе примера x. Нотация Rtrue(f) означает, что именно таков истинный риск, если бы у нас был доступ к неограниченному объему данных. Ожидание касается (бесконечного) множества всех возможных данных и меток. Возникают два практических вопроса, связанных с нашим желанием минимизировать ожидаемый риск1, и их мы обсудим в двух следую - щих подразделах: zКаким образом следует видоизменить нашу процедуру обучения, чтобы она хорошо обобщалась? zzКак оценивать ожидаемый риск от (конечных) данных? ПРИМЕЧАНИЕ Многие задачи машинного обучения формулируются с со- путствующей им мерой точности, например с указанием точности прогноза или среднеквадратичной ошибкой. Мера, характеризующая производитель - ность, может быть более сложной, чувствительной к ошибкам, либо охватывать подробности конкретного прикладного варианта. В принципе, проектирова - ние функции потерь для минимизации эмпирического риска должно прямо соответствовать мере расчета производительности, которая указана для данной задачи МО. На практике зачастую возникает несоответствие между проектированием функции потерь и измерением производительности. Это может быть связано с такими факторами, как простота реализации или эф - фективность оптимизации.  8.2.3. Регуляризация для борьбы с переобучением В этом разделе описан материал, дополняющий минимизацию эмпирического риска и обеспечивающий его хорошее обобщение (путем приблизительной минимизации ожидаемого риска). Напомним, с какой целью обучается предиктор при машинном обучении: требуется, чтобы он хорошо работал на ранее не из - вестных данных, то есть предиктор должен хорошо обобщаться. Мы имитируем такие новые данные, придерживая до поры до времени часть датасета. Такой придерживаемый набор данных называется тестовым . При наличии достаточ - но разнообразного класса функций для предиктора f мы, в сущности, можем запоминать обучающие данные для достижения нулевого эмпирического риска2. При том, насколько хорошо это помогает минимизировать потери (и, следова - тельно, риск) на обучающих данных, не приходится ожидать, что предиктор будет хорошо обобщаться на ранее не известных данных. На практике мы рас - полагаем только конечным набором данных, поэтому разбиваем наше множество 1 Ожидаемый риск также часто именуется «математическим ожиданием». 2 Даже знание лишь о производительности предиктора на тестовом множестве — это утечка информации (Blum and Hardt, 2015).\n--- Страница 332 ---\n332 Глава 8. О сочетании модели и данных данных на две выборки — обучающую и тестовую. Обучающий набор исполь - зуется для коррекции модели, а тестовый (не виденный алгоритмом в ходе обу чения) применяется для оценки качества обобщающей способности. Важно, чтобы пользователь не возвращался на новый круг обучения, после того как алгоритм пронаблюдал тестовый набор. При помощи нижних индексов train и test мы обозначаем обучающий и тестовый набор данных соответственно. В разделе 8.2.4 мы вернемся к этой идее, воспользовавшись конечным датасетом для оценки ожидаемого риска. Оказывается, что минимизация эмпирического риска может приводить к пере­ обучению , то есть предиктор слишком сильно подстраивается под обучающие данные и плохо обобщает новые (Mitchell, 1997). Этот общий феномен, связан - ный с достижением очень низких средних потерь на обучающем наборе при больших средних потерях на тестовом, обычно случается, когда у нас мало данных, но сложный класс гипотез. Для конкретного предиктора f (с фиксиро - ванными параметрами) феномен переобучения возникает, когда оценка риска по обучающим данным Remp(f, Xtrain, ytrain) приводит к недооценке ожидаемого риска Rtrue(f). Поскольку мы оцениваем ожидаемый риск Rtrue(f), применяя эмпирический риск на тестовом наборе Remp(f, Xtest, ytest), тот случай, в котором тестовый риск оказывается гораздо выше обучающего, явно свидетельствует о переобучении. Мы вернемся к этой идее в разделе 8.3.3. Следовательно, нам нужно каким-то образом сдвинуть поиск минимизатора эмпирического риска, введя штрафной показатель, который мешал бы оптими - затору возвращать чрезмерно гибкий предиктор. В машинном обучении такой штрафной показатель называется регуляризацией . Регуляризация — это свое- образный компромисс между точным решением при минимизации эмпириче - ского риска и учетом размера или сложности решения. Пример 8.3 (регуляризация в задаче наименьших квадратов) Регуляризация — это подход, предотвращающий сложные или экстре - мальные решения задачи оптимизации. Простейшая стратегия регуляри - зации — заменить задачу наименьших квадратов (8.11) из предыдущего примера «регуляризованной» задачей, в которую добав - лен штрафной показатель, включающий только θ: (8.12)\n--- Страница 333 ---\n8.2. Минимизация эмпирического риска 333 Дополнительный член || θ ||2 называется регуляризатором , а параметр λ — параметром регуляризации . Параметр регуляризации позволяет достичь компромисса при минимизации потерь на обучающем множестве и под- боре выраженности параметров θ. Часто бывает, что при переобучении получаемые значения параметров оказываются слишком велики (Bishop 2006). Параметр регуляризации иногда называется штрафным ; он сдвигает вектор θ ближе к началу координат. Идея регуляризации также присутствует в вероят - ностных моделях как априорная вероятность параметров. Как вы помните из раздела 6.6, чтобы апостериорное распределение повторяло по форме априорное распределение, априорные данные и вероятность должны быть сопряжены. Мы вернемся к этой идее в разделе 8.3.2. В главе 12 мы убедимся, что идея ре - гуляризатора эквивалентна идее широкого зазора. 8.2.4. Кросс-валидация для оценки производительности обобщения В предыдущем разделе мы упомянули, что измеряем ошибку обобщения, оцени - вая ее путем применения предиктора к тестовым данным. Такие данные также иногда именуются валидационным набором . Валидационный набор — это некое подмножество доступных обучающих данных, которое мы приберегаем до поры до времени. Практическая проблема, возникающая при таком подходе, заключа - ется в ограниченности объема доступных данных, а в идеале нам нужно для об - учения модели как можно больше данных. Для этого нам потребуется держать валидационный набор  компактным, что приведет к зашумленной оценке (с большим разбросом) прогностической способности. Одно из решений для до - стижения этих противоречивых целей (большое обучающее множество, большое валидационное множество) — использовать кросс­валидацию . K-кратная кросс- валидация фактически сегментирует данные на K фрагментов, K – 1 из которых образуют обучающее множество , а последний фрагмент служит валидационным множеством  (в соответствии с идеей, изложенной выше). При кросс-валидации перебираются (в идеале) все комбинации присваивания фрагментов к  и ; см. рис. 8.4. Эта процедура повторяется для всех K вариантов валидационной вы - борки, и производительность модели на K прогонах усредняется. Мы делим наш датасет на два множества  =  ∪ , так чтобы они не пересека - лись ( ∩  = ∅), где  — валидационный набор, а обучаем мы нашу модель на . После обучения мы оцениваем производительность предиктора f на валида - ционном множестве , вычисляя, например, среднеквадратичную ошибку\n--- Страница 334 ---\n334 Глава 8. О сочетании модели и данных Обучение Валидация Рис. 8.4. K-кратная кросс-валидация. Датасет делится на K = 5 фрагментов, K – 1 из которых служат обучающим набором (сплошная заливка), а один — валидационным (косая штриховка) (RMSE, root mean square error) обученной модели на этой валидационной вы - борке. Точнее, для каждого сегмента k обучающие данные (k) дают предиктор f(k), который затем применяется к валидационному множеству (k) для вычисления эмпирического риска R(f(k), (k)). Мы проходим через все возможные сегменты валидационных и обучающих выборок и вычисляем среднюю ошибку обобще - ния для предиктора. Кросс-валидация аппроксимирует ожидаемую ошибку обобщения. (8.13) где R(f(k), (k)) — это риск (например, RMSE) на валидационном множестве (k) для предиктора f(k). Аппроксимация происходит по двум причинам: во-первых, из-за конечного размера обучающего набора данных, что дает неоптимальный вариант f(k); во-вторых, из-за конечного размера валидационного набора, что приводит к неточной оценке риска R(f(k), (k)). Потенциальный недостаток K-кратной кросс-валидации заключается в вычислительных расходах на обуче - ние модели K раз, что может быть обременительно, если операция обучения ресурсозатратна. На практике зачастую недостаточно рассмотреть только лишь прямые параметры. Так, приходится исследовать параметры сложности (напри - мер, множество параметров регуляризации), которые могут не являться пря - мыми параметрами модели. Оценка качества модели в зависимости от этих гиперпараметров может вылиться в несколько обучающих прогонов, количество которых будет расти экспоненциально с ростом числа параметров модели. Для поиска хороших гиперпараметров можно использовать вложенную кросс- валидацию (раздел 8.6.1). Но кросс-валидация является чрезвычайно параллельной задачей , то есть ценой минимальных усилий такую задачу можно распараллелить на отдель -\n--- Страница 335 ---\n8.2. Минимизация эмпирического риска 335 ные подзадачи. При наличии достаточных вычислительных ресурсов (напри - мер, оборудования для облачных вычислений, серверных ферм) кросс- валидация занимает не больше времени, чем однократная оценка производительности. В этом разделе мы убедились, что минимизация эмпирического риска основана на следующих концепциях: класс функций, описывающих гипотезы, функция потерь, регуляризация. В разделе 8.3 будет рассмотрено, какой эффект дости - гается, если использовать вероятностное распределение вместо функций потерь и регуляризации. 8.2.5. Дальнейшее чтение В силу того что исходные разработки минимизации эмпирического риска (Vapnik, 1998) излагались тяжеловесным теоретическим языком, многие по - следующие разработки были теоретическими. Эта область исследований на - зывается статистическая теория обучения (Vapnik, 1999; Evgeniou et al., 2000; Hastie et al., 2001; von Luxburg and Sch ölkopf, 2011). Одну из свежих книг по машинному обучению, построенную на этой теоретической основе и посвящен - ную разработке эффективных алгоритмов обучения, написали Шалев-Швартц и Бен-Дэвид (Shalev-Shwartz, Ben-David, 2014). Концепция регуляризации коренится в решении плохо сформулированных обратных задач (Neumaier, 1998). Подход, представленный здесь, называется метод регуляризации Тихонова . Также существует весьма схожая ограниченная версия этого метода — регуляризация Иванова. Регуляризация Тихонова об - ладает глубокими взаимосвязями с компромиссной частотой исключений и под- бором признаков (B ühlmann and Van De Geer, 2011). В качестве альтернативы кросс-валидации используется метод бутстрепа и складного ножа (Hall, 1992; Efron and Tibshirani, 1993; Davidson and Hinkley, 1997). Неверно трактовать минимизацию эмпирического риска (раздел 8.2) как метод, полностью «лишенный вероятностей». Существует основополагающее неиз - вестное вероятностное распределение p(x, y), которое управляет генерацией данных. Однако при подходе с минимизацией эмпирического риска работа происходит без учета выбора такого распределения. Этим он отличается от стандартных статистических подходов, которые явно требуют знания p(x, y). Более того, поскольку распределение является совместным, сочетающим при - меры x и метки y, метки могут быть недетерминированными. В отличие от стандартной статистики, здесь не требуется указывать распределение шума для меток y.\n--- Страница 336 ---\n336 Глава 8. О сочетании модели и данных 8.3. ОЦЕНКА ПАРАМЕТРОВ В разделе 8.2 мы не занимались явным моделированием нашей задачи с исполь - зованием вероятностных распределений. В этом разделе будет рассмотрено, как применять вероятностные распределения для моделирования нашей неопреде - ленности, обусловленной процессом наблюдения и нашей неуверенностью в параметрах предикторов. В разделе 8.3.1 мы введем понятие правдоподобия, концептуально аналогичное функции потерь (раздел 8.2.2) при минимизации эмпирического риска. Концепция априорных вероятностей (раздел 8.3.2) ана - логична концепции регуляризации (раздел 8.2.3). 8.3.1. Метод максимального правдоподобия Идея в основе метода максимального правдоподобия (ММП) (maximum likelihood estimation) — определить функцию параметров, которая позволяет нам найти модель, хорошо обучающуюся на данных. Ключевой аспект в проблеме оценки — это функция правдоподобия или, точнее, ее отрицательный логарифм. Для данных, представленных случайной переменной x, и для семейства плотностей вероятностей p(x | θ), параметризованных θ, отрицательный логарифм правдо ­ подобия задается как x(θ) = −log p(x | θ). (8.14) Нотация x(θ) подчеркивает тот факт, что параметр θ варьируется, а данные x фиксированы. Часто мы опускаем ссылку на x при записи отрицательного ло - гарифма правдоподобия, поскольку в реальности он является функцией θ и за- писывается как (θ), когда случайная переменная, представляющая неопреде - ленность в данных, свободна от контекста. Давайте интерпретируем, что представляет собой плотность вероятностей p(x | θ), смоделировав ее для фиксированного значения θ. Это распределение, модели - рующее неопределенность данных. Иными словами, как только мы выберем тип функции, которую хотим видеть в качестве предиктора, правдоподобность дает нам вероятность, с которой можно наблюдать данные x. Дополнительно возможна такая перспектива: если рассматривать данные как фиксированные (поскольку мы их пронаблюдали) и варьировать параметры θ, то о чем нам сообщит (θ)? О том, насколько вероятна конкретная конфигура - ция θ для наблюдений x. Основываясь на этой второй перспективе, средство оценки максимального правдоподобия дает нам наиболее вероятный параметр θ для конкретного набора данных. Мы рассматриваем конфигурацию обучения с учителем, где мы получаем пары (x1, y1), , ( xN, yN) с xn ∈ D и метками yn ∈ . Мы хотим сконструировать пре -\n--- Страница 337 ---\n8.3. Оценка параметров 337 диктор, который бы принимал вектор признаков xn в качестве ввода и давал прогноз yn (или какой-нибудь близкий к нему). То есть, имея вектор xn, мы хотим получить распределение вероятностей для метки yn. Иными словами, мы ука - зываем условное вероятностное распределение меток, имея примеры конкретной конфигурации параметра θ. Пример 8.4 Первый часто применяемый пример — это указание, что условная веро - ятность меток в примерах описывается гауссовым распределением. Иными словами, мы предполагаем, что сможем объяснить наблюдаемую нами неопределенность независимым гауссовым шумом (раздел 6.5) с нулевым средним εn ∼  (0, σ2). Далее мы предполагаем, что линейная модель используется для прогноза. Таким образом, мы указываем гауссово правдоподобие для каждой используемой в качестве примеров пары xn, yn (8.15) Гауссово правдоподобие для заданного параметра θ проиллюстрировано на рис. 8.3. В разделе 9.2 будет рассмотрено, как явно расширить преды - дущее выражение в терминах гауссова распределения. Мы предполагаем, что примеры во множестве ( xn, yn), , ( xN, yN) являются неза­ висимыми и одинаково распределенными . Слово «независимый» (раздел 6.4.5) подразумевает, что правдоподобие всего набора данных (  = {y1 , yN} и  = {x1, , xN}) факторизуется в произведение правдоподобий каждого отдельного примера (8.16), где p(yn | xn, θ) — это конкретное распределение (в примере 8.4 оно было гауссо - вым). Выражение «одинаково распределено» означает, что все члены в произ - ведении (8.16) относятся к одному и тому же распределению, и все они исполь - зуют общие параметры. С точки зрения оптимизации зачастую проще вычислять такие функции, которые можно разложить на суммы более простых функций. Следовательно, в машинном обучении часто рассматривается отрицательный логарифм правдоподобия1 (8.17) 1 Как вы помните, log( ab) = log(a) + log( b).\n--- Страница 338 ---\n338 Глава 8. О сочетании модели и данных Явно напрашивается такая интерпретация: поскольку θ находится в правой части условия p(yn | xn, θ) (8.15), она должна трактоваться как наблюдаемая и фиксированная. Но эта интерпретация неверна. Отрицательный логарифм правдоподобия (θ) — это функция θ. Следовательно, чтобы найти хороший вектор параметров θ, который качественно объясняет данные x1, y1), , ( xN, yN), минимизируем отрицательный логарифм правдоподобия (θ) относительно θ. ПРИМЕЧАНИЕ Знак «минус» в (8.17) — исторический рудимент, сохранив - шийся из-за соглашения, что правдоподобие требуется максимизировать; но в литературе о численной оптимизации обычно изучается минимизация функ - ций.  Пример 8.5 Продолжая наш пример с гауссовыми правдоподобиями (8.15), отметим, что отрицательный логарифм правдоподобия можно переписать как (8.18 a) (8.18 b) (8.18 c) (8.18 d) Поскольку σ дано, второй член в (8.18 d) является константой, и миними - зация (θ) соответствует решению задачи наименьших квадратов (срав - ните с (8.8)), выраженной в первом члене. Оказывается, что для гауссовых правдоподобий результирующая задача опти - мизации, соответствующая оценке максимального правдоподобия, имеет зам - кнутую форму решения. Подробнее мы поговорим об этом в главе 9. На рис. 8.5 показан регрессионный датасет, получаемый на основе параметров максималь - ного правдоподобия. Оценка максимального правдоподобия может быть под - вержена переобучению (раздел 8.3.3), аналогично нерегуляризованной миними - зации эмпирического риска (раздел 9.2.3). Для других функций правдоподобия, например если мы моделируем наш шум при помощи негауссовых распределений,\n--- Страница 339 ---\n8.3. Оценка параметров 339 оценка максимального правдоподобия может не иметь аналитического решения в замкнутой форме. В таком случае приходится прибегнуть к методам численной оптимизации, рассматриваемым в главе 7. 0 10 20 30 40 50 60 70 80 x0255075100125150y Рис. 8.5. Для заданных данных оценка максимального правдоподобия параметров дает сплошную диагональную прямую. Квадрат соответствует прогнозируемому значению максимального правдоподобия при x = 60 8.3.2. Оценка апостериорного максимума Если у нас есть априорные знания о распределении параметров θ, то дополни - тельно к правдоподобию можно рассмотреть еще один член. Этот дополнитель - ный член — априорное вероятностное распределение по параметрам p(θ). Как нам следует обновить распределение θ для заданного априорного значения после наблюдения данных x? Иными словами, как нам представить тот факт, что мы обладаем более конкретными знаниями об θ, когда пронаблюдаем x? Теорема Байеса, рассмотренная в разделе 6.3, дает нам научно-теоретический аппарат для уточнения наших вероятностных распределений случайных пере - менных. Она позволяет вычислять апостериорное распределение p(θ | x) (более конкретное знание) о параметрах θ по общим априорным утверждениям (апри - орному распределению) p(θ) и функции p(x | θ), связывающей параметры θ и наблюдаемые данные x, называемой правдоподобием . (8.19) Напоминаем, что мы хотим найти такой параметр θ, который максимизиру - ет апостериорное значение. Поскольку распределение p(x) не зависит от θ,\n--- Страница 340 ---\n340 Глава 8. О сочетании модели и данных мы в целях оптимизации можем игнорировать значение знаменателя и полу- чить p(θ | x) ∝ p(x | θ)p(θ). (8.20) Предыдущее пропорциональное отношение скрывает плотность данных p(x), определить которую может быть сложно. Вместо оценки минимума отрицатель - ного алгоритма правдоподобия мы теперь оцениваем минимум отрицательного логарифма плотности апостериорного распределения, что также называется оценкой апостериорного максимума (maximum a posteriori estimation, MAP). На рис. 8.6 показано, что будет, если добавить гауссово априорное распределение с нулевым средним. 0 10 20MLE MAP 30 40 50 60 70 80 x0255075100125150y Рис. 8.6. Сравнение прогнозов, при которых делается оценка максимального правдоподобия (MLE) и оценка MAP при x = 60. Априорное значение немного выравнивает крутизну кривой, а точка пересечения становится ближе к нулю. В данном случае фактор, сдвигающий точку пересечения ближе к нулю, на самом деле увеличивает крутизну Пример 8.6 Дополнительно к предположению о гауссовом правдоподобии из преды - дущего примера, допустим, что вектор параметров распределен как многомерный гауссиан с нулевым средним, то есть p(θ) =  (0, Σ), где Σ — ковариационная матрица (раздел 6.5). Обратите внимание: сопря - женное априорное распределение гауссиана также является гауссианом (раздел 6.6.1). Следовательно, ожидается, что апостериорное распределе - ние также будет гауссовым. Подробнее максимум апостериорного рас - пределения мы рассмотрим в главе 9.\n--- Страница 341 ---\n8.3. Оценка параметров 341 Идея учета априорных знаний о том, где располагаются хорошие параме - тры, распространена в машинном обучении. В качестве альтернативной точки зрения, которую мы рассмотрим в разделе 8.2.3, предлагается ре - гуляризация. Регуляризация вводит в выражение дополнительный член, сдвигающий результирующие параметры ближе к началу координат. Максимальная апостериорная оценка может считаться мостиком между не вероятностным и вероятностным миром, поскольку явно признает необходимость априорного распределения, но все равно обеспечивает лишь точечную оценку параметров. ПРИМЕЧАНИЕ Оценка максимального правдоподобия θML обладает следу - ющими свойствами (Lehmann and Casella, 1998; Efron and Hastie, 2016): zАсимптотическая согласованность: оценка максимального правдоподобия сходится к истинному значению в пределе бесконечного множества наблю - дений плюс случайная ошибка, которая приблизительно нормальна. zРазмер выборок, необходимых для достижения этих свойств, может быть весьма велик. zДисперсия ошибки снижается как 1/ N, где N — количество точек данных. zzОценка максимального правдоподобия может приводить к переобучению, особенно в условиях «небольших» данных.  Принцип оценки максимального правдоподобия (и оценки апостериорного максимума) использует вероятностное моделирование при рассуждении о не- определенности параметров данных и модели. Однако мы здесь пока еще не используем вероятностное моделирование на полную мощность. В этом раз - деле результирующая процедура обучения по-прежнему дает точечную оценку предиктора, то есть в результате возвращается всего одно множество значений параметров, которое представляет наилучший предиктор. В разделе 8.4 мы ис - ходим из того, что значения параметров также должны трактоваться как слу - чайные переменные, и мы будем оценивать не «наилучшие» значения в данном распределении, а использовать при подготовке прогнозов все распределение параметров. 8.3.3. Обучение модели Рассмотрим ситуацию, в которой нам дан датасет, и мы хотим обучить на нем параметризованную модель. Говоря об обучении (fitting), мы, как правило, имеем в виду оптимизацию/обучение модели таким образом, чтобы минимизи -\n--- Страница 342 ---\n342 Глава 8. О сочетании модели и данных ровать некоторую функцию потерь, например отрицательный логарифм прав - доподобия. Рассуждая о максимальном правдоподобии (раздел 8.3.1) и оценке апостериорного максимума (8.3.2), мы уже затронули два распространенных алгоритма, применяемых для приближения модели. При параметризации модели определяется класс моделей Mθ, которым мы можем оперировать. Например, в случае линейной регрессии можно определить от - ношение между входными точками x и (свободными от шума) наблюдениями y, такими чтобы y = ax + b, где θ := {a, b} это параметры модели. В данном случае параметры модели θ описывают семейство аффинных функций, то есть прямых линий с уклоном a, соответствующим смещению от 0 до b. Допустим, данные поступают от модели M*, которая нам неизвестна. Для заданного обучающего набора мы оптимизируем θ таким образом, чтобы Mθ было как можно ближе к M*, где «близость» определяется объективной функцией, которую мы опти - мизируем (например, квадратичная функция потерь на обучающих данных). На рис. 8.7 проиллюстрирована ситуация, где у нас есть небольшой класс моде - лей (обозначенный кругом Mθ), а модель генерации данных M* не входит во множество рассмотренных моделей. Поиск параметров мы начинаем с . После оптимизации, то есть как только мы получим наилучшие возможные параметры θ*, возможны три случая: (i) переобучение, (ii) недообучение, (iii) под- ходящая модель. В общих чертах поясним, что означают три эти концепции. Mθ*Mθ M*Mθ0 Рис. 8.7. Обучение модели. В параметризованном классе Mθ моделей мы оптимизируем параметры модели θ, так чтобы минимизировать расстояние до истинной (неизвестной) модели M* Грубо говоря, под переобучением понимается ситуация, в которой параметризо - ванный класс моделей слишком насыщен, чтобы смоделировать набор данных, сгенерированный M*, то есть Mθ могла бы моделировать гораздо более сложные датасеты. Например, если набор данных был сгенерирован линейной функцией, а мы определяем Mθ как класс многочленов седьмого порядка, то можем в данном случае моделировать не только линейные функции, но и многочлены степени два, три и далее. Переобученные модели, как правило, содержат большое коли -\n--- Страница 343 ---\n8.3. Оценка параметров 343 чество параметров. Часто приходится наблюдать, что чрезмерно гибкий класс моделей Mθ использует всю свою силу моделирования, чтобы снизить ошибку при обучении1. Если обучающие данные зашумлены, то эта модель сможет вычленить в самом шуме какой-то полезный сигнал. Это спровоцирует колос - сальные проблемы, если мы попытаемся делать прогнозы в отрыве от обу чающих данных. На рис. 8.8(а) приведен пример переобучения в контексте регрессии, где параметры модели запоминаются средствами максимального правдоподобия (раздел 8.3.1). Подробнее о переобучении при регрессии мы поговорим в раз- деле 9.2.2. xy (a)/o.g0081 /yright/exclam.g00AF/yright/percent.g00BA/K.g00AD/three.g0082/.notdef.g0107/yright…/comma.g00D2/yright (b)/m.g0073 /yright/.notdef.g0105/percent.g00BA/percent.g00BA/K.g00AD/three.g0082/.notdef.g0107/yright…/comma.g00D2/yright (c)/o.g0081 /percent.g00BA/.notdef.g0105/period.g00B2/percent.g00BA/.notdef.g0105/space.g00AB/question.g009D/equal.g00C8/space.g00AB /.notdef.g00E4/percent.g00BA/.notdef.g0105/yright/.notdef.g00E3/.notdef.g010900 22 44 /dollar.g00DC2/dollar.g00DC2 /dollar.g00DC4/dollar.g00DC4/uni041E/uni0431/uni0443/uni0447/uni0430/uni044E/uni0449/uni0438/uni0435 /uni0434/uni0430/uni043D/uni043D/uni044B/uni0435 MLE xy 00 22 44 /dollar.g00DC2/dollar.g00DC2 /dollar.g00DC4/dollar.g00DC4/uni041E/uni0431/uni0443/uni0447/uni0430/uni044E/uni0449/uni0438/uni0435 /uni0434/uni0430/uni043D/uni043D/uni044B/uni0435 MLE xy 00 22 44 /dollar.g00DC2/dollar.g00DC2 /dollar.g00DC4/dollar.g00DC4/uni041E/uni0431/uni0443/uni0447/uni0430/uni044E/uni0449/uni0438/uni0435 /uni0434/uni0430/uni043D/uni043D/uni044B/uni0435 MLE (/uni0430) /uni041F/uni0435/uni0440/uni0435/uni043E/uni0431/uni0443/uni0447/uni0435/uni043D/uni0438/uni0435 (b) /uni041D/uni0435/uni0434/uni043E/uni043E/uni0431/uni0443/uni0447/uni0435/uni043D/uni0438/uni0435 (c) /uni041F/uni043E/uni0434/uni0445 /uni043E/uni0434/uni044F/uni0449/uni0430/uni044F /uni043C/uni043E/uni0434/uni0435/uni043B/uni044C □Рис. 8.8. Обучение (метод максимального правдоподобия, MLE) различных классов моделей на регрессионном датасете Сталкиваясь с недообучением , мы имеем дело с противоположной проблемой, где класс моделей Mθ недостаточно насыщен. Например, если наше множество данных было сгенерировано синусоидальной функцией, но θ параметризует лишь прямые, то наилучшая процедура оптимизации не приблизит нас к ис- тинной модели. Тем не менее, мы все равно оптимизируем параметры и находим наилучшую прямую, которая моделирует этот набор данных. На рис. 8.8(b) приводится пример модели, которая недообучается — так как данная модель недостаточно гибкая. Как правило, у недообучающихся моделей мало пара метров. Третий случай возникает, когда параметризованный класс моделей почти пра - вилен. В таком случае наша модель обеспечивает хорошее приближение, то есть не переобучается и не недообучается. Это означает, что наш класс моделей об - ладает вполне достаточной насыщенностью для описания того класса моделей, что нам дан. На рис. 8.8( c) показана модель, которая весьма хорошо описывает заданный датасет. В идеале именно с этим классом моделей нам хотелось бы работать, так как он обладает хорошими свойствами обобщения. 1 Один из способов выявить переобучение на практике — заметить, что модель облада - ет низким риском на обучающем датасете, но высоким риском на тестовом датасете; это обнаруживается при кросс-валидации (раздел 8.2.4).\n--- Страница 344 ---\n344 Глава 8. О сочетании модели и данных На практике зачастую определяются очень насыщенные классы моделей Mθ со многими параметрами, например глубокие нейронные сети. Чтобы сгладить проблему переобучения, можно использовать регуляризацию (раздел 8.2.3) или априорные значения (раздел 8.3.2). В разделе 8.6 мы поговорим о том, как вы - бирать класс модели. 8.3.4. Дополнительное чтение При рассмотрении вероятностных моделей принцип оценки максимального правдоподобия обобщает идею регрессии наименьших квадратов для линейных моделей, о чем мы подробно поговорим в главе 9. Ограничивая предиктор толь - ко линейной формой путем применения к ее выводу дополнительной нелиней - ной функции ϕ, то есть p (yn | xn, θ) = ϕ(θTxn), (8.21) можно рассматривать другие модели для иных прогностических задач, например для бинарной классификации или моделирования счетных данных (McCullagh and Nelder, 1989). В качестве альтернативной трактовки можно рассмотреть правдоподобия из экспоненциального семейства (раздел 6.6). Класс моделей, в которых имеется линейная зависимость между параметрами и данными и име- ется потенциально нелинейное преобразование ϕ (под названием функция связи ), называется классом обобщенных линейных моделей (Agresti, 2002, глава 4). У оценки максимального правдоподобия богатая история. Изначально этот метод был предложен сэром Рональдом Фишером (Ronald Fisher) в 1930-е. Идею вероятностных моделей мы подробнее рассмотрим в разделе 8.4. Исследовате - ли вероятностных моделей, в частности, спорят, что лучше — фреквентистская статистика или вероятностные модели. Как упоминалось в разделе 6.1.1, эта дискуссия сводится к определению вероятности. Как вы помните из раздела 6.1, вероятность можно трактовать как обобщение (с допущением неопределенности) логического рассуждения (Cheeseman, 1985; Jaynes, 2003). Метод оценки мак - симального правдоподобия по природе своей фреквентистский, поэтому заин - тересовавшиеся могут обратиться к Эфрону и Хасти (Efron and Hastie, 2016), чтобы составить взвешенное впечатление как о байесовской, так и о фреквен - тистской статистике. Существуют такие вероятностные модели, где оценка максимального правдо - подобия невозможна. Рекомендуем обратиться к более продвинутым книгам по статистике, например Казеллы и Бергера (Casella and Berger, 2002), где рассматриваются такие подходы, как метод импульсов, M-оценка и оценка уравнений.\n--- Страница 345 ---\n8.4. Вероятностные модели и инференс 345 8.4. ВЕРОЯТНОСТНЫЕ МОДЕЛИ И ИНФЕРЕНС В машинном обучении нас часто интересует интерпретация и анализ данных, например для прогнозирования будущих событий и принятия решений. Чтобы упростить такую задачу, зачастую строятся модели, описывающие генеративный процесс , при котором порождаются наблюдаемые данные. Например, можно описать результат эксперимента с подбрасыванием монетки («орел» или «решка») в два этапа. Сначала мы определяем параметр μ, описы - вающий вероятность «орла» как параметр распределения Бернулли (глава 6); далее можно сделать выборку результатов x ∈ {орел, решка} из распределения Бернулли p(x | μ) = Ber( μ). Параметр μ порождает конкретный набор данных  и зависит от используемой монетки. Поскольку μ заранее не известен и не под - дается непосредственному наблюдению, нужны механизмы, которые позволили бы что-то узнать о μ при наличии наблюдаемых результатов в экспериментах с подбрасыванием монетки. В дальнейшем мы обсудим, как для этой цели может использоваться вероятностное моделирование. 8.4.1. Вероятностные модели Вероятностные модели1 представляют неопределенный характер эксперимента как распределения вероятностей. Польза от применения вероятностных моделей в том, что они предлагают единый и согласованный аппарат теории вероятностей (раздел 6) для моделирования, инференса, прогнозирования и выбора модели. В вероятностном моделировании совместное распределение p(x, θ) наблюдаемых переменных x и скрытых параметров θ имеет первоочередное значение. В этом распределении заключена информация о: zаприорных значениях и правдоподобии (правило произведения, раздел 6.3); zмаргинальном правдоподобии p(x), которое сыграет важную роль в выборе модели (раздел 8.6); оно может быть рассчитано так: берется совместное распределение, и из него интегрируются параметры (по правилу суммы, раз - дел 6.3); zzапостериорном значении, которое можно получить, разделив совместное правдоподобие на маргинальное. Только совместное распределение обладает таким свойством. Следовательно, вероятностная модель описывается совместным распределением всех ее слу - чайных переменных. 1 Вероятностная модель описывается совместным распределением всех случайных переменных.\n--- Страница 346 ---\n346 Глава 8. О сочетании модели и данных 8.4.2. Байесовский инференс Ключевая задача в машинном обучении — взяв модель и данные, выявить зна - чения скрытых переменных модели, θ, имея наблюдаемые переменные x. В раз- деле 8.3.1 уже обсуждались два способа оценки параметров модели θ: с помощью максимального правдоподобия или апостериорного максимума1. В обоих слу - чаях мы получаем единственно лучшее значение θ, так что ключевая алгорит - мическая проблема оценки параметров сводится к решению задачи оптимизации. Когда нам известны эти точечные оценки θ*, мы пользуемся ими для прогнози - рования. В частности, прогностическое распределение будет p(x | θ*), где мы используем θ* в функции правдоподобия. Как обсуждалось в разделе 6.3, сосредоточение на одной статистике апостери - орного распределения (например, на параметре θ*, максимизирующем апосте - риорное значение) приводит к потере информации, которая может быть крити - чески важна в системе, использующей прогноз p(x | θ*) для принятия решений. Как правило, такие системы принятия решений имеют иные объективные функции, нежели правдоподобие, среднеквадратичная потеря или ошибка классификации. Следовательно, иметь под рукой полное апостериорное рас - пределение может быть исключительно полезно, это помогает принимать более надежные решения. Байесовский инференс заключается в нахождении такого апостериорного распределения (Gelman et al., 2004)2. Для набора данных  априорный параметр p(θ) и функция правдоподобия (8.22) получаются путем применения теоремы Байеса. Ключевая идея, располагающая к применению теоремы Байеса, — инвертировать отношение между параметра - ми θ и данными  (что определяется в зависимости от правдоподобия) для получения апостериорного распределения p(θ | )3. Если у нас есть апостериорное распределение параметров, то из этого следует, что с его помощью можно распространить неопределенность с параметров на данные. В частности, при распределении p(θ) для параметров наши прогнозы вычисляются как (8.23) 1 Оценка параметров может быть сформулирована как задача оптимизации. 2 Байесовский инференс связан с узнаванием распределения случайных переменных. 3 Байесовский инференс инвертирует отношение между параметрами и данными.\n--- Страница 347 ---\n8.4. Вероятностные модели и инференс 347 и уже не будут зависеть от параметров модели ( θ), которые мы смогли сделать несущественными / удалить интегрированием. Уравнение (8.23) показывает, что прогноз является средним среди всех вероятных значений параметра θ, где такая вероятность выражена в виде распределения параметров p(θ). Обсудив оценку параметров в разделе 8.3, а байесовский инференс здесь, давай - те сравним два этих подхода к обучению. Оценка параметров методом оценки максимального правдоподобия (MLE) или апостериорного максимума (MAP) дает непротиворечивую точечную оценку параметров, θ*, а основная вычисли - тельная задача, которую требуется при этом решить, — оптимизация. Напротив, байесовский инференс результирует в (апостериорное) распределение, и клю- чевая вычислительная задача, которую при этом приходится решить, — интегри - рование. Прогнозы с использованием точечных оценок прямолинейны, тогда как при прогнозировании с помощью байесовского метода требуется дополнительно решать задачу по интегрированию; см. (8.23). Тем не менее байесовский инференс обеспечивает систематический способ учета априорных сведений, побочной информации, а также структурных знаний — а все это непросто сделать в кон- тексте оценки параметров. Более того, распространение неопределенности па - раметров на прогноз может пригодиться в системах принятия решений для оценки рисков и исследовательской работы в контексте обучения с эффективным использованием данных (Deisenroth et al., 2015; Kamthe and Deisenroth, 2018). Хотя байесовский инференс — это систематический математический фреймворк для получения информации о параметрах и для выполнения прогнозов, с ним сопряжены некоторые практические сложности; точнее, с задачами интеграции, которые нам придется решать, см. (8.22) и (8.23). Еще точнее, если не выбрать сопряженное априорное распределение для параметров (раздел 6.6.1), то инте - гралы в (8.22) и (8.23) окажутся не находимыми аналитически, и мы не сможем вычислить апостериорные значения, прогнозы или маргинальное правдоподо - бие в замкнутой форме. В таких случаях приходится прибегать к приближени - ям, например методу Монте-Карло с использованием марковских цепей (Markov chain Monte Carlo, MCMC) (Gilks et al., 1996) или детерминированным при - ближениям, например приближению Лапласа (Bishop, 2006; Barber, 2012; Murphy, 2012), вариационному инферернсу (Jordan et al., 1999; Blei et al., 2017) или рас - пространению ожидания (Minka, 2001). Несмотря на все эти сложности, байесовский инференс успешно применяется для решения разнообразных задач, в том числе крупномасштабного тематиче - ского моделирования (Hoffman et al., 2013), прогнозирования кликабельности (Graepel et al., 2010), обучения с подкреплением в управляющих системах с эф- фективным использованием данных (Herbrich et al., 2007) и создания крупно - масштабных рекомендательных систем. Существуют универсальные подходы, например байесовская оптимизация (Brochu et al., 2009; Snoek et al., 2012;\n--- Страница 348 ---\n348 Глава 8. О сочетании модели и данных Shahriari et al., 2016), которые весьма полезны для эффективного поиска мета - параметров моделей или алгоритмов. ПРИМЕЧАНИЕ В литературе по машинному обучению наблюдается несколь - ко произвольное разделение между (случайными) «переменными» и «параме - трами». Тогда как параметры обычно рассчитываются (например, методом максимального правдоподобия), переменные обычно выводятся на периферию. В этой книге мы не так строго придерживаемся этого разделения, поскольку в принципе можем задать априорное значение для любого параметра, а потом убрать его методом интегрирования. В таком случае параметр превратится в случайную переменную, согласно вышеописанному разделению.  8.4.3. Модели латентных переменных На практике иногда бывает полезно иметь в составе модели дополнительные латентные переменные z (кроме параметров модели θ) (Moustaki et al., 2015). Такие латентные переменные отличаются от параметров модели θ, так как не параметризуют модель явно. Латентные переменные могут описывать процесс генерации данных, тем самым способствуя интерпретируемости модели. Также они зачастую помогают упростить структуру модели и позволяют определять более простые и насыщенные структуры моделей. Упрощение структуры моде - ли зачастую сопровождается тем, что такая модель имеет сравнительно немно - го параметров (Paquet, 2008; Murphy, 2012). Обучение на моделях с латентными переменными (хотя бы методом максимального правдоподобия) можно вы - полнять систематически при помощи алгоритма максимизации математическо - го ожидания (expectation maximization, EM) (Dempster et al., 1977; Bishop, 2006). Пример, в котором могут пригодиться такие латентные переменные, — это, в частности, анализ главных компонент для снижения размерности (глава 10), модели смеси гауссовых распределений для оценки плотности (глава 11), скры - тые марковские модели (Maybeck, 1979) или динамические системы (Ghahramani and Roweis, 1999; Ljung, 1999) для моделирования временных рядов, метаобу - чения и генерации задач (Hausman et al., 2018; S æmundsson et al., 2018). Хотя введение таких переменных может упростить как структуру модели, так и про- цесс порождения, обучение на моделях с латентными переменными, как прави - ло, протекает сложно, что будет показано в главе 11. Модели с латентными переменными также позволяют определять процесс, ге - нерирующий данные на основе параметров. Давайте его рассмотрим. Обозначив данные через x, параметры модели через θ, а латентные переменные через z, получим условное распределение p (x | θ, z), (8.24)\n--- Страница 349 ---\n8.4. Вероятностные модели и инференс 349 при помощи которого сможем сгенерировать данные для любых параметров модели и латентных переменных. Если z — это латентные переменные, задаем для них априорное значение p(z). Как и в случае с моделями, которые мы обсуждали выше, модели с латентными переменными могут применяться для изучения параметров и для инференса в контекстах, рассмотренных в разделах 8.3 и 8.4.2. Чтобы способствовать обу- чению (например, методом оценки максимального правдоподобия или методом байесовского инференса), придерживаемся двухэтапной процедуры. Сперва вычисляем правдоподобие p(x | θ) модели, не зависящее от латентных перемен - ных. Затем используем это правдоподобие для вычисления параметров или байесовского инференса, где применяем те же выражения, что и в разделах 8.3 и 8.4.2 соответственно. Поскольку функция правдоподобия p(x | θ) является предиктивным распреде - лением данных с учетом параметров модели, нам необходимо интегрировать наши латентные переменные, так чтобы (8.25) где p(x | z, θ) дано в (8.24), а p(z) является априорным для латентных перемен - ных. Обратите внимание, что правдоподобие не должно зависеть от латентных переменных z, а является лишь функцией данных x и параметров модели θ. Правдоподобие в (8.25) непосредственно располагает к оценке параметров методом максимального правдоподобия. MAP-оценка также легко выполняет - ся с дополнительным априорным значением для параметров модели θ, как рас - сматривалось в разделе 8.3.2. Более того, с учетом правдоподобия (8.25), байе - совский инференс (раздел 8.4.2) в модели с латентными переменными работает как обычно: задаем априорное значение p(θ) для параметров модели и при по - мощи теоремы Байеса получаем апостериорное распределение (8.26) по параметрам модели, имея набор данных. Апостериорное в (8.26) может ис - пользоваться для прогнозов в системе байесовского инференса; см. (8.23). Одна из трудностей, связанных с использованием такой модели латентных переменных, заключается в том, что правдоподобие p( | θ) требует маргинали - зации латентных переменных согласно (8.25). Кроме случаев, когда мы вы бираем априорное сопряжение p(z) для p(x |z, θ), маргинализация в (8.25) не является аналитически находимой, приходится довольствоваться приближениями (Bishop, 2006; Paquet, 2008; Murphy, 2012; Moustaki et al., 2015).\n--- Страница 350 ---\n350 Глава 8. О сочетании модели и данных Аналогично апостериорному распределению параметров (8.26), можно вычис - лить распределение для латентных переменных согласно , (8.27) где p(z) является априорным распределением латентных переменных, а p( | z) требует исключить параметры модели θ. Учитывая, насколько сложно аналитически вычислять интегралы, понятно, что одновременная маргинализация как латентных переменных, так и параметров модели вообще невозможна (Bishop, 2006; Murphy, 2012). Проще рассчитать апостериорное распределение латентных переменных, но эта величина обуслов - лена параметрами модели, то есть (8.28) где p(z) является априорным распределением латентных переменных, а p( | z, θ) дано в (8.24). В главах 10 и 11 мы выведем функции правдоподобия для анализа главных компонент и моделей смесей гауссовых распределений соответственно. Кроме того, мы вычислим апостериорные распределения (8.28) латентных переменных. ПРИМЕЧАНИЕ Возможно, в следующих главах мы не будем проводить столь четкого разделения между латентными переменными z и неопределенными параметрами модели θ и будем называть их как «латентными», так и «скрыты - ми» (поскольку они ненаблюдаемы). В главах 10 и 11, где используются латент - ные переменные z, мы все-таки обратим внимание на эту разницу, так как у нас будет два разных типа скрытых переменных: параметры модели θ и латентные переменные z.  Можно воспользоваться тем фактом, что все элементы вероятностной модели являются случайными переменными, и выработать унифицированный язык для их представления. В разделе 8.5 будет показан компактный графический язык для представления структуры вероятностных моделей. Мы воспользуемся им в следующих главах. 8.4.4. Дальнейшее чтение Вероятностные модели в машинном обучении (Bishop, 2006; Barber, 2012; Murphy, 2012) удобны для систематического описания неопределенности, присущей\n--- Страница 351 ---\n8.5. Направленные графические модели 351 данным и прогностическим моделям. У Гахрамани (Ghahramani, 2015) дан краткий обзор вероятностных моделей в машинном обучении. Возможно, нам удастся аналитически рассчитать интересующие параметры данной вероятност - ной модели, но в целом аналитические решения встречаются редко, и прихо - дится использовать вычислительные методы, такие как формирование выборки (Gilks et al., 1996; Brooks et al., 2011) или вариационный инференс (Jordan et al., 1999; Blei et al., 2017). Мустаки и др. (Moustaki et al., 2015) и Паке (Paquet, 2008) дают хороший обзор байесовского инференса в моделях с латентными пере - менными. В последние годы было предложено несколько языков программирования, цель которых — работа с переменными, определенными в программе как случайные величины, соответствующие вероятностным распределениям. Это нужно для написания сложных функций вероятностных распределений, пока под капотом компилятор автоматически обеспечивает выполнение правил байесовского инференса. Эта стремительно развивающаяся дисциплина называется вероят ­ ностным программированием . 8.5. НАПРАВЛЕННЫЕ ГРАФИЧЕСКИЕ МОДЕЛИ В этом разделе мы познакомимся с графическим языком для описания веро - ятностных моделей и получения так называемой направленной графической модели1. Этот язык обеспечивает лаконичный способ указания вероятностных моделей и позволяет читателю визуально разбирать зависимости между слу - чайными переменными. Графическая модель наглядно показывает, как имен - но совместное распределение, охватывающее все случайные переменные, можно разложить на произведение множителей, зависящее только от под - множества этих переменных. В разделе 8.4 мы показали, что совместное рас - пределение вероятностной модели — это основная интересующая нас вели - чина, так как в нем содержится информация об априорных и апостериорных значениях, а также о правдоподобии. Тем не менее, совместное распределение само по себе может быть весьма сложным и ничего не говорит нам о струк - турных характеристиках вероятностной модели. Например, совместное рас - пределение p(a, b, c) ничего не сообщает о независимости величин. Именно здесь в дело вступают графические модели. В этом разделе мы опираемся на концепции независимости и условной независимости, описанные в разде - ле 6.4.5. В графической модели узлы являются случайными переменными. На рис. 8.9(a) все узлы соответствуют случайным переменным a, b, c. Ребра представляют 1 Направленные графические модели также называются байесовскими сетями.\n--- Страница 352 ---\n352 Глава 8. О сочетании модели и данных вероятностные отношения между переменными, например условные вероят - ности. a b x5 c (a) Полносвязная (b) Неполносвязнаяx1x2 x4x3 Рис. 8.9. Примеры направленных графических моделей ПРИМЕЧАНИЕ Не всякое распределение можно представить в конкретной графической модели, выбранной нами. Этот вопрос подробно раскрывает Бишоп (Bishop, 2006).  У вероятностных графических моделей есть некоторые удобные свойства: zС их помощью просто визуализировать структуру вероятностной модели. zОни позволяют проектировать или обосновывать новые виды статистических моделей. zДостаточно рассмотреть только граф, чтобы составить впечатление о свой- ствах модели, например ее условной независимости. zСложные вычисления для инференса и обучения в статистических моделях можно выразить в терминах графических манипуляций. 8.5.1. Семантика графов Направленные графические модели / байесовские сети — это метод представления условных зависимостей в вероятностной модели. Они наглядно описывают условные вероятности и таким образом обеспечивают простой язык для пред - ставления сложных взаимосвязей. При модульном описании также снижается вычислительная сложность задачи. Направленные связи (стрелки) между дву - мя узлами (случайными переменными) указывают условные вероятности1. Например, стрелка между a и b на рис. 8.9(а) означает вероятность p(b | a) для b при условии события a. 1 При дополнительных допущениях стрелки могут использоваться для указания при - чинных отношений (Pearl 2009).\n--- Страница 353 ---\n8.5. Направленные графические модели 353 Направленные графические модели можно вывести из совместных распределе - ний, если нам что-либо известно об их факторизации. Пример 8.7 Рассмотрим совместное распределение p (a, b, c) = p(c | a, b)p(b | a)p(a) (8.29) трех случайных переменных a, b, c. Из факторизации совместного рас - пределения в (8.29) можно сделать некоторые выводы об отношении между случайными переменными: • c непосредственно зависит от a и b; • b непосредственно зависит от a; • a не зависит ни от b, ни от c. Для факторизации в (8.29) мы получаем направленную графическую модель, показанную на рис. 8.9( a). В принципе, можно построить соответствующую направленную графическую модель из факторизованного совместного распределения следующим образом: 1. Создать узел для всех случайных переменных. 2. Для каждого условного распределения добавить в граф прямую связь (стрел - ку) от узлов, соответствующих тем переменным, которыми обусловлено распределение. Топология графа зависит от того, какой вариант факторизации был выбран для совместного распределения. Мы обсуждали, как перейти от известной факторизации совместного распреде - ления к соответствующей направленной графической модели. Теперь сделаем прямо противоположное — опишем, как извлечь совместное распределение множества случайных переменных из заданной графической модели. Пример 8.8 Рассмотрев графическую модель с рис. 8.9(b), воспользуемся двумя ее свойствами: • Искомое нами совместное распределение p(x1, , x5) получается из множества условий, по одному на каждый узел в графе. В данном конкретном примере нам понадобится пять условий.\n--- Страница 354 ---\n354 Глава 8. О сочетании модели и данных • Каждое условие зависит только от узлов, родительских для конкрет - ного узла в графе. Например, x4 будет обусловлено x2. Эти два свойства дают желаемую факторизацию совместного распреде - ления p (x1, x2, x3, x4, x5) = p(x1)p(x5)p(x2 | x5)p(x3 | x1, x2)p(x4 | x2). (8.30) Вообще, совместное распределение p(x) = p(x1, , xK) задается как (8.31) где Pak означает «родительские узлы xk». Родительские узлы xk — это узлы, стрелки от которых указывают на xk. Закончим этот подраздел конкретным экспериментом — «орел и решка». Рас - смотрим схему Бернулли (пример 6.8), где вероятность, что результат x в данном случае будет решкой, равна p (x | μ) = Ber( μ). (8.32) Теперь мы повторяем этот эксперимент N раз и наблюдаем его исходы x1, , xN, так чтобы получить совместное распределение (8.33) Выражение в правой части является произведением распределений Бернулли по каждому отдельному результату, поскольку эксперименты независимы. Как вы помните из раздела 6.4.5, статистическая независимость означает, что рас - пределение факторизуется. Чтобы записать графическую модель для такой ситуации, нужно обозначить отличие между ненаблюдаемыми/латентными переменными и наблюдаемыми переменными. Графически наблюдаемые пере - менные обозначаются как затененные узлы, что позволяет нам получить такую графическую модель, как на рис. 8.10( а). Мы видим, что единственный пара - метр μ одинаков для всех xn, n = 1, , N, поскольку результаты xn распределены одинаково. Более компактная и при этом эквивалентная графическая модель для данной ситуации приведена на рис. 8.10( b), где используется нотация пла­ шек. В плашке (рамке) все, что внутри (в данном случае, наблюдения xn), по- вторяется N раз. Следовательно, обе графические модели эквивалентны, но\n--- Страница 355 ---\n8.5. Направленные графические модели 355 нотация с плашками более компактна. Графические модели сразу же позволяют нам присвоить априорное распределение гиперпараметра для μ. Априорное рас ­ пределение гиперпараметра (hyperprior) — это второй уровень априорных рас - пределений параметров над первым уровнем. На рис. 8.10(с) бета( α, β)-априорное распределение дается для латентной переменной μ. Если считать α и β детер - минированными параметрами, а не случайными переменными, то обводить их в кружок не нужно. /g80/g80 /g80 (a) /b.g007B/yright/exclam.g00AF“/comma.g00D2/space.g00AB “ /space.g00AB/quotedbl.g006D…/slash.g00A9/.notdef.g00E4 xnn = 1, . . . , Nn = 1, . . . , N (b) /b.g007B/yright/exclam.g00AF“/comma.g00D2/space.g00AB “ …/percent.g00BA/two.g0088/equal.g00C8/.notdef.g0106/comma.g00D2/yright/L.g00AE /C.g00B9/.notdef.g00E3/equal.g00C8/.notdef.g0108/yright/asterisk.g007D/g69 /g68 (c) /c.g0080/comma.g00D2/C.g00B9/yright/exclam.g00AF/C.g00B9/equal.g00C8/exclam.g00AF/equal.g00C8/.notdef.g00E4/yright/two.g0088/exclam.g00AF/slash.g00A9 /g68 /comma.g00D2 /g69 /.notdef.g0105/.notdef.g00E3/space.g00AB /.notdef.g00E3/equal.g00C8/two.g0088/yright…/two.g0088…/percent.g00BA/.notdef.g0104/percent.g00BA /g80x1xNxnxn Рис. 8.10. Графическое моделирование для серии испытаний Бернулли 8.5.2. Условная независимость и d-разбиение При помощи направленных графических моделей можно находить свойства условной независимости (раздел 6.4.5) для совместного распределения, просто посмотрев на граф. Ключевую роль в данном случае играет концепция под на - званием d­разбиение . Рассмотрим обычный направленный граф, где , ,  — произвольные непере - секающиеся множества узлов (объединение которых может быть меньше пол - ного количества узлов в графе). Мы хотим убедиться, что утверждение о кон- кретном условном распределении «  условно независимо от  при », обозначающееся как  ⫫  | , (8.34) может быть представлено в виде направленного ациклического графа. Чтобы сделать это, рассмотрим все возможные пути (игнорирующие направление стрелок) от любого узла в  к любым узлам в . Такой путь называют заблоки - рованным, если он содержит хотя бы один узел, для которого верно одно из следующих утверждений: zСтрелки на пути встречаются в узле либо голова в хвост, либо хвост в хвост, а узел относится к множеству .\n--- Страница 356 ---\n356 Глава 8. О сочетании модели и данных zzСтрелки встречаются в узле голова в голову, и ни этот узел, ни один из его потомков не относится к множеству . Если все пути заблокированы, то говорят, что  d-отделено от  из-за . В таком случае совместное распределение по всем переменным в графе будет удовлет - ворять  ⫫  | . Пример 8.9 (условная независимость) Рассмотрим графическую модель с рис. 8.11. Из рисунка видно, что b ⫫ d | a, c; (8.35) a ⫫ c | b; (8.36) b ⫫/ d | c; (8.37) a ⫫/ c | b, e. (8.38) a b c d e Рис. 8.11. Пример d-разделения Направленные графические модели обеспечивают компактное представление вероятностных моделей. В главах 9–11 мы увидим примеры направленных графических моделей. Это представление, вместе с концепцией условной неза - висимости, позволяет факторизовать соответствующие вероятностные модели в выражения, которые проще оптимизируются. Графическое представление вероятностной модели позволяет наглядно показать влияние на структуру модели того или иного выбора при проектировании. Ча - сто приходится делать обобщенные допущения о структуре модели. Эти допу - щения при моделировании (гиперпараметры) влияют на прогностическую производительность, но их нельзя выбрать напрямую при помощи тех подходов, что мы уже успели рассмотреть. Иные способы выбора структуры будут рас - смотрены в разделе 8.6.\n--- Страница 357 ---\n8.5. Направленные графические модели 357 8.5.3. Дальнейшее чтение Введение в вероятностные графические модели дает Бишоп (Bishop, 2006) в главе 8, а расширенное описание возможностей их применения и соответству - ющих алгоритмических следствий дается в книге Коллера и Фридмана ( Koller and Friedman, 2009). Существует три основных типа вероятностных графических моделей: zНаправленные графические модели (байесовские сети) (рис. 8.12( a)). zНенаправленные графические модели (марковские случайные поля) (рис. 8.12( b)). zФактор­графы (рис. 8.12( c)). a b c (a) Направленная графическая модельa b c (b) Ненаправленная графическая модельa b c (c) Фактор-граф Рис. 8.12. Три типа графических моделей: ( a) направленные графические модели (байесовские сети); ( b) ненаправленные графические модели (марковские случайные поля); ( c) фактор-графы На основании графических моделей можно применять графовые алгоритмы для инференса и обучения, например путем локального обмена сообщениями. Об - ласти могут быть разными: построение рейтинга в онлайн-играх (Herbrich et al., 2007), компьютерное зрение (такие задачи, как сегментация изображений, се - мантическая разметка, сглаживание изображений, восстановление изображений (Kittler and F öglein, 1984; Sucar and Gillies, 1994; Shotton et al., 2006; Szeliski et al., 2008)), теория кода (McEliece et al., 1998), решение систем линейных урав - нений (Shental et al., 2008), итеративная байесовская оценка состояния при обработке сигналов (Bickson et al., 2007; Deisenroth and Mohamed, 2012). Одна из тем, особенно важная в реальном применении, но не рассматриваемая в этой книге, — структурное прогнозирование (Bakir et al., 2007; Nowozin et al., 2014). Такие модели позволяют обрабатывать при помощи моделей машинного обучения структурные прогнозы, например последовательности, деревья, графы. Популярность моделей нейронных сетей располагает к использованию более гибких вероятностных моделей, благодаря чему находится множество вариантов полезного применения структурных моделей (Goodfellow et al., 2016, глава 16).\n--- Страница 358 ---\n358 Глава 8. О сочетании модели и данных В последние годы вновь растет интерес к графическим моделям, поскольку они применимы для причинного вывода / инференса (Pearl, 2009; Imbens and Rubin, 2015; Peters et al., 2017; Rosenbaum, 2017). 8.6. ВЫБОР МОДЕЛИ В машинном обучении часто приходится принимать высокоуровневые решения, которые критично влияют на производительность модели. Наш выбор (напри - мер, функциональная форма правдоподобия) влияет на количество и типы свободных параметров модели, а значит, также на гибкость и выразительность модели. Чем сложнее модель, тем более она гибкая в том смысле, что ее можно использовать для описания большего количества датасетов. Например, много- член степени 1 (вида y = a0 + a1x) может применяться лишь для описания ли - нейных отношений между входными значениями x и наблюдениями y. Много- член степени 2 также может описывать квадратичные отношения между исходными данными и наблюдениями1. В таком случае можно подумать, что очень гибкие модели в целом предпочти - тельнее более простых моделей, поскольку гибкие модели выразительнее. Общая проблема в том, что во время обучения можно оценивать производительность модели и изучать ее параметры только на основании обучающего датасета. Од - нако производительность работы на этом наборе данных нас не очень интере - сует. В разделе 8.3 мы убедились, что оценка максимального правдоподобия может приводить к переобучению, особенно когда обучающая выборка неболь - шая. В идеале наша модель (также) хорошо работает на тестовом наборе (кото - рый в период обучения недоступен). Следовательно, нам нужны какие-то ме - ханизмы, которые помогут оценить, как модель обобщает неизвестные тестовые данные. Выбор модели нужен для решения именно этой проблемы. 8.6.1. Вложенная кросс-валидация Мы уже рассматривали подход (кросс-валидация, раздел 8.2.4), который может использоваться для выбора модели. Как вы помните, кросс-валидация позво - ляет оценить ошибку обобщения, многократно разбивая датасет на два набора данных, обучающий и валидационный. Применим эту идею еще раз, то есть для каждого разделения снова будем проводить кросс-валидацию. Иногда такой подход называется вложенной кросс­валидацией (рис. 8.13). Внутренний уровень используется для оценки производительности конкретной выбранной модели или гиперпараметра внутреннего валидационного датасета. Внешний уровень 1 Многочлен y = a0 + a1x + a2x2 также может описывать линейные функции, если задать a2 = 0, то есть он строго более выразителен, чем многочлен первого порядка.\n--- Страница 359 ---\n8.6. Выбор модели 359 используется для оценки обобщающей способности наилучшей модели, которая была выбрана во внутреннем цикле. Во внутреннем цикле можно тестировать различные варианты выбранных моделей и гиперпараметров. Для различения этих двух уровней набор, который используется для оценки производитель - ности обобщения, часто называется тестовым , а тот, что используется для вы - бора наилучшей модели — валидационным . Внутренний цикл оценивает ожи - даемое значение ошибки обобщения для данной модели (8.39), аппроксимируя погрешность путем применения эмпирической ошибки к валидационному на - бору, то есть (8.39) где R( | M) — это эмпирический риск (например, среднеквадратичная ошибка) валидационного набора  для модели M. Эту процедуру повторим для всех моделей и выберем из них наиболее производительную. Обратите внимание: кросс-валидация дает нам не только ожидаемую ошибку обобщения, но и по- зволяет получить статистику высшего порядка, например стандартную ошиб - ку1 — примерный показатель того, насколько неопределенной является средняя оценка. Когда модель выбрана, можно оценить окончательную производитель - ность тестового набора. /b.g007B“/yright /exclam.g00AF/equal.g00C8ƒ/.notdef.g00E4/yright/.notdef.g0107/yright……/slash.g00A9/yright /.notdef.g0105/equal.g00C8……/slash.g00A9/yright /b.g007B“/yright /three.g0082/.notdef.g0107/yright/K.g00AD…/slash.g00A9/yright /.notdef.g0105/equal.g00C8……/slash.g00A9/yrightŠ/yright“/two.g0088/percent.g00BA/quotedbl.g006D/slash.g00A9/yright /.notdef.g0105/equal.g00C8……/slash.g00A9/yright /n.g007C/K.g00AD/three.g0082/.notdef.g0107/yright…/comma.g00D2/yright /.notdef.g00E4/percent.g00BA/.notdef.g0105/yright/.notdef.g00E3/comma.g00D2 /b.g007B/equal.g00C8/.notdef.g00E3/comma.g00D2/.notdef.g0105/equal.g00C8/.notdef.g0106/comma.g00D2/space.g00AB Рис. 8.13. Вложенная кросс-валидация. Выполняется два уровня K-кратной кросс-валидации 8.6.2. Выбор байесовской модели Существует множество подходов к выбору модели, некоторые из них рассмо - трены в этом разделе. В целом, все они нацелены на достижение компромисса между сложностью модели и прогнозируемостью данных. Предполагается, что чем проще модель, тем менее она подвержена переобучению, поэтому цель вы - 1 Стандартная ошибка определяется как , где K — это количество экспериментов, а σ — стандартное отклонение риска для каждого эксперимента.\n--- Страница 360 ---\n360 Глава 8. О сочетании модели и данных бора модели — найти простейшую модель, которая вполне хорошо описывает имеющиеся данные. Эта концепция также называется бритва Оккама . ПРИМЕЧАНИЕ Если подойти к выбору модели как к задаче проверки гипо - тезы, то мы будем искать простейшую гипотезу, согласующуюся с данными (Murphy, 2012).  Можно попробовать отдать приоритет тем моделям, которые тяготеют к про- стоте. Правда, делать это необязательно: «автоматическая бритва Оккама» ко - личественно воплощена в применении байесовского вероятностного подхода (Smith and Spiegelhalter 1980; Jefferys and Berger, 1992; MacKay, 1992). Рису - нок 8.14, сделанный по образцу из книги МакКея (MacKay, 2003), позволяет интуитивно понять, почему сложные и очень выразительные модели — порой не самый лучший выбор для моделирования датасета . Давайте представим, что по горизонтальной оси представлено пространство всех возможных наборов данных . Если нас интересует апостериорная вероятность p(Mi | ) для моде - ли Mi, на основе данных , то можно воспользоваться теоремой Байеса. Пред - полагая, что мы используем однородное априорное p(M) для всех моделей, тео рема Байеса присваивает моделям вознаграждение пропорционально тому, насколько хорошо они позволили спрогнозировать возникшие данные. Прогноз Доказательность p (D | M1) p (D | M2) D C Рис. 8.14. Байесовский инференс — это воплощение бритвы Оккама. По горизонтальной оси представлено пространство всех возможных множеств данных . Доказательность (вертикальная ось) позволяет оценить, насколько хорошо модель прогнозирует доступные данные. Поскольку p( | Mi) должно интегрироваться до 1, мы выбираем модель с наибольшим значением по оси доказательности. Иллюстрация взята из книги МакКея (MacKay, 2003)\n--- Страница 361 ---\n8.6. Выбор модели 361 данных при наличии модели Mi, p( | Mi), называется доказательностью Mi. Простая модель M1 может прогнозировать лишь небольшое количество датасе - тов, что демонстрируется на примере p( | M1); более производительная мо - дель M2, имеющая, например, больше свободных параметров, чем M1, позволяет прогнозировать более разнообразные датасеты1. Однако это означает, что M2 не прогнозирует датасеты в области C, равно как и M1. Предположим, что двум моделям присвоены одинаковые априорные вероятности. Затем, если датасет приходится на область C, менее мощная модель M1 дает более качественные вероятности. Выше в этой главе утверждалось, что модели должны объяснять данные, то есть должен быть способ сгенерировать данные на основе имеющейся модели. Кроме того, если модель как следует обучилась на данных, то мы рассчитываем, что сгенерированные ею данные должны быть похожи на эмпирические. Для этого полезно сформулировать проблему выбора модели как задачу иерархического инференса, что позволяет нам вычислить апостериорное распределение для разных моделей. Рассмотрим конечное количество моделей M = {M1, , MK}, где каждая модель обладает параметрами θk. При байесовском выборе модели априорное значение p(M) присваивается множеству моделей. Соответствующий генеративный про ­ цесс, позволяющий получить данные на основе этой модели, таков: Mk ∼ p(M); (8.40) θk ∼ p(θ | Mk); (8.41)  ∼ p( | θk), (8.42) как показано на рис. 8.15. Имея обучающий набор данных , мы применяем теорему Байеса и рассчитываем апостериорное распределение для моделей как p (Mk | ) ∝ p(Mk)p( | Mk). (8.43) Обратите внимание: это апостериорное распределение более не зависит от па - раметров модели θk, так как в байесовской постановке задачи они были сокра - щены методом интегрирования, поскольку (8.44) 1 Прогнозы количественно выражаются нормализованным распределением вероятно - стей , то есть результат должен интегрироваться/суммироваться до 1.\n--- Страница 362 ---\n362 Глава 8. О сочетании модели и данных где p(θk | Mk) — это априорное распределение параметров θk модели Mk. Член (8.44) называется доказательностью модели , или маргинальным правдоподобием . Из аппостериорного значения в (8.43) мы определяем оценку MAP (8.45) При однородном априорном p(Mk) = , дающем всем моделям равную (апри - орную) вероятность, определение MAP сводится к выбору модели, обладающей максимальной доказательностью (8.44). M θ D Рис. 8.15. Иллюстрация иерархического генеративного процесса при байесовском выборе модели. Устанавливаем априорное значение p(M) для множества моделей. Для каждой модели существует распределение p(θ | M) для соответствующих параметров. Оно используется для генерации данных  ПРИМЕЧАНИЕ Существует важное отличие между правдоподобием и мар- гинальным правдоподобием (доказательностью). Тогда как правдоподобие может страдать из-за переобучения, маргинального правдоподобия это обычно не касается, так как параметры модели были исключены (то есть нам больше не требуется подстраивать ее с учетом этих параметров). Более того, маргинальное правдоподобие автоматически воплощает компромисс между сложностью мо - дели и прогнозируемостью данных (бритва Оккама).  8.6.3. Коэффициент Байеса для сравнения моделей Рассмотрим задачу сравнения двух вероятностных моделей M1, M2 при наличии набора данных . Если вычислить апостериорные вероятности p(M1 | ) и p(M2 | ), то можно рассчитать и соотношение апостериорных (8.46)\n--- Страница 363 ---\n8.6. Выбор модели 363 Отношение апостериорных вероятностей также называется апостериорными шансами . Первая дробь в правой части выражения (8.46), априорные шансы , позволяет измерить, насколько M1 предпочтительнее M2 согласно нашим апри - орным (исходным) представлениям. Отношение маргинальных правдоподобий (вторая дробь в правой части) называется коэффициентом Байеса и позволяет измерить, насколько успешнее M1 по сравнению с M2 прогнозирует . ПРИМЕЧАНИЕ Согласно парадоксу Джеффриса — Линдли , «коэффициент Байеса всегда в пользу более простой модели, так как вероятность данных в сложной модели с диффузным априорным распределением будет очень не - велика» (Murphy, 2012). Здесь диффузным называется такое априорное рас - пределение, которое не отдает предпочтения конкретным моделям, то есть при таких априорных данных много моделей, которые кажутся вполне возможными.  Если выбрать однородное априорное для множества моделей, то член априорных шансов в (8.46) будет равен 1, то есть апостериорные шансы будут равны от - ношению маргинальных правдоподобий (коэффициенту Байеса) (8.47) Если коэффициент Байеса больше 1, то мы выбираем модель M1, в противном случае — M2. По аналогии с частотной статистикой, здесь существуют рекомен - дации по величине отношения, которые нужно учесть, прежде чем судить о «значимости» результата (Jeffreys, 1961). ПРИМЕЧАНИЕ Маргинальное правдоподобие играет важную роль при вы - боре модели. Необходимо вычислить коэффициенты Байеса (8.46) и апостери - орные распределения для моделей (8.43). К сожалению, для расчета маргинального правдоподобия требуется брать ин - теграл (8.44). Такая интеграция обычно аналитически невычислима, поэтому приходится довольствоваться приемами приближения, например численным интегрированием (Stoer and Burlirsch, 2002), стохастическими приближениями с применением метода Монте-Карло (Murphy, 2012) или байесовскими метода - ми Монте-Карло (O’Hagan, 1991; Rasmussen and Ghahramani, 2003). Однако в некоторых частных случаях такие задачи решаемы. В разделе 6.6.1 мы обсуждали сопряженные модели. Если выбрать для сопряженных параметров априорное распределение p(θ), то можно вычислить маргинальное правдопо - добие в замкнутой форме. В главе 9 именно это мы сделаем в контексте линей - ной регрессии. \n--- Страница 364 ---\n364 Глава 8. О сочетании модели и данных В этой главе мы познакомились с базовыми концепциями машинного обучения. В оставшейся части книги мы рассмотрим, как три различные варианта обучения, представленные в разделах 8.2–8.4, применяются при работе с четырьмя стол - пами МО (регрессия, снижение размерности, оценка плотности и классифика - ция). 8.6.4. Дальнейшее чтение В начале раздела мы упоминали, что при высокоуровневом моделировании принимаются решения, выбор которых влияет на производительность модели. Среди примеров такого рода: zстепень многочлена при подготовке регрессии; zколичество компонентов в смешанной модели; zархитектура (глубокой) нейронной сети; zтип ядра в методе опорных векторов; zразмерность латентного пространства при анализе главных компонент; zzскорость (план) обучения в оптимизационном алгоритме. Расмуссен и Гахрамани (Rasmussen and Ghahramani, 2001) показали, что не - обходимо добавлять ограничения (штраф) ряду параметров модели, однако с точки зрения сложности функций модель активна. Они также показали, что автоматическая бритва Оккама применима и для байесовских непараме - трических моделей с множеством параметров, например для гауссовых про - цессов1. Если сосредоточиться на оценке максимального правдоподобия, то можно использовать ряд эвристических приемов, действующих при подборе модели и снижающих вероятность переобучения. Они называются информационны - ми критериями, и мы выбираем модель с наивысшим значением. Таков инфор - мационный критерий Акаике (Akaike information criterion, AIC) (Akaike, 1974), позволяющий скорректировать смещение оценки максимального правдопо - добия: log p(x | θ) – M; (8.48) это делается путем добавления штрафного члена, позволяющего смягчить пере - обучение сравнительно сложных моделей с большим количеством параметров. 1 В параметрических моделях количество параметров часто связано со сложностью класса моделей.\n--- Страница 365 ---\n8.6. Выбор модели 365 Здесь M — количество параметров модели. Критерий AIC оценивает относи - тельную информацию, теряемую заданной моделью. Байесовский информационный критерий (Bayesian information criterion, BIC) (Schwarz, 1978) (8.49) может использоваться для экспоненциальных распределений семейств. Здесь N — это количество точек данных, а M — число параметров. BIC штрафует слож - ность моделей активнее, чем AIC.\n--- Страница 366 ---\n9 Линейная регрессия В данной главе мы применим математические понятия, изученные в главах 2 и 5–7, к решению задачи линейной регрессии (являющейся частным случаем задачи аппроксимации кривой). Цель задачи регрессии — найти функцию f, сопоставляющую входным точкам x ∈ D значения f(x) ∈ . Предполагается, что нам дан обучающий набор входных значений xn и соответствующих зашум - ленных наблюдений yn = f(xn) + ε, где ε — случайная величина, описывающая возникающий при наблюдениях/измерениях шум, а также какие-то не учтенные в модели процессы (мы не будем их рассматривать здесь). В данной главе мы предполагаем, что шум имеет гауссово распределение с нулевым средним. Нашей задачей будет найти функцию, которая не только хорошо моделирует обучающий набор данных, но и успешно предсказывает значения функции, когда входная точка не принадлежит обучающей выборке (см. главу 8). Пример задачи регрес - сии показан на рис. 9.1. (a) /o.g0081/exclam.g00AF/percent.g00BA/K.g00AD/.notdef.g00E3/yright/.notdef.g00E4/equal.g00C8 /exclam.g00AF/yright/.notdef.g0104/exclam.g00AF/yright““/comma.g00D2/comma.g00D2: …/equal.g00C8/K.g00AD/.notdef.g00E3/.notdef.g010A/.notdef.g0105/equal.g00C8/yright/.notdef.g00E4/slash.g00A9/yright ƒ/equal.g00C8/.notdef.g0108/three.g0082/.notdef.g00E4/.notdef.g00E3/yright……/slash.g00A9/yright ƒ…/equal.g00C8/.notdef.g0107/yright…/comma.g00D2/space.g00AB /hyphen.g00C1/three.g0082…/asterisk.g007D/.notdef.g0106/comma.g00D2/comma.g00D2, /comma.g00D2ƒ /asterisk.g007D/percent.g00BA/two.g0088/percent.g00BA/exclam.g00AF/slash.g00A9/period.g00B2 /.notdef.g00E4/slash.g00A9 /period.g00B2/percent.g00BA/two.g0088/comma.g00D2/.notdef.g00E4 /quotedbl.g006D/slash.g00A9/quotedbl.g006D/yright“/two.g0088/comma.g00D2 /K.g00AD/equal.g00C8ƒ/percent.g00BA/quotedbl.g006D/three.g0082/.notdef.g010A /hyphen.g00C1/three.g0082…/asterisk.g007D/.notdef.g0106/comma.g00D2/.notdef.g010A, /C.g00B9/percent.g00BA/exclam.g00AF/percent.g00BA/.notdef.g0105/comma.g00D2/quotedbl.g006D/.notdef.g0108/three.g0082/.notdef.g010A /.notdef.g0105/equal.g00C8……/slash.g00A9/yright(b) /p.g0063/yright/.notdef.g0108/yright…/comma.g00D2/yright /exclam.g00AF/yright/.notdef.g0104/exclam.g00AF/yright““/comma.g00D2/comma.g00D2: /hyphen.g00C1/three.g0082…/asterisk.g007D/.notdef.g0106/comma.g00D2/space.g00AB, /asterisk.g007D/percent.g00BA/two.g0088/percent.g00BA/exclam.g00AF/equal.g00C8/space.g00AB /.notdef.g00E4/percent.g00BA/.notdef.g0104/.notdef.g00E3/equal.g00C8 /K.g00AD/slash.g00A9 “/.notdef.g0104/yright…/yright/exclam.g00AF/comma.g00D2/exclam.g00AF/percent.g00BA/quotedbl.g006D/equal.g00C8/two.g0088/.notdef.g0109 /.notdef.g0105/equal.g00C8……/slash.g00A9/yright (/two.g0088/yright/.notdef.g00E4…/equal.g00C8/space.g00AB /.notdef.g00E3/comma.g00D2…/comma.g00D2/space.g00AB), “/greater.g00C4/three.g0082/asterisk.g007D/equal.g00C8ƒ/equal.g00C8…/comma.g00D2/yright/.notdef.g00E4 /.notdef.g0108/three.g0082/.notdef.g00E4/equal.g00C8 /comma.g00D2ƒ/.notdef.g00E4/yright/exclam.g00AF/yright…/comma.g00D2/space.g00AB /hyphen.g00C1/three.g0082…/asterisk.g007D/.notdef.g0106/comma.g00D2/comma.g00D2 …/equal.g00C8 “/percent.g00BA/percent.g00BA/two.g0088/quotedbl.g006D/yright/two.g0088“/two.g0088/quotedbl.g006D/three.g0082/.notdef.g010A/question.g009D/comma.g00D2/period.g00B2 /quotedbl.g006D/period.g00B2/percent.g00BA/.notdef.g0105…/slash.g00A9/period.g00B2 /two.g0088/percent.g00BA/.notdef.g0107/asterisk.g007D/equal.g00C8/period.g00B2 (“/quotedbl.g006D/yright/two.g0088/.notdef.g00E3/slash.g00A9/yright /.notdef.g00E3/comma.g00D2…/comma.g00D2/comma.g00D2)xy /dollar.g00DC4 /dollar.g00DC2 2 04 x/dollar.g00DC4 /dollar.g00DC2 2 04/dollar.g00DC0,4/dollar.g00DC0,20,2 0,00,4 y /dollar.g00DC0,4/dollar.g00DC0,20,2 0,00,4 Рис. 9.1. (a) Набор данных; ( b) возможное решение задачи регрессии\n--- Страница 367 ---\n367 Линейная регрессия Типичная постановка задачи регрессии дана на рис. 9.2(a): для некоторых вход - ных точек xn мы наблюдаем (зашумленные) значения yn = f(xn) + ε. Задача — вос - становить функцию f, породившую данные и хорошо обобщающуюся на значе - ния функции на новых входных данных. Возможное решение дано на рис. 9.2( b), где также показаны три распределения со средними в значениях f(x), представ - ляющие шум. (a) Примеры функций, описываемых линейной моделью (9.4)(b) Обучающая выборка (c) Оценка максимальног о правдоподобияxy –2020 0 y –10 –1010 100 0 x–10 –5 51 0 0 x–10 –5 51 0 0y –1010 0 Рис. 9.2. Пример линейной регрессии. ( a) Примеры линейных функций; (b) обучающая выборка; ( c) оценка максимального правдоподобия Регрессия играет важнейшую роль в машинном обучении, задачи регрессии появляются в самых разных теоретических и прикладных областях, таких как анализ временных рядов (например, системы распознавания), управление и ро- бототехника (например, обучение с подкреплением), оптимизация (например, линейный поиск, глобальная оптимизация) и глубокое обучение (например, компьютерные игры, распознавание речи и изображений, автоматическая ан - нотация видео). Регрессия также является важным шагом в некоторых алгорит - мах классификации. Нахождение функции регрессии требует решения многих задач, в частности: zВыбор модели (типа функции регрессии) и ее параметризации. Какой класс функций (например, многочлены) подойдет для построения модели при имеющихся данных, и какие конкретные параметры (например, степень многочлена) выбрать? При выборе модели, как мы уже обсуждали в разде - ле 8.6, можно сравнить разные варианты и выбрать самый простой и доста - точно хорошо объясняющий обучающие данные1. zНахождение подходящих параметров. Как, выбрав тип функции регрессии, найти хорошие параметры модели? Необходимо рассмотреть разные целевые функции / функции потерь (определяющие «хорошие» параметры) и алго- ритмы оптимизации, используемые для минимизации потерь. 1 Обычно при выборе модели задают и тип шума, но мы в этой главе считаем шум гаус - совым.\n--- Страница 368 ---\n368",
      "debug": {
        "start_page": 320,
        "end_page": 368
      }
    },
    {
      "name": "Глава 9. Линейная регрессия 366",
      "content": "--- Страница 368 --- (продолжение)\nГлава 9. Линейная регрессия zПереобучение и выбор модели (model selection)1. Переобучением называ - ется ситуация, когда функция регрессии «слишком хорошо» запоминает обучающие данные, но не обобщается на незнакомые тестовые данные. Пере - обучение обычно происходит, если выбранная модель (или ее параметры) обладает излишней гибкостью и выразительностью (раздел 8.6). Мы рас - смотрим причины этого и обсудим способы уменьшить эффект переобучения для линейной регрессии. zСвязь между функцией потерь и априорными предположениями о пара- метрах модели. Выбор функции потерь часто обусловлен некоторой веро - ятностной моделью. Мы рассмотрим, как функция потерь связана с нашими исходными предположениями. zzМоделирование неопределенности. В любой прикладной задаче нам до - ступен лишь ограниченный (хотя потенциально он может быть большим) набор обучающих данных для выбора модели и ее параметров. Если этот ограниченный набор не покрывает все возможные ситуации, мы можем за - даться вопросом об описании неопределенности параметров (как меры уверенности в предсказаниях модели на тестовом множестве). Чем меньше обучающая выборка, тем важнее учитывать неопределенность. При правиль - ном моделировании неопределенности предсказания модели снабжаются доверительными границами. В данной главе мы применим математический инструментарий глав 3 и 5–7 к решению задач линейной регрессии. Мы обсудим использование оценок мак - симального правдоподобия (MLE) и апостериорного максимума (MAP) для нахождения оптимальных параметров модели. Используя эти оценки параметров, мы вкратце рассмотрим ошибки обобщения и переобучение. Ближе к концу главы мы обучим байесовскую линейную регрессию, которая позволяет нам рассуждать о параметрах модели на более высоком уровне, тем самым устраняя некоторые проблемы оценки методом максимального правдоподобия и апосте - риорного максимума. 9.1. ПОСТАНОВКА ЗАДАЧИ Из-за наличия в наблюдениях шума нам придется обратиться к вероятностному подходу и явно смоделировать шум с помощью функции правдоподобия. В част- ности, в этой главе мы рассмотрим задачу регрессии с функцией правдоподобия p(y | x) =  (y | f(x), σ2). (9.1) 1 Это не то же самое, что «выбор модели» из первого пункта. Model selection — отдель - ная задача выбора модели из набора «моделей-кандидатов». — Примеч. ред.\nГлава 9. Линейная регрессия zПереобучение и выбор модели (model selection)1. Переобучением называ - ется ситуация, когда функция регрессии «слишком хорошо» запоминает обучающие данные, но не обобщается на незнакомые тестовые данные. Пере - обучение обычно происходит, если выбранная модель (или ее параметры) обладает излишней гибкостью и выразительностью (раздел 8.6). Мы рас - смотрим причины этого и обсудим способы уменьшить эффект переобучения для линейной регрессии. zСвязь между функцией потерь и априорными предположениями о пара- метрах модели. Выбор функции потерь часто обусловлен некоторой веро - ятностной моделью. Мы рассмотрим, как функция потерь связана с нашими исходными предположениями. zzМоделирование неопределенности. В любой прикладной задаче нам до - ступен лишь ограниченный (хотя потенциально он может быть большим) набор обучающих данных для выбора модели и ее параметров. Если этот ограниченный набор не покрывает все возможные ситуации, мы можем за - даться вопросом об описании неопределенности параметров (как меры уверенности в предсказаниях модели на тестовом множестве). Чем меньше обучающая выборка, тем важнее учитывать неопределенность. При правиль - ном моделировании неопределенности предсказания модели снабжаются доверительными границами. В данной главе мы применим математический инструментарий глав 3 и 5–7 к решению задач линейной регрессии. Мы обсудим использование оценок мак - симального правдоподобия (MLE) и апостериорного максимума (MAP) для нахождения оптимальных параметров модели. Используя эти оценки параметров, мы вкратце рассмотрим ошибки обобщения и переобучение. Ближе к концу главы мы обучим байесовскую линейную регрессию, которая позволяет нам рассуждать о параметрах модели на более высоком уровне, тем самым устраняя некоторые проблемы оценки методом максимального правдоподобия и апосте - риорного максимума. 9.1. ПОСТАНОВКА ЗАДАЧИ Из-за наличия в наблюдениях шума нам придется обратиться к вероятностному подходу и явно смоделировать шум с помощью функции правдоподобия. В част- ности, в этой главе мы рассмотрим задачу регрессии с функцией правдоподобия p(y | x) =  (y | f(x), σ2). (9.1) 1 Это не то же самое, что «выбор модели» из первого пункта. Model selection — отдель - ная задача выбора модели из набора «моделей-кандидатов». — Примеч. ред.\n--- Страница 369 ---\n9.1. Постановка задачи 369 Здесь x ∈ D — входные данные, а y ∈  — зашумленные значения функции. Как было сказано в 9.1, зависимость между x и y задается формулой y = f(x) + ε, (9.2) где ε ∼ (0, σ2) — независимый одинаково распределенный гауссов шум из - мерений со средним 0 и дисперсией σ2. Наша цель — найти функцию, которая близка к неизвестной функции f, породившей наши данные, и легко обобща - ется. В данной главе мы сосредоточимся на параметрических моделях, то есть вы - берем параметризованную функцию и найдем параметры θ, которые хорошо подходят для моделирования данных. Пока предположим, что дисперсия шума σ2 известна, и сконцентрируемся на нахождении параметров модели θ. Линейная регрессия представляет собой частный случай, когда параметры θ входят в фор- мулу линейно. Примером линейной регрессии может служить p (y | x, θ) =  (y | xTθ, σ2) ⇔ (9.3) (9.4) где θ ∈ D — искомые параметры. Класс функций, заданных формулой (9.4), — это прямые, проходящие через начало координат. В формуле (9.4) мы берем параметризацию f (x) = xTθ. Правдоподобие в (9.3) — это плотность распределения y при условии xTθ. За- метим, что единственным источником неопределенности будет шум в наблю - дениях (так как в (9.3) предполагается, что x и θ известны). Без шума зависимость между x и y была бы детерминированной, и формула (9.3) задавала бы дельта- функцию Дирака1. Пример 9.1 При x, θ ∈  линейная регрессионная модель (9.4) описывает прямые (линейные функции), а параметр θ равен тангенсу угла наклона прямой. На рис. 9.2( a) показаны примеры функций, соответствующих различным значениям θ. 1 Дельта-функция Дирака равна нулю всюду, кроме одной точки. Ее интеграл равен нулю. Можно считать дельта-функцию гауссовой (предельный случай при стремя - щейся к нулю дисперсии).\n--- Страница 370 ---\n370 Глава 9. Линейная регрессия Линейная регрессионная модель, заданная (9.3) и (9.4), линейна не только по параметрам1, но и по входным значениям x. На рис. 9.2( a) показаны примеры таких функций. Далее мы поймем, что y = ϕT(x)θ для нелинейного преобразо - вания ϕ — на самом деле также линейная регрессионная модель, так как это понятие относится к моделям, линейным по параметрам, то есть задающимся линейными комбинациями входных признаков. Здесь роль признаков играют преобразования ϕ(x) входных значений x. В дальнейшем мы более подробно обсудим, как находить подходящие парамет- ры θ и оценивать, насколько хорошо они подобраны. Пока мы будем считать, что дисперсия шума σ2 известна. 9.2. ОЦЕНКА ПАРАМЕТРОВ Пусть нам поставлена задача линейной регрессии (9.4) и дана обучающая вы ­ борка  := {(x1, y1), , (xN, yN)}, состоящая из N входных значений xn ∈ D и со- ответствующих наблюдений yn ∈ , n = 1, , N. Графическая модель показана на рис. 9.3. Заметим, что yi и yj условно независимы относительно xi, xj, так что правдоподобие выражается как (9.5a) (9.5b) где  := {x1, , xN} и  := {y1, , yN} — множество входных значений обучающего набора и соответствующих наблюдений. Правдоподобие, как и каждый из со - множителей p(yn | xn, θ), из-за распределения шума будет гауссовым, см. (9.3). θ σ n = 1, . . . , Nxnyn Рис. 9.3. Графическая вероятностная модель для линейной регрессии. Наблюдаемые значения закрашены, детерминированные/известные величины не обведены кружком Далее мы обсудим нахождение оптимальных параметров θ* ∈ D для линейной регрессионной модели (9.4). Найдя параметры θ*, мы можем подставить эту 1 Моделями линейной регрессии называют модели, линейные по параметрам.\n--- Страница 371 ---\n9.2. Оценка параметров 371 оценку в (9.4) и предсказывать значения функции, так что на входном тестовом значении x* распределением y* будет (9.6) Далее мы рассмотрим оценку параметров методом максимального правдоподо - бия, эту тему мы уже немного затрагивали в разделе 8.3. 9.2.1. Оценка максимального правдоподобия При нахождении оптимальных параметров θML часто используют метод макси ­ мального правдоподобия , при котором выбираются такие θML, которые максими - зируют правдоподобие (9.5 b). Можно понимать это как максимизацию вероят - ности обучающих данных при заданных параметрах модели. Параметры максимального правдоподобия ищутся как (9.7) ПРИМЕЧАНИЕ Правдоподобие p(y | x, θ) не является распределением θ. Это функция от θ, интеграл от которой не равен 1 (то есть не нормализованная) и может вообще не существовать. Однако правдоподобие (9.7) является норма - лизованным распределением y.  Чтобы найти искомые θML, максимизирующие правдоподобие, обычно приме - няют градиентный подъем (градиентный спуск относительно взятого с противо - положным знаком правдоподобия). В рассматриваемом нами здесь случае ли - нейной регрессии, однако, существует решение в явном виде, так что градиентный спуск не нужен. На практике вместо максимизации самого прав - доподобия мы берем его логарифм1 с противоположным знаком и ищем мини - мум получившейся функции. ПРИМЕЧАНИЕ Поскольку правдоподобие в (9.5b) является произведением N гауссовых распределений, логарифмическое преобразование оказывается полезным, так как ( a) не вызывает проблем потери значимости и (б) упрощает дифференцирование. Потерей значимости называется ситуация, возникающая, например, при пере - множении N вероятностей, где N — число точек с данными, поскольку невоз - можно представить очень маленькие числа, вроде 10–256. Логарифмическое преобразование также превращает произведение в сумму логарифмов вероят - 1 Так как логарифм — монотонная строго возрастающая функция, оптимум f и оптимум log f достигаются в одной точке.\n--- Страница 372 ---\n372 Глава 9. Линейная регрессия ностей, так что градиент произведения N сомножителей раскладывается в сум- му градиентов, и нет необходимости много раз применять правило произведе - ния (5.46).  Чтобы найти оптимальные параметры θML нашей задачи линейной регрессии, мы максимизируем взятый с противоположным знаком логарифм правдопо- добия1 (9.8) где воспользуемся тем, что правдоподобие (9.5 b) благодаря независимости точек обучающей выборки раскладывается в произведение. В модели линейной регрессии (9.4) правдоподобие будет гауссовым (из-за до - бавления гауссового шума), так что мы получаем (9.9) где константа содержит все слагаемые, не зависящие от θ. Подставив (9.9) в (9.8), получаем (проигнорировав константные слагаемые) (9.10 a) (9.10 b) где X := [x1, , xN]T ∈ N×D — матрица признаков для обучающего набора, а y := [y1, , yN]T ∈ N — вектор целевых параметров для обучающей выборки. Заметим, что n-я строка в матрице признаков X соответствует входной точке xn. В (9.10 b) мы пользовались тем, что сумма квадратов ошибок2 yn и соответствующих пред - сказаний модели равна квадрату расстояния между y и Xθ. Формула (9.10 b) задает в явном виде функцию, которую мы хотим оптимизи - ровать. Относительно θ (9.10 b) квадратична. Это означает, что у  существует единственный глобальный минимум θML. Мы можем найти его, взяв градиент , приравняв его нулю и решив полученное уравнение относительно θ. 1 Отрицательное логарифмическое правдоподобие также называют функцией ошибок. 2 Квадрат ошибки часто используется для измерения расстояния. В разделе 3.1 мы об - суждали, что || x ||2 = xTx при выборе стандартного скалярного произведения.\n--- Страница 373 ---\n9.2. Оценка параметров 373 Используя результаты главы 5, найдем градиент  по параметрам: (9.11 a) (9.11 b) (9.11 c) Оценку максимального правдоподобия θML ищем как решение уравнения d/dθ = 0T (необходимое условие экстремума): (9.12 a) (9.12 b) (9.12 c) Первое уравнение можно домножать справа на ( XTX)–1, поскольку при rk( X) = D (где rk( X) — ранг X) XTX положительно определена1. ПРИМЕЧАНИЕ Равенство градиента нулю является необходимым и доста - точным условием глобального минимума, поскольку матрица Гессе = XTX ∈ D×D положительно определена.  ПРИМЕЧАНИЕ Поиск параметров максимального правдоподобия из (9.12 c) требует решения системы линейных уравнений Aθ = b, где A = XTX и b = XTy.  Пример 9.2 (проведение прямой) Обратимся к рис. 9.2. Мы хотим методом максимального правдоподобия по имеющимся данным построить прямую f(x) = θx, где θ — неизвестный нам тангенс угла наклона. Примеры функций из выбранного нами класса моделей (прямые) показаны на рис. 9.2( a). По данным с рис. 9.2( b) мы находим оценку максимального правдоподобия для параметра θ с помо - щью (9.12 c). Линейная функция максимального правдоподобия изобра - жена на рис. 9.2( c). 1 Мы считаем, что данные не повторяются и rk(X) = D при N ≥ D, то есть параметров не больше, чем точек с данными.\n--- Страница 374 ---\n374 Глава 9. Линейная регрессия Оценка максимального правдоподобия с преобразованием признаков До сих пор мы рассматривали линейную регрессию, описанную в разделе 9.4, которая позволяла нам с помощью оценки максимального правдоподобия про - водить прямую. Однако в случае более интересных данных оказывается, что прямые не очень выразительны. К счастью, линейная регрессия позволяет нам находить и нелинейные функции. Понятие «линейность» относится лишь к ли- нейной зависимости параметров, тогда как над входными значениями x можно делать произвольные нелинейные преобразования φ (x), а затем строить из них линейную комбинацию. Соответствующая модель выглядит так: (9.13) где φ : D→K — нелинейное преобразование входных значений x, а φk : D→K — k-я компонента вектора преобразования φ. Заметим, что параметры модели θ по-прежнему входят в выражение линейно. Пример 9.3 (полиномиальная регрессия) Нас интересует задача регрессии y = φT(x)θ + ε, где x ∈  и θ ∈ K. В этой ситуации часто используют преобразование (9.14) Таким образом, мы «поднимаем» входы из исходного одномерного про - странства в K-мерное пространство признаков с базисом из всех одно- членов xk для k = 0, , K – 1. Теперь мы, оставаясь в рамках метода линей - ной регрессии, можем брать в качестве модели любые многочлены степени не выше K – 1. Многочлен степени K – 1 выглядит как (9.15) где φ определена в (9.14) и θ = [θ0, , θK–1]T ∈ K — вектор параметров θk.\n--- Страница 375 ---\n9.2. Оценка параметров 375 Рассмотрим теперь оценку максимального правдоподобия для параметров θ в модели линейной регрессии (9.13). Пусть обучающая выборка состоит из входных значений xn ∈ D и yn ∈ , n = 1, , N. Определим матрицу признаков как (9.16) где Φij = φj (xi) и φj : D→ . Пример 9.4 (матрица признаков для многочленов степени 2) Для многочленов степени 2 и обучающей выборки размера N, состоящей из точек xn ∈ , n = 1, , N, матрицей признаков будет (9.17) Для матрицы признаков Φ из (9.16) взятый с противоположным знаком лога - рифм правдоподобия (9.13) можно записать как (9.18) Сравнив формулу (9.18) с (9.10 b), где не происходило преобразования при - знаков, мы видим просто замену X на Φ. Так как и X, и Φ не зависят от пара - метров θ, по которым мы оптимизируем, мы получаем оценку максимального правдоподобия θML = (ΦTΦ)–1ΦTy (9.19) для задачи линейной регрессии с преобразованием признаков (9.13). ПРИМЕЧАНИЕ В отсутствие преобразования признаков мы требовали, что - бы XTX была обратимой, то есть чтобы строки X были линейно независимы. В (9.19) мы требуем обратимости ΦTΦ ∈ K×K, что выполняется тогда и только тогда, когда rk( Φ) = K. \n--- Страница 376 ---\n376 Глава 9. Линейная регрессия Пример 9.5 (многочлен максимального правдоподобия) Рассмотрим набор данных с рис. 9.4( a). Он состоит из N = 10 пар ( xn, yn), где xn ∼ [−5, 5] и yn = − sin(xn/5) + cos( xn) + ε, где ε ∼ (0, 0,22). Мы находим подходящий многочлен степени 4, используя метод макси - мального правдоподобия, то есть параметры θML задаются формулой (9.19). Оценка максимального правдоподобия дает нам значения функции ϕT(x*)θML на тестовых данных x*. Результат показан на рис. 9.4( b). 00 22 –2–2 44 –4–4 x02 –2 4 –4 xy 02 –24 –4y (a) Обучающая выборка для задачи регресии(b) Многочлен, соответствующий оценке максимального правдоподобияОбучающая выборка MLE Рис. 9.4. Полиномиальная регрессия: ( a) обучающая выборка из пар ( xn, yn), n = 1, , 10; ( b) многочлен максимального правдоподобия степени 4 Оценка дисперсии шума До сих пор мы считали, что дисперсия шума σ2 известна. Однако мы можем применить метод максимального правдоподобия и для оценки этой дисперсии. Для этого мы последуем стандартной процедуре: запишем логарифм правдопо - добия, найдем его производную по σ2 > 0, приравняем ее к нулю и решим полу - ченное уравнение. Логарифм правдоподобия выглядит как (9.20 a) (9.20 b) (9.20 c)\n--- Страница 377 ---\n9.2. Оценка параметров 377 Частная производная логарифма правдоподобия по σ2 равна (9.21 a) , (9.21 b) так что мы получаем (9.22) Таким образом, оценка максимального правдоподобия для дисперсии шума — это эмпирическое среднее квадратов расстояний между значениями функции без учета шума φT(xn)θ и соответствующими зашумленными наблюдениями yn на входных точках xn. 9.2.2. Переобучение при линейной регрессии Мы только что обсуждали, как использовать оценку максимального правдопо - добия для подгонки линейной модели (например, многочленов) к данным. Качество модели можно оценить, вычислив функцию ошибок (потерь). Один из способов это сделать — взять логарифм правдоподобия с противоположным знаком (9.10 b) и, минимизируя его, найти оценку максимального правдоподобия. Другой способ работает, если σ2 не является параметром модели. В этом случае можно проигнорировать множитель 1/ σ2 и прийти к квадратичной функции потерь || y − Φ ||2. Вместо нее часто используют среднеквадратичное отклонение (9.23) которое ( a) позволяет сравнивать ошибки на наборах данных разной величины и (б) имеет тот же масштаб и те же единицы измерения, что и наблюдаемые значения функции yn1. Например, если мы обучаем модель, сопоставляющую географические координаты ( x — широта и долгота) с ценой домов (значения y даны в евро), среднеквадратичная ошибка ( RMSE ) также измеряется в евро, тогда как квадрат ошибки — как «евро в квадрате». Если мы вернем множитель σ2 из исходной формулы (9.10 b), то получим безразмерную целевую функцию, то есть больше не имеющую единицы измерения. 1 Среднеквадратичное отклонение нормализовано.\n--- Страница 378 ---\n378 Глава 9. Линейная регрессия При выборе модели (см. раздел 8.6) можно использовать среднеквадратичную ошибку (или логарифм правдоподобия с противоположным знаком1), чтобы определить оптимальную степень многочлена, то есть такую степень M, которая минимизирует целевую функцию. Так как степень многочлена — число нату - ральное, можно действовать перебором, проверяя все разумные значения M. Когда обучающая выборка имеет размер N, достаточно проверить значения M от 0 до N − 1 включительно. При M < N оценка максимального правдоподобия будет единственной. При M ≥ N параметров больше, чем точек с данными, так что приходится решать неопределенную систему линейных уравнений ( ΦTΦ из (9.19) больше не будет обратимой), и возможных оценок максимального прав - доподобия будет бесконечно много. На рис. 9.5 показан ряд попыток обучения многочленов, проведенных по на - бору данных размера N = 10 с рис. 9.4(a) с помощью метода максимального правдоподобия. Заметим, что многочлены маленькой степени (например, кон - станты ( M = 0) или линейные функции ( M = 1)) плохо подгоняются к данным и, таким образом, плохо приближают реальную породившую их функцию. Для степеней M = 3, , 5 мы видим неплохое гладкое приближение. Переходя к мно- гочленам более высоких степеней, мы видим, что они больше и больше при - ближаются к данным. В предельном случае M = N − 1 = 9 график проходит через (a) M = 002 –2 4 –4 x (b) M = 102 –2 4 –4 x (c) M = 302 –2 4 –4 x (d) M = 402 –2 4 –4 x (e) M = 602 –2 4 –4 x (f) M = 902 –2 4 –4 x02 –24 –4y 02 –24 –4y02 –24 –4y 02 –24 –4y02 –24 –4y 02 –24 –4yОбучающая выборка MLEОбучающая выборка MLEОбучающая выборка MLE Обучающая выборка MLEОбучающая выборка MLEОбучающая выборка MLE Рис. 9.5. Решения максимального правдоподобия для различных степеней многочлена M 1 Отрицательный логарифм правдоподобия не имеет единицы измерения.\n--- Страница 379 ---\n9.2. Оценка параметров 379 все заданные точки1. Однако многочлены высокой степени сильно колеблются и поэтому плохо представляют функцию, породившую данные, то есть мы име - ем дело с переобучением. Вспомним, что нашей целью является обобщение закономерностей и предска - зание значений функции для новых (неизвестных) данных. То, как способность к обобщению зависит от степени многочлена M, можно увидеть, рассмотрев отдельную тестовую выборку из 200 точек, порожденных той же процедурой, что и обучающая выборка. В качестве тестовых входных данных мы возьмем 200 точек на интервале [ −5, 5], расположенных на одинаковых расстояниях друг от друга. Для каждого M мы вычисляем среднеквадратичную ошибку (9.23) на обучающей и тестовой выборках. Посмотрев на величину ошибки на тестовой выборке, которая показывает обоб - щающую способность соответствующего многочлена, мы замечаем, что сначала при увеличении степени многочлена ошибка уменьшается (рис. 9.6, светлая линия). Для многочленов четвертой степени ошибка на тестовой выборке не - большая и остается примерно такой же для пятой степени. Однако начиная с шестой степени и далее, ошибка на тестовой выборке значительно возрастает, что говорит об очень плохой обобщающей способности многочленов высоких степеней. В нашем примере это видно из графиков многочленов максимально - го правдоподобия с рис. 9.5. Заметим, что ошибка на обучающей выборке (чер - ная линия на рис. 9.6) не может увеличиться при увеличении степени много члена. В нашем примере наилучшая обобщающая способность достигается при степе - ни многочлена M = 4 (когда ошибка на тестовой выборке минимальна). /q.g0076/two.g0088/yright/C.g00B9/yright…/.notdef.g0109 /.notdef.g00E4…/percent.g00BA/.notdef.g0104/percent.g00BA/.notdef.g0107/.notdef.g00E3/yright…/equal.g00C8RMSE/n.g007C/K.g00AD/three.g0082/.notdef.g0107/equal.g00C8/.notdef.g010A/question.g009D/equal.g00C8/space.g00AB Š/yright“/two.g0088/percent.g00BA/quotedbl.g006D/equal.g00C8/space.g00AB 00 1010 22 44 66 88 Рис. 9.6. Ошибки на обучающей и тестовой выборке 1 Случай M = N − 1 является предельным в том смысле, что иначе пространство решений соответствующей однородной системы линейных уравнений будет ненулевым и за- дача линейной регрессии будет иметь бесконечно много оптимальных решений.\n--- Страница 380 ---\n380 Глава 9. Линейная регрессия 9.2.3. Оценка апостериорного максимума Мы только что увидели, что метод максимального правдоподобия подвержен переобучению. Часто переобучение проявляется в увеличении абсолютных значений параметров (Bishop, 2006). Чтобы уменьшить этот эффект, можно установить априорное распределение вероятностей p(θ) для параметров. Это априорное распределение явно задает, какие значения параметров мы считаем разумными еще до знакомства с данными. Например, априорное гауссово рас - пределение p(θ) = (0, 1) для единственного параметра θ означает, что мы ожидаем значения параметра из интервала [ −2, 2] (два стандартных отклонения от среднего). Когда у нас появляется набор данных , , вместо максимизации правдоподобия мы максимизируем плотность апостериорной вероятности p(θ | , ). Этот метод называется оценкой апостериорного максимума (MAP). Плотность апостериорной вероятности значений θ при обучающей выборке ,  мы найдем по теореме Байеса (раздел 6.3): (9.24) Так как апостериорное распределение зависит от априорного распределения p(θ), априорное распределение влияет на то, какое значение параметра соот - ветствует максимуму апостериорного распределения. Мы продемонстрируем это чуть позже. Вектор параметров θMAP, максимизирующий апостериорную вероятность (9.24), будет оценкой апостериорного максимума. Последовательность действий при нахождении MAP-оценки похожа на алгоритм, используемый в методе максимального правдоподобия. Начнем с взятия лога - рифма. Логарифм плотности апостериорной вероятности выглядит как log p(θ | , ) = log p( | , θ) + log p(θ) + const, (9.25) где константа объединяет все слагаемые, которые не зависят от θ. Видно, что логарифм плотности апостериорной вероятности (9.25) является суммой лога - рифма правдоподобия p( | , θ) и логарифма плотности априорной вероят - ности log p(θ), так что MAP-оценка будет своего рода «компромиссом» между нашими априорными предположениями о разумных значениях параметров (до знакомства с данными) и правдоподобием (зависящим от данных). Чтобы найти MAP-оценку θMAP, мы минимизируем взятый с противоположным знаком логарифм плотности апостериорного распределения (относительно θ), то есть находим (9.26)\n--- Страница 381 ---\n9.2. Оценка параметров 381 Градиент логарифма плотности апостериорного распределения по θ равен , (9.27) где первое слагаемое в правой части представляет собой градиент взятого с противоположным знаком правдоподобия (9.11 c). Используя сопряженное априорное гауссово распределение p(θ) = (0, b2I) для параметров θ из взятого с противоположным знаком логарифма плотности апостериорного распределения (9.13), получим (9.28) Здесь первое слагаемое соответствует вкладу логарифма правдоподобия, а вто- рое — логарифма априорного распределения. Градиент логарифма плотности апостериорного распределения по параметрам θ тогда равен (9.29) Найдем MAP-оценку θMAP, приравнивая градиент к 0T и решая уравнение. Полу - чаем (9.30 a) (9.30 b) (9.30 c) (9.30 d) и транспонируем обе части последнего равенства, так что MAP-оценка равна (9.31) Сравнивая MAP-оценку (9.31) с оценкой максимального правдоподобия (9.19), мы видим, что разница между ними — дополнительное слагаемое ( σ2 / b2)I в об- ратной матрице. Оно гарантирует, что ΦTΦ + (σ2 / b2)I — симметричная и стро-\n--- Страница 382 ---\n382 Глава 9. Линейная регрессия го положительно определенная матрица (так она обратима и MAP-оценка — это единственное решение системы линейных уравнений)1. Своим появлением это слагаемое обязано регуляризации. Пример 9.6 (MAP-оценка для полиномиальной регрессии) В примере полиномиальной регрессии из раздела 9.2.1 мы выбрали апри - орное гауссово распределение p(θ) = (0, I) для параметров θ и нашли MAP-оценку (9.31). На рис. 9.7 показаны оценка максимального правдо - подобия и MAP-оценка для многочленов степени 6 (слева) и степени 8 (справа). Априорное распределение (регуляризатор) не сильно влияет на оценку для многочленов небольшой степени, однако для более высоких степеней предотвращает большие колебания. Хотя MAP-оценка может смягчить проблему переобучения, она, вообще говоря, не решает ее полностью, так что, чтобы справляться с переобучением, нам нужен дру - гой подход. Обучающая выборка (a) Многочлены степени 60 2 –2 4 –4 x02 –2 4 –4 x (b) многочлены степени 8y –4–22 04 y –4–22 04Обучающая выборка MLE MAP MLE MAP Рис. 9.7 . Полиномиальная регрессия: оценка максимального правдоподобия (MLE — maximum likelihood estimate ) и MAP -оценка 9.2.4. MAP -оценивание как регуляризация Для смягчения эффекта переобучения вместо принятия априорного распреде - ления параметров θ можно ввести регуляризацию, штрафуя за слишком большие значения коэффициентов. Мы минимизируем относительно θ (раздел 8.2.3) функцию потерь для метода наименьших квадратов с регуляризацией: (9.32) 1 ΦTΦ — симметричная положительно полуопределенная. Дополнительное слагаемое в (9.31) строго положительно определено, так что существует обратная матрица.\n--- Страница 383 ---\n9.2. Оценка параметров 383 Здесь первое слагаемое, пропорциональное логарифму правдоподобия с проти - воположным знаком, см. (9.10 b), отвечает за соответствие данным. Второе слагаемое называют регуляризатором, и параметр регуляризации λ ≥ 0 задает «жесткость» регуляризации. ПРИМЕЧАНИЕ Вместо евклидовой нормы || ·||2, в (9.32) можно взять произ - вольную p-норму || ·||p. На практике меньшие значения p приводят к более раз - реженным решениям. Здесь разреженность означает, что многие из параметров θd = 0, что удобно для выбора переменных. При p = 1 регуляризатор был пред - ложен Тибширани (Tibshirani, 1996) и назван LASSO (least absolute shrinkage and selection operator — оператор наименьшего абсолютного сжатия и выбора).  Регуляризатор в (9.32) можно интерпретировать как логарифм гауссова априорного распределения (с противоположным знаком), использованный нами ранее при MAP-оценивании, см. (9.26). А именно, из априорного гауссового распределения p(θ) = (0, b2I) получаем , (9.33) так что при λ = 1 / (2 b2) слагаемое регуляризации и логарифм гауссового апри - орного распределения (с противоположным знаком) совпадают. Так как функция потерь для метода наименьших квадратов с регуляризацией (regularized least-squares, RLS) (9.32) состоит из слагаемых, соответствующих логарифмам правдоподобия и априорного распределения, неудивительно, что ее минимизация дает нам решение, похожее на MAP-оценку (9.31), а именно (9.34) совпадающее с MAP-оценкой (9.31) при λ = σ2 / b2, где σ2 — дисперсия шума, а b2 — дисперсия априорного гауссового распределения p(θ) = (0, b2I). К этому моменту мы рассмотрели оценку параметров методами максимального правдоподобия и MAP , в которых находятся точечные оценки1 θ*, оптимизиру - ющие целевую функцию (правдоподобие или апостериорную вероятность). Мы видели, что оба метода могут приводить к переобучению. В следующем разделе мы обсудим байесовскую линейную регрессию, предсказания которой основаны на использовании байесовского инференса для нахождения апосте - риорного распределения неизвестных параметров. А именно, вместо точечной оценки происходит усреднение по всем возможным наборам параметров. 1 Точечная оценка — это одно конкретное значение параметра, а не распределение веро - ятностей возможных значений.\n--- Страница 384 ---\n384 Глава 9. Линейная регрессия 9.3. БАЙЕСОВСКАЯ ЛИНЕЙНАЯ РЕГРЕССИЯ До этого мы, рассматривая модели линейной регрессии, оценивали их парамет- ры θ методом максимального правдоподобия или MAP. Мы увидели, что метод наименьших квадратов может приводить к сильному переобучению, например при маленьком объеме данных. Метод MAP борется с этой про - блемой, вводя априорное распределение параметров, выполняющее роль регуляризатора. Метод байесовской линейной регрессии (Bayesian linear regression, BLR) пред - ставляет собой развитие идеи априорных предположений о параметрах. Он даже не пытается найти точечную оценку параметров при прогнозировании, а рас- сматривает апостериорное распределение параметров целиком. Таким образом, мы не подбираем параметры, а вычисляем среднее по всем их возможным зна - чениям (в соответствии с апостериорным распределением). 9.3.1. Модель В задаче байесовской линейной регрессии модель выглядит как (9.35) при этом мы в явном виде задаем априорное гауссово распределение p(θ) = (m0, S0) на θ, то есть рассматриваем вектор параметров как случайную величину. Это позволяет нам составить графическую модель (рис. 9.8), на которой явно по - казаны параметры априорного гауссового распределения для θ. Полная вероят - ностная модель, то есть совместное распределение наблюдаемых и ненаблюда - емых переменных, y и θ соответственно, имеет вид p(y, θ | x) = p(y | x, θ)p(θ). (9.36) θ σS0m0 y x Рис. 9.8. Графическая модель байесовской линейной регрессии\n--- Страница 385 ---\n9.3. Байесовская линейная регрессия 385 9.3.2. Априорные предсказания На практике нас обычно мало интересуют сами по себе значения параметров θ. Чаще нам важны предсказания, которые можно делать по этим значениям. При байесовской постановке задачи мы, зная распределение параметров, берем среднее по всем возможным значениям параметров. Это позволяет нам сделать прогноз. Чтобы предсказать результат для входного значения x*, мы интегриру - ем по θ и получаем (9.37) что можно понимать как среднее значение предсказаний y* | x*, θ для всех воз - можных параметров θ относительно априорного распределения p(θ). Заметим, что предсказания, использующие априорное распределение, не требуют от нас наличия обучающей выборки, а лишь входную точку x*. В модели (9.35) мы берем сопряженное (гауссово) распределение для θ, так что предсказываемое распределение также является гауссовым и может быть за - писано в явном виде. Для априорного распределения p(θ) = (m0, S0) прогноз будет выглядеть как p(y* | x*) = (ϕT(x*)m0, ϕT(x*)S0ϕ(x*) + σ2), (9.38) где мы использовали (i) то, что предсказанное распределение тоже является гауссовым из-за сопряженности (раздел 6.6) и свойства маргинализации (раз - дел 6.5), (ii) независимость гауссова шума, из которой следует  [y*] = θ[ϕT(x*)θ] + ε[ε], (9.39) и (iii) то, что y* — линейное преобразование θ, так что при вычислении средне - го и ковариации предсказаний мы можем использовать (6.50) и (6.51) соответ - ственно. В (9.38) слагаемое ϕT(x*)S0 ϕ(x*) в дисперсии предсказаний соответ - ствует неопределенности в значениях θ, в то время как σ2 — неопределенность, вызванная наличием шума. Если нас интересуют не зашумленные y*, а значения функции без влияния шума f (x*) = ϕT(x*)θ, получим формулу p (f(x*)) = (ϕT(x*)m0, ϕT(x*)S0ϕ(x*)), (9.40) отличающуюся от (9.38) только отсутствием дисперсии шума σ2 в предсказанной дисперсии.\n--- Страница 386 ---\n386 Глава 9. Линейная регрессия ПРИМЕЧАНИЕ Так как можно представить распределение p(θ) с помощью множества случайных значений θi, каждому из которых соответствует функция , то распределение параметров p(θ) индуцирует распределение p(f (·)) на функциях. Здесь ( ·) — обозначение для функциональной зависимости.  Пример 9.7 (априорное распределение на функциях) Рассмотрим задачу байесовской линейной регрессии для многочленов сте- пени 5. Возьмем априорное распределение параметров p(θ) = (0, (1/4) I). На рис. 9.9 показаны индуцированное им априорное распределение на функциях (темно-серый цвет соответствует уровню доверия 67%, светло- серый — уровню доверия 95%) и некоторая выборка функций. Выборка функций генерируется так: берется вектор значений параметров θi ∼ p(θ) и вычисляются . Преобразование ϕ(·) применялось к 200 входным значениям x* ∈ [−5, 5]. Неопределенность (показанная на рис. 9.9 как закрашенная область) появляется лишь из-за неопределен - ности параметров, поскольку мы рассматриваем предсказания без учета шума (9.40). (b) Примеры функций из этого распределения(a) Априорное распределение на функцияхxy –4 –2 2 0 4 x–4 –2 2 0 4–4–22 04 y –4–22 04 Рис. 9.9. Априорное распределение на функциях. ( a) Распределение на функциях изображено как средняя функция (черная линия) и доверительные интервалы 67 и 95% (закрашены). ( b) Примеры функций из этого распределения, сгенерированные по взятым параметрам из априорного распределения параметров До сих пор мы рассматривали предсказания с использованием априорного рас - пределения параметров p(θ). Однако если мы знаем апостериорное распреде - ление параметров (то есть нам дана обучающая выборка , ), верны те же\n--- Страница 387 ---\n9.3. Байесовская линейная регрессия 387 принципы предсказания и инференса (9.37) — достаточно заменить априорное распределение p(θ) апостериорным p(θ | , ). Далее мы приведем подробный вывод формулы апостериорного распределения. 9.3.3. Апостериорное распределение Пусть нам дана обучающая выборка из точек xn ∈ D и соответствующих на - блюдений yn ∈ , n = 1, , N. Вычислим апостериорное распределение параметров по теореме Байеса: (9.41) где  — множество экземпляров из обучающей выборки, а  — множество со - ответствующих значений. Пусть p( | , θ) — правдоподобие, p(θ) — априорное распределение параметров, а (9.42) — маргинальное правдоподобие, не зависящее от параметров θ и обеспечивающее то, что апостериорное распределение нормализовано (то есть его интеграл равен 1). Можно думать о маргинальном правдоподобии как о среднем правдоподобии по всем возможным параметрам (относительно априорного распределения p(θ)). Теорема 9.1 (апостериорное распределение параметров). В нашей модели (9.35) формула для апостериорного распределения параметров (9.41) будет выглядеть как p (θ | , ) = (θ | mN, SN); (9.43 a) (9.43 b) (9.43 c) где N — размер обучающей выборки. Доказательство . По теореме Байеса, плотность апостериорного распределения p(θ |, ) пропорциональна произведению правдоподобия p( | , θ), и плотность априорного распределения p(θ): ; (9.44 a) ; (9.44 b) (9.44 c)\n--- Страница 388 ---\n388 Глава 9. Линейная регрессия Вместо того чтобы работать с произведением, можно прологарифмировать вы - ражение и найти среднее и дисперсию апостериорного распределения. Сумма логарифмов плотности априорного распределения и правдоподобия равна (9.45 a) , (9.45 b) где константа объединяет все слагаемые, не зависящие от θ. Далее мы не будем писать константу. Раскроем скобки в (9.45 b) и получим (9.46 a) (9.46 b) где константа объединяет слагаемые в (9.46a), не зависящие от θ ( и ). В последней формуле — слагаемые, линейные по θ, а — квадратично зависящие от θ. Присмотревшись к (9.46 b), мы видим, что это уравнение квадратично относительно θ. Так как логарифм ненормализованного апостериорного распределения — квадратичная функция, само это апостериорное распределение будет гауссовым: (9.47 a) (9.47 b) где в последней строке мы воспользовались (9.46 b). Остается привести это ненормализованное гауссово распределение к виду, пропорциональному (θ | mN, SN). Иными словами, нам нужно определить среднее mN и ковариационную матрицу SN. Для этого мы используем дополне - ние до полного квадрата. Логарифм искомого апостериорного распределения равен (9.48 a) (9.48 b)\n--- Страница 389 ---\n9.3. Байесовская линейная регрессия 389 Здесь мы раскрыли скобки в выражении ( θ − mN)T (θ − mN) и получили ква - дратичное по θ слагаемое ( ), линейное по θ слагаемое ( ) и кон- стантное слагаемое ( ). Это позволяет нам определить SN и mN, прирав - нивая друг другу слагаемые одного цвета в (9.46 b) и (9.48 b): (9.49 a) (9.49 b) и (9.50 a) (9.50 b)  ПРИМЕЧАНИЕ Если нам дано выражение xTAx − 2aTx + const1, (9.51) где A — симметричная положительно определенная матрица, и мы хотим при - вести его к виду ( x − μ)TΣ(x − μ) + const2, (9.52) это можно сделать, задав Σ := A, (9.53) μ := Σ–1a (9.54) и const2 = const1 − μTΣμ.  Можно заметить, что выражение под экспонентой в (9.47 b) имеет вид (9.51) с (9.55) (9.56) Так как в выражениях вида (9.46 a) бывает сложно определить A, a, при поиске решения часто помогает привести их к виду (9.51), разделяющему квадратичные, линейные и константные слагаемые. 9.3.4. Апостериорные предсказания В (9.37) было вычислено распределение y* на тестовой выборке x* при априорном распределении параметров p(θ). Предсказание с помощью апостериорного рас -\n--- Страница 390 ---\n390 Глава 9. Линейная регрессия пределения p(θ | , ) в целом мало от него отличается, так как оба распределения гауссовы (хотя и с разными средним и дисперсией). Таким образом, как и в нашем примере из раздела 9.3.2, получим апостериорное предсказанное распределение (9.57 a) (9.57 b) (9.57 с) Слагаемое φT(x*)SNϕ (x*) отвечает за неопределенность параметров θ. Заметим, что SN зависит от обучающей выборки через Φ; см. (9.43 b). Предсказанное среднее φT(x*)mN совпадает с MAP-оценкой. ПРИМЕЧАНИЕ Вместо интеграла в (9.57 a) предсказанное распределение можно записать как θ | ,  [p(y* | x*, θ)], где математическое ожидание берется относительно распределения p(θ | , ). Запись предсказанного апостериорного распределения в таком виде подчерки - вает сходство с маргинальным правдоподобием (9.42). Главные различия между маргинальным правдоподобием и предсказанным апостериорным рас - пределением состоят в том, что (i) о маргинальном правдоподобии можно думать как о способе предсказать результат для обучающей выборки ( y), а не для те - стовой ( y*), и (ii) маргинальное правдоподобие усредняется относительно априорного, а не апостериорного распределения параметров.  ПРИМЕЧАНИЕ Зачастую нам не очень интересно предсказанное распреде - ление p(y* | , , x*) зашумленных наблюдений y*. Вместо этого мы хотим узнать распределение значений функции без учета шума f(x*) = ϕT(x*)θ. Мы вычислим, когда он появляется, пользуясь свойствами среднего и дисперсии: (9.58) (9.59) Видно, что предсказанное среднее будет тем же самым, что и для зашумленных наблюдений (так как среднее значение шума равно 0, а предсказанная дисперсия отличается только на σ2, то есть дисперсию шума). Предсказывая зашумленные значения функции, мы должны учитывать эту неопределенность и добавлять σ2, а для значений без учета шума этого делать не надо — единственным источ - ником неопределенности будут параметры. \n--- Страница 391 ---\n9.3. Байесовская линейная регрессия 391 ПРИМЕЧАНИЕ Интегрирование по параметрам θ задает распределение на функциях. Взяв случайные θi ∼ p(θ | , ) из апостериорного распределения параметров, получим . Средняя функция, то есть все множество ожидаемых значений функции θ[f(·) | θ, , ], для этого распределения задается формулой . Маргинальная дисперсия, то есть дисперсия f(·), вычисляется как ϕ(·)TSNϕ(·).  Пример 9.8 (апостериорное распределение на функциях) Вернемся к задаче байесовской линейной регрессии для многочленов степени 5. Возьмем априорное распределение параметров p(θ) = = (0, (1/4 I). На рис. 9.9 показано априорное распределение на функци - ях, заданное априорным распределением параметров и случайно выбран - ными из этого распределения функциями. На рис. 9.10 показано апостериорное распределение на функциях, полу - ченное методом байесовской линейной регрессии (BLR). Обучающая выборка показана в части ( a); часть ( b) показывает апостериорное рас - пределение на функциях, в том числе функции, которые мы бы получили методами максимального правдоподобия и MAP-оценивания. Функция, полученная методом MAP-оценивания, соответствует апостериорному среднему в задаче байесовской линейной регрессии. Часть ( c) демонстри - рует некоторые функции, взятые из найденного апостериорного распре - деления. (a) /n.g007C/K.g00AD/three.g0082/.notdef.g0107/equal.g00C8/.notdef.g010A/question.g009D/equal.g00C8/space.g00AB /quotedbl.g006D/slash.g00A9/K.g00AD/percent.g00BA/exclam.g00AF/asterisk.g007D/equal.g00C8 (b) /grave.g006B/C.g00B9/percent.g00BA“/two.g0088/yright/exclam.g00AF/comma.g00D2/percent.g00BA/exclam.g00AF…/percent.g00BA/yright /exclam.g00AF/equal.g00C8“/C.g00B9/exclam.g00AF/yright/.notdef.g0105/yright/.notdef.g00E3/yright…/comma.g00D2/yright …/equal.g00C8 /hyphen.g00C1/three.g0082…/asterisk.g007D/.notdef.g0106/comma.g00D2/space.g00AB/period.g00B2, /.notdef.g0105/percent.g00BA/quotedbl.g006D/yright/exclam.g00AF/comma.g00D2/two.g0088/yright/.notdef.g00E3/.notdef.g0109…/slash.g00A9/yright /comma.g00D2…/two.g0088/yright/exclam.g00AF/quotedbl.g006D/equal.g00C8/.notdef.g00E3/slash.g00A9 67 /comma.g00D2 95% (ƒ/equal.g00C8/asterisk.g007D/exclam.g00AF/equal.g00C8/.notdef.g0108/yright…/slash.g00A9), /hyphen.g00C1/three.g0082…/asterisk.g007D/.notdef.g0106/comma.g00D2/comma.g00D2, “/percent.g00BA/percent.g00BA/two.g0088/quotedbl.g006D/yright/two.g0088“/two.g0088/quotedbl.g006D/three.g0082/.notdef.g010A/question.g009D/comma.g00D2/yright /percent.g00BA/.notdef.g0106/yright…/asterisk.g007D/yright /.notdef.g00E4/equal.g00C8/asterisk.g007D“/comma.g00D2/.notdef.g00E4/equal.g00C8/.notdef.g00E3/.notdef.g0109…/percent.g00BA/.notdef.g0104/percent.g00BA /C.g00B9/exclam.g00AF/equal.g00C8/quotedbl.g006D/.notdef.g0105/percent.g00BA/C.g00B9/percent.g00BA/.notdef.g0105/percent.g00BA/K.g00AD/comma.g00D2/space.g00AB (MLE) /comma.g00D2 MAP-/percent.g00BA/.notdef.g0106/yright…/asterisk.g007D/yright (/C.g00B9/percent.g00BA“/.notdef.g00E3/yright/.notdef.g0105…/space.g00AB/space.g00AB “/percent.g00BA/quotedbl.g006D/C.g00B9/equal.g00C8/.notdef.g0105/equal.g00C8/yright/two.g0088 “ /equal.g00C8/C.g00B9/percent.g00BA“/two.g0088/yright/exclam.g00AF/comma.g00D2/percent.g00BA/exclam.g00AF…/slash.g00A9/.notdef.g00E4 “/exclam.g00AF/yright/.notdef.g0105…/comma.g00D2/.notdef.g00E4)(c) /q.g0076/yright/.notdef.g00E4/C.g00B9/.notdef.g00E3/slash.g00A9 /comma.g00D2ƒ /equal.g00C8/C.g00B9/percent.g00BA“/two.g0088/yright/exclam.g00AF/comma.g00D2/percent.g00BA/exclam.g00AF…/percent.g00BA/.notdef.g0104 /percent.g00BA /exclam.g00AF/equal.g00C8“/C.g00B9/exclam.g00AF/yright/.notdef.g0105/yright/.notdef.g00E3/yright…/comma.g00D2/space.g00AB …/equal.g00C8 /hyphen.g00C1/three.g0082…/asterisk.g007D/.notdef.g0106/comma.g00D2/space.g00AB/period.g00B2, /C.g00B9/percent.g00BA/.notdef.g00E3/three.g0082/.notdef.g0107/yright……/slash.g00A9/yright /C.g00B9/percent.g00BA “/yright/.notdef.g00E4/C.g00B9/.notdef.g00E3/equal.g00C8/.notdef.g00E4 /comma.g00D2ƒ /equal.g00C8/C.g00B9/percent.g00BA“/two.g0088/yright/exclam.g00AF/comma.g00D2/percent.g00BA/exclam.g00AF…/percent.g00BA/.notdef.g0104/percent.g00BA /exclam.g00AF/equal.g00C8“/C.g00B9/exclam.g00AF/yright/.notdef.g0105/yright/.notdef.g00E3/yright…/comma.g00D2/space.g00AB /C.g00B9/equal.g00C8/exclam.g00AF/equal.g00C8/.notdef.g00E4/yright/two.g0088/exclam.g00AF/percent.g00BA/quotedbl.g006Dxy /dollar.g00DC4/dollar.g00DC2 204 x/dollar.g00DC4/dollar.g00DC2 204 x/dollar.g00DC4/dollar.g00DC2 204/dollar.g00DC4/dollar.g00DC22 04 y /dollar.g00DC4/dollar.g00DC22 04 y /dollar.g00DC4/dollar.g00DC22 04 /n.g007C/K.g00AD/three.g0082/.notdef.g0107/equal.g00C8/.notdef.g010A/question.g009D/equal.g00C8/space.g00AB /quotedbl.g006D/slash.g00A9/K.g00AD/percent.g00BA/exclam.g00AF/asterisk.g007D/equal.g00C8 MLE MAP BLR Рис. 9.10. Байесовская линейная регрессия и апостериорное распределение на функциях: ( a) обучающая выборка; ( b) апостериорное распределение на функциях; ( c) функции (выборка) из апостериорного распределения\n--- Страница 392 ---\n392 Глава 9. Линейная регрессия (a) /grave.g006B/C.g00B9/percent.g00BA“/two.g0088/yright/exclam.g00AF/comma.g00D2/percent.g00BA/exclam.g00AF…/percent.g00BA/yright /exclam.g00AF/equal.g00C8“/C.g00B9/exclam.g00AF/yright/.notdef.g0105/yright/.notdef.g00E3/yright…/comma.g00D2/yright …/equal.g00C8 /.notdef.g00E4…/percent.g00BA/.notdef.g0104/percent.g00BA/.notdef.g0107/.notdef.g00E3/yright…/equal.g00C8/period.g00B2 “/two.g0088/yright/C.g00B9/yright…/comma.g00D2 M = 3 (“/.notdef.g00E3/yright/quotedbl.g006D/equal.g00C8) /comma.g00D2 “/yright/.notdef.g00E4/C.g00B9/.notdef.g00E3/slash.g00A9 /comma.g00D2ƒ …/yright/.notdef.g0104/percent.g00BA (“/C.g00B9/exclam.g00AF/equal.g00C8/quotedbl.g006D/equal.g00C8) (b) /grave.g006B/C.g00B9/percent.g00BA“/two.g0088/yright/exclam.g00AF/comma.g00D2/percent.g00BA/exclam.g00AF…/percent.g00BA/yright /exclam.g00AF/equal.g00C8“/C.g00B9/exclam.g00AF/yright/.notdef.g0105/yright/.notdef.g00E3/yright…/comma.g00D2/yright …/equal.g00C8 /.notdef.g00E4…/percent.g00BA/.notdef.g0104/percent.g00BA/.notdef.g0107/.notdef.g00E3/yright…/equal.g00C8/period.g00B2 “/two.g0088/yright/C.g00B9/yright…/comma.g00D2 M = 5 (“/.notdef.g00E3/yright/quotedbl.g006D/equal.g00C8) /comma.g00D2 “/yright/.notdef.g00E4/C.g00B9/.notdef.g00E3/slash.g00A9 /comma.g00D2ƒ …/yright/.notdef.g0104/percent.g00BA (“/C.g00B9/exclam.g00AF/equal.g00C8/quotedbl.g006D/equal.g00C8) (c) /grave.g006B/C.g00B9/percent.g00BA“/two.g0088/yright/exclam.g00AF/comma.g00D2/percent.g00BA/exclam.g00AF…/percent.g00BA/yright /exclam.g00AF/equal.g00C8“/C.g00B9/exclam.g00AF/yright/.notdef.g0105/yright/.notdef.g00E3/yright…/comma.g00D2/yright …/equal.g00C8 /.notdef.g00E4…/percent.g00BA/.notdef.g0104/percent.g00BA/.notdef.g0107/.notdef.g00E3/yright…/equal.g00C8/period.g00B2 “/two.g0088/yright/C.g00B9/yright…/comma.g00D2 M = 37 (“/.notdef.g00E3/yright/quotedbl.g006D/equal.g00C8) /comma.g00D2 “/yright/.notdef.g00E4/C.g00B9/.notdef.g00E3/slash.g00A9 /comma.g00D2ƒ …/yright/.notdef.g0104/percent.g00BA (“/C.g00B9/exclam.g00AF/equal.g00C8/quotedbl.g006D/equal.g00C8)x/dollar.g00DC4 /dollar.g00DC2 2 04 x/dollar.g00DC4 /dollar.g00DC2 2 04 x/dollar.g00DC4 /dollar.g00DC2 2 04 x/dollar.g00DC4 /dollar.g00DC2 2 04x/dollar.g00DC4 /dollar.g00DC2 2 04 x/dollar.g00DC4 /dollar.g00DC2 2 04y /dollar.g00DC4/dollar.g00DC22 04 y /dollar.g00DC4/dollar.g00DC22 04y /dollar.g00DC4/dollar.g00DC22 04 y /dollar.g00DC4/dollar.g00DC22 04y /dollar.g00DC4/dollar.g00DC22 04 y /dollar.g00DC4/dollar.g00DC22 04/n.g007C/K.g00AD/three.g0082/.notdef.g0107/equal.g00C8/.notdef.g010A/question.g009D/equal.g00C8/space.g00AB /quotedbl.g006D/slash.g00A9/K.g00AD/percent.g00BA/exclam.g00AF/asterisk.g007D/equal.g00C8 MLE MAP BLR /n.g007C/K.g00AD/three.g0082/.notdef.g0107/equal.g00C8/.notdef.g010A/question.g009D/equal.g00C8/space.g00AB /quotedbl.g006D/slash.g00A9/K.g00AD/percent.g00BA/exclam.g00AF/asterisk.g007D/equal.g00C8 MLE MAP BLR/n.g007C/K.g00AD/three.g0082/.notdef.g0107/equal.g00C8/.notdef.g010A/question.g009D/equal.g00C8/space.g00AB /quotedbl.g006D/slash.g00A9/K.g00AD/percent.g00BA/exclam.g00AF/asterisk.g007D/equal.g00C8 MLE MAP BLR Рис. 9.11. Байесовская линейная регрессия. Слева: закрашены доверительные интервалы 67% (темно-серый) и 95% (светло-серый). Средняя функция для байесовской линейной регрессии совпадает с MAP-оценкой. Дисперсия равна сумме дисперсии шума и слагаемого, отвечающего за неопределенность параметров (зависит от тестового входа). Справа: образцы из апостериорного распределения на функциях\n--- Страница 393 ---\n9.3. Байесовская линейная регрессия 393 На рис. 9.11 показаны некоторые апостериорные распределения на функциях, заданные апостериорным распределением параметров. Для различных степеней многочлена M слева изображены функция максимального правдоподобия , MAP-функция (совпадающая с апостериорным средним) и закрашены доверительные интервалы 67 и 95%, найденные методом байесовской линейной регрессии. Справа изображены случайно выбранные функции из апостериорного распре - деления. Мы сформировали выборку параметров θi из апостериорного распре - деления параметров и нашли функцию ϕT(x*)θi. Для многочленов небольшой степени апостериорное распределение не позволяет параметрам сильно менять - ся — случайные функции почти одинаковы. Увеличивая гибкость модели путем добавления большего количества параметров (то есть повышения степени многочленов), мы ослабляем ограничения на эти параметры, и случайно взятые функции могут сильно различаться. Можно заметить в левой части рисунка, как растет неопределенность, особенно на границах. Хотя для многочленов седьмой степени MAP-оценка дает хорошее совпадение с данными, байесовская линейная регрессия говорит нам о большой апосте - риорной дисперсии. Это может быть важно, когда предсказания используют - ся в системах принятия решений, где неудачные решения могут привести к серьезным последствиям (например, в обучении с подкреплением или в ро- бототехнике). 9.3.5. Вычисление маргинального правдоподобия В разделе 8.6.2 мы подчеркивали важность маргинального правдоподобия в бай- есовском выборе модели. Далее мы будем вычислять маргинальное правдопо - добие для байесовской линейной регрессии с сопряженным гауссовым априор - ным распределением на параметрах, то есть в той постановке задачи, которая была дана в этой главе. Вспомним, что мы рассматриваем следующие порождающие процессы: θ ∼ (m0, S0); (9.60 a) (9.60 b) n = 1, , N. Маргинальное правдоподобие вычисляется как (9.61 a) (9.61 b)\n--- Страница 394 ---\n394 Глава 9. Линейная регрессия где при интегрировании исчезает зависимость от параметров модели θ. Мар - гинальное правдоподобие мы вычисляем в два шага: сначала показываем, что оно (как распределение y) является гауссовым, потом находим среднее и ко- вариацию1. 1. Маргинальное правдоподобие гауссово. Из раздела 6.5.2 мы знаем, что (i) произведение двух гауссовых случайных величин будет (ненормали - зованной) гауссовой случайной величиной, и (ii) при линейном преоб - разовании гауссовой случайной величины получается гауссово распреде - ление. В формуле (9.61 b) мы с помощью линейных преобразований превращаем (y | Xθ, σ2I) в (θ | μ, Σ) для некоторых μ, Σ. После этого можно вычислить интеграл и получить нормализующую константу для произведения двух гауссиан. Сама эта константа будет иметь гауссово распределение, см. (6.76). 2. Среднее и ковариация. Среднее и ковариационная матрица для маргиналь - ного правдоподобия вычисляются через стандартные свойства среднего и ковариации, касающихся аффинных преобразований случайных величин (раздел 6.4.4). Среднее для маргинального правдоподобия вычисляется как θ[ | ] = θ[Xθ + ε] = Xθ[θ] = Xm0. (9.62) Заметим, что ε ∼ (0, σ2I) — вектор из независимых одинаково распределенных случайных величин. Матрица ковариации вычисляется как Covθ[ | ] = Cov[Xθ] + σ2I = X Covθ[θ]XT+ σ2I = (9.63 a) = X S0XT+ σ2I. (9.63 b) Значит, маргинальное правдоподобие задается формулой (9.64 a) (9.64 b) Учитывая связь маргинального правдоподобия и предсказанного апостериор - ного распределения (см. примечание ранее в этом разделе), формула не должна вызывать удивления. 1 Маргинальное правдоподобие можно рассматривать как математическое ожидание правдоподобия относительно априорного распределения параметров.\n--- Страница 395 ---\n9.4. Максимальное правдоподобие как ортогональная проекция 395 9.4. МАКСИМАЛЬНОЕ ПРАВДОПОДОБИЕ КАК ОРТОГОНАЛЬНАЯ ПРОЕКЦИЯ Продравшись через алгебраические дебри при выводе оценок максимального правдоподобия и MAP , мы предоставим геометрическую интерпретацию оценки максимального правдоподобия. Рассмотрим простую задачу линейной регрессии y = xθ + ε, ε ∼ (0, σ2) (9.65) с линейными функциями f :  → , проходящими через начало координат (для простоты мы не используем преобразования). Параметр θ задает угол наклона прямой. На рис. 9.12( a) показан одномерный датасет. x (a) /m.g0073/equal.g00C8/K.g00AD/percent.g00BA/exclam.g00AF /.notdef.g0105/equal.g00C8……/slash.g00A9/period.g00B2 ƒ/equal.g00C8/.notdef.g0105/equal.g00C8/.notdef.g0107/comma.g00D2 /exclam.g00AF/yright/.notdef.g0104/exclam.g00AF/yright““/comma.g00D2/comma.g00D2: ƒ/equal.g00C8/.notdef.g0108/three.g0082/.notdef.g00E4/.notdef.g00E3/yright……/slash.g00A9/yright …/equal.g00C8/K.g00AD/.notdef.g00E3/.notdef.g010A/.notdef.g0105/yright…/comma.g00D2/space.g00AB yn (/C.g00B9/percent.g00BA/asterisk.g007D/equal.g00C8ƒ/equal.g00C8…/slash.g00A9 /two.g0088/yright/.notdef.g00E4…/slash.g00A9/.notdef.g00E4) ƒ…/equal.g00C8/.notdef.g0107/yright…/comma.g00D2/L.g00AE /hyphen.g00C1/three.g0082…/asterisk.g007D/.notdef.g0106/comma.g00D2/comma.g00D2 f(xn) …/equal.g00C8 /quotedbl.g006D/period.g00B2/percent.g00BA/.notdef.g0105/equal.g00C8/period.g00B2 xn/o.g0081/exclam.g00AF/percent.g00BA/yright/asterisk.g007D/.notdef.g0106/comma.g00D2/comma.g00D2 /m.g0073/equal.g00C8/K.g00AD/.notdef.g00E3/.notdef.g010A/.notdef.g0105/yright…/comma.g00D2/space.g00AB /p.g0063/yright/.notdef.g0108/yright…/comma.g00D2/yright /.notdef.g00E4/equal.g00C8/asterisk.g007D“/comma.g00D2/.notdef.g00E4/equal.g00C8/.notdef.g00E3/.notdef.g0109…/percent.g00BA/.notdef.g0104/percent.g00BA /C.g00B9/exclam.g00AF/equal.g00C8/quotedbl.g006D/.notdef.g0105/percent.g00BA/C.g00B9/percent.g00BA/.notdef.g0105/percent.g00BA/K.g00AD/comma.g00D2/space.g00AB (b) /q.g0076/quotedbl.g006D/yright/two.g0088/.notdef.g00E3/slash.g00A9/yright /two.g0088/percent.g00BA/.notdef.g0107/asterisk.g007D/comma.g00D2 /numbersign.g00DB /C.g00B9/exclam.g00AF/percent.g00BA/yright/asterisk.g007D/.notdef.g0106/comma.g00D2/comma.g00D2 ƒ/equal.g00C8/.notdef.g0108/three.g0082/.notdef.g00E4/.notdef.g00E3/yright……/slash.g00A9/period.g00B2 …/equal.g00C8/K.g00AD/.notdef.g00E3/.notdef.g010A/.notdef.g0105/yright…/comma.g00D2/L.g00AE (/two.g0088/yright/.notdef.g00E4…/slash.g00A9/yright /two.g0088/percent.g00BA/.notdef.g0107/asterisk.g007D/comma.g00D2) …/equal.g00C8 /C.g00B9/exclam.g00AF/space.g00AB/.notdef.g00E4/three.g0082/.notdef.g010A /g84MLx. /p.g0063/yright/.notdef.g0108/yright…/comma.g00D2/yright /.notdef.g00E4/equal.g00C8/asterisk.g007D“/comma.g00D2/.notdef.g00E4/equal.g00C8/.notdef.g00E3/.notdef.g0109…/percent.g00BA/.notdef.g0104/percent.g00BA /C.g00B9/exclam.g00AF/equal.g00C8/quotedbl.g006D/.notdef.g0105/percent.g00BA/C.g00B9/percent.g00BA/.notdef.g0105/percent.g00BA/K.g00AD/comma.g00D2/space.g00AB /numbersign.g00DB /period.g00AA/two.g0088/percent.g00BA /C.g00B9/percent.g00BA/.notdef.g0105/C.g00B9/exclam.g00AF/percent.g00BA“/two.g0088/exclam.g00AF/equal.g00C8…“/two.g0088/quotedbl.g006D/percent.g00BA (/C.g00B9/exclam.g00AF/space.g00AB/.notdef.g00E4/equal.g00C8/space.g00AB), “/three.g0082/.notdef.g00E4/.notdef.g00E4/equal.g00C8 /exclam.g00AF/equal.g00C8““/two.g0088/percent.g00BA/space.g00AB…/comma.g00D2/L.g00AE /percent.g00BA/two.g0088 /quotedbl.g006D“/yright/period.g00B2 …/equal.g00C8/K.g00AD/.notdef.g00E3/.notdef.g010A/.notdef.g0105/yright…/comma.g00D2/L.g00AE /.notdef.g0105/percent.g00BA /asterisk.g007D/percent.g00BA/two.g0088/percent.g00BA/exclam.g00AF/percent.g00BA/L.g00AE /.notdef.g00E4/comma.g00D2…/comma.g00D2/.notdef.g00E4/equal.g00C8/.notdef.g00E3/.notdef.g0109…/equal.g00C8y /dollar.g00DC4 /dollar.g00DC2 2 04 x/dollar.g00DC4 /dollar.g00DC2 2 04/dollar.g00DC4/dollar.g00DC22 04 y /dollar.g00DC4/dollar.g00DC22 04 Рис. 9.12. Геометрическая интерпретация метода наименьших квадратов. ( a) Данные; (b) решение максимального правдоподобия как проекция Теперь, когда у нас есть обучающий набор данных {( x1, y1), , ( xN, yN)}, следует вспомнить результаты раздела 9.2.1 и найти оценку максимального правдопо - добия для тангенса угла наклона: (9.66) где X = [x1, , xN]T ∈ N, y = [y1, , yN]T ∈ N. Таким образом, на входных точках X обучающей выборки получаем оптималь - ные (максимально правдоподобные) значения ответов (9.67)\n--- Страница 396 ---\n396 Глава 9. Линейная регрессия то есть наилучшее (в смысле квадратичной функции ошибки) приближение Xθ к y. Так как мы ищем такое θ, что y = Xθ, можно рассматривать задачу линейной регрессии как решение систем линейных уравнений. Значит, мы можем обра - щаться к идеям из линейной алгебры и аналитической геометрии, которые мы обсуждали в главах 2 и 3. В частности, присмотревшись к формуле (9.67), мы видим, что оценка максимального правдоподобия θML в примере (9.65) фак - тически задает ортогональную проекцию y на одномерное подпространство с системой образующих X. Вспомнив результаты раздела 3.8, касающиеся орто - гональных проекций, мы понимаем, что — матрица проекции, θML — ко- ординаты проекции на одномерное подпространство в N, натянутое на X, а XθML — ортогональная проекция y на это подпространство. Таким образом, метод максимального правдоподобия находит геометрически оптимальное решение, то есть ближайший к наблюдениям y вектор из подпро - странства, натянутого на X, где мерой близости является квадрат расстояния от yn до xnθ. Этим ближайшим вектором будет ортогональная проекция. На рис. 9.12( b) показана проекция зашумленных наблюдений на подпространство, соответствующая минимуму квадрата расстояния от исходного набора данных до подпространства (заметьте, что x-координаты фиксированы). Это решение максимального правдоподобия. В общем случае линейной регрессии, где y = ϕT(x)θ + ε, ε ∼ (0, σ2) (9.68) с векторнозначными преобразованиями ϕ (x) ∈ K, также можно интерпрети - ровать оценку максимального правдоподобия y ≈ ΦθML; (9.69) θML = (ΦTΦ)–1ΦTy (9.70) как проекцию на K-мерное подпространство в N, натянутое на столбцы матри - цы преобразования Φ (раздел 3.8.2). В частном случае, когда функции преобразования ϕk, используемые для постро - ения Φ, ортонормальны (раздел 3.7), то есть столбцы Φ образуют ортонормиро - ванный базис (раздел 3.5), для которого ΦTΦ = I, проекция равна , (9.71)\n--- Страница 397 ---\n9.5. Для дальнейшего чтения 397 то есть линейная зависимость разных преобразований исчезает, и проекция максимального правдоподобия будет просто суммой проекций y на векторы базиса ϕk, то есть столбцы Φ. Многие популярные в обработке сигналов базисные функции (вейвлеты, базис Фурье) образуют ортогональные базисы. Базис, не являющийся ортогональным, можно привести к таковому методом Грама — Шмидта (Strang, 2003). 9.5. ДЛЯ ДАЛЬНЕЙШЕГО ЧТЕНИЯ В данной главе мы обсуждали линейную регрессию для гауссовых функций правдоподобия и сопряженных априорных распределений параметров. Эти условия позволяли получить явные формулы для байесовского инференса. Однако в некоторых приложениях нам может понадобиться другая функция правдоподобия. Например, в задаче бинарной классификации возможных от - ветов только два, и гауссово распределение здесь не подходит. Вместо него мы можем взять правдоподобие Бернулли, дающее нам вероятность предсказания, которое говорит о принадлежности к классу (1 или 0). Советуем обратиться к книгам Бишопа (Bishop, 2006), Мерфи (Murphy, 2012) и Барбера (Barber, 2012) для подробного введения в задачи классификации. Другой пример не - гауссового правдоподобия возникает в ситуации, когда данные — это целые неотрицательные числа (например, когда речь идет о количестве единиц чего- либо). В этом случае биномиальное или пуассоновское правдоподобие будет более удачным выбором, чем гауссово. Все эти примеры относятся к классу обобщенных линейных моделей, представляющих собой гибкое обобщение линейной регрессии, которое позволяет ошибкам иметь отличное от гауссова распределение. Обобщенные линейные модели позволяют связывать линейную модель с наблюдаемыми значениями через гладкую обратимую (возможно, нелинейную) функцию σ(·), то есть y = σ((f(x)), где f(x) = θTϕ(x) — модель линейной регрессии (9.13). Можно воспринимать обобщенную линейную модель как композицию функций y = σ ◦ f, где f — модель линейной регрессии, а σ — функция активации. Заметим, что хотя модель и называется «обобщен - ной линейной», ее предсказания больше не будут линейными по параметрам θ. В логистической регрессии в качестве функции активации берется логистиче ­ ская сигмоида σ(f) = 1/(1 + exp( −f )) ∈ [0, 1], которую можно понимать как вероятность наблюдать значение y = 1 бернуллиевской случайной величины y ∈ {0, 1}. Функцию σ(·) называют функцией активации , а обратную к ней — канонической функцией связи . Таким образом, понятно, что обобщенные ли - нейные модели являются «кирпичиками» для глубоких нейронных сетей прямого распространения. Фактически, обобщенная линейная модель y = σ(Ax + b), где A — матрица весов, а b — вектор смещения, описывает одно -\n--- Страница 398 ---\n398 Глава 9. Линейная регрессия слойную нейронную сеть с функцией активации1 σ(·). Теперь можно построить композицию таких функций xk+1 = fk(xk); (9.72) fk(xk) = σk(Ak xk + bk) для k = 0, , K − 1, где x0 — входные признаки, xK = y — наблюдения, а fK–1 ◦ ··· ◦ f0 — нейронная сеть из K слоев. Таким образом, «кирпичиками» такой глубокой нейронной сети являются обобщенные линейные модели, определенные в (9.72)2. Нейронные сети (Bishop, 1995; Goodfellow et al., 2016) значительно превосходят по выразительности и гибкости модели линейной регрессии. Однако задача оценки максимального правдоподобия для параметров — невыпуклая задача оптимизации, а задача маргинализации параметров при полностью байесовской ее постановке не имеет аналитического решения. Мы уже упоминали, что распределение параметров задает распределение функ - ций регрессии. Г ауссовы процессы (Rasmussen and Williams, 2006) являются моделями регрессии, в которых распределение функций играет ключевую роль. Вместо введения распределения параметров в гауссовых процессах мы сразу работаем с распределением функций. Для этого применяется « ядерный трюк » (kernel trick, Sch ölkopf and Smola, 2002), позволяющий вычислять скалярное произведение двух значений функции f(xi), f(xj) по входным xi, xj. Гауссовы процессы тесно связаны как с байесовской линейной регрессией, так и с методом опорных векторов, но их можно интерпретировать и как байесовские нейронные сети с одним скрытым слоем и стремящимся к бесконечности количеством узлов (Neal, 1996; Williams, 1997). Отличное введение в гауссовы процессы можно найти у МакКея (MacKay, 1998), а также у Расмуссена и Уильямса (Rasmussen and Williams, 2006). В этой главе мы в основном уделяли внимание гауссовым априорным рас - пределениям параметров, поскольку они позволяют вывести формулы для задач линейной регрессии в явном виде. Однако даже при гауссовом правдо - подобии можно выбрать негауссово априорное распределение. Рассмотрим постановку задачи, в которой входные значения x ∈ D, а обучающая выборка маленькая — размера N ≪ D. Это означает, что задача регрессии недоопреде - лена. В этом случае можно выбрать разреженное априорное распределение параметров, приравнивающее как можно большее число параметров к нулю (отбор признаков). Такое априорное распределение дает более сильную регу - 1 Для обычной линейной регрессии функцией активации будет тождественная функ - ция. 2 На https://tinyurl.com/glm-dnn есть отличный пост о связи между обобщенными моделями и глубокими нейронными сетями.\n--- Страница 399 ---\n9.5. Для дальнейшего чтения 399 ляризацию, чем гауссово априорное распределение, что часто помогает точ - ности предсказаний и интерпретируемости модели. Одним из часто исполь - зуемых априорных распределений является распределение Лапласа. Модель линейной регрессии с априорным распределением Лапласа эквивалентна линейной регрессии с L1-регуляризацией (LASSO) (Tibshirani, 1996). Рас - пределение Лапласа имеет острый пик в нуле (первая производная имеет разрыв) и более сконцентрировано вблизи нуля, чем гауссово, то есть сильнее принуждает параметры быть равными нулю. Ненулевые параметры являются важными для задачи регрессии, поэтому мы часто говорим об «отборе при - знаков» («отборе переменных»).\n--- Страница 400 ---\n10 Снижение размерности с помощью анализа главных компонент Непосредственная работа с многомерными данными, такими как изображения, сопряжена с некоторыми трудностями: их тяжело анализировать, сложно интерпретировать, визуализация практически невозможна, и (с практической точки зрения) хранение векторов данных может быть дорогостоящим. Одна - ко многомерные данные часто обладают свойствами, которые мы можем ис - пользовать. Например, данные большой размерности часто являются избы - точными, то есть многие измерения являются избыточными и могут быть объяснены комбинацией других измерений. Кроме того, измерения в данных высокой размерности часто коррелируют, так что данные обладают внутрен - ней структурой с более низкой размерностью. Снижение размерности ис - пользует структуру и корреляцию и позволяет нам работать с более компакт - ным представлением данных, в идеале без потери информации. Мы можем рассматривать уменьшение размерности как метод сжатия, подобный jpeg или mp3, которые представляют собой алгоритмы сжатия изображений и му- зыки. В этой главе мы обсудим анализ главных компонент (PCA — principal component analysis ) и алгоритм уменьшения линейной размерности . PCA, предложенный Пирсоном (1901) и Хотеллингом (1933), существует уже более 100 лет и до сих пор остается одним из наиболее часто используемых методов сжатия и визуа - лизации данных. Он также используется для идентификации простых законо - мерностей, скрытых факторов и структур многомерных данных. В сообществе\n--- Страница 401 ---\n10.1. Постановка проблемы 401 обработки сигналов PCA также известен как преобразование Карунена — Лоэва . В этой главе мы выводим PCA из первых принципов, опираясь на наше пони - мание базиса и изменения базиса (разделы 2.6.1 и 2.7.2), прогнозов (раздел 3.8), собственных значений (раздел 4.2), гауссовых распределений (раздел 6.5), и ограниченной оптимизации (раздел 7.2). Снижение размерности обычно использует свойство данных с высокой раз - мерностью (например, изображений), которое часто находится в подпростран - стве с низкой размерностью. На рис. 10.1 показан наглядный пример в двух измерениях. Хотя данные на рис. 10.1( a) не совсем лежат на прямой, данные не сильно различаются в направлении x2, так что мы можем выразить их так, как если бы они были на прямой — почти без потерь; см. рис. 10.1( b). Для описания данных на рис. 10.1( b) требуется только координата x1, а данные лежат в одно- мерном подпространстве 2. 10.1. ПОСТАНОВКА ПРОБЛЕМЫ В PCA мы заинтересованы в том, чтобы найти проекции точек данных xn, которые максимально похожи на исходные точки данных, но которые имеют значительно более низкую внутреннюю размерность. На рис. 10.1 показано, как это может выглядеть. x1x2 x2 x1 (а) Набор данных с координатами x1 и x2(b) Сжатый набор данных, где важна только координата x14 2 0 –2 –44 2 0 –2 –4 –5,0 –5,0 5,0 5,0 –2,5 –2,5 2,5 2,5 0,0 0,0 Риc. 10.1. Иллюстрация: уменьшение размерности. ( а) Набор данных с координатами x1 и x2. (b) Сжатый набор данных, где важна только координата x1. (а) Исходный набор данных не сильно меняется в направлении x2. (b) Данные из ( a) могут быть представлены с использованием только координаты x1 почти без потерь\n--- Страница 402 ---\n402",
      "debug": {
        "start_page": 368,
        "end_page": 402
      }
    },
    {
      "name": "Глава 10. Снижение размерности с помощью анализа главных компонент 400",
      "content": "--- Страница 402 --- (продолжение)\nГлава 10. Снижение размерности с помощью анализа главных компонент Более конкретно, мы рассматриваем независимый и одинаково распределенный набор данных  = {x1, , xN, xn } ∈ D, со средним значением 0, который обладает ковариационной матрицей данных (6.42) (10.1) Кроме того, мы предполагаем, что существует низкоразмерное сжатое пред - ставление (код) zn = BTxn ∈ M (10.2) для xn, где мы определяем проекционную матрицу B := [b1, , bM] ∈ D×M. (10.3) Мы предполагаем, что столбцы B ортонормированы (определение 3.7), так что тогда и только тогда, когда i ≠ j и . Ищем M-мерное подпро - странство U ⊆ D, dim( U) = M < D, на которое мы проецируем данные. Обозна - чим проецируемые данные через ∈ U, а их координаты (относительно базис - ных векторов b1, , bM матрицы U) через zn. Наша цель — найти проекции ⊆ D (или, что эквивалентно, коды zn и базисные векторы b1, , bM), чтобы они были максимально похожи на исходные данные xn и минимизировали потери из-за сжатия1. Пример 10.1 (координатное представление / код) Рассмотрим 2 с каноническим базисом e1 = [1, 0]T, e2 = [0, 1]T. Из главы 2 мы знаем, что x ∈ 2 можно представить как линейную комбинацию этих базисных векторов, например (10.4) Однако, когда мы рассматриваем векторы вида (10.5) их всегда можно записать как 0 e1 + ze2. Чтобы представить эти векторы, достаточно запомнить/сохранить координату/код z вектора относи - 1 Столбцы b1, , bM пространства B образуют базис M-мерного подпространства, в ко- тором живут проецируемые данные = BB T x ∈ D.\nГлава 10. Снижение размерности с помощью анализа главных компонент Более конкретно, мы рассматриваем независимый и одинаково распределенный набор данных  = {x1, , xN, xn } ∈ D, со средним значением 0, который обладает ковариационной матрицей данных (6.42) (10.1) Кроме того, мы предполагаем, что существует низкоразмерное сжатое пред - ставление (код) zn = BTxn ∈ M (10.2) для xn, где мы определяем проекционную матрицу B := [b1, , bM] ∈ D×M. (10.3) Мы предполагаем, что столбцы B ортонормированы (определение 3.7), так что тогда и только тогда, когда i ≠ j и . Ищем M-мерное подпро - странство U ⊆ D, dim( U) = M < D, на которое мы проецируем данные. Обозна - чим проецируемые данные через ∈ U, а их координаты (относительно базис - ных векторов b1, , bM матрицы U) через zn. Наша цель — найти проекции ⊆ D (или, что эквивалентно, коды zn и базисные векторы b1, , bM), чтобы они были максимально похожи на исходные данные xn и минимизировали потери из-за сжатия1. Пример 10.1 (координатное представление / код) Рассмотрим 2 с каноническим базисом e1 = [1, 0]T, e2 = [0, 1]T. Из главы 2 мы знаем, что x ∈ 2 можно представить как линейную комбинацию этих базисных векторов, например (10.4) Однако, когда мы рассматриваем векторы вида (10.5) их всегда можно записать как 0 e1 + ze2. Чтобы представить эти векторы, достаточно запомнить/сохранить координату/код z вектора относи - 1 Столбцы b1, , bM пространства B образуют базис M-мерного подпространства, в ко- тором живут проецируемые данные = BB T x ∈ D.\n--- Страница 403 ---\n10.1. Постановка проблемы 403 тельно вектора e2. Точнее, набор векторов (со стандартным векторным сложением и скалярным умножением) образует векторное подпростран - ство U (раздел 2.4) с dim( U) = 1, поскольку U = span [ e2]1. В разделе 10.2 мы найдем низкоразмерные представления, которые сохраняют как можно больше информации и минимизируют потери при сжатии. Альтер - нативный вывод PCA приведен в разделе 10.3, где мы рассмотрим минимизацию квадрата ошибки восстановления между исходными данными и их проекцией . Рисунок 10.2 иллюстрирует настройку, которую мы рассматриваем в PCA, где z представляет низкоразмерное представление сжатых данных и играет роль узкого места, которое контролирует, сколько информации может передаваться между x и . В PCA мы рассматриваем линейную зависимость между исход - ными данными x и их низкоразмерным кодом z, так что z = BTx и = Bz для подходящей матрицы B. Основываясь на мотивации представления PCA как метода сжатия данных, мы можем интерпретировать стрелки на рис. 10.2 как пару операций, представляющих кодеры и декодеры. Линейное отображение, представленное буквой B, можно представить себе как декодер, который ото - бражает низкоразмерный код z ∈ M обратно в исходное пространство данных D. Точно так же B можно представить себе кодировщиком, который кодирует ис - ходные данные x как низкоразмерный (сжатый) код z. /h.g006A“/period.g00B2/percent.g00BA/.notdef.g0105…/slash.g00A9/L.g00AE /q.g0076›/equal.g00C8/two.g0088/slash.g00A9/L.g00AE RD RMRD xz x~ Рис. 10.2. Графическое изображение PCA. В PCA мы находим сжатую версию z исходных данных x. Сжатые данные могут быть преобразованы в набор , который живет в исходном пространстве данных, но имеет внутреннее представление более низкой размерности, чем x В этой главе мы будем использовать набор данных MNIST в качестве повторя - ющегося примера, который содержит 60 000 примеров рукописных цифр от 0 до 9. Каждая цифра представляет собой изображение в градациях серого раз - мером 28 × 28, то есть содержит 784 пикселя, чтобы мы могли интерпретировать 1 Размерность векторного пространства соответствует количеству его базисных векто - ров (раздел 2.6.1).\n--- Страница 404 ---\n404 Глава 10. Снижение размерности с помощью анализа главных компонент каждое изображение в этом наборе данных как вектор x ∈ 784. Примеры этих цифр показаны на рис. 10.3. Рис. 10.3. Примеры рукописных цифр из набора данных MNIST. http://yann.lecun.com/exdb/mnist/ 10.2. ПЕРСПЕКТИВА МАКСИМАЛЬНОЙ ДИСПЕРСИИ На рис. 10.1 приведен пример того, как двумерный набор данных может быть представлен с использованием одной координаты. На рис. 10.1( b) мы решили игнорировать координату x2 данных, потому что она не добавляла слишком много информации, так что сжатые данные похожи на исходные данные на рис. 10.1( a). Мы могли бы игнорировать координату x1, но тогда сжатые данные сильно отличались от исходных данных, и большая часть информации в данных была бы потеряна. Если мы интерпретируем информационное содержание данных как «простран - ство, заполняющее» набор данных, то мы можем описать информацию, содер - жащуюся в данных, глядя на разброс данных. Из раздела 6.4.1 мы знаем, что дисперсия является индикатором распространения данных, и мы можем вы - вести PCA как алгоритм уменьшения размерности, который максимизирует дисперсию в низкоразмерном представлении данных, чтобы сохранить как можно больше информации. Рисунок 10.4 иллюстрирует это. Рис. 10.4. PCA находит подпространство (линия) меньшей размерности, которое поддерживает как можно большую дисперсию (разброс данных), когда данные (черные точки) проецируются на это подпространство (светлые линии) Учитывая настройку, обсуждаемую в разделе 10.1, наша цель — найти матрицу B (раздел 10.3), которая сохраняет как можно больше информации при сжатии данных, проецируя ее на подпространство, охватываемое столбцами b1, , bM матрицы B. Сохранение большей части информации после сжатия данных эк - вивалентно улавливанию наибольшего количества дисперсии в коде низкой размерности (Hotelling, 1933).\n--- Страница 405 ---\n10.2. Перспектива максимальной дисперсии 405 ПРИМЕЧАНИЕ Для ковариационной матрицы данных в (10.1) мы приняли центрированные данные. Мы можем сделать это предположение без ограниче - ния общности: предположим, что μ — это среднее значение данных. Используя свойства дисперсии, которые мы обсуждали в разделе 6.4.4, получаем z[z] = x[BT(x − μ)] = x[BTx − BTμ] = x[BTx], (10.6) то есть дисперсия низкоразмерного кода не зависит от среднего значения данных. Поэтому без ограничения общности мы предполагаем, что данные имеют среднее значение 0 до конца этого раздела. С этим предположением среднее значение низкоразмерного кода также равно 0, поскольку z[z] = x[BTx] = = BTx[x] = 0.  10.2.1. Направление с максимальной дисперсией Мы максимизируем дисперсию низкоразмерного кода, используя последова - тельный подход. Мы начинаем с поиска единственного вектора b1 ∈ D, который максимизирует дисперсию проецируемых данных1, то есть мы стремимся мак - симизировать дисперсию первой координаты z1 элемента z ∈ M, так чтобы (10.7) было максимизировано, где мы использовали предположение о независимости и одинаковом распределении данных и определили z1n как первую координату низкоразмерного представления zn ∈ M элемента xn ∈ D. Обратите внимание, что первая компонента zn задается формулой (10.8) то есть это координата ортогональной проекции xn на одномерное подпростран - ство, натянутое на b1 (раздел 3.8). Подставляем (10.8) в (10.7), что дает (10.9 a) (10.9 b) 1 Вектор b1 будет первым столбцом матрицы B и, следовательно, первым из M ортонор - мированных базисных векторов, которые охватывают подпространство меньшей раз - мерности.\n--- Страница 406 ---\n406 Глава 10. Снижение размерности с помощью анализа главных компонент где S — ковариационная матрица данных, определенная в (10.1). В (10.9 а) мы использовали тот факт, что скалярное произведение двух векторов симметрич - но по своим аргументам, то есть . Обратите внимание, что произвольное увеличение величины вектора b1 увели - чивает V1, то есть вектор b1, который в два раза длиннее, может привести к V1, который потенциально в четыре раза больше. Поэтому мы ограничиваем все решения до , что приводит к задаче ограниченной оптимизации, в кото- рой мы ищем направление, в котором данные изменяются больше всего. При ограничении пространства решений единичными векторами вектор b1, который указывает в направлении максимальной дисперсии, может быть найден с по- мощью задачи оптимизации с ограничениями (10.10) Следуя разделу 7.2, получаем лагранжиан (10.11) для решения этой задачи оптимизации с ограничениями. Частные производные  по b1 и λ1 равны (10.12) соответственно. Установка этих частных производных равными 0 дает нам со - отношения Sb1 = λ1b1, (10.13) (10.14) Сравнивая это с определением разложения по собственным значениям (раз - дел 4.4), мы видим, что b1 является собственным вектором ковариационной матрицы данных S, а множитель Лагранжа λ1 играет роль соответствующего собственного значения1. Это свойство собственного вектора (10.13) позволяет нам переписать нашу цель по дисперсии (10.10) как (10.15) 1 Величина также называется загрузкой единичного вектора b1 и представляет со - бой стандартное отклонение данных, приходящихся на главный промежуток подпро - странства [ b1].\n--- Страница 407 ---\n10.2. Перспектива максимальной дисперсии 407 то есть дисперсия данных, проецируемых на одномерное подпространство, равна собственному значению, которое связано с базисным вектором b1, охва - тывающим это подпространство. Следовательно, чтобы максимизировать дис - персию низкоразмерного кода, мы выбираем базисный вектор, связанный с наибольшим собственным значением матрицы ковариации данных. Этот собственный вектор называется первой главной компонентой . Мы можем опре - делить влияние/вклад главной компоненты b1 в исходное пространство данных, отображая координату z1n обратно в пространство данных, что дает нам про - гнозируемую точку данных (10.16) в исходном пространстве данных. ПРИМЕЧАНИЕ Хотя является D-мерным вектором, для его представления относительно базисного вектора b1 ∈ D требуется только одна координата z1n.  10.2.2. M-мерное подпространство с максимальной дисперсией Предположим, что мы нашли первые m – 1 главных компонент как m – 1 соб - ственных векторов S, которые связаны с наибольшими m – 1 собственными значениями. Поскольку S симметрично, спектральная теорема (теорема 4.15) утверждает, что мы можем использовать эти собственные векторы для постро - ения ортонормированного собственного базиса ( m – 1)-мерного подпространства в D. Как правило, m-я главная компонента может быть найдена путем вычита - ния влияния первых m – 1 главных компонент b1, , bm–1 из данных, тем самым помогая найти главные компоненты, которые сжимают оставшуюся информацию. Затем мы приходим к новой матрице данных (10.17) где X = [x1, , xN] ∈ D×N содержит точки данных как векторы-столбцы1, а — матрица проекции, которая проецируется на подпространство, натянутое на b1, , bm–1. 1 Матрица = [ 1, , N] ∈ D×N в (10.17) содержит информацию в данных, которые еще не были сжаты.\n--- Страница 408 ---\n408 Глава 10. Снижение размерности с помощью анализа главных компонент ПРИМЕЧАНИЕ На протяжении всей этой главы мы не следуем соглашению о сборе данных x1, , xN в качестве строк матрицы данных, но мы определяем их как столбцы X. Это означает, что наша матрица данных X является матрицей D × N вместо обычной матрицы N × D. Причина нашего выбора заключается в том, что алгебраические операции работают гладко, без необходимости транс - понировать матрицу или переопределять векторы как векторы-строки, которые умножаются слева на матрицы.  Чтобы найти m-ю главную компоненту, мы максимизируем дисперсию (10.18) при условии , где мы проделали те же шаги, что и в (10.9 b), и опреде - лили как матрицу ковариации данных преобразованного набора данных . Как и раньше, когда мы рассматривали только первую главную компоненту, мы решаем задачу оптимизации с ограничениями и обнаруживаем, что оптимальное решение bm является собственным вектором , который связан с наибольшим собственным значением . Оказывается, что bm также является собственным вектором S. В общем, наборы собственных векторов S и идентичны. Поскольку и S, и симметричны, мы можем найти ОНБ собственных векторов (спектральная теорема 4.15), то есть существует D различных собственных векторов как для S, так и для . Далее мы покажем, что каждый собственный вектор оператора S является собственным вектором оператора . Предположим, мы уже нашли собственные векторы b1, , bm–1 матрицы . Рассмотрим собственный вектор bi оператора S, то есть Sbi = λibi. В общем , (10.19 a) = (S − SBm–1 − Bm–1S + Bm–1SBm–1)bi. (10.19 b) Мы различаем два случая. Если i ≥ m, то есть bi является собственным вектором, который не входит в число первых m – 1 главных компонент, то bi ортогонален первым m – 1 главным компонентам и Bm–1bi = 0. Если i < m, то есть bi входит в число первых m – 1 главных компонент, то bi является базисным вектором главного подпространства, на которое проецируется Bm–1. Поскольку b1, , bm–1 являются ОНБ этого главного подпространства, получаем Bm–1 bi = bi. Эти два случая можно резюмировать следующим образом: Bm–1bi = bi, если i < m; Bm–1bi = 0, если i ≥ m. (10.20)\n--- Страница 409 ---\n10.2. Перспектива максимальной дисперсии 409 В случае i ≥ m, используя (10.20) в (10.19 b), получаем = (S – Bm–1S)bi = Sbi = = λi bi то есть bi также является собственным вектором с собственным значе - нием λi. А именно: (10.21) Уравнение (10.21) показывает, что bm является собственным вектором не только S, но и . В частности, λm — наибольшее собственное значение , а λm — m-е наи - большее собственное значение S, и оба имеют связанный собственный вектор bm. В случае i < m, используя (10.20) в (10.19 b), получаем . (10.22) Это означает, что b1, , bm–1 также являются собственными векторами , но они связаны с собственным значением 0, так что b1, , bm–1 покрывают нулевое про - странство . В целом каждый собственный вектор S также является собственным вектором . Однако, если собственные векторы S являются частью ( m – 1)-мерного глав - ного подпространства, то соответствующее собственное значение равно 01. При соотношении (10.21) и дисперсия данных, проецируемых на m-ю главную компоненту, равна (10.23) Это означает, что дисперсия данных при проецировании на M-мерное подпро - странство равна сумме собственных значений, которые связаны с соответству - ющими собственными векторами ковариационной матрицы данных. Пример 10.2 (собственные значения MNIST «8») Взяв все цифры «8» в обучающих данных MNIST, мы вычисляем соб - ственные значения ковариационной матрицы данных. На рис. 10.5( а) показаны 200 наибольших собственных значений ковариационной ма - трицы данных. Мы видим, что только некоторые из них имеют значение, значительно отличающееся от 0. Следовательно, большая часть дисперсии при проецировании данных на подпространство, охватываемое соответ - 1 Этот вывод показывает, что существует тесная связь между M-мерным подпростран - ством с максимальной дисперсией и разложением по собственным значениям. Мы вернемся к этой связи в разделе 10.4.\n--- Страница 410 ---\n410 Глава 10. Снижение размерности с помощью анализа главных компонент ствующими собственными векторами, улавливается только несколькими главными компонентами, как показано на рис. 10.5( b). 0 50 100 150 2000 50 1001 50 200 /h.g006A…/.notdef.g0105/yright/asterisk.g007D“01020304050/q.g0076/percent.g00BA/K.g00AD“/two.g0088/quotedbl.g006D/yright……/percent.g00BA/yright ƒ…/equal.g00C8/.notdef.g0107/yright…/comma.g00D2/yright (a) /q.g0076/percent.g00BA/K.g00AD“/two.g0088/quotedbl.g006D/yright……/slash.g00A9/yright ƒ…/equal.g00C8/.notdef.g0107/yright…/comma.g00D2/space.g00AB (/percent.g00BA/two.g0088“/percent.g00BA/exclam.g00AF/two.g0088/comma.g00D2/exclam.g00AF/percent.g00BA/quotedbl.g006D/equal.g00C8……/slash.g00A9/yright /quotedbl.g006D /C.g00B9/percent.g00BA/exclam.g00AF/space.g00AB/.notdef.g0105/asterisk.g007D/yright /three.g0082/K.g00AD/slash.g00A9/quotedbl.g006D/equal.g00C8…/comma.g00D2/space.g00AB) /asterisk.g007D/percent.g00BA/quotedbl.g006D/equal.g00C8/exclam.g00AF/comma.g00D2/equal.g00C8/.notdef.g0106/comma.g00D2/percent.g00BA……/percent.g00BA/L.g00AE/.notdef.g00E4/equal.g00C8/two.g0088/exclam.g00AF/comma.g00D2/.notdef.g0106/slash.g00A9 /.notdef.g0105/equal.g00C8……/slash.g00A9/period.g00B2 /quotedbl.g006D“/yright/period.g00B2 /.notdef.g0106/comma.g00D2/hyphen.g00C1/exclam.g00AF /ampersand.g00BD8/bracketleft.g0085 /quotedbl.g006D /percent.g00BA/K.g00AD/three.g0082/.notdef.g0107/equal.g00C8/.notdef.g010A/question.g009D/yright/.notdef.g00E4 …/equal.g00C8/K.g00AD/percent.g00BA/exclam.g00AF/yright MNIST/j.g007A/percent.g00BA/.notdef.g00E3/comma.g00D2/.notdef.g0107/yright“/two.g0088/quotedbl.g006D/percent.g00BA /.notdef.g0104/.notdef.g00E3/equal.g00C8/quotedbl.g006D…/slash.g00A9/period.g00B2 /asterisk.g007D/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088100200300400500/g.g007E/equal.g00C8/hyphen.g00C1/comma.g00D2/asterisk.g007D“/comma.g00D2/exclam.g00AF/percent.g00BA/quotedbl.g006D/equal.g00C8……/equal.g00C8/space.g00AB /.notdef.g0105/comma.g00D2“/C.g00B9/yright/exclam.g00AF“/comma.g00D2/space.g00AB (/K.g00AD) /d.g0069/comma.g00D2“/C.g00B9/yright/exclam.g00AF“/comma.g00D2/space.g00AB /C.g00B9/percent.g00BA /percent.g00BA“…/percent.g00BA/quotedbl.g006D…/slash.g00A9/.notdef.g00E4 /asterisk.g007D/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088/equal.g00C8/.notdef.g00E4 Рис. 10.5. Свойства обучающих данных MNIST «8». ( а) Собственные значения, отсортированные по убыванию; ( b) дисперсия, зафиксированная главными компонентами, связанными с наибольшими собственными значениями В целом, чтобы найти M-мерное подпространство D, которое сохраняет как можно больше информации, PCA говорит нам выбрать столбцы матрицы B в (10.3) в качестве M собственных векторов матрицы ковариации данных S, которые связаны с M наибольшими собственными значениями. Максимальное количество дисперсии, которое PCA может зафиксировать с помощью первых M главных компонент, равно (10.24) где λm — это M наибольших собственных значений ковариационной матрицы данных S. Следовательно, дисперсия, теряемая при сжатии данных через PCA, составляет (10.25) Вместо этих абсолютных величин мы можем определить относительную дис - персию, захваченную как , и относительную дисперсию, потерянную при сжатии, как .\n--- Страница 411 ---\n10.3. Проекционная перспектива 411 10.3. ПРОЕКЦИОННАЯ ПЕРСПЕКТИВА Далее мы выведем PCA как алгоритм, который напрямую минимизирует сред - нюю ошибку реконструкции. Эта перспектива позволяет нам интерпретировать PCA как реализацию оптимального линейного автокодировщика. Мы будем опираться на главы 2 и 3. В предыдущем разделе мы вывели PCA, максимизируя дисперсию в проециру - емом пространстве, чтобы сохранить как можно больше информации. Далее мы рассмотрим векторы разности между исходными данными xn и их реконструк - цией и минимизируем это расстояние так, чтобы xn и были как можно ближе. Рисунок 10.6 иллюстрирует эту настройку. Рис. 10.6. Иллюстрация подхода к проекции: найдите подпространство (прямую), которое минимизирует длину вектора разницы между проецируемыми (светлые линии) и исходными (черные точки) данными 10.3.1. Настройка и цели Предположим (упорядоченный) ортонормированный базис (ОНБ) B = (b1, , bD) D, то есть тогда и только тогда, когда i = j, и 0 в противном случае. Из раздела 2.5 мы знаем, что для базиса ( b1, , bD) D любое x ∈ D можно за - писать как линейную комбинацию базисных векторов D 1, то есть (10.26) для подходящих координат ζd ∈ . 1 Векторы ∈ U могут быть векторами на плоскости в 3. Размерность плоскости рав - на 2, но векторы по-прежнему имеют три координаты относительно стандартного ба - зиса 3.\n--- Страница 412 ---\n412 Глава 10. Снижение размерности с помощью анализа главных компонент Нас интересует нахождение векторов ∈ D, которые живут в подпространстве U ⊆ D меньшей размерности, dim( U) = M, так что (10.27) максимально похоже на x. Отметим, что в этой точке нам нужно предположить, что координаты zm точки и ζm точки x не идентичны. В дальнейшем мы будем использовать именно такое представление для на - хождения оптимальных координат z и базисных векторов b1, , bM, таких что аналогичен исходной точке данных x, то есть мы стремимся минимизировать (евклидово) расстояние . Рисунок 10.7 иллюстрирует эту настройку. bU (/equal.g00C8) /m.g0073/equal.g00C8“/two.g0088/exclam.g00AF/percent.g00BA/L.g00AE/asterisk.g007D/equal.g00C80,00,0 0,50,5 /dollar.g00DC1,0 /dollar.g00DC0,5/dollar.g00DC0,5 1,01,0 1,51,5 2,02,02,5 bU (b) /p.g0063/equal.g00C8ƒ…/percent.g00BA“/two.g0088/comma.g00D2 x /dollar.g00DC xi /.notdef.g0105/.notdef.g00E3/space.g00AB 50 /exclam.g00AF/equal.g00C8ƒ/.notdef.g00E3/comma.g00D2/.notdef.g0107…/slash.g00A9/period.g00B2 xi /C.g00B9/percent.g00BA/asterisk.g007D/equal.g00C8ƒ/equal.g00C8…/slash.g00A9 /.notdef.g0108/two.g0088/exclam.g00AF/comma.g00D2/period.g00B2/percent.g00BA/quotedbl.g006D/asterisk.g007D/percent.g00BA/L.g00AE~~x1x2 0,00,0 0,50,5 /dollar.g00DC1,0 /dollar.g00DC0,5/dollar.g00DC0,5 1,01,0 1,51,5 2,02,02,5 x1x2 Рис. 10.7. Упрощенная настройка проекции. ( a) Вектор x ∈ 2 (крестик) должен быть спроектирован на одномерное подпространство U ⊆ 2, натянутое на b. (b) Показаны разностные векторы между x и некоторыми кандидатами Без ограничения общности мы предполагаем, что набор данных  = {x1, , xN}, xn ∈ D, с центром в 0, то есть [] = 0. Без предположения о нулевом среднем мы пришли бы к точно такому же решению, но обозначения были бы значитель - но более загроможденными. Нас интересует поиск наилучшей линейной проекции на подпространство U меньшей размерности в D с dim(U) = M и ортонормированными базисными векторами b1, , bM. Назовем это подпространство U) главным подпространством. Проекции точек данных обозначены: (10.28)\n--- Страница 413 ---\n10.3. Проекционная перспектива 413 где zn := [z1n, , zMn]T ∈ M — вектор координат относительно базиса ( b1, , bM). В частности, мы заинтересованы в том, чтобы было как можно более похожим на xn. Мера подобия, которую мы используем ниже, — это квадрат евклидовой нормы между x и . Поэтому мы определяем нашу цель как минимизацию среднего квадрата евклидова расстояния (ошибка реконструкции) (Pearson, 1901) (10.29) где мы явно указываем, что размерность подпространства, на которое мы про - ецируем данные, равна M. Чтобы найти эту оптимальную линейную проекцию, нам нужно найти ортонормированный базис главного подпространства и коор- динаты zn ∈ M проекций относительно этого базиса. Чтобы найти координаты zn и ОНБ главного подпространства, мы следуем двухэтапному подходу. Сначала оптимизируем координаты zn для данного ОНБ (b1, , bM); во-вторых, находим оптимальный ОНБ. 10.3.2. Поиск оптимальных координат Начнем с поиска оптимальных координат z1n, , zMn проекций при n = 1, , N. Рассмотрим рис. 10.8( b), где главное подпространство натянуто на один вектор b. С геометрической точки зрения, нахождение оптимальных координат z соот - ветствует нахождению представления линейной проекции относительно b, которая минимизирует расстояние между – x. Из рис. 10.8( b) ясно, что это будет ортогональная проекция, и ниже мы покажем именно это. Предположим, что U ⊆ D есть ОНБ ( b1, , bM). Чтобы найти оптимальные координаты zm относительно этого базиса, нам потребуются частные произ - водные (10.30 a) (10.30 b) (10.30 c)\n--- Страница 414 ---\n414 Глава 10. Снижение размерности с помощью анализа главных компонент (a) Расстояния || x – x || для некоторого x = z1b ∈ U = span[b]; см. (b) для настройкиbUx~ (b) Вектор x, который минимизирует расстояние на (a), является его ортогональной проекцией на U. Координата проекции x относительно базисного вектора b , который охватывает U, является множителем, который нам нужен для масштабирования b, чтобы «дотянуться» до x0,00,0 0,50,5 –1,0 –0,5–0,5 1,01,0 1,51,5 2,02,02,5 x1x2 1,251,501,752,002,252,502,753,003,25|| x – x ||~ ~~ ~ ~~0,0 0,5 –1,0 –0,5 1,0 1,5 2,0 x1 Рис. 10.8. Оптимальная проекция вектора x ∈ 2 на одномерное подпространство (продолжение рис. 10.7). ( a) Расстояния для некоторого ∈ U. (b) Ортогональная проекция и оптимальные координаты для i = 1, , M, так что получаем (10.31 a) , (10.31 b) так как . Установка этой частной производной на 0 сразу же дает опти - мальные координаты (10.32) для i = 1, , M и n = 1, , N. Это означает, что оптимальные координаты zin про- екции являются координатами ортогональной проекции (раздел 3.8) ис - ходной точки данных xn на одномерное подпространство, натянутое на bi. Сле- довательно: zОптимальная линейная проекция точки xn является ортогональной.\n--- Страница 415 ---\n10.3. Проекционная перспектива 415 zКоординаты относительно базиса ( b1, , bM) являются координатами ортогональной проекции xn на главное подпространство. zОртогональная проекция — лучшее линейное отображение с учетом цели (10.29). zzКоординаты ζm точки x в (10.26) и координаты zm точки в (10.27) должны быть идентичны для m = 1, , M, поскольку U⊥ = span [ bM+1, , bD] является ортогональным дополнением (раздел 3.6) к U = span [ b1, , bM]. ПРИМЕЧАНИЕ Кратко напомним ортогональные проекции из раздела 3.8. Если ( b1, , bD) — ортонормированный базис D, то (10.33) — ортогональная проекция x на подпространство, натянутое на j-й базисный вектор, — координата этой проекции относительно базисного вектора bj, который охватывает это подпространство1, поскольку zjbj = . Рисунок 10.8( b) иллюстрирует эту настройку. В более общем смысле, если мы стремимся проецировать на M-мерное подпро - странство D, мы получаем ортогональную проекцию x на M-мерное подпро - странство с ортонормированными базисными векторами b1, , bM как (10.34) где мы определили B : = [b1, , bM] ∈ D×M. Координаты этой проекции относи - тельно упорядоченного базиса ( b1, , bM) равны z: = BTx, как описано в разделе 3.8. Мы можем думать о координатах как о представлении спроецированного век - тора в новой системе координат, определяемой ( b1, , bM). Заметим, что хотя ∈ D, нам нужно только M координат z1, , zM для представления этого век - тора; остальные координаты D − M относительно базисных векторов ( bM+1, , bD) всегда равны 0.  До сих пор мы показали, что для данного ОНБ мы можем найти оптимальные координаты с помощью ортогональной проекции на главное подпространство. Далее мы определим, какой базис является наилучшим. 10.3.3. Нахождение базиса главного подпространства Для определения базисных векторов b1, , bM главного подпространства мы перефразируем функцию потерь (10.29), используя полученные до сих пор 1 — координата ортогональной проекции x на подпространство, натянутое на bj.\n--- Страница 416 ---\n416 Глава 10. Снижение размерности с помощью анализа главных компонент результаты. Это упростит поиск базисных векторов. Чтобы переформулировать функцию потерь, мы используем наши предыдущие результаты и получаем (10.35) Теперь мы используем симметрию скалярного произведения, которая дает (10.36) Поскольку в общем случае мы можем записать исходную точку данных xn как линейную комбинацию всех базисных векторов, то (10.37 a) (10.37 b) где мы разбиваем сумму с D членами на сумму по M и сумму по D − M слагаемых. С этим результатом мы находим, что вектор смещения xn – , то есть вектор разности между исходной точкой данных и ее проекцией равен (10.38 a) (10.38 b) Это означает, что разница — это в точности проекция точки данных на ортого - нальное дополнение главного подпространства: мы идентифицируем матрицу в (10.38 a) как матрицу проекции, которая выполняет эту проекцию. Следовательно, вектор смещения xn – лежит в подпространстве, ортогональ - ном главному подпространству, как показано на рис. 10.9. ПРИМЕЧАНИЕ В (10.38 a) мы видели, что матрица проекции, проецирующая x на , имеет вид . (10.39) По построению это сумма матриц первого ранга, и мы видим, что BBT симметрична и имеет ранг M. Следовательно, средний квадрат ошибки восста - новления также может быть записан как\n--- Страница 417 ---\n10.3. Проекционная перспектива 417 (10.40 a) (10.40 b) U⊥ U 00 5 –5–6–4–24 26 x1x2 Рис. 10.9. Ортогональные векторы проекции и смещения. При проецировании точек данных x n (черные кружки) на подпространство U 1 мы получаем (светлым). Вектор смещения – xn полностью лежит в ортогональном дополнении U2 к U1 Нахождение ортонормированных базисных векторов b1, , bM, которые мини - мизируют разницу между исходными данными xn и их проекциями , эквива - лентно поиску наилучшего приближения BBT ранга M единичной матрицы I (раздел 4.6)1.  Теперь у нас есть все инструменты для переформулирования функции потерь (10.29): (10.41) Теперь мы явно вычисляем квадрат нормы и используем тот факт, что bj обра - зуют ОНБ, что дает (10.42 a) (10.42 b) 1 PCA находит наилучшее приближение ранга M единичной матрицы.\n--- Страница 418 ---\n418 Глава 10. Снижение размерности с помощью анализа главных компонент где мы использовали симметрию скалярного произведения на последнем шаге, чтобы записать . Теперь меняем местами суммы и получаем (10.43 a) (10.43 b) где мы воспользовались тем свойством, что оператор следа tr(·) (4.18) является линейным и инвариантным по отношению к циклическим перестановкам его аргументов. Поскольку мы предположили, что наш набор данных центрирован, то есть [] = 0, мы идентифицируем S как ковариационную матрицу данных. Поскольку матрица проекции в (10.43 b) построена как сумма матриц первого ранга, она сама имеет ранг D – M. Уравнение (10.43 a) подразумевает, что мы можем сформулировать средний квадрат ошибки1 восстановления эквивалентно как ковариационную матрицу данных, спроецированную на ортогональное дополнение главного подпространства. Таким образом, минимизация среднего квадрата ошибки восстановления эквивалентна минимизации дисперсии данных при проецировании на подпространство, которое мы игнорируем, то есть ортогональное дополнение к главному подпространству. Точно так же мы максимизируем дисперсию проекции, которую мы сохраняем в главном подпространстве, которое непосредственно связывает потерю проекции с формулировкой PCA с максимальной дисперсией, обсуждаемой в разделе 10.2. Но тогда это также означает, что мы получим то же решение, что и для перспекти - вы максимальной дисперсии2. Поэтому мы опускаем вывод, который идентичен приведенному в разделе 10.2, и суммируем полученные ранее результаты в свете проекционной перспективы. Средний квадрат ошибки восстановления при про - ецировании на M-мерное главное подпространство составляет (10.44) где λj — собственные значения ковариационной матрицы данных. Следователь - но, чтобы минимизировать (10.44), нам нужно выбрать наименьшие собственные 1 Минимизация среднего квадрата ошибки восстановления эквивалентна минимизации проекции ковариационной матрицы данных на ортогональное дополнение главного подпространства. 2 Минимизация средней квадратичной ошибки восстановления эквивалентна макси - мальному увеличению дисперсии прогнозируемых данных.\n--- Страница 419 ---\n10.4. Вычисление собственного вектора и приближения низкого ранга 419 значения D – M, из чего следует, что соответствующие им собственные векторы являются базисом ортогонального дополнения главного подпространства. Сле - довательно, это означает, что в базис главного подпространства входят собствен - ные векторы b1, , bM, которые связаны с наибольшими M собственными значе - ниями ковариационной матрицы данных. Пример 10.3 (встраивание цифр MNIST) На рис. 10.10 показаны обучающие данные цифр «0» и «1» MNIST, встро - енные в векторное подпространство, охватываемое первыми двумя глав - ными компонентами. Мы наблюдаем относительно четкое разделение между «0» (темные точки) и «1» (светлые точки), и мы видим вариации внутри каждого отдельного кластера. Четыре вложения цифр «0» и «1» в главное подпространство показаны линиями вместе с соответствующи - ми исходными цифрами. Рисунок показывает, что вариация в наборе «0» значительно больше, чем вариация в наборе «1». Рис. 10.10. Встраивание цифр 0 (темным) и 1 (светлым) MNIST в двумерное главное подпространство с использованием PCA. Четыре вложения цифр «0» и «1» в главное подпространство показаны линиями вместе с соответствующими исходными цифрами 10.4. ВЫЧИСЛЕНИЕ СОБСТВЕННОГО ВЕКТОРА И ПРИБЛИЖЕНИЯ НИЗКОГО РАНГА В предыдущих разделах мы получили базис главного подпространства как собственные векторы, которые связаны с наибольшими собственными значе - ниями ковариационной матрицы данных (10.45) X = [x1, , xN] ∈ D×N. (10.46)\n--- Страница 420 ---\n420 Глава 10. Снижение размерности с помощью анализа главных компонент Обратите внимание, что X — это матрица D × N, то есть это транспонированная матрица «типичных» данных (Bishop, 2006; Murphy, 2012). Чтобы получить собственные значения (и соответствующие собственные векторы1) S, мы можем использовать два подхода: zМы выполняем собственное разложение (раздел 4.2) и вычисляем собствен - ные значения и собственные векторы S напрямую. zzМы используем разложение по сингулярным числам (раздел 4.5). По сколь - ку S симметрична и разлагается на XXT (без учета множителя ), собствен - ные значения S являются квадратами сингулярных значений X. Более конкретно, сингулярное разложение (SVD) X определяется как (10.47) где U ∈ D×D и VT ∈ N×N — ортогональные матрицы, а Σ ∈ D×N — матрица, един - ственными ненулевыми элементами которой являются сингулярные значения σii ≥ 0. Отсюда следует, что (10.48) Используя результаты раздела 4.5, мы получаем, что столбцы U являются соб - ственными векторами XXT (и, следовательно, S). Кроме того, собственные значения λd матрицы S связаны с сингулярными значениями X через (10.49) Эта связь между собственными значениями S и сингулярными значениями X обеспечивает связь между представлением максимальной дисперсии (раз - дел 10.2) и разложением по сингулярным значениям. 10.4.1. PCA с использованием матричных приближений низкого ранга Чтобы максимизировать дисперсию проецируемых данных (или минимизиро - вать среднюю квадратичную ошибку восстановления), PCA выбирает столбцы U в (10.48) как собственные векторы, которые связаны с M наибольшими соб - ственными значениями ковариационной матрицы данных S, так что мы иден - 1 Используйте собственное разложение либо SVD для вычисления собственных век- торов.\n--- Страница 421 ---\n10.4. Вычисление собственного вектора и приближения низкого ранга 421 тифицируем U как матрицу проекции B в (10.3), которая проецирует исходные данные на подпространство меньшей размерности — размерности M. Теорема Эккарта — Юнга (теорема 4.25 в разделе 4.6) предлагает прямой способ оценки низкоразмерного представления. Рассмотрим наилучшее приближение X ранга M , (10.50) где — спектральная норма, определенная в (4.93). Теорема Эккарта — Юнга утверждает, что задается усечением SVD в сингулярном значении top- M. Другими словами, получаем (10.51) с ортогональными матрицами UM := [u1, , uM] ∈ D×M и VM : = [v1, , vM] ∈ N×M и диагональной матрицей ΣM ∈ M×M, диагональные элементы которой являют - ся M наибольшими сингулярными значениями матрицы X. 10.4.2. Практические аспекты Нахождение собственных значений и собственных векторов также важно в дру- гих фундаментальных методах машинного обучения, которые требуют разло - жения матриц. Теоретически, как мы обсуждали в разделе 4.2, мы можем найти собственные значения как корни характеристического полинома. Однако для матриц больше 4 × 4 это невозможно, потому что нам нужно будет найти корни многочлена степени 5 или выше. Однако теорема Абеля — Руффини (Ruffini, 1799; Abel, 1826) утверждает, что не существует алгебраического решения этой проблемы для многочленов степени 5 или выше. Поэтому на практике мы на - ходим собственные значения или сингулярные значения, используя итераци - онные методы, которые реализованы во всех современных пакетах линейной алгебры1. Во многих приложениях (например, в PCA, представленном в этой главе) нам требуется только несколько собственных векторов. Было бы расточительно вычислять полное разложение, а затем отбрасывать все собственные векторы с собственными значениями, превышающими несколько первых. Оказывается, что если нас интересуют только первые несколько собственных векторов (с наи- большими собственными значениями), то итерационные процессы, которые напрямую оптимизируют эти собственные векторы, в вычислительном отноше - нии более эффективны, чем полное собственное разложение (или SVD). В край- нем случае, когда нужен только первый собственный вектор, очень эффективен простой метод, называемый методом степенных итераций . Метод степенных 1 См. np.linalg.eigh или np.linalg.svd .\n--- Страница 422 ---\n422 Глава 10. Снижение размерности с помощью анализа главных компонент итераций выбирает случайный вектор x0, который не находится в нулевом про - странстве S, и следует за итерацией (10.52) Это означает, что вектор xk умножается на S на каждой итерации, а затем нор - мализуется, то есть мы всегда имеем . Эта последовательность векторов сходится к собственному вектору, связанному с наибольшим собственным значением S 1. Исходный алгоритм Google PageRank (Page et al., 1999) исполь - зует такой алгоритм для ранжирования веб-страниц на основе их гиперссылок. 10.5. PCA В БОЛЬШИХ РАЗМЕРАХ Чтобы выполнить PCA, нам нужно вычислить ковариационную матрицу данных. В размерностях D ковариационная матрица данных представляет собой матри - цу D × D. Вычисление собственных значений и собственных векторов этой матрицы требует больших затрат вычислительных ресурсов, поскольку масшта - бируется кубическим образом в D. Следовательно, PCA, как мы обсуждали ранее, будет невозможным в очень больших измерениях. Например, если наши xn — это изображения с 10 000 пикселей (например, изображения 100 × 100 пик- селей), нам нужно будет вычислить собственное разложение ковариационной матрицы 10 000 × 10 000. Далее мы предлагаем решение этой проблемы для случая, когда у нас значительно меньше точек данных, чем размеров, то есть N << D. Предположим, у нас есть центрированный набор данных x1, , xN, xn ∈ D. Тогда ковариационная матрица данных имеет вид (10.53) где X = [x1, , xN] — матрица размера D × N, столбцы которой являются точками данных. Теперь предположим, что N << D, то есть количество точек данных меньше раз - мерности данных. Если нет повторяющихся точек данных, ранг ковариационной матрицы S равен N, поэтому она имеет D – N + 1 собственных значений, которые равны 0. Интуитивно это означает, что есть некоторые избыточности. Далее мы воспользуемся этим и превратим ковариационную матрицу D × D в ковариаци - онную матрицу N × N, все собственные значения которой положительны. 1 Если S обратима, достаточно убедиться, что x0 ≠ 0.\n--- Страница 423 ---\n10.5. PCA в больших размерах 423 В PCA мы получили уравнение для собственных векторов Sbm = λmbm, m = 1, , M, (10.54) где bm — базисный вектор главного подпространства. Давайте немного пере - пишем это уравнение: с S, определенным в (10.53), мы получаем (10.55) Умножим теперь XT ∈ N×D из левой части, что дает (10.56) и мы получаем новое уравнение «собственный вектор / собственное значение»: λm остается собственным значением, что подтверждает наши результаты из раз - дела 4.5.3 о том, что ненулевые собственные значения XXT равны ненулевым собственным значениям XTX. Мы получаем собственный вектор матрицы XTX ∈ N×N, ассоциированный с λm как cm : = XTbm. Предполагая, что у нас нет повторяющихся точек данных, эта матрица имеет ранг N и обратима. Это также означает, что XTX имеет те же (ненулевые) собственные значения, что и ко- вариационная матрица данных S. Но теперь это матрица размера N × N, так что мы можем вычислять собственные значения и собственные векторы гораздо эффективнее, чем для исходной ковариационной матрицы данных D × D. Теперь, когда у нас есть собственные векторы XTX, мы собираемся восстано - вить исходные собственные векторы, которые нам все еще нужны для PCA. В настоящее время мы знаем собственные векторы XTX. Если мы умножим слева наше уравнение «собственное значение / собственный вектор» на X, мы получим (10.57) и мы снова восстанавливаем матрицу ковариации данных. Теперь это также означает, что мы восстанавливаем Xcm как собственный вектор S. ПРИМЕЧАНИЕ Если мы хотим применить алгоритм PCA, который мы об - суждали в разделе 10.6, нам нужно нормализовать собственные векторы Xcm матрицы S так, чтобы они имели норму 1. \n--- Страница 424 ---\n424 Глава 10. Снижение размерности с помощью анализа главных компонент 10.6. КЛЮЧЕВЫЕ ШАГИ PCA НА ПРАКТИКЕ Далее мы рассмотрим отдельные шаги PCA на рабочем примере, который кра - тко представлен на рис. 10.11. Нам дан двумерный датасет (рис. 10.11( a)), и мы хотим использовать PCA, чтобы спроецировать его на одномерное подпростран - ство. 1. Вычитание среднего значения Мы начинаем с центрирования данных, вычисляя среднее значение μ дата- сета и вычитая его из каждой отдельной точки данных. Это гарантирует, что датасет имеет среднее значение 0 (рис. 10.11( b)). Вычитание среднего не обязательно, но снижает риск вычислительных проблем. 0 5 (/equal.g00C8) /h.g006A“/period.g00B2/percent.g00BA/.notdef.g0105…/slash.g00A9/L.g00AE /.notdef.g0105/equal.g00C8/two.g0088/equal.g00C8“/yright/two.g00880 5 (b) /x.g0062/equal.g00C8/.notdef.g0104 1: /zero.g00A1/yright…/two.g0088/exclam.g00AF/comma.g00D2/exclam.g00AF/percent.g00BA/quotedbl.g006D/equal.g00C8…/comma.g00D2/yright /C.g00B9/three.g0082/two.g0088/yright/.notdef.g00E4 /quotedbl.g006D/slash.g00A9/.notdef.g0107/comma.g00D2/two.g0088/equal.g00C8…/comma.g00D2/space.g00AB “/exclam.g00AF/yright/.notdef.g0105…/yright/.notdef.g0104/percent.g00BA /comma.g00D2ƒ /asterisk.g007D/equal.g00C8›/.notdef.g0105/percent.g00BA/L.g00AE /two.g0088/percent.g00BA/.notdef.g0107/asterisk.g007D/comma.g00D2 /.notdef.g0105/equal.g00C8……/slash.g00A9/period.g00B20 5 (c) /x.g0062/equal.g00C8/.notdef.g0104 2: /d.g0069/yright/.notdef.g00E3/yright…/comma.g00D2/yright …/equal.g00C8 “/two.g0088/equal.g00C8…/.notdef.g0105/equal.g00C8/exclam.g00AF/two.g0088…/percent.g00BA/yright /percent.g00BA/two.g0088/asterisk.g007D/.notdef.g00E3/percent.g00BA…/yright…/comma.g00D2/yright, /.notdef.g0107/two.g0088/percent.g00BA/K.g00AD/slash.g00A9 /percent.g00BA“/quotedbl.g006D/percent.g00BA/K.g00AD/percent.g00BA/.notdef.g0105/comma.g00D2/two.g0088/.notdef.g0109 /yright/.notdef.g0105/comma.g00D2…/comma.g00D2/.notdef.g0106/three.g0082 /.notdef.g0105/equal.g00C8……/slash.g00A9/period.g00B2. /d.g0069/equal.g00C8……/slash.g00A9/yright /comma.g00D2/.notdef.g00E4/yright/.notdef.g010A/two.g0088 /.notdef.g0105/comma.g00D2“/C.g00B9/yright/exclam.g00AF“/comma.g00D2/.notdef.g010A 1 /C.g00B9/percent.g00BA /asterisk.g007D/equal.g00C8›/.notdef.g0105/percent.g00BA/L.g00AE /percent.g00BA“/comma.g00D2 0 5 (d) /x.g0062/equal.g00C8/.notdef.g0104 3. /b.g007B/slash.g00A9/.notdef.g0107/comma.g00D2“/.notdef.g00E3/yright…/comma.g00D2/yright “/percent.g00BA/K.g00AD“/two.g0088/quotedbl.g006D/yright……/slash.g00A9/period.g00B2 ƒ…/equal.g00C8/.notdef.g0107/yright…/comma.g00D2/L.g00AE /comma.g00D2 “/percent.g00BA/K.g00AD“/two.g0088/quotedbl.g006D/yright……/slash.g00A9/period.g00B2 /quotedbl.g006D/yright/asterisk.g007D/two.g0088/percent.g00BA/exclam.g00AF/percent.g00BA/quotedbl.g006D (“/two.g0088/exclam.g00AF/yright/.notdef.g00E3/asterisk.g007D/comma.g00D2) /asterisk.g007D/percent.g00BA/quotedbl.g006D/equal.g00C8/exclam.g00AF/comma.g00D2/equal.g00C8/.notdef.g0106/comma.g00D2/percent.g00BA……/percent.g00BA/L.g00AE /.notdef.g00E4/equal.g00C8/two.g0088/exclam.g00AF/comma.g00D2/.notdef.g0106/slash.g00A9 /.notdef.g0105/equal.g00C8……/slash.g00A9/period.g00B2 (/period.g00AA/.notdef.g00E3/.notdef.g00E3/comma.g00D2/C.g00B9“)0 5 (e) /x.g0062/equal.g00C8/.notdef.g0104 4. /o.g0081/exclam.g00AF/percent.g00BA/yright/asterisk.g007D/.notdef.g0106/comma.g00D2/space.g00AB /.notdef.g0105/equal.g00C8……/slash.g00A9/period.g00B2 /quotedbl.g006D /.notdef.g0104/.notdef.g00E3/equal.g00C8/quotedbl.g006D…/percent.g00BA/yright /C.g00B9/percent.g00BA/.notdef.g0105/C.g00B9/exclam.g00AF/percent.g00BA“/two.g0088/exclam.g00AF/equal.g00C8…“/two.g0088/quotedbl.g006D/percent.g00BA0 5 (f) /n.g007C/two.g0088/.notdef.g00E4/yright…/equal.g00C8 “/two.g0088/equal.g00C8…/.notdef.g0105/equal.g00C8/exclam.g00AF/two.g0088/comma.g00D2ƒ/equal.g00C8/.notdef.g0106/comma.g00D2/comma.g00D2 /comma.g00D2 /C.g00B9/yright/exclam.g00AF/yright/.notdef.g00E4/yright/question.g009D/yright…/comma.g00D2/yright /C.g00B9/exclam.g00AF/percent.g00BA/.notdef.g0104…/percent.g00BAƒ/comma.g00D2/exclam.g00AF/three.g0082/yright/.notdef.g00E4/slash.g00A9/period.g00B2 /.notdef.g0105/equal.g00C8……/slash.g00A9/period.g00B2 /percent.g00BA/K.g00AD/exclam.g00AF/equal.g00C8/two.g0088…/percent.g00BA /quotedbl.g006D /comma.g00D2“/period.g00B2/percent.g00BA/.notdef.g0105…/percent.g00BA/yright /C.g00B9/exclam.g00AF/percent.g00BA“/two.g0088/exclam.g00AF/equal.g00C8…“/two.g0088/quotedbl.g006D/percent.g00BA /.notdef.g0105/equal.g00C8……/slash.g00A9/period.g00B2 /comma.g00D2ƒ (a)x10,0 /dollar.g00DC2,52,55,0x20,0 /dollar.g00DC2,52,55,0x2 0,0 /dollar.g00DC2,52,55,0x20,0 /dollar.g00DC2,52,55,0x2 0,0 /dollar.g00DC2,52,55,0x20,0 /dollar.g00DC2,52,55,0x2 x1x1x1x1x1 Рис. 10.11. Этапы PCA. ( а) исходный датасет; ( b) центрирование; ( c) разделение на стандартное отклонение; ( d) собственный состав; ( e) проекция; ( f) отображение обратно в исходное пространство данных\n--- Страница 425 ---\n10.6. Ключевые шаги PCA на практике 425 2. Стандартизация Разделите точки данных на стандартное отклонение σd датасета для каждого измерения d = 1, , D. Теперь данные не содержат единиц измерения и имеют отклонение 1 по каждой оси, что обозначено двумя стрелками на рис. 10.11( d). На этом этапе стандартизация данных завершается. 3. Собственное разложение ковариационной матрицы Вычислите матрицу ковариации данных, ее собственные значения и соот- ветствующие собственные векторы. Поскольку ковариационная матрица симметрична, спектральная теорема (теорема 4.15) утверждает, что мы можем найти ОНБ собственных векторов. На рис. 10.11( d) собственные векторы масштабированы по величине соответствующего собственного значения. Более длинный вектор охватывает главное подпространство, которое мы обозначим через U. Ковариационная матрица данных представлена эллипсом. 4. Прогноз Мы можем спроецировать любую точку данных x* ∈ D на главное подпро - странство: чтобы сделать это правильно, нам нужно стандартизировать x*, используя среднее значение μd и стандартное отклонение σd обучающих данных в d-м измерении соответственно, так что , (10.58) где — d-я компонента x*. Получим проекцию как (10.59) с координатами z* = BTx* (10.60) относительно базиса главного подпространства. Здесь B — матрица, которая содержит собственные векторы, которые связаны с наибольшими собствен - ными значениями ковариационной матрицы данных в виде столбцов. PCA возвращает координаты (10.60), а не проекции x*. После стандартизации нашего набора данных (10.59) дает только прогнозы в контексте стандартизированного набора данных. Чтобы получить нашу про - екцию в исходном пространстве данных (то есть до стандартизации), нам нуж - но отменить стандартизацию (10.58) и умножить на стандартное отклонение перед добавлением среднего, чтобы мы получили . (10.61)\n--- Страница 426 ---\n426 Глава 10. Снижение размерности с помощью анализа главных компонент Рисунок 10.11( f) иллюстрирует проекцию в исходное пространство данных. Пример 10.4 (цифры MNIST: реконструкция) Далее мы применим PCA к набору данных цифр MNIST, который содер - жит 60 000 примеров рукописных цифр от 0 до 9. Каждая цифра пред - ставляет собой изображение размером 28 × 28, то есть содержит 784 пик- селя, чтобы мы могли интерпретировать каждое изображение в этот датасет как вектор x ∈ 784. Примеры этих цифр показаны на рис. 10.3. В целях иллюстрации мы применяем PCA к подмножеству цифр MNIST и сосредоточиваемся на цифре «8». Мы использовали 5389 обучающих образов цифры «8» и определили главное подпространство, как подробно описано в этой главе. Затем мы использовали изученную матрицу про - екций для восстановления набора тестовых изображений, который по - казан на рис. 10.12. В первой строке рис. 10.12 показан набор из четырех исходных цифр из тестового набора. В следующих строках показаны реконструкции именно этих цифр при использовании главного подпро - странства размерностей 1, 10, 100 и 500 соответственно. Мы видим, что даже с одномерным основным подпространством мы получаем довольно приличную реконструкцию исходных цифр, которая, однако, является размытой и общей. С увеличением числа главных компонент (ГК) рекон - струкции становятся более четкими, и учитывается все больше деталей. С 500 главными компонентами мы фактически получаем почти идеальную реконструкцию. Если бы мы выбрали 784 ГК, мы бы восстановили точную цифру без потери сжатия. /h.g006A“/period.g00B2/percent.g00BA/.notdef.g0105…/slash.g00A9 /yright /c.g0080/j.g007A:1 /c.g0080/j.g007A:10 /c.g0080/j.g007A:100 /c.g0080/j.g007A:500 Рис. 10.12. Влияние увеличения количества главных компонент на реконструкцию\n--- Страница 427 ---\n10.7. Латентная переменная 427 0 200 400 600 800 /j.g007A/percent.g00BA/.notdef.g00E3/comma.g00D2/.notdef.g0107/yright“/two.g0088/quotedbl.g006D/percent.g00BA /c.g0080/j.g007A0100200300400500/q.g0076/exclam.g00AF/yright/.notdef.g0105…/space.g00AB/space.g00AB /asterisk.g007D/quotedbl.g006D/equal.g00C8/.notdef.g0105/exclam.g00AF/equal.g00C8/two.g0088/comma.g00D2/.notdef.g0107…/equal.g00C8/space.g00AB /percent.g00BA/.notdef.g0108/comma.g00D2/K.g00AD/asterisk.g007D/equal.g00C8 /quotedbl.g006D/percent.g00BA““/two.g0088/equal.g00C8…/percent.g00BA/quotedbl.g006D/.notdef.g00E3/yright…/comma.g00D2/space.g00AB Рис. 10.13. Средняя квадратичная ошибка восстановления в зависимости от количества главных компонент. Средний квадрат ошибки восстановления — это сумма собственных значений в ортогональном дополнении к главному подпространству На рис. 10.13 показан средний квадрат ошибки восстановления, равный (10.62) как функция количества главных компонент M. Мы видим, что важность главных компонент быстро падает, и лишь незначительный выигрыш может быть достигнут за счет добавления дополнительных ГК. Это в точ- ности совпадает с нашим наблюдением на рис. 10.5, где мы обнаружили, что большая часть дисперсии прогнозируемых данных улавливается только несколькими главными компонентами. Имея около 550 ГК, мы можем практически полностью восстановить обучающие данные, содер - жащие цифру «8» (некоторые пиксели вокруг границ не показывают изменений в наборе данных, поскольку они всегда черные). 10.7. ЛАТЕНТНАЯ ПЕРЕМЕННАЯ В предыдущих разделах мы вывели PCA без какого-либо понятия вероятност - ной модели, используя подход с максимальной дисперсией и проекцией. С одной стороны, этот подход может быть привлекательным, поскольку он позволяет нам обойти все математические трудности, связанные с теорией вероятностей, но с другой стороны, вероятностная модель предоставит нам больше гибкости и полезного понимания. В частности, вероятностная модель будет:\n--- Страница 428 ---\n428 Глава 10. Снижение размерности с помощью анализа главных компонент zвключать в себя функцию правдоподобия, и мы можем явно иметь дело с за- шумленными наблюдениями (которые мы даже не обсуждали ранее); zпозволять нам провести сравнение байесовских моделей с помощью предель - ного правдоподобия, как описано в разделе 8.6; zрассматривать PCA как генеративную модель, которая позволяет нам моде - лировать новые данные; zпозволять нам напрямую подключаться к связанным алгоритмам; zработать со случайно пропущенными измерениями данных, применяя тео - рему Байеса; zдавать нам представление о новизне новой точки данных; zдавать нам принципиальный способ расширить модель, например до смеси моделей PCA; zиспользовать PCA, полученный нами в предыдущих разделах, как особый случай; zобеспечивать полностью байесовский подход, исключив параметры модели. Вводя латентную переменную z ∈ M с непрерывным знаком, можно сформу - лировать PCA как вероятностную модель латентных переменных. Типпинг и Бишоп (Tipping and Bishop, 1999) предложили эту модель латентных пере - менных как вероятностный PCA (PPCA). PPCA решает большинство из вы - шеупомянутых проблем, и решение PCA, которое мы получили путем макси - мизации дисперсии в проецируемом пространстве или минимизации ошибки восстановления, получено как частный случай оценки максимального правдо - подобия в настройке без шума. 10.7.1. Генеративный процесс и вероятностная модель В PPCA мы явно записываем вероятностную модель для уменьшения линейной размерности. Для этого мы предполагаем непрерывную латентную переменную z ∈ M со стандартным нормальным априорным p (z) =  (0, I) и линейной за - висимостью между латентными переменными и наблюдаемыми данными x, где x = Bz + μ + ε ∈ D, (10.63) где ∼  (0, σ2I) — гауссов шум наблюдения, а B ∈ D × M и μ ∈ D описывают ли - нейное/аффинное отображение латентных переменных в наблюдаемые. Таким образом, PPCA связывает латентные и наблюдаемые переменные через p (x | z, B, μ, σ2) =  (x | Bz + μ, σ2I). (10.64) В целом PPCA вызывает следующий процесс генерации:\n--- Страница 429 ---\n10.7. Латентная переменная 429 zn ∼  (z | 0, I); (10.65) xn | zn ∼  (x | Bzn + μ, σ2I). (10.66) Чтобы сгенерировать точку данных, которая типична для данных параметров модели, мы следуем схеме наследственного отбора: сначала мы выбираем ла - тентную переменную zn из p(z). Затем мы используем zn в (10.64) для выборки точки данных, обусловленной выбранным zn, то есть xn ~ p(x | zn, B, μ, σ2). Этот процесс генерации позволяет нам записать вероятностную модель (то есть совместное распределение всех случайных величин (раздел 8.4)) как p (x, z | B, μ, σ2) = p(x | z, B, μ, σ2)p(z), (10.67) что сразу дает начало графической модели на рис. 10.14, использующей резуль - таты из раздела 8.5. n = 1, . . . , NB σµzn xn Рис. 10.14. Графическая модель для вероятностного PCA. Наблюдения xn явно зависят от соответствующих скрытых переменных zn ∼  (0, I). Параметры модели B, μ и параметр правдоподобия σ являются общими для всего набора данных ПРИМЕЧАНИЕ Обратите внимание на направление стрелки, которая соеди - няет латентные переменные z и наблюдаемые данные x: стрелка указывает от z к x, что означает, что модель PPCA предполагает скрытую причину более низкой размерности z для наблюдений x высокой размерности. В конце концов, мы, очевидно, заинтересованы в том, чтобы узнать что-нибудь о z с учетом некото - рых наблюдений. Для этого мы применим байесовский инференс, чтобы неявно «перевернуть» стрелку и перейти от наблюдений к латентным переменным.  Пример 10.5 (генерация новых данных с использованием латентных переменных) На рис. 10.15 показаны скрытые координаты цифр «8» MNIST, найденные PCA при использовании двумерного главного подпространства (точки на рисунке). Мы можем запросить любой вектор z* в этом скрытом про - странстве и сгенерировать изображение = Bz*, напоминающее циф -\n--- Страница 430 ---\n430 Глава 10. Снижение размерности с помощью анализа главных компонент ру «8». Мы показываем восемь таких сгенерированных изображений с соответствующим им скрытым пространственным представлением. В зависимости от того, где мы запрашиваем скрытое пространство, сге - нерированные изображения выглядят по-разному (форма, поворот, размер и т. д.). Если мы откажемся от обучающих данных, мы увидим все больше и больше артефактов, например верхние левые и верхние правые цифры. Обратите внимание, что внутренняя размерность этих сгенерированных изображений равна только двум. Рис. 10.15. Создание новых цифр MNIST. Латентные переменные z можно использовать для генерации новых данных = Bz. Чем ближе мы приближаемся к обучающим данным, тем реалистичнее получаются данные 10.7.2. Правдоподобие и совместное распределение Используя результаты главы 6, мы получаем правдоподобие1 этой вероятност - ной модели путем интегрирования латентной переменной z (раздел 8.4.3), так что (10.68 a) (10.68 b) Из раздела 6.5 мы знаем, что решением этого интеграла является гауссово рас - пределение со средним значением x[x] = z[Bz + μ] + ε[ε] = μ (10.69) 1 Правдоподобие не зависит от латентных переменных z.\n--- Страница 431 ---\n10.7. Латентная переменная 431 и с ковариационной матрицей  [x] = z[Bz + μ] + ε[ε] = z[Bz] + σ2I = (10.70 a) = Bz[z]BT + σ2I = BBT + σ2I. (10.70 b) Правдоподобие в (10.68 b) может использоваться для оценки максимального правдоподобия или MAP параметров модели. ПРИМЕЧАНИЕ Мы не можем использовать условное распределение в (10.64) для оценки максимального правдоподобия, поскольку оно все еще зависит от латентных переменных. Функция правдоподобия, которая нам нужна для оцен - ки максимального правдоподобия (или MAP), должна быть только функцией данных x и параметров модели, но не должна зависеть от латентных переменных.  Из раздела 6.5 мы знаем, что гауссова случайная величина z и ее линейное/ аффинное преобразование x = Bz совместно распределены по Гауссу. Мы уже знаем маргиналы p(z) =  (z | 0, I) и p (x) =  (x | μ, BBT + σ2I). Отсутствующая кросс-ковариация выражается как Cov[x, z] = Covz[Bz + μ] = BCovz[z, z] = B. (10.71) Следовательно, вероятностная модель PPCA, то есть совместное распределение скрытых и наблюдаемых случайных величин, явно задается выражением (10.72) со средним вектором длины D + M и ковариационной матрицей размера (D + M) × (D + M). 10.7.3. Апостериорное распределение Совместное гауссово распределение p (x, z | B, μ, σ2) в (10.72) позволяет нам определить апостериорное распределение p (z | x) немедленно, применяя пра - вила гауссова процесса из раздела 6.5.1. Тогда апостериорное распределение латентной переменной с учетом наблюдения x равно p (z | x) =  (z | m, C); (10.73) m = BT(BBT + σ2I)–1(x − μ); (10.74) C = I − BT(BBT+ σ2I)–1B. (10.75)\n--- Страница 432 ---\n432 Глава 10. Снижение размерности с помощью анализа главных компонент Обратите внимание, что апостериорная ковариация не зависит от наблюдаемых данных x. Для нового наблюдения x* в пространстве данных мы используем (10.73) для определения апостериорного распределения соответствующей ла - тентной переменной z*. Ковариационная матрица C позволяет нам оценить, насколько надежно вложение. Ковариационная матрица C с малым детерми - нантом (который измеряет объемы) говорит нам, что скрытое вложение z* до- статочно надежно. Если мы получим апостериорное распределение p(z* | x*) с большой дисперсией, мы можем столкнуться с выбросом. Однако мы можем изучить это апостериорное распределение, чтобы понять, какие другие точки данных x вероятны при этом апостериорном распределении. Для этого мы ис - пользуем процесс генерации, лежащий в основе PPCA, который позволяет нам исследовать апостериорное распределение латентных переменных путем гене - рации новых данных, которые являются правдоподобными при следующем апостериорном распределении: 1. Выберите латентную переменную z* ∼ p(z | x*) из апостериорного распреде - ления по латентным переменным (10.73). 2. Выберите восстановленный вектор ∼ p(x | z*, B, μ, σ2) из (10.64). Если мы повторим этот процесс много раз, мы сможем исследовать апостери - орное распределение (10.73) латентных переменных z* и его влияние на наблю - даемые данные. Процесс выборки эффективно выдвигает гипотезу о данных, что является правдоподобным при апостериорном распределении. 10.8. ДОПОЛНИТЕЛЬНОЕ ЧТЕНИЕ Мы получили PCA с двух точек зрения: ( а) максимизация дисперсии в проеци - руемом пространстве; и (б) минимизация средней ошибки восстановления. Однако PCA также можно интерпретировать с разных точек зрения. Подведем итоги того, что мы сделали: мы взяли данные большой размерности x ∈ D и ис- пользовали матрицу BT, чтобы найти представление z ∈ M меньшей размерности. Столбцы B являются собственными векторами ковариационной матрицы дан - ных S, которые связаны с наибольшими собственными значениями. Когда у нас есть низкоразмерное представление z, мы можем получить его многомерную версию (в исходном пространстве данных) как ∈ D, где BBT — матрица проекции. Мы также можем рассматривать PCA как линейный автокодер , как показано на рис. 10.16. Автокодер кодирует данные xn ∈ D в код zn ∈ M и декодирует их в , аналогично xn. Отображение данных в код называется кодером , а отобра - жение кода обратно в исходное пространство данных называется декодером . Если мы рассмотрим линейные отображения, где код задается как zn = BTxn ∈ M,\n--- Страница 433 ---\n10.8. Дополнительное чтение 433 и нас интересует минимизация средней квадратичной ошибки между данными xn и их реконструкцией = Bzn, n = 1, , N, мы получим (10.76) Это означает, что мы получаем ту же целевую функцию, что и в (10.29), которую мы обсуждали в разделе 10.3, так что мы получаем решение PCA, когда мини - мизируем возведенные в квадрат потери автокодирования. Если мы заменим линейное отображение PCA на нелинейное отображение, мы получим нелиней - ный автокодер. Ярким примером этого является глубокий автокодер, в котором линейные функции заменены глубокими нейронными сетями. В этом контексте кодер также известен как сеть распознавания , или сеть вывода , тогда как деко - дер также называется генератором . /h.g006A“/period.g00B2/percent.g00BA/.notdef.g0105…/slash.g00A9/L.g00AE /j.g007A/percent.g00BA/.notdef.g0105/yright/exclam.g00AF /d.g0069/yright/asterisk.g007D/percent.g00BA/.notdef.g0105/yright/exclam.g00AF/j.g007A/percent.g00BA/.notdef.g0105/comma.g00D2/exclam.g00AF/percent.g00BA/quotedbl.g006D/equal.g00C8…/comma.g00D2/yright B BTRD RMRD xz x~ Рис. 10.16. PCA можно рассматривать как линейный автокодер. Он кодирует данные x большой размерности в представление (код) меньшей размерности z ∈  M и декодирует z с помощью декодера. Декодированный вектор является ортогональной проекцией исходных данных x на M-мерное главное подпространство Другая интерпретация PCA связана с теорией информации. Мы можем рас - сматривать код как уменьшенную или сжатую версию исходной точки данных. Когда мы восстанавливаем наши исходные данные с помощью кода, мы полу - чаем не точную точку данных, а ее слегка искаженную или зашумленную версию. Это означает, что наше сжатие происходит с потерями. Интуитивно мы хотим максимизировать корреляцию между исходными данными и низкоразмерным кодом1. Более формально это относится к взаимной информации. Затем мы получили бы то же решение для PCA, которое мы обсуждали в разделе 10.3, путем максимизации взаимной информации, ключевой концепции в теории информации (MacKay, 2003). В нашем обсуждении PPCA мы предположили, что параметры модели, то есть B, μ и параметр правдоподобия σ2, известны. Типпинг и Бишоп (1999) описы - 1 Код представляет собой сжатую версию исходных данных.\n--- Страница 434 ---\n434 Глава 10. Снижение размерности с помощью анализа главных компонент вают, как получить оценки максимального правдоподобия для этих параметров в настройке PPCA (обратите внимание, что в этой главе мы используем другие обозначения). Параметры максимального правдоподобия при проецировании D-мерных данных на M-мерное подпространство равны (10.77) (10.78) (10.79) где T ∈ D × M содержит M собственных векторов ковариационной матрицы дан - ных, Λ = diag ( λ1, , λM) ∈ M × M — диагональная матрица, собственные значения которой связаны с главными осями на ее диагонали, а R ∈ M × M является про - извольной ортогональной матрицей1. Решение максимального правдоподобия BML является уникальным с точностью до произвольного ортогонального пре - образования, например мы можем умножить BML справа на любую матрицу вращения R, так что (10.78) по существу является разложением по сингулярным значениям (раздел 4.5). Схема доказательства дана Типпингом и Бишопом (1999). Оценка максимального правдоподобия для μ, приведенная в (10.77), является выборочным средним для данных. Оценка максимального правдоподобия для дисперсии шума наблюдения σ2, приведенная в (10.79), представляет собой среднюю дисперсию в ортогональном дополнении главного подпространства, то есть средняя оставшаяся дисперсия, которую мы не можем зафиксировать с помощью первых M главных компонент, рассматривается как шум наблюдения. В пределе отсутствия шума, когда σ → 0, PPCA и PCA предоставляют идентич - ные решения: поскольку матрица ковариации данных S симметрична, ее можно диагонализовать (раздел 4.4), то есть существует матрица T собственных век - торов S, поэтому S = T Λ T –1. (10.80) В модели PPCA ковариационная матрица данных является ковариационной матрицей гауссова правдоподобия p(x | B, μ, σ2), которая равна BBT + σ2I, см. (10.70 b). При σ → 0 мы получаем BBT, так что эта ковариация данных долж - 1 Матрица Λ – σ2I в (10.78) гарантированно будет положительно полуопределенной, поскольку наименьшее собственное значение матрицы ковариации данных ограниче - но снизу дисперсией шума σ2.\n--- Страница 435 ---\n10.8. Дополнительное чтение 435 на равняться ковариации данных PCA (и ее факторизации, заданной в (10.80)), так что (10.81) то есть мы получаем оценку максимального правдоподобия в (10.78) для σ = 0. Из (10.78) и (10.80) становится ясно, что (P)PCA выполняет разложение кова - риационной матрицы данных. В настройках потоковой передачи, когда данные поступают последовательно, рекомендуется использовать алгоритм максимизации итеративного ожидания (EM) для оценки максимального правдоподобия (Roweis, 1998). Чтобы определить размерность скрытых переменных (длина кода, размерность подпространства меньшей размерности, на которое мы проецируем данные), Гавиш и Донохо (Gavish and Donoho, 2014) предлагают такую эвристику: если мы можем оценить дисперсию шума σ2 данных, мы должны отбросить все син - гулярные значения, меньшие . В качестве альтернативы мы можем ис - пользовать (вложенную) перекрестную проверку (раздел 8.6.1) или критерии выбора байесовской модели (обсуждаемые в разделе 8.6.2), чтобы определить хорошую оценку внутренней размерности данных (Minka, 2001 b). Подобно нашему обсуждению линейной регрессии в главе 9, мы можем по - местить априорное распределение для параметров модели и интегрировать их. Поступая таким образом, мы ( а) избегаем точечных оценок параметров и проблем, связанных с этими точечными оценками (раздел 8.6), и (б) до- пускаем автоматический выбор подходящей размерности M скрытого про - странства. В этом байесовском PCA , который был предложен Бишопом (1999), априорное значение p(μ, B, σ2) помещается в параметры модели. Генератив - ный процесс позволяет нам интегрировать параметры модели, вместо того чтобы определять их, что решает проблемы переобучения. Поскольку эта интеграция аналитически неразрешима, Бишоп (1999) предлагает исполь - зовать приближенные методы вывода, такие как MCMC или вариационный вывод. Мы ссылаемся на работы Gilks et al. (1996) и Blei et al. (2017) для получения более подробной информации об этих приблизительных методах вывода. В PPCA мы рассматривали линейную модель p(xn | zn) =  (xn | Bzn + μ, σ2I), с предшествующим p(zn) =  (0, I), где на все измерения наблюдения влияет одинаковое количество шума. Если мы позволим каждому измерению наблю - дения d иметь различную дисперсию , мы получим факторный анализ (factor analysis — FA) (Spearman, 1904; Bartholomew et al., 2011). Это означает, что FA\n--- Страница 436 ---\n436 Глава 10. Снижение размерности с помощью анализа главных компонент дает вероятность большей гибкости1, чем PPCA, но по-прежнему заставляет данные объясняться параметрами модели B, μ. Однако FA больше не допускает решения максимального правдоподобия в замкнутой форме, поэтому нам нуж - но использовать итерационную схему, такую как алгоритм максимизации мате - матического ожидания, для оценки параметров модели. В то время как в PPCA все стационарные точки являются глобальными оптимумами, это больше не выполняется для FA. По сравнению с PPCA, FA не изменяется, если мы масшта - бируем данные, но он возвращает разные решения, если мы вращаем данные. Алгоритм, который также тесно связан с PCA, представляет собой анализ неза ­ висимых компонент (Independent component analysis, ICA (Hyvarinen et al., 2001)). Начиная снова с точки зрения скрытых переменных p(xn | zn) = =  (xn | Bzn + μ, σ2I), мы теперь меняем априорность zn на негауссовы распреде - ления. ICA можно использовать для слепого разделения источников . Представь - те, что вы находитесь на оживленном вокзале, и много людей разговаривают. Ваши уши играют роль микрофонов, и они линейно смешивают различные речевые сигналы на вокзале. Цель разделения источников состоит в том, чтобы идентифицировать составные части смешанных сигналов. Как обсуждалось ранее в контексте оценки максимального правдоподобия для PPCA, исходное решение PCA инвариантно к любому вращению. Следовательно, PCA может идентифицировать лучшее подпространство более низкой размерности, в кото- ром живут сигналы, но не сами сигналы (Murphy, 2012). ICA решает эту про - блему, изменяя априорное распределение p(z) для скрытых источников, чтобы требовать негауссовых априорных значений p(z). Мы ссылаемся на книги Hyvarinen et al. (2001) и Murphy (2012) для получения более подробной инфор - мации об ICA. PCA, факторный анализ и ICA — три примера уменьшения размерности с по- мощью линейных моделей. Cunningham и Ghahramani (2015) предоставляют более широкий обзор снижения линейной размерности. Модель (P)PCA, которую мы здесь обсуждали, допускает несколько важных расширений. В разделе 10.5 мы объяснили, как выполнить PCA, когда входная размерность D значительно превышает количество N точек данных. Используя понимание того, что PCA может быть реализовано путем вычисления (многих) внутренних продуктов, эта идея может быть доведена до крайности, рассматри - вая бесконечномерные функции. Уловка с ядром является основой ядра PCA и позволяет нам неявно вычислять внутренние продукты между бесконечно - мерными функциями (Sch ölkopf et al., 1998; Scholkopf and Smola, 2002). 1 Чрезмерно гибкая вероятность могла бы объяснить больше, чем просто шум.\n--- Страница 437 ---\n10.8. Дополнительное чтение 437 Существуют методы нелинейного уменьшения размерности, заимствованные из PCA (Burges, 2010, дает хороший обзор). Перспектива автокодировщика PCA, которую мы обсуждали ранее в этом разделе, может использоваться для ренде - ринга PCA как особого случая глубокого автокодировщика. В глубоком авто ­ кодере и кодер, и декодер представлены многослойными нейронными сетями с прямой связью, которые сами по себе являются нелинейными отображениями. Если мы установим функции активации в этих нейронных сетях как идентичные, модель станет эквивалентной PCA. Другой подход к уменьшению нелинейной размерности — это модель скрытых переменных гауссового процесса ( Gaussian process latent­variable model — GP-LVM), предложенная Lawrence (2005). GP- LVM начинается с точки зрения скрытых переменных, которую мы использо - вали для получения PPCA, и заменяет линейную связь между скрытыми пере - менными z и наблюдениями x на гауссов процесс (Gaussian process — GP). Вместо оценки параметров отображения (как мы делаем в PPCA) GP-LVM маргинализирует параметры модели и делает точечные оценки скрытых пере - менных z. Подобно байесовскому PCA, байесовский GP­LVM , предложенный Титсиасом и Лоуренсом (Titsias and Lawrence, 2010), поддерживает распреде - ление скрытых переменных z и также использует приближенный вывод для их интеграции.\n--- Страница 438 ---\n11 Оценка плотности с помощью моделей гауссовой смеси В предыдущих главах мы уже рассмотрели две фундаментальные проблемы машинного обучения: регрессию (глава 9) и уменьшение размерности (глава 10). В этой главе мы рассмотрим третий столп машинного обучения: оценку плот - ности. По пути мы познакомим вас с важными концепциями, такими как алго - ритм максимизации ожидания (expectation maximization — EM) и перспектива скрытых переменных для оценки плотности с помощью моделей смеси. Когда мы применяем машинное обучение к данным, мы часто стремимся каким- то образом представить данные. Самый простой способ — взять сами точки данных как представление данных; см. рис. 11.1 для примера. Однако этот под - ход может оказаться бесполезным, если набор данных огромен или если мы заинтересованы в представлении характеристик данных. При оценке плотности мы представляем данные компактно, используя плотность из параметрическо - го семейства, например гауссово или бета-распределение. Например, мы можем искать среднее значение и дисперсию набора данных, чтобы компактно пред - ставить данные с использованием распределения Гаусса. Среднее значение и дисперсию можно найти с помощью инструментов, которые мы обсуждали в разделе 8.3: максимальная вероятность или максимальная апостериорная оценка. Затем мы можем использовать среднее значение и дисперсию этого гауссиана для представления распределения, лежащего в основе данных, то есть мы думаем, что набор данных является типичной реализацией этого распреде - ления, если бы мы брали из него выборку. На практике гауссово распределение (или аналогично все другие распределения, с которыми мы встречались до сих пор) имеет ограниченные возможности мо - делирования. Например, гауссово приближение плотности, сгенерировавшее\n--- Страница 439 ---\n439 Оценка плотности с помощью моделей гауссовой смеси данные на рис. 11.1, было бы плохим приближением. Далее мы рассмотрим более выразительное семейство распределений, которое мы можем использовать для оценки плотности: модели смеси. Смесительные модели могут использо - ваться для описания распределения p(x) выпуклой комбинацией K простых (базовых) распределений. ; (11.1) (11.2) где компоненты pk являются членами семейства основных распределений, на - пример гауссианов, бернуллиевых или гамма-распределений, а πk — веса смеси . Смесительные модели более выразительны, чем соответствующие базовые рас - пределения, поскольку они допускают мультимодальные представления данных, то есть они могут описывать наборы данных с несколькими «кластерами», как в примере на рис. 11.1. 00 54 2 –5–4–2 x1x2 Рис. 11.1. Двумерный набор данных, который не может быть осмысленно представлен с помощью гауссиана Мы сосредоточимся на моделях гауссовой смеси (Gaussian mixture models — GMM), где основные распределения являются гауссианами. Для данного на - бора данных мы стремимся максимизировать вероятность параметров модели для обучения GMM. Для этого воспользуемся результатами из глав 5, 6 и 7.2. Однако в отличие от других приложений, которые мы обсуждали ранее (линей - ная регрессия или PCA), мы не найдем решения максимального правдоподобия в замкнутой форме. Вместо этого мы придем к системе зависимых одновремен - ных уравнений, которую мы можем решить только итеративно.\n--- Страница 440 ---\n440",
      "debug": {
        "start_page": 402,
        "end_page": 440
      }
    },
    {
      "name": "Глава 11. Оценка плотности с помощью моделей гауссовой смеси 438",
      "chapters": [
        {
          "name": "11.1. Модель гауссовой смеси",
          "content": "11.1. МОДЕЛЬ ГАУССОВОЙ СМЕСИ Модель гауссовой смеси (Gaussian mixture model, GMM) — это модель плот - ности, в которой мы комбинируем конечное число K гауссовых распределений  (x | μk, Σk), так чтобы ; (11.3) (11.4) где мы определили θ : = {μk, Σk, πk : k = 1, , K} как совокупность всех параметров модели. Эта выпуклая комбинация гауссова распределения дает нам значитель - но большую гибкость для моделирования сложных плотностей, чем простое гауссово распределение (которое мы восстанавливаем из (11.3) для K = 1). На рис. 11.2 представлена иллюстрация, на которой показаны взвешенные компо - ненты и плотность смеси, которая представлена как (11.5) 00,000,050,100,150,200,250,30 2 4 /dollar.g00DC2 /dollar.g00DC4 6 8/j.g007A/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088 1 /j.g007A/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088 2 /j.g007A/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088 3 /o.g0081/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM xp(x) Рис.11.2. Модель гауссовой смеси. Распреде-ление GMM (черная линия) состоит из выпуклой комбинации распределений Гаусса и является более вырази- тельным, чем любой отдельный компонент. Пунктирные линии представляют взвешенные гауссовы компоненты",
          "debug": {
            "start_page": 440,
            "end_page": 440
          }
        },
        {
          "name": "11.2. Изучение параметров с помощью максимального правдоподобия",
          "content": "--- Страница 440 --- (продолжение)\nГлава 1 1. Оценка плотности с помощью моделей гауссовой смеси 1 1.1. МОДЕЛЬ ГАУССОВОЙ СМЕСИ Модель гауссовой смеси (Gaussian mixture model, GMM) — это модель плот - ности, в которой мы комбинируем конечное число K гауссовых распределений  (x | μk, Σk), так чтобы ; (11.3) (11.4) где мы определили θ : = {μk, Σk, πk : k = 1, , K} как совокупность всех параметров модели. Эта выпуклая комбинация гауссова распределения дает нам значитель - но большую гибкость для моделирования сложных плотностей, чем простое гауссово распределение (которое мы восстанавливаем из (11.3) для K = 1). На рис. 11.2 представлена иллюстрация, на которой показаны взвешенные компо - ненты и плотность смеси, которая представлена как (11.5) 00,000,050,100,150,200,250,30 2 4 /dollar.g00DC2 /dollar.g00DC4 6 8/j.g007A/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088 1 /j.g007A/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088 2 /j.g007A/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088 3 /o.g0081/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM xp(x) Рис.11.2. Модель гауссовой смеси. Распреде-ление GMM (черная линия) состоит из выпуклой комбинации распределений Гаусса и является более вырази- тельным, чем любой отдельный компонент. Пунктирные линии представляют взвешенные гауссовы компоненты 1 1.2. ИЗУЧЕНИЕ ПАРАМЕТРОВ С ПОМОЩЬЮ МАКСИМАЛЬНОГО ПРАВДОПОДОБИЯ Предположим, нам дан набор данных  = {x1, , xN}, где xn, n = 1, , N, рисуются независимыми и одинаково распределенными из неизвестного распределения p(x). Наша цель — найти хорошее приближение/представление этого неизвест - ного распределения p(x) с помощью GMM с K компонентов смеси. Параметра - ми GMM являются K-средние μk, ковариации Σk и веса смеси πk. Суммируем все эти свободные параметры в θ : = {πk, μk, Σk: k = 1, , К}.\n1 1.2. ИЗУЧЕНИЕ ПАРАМЕТРОВ С ПОМОЩЬЮ МАКСИМАЛЬНОГО ПРАВДОПОДОБИЯ Предположим, нам дан набор данных  = {x1, , xN}, где xn, n = 1, , N, рисуются независимыми и одинаково распределенными из неизвестного распределения p(x). Наша цель — найти хорошее приближение/представление этого неизвест - ного распределения p(x) с помощью GMM с K компонентов смеси. Параметра - ми GMM являются K-средние μk, ковариации Σk и веса смеси πk. Суммируем все эти свободные параметры в θ : = {πk, μk, Σk: k = 1, , К}.\n--- Страница 441 ---\n1 1.2. Изучение параметров с помощью максимального правдоподобия 441 Пример 11.1 (первоначальная настройка) В этой главе у нас будет простой рабочий пример, который поможет нам проиллюстрировать и визуализировать важные концепции. Мы рассматриваем одномерный набор данных  = {−3, −2,5, −1, 0, 2, 4, 5}, состоящий из семи точек данных, и хотим найти GMM с K = 3 компонен - тами, который моделирует плотность данных. Мы инициализируем компоненты смеси как p1(x) =  (x | –4, 1); (11.6) p2(x) =  (x | 0, 0,2); (11.7) p3(x) =  (x | 8, 3) (11.8) и присваиваем им равные веса . Соответствующая модель (и точки данных) показаны на рис. 11.3. x/g831N (x | /g801, /g8612) /g832N (x | /g802, /g8622) /g833N (x | /g803, /g8632) 0,000,050,100,150,200,250,30 /dollar.g00DC5 5 01 01 5/o.g0081/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMMp(x) Рис. 11.3. Начальная настройка: GMM (черная линия) со смесью трех компонентов смеси (пунктир) и семи точек данных (точки) Далее мы подробно описываем, как получить оценку максимального правдопо - добия θML параметров модели θ. Мы начинаем с записи вероятности, то есть прогнозируемого распределения обучающих данных с учетом параметров. Мы используем наше независимое и одинаково распределенное предположение, которое приводит к факторизованной вероятности (11.9) где каждый отдельный член вероятности p(xn | θ) является плотностью гауссовой смеси. Тогда мы получаем логарифмическое правдоподобие как\n--- Страница 442 ---\n442 Глава 1 1. Оценка плотности с помощью моделей гауссовой смеси (11.10) Наша цель — найти параметры , которые максимизируют логарифмическое правдоподобие , определенное в (11.10). Наша «нормальная» процедура за - ключалась бы в вычислении градиента d/dθ логарифма правдоподобия от - носительно параметров модели θ, установке его в 0 и решении относительно θ. Однако в отличие от наших предыдущих примеров оценки максимального правдоподобия (например, когда мы обсуждали линейную регрессию в разде - ле 9.2), мы не можем получить решение в замкнутой форме. Тем не менее, мы можем использовать итеративную схему, чтобы найти хорошие параметры модели θML, которые, в конечном итоге, будут алгоритмом EM для GMM. Клю - чевая идея состоит в том, чтобы обновлять один параметр модели за один шаг итерации, оставляя другие неизменными. ПРИМЕЧАНИЕ Если бы мы рассмотрели один гауссиан как желаемую плот - ность, сумма по k в (11.10) обращается в нуль, и log можно применить непо - средственно к гауссовой составляющей, так что мы получим (11.11) Эта простая форма позволяет нам найти оценки максимального правдоподобия μ и Σ в замкнутой форме, как обсуждалось в главе 8. В (11.10) мы не можем пере - местить log в сумму по k, так что мы не можем получить простое решение с мак- симальным правдоподобием в замкнутой форме.  Любой локальный оптимум функции обладает тем свойством, что его градиент по параметрам должен обращаться в нуль (необходимое условие); см. главу 7. В нашем случае мы получаем следующие необходимые условия, когда опти - мизируем логарифмическое правдоподобие в (11.10) по параметрам GMM μk, Σk, πk: (11.12) (11.13) (11.14)\n--- Страница 443 ---\n1 1.2. Изучение параметров с помощью максимального правдоподобия 443 Для всех трех необходимых условий, применяя цепное правило (раздел 5.2.2), нам потребуются частные производные вида (11.15) где θ = {μk, Σk, πk, k = 1, , K} — параметры модели и (11.16) Далее мы вычислим частные производные (11.12)–(11.14). Но прежде, чем мы это сделаем, мы представим величину, которая будет играть центральную роль в оставшейся части этой главы: ответственность.",
          "debug": {
            "start_page": 440,
            "end_page": 443
          }
        },
        {
          "name": "11.2.1. Ответственность",
          "content": "--- Страница 443 --- (продолжение)\n1 1.2.1. Ответственность Определяем количество (11.17) как ответственность k-го компонента смеси за n-ю точку данных. Ответствен - ность rnk k-го компонента смеси для точки данных xn пропорциональна правдо - подобию p (xn | πk,μk,Σk) = πk (xn | μk, Σk) (11.18) компонента смеси с учетом точки данных1. Следовательно, компоненты смеси несут большую ответственность за точку данных, когда точка данных может быть допустимой пробой из этого компонента смеси. Обратите внимание, что rn : = [rn1, , rnK]T ∈ k является (нормированным) вектором вероятности, то есть Σk rnk = 1 ≥ 0. Этот вектор вероятности распределяет вероятностную массу между K компонентами смеси, и мы можем рассматривать rn как «мягкое при - своение» xn компонентам K смеси. Следовательно, ответственность rnk из (11.17) представляет собой вероятность того, что xn был сгенерирован k-м компонентом смеси2. 1 rn следует распределению Больцмана — Гиббса. 2 Ответственность rnk — это вероятность того, что k-й компонент смеси сгенерировал n-ю точку данных.\n1 1.2.1. Ответственность Определяем количество (11.17) как ответственность k-го компонента смеси за n-ю точку данных. Ответствен - ность rnk k-го компонента смеси для точки данных xn пропорциональна правдо - подобию p (xn | πk,μk,Σk) = πk (xn | μk, Σk) (11.18) компонента смеси с учетом точки данных1. Следовательно, компоненты смеси несут большую ответственность за точку данных, когда точка данных может быть допустимой пробой из этого компонента смеси. Обратите внимание, что rn : = [rn1, , rnK]T ∈ k является (нормированным) вектором вероятности, то есть Σk rnk = 1 ≥ 0. Этот вектор вероятности распределяет вероятностную массу между K компонентами смеси, и мы можем рассматривать rn как «мягкое при - своение» xn компонентам K смеси. Следовательно, ответственность rnk из (11.17) представляет собой вероятность того, что xn был сгенерирован k-м компонентом смеси2. 1 rn следует распределению Больцмана — Гиббса. 2 Ответственность rnk — это вероятность того, что k-й компонент смеси сгенерировал n-ю точку данных.\n--- Страница 444 ---\n444 Глава 1 1. Оценка плотности с помощью моделей гауссовой смеси Пример 11.2 (ответственности) В нашем примере из рис. 11.3 мы вычисляем ответственности rnk (11.19) Здесь n-я строка сообщает нам ответственности всех компонентов смеси для xn. Сумма всех K ответственностей для точки данных (сумма каждой строки) равна 1. k-й столбец дает нам обзор ответственности k-го компо - нента смеси. Мы видим, что третий компонент смеси (третий столбец) не отвечает ни за одну из первых четырех точек данных, но берет на себя большую ответственность за остальные точки данных. Сумма всех за - писей столбца дает нам значения Nk, то есть полную ответственность k-го компонента смеси. В нашем примере мы получаем N1 = 2,058, N2 = 2,008, N3 = 2,934. Далее мы определяем обновления параметров модели μk, Σk, πk для заданных ответственностей. Мы увидим, что все уравнения обновления зависят от от - ветственностей, что делает невозможным решение задачи оценки максималь - ного правдоподобия в замкнутой форме. Однако для данных ответственностей мы будем обновлять один параметр модели за один шаг, оставляя другие неиз - менными. После этого мы пересчитаем ответственности. Итерация этих двух шагов в конечном итоге приведет к локальному оптимуму и является конкрет - ным воплощением алгоритма EM. Мы обсудим это более подробно в разделе 11.3.",
          "debug": {
            "start_page": 443,
            "end_page": 444
          }
        },
        {
          "name": "11.2.2. Обновление средних",
          "content": "--- Страница 444 --- (продолжение)\n1 1.2.2. Обновление средних Теорема 11.1 (обновление средних GMM). Обновление средних параметров GMM μk, k = 1, , K определяется выражением (11.20) где ответственности rnk определены в (11.17).\n1 1.2.2. Обновление средних Теорема 11.1 (обновление средних GMM). Обновление средних параметров GMM μk, k = 1, , K определяется выражением (11.20) где ответственности rnk определены в (11.17).\n--- Страница 445 ---\n1 1.2. Изучение параметров с помощью максимального правдоподобия 445 ПРИМЕЧАНИЕ Обновление средних μk отдельных компонентов смеси в (11.20) зависит от всех средних, ковариационных матриц Σk и весов смеси πk через rnk, указанные в (11.17). Следовательно, мы не можем получить решение в замкнутой форме для всех μk сразу.  Доказательство . Из (11.15) мы видим, что градиент логарифма правдоподобия относительно средних параметров μk, k = 1, , K, требует, чтобы мы вычислили частную производную (11.21 a) (11.21 b) где мы использовали то, что только k-й компонент смеси зависит от μk. Мы используем наш результат из (11.21 b) в (11.15) и собираем все вместе, так чтобы искомая частная производная  по μk была задана как (11.22 a) (11.22 b) (11.22 c) Здесь мы использовали тождество из (11.16) и результат частной производной в (11.21 b), чтобы получить (11.22 b). Значения rnk — это ответственности, которые мы определили в (11.17). Решим теперь (11.22 c) относительно , так что , и получим (11.23) где мы определили (11.24)\n--- Страница 446 ---\n446 Глава 1 1. Оценка плотности с помощью моделей гауссовой смеси как полную ответственность k-го компонента смеси за весь набор данных. Это завершает доказательство теоремы 11.1.  Интуитивно (11.20) можно интерпретировать как взвешенную по важности оценку Монте-Карло среднего, где веса важности точки данных xn — это ответ - ственности rnk k-го кластера для xn, k = 1, , K. Следовательно, среднее значение μk притягивается к точке данных xn с силой, заданной параметром rnk. Средние значения тянутся сильнее к точкам данных, для которых соответствующий компонент смеси имеет высокую ответственность, то есть высокую вероятность. Рисунок 11.4 иллюстрирует это. Мы также можем интерпретировать среднее обновление в (11.20) как ожидаемое значение всех точек данных в соответствии с распределением, заданным формулой rk := [r1k, , rNk]T /Nk, (11.25) которое является нормализованным вектором вероятности, то есть (11.26) µx1x2x3 r1r2r3 Рис. 11.4. Обновление среднего параметра компонента смеси в GMM. Среднее значение μ приближается к отдельным точкам данных с весами, заданными соответствующими ответственностями Пример 11.3 (обновления средних) В нашем примере из рис. 11.3 средние значения обновляются следующим образом: μ1 : −4 → −2,7; (11.27) μ2 : 0 → −0,4; (11.28) μ3 : 8 → 3,7. (11.29) Здесь мы видим, что средние значения первого и третьего компонентов смеси смещаются в сторону режима данных, тогда как среднее значение второго компонента не меняется так резко. Рисунок 11.5 иллюстрирует это изменение, где рис. 11.5( a) показывает плотность GMM до обновления средних значений, а рис. 11.5( b) показывает плотность GMM после об - новления средних значений μk.\n--- Страница 447 ---\n1 1.2. Изучение параметров с помощью максимального правдоподобия 447 (a) /o.g0081/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM /comma.g00D2 /percent.g00BA/two.g0088/.notdef.g0105/yright/.notdef.g00E3/.notdef.g0109…/slash.g00A9/yright /asterisk.g007D/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088/slash.g00A9 /.notdef.g0105/percent.g00BA /percent.g00BA/K.g00AD…/percent.g00BA/quotedbl.g006D/.notdef.g00E3/yright…/comma.g00D2/space.g00AB “/exclam.g00AF/yright/.notdef.g0105…/comma.g00D2/period.g00B2 ƒ…/equal.g00C8/.notdef.g0107/yright…/comma.g00D2/L.g00AE(b) /o.g0081/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM /comma.g00D2 /percent.g00BA/two.g0088/.notdef.g0105/yright/.notdef.g00E3/.notdef.g0109…/slash.g00A9/yright /asterisk.g007D/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088/slash.g00A9 /C.g00B9/percent.g00BA“/.notdef.g00E3/yright /percent.g00BA/K.g00AD…/percent.g00BA/quotedbl.g006D/.notdef.g00E3/yright…/comma.g00D2/space.g00AB “/exclam.g00AF/yright/.notdef.g0105…/comma.g00D2/period.g00B2 ƒ…/equal.g00C8/.notdef.g0107/yright…/comma.g00D2/L.g00AExx/dollar.g00DC5 5 01 0/dollar.g00DC 55 01 0 151 50,000,050,100,150,200,250,30p(x) 0,000,050,100,150,200,250,30p(x)/g831N (x | /g801, /g8612) /g832N (x | /g802, /g8622) /g833N (x | /g803, /g8632) /C.g00B9/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM/g831N (x | /g801, /g8612) /g832N (x | /g802, /g8622) /g833N (x | /g803, /g8632) /C.g00B9/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM Рис. 11.5. Эффект обновления средних значений в GMM. ( а) GMM перед обновлением средних значений; ( b) GMM после обновления средних значений μk с сохранением дисперсии и веса смеси Обновление средних параметров в (11.20) выглядит довольно просто. Однако обратите внимание, что ответственности rnk являются функцией πj, μj, Σj для всех j = 1, , K, так что обновления в (11.20) зависят от всех параметров GMM, и ре- шение в замкнутой форме, которое мы получили для линейной регрессии в раз- деле 9.2 или PCA в главе 10, получить невозможно.",
          "debug": {
            "start_page": 444,
            "end_page": 447
          }
        },
        {
          "name": "11.2.3. Обновление ковариаций",
          "content": "--- Страница 447 --- (продолжение)\n1 1.2.3. Обновление ковариаций Теорема 11.2 (обновления ковариаций GMM). Обновление параметров кова ­ риации Σk, k = 1, , K, GMM определяется выражением (11.30) где rnk и Nk определены в (11.17) и (11.24) соответственно . Доказательство . Наш подход доказательства теоремы 11.2 состоит в том, чтобы вычислить частные производные логарифма правдоподобия  по ковариациям Σk, установить их равными 0 и решить относительно Σk. Начнем с нашего обще - го подхода (11.31) Мы уже знаем 1/ p (xn | θ) из (11.16). Чтобы получить оставшуюся частную про - изводную ∂p(xn | θ) / ∂Σk, запишем определение гауссова распределения p(xn | θ) (11.9) и отбросим все члены, кроме k-го. Тогда получаем\n1 1.2.3. Обновление ковариаций Теорема 11.2 (обновления ковариаций GMM). Обновление параметров кова ­ риации Σk, k = 1, , K, GMM определяется выражением (11.30) где rnk и Nk определены в (11.17) и (11.24) соответственно . Доказательство . Наш подход доказательства теоремы 11.2 состоит в том, чтобы вычислить частные производные логарифма правдоподобия  по ковариациям Σk, установить их равными 0 и решить относительно Σk. Начнем с нашего обще - го подхода (11.31) Мы уже знаем 1/ p (xn | θ) из (11.16). Чтобы получить оставшуюся частную про - изводную ∂p(xn | θ) / ∂Σk, запишем определение гауссова распределения p(xn | θ) (11.9) и отбросим все члены, кроме k-го. Тогда получаем\n--- Страница 448 ---\n448 Глава 1 1. Оценка плотности с помощью моделей гауссовой смеси (11.32 a) (11.32 b) (11.32 c) Теперь мы используем тождества (11.33) (11.34) и получим (после некоторой перестановки) искомую частную производную, требуемую в (11.31), как (11.35) Собирая все вместе, частная производная логарифма правдоподобия по Σk да- ется выражением (11.36 a) (11.36 b) (11.36 c)\n--- Страница 449 ---\n1 1.2. Изучение параметров с помощью максимального правдоподобия 449 (11.36 d) Мы видим, что ответственности rnk также присутствуют в этой частной произ - водной. Приравнивая эту частную производную к 0, получаем необходимое условие оптимальности (11.37 a) (11.37 b) Решая относительно Σk, получаем (11.38) где rk — вектор вероятности, определенный в (11.25). Это дает нам простое пра - вило обновления Σk для k = 1, , K и доказывает теорему 11.2.  Подобно обновлению μk в (11.20), мы можем интерпретировать обновление ковариации в (11.30) как взвешенное по важности ожидаемое значение квадра - та центрированных данных : = {x1 – μk, , xN – μk}. Пример 11.4 (обновления дисперсии) В нашем примере из рис. 11.3 дисперсии обновляются следующим об - разом: ; (11.39) ; (11.40) . (11.41) Здесь мы видим, что дисперсия первого и третьего компонентов значи - тельно сокращается, тогда как дисперсия второго компонента немного увеличивается. Рисунок 11.6 иллюстрирует эту настройку. Рисунок 11.6( a) идентичен рис. 11.5( b) (но увеличенному) и показывает плотность GMM и ее от- дельные компоненты до обновления дисперсий. На рис. 11.6( b) показана плотность GMM после обновления дисперсий.\n--- Страница 450 ---\n450 Глава 1 1. Оценка плотности с помощью моделей гауссовой смеси (a) /o.g0081/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM /comma.g00D2 /percent.g00BA/two.g0088/.notdef.g0105/yright/.notdef.g00E3/.notdef.g0109…/slash.g00A9/yright /asterisk.g007D/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088/slash.g00A9 /.notdef.g0105/percent.g00BA /percent.g00BA/K.g00AD…/percent.g00BA/quotedbl.g006D/.notdef.g00E3/yright…/comma.g00D2/space.g00AB /.notdef.g0105/comma.g00D2“/C.g00B9/yright/exclam.g00AF“/comma.g00D2/L.g00AE(b) /o.g0081/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM /comma.g00D2 /percent.g00BA/two.g0088/.notdef.g0105/yright/.notdef.g00E3/.notdef.g0109…/slash.g00A9/yright /asterisk.g007D/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088/slash.g00A9 /C.g00B9/percent.g00BA“/.notdef.g00E3/yright /percent.g00BA/K.g00AD…/percent.g00BA/quotedbl.g006D/.notdef.g00E3/yright…/comma.g00D2/space.g00AB /.notdef.g0105/comma.g00D2“/C.g00B9/yright/exclam.g00AF“/comma.g00D2/L.g00AEx/dollar.g00DC4 /dollar.g00DC2 2 04 68 x/dollar.g00DC4 /dollar.g00DC2 2 04 680,000,050,100,150,200,250,30p(x) 0,000,050,100,150,200,250,35 0,30p(x)/g831N (x | /g801, /g8612) /g832N (x | /g802, /g8622) /g833N (x | /g803, /g8632) /C.g00B9/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM/g831N (x | /g801, /g8612) /g832N (x | /g802, /g8622) /g833N (x | /g803, /g8632) /C.g00B9/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM Рис. 11.6. Эффект обновления отклонений в GMM. ( а) GMM перед обновлением отклонений; ( b) GMM после обновления дисперсий с сохранением средних значений и веса смеси Подобно обновлению средних параметров, мы можем интерпретировать (11.30) как оценку Монте-Карло взвешенной ковариации точек данных xn, связанных с k-м компонентом смеси, где веса — это ответственности rnk. Как и обновления средних параметров, это обновление зависит от всех πj, μj, Σj, j = 1, , K, через ответственности rnk, запрещающие решение в замкнутой форме.",
          "debug": {
            "start_page": 447,
            "end_page": 450
          }
        },
        {
          "name": "11.2.4. Обновление весов смеси",
          "content": "--- Страница 450 --- (продолжение)\n1 1.2.4. Обновление весов смеси Теорема 11.3 (обновление весов смеси GMM). Вес смеси GMM обновляется как (11.42) где N — количество точек данных, а Nk определено в (11.24). Доказательство . Чтобы найти частную производную логарифма правдоподобия по весовым параметрам πk, k = 1, , K, мы учитываем ограничение Σk πk = 1 с по- мощью множителей Лагранжа (раздел 7.2). Лагранжиан (11.43 a) (11.43 b) где  — логарифм правдоподобия из (11.10), а второй член кодирует ограниче - ние равенства, которое необходимо для суммирования всех весов смеси до 1. Мы получаем частную производную по πk как\n1 1.2.4. Обновление весов смеси Теорема 11.3 (обновление весов смеси GMM). Вес смеси GMM обновляется как (11.42) где N — количество точек данных, а Nk определено в (11.24). Доказательство . Чтобы найти частную производную логарифма правдоподобия по весовым параметрам πk, k = 1, , K, мы учитываем ограничение Σk πk = 1 с по- мощью множителей Лагранжа (раздел 7.2). Лагранжиан (11.43 a) (11.43 b) где  — логарифм правдоподобия из (11.10), а второй член кодирует ограниче - ние равенства, которое необходимо для суммирования всех весов смеси до 1. Мы получаем частную производную по πk как\n--- Страница 451 ---\n1 1.2. Изучение параметров с помощью максимального правдоподобия 451 (11.44 a) (11.44 b) и частную производную по множителю Лагранжа λ как (11.45) Приравнивая обе частные производные к 0 (необходимое условие оптимума), получаем систему уравнений ; (11.46) (11.47) Используя (11.46) в (11.47) и решая относительно πk, получаем (11.48) (11.49) что дает нам обновление для весовых параметров πk и доказывает теорему 11.3.  Мы можем определить вес смеси в (11.42) как отношение общей ответствен - ности k-го кластера к количеству точек данных. Поскольку N = ΣkNk, количество точек данных также можно интерпретировать как общую ответственность всех компонентов смеси вместе, так что πk — относительная важность k-го компонен - та смеси для набора данных. ПРИМЕЧАНИЕ Поскольку , уравнение обновления (11.42) вы - ражает зависимости величин весов смеси π от всех величин πj, μj, Σj, j = 1, , K через коэффициенты ответственности r. \n--- Страница 452 ---\n452 Глава 1 1. Оценка плотности с помощью моделей гауссовой смеси Пример 11.5 (обновления весовых параметров) В нашем рабочем примере из рис. 11.3 веса смеси обновляются следующим образом: ; (11.50) ; (11.51) . (11.52) Здесь мы видим, что третий компонент получает больший вес/важность, в то время как другие компоненты становятся немного менее важными. На рис. 11.7 показан эффект обновления весов смеси. Рисунок 11.7( a) идентичен рис. 11.6( b) и показывает плотность GMM и его отдельные компоненты до обновления весов смеси. На рис. 11.7( b) показана плот - ность GMM после обновления весов смеси. (a) /o.g0081/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM /comma.g00D2 /percent.g00BA/two.g0088/.notdef.g0105/yright/.notdef.g00E3/.notdef.g0109…/slash.g00A9/yright /asterisk.g007D/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088/slash.g00A9 /.notdef.g0105/percent.g00BA /percent.g00BA/K.g00AD…/percent.g00BA/quotedbl.g006D/.notdef.g00E3/yright…/comma.g00D2/space.g00AB /quotedbl.g006D/yright“/equal.g00C8 “/.notdef.g00E4/yright“/comma.g00D2/g831N (x | /g801, /g8612) /g832N (x | /g802, /g8622) /g833N (x | /g803, /g8632) /C.g00B9/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM/g831N (x | /g801, /g8612) /g832N (x | /g802, /g8622) /g833N (x | /g803, /g8632) /C.g00B9/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM x/dollar.g00DC4 /dollar.g00DC2 2 04 6 8 x/dollar.g00DC4 /dollar.g00DC2 2 04 6 80,000,050,100,150,200,250,30p(x) 0,000,050,100,150,200,250,35 0,30p(x) (b) /o.g0081/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM /comma.g00D2 /percent.g00BA/two.g0088/.notdef.g0105/yright/.notdef.g00E3/.notdef.g0109…/slash.g00A9/yright /asterisk.g007D/percent.g00BA/.notdef.g00E4/C.g00B9/percent.g00BA…/yright…/two.g0088/slash.g00A9 /C.g00B9/percent.g00BA“/.notdef.g00E3/yright /percent.g00BA/K.g00AD…/percent.g00BA/quotedbl.g006D/.notdef.g00E3/yright…/comma.g00D2/space.g00AB /quotedbl.g006D/yright“/equal.g00C8 “/.notdef.g00E4/yright“/comma.g00D2 Рис. 11.7. Эффект обновления весов смеси в GMM. ( а) GMM перед обновле - нием весов смеси; ( b) GMM после обновления весов смесей с сохранением средних значений и отклонений. Обратите внимание на разные масштабы вертикальных осей В целом, обновив средние значения, дисперсию и веса один раз, мы получаем GMM, показанную на рис. 11.7( b). По сравнению с инициализацией, показанной на рис. 11.3, мы можем видеть, что обновления параметров заставили плотность GMM сместить часть своей массы в сторону точек данных. После однократного обновления средних, дисперсий и весов соответствие GMM на рис. 11.7( b) уже заметно лучше, чем ее инициализация на рис. 11.3. Об этом также свидетельствуют значения логарифма правдоподобия, которые увеличи - лись с 28,3 (инициализация) до 14,4 после одного полного цикла обновления.",
          "debug": {
            "start_page": 450,
            "end_page": 452
          }
        },
        {
          "name": "11.3. EM-алгоритм",
          "content": "--- Страница 453 --- (продолжение)\n1 1.3. EM-алгоритм 453 1 1.3. EM-АЛГОРИТМ К сожалению, обновления в (11.20), (11.30) и (11.42) не представляют собой решение в замкнутой форме для обновлений параметров μk, Σk, πk модели смеси, поскольку ответственности rnk зависят от этих параметров сложным способом. Однако результаты предлагают простую итерационную схему для поиска реше - ния задачи оценки параметров с помощью максимального правдоподобия. Алгоритм максимизации ожидания (алгоритм EM — expectation maximization) был предложен Dempster et al. (1977) и представляет собой общую итеративную схему для изучения параметров (максимального правдоподобия или MAP) в смешанных моделях и, в более общем смысле, в моделях со скрытыми пере - менными. В нашем примере модели гауссовой смеси мы выбираем начальные значения для μk, Σk, πk и меняем их до сходимости между E-шагом и M-шагом: zE­шаг : оцените ответственности rnk (апостериорная вероятность того, что точка данных n принадлежит компоненту смеси k). zzM­шаг : используйте обновленные ответственности для вычисления новой оценки параметров μk, Σk, πk. Каждый шаг в алгоритме EM увеличивает логарифмическую функцию прав - доподобия (Neal and Hinton, 1999). Для сходимости мы можем напрямую проверить логарифмическую вероятность или параметры. Конкретная реали - зация алгоритма EM для оценки параметров GMM выглядит следующим образом: 1. Инициализируйте μk, Σk, πk. 2. E­шаг : оцените ответственности rnk для каждой точки данных xn, используя текущие параметры πk, μk, Σk: (11.53) 1. M­шаг : заново оценить параметры πk, μk, Σk, используя текущие ответствен - ности rnk (из E-шага): (11.54) (11.55) (11.56)\n1 1.3. EM-алгоритм 453 1 1.3. EM-АЛГОРИТМ К сожалению, обновления в (11.20), (11.30) и (11.42) не представляют собой решение в замкнутой форме для обновлений параметров μk, Σk, πk модели смеси, поскольку ответственности rnk зависят от этих параметров сложным способом. Однако результаты предлагают простую итерационную схему для поиска реше - ния задачи оценки параметров с помощью максимального правдоподобия. Алгоритм максимизации ожидания (алгоритм EM — expectation maximization) был предложен Dempster et al. (1977) и представляет собой общую итеративную схему для изучения параметров (максимального правдоподобия или MAP) в смешанных моделях и, в более общем смысле, в моделях со скрытыми пере - менными. В нашем примере модели гауссовой смеси мы выбираем начальные значения для μk, Σk, πk и меняем их до сходимости между E-шагом и M-шагом: zE­шаг : оцените ответственности rnk (апостериорная вероятность того, что точка данных n принадлежит компоненту смеси k). zzM­шаг : используйте обновленные ответственности для вычисления новой оценки параметров μk, Σk, πk. Каждый шаг в алгоритме EM увеличивает логарифмическую функцию прав - доподобия (Neal and Hinton, 1999). Для сходимости мы можем напрямую проверить логарифмическую вероятность или параметры. Конкретная реали - зация алгоритма EM для оценки параметров GMM выглядит следующим образом: 1. Инициализируйте μk, Σk, πk. 2. E­шаг : оцените ответственности rnk для каждой точки данных xn, используя текущие параметры πk, μk, Σk: (11.53) 1. M­шаг : заново оценить параметры πk, μk, Σk, используя текущие ответствен - ности rnk (из E-шага): (11.54) (11.55) (11.56)\n--- Страница 454 ---\n454 Глава 1 1. Оценка плотности с помощью моделей гауссовой смеси Пример 11.6 (GMM-соответствие) Когда мы запускаем EM в нашем примере из рис. 11.3, мы получаем окон - чательный результат, показанный на рис. 11.8( a), после пяти итераций, а рис. 11.8( b) показывает, как отрицательное логарифмическое правдо - подобие изменяется в зависимости от итераций EM. Окончательная GMM представлена как (11.57) (/equal.g00C8) /n.g007C/asterisk.g007D/percent.g00BA…/.notdef.g0107/equal.g00C8/two.g0088/yright/.notdef.g00E3/.notdef.g0109…/equal.g00C8/space.g00AB /C.g00B9/percent.g00BA/.notdef.g0105/.notdef.g0104/percent.g00BA…/asterisk.g007D/equal.g00C8 GMM. /o.g0081/percent.g00BA“/.notdef.g00E3/yright /C.g00B9/space.g00AB/two.g0088/comma.g00D2 /comma.g00D2/two.g0088/yright/exclam.g00AF/equal.g00C8/.notdef.g0106/comma.g00D2/L.g00AE /equal.g00C8/.notdef.g00E3/.notdef.g0104/percent.g00BA/exclam.g00AF/comma.g00D2/two.g0088/.notdef.g00E4 EM “/period.g00B2/percent.g00BA/.notdef.g0105/comma.g00D2/two.g0088“/space.g00AB /comma.g00D2 /quotedbl.g006D/percent.g00BAƒ/quotedbl.g006D/exclam.g00AF/equal.g00C8/question.g009D/equal.g00C8/yright/two.g0088 /period.g00AA/two.g0088/three.g0082 GMM/h.g006A/two.g0088/yright/exclam.g00AF/equal.g00C8/.notdef.g0106/comma.g00D2/space.g00AB/n.g007C/two.g0088/exclam.g00AF/comma.g00D2/.notdef.g0106/equal.g00C8/two.g0088/yright/.notdef.g00E3/.notdef.g0109…/equal.g00C8/space.g00AB /.notdef.g00E3/percent.g00BA/.notdef.g0104/equal.g00C8/exclam.g00AF/comma.g00D2/hyphen.g00C1/.notdef.g00E4 /C.g00B9/exclam.g00AF/equal.g00C8/quotedbl.g006D/.notdef.g0105/percent.g00BA/C.g00B9/percent.g00BA/.notdef.g0105/percent.g00BA/K.g00AD/comma.g00D2/space.g00AB/g831N (x | /g801, /g8612) /g832N (x | /g802, /g8622) /g833N (x | /g803, /g8632) /C.g00B9/.notdef.g00E3/percent.g00BA/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 GMM x/dollar.g00DC5 05 05 1234 10 150,00 1416182022242628 0,050,100,150,200,250,30p(x) (/K.g00AD) /n.g007C/two.g0088/exclam.g00AF/comma.g00D2/.notdef.g0106/equal.g00C8/two.g0088/yright/.notdef.g00E3/.notdef.g0109…/slash.g00A9/L.g00AE /.notdef.g00E3/percent.g00BA/.notdef.g0104/equal.g00C8/exclam.g00AF/comma.g00D2/hyphen.g00C1/.notdef.g00E4 /C.g00B9/exclam.g00AF/equal.g00C8/quotedbl.g006D/.notdef.g0105/percent.g00BA/C.g00B9/percent.g00BA/.notdef.g0105/percent.g00BA/K.g00AD/comma.g00D2/space.g00AB /asterisk.g007D/equal.g00C8/asterisk.g007D /hyphen.g00C1/three.g0082…/asterisk.g007D/.notdef.g0106/comma.g00D2/space.g00AB /percent.g00BA/two.g0088 /e.g0070/l.g006C-/comma.g00D2/two.g0088/yright/exclam.g00AF/equal.g00C8/.notdef.g0106/comma.g00D2/L.g00AE Рис. 11.8. EM-алгоритм, примененный к GMM из рис. 11.2. (а) Окончательная подгонка GMM; ( b) отрицательное логарифмическое правдоподобие как функция ЕМ-итераций Мы применили алгоритм EM к двумерному набору данных, показанному на рис. 11.1, с K = 3 компонентами смеси. Рисунок 11.9 иллюстрирует некоторые шаги алгоритма EM и показывает отрицательную логарифмическую вероятность как функцию итерации EM (рис. 11.9( b)). На рис. 11.10( а) показана соответ - ствующая окончательная подгонка GMM. Рисунок 11.10( b) визуализирует окончательные ответственности компонентов смеси для точек данных. Набор данных окрашен в соответствии с ответственностями компонентов смеси, когда EM сходится. Хотя один компонент смеси явно отвечает за данные слева, пере - крытие двух кластеров данных справа могло быть создано двумя компонентами смеси. Становится ясно, что есть точки данных, которые не могут быть одно - значно присвоены одному компоненту (синему или желтому), так что ответ - ственность этих двух кластеров за эти точки составляет около 0,5.\n--- Страница 455 ---\n1 1.3. EM-алгоритм 455 (/equal.g00C8) /m.g0073/equal.g00C8/K.g00AD/percent.g00BA/exclam.g00AF /.notdef.g0105/equal.g00C8……/slash.g00A9/period.g00B2EM-/comma.g00D2/two.g0088/yright/exclam.g00AF/equal.g00C8/.notdef.g0106/comma.g00D2/comma.g00D2/n.g007C/two.g0088/exclam.g00AF/comma.g00D2/.notdef.g0106/equal.g00C8/two.g0088/yright/.notdef.g00E3/.notdef.g0109…/equal.g00C8/space.g00AB /.notdef.g00E3/percent.g00BA/.notdef.g0104/equal.g00C8/exclam.g00AF/comma.g00D2/hyphen.g00C1/.notdef.g00E4/comma.g00D2/.notdef.g0107/yright“/asterisk.g007D/equal.g00C8/space.g00AB /quotedbl.g006D/yright/exclam.g00AF/percent.g00BA/space.g00AB/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 (b) /n.g007C/two.g0088/exclam.g00AF/comma.g00D2/.notdef.g0106/equal.g00C8/two.g0088/yright/.notdef.g00E3/.notdef.g0109…/equal.g00C8/space.g00AB /.notdef.g00E3/percent.g00BA/.notdef.g0104/equal.g00C8/exclam.g00AF/comma.g00D2/hyphen.g00C1/.notdef.g00E4/comma.g00D2/.notdef.g0107/yright“/asterisk.g007D/equal.g00C8/space.g00AB /quotedbl.g006D/yright/exclam.g00AF/percent.g00BA/space.g00AB/two.g0088…/percent.g00BA“/two.g0088/.notdef.g0109 (c) /h.g006A…/comma.g00D2/.notdef.g0106/comma.g00D2/equal.g00C8/.notdef.g00E3/comma.g00D2ƒ/equal.g00C8/.notdef.g0106/comma.g00D2/space.g00AB EM (d) EM /C.g00B9/percent.g00BA“/.notdef.g00E3/yright /percent.g00BA/.notdef.g0105…/percent.g00BA/L.g00AE /comma.g00D2/two.g0088/yright/exclam.g00AF/equal.g00C8/.notdef.g0106/comma.g00D2/comma.g00D2 (e) EM /C.g00B9/percent.g00BA“/.notdef.g00E3/yright 10 /comma.g00D2/two.g0088/yright/exclam.g00AF/equal.g00C8/.notdef.g0106/comma.g00D2/L.g00AE (f) /e.g0070/l.g006C /C.g00B9/percent.g00BA“/.notdef.g00E3/yright 62 /comma.g00D2/two.g0088/yright/exclam.g00AF/equal.g00C8/.notdef.g0106/comma.g00D2/L.g00AEx2 /dollar.g00DC10/dollar.g00DC50510x2 /dollar.g00DC10/dollar.g00DC50510x2 /dollar.g00DC10/dollar.g00DC50510 x2 /dollar.g00DC10/dollar.g00DC50510x2 /dollar.g00DC10/dollar.g00DC50510x1/dollar.g00DC10 /dollar.g00DC5 05 10 x1/dollar.g00DC10 /dollar.g00DC5 05 10 x1/dollar.g00DC10 /dollar.g00DC5 05 10 x1/dollar.g00DC10 /dollar.g00DC5 05 10x1/dollar.g00DC10 /dollar.g00DC5 05 1004 /g117 1036 /g117 103104 20 40 60 Рис. 11.9. Иллюстрация алгоритма EM для подгонки модели гауссовой смеси с тремя компонентами к двумерному набору данных. ( а) Набор данных; ( b) отрицательная логарифмическая вероятность (чем ниже, тем лучше) как функция от итераций ЕМ. Точки указывают итерации, для которых подходят компоненты смеси соответствующей GMM, показанные в пунктах ( c)–(f). Светлые диски показывают средние значения компонентов гауссовой смеси. На рис. 11.11( а) показана окончательная подгонка GMM\n--- Страница 456 ---\n456 Глава 1 1. Оценка плотности с помощью моделей гауссовой смеси (а) Подбор GMM после 62 итераций (b) Набор данных окрашен в соответстви и с обязанностями компонентов смесиx1x1x2 –2246 550 00–4 –6 x2 –2246 0 –4 –6 –5 –5 Рис. 11.10. Соответствие GMM и его обязанности при сближении EM. (а) GMM подходит, когда EM сходится. ( b) Каждая точка данных окрашена в соответствии с обязанностями компонентов смеси",
          "debug": {
            "start_page": 453,
            "end_page": 456
          }
        },
        {
          "name": "11.4. Скрытая перспектива",
          "content": "11.4. СКРЫТАЯ ПЕРСПЕКТИВА Мы можем взглянуть на GMM с точки зрения модели дискретных скрытых переменных, то есть где скрытая переменная z может принимать только конеч - ный набор значений. Это контрастирует с PCA, где скрытые переменные были числами с непрерывными значениями в М. Преимущества вероятностной перспективы заключаются в том, что (i) она оправдывает некоторые специальные решения, которые мы приняли в пре- дыдущих разделах, (ii) она позволяет конкретную интерпретацию ответствен - ности как апостериорные вероятности и (iii) итерационный алгоритм обнов - ления параметров модели может быть выведен как алгоритм EM для оценки параметра максимального правдоподобия в моделях со скрытыми перемен - ными.",
          "debug": {
            "start_page": 456,
            "end_page": 456
          }
        },
        {
          "name": "11.4.1. Генеративный процесс и вероятностная модель",
          "content": "--- Страница 456 --- (продолжение)\n1 1.4. СКРЫТАЯ ПЕРСПЕКТИВА Мы можем взглянуть на GMM с точки зрения модели дискретных скрытых переменных, то есть где скрытая переменная z может принимать только конеч - ный набор значений. Это контрастирует с PCA, где скрытые переменные были числами с непрерывными значениями в М. Преимущества вероятностной перспективы заключаются в том, что (i) она оправдывает некоторые специальные решения, которые мы приняли в пре- дыдущих разделах, (ii) она позволяет конкретную интерпретацию ответствен - ности как апостериорные вероятности и (iii) итерационный алгоритм обнов - ления параметров модели может быть выведен как алгоритм EM для оценки параметра максимального правдоподобия в моделях со скрытыми перемен - ными. 1 1.4.1. Генеративный процесс и вероятностная модель Чтобы вывести вероятностную модель для GMM, полезно подумать о генера - тивном процессе, то есть о процессе, который позволяет нам генерировать данные, используя вероятностную модель. Мы предполагаем, что модель смеси — с K компонентами и что точка данных x может быть сгенерирована ровно одним компонентом смеси. Мы вводим дво - ичную индикаторную переменную zk ∈ {0, 1} с двумя состояниями (раздел 6.2), которая указывает, сгенерировал ли k-й компонент смеси эту точку данных, так что p (x | zk = 1) =  (x | μk, Σk). (11.58)\n1 1.4.1. Генеративный процесс и вероятностная модель Чтобы вывести вероятностную модель для GMM, полезно подумать о генера - тивном процессе, то есть о процессе, который позволяет нам генерировать данные, используя вероятностную модель. Мы предполагаем, что модель смеси — с K компонентами и что точка данных x может быть сгенерирована ровно одним компонентом смеси. Мы вводим дво - ичную индикаторную переменную zk ∈ {0, 1} с двумя состояниями (раздел 6.2), которая указывает, сгенерировал ли k-й компонент смеси эту точку данных, так что p (x | zk = 1) =  (x | μk, Σk). (11.58)\n--- Страница 457 ---\n1 1.4. Скрытая перспектива 457 Определим z : = [z1, , zK]T ∈ K как вектор вероятности, состоящий из K − 1, многих нулей и ровно одной 1. Например, для K = 3 действительным z будет z = [z1, z2, z3]T = [0, 1, 0]T, что выберет вторую компоненту смеси, так как z2 = 1. ПРИМЕЧАНИЕ Иногда такое распределение вероятностей называют «муль - тинулли» — обобщением распределения Бернулли на более чем два значения (Murphy, 2012).  Свойства z подразумевают, что Следовательно, z — горячее кодиро ­ вание (также: представление 1 из K). До сих пор мы предполагали, что индикаторные переменные zk известны. Од - нако на практике это не так, и мы помещаем априорное распределение (11.59) от скрытой переменной z. Тогда k-я запись πk = p(zk = 1) (11.60) этого вектора вероятности описывает вероятность того, что k-й компонент сме - си сгенерировал точку данных x. ПРИМЕЧАНИЕ Построение этой модели скрытых переменных (см. cо- ответствующую графическую модель на рис. 11.11) позволяет использовать очень простую процедуру выборки (генеративный процесс) для генерации данных: 1. Выборка z (i) ∼ p(z). 2. Выборка x (i) ∼ p(x | z (i) = 1). π z x k = 1, . . . , KΣkµk Рис. 11.11. Графическая модель для GMM с одной точкой данных На первом этапе мы выбираем компонент смеси i (с помощью горячего кодиро - вания z) случайным образом в соответствии с p(z) = π; на втором этапе мы берем\n--- Страница 458 ---\n458 Глава 1 1. Оценка плотности с помощью моделей гауссовой смеси образец из соответствующего компонента смеси. Когда мы отбрасываем вы - борки скрытой переменной, так что у нас остается x(i), у нас есть действительные выборки из GMM. Такой вид выборки, когда выборки случайных величин за - висят от выборок от родителей переменной в графической модели, называется наследственной выборкой .  Обычно вероятностная модель определяется совместным распределением дан - ных и скрытых переменных (раздел 8.4). Используя априорную p(z), опреде - ленную в (11.59) и (11.60), и условную p(x | z) из (11.58), мы получаем все K компонент этого совместного распределения через p (x, zk = 1) = p(x | zk = 1)p(zk = 1) = πk (x | μk, Σk) (11.61) для k = 1, , K, так что (11.62) что полностью определяет вероятностную модель.",
          "debug": {
            "start_page": 456,
            "end_page": 458
          }
        },
        {
          "name": "11.4.2. Правдоподобие",
          "content": "--- Страница 458 --- (продолжение)\n1 1.4.2. Правдоподобие Чтобы получить вероятность p(x | θ) в модели со скрытыми переменными, нам нужно исключить скрытые переменные (раздел 8.4.3). В нашем случае это мож - но сделать, суммируя все скрытые переменные из соединения p(x, z) в (11.62), так чтобы (11.63) Теперь мы явно обусловливаем параметры θ вероятностной модели, которые мы ранее опускали. В (11.63) мы просуммируем по всем K возможным горячим кодировкам z, которое обозначается Σz. Поскольку в каждом z есть только одна ненулевая одиночная запись, существует только K возможных конфигураций/ настроек z. Например, если K = 3, то z может иметь конфигурации (11.64) Суммирование по всем возможным конфигурациям z в (11.63) эквивалентно просмотру ненулевого элемента z-вектора и записи (11.65 a)\n1 1.4.2. Правдоподобие Чтобы получить вероятность p(x | θ) в модели со скрытыми переменными, нам нужно исключить скрытые переменные (раздел 8.4.3). В нашем случае это мож - но сделать, суммируя все скрытые переменные из соединения p(x, z) в (11.62), так чтобы (11.63) Теперь мы явно обусловливаем параметры θ вероятностной модели, которые мы ранее опускали. В (11.63) мы просуммируем по всем K возможным горячим кодировкам z, которое обозначается Σz. Поскольку в каждом z есть только одна ненулевая одиночная запись, существует только K возможных конфигураций/ настроек z. Например, если K = 3, то z может иметь конфигурации (11.64) Суммирование по всем возможным конфигурациям z в (11.63) эквивалентно просмотру ненулевого элемента z-вектора и записи (11.65 a)\n--- Страница 459 ---\n1 1.4. Скрытая перспектива 459 (11.65 b) так что желаемое маржинальное распределение задается как (11.66 a) (11.66 b) которую мы идентифицируем как модель GMM из (11.3). Учитывая набор данных , мы сразу получаем вероятность (11.67) что и есть правдоподобие GMM из (11.9). Таким образом, модель со скрытыми переменными со скрытыми индикаторами zk — это эквивалентный подход к мо- дели гауссовой смеси.",
          "debug": {
            "start_page": 458,
            "end_page": 459
          }
        },
        {
          "name": "11.4.3. Апостериорное распределение",
          "content": "1 1.4.3. Апостериорное распределение Давайте кратко рассмотрим апостериорное распределение скрытой переменной z. Согласно теореме Байеса, апостериорная часть k-го компонента, сгенерировав - шая точку данных x, (11.68) где маргинал p(x) задан в (11.66 b). Это дает апостериорное распределение для k-й индикаторной переменной zk (11.69) которое мы определяем как ответственность k-го компонента смеси за точку данных x. Отметим, что мы опустили явное обусловливание параметров GMM πk, μk, Σk, где k = 1, , K.",
          "debug": {
            "start_page": 459,
            "end_page": 459
          }
        },
        {
          "name": "11.4.4. Расширение до полного набора данных",
          "content": "--- Страница 459 --- (продолжение)\n1 1.4.3. Апостериорное распределение Давайте кратко рассмотрим апостериорное распределение скрытой переменной z. Согласно теореме Байеса, апостериорная часть k-го компонента, сгенерировав - шая точку данных x, (11.68) где маргинал p(x) задан в (11.66 b). Это дает апостериорное распределение для k-й индикаторной переменной zk (11.69) которое мы определяем как ответственность k-го компонента смеси за точку данных x. Отметим, что мы опустили явное обусловливание параметров GMM πk, μk, Σk, где k = 1, , K. 1 1.4.4. Расширение до полного набора данных До сих пор мы обсуждали только случай, когда набор данных состоит только из одной точки данных x. Однако концепции априорного и апостериорного могут быть непосредственно расширены на случай N точек данных  : = {x1, , xN}.\n1 1.4.4. Расширение до полного набора данных До сих пор мы обсуждали только случай, когда набор данных состоит только из одной точки данных x. Однако концепции априорного и апостериорного могут быть непосредственно расширены на случай N точек данных  : = {x1, , xN}.\n--- Страница 460 ---\n460 Глава 1 1. Оценка плотности с помощью моделей гауссовой смеси В вероятностной интерпретации GMM каждая точка данных xn обладает соб - ственной скрытой переменной zn = [zn1, , znK]T ∈ K. (11.70) Раньше (когда мы рассматривали только одну точку данных x) мы опускали индекс n, но теперь это становится важным. Мы разделяем одно и то же априорное распределение π для всех скрытых пере - менных zn. Соответствующая графическая модель показана на рис. 11.12, где мы используем обозначения на табличке. π zn xn k = 1, . . . , Kn = 1, . . . , NΣkµk Рис. 11.12. Графическая модель GMM для N точек Условное распределение p(x1, , xN | z1, , zN) факторизуется по точкам данных и задается как . (11.71) Чтобы получить апостериорное распределение p(znk = 1 | xn), мы следуем тем же рассуждениям, что и в разделе 11.4.3, и применяем теорему Байеса для полу - чения (11.72 a) (11.72 b) Это означает, что p(zk = 1 | xn) — это (апостериорная) вероятность того, что k-й компонент смеси сгенерировал точку данных xn, и соответствует ответствен -\n--- Страница 461 ---\n1 1.5. Дополнительное чтение 461 ности rnk, которую мы ввели в (11.17). Теперь обязанности также имеют не только интуитивно понятную, но и математически обоснованную интерпретацию как апостериорные вероятности.",
          "debug": {
            "start_page": 459,
            "end_page": 461
          }
        },
        {
          "name": "11.4.5. Еще раз про EM-алгоритм",
          "content": "11.4.5. Еще раз про EM-алгоритм Алгоритм EM, который мы представили как итеративную схему для оценки максимального правдоподобия, может быть принципиально выведен с точки зрения скрытых переменных. Учитывая текущую настройку θ(t) параметров модели, E-шаг вычисляет ожидаемое логарифмическое правдоподобие ; (11.73 a) (11.73 b) где математическое ожидание log p(x, z | θ) берется относительно апостериор - ного p(z | x, θ(t)) скрытых переменных. На M-шаге выбирается обновленный набор параметров модели θ(t + 1) путем максимизации (11.73 b). Хотя итерация EM увеличивает логарифмическую вероятность, нет никаких гарантий, что EM сходится к решению с максимальным правдоподобием. Воз - можно, что алгоритм EM сходится к локальному максимуму логарифма прав - доподобия. Различные инициализации параметров θ могут использоваться в нескольких прогонах EM, чтобы снизить риск попадания в плохой локальный оптимум. Мы не будем вдаваться в подробности здесь, а обратимся к превос - ходным изложениям Rogers and Girolami (2016) и Bishop (2006).",
          "debug": {
            "start_page": 461,
            "end_page": 461
          }
        },
        {
          "name": "11.5. Дополнительное чтение",
          "content": "--- Страница 461 --- (продолжение)\n1 1.4.5. Еще раз про EM-алгоритм Алгоритм EM, который мы представили как итеративную схему для оценки максимального правдоподобия, может быть принципиально выведен с точки зрения скрытых переменных. Учитывая текущую настройку θ(t) параметров модели, E-шаг вычисляет ожидаемое логарифмическое правдоподобие ; (11.73 a) (11.73 b) где математическое ожидание log p(x, z | θ) берется относительно апостериор - ного p(z | x, θ(t)) скрытых переменных. На M-шаге выбирается обновленный набор параметров модели θ(t + 1) путем максимизации (11.73 b). Хотя итерация EM увеличивает логарифмическую вероятность, нет никаких гарантий, что EM сходится к решению с максимальным правдоподобием. Воз - можно, что алгоритм EM сходится к локальному максимуму логарифма прав - доподобия. Различные инициализации параметров θ могут использоваться в нескольких прогонах EM, чтобы снизить риск попадания в плохой локальный оптимум. Мы не будем вдаваться в подробности здесь, а обратимся к превос - ходным изложениям Rogers and Girolami (2016) и Bishop (2006). 1 1.5. ДОПОЛНИТЕЛЬНОЕ ЧТЕНИЕ GMM можно рассматривать как генеративную модель в том смысле, что легко генерировать новые данные с использованием наследственной выборки (Bishop, 2006). Для заданных параметров GMM πk, μk, Σk, k = 1, , K, мы выбираем индекс k из вектора вероятности [ π1, , πK], а затем выбираем точку данных x ∼  (μk, Σk). Если мы повторим это N раз, мы получим набор данных, созданный GMM. Рисунок 11.1 был создан с использованием этой процедуры. На протяжении всей главы мы предполагали, что количество компонентов K известно. На практике часто бывает не так. Однако мы могли бы использовать вложенную перекрестную проверку, рассмотренную в разделе 8.6.1, чтобы най - ти хорошие модели. Модели гауссовой смеси тесно связаны с алгоритмом кластеризации K средних. Алгоритм кластеризации K средних также использует алгоритм EM для назна - чения точек данных кластерам. Если мы рассматриваем средние в GMM как\n1 1.5. ДОПОЛНИТЕЛЬНОЕ ЧТЕНИЕ GMM можно рассматривать как генеративную модель в том смысле, что легко генерировать новые данные с использованием наследственной выборки (Bishop, 2006). Для заданных параметров GMM πk, μk, Σk, k = 1, , K, мы выбираем индекс k из вектора вероятности [ π1, , πK], а затем выбираем точку данных x ∼  (μk, Σk). Если мы повторим это N раз, мы получим набор данных, созданный GMM. Рисунок 11.1 был создан с использованием этой процедуры. На протяжении всей главы мы предполагали, что количество компонентов K известно. На практике часто бывает не так. Однако мы могли бы использовать вложенную перекрестную проверку, рассмотренную в разделе 8.6.1, чтобы най - ти хорошие модели. Модели гауссовой смеси тесно связаны с алгоритмом кластеризации K средних. Алгоритм кластеризации K средних также использует алгоритм EM для назна - чения точек данных кластерам. Если мы рассматриваем средние в GMM как\n--- Страница 462 ---\n462 Глава 1 1. Оценка плотности с помощью моделей гауссовой смеси центры кластеров и игнорируем ковариации (или устанавливаем их равными I), мы приходим к алгоритму K средних. Как также хорошо описано MacKay (2003), алгоритм K средних выполняет «жесткое» назначение точек данных центрам кластера μk, тогда как GMM выполняет «мягкое» назначение через обязанности. Мы только коснулись перспективы GMM и алгоритма EM со скрытыми пере - менными. Обратите внимание, что EM может использоваться для обучения параметрам в общих моделях с латентными переменными, например в нелиней - ных моделях пространства состояний (Ghahramani and Roweis, 1999; Roweis and Ghahramani, 1999), а также для обучения с подкреплением, как обсуждалось в Barber (2012). Следовательно, перспектива скрытых переменных в GMM по - лезна для принципиального вывода соответствующего алгоритма EM (Bishop, 2006; Barber, 2012; Murphy, 2012). Мы обсудили только оценку максимального правдоподобия (с помощью алго - ритма EM) для нахождения параметров GMM. Здесь также применимы стан - дартные критические замечания максимальной вероятности: zКак и в случае линейной регрессии, максимальная вероятность может по - страдать от серьезного переобучения. В случае GMM это происходит, когда среднее значение компонента смеси идентично точке данных, а ковариация стремится к 0. Затем вероятность приближается к бесконечности. Bishop (2006) и Barber (2012) подробно обсуждают этот вопрос. zzМы получаем точечную оценку параметров πk, μk, Σk только для k = 1, , K, что не указывает на неопределенность значений параметров. Байесовский подход предусматривает априорность параметров, которую можно исполь - зовать для получения апостериорного распределения параметров. Эта апо - стериорная оценка позволяет нам вычислить свидетельство модели (предель - ное правдоподобие), которое можно использовать для сравнения моделей, что дает нам принципиальный способ определения количества компонентов смеси. К сожалению, вывод в замкнутой форме невозможен в этой настрой - ке, потому что для этой модели нет сопряженного априорного значения. Однако приближения, такие как вариационный вывод, можно использовать для получения приблизительного апостериорного анализа (Bishop, 2006). В этой главе мы обсудили модели смеси для оценки плотности. Существует множество доступных методов оценки плотности. На практике мы часто ис - пользуем гистограммы и оценку плотности ядра. Гистограммы обеспечивают непараметрический способ представления непре - рывных плотностей и были предложены Pearson (1895). Гистограмма строится путем «разделения» пространства данных и подсчета количества точек данных, попадающих в каждую ячейку. Затем в центре каждой ячейки рисуется полоса, высота которой пропорциональна количеству точек данных в этой ячейке. Раз -\n--- Страница 463 ---\n1 1.5. Дополнительное чтение 463 мер ячейки является критическим гиперпараметром, и неправильный выбор может привести к переобучению или неполному подбору. Перекрестная про - верка, как описано в разделе 8.2.4, может использоваться для определения под - ходящего размера ячейки. Оценка плотности ядра (Kernel density estimation, KDE), независимо предло - женная Rosenblatt (1956) и Parzen (1962), является непараметрическим спосо - бом оценки плотности. Учитывая N независимых и одинаково распределенных выборок, оценщик плотности ядра представляет лежащее в основе распределе - ние как (11.74) где k — функция ядра, то есть неотрицательная функция, которая интегрирует - ся до 1, а h > 0 — параметр сглаживания/полосы пропускания, который играет такую же роль, как размер ячейки в гистограммах. Обратите внимание, что мы помещаем ядро в каждую точку данных xn в наборе данных. Обычно использу - емые ядерные функции — это равномерное распределение и распределение Гаусса. Оценки плотности ядра тесно связаны с гистограммами, но, выбирая подходящее ядро, мы можем гарантировать гладкость оценки плотности. На рис. 11.13 показана разница между гистограммой и оценкой плотности ядра (с ядром в форме Гаусса) для данного набора данных из 250 точек данных. /d.g0069/equal.g00C8……/slash.g00A9/yright KDE /c.g0080/comma.g00D2“/two.g0088/percent.g00BA/.notdef.g0104/exclam.g00AF/equal.g00C8/.notdef.g00E4/.notdef.g00E4/equal.g00C8 x/dollar.g00DC4 /dollar.g00DC2 2 04 680,000,050,100,150,200,250,30p(x) Рис. 11.13. Гистограмма (столбцы с серой заливкой) и оценка плотности ядра (черная линия). Механизм оценки плотности ядра дает гладкую оценку отсчетной плотности, в то время как гистограмма дает несглаженную счетную меру и позволяет измерить, сколько (черных) точек попадают в один интервал\n--- Страница 464 ---\n12 Классификация методом опорных векторов Зачастую мы хотим, чтобы алгоритм машинного обучения в качестве предска - зания выбирал один из дискретного множества классов. Например, почтовый клиент должен отличать спам от личного письма (два класса). При астрономи - ческих наблюдениях нужно идентифицировать небесный объект как галактику, звезду или планету. Возможных классов, как правило, немного, и, что важнее, у них обычно нет дополнительной структуры1. В данной главе мы рассмотрим задачу бинарной классификации — предсказание одного из двух возможных значений. Заметим, что постановка здесь отличается от задачи из главы 9 тем, что там мы предсказывали значения из непрерывного множества. В задаче бинарной классификации множество возможных ответов (меток классов) состоит из двух элементов, которые мы обозначим как {+1, −1}. Иными словами, мы работаем с предсказаниями вида f : D → {+1, −1}. (12.1) Вспомним, как в главе 8 мы записывали каждый пример (объект обучающей выборки) xn в виде вектора признаков, состоящего из D вещественных чисел2. Метки классов часто называют соответственно положительным и отрицательным классом. Заметим, что «положительность» класса +1 вовсе не обязательно оз - начает что-то хорошее. Например, в задаче диагностики рака, как правило, метка +1 соответствует пациенту, у которого диагностировано это заболевание. 1 Примером структуры является наличие порядка вариантов ответа: например, размеры футболок упорядочены (S, M, L). 2 Объекты обучающей выборки также называют точками данных, входными значения - ми или примерами.\n--- Страница 465 ---\n465 Классификация методом опорных векторов В принципе, можно использовать в качестве меток любое множество из двух элементов, например, {истина, ложь}, {0, 1} или {красный, синий}1. Задача би - нарной классификации хорошо изучена, и мы отложим обзор альтернативных подходов до раздела 12.6. Сейчас для решения задачи бинарной классификации рассмотрим так называ - емый метод опорных векторов (support vector machine, SVM). Эта задача, как и задача регрессии, относится к машинному обучению с учителем. Имеется набор объектов xn ∈ D с соответствующими бинарными метками yn ∈ {+1, −1}. По обучающей выборке из пар «объект — метка» {( x1, y1), , ( xN, yN)} мы хотим найти параметры модели, дающие наименьшую ошибку классификации. Как и в главе 9, мы строим линейную модель, а всю нелинейность зашиваем в пре- образование признаков ϕ (9.13). Мы еще вернемся к преобразованию ϕ в раз- деле 12.4. Метод опорных векторов на данный момент является одним из лучших для многих приложений, а кроме того, для него есть теоретические гарантии качества результата (Steinwart and Christmann, 2008). Есть две главные причины тому, что мы решили рассказывать про бинарную классификацию, используя SVM. Во-первых, этот метод позволяет думать о задачах обучения с учителем геоме - трически. В то время как в главе 9 мы рассматривали задачу МО в терминах вероятностных моделей и решали ее с помощью оценок максимального прав - доподобия и байесовского инференса, в данной главе мы рассмотрим другой подход, основанный на геометрических рассуждениях. Важнейшую роль в нем играют понятия, введенные в главе 3, например скалярные произведения и про- екции. Во-вторых, задачи, в поторых используется метод SVM , не имеют ана - литического решения — так что нам остается пользоваться различными мето - дами оптимизации из главы 7. Метод опорных векторов по своим основным идеям отличается от подхода максимального правдоподобия из главы 9. В последнем модель, приводящая в итоге к задаче оптимизации, строится с помощью вероятностного подхода (рассматривается распределение данных). Метод опорных векторов строит функцию, оптимизируемую при обучении, используя геометрические сообра - жения. Нечто подобное мы уже видели в главе 10, выводя из геометрических идей метод главных компонент. В случае SVM мы начинаем с построения це - левой функции, а потом минимизируем ее на обучающей выборке методами минимизации эмпирического риска (раздел 8.1). Можно трактовать этот подход как построение специфической функции потерь. 1 В вероятностных задачах удобнее использовать бинарное представление {0, 1}, см. примечание после примера 6.12.\n--- Страница 466 ---\n466",
          "debug": {
            "start_page": 461,
            "end_page": 466
          }
        }
      ]
    },
    {
      "name": "Глава 12. Классификация методом опорных векторов",
      "content": "(Не найдено)"
    },
    {
      "name": "Глава 12. Классификация методом опорных векторов 464",
      "chapters": [
        {
          "name": "12.1. Разделяющие гиперплоскости",
          "content": "--- Страница 466 --- (продолжение)\nГлава 1 2. Классификация методом опорных векторов x(1)x(2) Рис. 12.1. Пример двумерных данных, на котором видно наличие линейного классификатора, способного разделить светлые крестики и темные кружочки Давайте сформулируем задачу оптимизации, возникающую при использовании метода опорных векторов на обучающем множестве пар «объект — метка». На рис. 12.1 показаны данные, которые можно разделить на два класса гипер - плоскостью. Каждый из объектов xn (векторов размерности 2) будет точкой в двумерном пространстве (с координатами и ), а соответствующая мет - ка показана одним из двух символов — светлым крестиком или темным кружоч - ком. Слово «гиперплоскость» часто употребляется в МО, и мы уже знакомились с гиперплоскостями в разделе 2.8. Гиперплоскостью называют аффинное под - пространство размерности D − 1 (в векторном пространстве размерности D). Объекты делятся на два класса (соответствующие двум возможным меткам), при этом их признаки (координаты представляющих их векторов) должны по - зволять нам разделить классы прямой линией. Далее мы формализуем идею «найти линейный разделитель классов». Введем идею отступа и расширим понятие линейного разделения, позволив примерам попадать на «неправильную» сторону, то есть допустив ошибки классификации. Покажем два способа формализации метода опорных векторов: геометрический (в разделе 12.2.4) и с помощью функции потерь (в разделе 12.2.5). Выведем двойственную задачу SVM, используя множители Лагранжа (раздел 7.2). Двойственная задача поможет нам увидеть третий способ смотреть на SVM: в терминах выпуклых оболочек для примеров каждого класса (раздел 12.3.2). В конце вкратце обсудим ядра и численное решение задачи SVM с нелинейным ядром. 1 2.1. РАЗДЕЛЯЮЩИЕ ГИПЕРПЛОСКОСТИ Один из способов вычислить сходство между двумя примерами, заданными векторами xi и xj, — найти скалярное произведение . Из раздела 3.2 мы знаем, что скалярные произведения связаны с углом между двумя векторами. Значение скалярного произведения зависит от длины (нормы) каждого из век -\n1 2.1. РАЗДЕЛЯЮЩИЕ ГИПЕРПЛОСКОСТИ Один из способов вычислить сходство между двумя примерами, заданными векторами xi и xj, — найти скалярное произведение . Из раздела 3.2 мы знаем, что скалярные произведения связаны с углом между двумя векторами. Значение скалярного произведения зависит от длины (нормы) каждого из век -\n--- Страница 467 ---\n1 2.1. Разделяющие гиперплоскости 467 торов. Далее скалярные произведения позволяют нам строго определить такие геометрические понятия, как ортогональность и проекции. Главная идея многих алгоритмов классификации — представить данные как векторы в D, а затем разбить пространство на части, в идеале так, чтобы в каждой из частей находились примеры одного класса и только они. В случае бинарной классификации пространство будет разделено на две части, соответствующие положительному и отрицательному классам. Мы рассмотрим особенно удобный способ разделить пространство на две части — с помощью гиперплоскости. Рассмотрим пример x ∈ D из пространства данных. Возьмем функцию f : D → ; (12.2 a) (12.2 b) с параметрами w ∈ D и b ∈ . Вспомним (раздел 2.8), что гиперплоскости яв - ляются аффинными подпространствами. Таким образом, гиперплоскость, раз - деляющую два класса в нашей задаче, можно задать как { x ∈ D : f(x) = 0}. (12.3) (a) Разделяющая гиперплоскость в трехмерном пространстве(b) Проекция (a) на плоскостьw Положительный класс . Отрицательный классw b 0 Рис. 12.2. Уравнение разделяющей гиперплоскости (12.3). ( a) Стандартное представление в трехмерном пространстве. ( b) Для удобства изображения мы смотрим на гиперплоскость «с ребра» Гиперплоскость изображена на рис. 12.2, где вектор w — нормаль к гиперпло - скости, а b — смещение. Мы можем показать, что w — нормаль к гиперплоскости (12.3), выбрав две произвольные точки xa и xb на гиперплоскости и показав, что соединяющий их вектор ортогонален w. Это можно записать так: (12.4 a) (12.4 b)\n--- Страница 468 ---\n468 Глава 1 2. Классификация методом опорных векторов где вторая строка получена из линейности скалярного произведения (раздел 3.2). Так как мы выбрали xa и xb принадлежащими гиперплоскости, то f(xa) = 0 и f(xb) = = 0, значит, . Вспомним, что два вектора ортогональны, если их скалярное произведение равно нулю. Таким образом, w ортогонален любому вектору из гиперплоскости. ПРИМЕЧАНИЕ Из главы 2 мы знаем, что можно смотреть на векторы с разных точек зрения. В данной главе мы будем воспринимать вектор w как «стрелку», указывающую направление, то есть как геометрический вектор. Напротив, о векторе x мы будем думать как о точке, то есть как о наборе координат отно - сительно стандартного базиса.  Получив тестовый объект, мы классифицируем его как положительный или отрицательный, в зависимости от того, с какой стороны от гиперплоскости он оказался. Заметим, что формула (12.3) не только задает гиперплоскость, но и определяет направление, а значит, «положительную и отрицательную сторо - ны» по отношению к гиперплоскости. Таким образом, чтобы классифицировать тестовый пример xtest, мы должны вычислить значение функции f(xtest), а затем присвоить примеру метку класса +1 при f(xtest) ≥ 0 и метку класса −1 в против - ном случае. С геометрической точки зрения, положительные примеры лежат «над» гипер - плоскостью, а отрицательные — «под» ней. При обучении классификатора мы хотим добиться того, чтобы примеры с положительными метками находились с положительной стороны от гиперплоскости, то есть , (12.5) а примеры с отрицательными метками — с отрицательной стороны, то есть (12.6) Помочь в геометрическом понимании классификации может рис. 12.2. Два при - веденных выше условия часто объединяют формулой (12.7) Уравнение (12.7) эквивалентно (12.5) и (12.6) благодаря умножению (12.5) и (12.6) соответственно на yn = 1 и yn = –1.",
          "debug": {
            "start_page": 466,
            "end_page": 468
          }
        },
        {
          "name": "12.2. Прямая задача метода опорных векторов",
          "content": "--- Страница 468 --- (продолжение)\n1 2.2. ПРЯМАЯ ЗАДАЧА МЕТОДА ОПОРНЫХ ВЕКТОРОВ Мы знакомы с понятием расстояния от точки до гиперплоскости и теперь гото - вы обсуждать метод опорных векторов. Для линейно разделимого набора данных\n1 2.2. ПРЯМАЯ ЗАДАЧА МЕТОДА ОПОРНЫХ ВЕКТОРОВ Мы знакомы с понятием расстояния от точки до гиперплоскости и теперь гото - вы обсуждать метод опорных векторов. Для линейно разделимого набора данных\n--- Страница 469 ---\n1 2.2. Прямая задача метода опорных векторов 469 {(x1, y1), , ( xN, yN)} существует бесконечное число подходящих гиперплоскостей (см. рис. 12.3), а значит, и классификаторов, решающих задачу без ошибок (на обучающей выборке). Один из вариантов, как прийти к единственному реше - нию — выбрать гиперплоскость, максимизирующую отступ между классами. Другими словами, мы хотим, чтобы отступ между положительными и отрица - тельными примерами был как можно больше (раздел 12.2.1)1. Далее мы займем - ся вычислением расстояния от объекта до гиперплоскости. Вспомним, что ближайшая к данной точке xn точка на гиперплоскости получается с помощью ортогональной проекции (раздел 3.8). x(1)x(2) Рис. 12.3. Возможные разделяющие гиперплоскости. Существует множество линейных классификаторов (сплошные линии), отделяющих светлые крестики от темных кружочков",
          "debug": {
            "start_page": 468,
            "end_page": 469
          }
        },
        {
          "name": "12.2.1. Понятие зазора",
          "content": "--- Страница 469 --- (продолжение)\n1 2.2.1. Понятие зазора Понятие отступа достаточно простое: это расстояние от разделяющей гипер - плоскости до ближайшего объекта из выборки2 (здесь мы предполагаем, что выборка линейно разделима). Однако при дальнейшей формализации возни - кает один технический нюанс: нам нужна шкала для измерения расстояний. Можно просто использовать ту же шкалу, в которой нам даны значения данных xn. Здесь возможна проблема: если мы изменим единицы измерения для xn, их значения также поменяются, а вместе с ними и расстояние до гиперплоскости. Как мы вскоре увидим, шкала измерения расстояний будет определена из само - го уравнения гиперплоскости (12.3). Рассмотрим гиперплоскость и пример xa, как показано на рис. 12.4. Не умаляя общности, мы можем считать, что объект xa расположен с положи - тельной стороны от гиперплоскости, то есть . Мы хотим вычислить расстояние r > 0 от xa до гиперплоскости. Для этого мы находим ортогональную проекцию (раздел 3.8) xa на гиперплоскость, обозначаемую . Так как w орто - 1 Классификатор с большим отступом обладает хорошей способностью к обобщению (Steinwart and Christmann, 2008). 2 Ближайших объектов может быть несколько.\n1 2.2.1. Понятие зазора Понятие отступа достаточно простое: это расстояние от разделяющей гипер - плоскости до ближайшего объекта из выборки2 (здесь мы предполагаем, что выборка линейно разделима). Однако при дальнейшей формализации возни - кает один технический нюанс: нам нужна шкала для измерения расстояний. Можно просто использовать ту же шкалу, в которой нам даны значения данных xn. Здесь возможна проблема: если мы изменим единицы измерения для xn, их значения также поменяются, а вместе с ними и расстояние до гиперплоскости. Как мы вскоре увидим, шкала измерения расстояний будет определена из само - го уравнения гиперплоскости (12.3). Рассмотрим гиперплоскость и пример xa, как показано на рис. 12.4. Не умаляя общности, мы можем считать, что объект xa расположен с положи - тельной стороны от гиперплоскости, то есть . Мы хотим вычислить расстояние r > 0 от xa до гиперплоскости. Для этого мы находим ортогональную проекцию (раздел 3.8) xa на гиперплоскость, обозначаемую . Так как w орто - 1 Классификатор с большим отступом обладает хорошей способностью к обобщению (Steinwart and Christmann, 2008). 2 Ближайших объектов может быть несколько.\n--- Страница 470 ---\n470 Глава 1 2. Классификация методом опорных векторов гонален гиперплоскости, расстояние r — это коэффициент, на который умножа - ется w. Если известна длина w, можно найти коэффициент r и «настоящее» расстояние от xa до . Для удобства мы возьмем вектор единичной длины, полученный делением w на его норму || w || (раздел 2.4), и заметим, что . (12.8) .r xa′w 0xa Рис. 12.4. Сложение векторов для нахождения расстояния до гиперплоскости: Другой способ — воспринимать число r как координату xa в подпространстве, натянутом на w/ || w ||. Теперь r — расстояние от xa до гиперплоскости, и если xa — ближайшая к гиперплоскости точка данных, то r называют отступом. Мы хотим, чтобы положительные примеры находились на расстоянии, не мень - шем r, от гиперплоскости, а отрицательные примеры — также на расстоянии, не меньшем r, но с противоположной стороны. Аналогично тому, как мы получали формулу (12.7) из (12.5) и (12.6), можно переформулировать наши цели как (12.9) Таким образом мы объединили требования к расстояниям между положитель - ными примерами и гиперплоскостью и к расстояниям между отрицательными примерами и гиперплоскостью в одно неравенство. Так как нас интересует только направление вектора параметров w, мы добавля - ем предположение, что w — вектор единичной длины, || w || = 1, где || w || = — евклидова норма (раздел 3.1)1. Это предположение также делает более понятным определение расстояния r (12.8) как коэффициента при векторе единичной длины. ПРИМЕЧАНИЕ Читатель, знакомый с другими определениями отступа, за - метит, что наше предположение || w || = 1 отличается от стандартного изложения 1 Использование других скалярных произведений (раздел 3.2) мы увидим в раз- деле 12.4.\n--- Страница 471 ---\n1 2.2. Прямая задача метода опорных векторов 471 метода опорных векторов, например у Шелькопфа и Смолы (Sch ölkopf and Smola, 2002). В разделе 12.2.3 мы покажем эквивалентность обоих подходов.  Объединяя три требования в одну задачу оптимизации с ограничениями (ус - ловной оптимизации), получим целевую функцию (12.10) это означает, что мы хотим максимизировать отступ r, при этом данные должны лежать с правильной стороны от гиперплоскости. ПРИМЕЧАНИЕ Понятие отступа активно проникает в разные области ма - шинного обучения. Владимир Вапник и Алексей Червоненкис использовали его, чтобы показать, что при большом отступе «сложность» класса функций низкая и возможно обучение (Vapnik, 2000). Оно также оказалось полезным в различных методах теоретического анализа ошибок обобщения (Steinwart and Christmann, 2008; Shalev-Shwartz and Ben-David, 2014). ",
          "debug": {
            "start_page": 469,
            "end_page": 471
          }
        },
        {
          "name": "12.2.2. Нахождение зазора: традиционный способ",
          "content": "--- Страница 471 --- (продолжение)\n1 2.2.2. Нахождение зазора: традиционный способ В предыдущем разделе мы вывели функцию (12.10), заметили, что нас интере - сует направление w, но не его длина, и приняли, что || w || = 1. В этом разделе мы придем к задаче максимизации отступа из других предпосылок. Вместо взятия нормализованного вектора параметров мы поменяем шкалу измерения для данных. Мы делаем это перенормирование так, чтобы значение предсказания на ближайшем объекте было равно 1. Обозначим ближайший к ги- перплоскости пример за xa1. Рисунок 12.5 совпадает с рис. 12.4, за исключением перенормирования осей, так что пример xa лежит в точности на расстоянии отступа, . Так как точка — ортогональная проекция xa на гиперплоскость, она по определению лежит на гиперплоскости, то есть (12.11) Подставив (12.8) в (12.11), получим (12.12) 1 Напомним, что пока мы рассматриваем линейно разделимые выборки.\n1 2.2.2. Нахождение зазора: традиционный способ В предыдущем разделе мы вывели функцию (12.10), заметили, что нас интере - сует направление w, но не его длина, и приняли, что || w || = 1. В этом разделе мы придем к задаче максимизации отступа из других предпосылок. Вместо взятия нормализованного вектора параметров мы поменяем шкалу измерения для данных. Мы делаем это перенормирование так, чтобы значение предсказания на ближайшем объекте было равно 1. Обозначим ближайший к ги- перплоскости пример за xa1. Рисунок 12.5 совпадает с рис. 12.4, за исключением перенормирования осей, так что пример xa лежит в точности на расстоянии отступа, . Так как точка — ортогональная проекция xa на гиперплоскость, она по определению лежит на гиперплоскости, то есть (12.11) Подставив (12.8) в (12.11), получим (12.12) 1 Напомним, что пока мы рассматриваем линейно разделимые выборки.\n--- Страница 472 ---\n472 Глава 1 2. Классификация методом опорных векторов . w, x + b = 0w, x + b = 1.r xa′wxa Рис. 12.5. Нахождение отступа: r = 1/|| w || Пользуясь билинейностью скалярного произведения (раздел 3.2), приходим к (12.13) Заметим, что благодаря нашему перенормированию первое слагаемое равно 1, то есть . Из формулы (3.16) раздела 3.1 мы знаем, что < w, w>= || w ||2, и второе слагаемое превращается в . Благодаря этим упрощениям получаем (12.14) Таким образом мы выразили расстояние r через нормальный к гиперплоскости вектор w. На первый взгляд, это равенство нелогично: мы выразили расстояние от гиперплоскости через длину вектора w, но этот вектор мы пока не знаем. Однако можно думать о расстоянии r как о временной величине, введенной только для данного рассуждения1. Далее в этой главе мы будем обозначать рас - стояние до гиперплоскости как . В разделе 12.2.3 мы увидим, что выбор отступа, равного 1, эквивалентен нашему предыдущему предположению || w || = 1 из раздела 12.2.1. Мы хотим, чтобы и положительные, и отрицательные примеры находились хотя бы на единичном расстоянии от гиперплоскости. Воспользуемся тем же спосо - бом, что и при выводе (12.9), и запишем наши условия как (12.15) 1 Можно также рассматривать расстояние как «ошибку проекции» xa на гиперпло - скость.\n--- Страница 473 ---\n1 2.2. Прямая задача метода опорных векторов 473 Объединяя требования максимизации отступа и того, чтобы примеры находились с правильной стороны от гиперплоскости, получим (12.16) для n = 1, , N. (12.17) Вместо максимизации обратной нормы, как в (12.16), часто минимизируют квадрат нормы1, нередко вводя множитель 1/2, не влияющий на оптимальные значения w, b, но дающий более приятный вид градиента. Тогда наша целевая функция превращается в (12.18) для n = 1, , N. (12.19) Задача (12.18) известна как SVM с жестким зазором (hard margin SVM). «Жест - кость» проявляется в том, что данная формулировка не допускает нарушений условий использования зазора. В разделе 12.2.4 мы увидим, что это условие можно ослабить, допустив нарушения в тех случаях, когда данные не являются линейно разделимыми.",
          "debug": {
            "start_page": 471,
            "end_page": 473
          }
        },
        {
          "name": "12.2.3. Почему можно взять зазор, равный 1",
          "content": "--- Страница 473 --- (продолжение)\n1 2.2.3. Почему можно взять зазор, равный 1 В разделе 12.2.1 мы договорились максимизировать значение r, расстояния от ближайшего примера до гиперплоскости. В разделе 12.2.2 мы изменяли масштаб данных так, чтобы ближайший пример лежал на расстоянии 1 от гиперплоско - сти. В данном разделе мы свяжем эти два подхода и поймем, что на самом деле они равносильны. Теорема 12.1. Максимизация зазора r при нормализованных весах, как в форму ­ ле (12.10): (12.20) эквивалентна перенормированию данных, при котором зазор равен 1: (12.21) 1 Квадрат нормы дает выпуклую задачу квадратичного программирования (раздел 12.5).\n1 2.2.3. Почему можно взять зазор, равный 1 В разделе 12.2.1 мы договорились максимизировать значение r, расстояния от ближайшего примера до гиперплоскости. В разделе 12.2.2 мы изменяли масштаб данных так, чтобы ближайший пример лежал на расстоянии 1 от гиперплоско - сти. В данном разделе мы свяжем эти два подхода и поймем, что на самом деле они равносильны. Теорема 12.1. Максимизация зазора r при нормализованных весах, как в форму ­ ле (12.10): (12.20) эквивалентна перенормированию данных, при котором зазор равен 1: (12.21) 1 Квадрат нормы дает выпуклую задачу квадратичного программирования (раздел 12.5).\n--- Страница 474 ---\n474 Глава 1 2. Классификация методом опорных векторов Доказательство . Рассмотрим (12.20). Так как возведение в квадрат строго моно - тонно на положительной полуоси, точка достижения максимума не изменится, если брать в качестве целевой функции r 2. Так как || w || = 1, можно перепара- метризовать уравнение с ненормализованным вектором весов , используя . Мы получим (12.22) Условия (12.22) явно задают, что r положительно. Поэтому можно разделить обе части первого условия на r 1и получить (12.23) переименовав параметры в и . Так как , приходим к (12.24) После подстановки результата в (12.23) задача выглядит как (12.25) Последний шаг — убедиться, что максимизация дает тот же результат, что и минимизация , и теорема 12.1 доказана.",
          "debug": {
            "start_page": 473,
            "end_page": 474
          }
        },
        {
          "name": "12.2.4. SVM с мягким зазором: геометрический подход",
          "content": "--- Страница 474 --- (продолжение)\n1 2.2.4. SVM с мягким зазором: геометрический подход В случае, когда данные не являются линейно разделимыми, мы можем допустить, чтобы некоторые примеры попадали в разделяющую полосу или даже оказыва - лись с неверной стороны от гиперплоскости, как показано на рис. 12.6. 1 Обратите внимание, что r > 0, поскольку мы предполагаем линейную сепарабель - ность, следовательно проблема с делением на r не возникает.\n1 2.2.4. SVM с мягким зазором: геометрический подход В случае, когда данные не являются линейно разделимыми, мы можем допустить, чтобы некоторые примеры попадали в разделяющую полосу или даже оказыва - лись с неверной стороны от гиперплоскости, как показано на рис. 12.6. 1 Обратите внимание, что r > 0, поскольку мы предполагаем линейную сепарабель - ность, следовательно проблема с делением на r не возникает.\n--- Страница 475 ---\n1 2.2. Прямая задача метода опорных векторов 475 (a) Линейно разделимые данные с широкой разделяющей полосой(b) Линейно неразделимые данныеx(1)x(2) x(1)x(2) Рис. 12.6. (a) Линейно разделимые и ( b) линейно неразделимые данные Модель, допускающую ошибки классификации, называют SVM с мягким зазором (soft margin SVM). В этом разделе мы придем к соответствующей задаче опти - мизации с помощью геометрических рассуждений. В разделе 12.2.5 мы выведем эквивалентную задачу оптимизации, пользуясь понятием функции потерь. Используя множители Лагранжа (раздел 7.2), мы поставим двойственную за - дачу оптимизации для SVM в разделе 12.3. Эта двойственная задача даст нам третий способ интерпретировать SVM: как проведение гиперплоскости, про - ходящей посередине между выпуклыми оболочками объектов положительного и отрицательного классов (раздел 12.3.2). ξ x+ w, x + b = 0w, x + b = 1w Рис. 12.7. Задача SVM с мягким зазором допускает, что объекты попадают в разделяющую полосу или даже оказываются с невер- ной стороны от гиперплоскости. Невязка ξ, если x + оказался с неверной стороны, измеряет расстояние от положительного примера x + до гиперплоскости положительного зазора Ключевая геометрическая идея — ввести для каждой пары «объект — метка» (xn, yn) невязку ξn, позволяющую объекту попадать в разделяющую полосу или даже оказываться с неверной стороны от гиперплоскости (см. рис. 12.7). Зна - чение ξn вычитается из зазора, при этом оно должно быть неотрицательным. Для поощрения правильной классификации мы прибавляем ξn к целевой функции\n--- Страница 476 ---\n476 Глава 1 2. Классификация методом опорных векторов (12.26 a) (12.26 b) , (12.26 c) для n = 1, , N. Данную задачу называют задачей оптимизации (12.18) уже не для SVM с жестким зазором, а для SVM с мягким зазором . Параметр C > 0 от - вечает за компромисс между шириной разделяющего зазора и общей суммой невязок. Его называют параметром регуляризации . Как мы увидим в следующем разделе, первое слагаемое в целевой функции (12.26 a) выполняет роль регуляризатора , и во многих книгах по численной оптимизации именно его домножают на параметр регуляризации (раздел 8.2.3). В этом разделе мы ис - пользуем другую формулировку, в которой б ольшие значения C соответствуют слабой регуляризации — давая больший вес поправкам и, соответственно, боль - ший приоритет объектам, оказавшимся с неправильной стороны от гиперпло - скости. ПРИМЕЧАНИЕ В формулировке задачи SVM с мягким зазором (12.26 a) регуляризован параметр w, но не b, — регуляризатор b не содержит1. Этот не - регуляризованный член усложняет теоретический анализ (Steinwart and Christmann, 2008, глава 1) и ухудшает вычислительную эффективность (Fan et al., 2008). ",
          "debug": {
            "start_page": 474,
            "end_page": 476
          }
        },
        {
          "name": "12.2.5. SVM с мягким зазором: подход с использованием функции потерь",
          "content": "--- Страница 476 --- (продолжение)\n1 2.2.5. SVM с мягким зазором: подход с использованием функции потерь Рассмотрим другой подход к постановке задачи SVM, следующий принципу минимизации эмпирического риска (раздел 8.2). Мы должны выбрать гипер - плоскость из класса гипотез (12.27) В этом разделе мы увидим, что отступ соответствует регуляризатору. Остается выяснить, как выглядит функция потерь . В отличие от главы 9, где мы рассма - тривали задачу регрессии (в которой ответом является вещественное число), теперь мы рассматриваем задачу бинарной классификации (где предсказыва - ется принадлежность объекта одному из классов {+1, −1}). Таким образом, функция ошибок/потерь для каждой пары «объект — метка» должна подходить 1 Существуют альтернативные параметризации этой регуляриации, поэтому зада - чу (12.26a) часто называют C-SVM.\n1 2.2.5. SVM с мягким зазором: подход с использованием функции потерь Рассмотрим другой подход к постановке задачи SVM, следующий принципу минимизации эмпирического риска (раздел 8.2). Мы должны выбрать гипер - плоскость из класса гипотез (12.27) В этом разделе мы увидим, что отступ соответствует регуляризатору. Остается выяснить, как выглядит функция потерь . В отличие от главы 9, где мы рассма - тривали задачу регрессии (в которой ответом является вещественное число), теперь мы рассматриваем задачу бинарной классификации (где предсказыва - ется принадлежность объекта одному из классов {+1, −1}). Таким образом, функция ошибок/потерь для каждой пары «объект — метка» должна подходить 1 Существуют альтернативные параметризации этой регуляриации, поэтому зада - чу (12.26a) часто называют C-SVM.\n--- Страница 477 ---\n1 2.2. Прямая задача метода опорных векторов 477 для бинарной классификации. Например, квадратичная функция потерь (9.10 b), используемая в задачах регрессии, для бинарной классификации не подходит. ПРИМЕЧАНИЕ Идеальная функция потерь для бинарных меток подсчиты - вает количество несовпадений между предсказанием классификатора и насто - ящей меткой. Это значит, что для классификатора f и объекта xn мы сравниваем ответ классификатора f(xn) с меткой yn. Функция потерь равна нулю, если они совпадают, и единице, если они не совпадают. Такую функцию обозначают как 1(f(xn) ≠ yn) и называют нулевой потерей (zero­one loss). К сожалению, при ну - левой потере нахождение оптимальных параметров w, b превращается в задачу комбинаторной оптимизации. Задачи комбинаторной оптимизации в общем случае гораздо сложнее, чем задачи непрерывной оптимизации, которые мы обсуждали в главе 7.  Использование какой функции потерь приводит к методу опорных векторов? Рассмотрим разность между предсказанием классификатора f(xn) и меткой yn. Функция потерь дает нам ошибку на обучающем множестве. Вывести фор - мулу (12.26 a) можно, используя кусочно­линейную функцию потерь (hinge loss): (12.28) Если значение f(x) расположено с правильной стороны от гиперплоскости (со - впадает с меткой y) на расстоянии, большем 1, то t ≥ 1 и кусочно-линейная функция равна нулю. Если f(x) расположено с правильной стороны, но слишком близко к гиперплоскости (0 < t < 1), объект x лежит в разделяющей полосе и значение кусочно-линейной функции положительно. Если же объект оказал - ся с неправильной стороны от гиперплоскости ( t < 0), функция потерь возвра - щает еще большее значение, которое притом линейно растет. Проще говоря, мы платим штраф, когда подходим к гиперплоскости слишком близко (даже при правильной классификации), при этом штраф линейно растет. Можно рассмо - треть кусочно-линейную функцию как состоящую из двух линейных кусков (12.29) что изображено на рис. 12.8. Функция потерь, соответствующая SVM с жестким зазором (12.18), задается формулой (12.30) Можно истолковать эту формулу как запрет объектам попадать в разделяющую полосу.\n--- Страница 478 ---\n478 Глава 1 2. Классификация методом опорных векторов 0 2 /dollar.g00DC2024/m.g0073/three.g0082/.notdef.g00E3/yright/quotedbl.g006D/equal.g00C8/space.g00AB /j.g007A/three.g0082“/percent.g00BA/.notdef.g0107…/percent.g00BA-/.notdef.g00E3/comma.g00D2…/yright/L.g00AE…/equal.g00C8/space.g00ABmax{0, 1 /dollar.g00DC t} t Рис. 12.8. Кусочно-линейная функция потерь является выпуклой границей сверху для индикатора ошибки Для заданной обучающей выборки {( x1, y1), , ( xN, yN)} мы минимизируем сум - марные потери (эмпирический риск), применяя к целевой функции 2-регуля - ризацию (раздел 8.2.3). Использование кусочно-линейной функции (12.28) дает задачу безусловной (без ограничений) оптимизации (12.31) Первое слагаемое в (12.31) называют регуляризатором (regularizer) (раздел 9.2.3), а второе — слагаемым ошибок (error term). Из раздела 12.2.4 мы знаем, что сла - гаемое появляется непосредственно из значения отступа. Иными слова - ми, мак симизацию отступа можно рассматривать как регуляризацию . Вообще говоря, задачу безусловной оптимизации (12.31) можно решить на - прямую, используя методы (суб-)градиентного спуска, как описано в разделе 7.1. Чтобы понять, что (12.31) и (12.26a) эквивалентны, заметим, что функция по - терь (12.28) состоит из двух слагаемых, как понятно из (12.29). Возьмем ее значение для пары «объект — метка» (12.28). Можно заменить минимизацию кусочно-линейной функции по t на минимизацию невязки ξ при условии двух ограничений. На языке формул (12.32) эквивалентно (12.33) Подставив это выражение в (12.31), получим в точности задачу SVM с мягким зазором (12.26 a).",
          "debug": {
            "start_page": 476,
            "end_page": 478
          }
        },
        {
          "name": "12.3. Двойственная задача SVM",
          "content": "12.3. Двойственная задача SVM 479 ПРИМЕЧАНИЕ Сравним нашу нынешнюю функцию потерь с функцией потерь для линейной регрессии в главе 9. Из раздела 9.2.1 мы знаем, что для нахождения оценки максимального правдоподобия обычно минимизируется взятый с противоположным знаком логарифм правдоподобия. Далее, так как слагаемое правдоподобия для линейной регрессии с гауссовым шумом будет гауссовым, взятый с противоположным знаком логарифм правдоподобия для каждого из примеров будет совпадать с квадратичной функцией ошибок. Именно квадратичная функция ошибок и будет функцией потерь, которую мы минимизируем при поиске решения максимального правдоподобия .  1 2.3. ДВОЙСТВЕННАЯ ЗАДАЧА SVM Постановку задачи SVM в предыдущих разделах, в которой мы ищем w и b, называют прямой задачей SVM. Вспомним, что входные значения x ∈ D имеют D признаков. Так как размерность вектора w совпадает с размерностью x, число параметров (размерность w) задачи оптимизации растет линейно с увеличени - ем количества признаков. Далее мы рассмотрим эквивалентную задачу оптимизации (используя понятие двойственности), не зависящую от количества признаков. В ней число параме - тров будет увеличиваться с увеличением размера обучающей выборки. С по- хожим приемом мы встречались в главе 19, когда записывали задачу в виде, не зависящем от количества признаков. Такой подход помогает в задачах, где при - знаков больше, чем примеров в обучающей выборке. Двойственная задача SVM хороша и тем, что в ней легко ввести ядро, — это мы увидим в конце данной главы. Слово «двойственный» часто встречается в математической литературе, сейчас мы имеем в виду двойственность для задач выпуклой оптимизации. Следующие главы, по сути, посвящены применению того, что мы узнали про двойственные задачи в разделе 7.2.",
          "debug": {
            "start_page": 479,
            "end_page": 479
          }
        },
        {
          "name": "12.3.1. Двойственность и множители Лагранжа",
          "content": "--- Страница 479 --- (продолжение)\n1 2.3. Двойственная задача SVM 479 ПРИМЕЧАНИЕ Сравним нашу нынешнюю функцию потерь с функцией потерь для линейной регрессии в главе 9. Из раздела 9.2.1 мы знаем, что для нахождения оценки максимального правдоподобия обычно минимизируется взятый с противоположным знаком логарифм правдоподобия. Далее, так как слагаемое правдоподобия для линейной регрессии с гауссовым шумом будет гауссовым, взятый с противоположным знаком логарифм правдоподобия для каждого из примеров будет совпадать с квадратичной функцией ошибок. Именно квадратичная функция ошибок и будет функцией потерь, которую мы минимизируем при поиске решения максимального правдоподобия .  1 2.3. ДВОЙСТВЕННАЯ ЗАДАЧА SVM Постановку задачи SVM в предыдущих разделах, в которой мы ищем w и b, называют прямой задачей SVM. Вспомним, что входные значения x ∈ D имеют D признаков. Так как размерность вектора w совпадает с размерностью x, число параметров (размерность w) задачи оптимизации растет линейно с увеличени - ем количества признаков. Далее мы рассмотрим эквивалентную задачу оптимизации (используя понятие двойственности), не зависящую от количества признаков. В ней число параме - тров будет увеличиваться с увеличением размера обучающей выборки. С по- хожим приемом мы встречались в главе 19, когда записывали задачу в виде, не зависящем от количества признаков. Такой подход помогает в задачах, где при - знаков больше, чем примеров в обучающей выборке. Двойственная задача SVM хороша и тем, что в ней легко ввести ядро, — это мы увидим в конце данной главы. Слово «двойственный» часто встречается в математической литературе, сейчас мы имеем в виду двойственность для задач выпуклой оптимизации. Следующие главы, по сути, посвящены применению того, что мы узнали про двойственные задачи в разделе 7.2. 1 2.3.1. Двойственность и множители Лагранжа Вернемся к прямой задаче SVM с мягким зазором (12.26 a). Назовем присут - ствующие в ней переменные w, b и ξ прямыми переменными. Введем множитель Лагранжа αn ≥ 0, соответствующий ограничению (12.26 b) на правильность классификации, и множитель γn ≥ 0, который соответствует условию неотрица - тельности невязок (12.26 c)1. Лагранжиан выглядит как 1 В главе 7 мы именовали множители Лагранжа как λ. В этом разделе мы следуем обо - значениям, принятым в литературе по SVM, и используем α и γ.\n1 2.3.1. Двойственность и множители Лагранжа Вернемся к прямой задаче SVM с мягким зазором (12.26 a). Назовем присут - ствующие в ней переменные w, b и ξ прямыми переменными. Введем множитель Лагранжа αn ≥ 0, соответствующий ограничению (12.26 b) на правильность классификации, и множитель γn ≥ 0, который соответствует условию неотрица - тельности невязок (12.26 c)1. Лагранжиан выглядит как 1 В главе 7 мы именовали множители Лагранжа как λ. В этом разделе мы следуем обо - значениям, принятым в литературе по SVM, и используем α и γ.\n--- Страница 480 ---\n480 Глава 1 2. Классификация методом опорных векторов (12.34) Дифференцируя лагранжиан (12.34) по трем прямым переменным w, b и ξ со- ответственно, получим (12.35) (12.36) (12.37) Теперь найдем максимум лагранжиана, приравняв каждую из этих производных к нулю. Для (12.35) получим (12.38) частный случай теоремы о представителе (Kimeldorf and Wahba, 1970). Урав - нение (12.38) говорит нам, что оптимальным вектором весов будет линейная комбинация объектов xn. Из раздела 2.6.1 мы знаем, что это можно переформу - лировать так: решение задачи оптимизации принадлежит линейной оболочке обучающей выборки. Ограничение, полученное приравниванием к нулю вы - ражения (12.36), говорит, что оптимальный вектор весов — аффинная комби - нация объектов обучающей выборки. Теорема о представителе выполняется для очень общей задачи минимизации регуляризованного эмпирического риска (Hofmann et al., 2008; Argyriou and Dinuzzo, 2014). Теорема существует и в более общих формулировках (Sch ölkopf et al., 2001), необходимые и достаточные условия ее выполнения можно найти у Ю и др. (Yu et al, 2013)1. ПРИМЕЧАНИЕ Теорема о представителе (12.38) объясняет название «метод опорных векторов». Для объектов xn соответствующие параметры αn = 0 не вносят никакого вклада в решение w. Другие объекты, для которых αn > 0, на - зывают опорными векторами — на них «опирается» гиперплоскость.  1 Теорема о представителе — это фактически набор теорем, говорящих, что решение задачи минимизации эмпирического риска лежит в подпространстве (раздел 2.4.3), заданном обучающей выборкой.\n--- Страница 481 ---\n1 2.3. Двойственная задача SVM 481 Подставляя выражение для w в лагранжиан (12.34), получим двойственный лагранжиан (12.39) Заметим, что больше ни одно слагаемое не содержит прямой переменной w. Приравняв (12.36) нулю, получим . Таким образом, пропадает и слагаемое, содержащее b. Вспомним о симметричности и билинейности ска - лярного произведения (раздел 3.2). Поэтому первые два слагаемых в (12.39) можно объединить и получить (12.40) Последнее слагаемое в этом выражении объединяет все слагаемые, содержавшие невязки ξi. Приравняв (12.37) к нулю, видим, что последнее слагаемое в (12.40) также равно нулю. Далее, из того же уравнения и неотрицательности множите - лей Лагранжа γi выводим, что αi ≤ C. Мы получили двойственную задачу опти - мизации для SVM, выраженную только через множители Лагранжа αi. Вспомним определение двойственности Лагранжа (определение 7.1) и то, что двойственная задача — задача максимизации. Можно взять целевую функцию с противопо - ложным знаком и минимизировать ее, придя к двойственной задаче SVM : (12.41) Ограничение-равенство в (12.41) получено из приравнивания (12.36) к нулю. Ограничение αi ≥ 0 — условие, накладываемое на множители Лагранжа в не- равенствах (раздел 7.2). Ограничение αi ≤ C обсуждалось выше. Ограничения-неравенства говорят нам, что вектор множителей Лагранжа α = [α1, , αN]T ∈ N лежит внутри квадрата со сторонами, параллельными осям координат (такие ограничения иногда называют квадратными, box constraints). Такие ограничения очень удобны для численных методов (Dost ál, 2009, глава 5). Найдя значения двойственных параметров α, мы можем восстановить значения прямых параметров w с помощью теоремы о представителе (12.38). Обозначим оптимальные прямые параметры w*. Однако остается вопрос, как найти пара -\n--- Страница 482 ---\n482 Глава 1 2. Классификация методом опорных векторов метр b*. Рассмотрим объект xn, лежащий точно на границе разделяющей полосы1, то есть . Мы знаем, что yn равно +1 либо −1. Таким образом, не - известным остается только b, которое можно вычислить как (12.42) ПРИМЕЧАНИЕ Вообще говоря, объектов, лежащих точно на границе разде - ляющей полосы, может и не существовать. В этом случае мы должны найти для всех опорных векторов и взять в качестве b* медианное значе - ние этого модуля разности. Доказательство можно найти на http://fouryears. eu/2012/06/07/the-svm-bias-term-conspiracy/ . ",
          "debug": {
            "start_page": 479,
            "end_page": 482
          }
        },
        {
          "name": "12.3.2. Двойственность и выпуклая оболочка",
          "content": "--- Страница 482 --- (продолжение)\n1 2.3.2. Двойственность и выпуклая оболочка Альтернативный способ получить двойственную задачу SVM — геометрический. Рассмотрим множество примеров xn с одинаковыми метками. Мы хотим по - строить минимальное выпуклое множество, содержащее все эти примеры. Оно называется выпуклой оболочкой и изображено на рис. 12.9. Познакомимся поближе с выпуклыми комбинациями точек. Рассмотрим две точки x1 и x2 и соответствующие неотрицательные веса α1, α2 ≥ 0, такие что α1 + + α2 = 1. Выражение α1x1 + α2x2 задает любую точку на отрезке, соединяющем x1 и x2. Посмотрим, что произойдет при добавлении третьей точки x3 с весом α3 ≥ 0, таким что . Выпуклые комбинации трех точек x1, x2, x3 задают двумерную фигуру. Ее выпуклой оболочкой будет треугольник, стороны которо - го соединяют каждую из пар точек. Если мы добавим больше точек, и их коли - чество превысит размерность пространства, некоторые из точек окажутся внутри выпуклой оболочки, как мы видим на рис. 12.9( a). В общем случае для получения выпуклой оболочки надо ввести неотрицатель - ный вес αn ≥ 0 для каждого из объектов xn. Тогда выпуклую оболочку можно описать как множество (12.43) для всех n = 1, , N. Если два облака точек, соответствующие положительному и отрицательному классам, отделены друг от друга, выпуклые оболочки не пере - секутся. Имея обучающую выборку {( x1, y1), , ( xN, yN)}, мы построим две ли - нейные оболочки, соответствующие положительному и отрицательному классам. 1 Как выясняется, объекты, лежащие точно на границе разделяющей полосы, — это те же самые объекты, для которых двойственные параметры удовлетворяют строгим квадратным неравенствам 0 < αi < C. Доказывается это через условия Каруша — Куна- Такера, см., например, Sch ölkopf and Smola (2002).\n1 2.3.2. Двойственность и выпуклая оболочка Альтернативный способ получить двойственную задачу SVM — геометрический. Рассмотрим множество примеров xn с одинаковыми метками. Мы хотим по - строить минимальное выпуклое множество, содержащее все эти примеры. Оно называется выпуклой оболочкой и изображено на рис. 12.9. Познакомимся поближе с выпуклыми комбинациями точек. Рассмотрим две точки x1 и x2 и соответствующие неотрицательные веса α1, α2 ≥ 0, такие что α1 + + α2 = 1. Выражение α1x1 + α2x2 задает любую точку на отрезке, соединяющем x1 и x2. Посмотрим, что произойдет при добавлении третьей точки x3 с весом α3 ≥ 0, таким что . Выпуклые комбинации трех точек x1, x2, x3 задают двумерную фигуру. Ее выпуклой оболочкой будет треугольник, стороны которо - го соединяют каждую из пар точек. Если мы добавим больше точек, и их коли - чество превысит размерность пространства, некоторые из точек окажутся внутри выпуклой оболочки, как мы видим на рис. 12.9( a). В общем случае для получения выпуклой оболочки надо ввести неотрицатель - ный вес αn ≥ 0 для каждого из объектов xn. Тогда выпуклую оболочку можно описать как множество (12.43) для всех n = 1, , N. Если два облака точек, соответствующие положительному и отрицательному классам, отделены друг от друга, выпуклые оболочки не пере - секутся. Имея обучающую выборку {( x1, y1), , ( xN, yN)}, мы построим две ли - нейные оболочки, соответствующие положительному и отрицательному классам. 1 Как выясняется, объекты, лежащие точно на границе разделяющей полосы, — это те же самые объекты, для которых двойственные параметры удовлетворяют строгим квадратным неравенствам 0 < αi < C. Доказывается это через условия Каруша — Куна- Такера, см., например, Sch ölkopf and Smola (2002).\n--- Страница 483 ---\n1 2.3. Двойственная задача SVM 483 (a) /b.g007B/slash.g00A9/C.g00B9/three.g0082/asterisk.g007D/.notdef.g00E3/equal.g00C8/space.g00AB /percent.g00BA/K.g00AD/percent.g00BA/.notdef.g00E3/percent.g00BA/.notdef.g0107/asterisk.g007D/equal.g00C8c d (b) /b.g007B/slash.g00A9/C.g00B9/three.g0082/asterisk.g007D/.notdef.g00E3/slash.g00A9/yright /percent.g00BA/K.g00AD/percent.g00BA/.notdef.g00E3/percent.g00BA/.notdef.g0107/asterisk.g007D/comma.g00D2 /C.g00B9/percent.g00BA/.notdef.g00E3/percent.g00BA›/comma.g00D2/two.g0088/yright/.notdef.g00E3/.notdef.g0109…/slash.g00A9/period.g00B2 (/asterisk.g007D/exclam.g00AF/three.g0082›/asterisk.g007D/comma.g00D2) /comma.g00D2 /percent.g00BA/two.g0088/exclam.g00AF/comma.g00D2/.notdef.g0106/equal.g00C8/two.g0088/yright/.notdef.g00E3/.notdef.g0109…/slash.g00A9/period.g00B2 (/asterisk.g007D/exclam.g00AF/yright“/two.g0088/comma.g00D2/asterisk.g007D/comma.g00D2) /C.g00B9/exclam.g00AF/comma.g00D2/.notdef.g00E4/yright/exclam.g00AF/percent.g00BA/quotedbl.g006D. /p.g0063/equal.g00C8““/two.g0088/percent.g00BA/space.g00AB…/comma.g00D2/yright /.notdef.g00E4/yright›/.notdef.g0105/three.g0082 /.notdef.g0105/quotedbl.g006D/three.g0082/.notdef.g00E4/space.g00AB /quotedbl.g006D/slash.g00A9/C.g00B9/three.g0082/asterisk.g007D/.notdef.g00E3/slash.g00A9/.notdef.g00E4/comma.g00D2 /percent.g00BA/K.g00AD/percent.g00BA/.notdef.g00E3/percent.g00BA/.notdef.g0107/asterisk.g007D/equal.g00C8/.notdef.g00E4/comma.g00D2 /numbersign.g00DB /.notdef.g0105/.notdef.g00E3/comma.g00D2…/equal.g00C8 /exclam.g00AF/equal.g00C8ƒ…/percent.g00BA“/two.g0088/comma.g00D2 /quotedbl.g006D/yright/asterisk.g007D/two.g0088/percent.g00BA/exclam.g00AF/percent.g00BA/quotedbl.g006D c − d Рис. 12.9. Выпуклые оболочки. ( a) Выпуклая оболочка точек, некоторые из которых лежат внутри ее границ. ( b) Выпуклые оболочки положительных и отрицательных примеров Возьмем точку c в выпуклой оболочке множества положительных примеров, ближайшую к отрицательному классу. Точно так же возьмем точку d в выпуклой оболочке множества отрицательных примеров, ближайшую к положительному классу, см. рис. 12.9( b). Обозначим разность d и c как w := c − d. (12.44) Требование, чтобы c и d были как можно ближе друг к другу, эквивалент - но минимизации длины (нормы) w, так что мы приходим к задаче оптими- зации (12.45) Так как c должна лежать в выпуклой оболочке положительных примеров, ее можно выразить как выпуклую комбинацию положительных примеров, то есть для некоторых неотрицательных коэффициентов (12.46) В формуле (12.46) мы использовали обозначение n : yn = +1 для множества индексов n, таких что yn = +1. Аналогично, для отрицательных примеров имеем (12.47)\n--- Страница 484 ---\n484 Глава 1 2. Классификация методом опорных векторов Подставив (12.44), (12.46) и (12.47) в (12.45), получим целевую функцию (12.48) Обозначим через α множество всех коэффициентов, то есть объединение α+ и α–. Вспомним о требовании к элементам выпуклой оболочки — сумма коэффици - ентов равна единице: (12.49) Приходим к ограничению (12.50) Этот результат мы получаем, перемножив условия для каждого из классов: (12.51 a) (12.51 b) Целевая функция (12.48) и ограничения (12.50), вместе с предположением, что α ≥ 0, задают задачу выпуклой условной оптимизации. Можно показать, что это двойственная задача к SVM с жестким зазором (Bennett and Bredensteiner, 2000a). ПРИМЕЧАНИЕ Чтобы получить задачу, двойственную к SVM с мягким за - зором, мы рассматриваем выпуклую оболочку с ограничением сверху на коэф - фициенты α. Ограничение на α сжимает эту оболочку (Bennett and Bre densteiner, 2000b). ",
          "debug": {
            "start_page": 482,
            "end_page": 484
          }
        },
        {
          "name": "12.4. Ядра",
          "content": "--- Страница 484 --- (продолжение)\n1 2.4. ЯДРА Рассмотрим формулировку задачи, двойственной к SVM (12.41). Заметим, что в целевой функции присутствуют только скалярные произведения объектов xi и xj, но не скалярные произведения объектов и параметров. Следовательно, если объект xi представлен набором признаков ϕ(xi), единственным изменением в двойственной задаче SVM будет замена скалярного произведения. Такая «мо - дульность» задачи, когда выбор метода классификации (SVM) и представления признаков ϕ(x) можно рассматривать отдельно, обеспечивает гибкость при решении. В данном разделе мы обсудим представление ϕ(x) и вкратце опишем идею ядра, однако не станем вдаваться в технические детали.\n1 2.4. ЯДРА Рассмотрим формулировку задачи, двойственной к SVM (12.41). Заметим, что в целевой функции присутствуют только скалярные произведения объектов xi и xj, но не скалярные произведения объектов и параметров. Следовательно, если объект xi представлен набором признаков ϕ(xi), единственным изменением в двойственной задаче SVM будет замена скалярного произведения. Такая «мо - дульность» задачи, когда выбор метода классификации (SVM) и представления признаков ϕ(x) можно рассматривать отдельно, обеспечивает гибкость при решении. В данном разделе мы обсудим представление ϕ(x) и вкратце опишем идею ядра, однако не станем вдаваться в технические детали.\n--- Страница 485 ---\n1 2.4. Ядра 485 Так как ϕ(x) может быть нелинейной функцией, можно использовать SVM (который считается линейным классификатором) для построения классифи - каторов, не линейных по примерам xn. Это дает еще один путь работы с на- бором данных, который не является линейно разделимым (первый — исполь - зование мягкого отступа). На самом деле существует множество алгоритмов и статистических методов, обладающих тем же свойством, что и двойственная задача SVM: скалярные произведения вычисляются только между объектами обучающей выборки. Вместо того чтобы явно задать нелинейную карту при - знаков ϕ(·) и вычислить получившееся скалярное произведение xi и xj, мы определим функцию близости между xi и xj, обозначаемую k(xi, xj). Некоторый класс функций близости — ядерные функции (ядра) — неявно задает нели - нейную карту признаков ϕ(·). Ядерные функции — это функции k :  ×  → , для которых существует гильбертово пространство  и карта признаков ϕ :  → , такие что1 (12.52) С каждым ядром k связано единственное (воспроизводящее ядро) гильбертово пространство (Aronszajn, 1950; Berlinet and Thomas-Agnan, 2004). При этом ϕ(x) = k(·, x) называют канонической картой признаков . Обобщение скалярного произведения до ядерной функции (12.52) известно как ядерный трюк (Sch ölkopf and Smola, 2002; Shawe-Taylor and Cristianini, 2004), прячущий нелинейное преобразование признаков. Матрица K ∈ N × N, получающаяся применением скалярного произведения или ядерной функции k(·, ·) к обучающей выборке, называется матрицей Грама , или ядерной матрицей . Ядра должны быть симметричными положительно полуопре - деленными функциями, так что любая ядерная матрица K симметрична и по- ложительно полуопределена (раздел 3.2.3): (12.53) К широко применяемым ядерным функциям для многомерных вещественных данных xi ∈ D относятся полиномиальное ядро, гауссова радиально-базисная функция (RBF) и рациональное квадратичное ядро (Sch ölkopf and Smola, 2002; Rasmussen and Williams, 2006). На рис. 12.10 показан эффект применения раз - личных ядер к разделяющим гиперплоскостям на примере одного и того же набора данных. Заметим, что мы все еще ищем гиперплоскость, то есть классом гипотез по-прежнему являются линейные функции. Нелинейные поверхности появляются под действием ядерной функции. 1 Область определения ядерной функции не обязана представлять собой D.\n--- Страница 486 ---\n486 Глава 1 2. Классификация методом опорных векторов Первый признакВторой признак (a) SVM с линейным ядромПервый признакВторой признак (b) SVM с ядром RBF Первый признакВторой признак (c) SVM с полиномиальным ядром степени 2Первый признакВторой признак (d) SVM с полиномиальным ядром степени 3 Рис. 12.10. SVM с различными ядрами. Заметим, что хотя в решении граница нелинейна, внутри решается задача поиска разделяющей гиперплоскости (хотя и с использованием нелинейного ядра) ПРИМЕЧАНИЕ К сожалению для начинающего изучать МО, слово «ядро» имеет много значений. В этой главе оно употребляется в смысле воспроизводя - щих ядер гильбертовых пространств (reproducing kernel Hilbert space, RKHS) (Aronszajn, 1950; Saitoh, 1988). Мы уже обсуждали понятие ядра в линейной алгебре (раздел 2.7.3), где оно является синонимом пространства решений одно - родной линейной системы. Третья область МО, где употребляется понятие ядра (сглаживающее ядро), — ядерные оценки плотности (раздел 11.5).  Так как явное представление ϕ(x) математически эквивалентно ядерному пред - ставлению k(xi, xj), на практике часто берут такую ядерную функцию, которую удобнее вычислять, чем скалярное произведение преобразованных признаков.",
          "debug": {
            "start_page": 484,
            "end_page": 486
          }
        },
        {
          "name": "12.5. Численное решение",
          "content": "--- Страница 487 --- (продолжение)\n1 2.5. Численное решение 487 Рассмотрим, например, полиномиальное ядро (Sch ölkopf and Smola, 2002), где количество слагаемых в явном разложении растет чрезвычайно быстро (даже для многочленов небольшой степени), в то время как размерность входов вели - ка. Ядерная функция требует числа умножений, равного размерности, что значительно упрощает вычисления. Другим примером может служить гауссова радиально-базисная функция (Sch ölkopf and Smola, 2002; Rasmussen and Williams, 2006), соответствующее которой пространство признаков бесконечномерно. В этом случае мы не можем напрямую работать с пространством признаков, но все еще в состоянии вычислять близость двух объектов с помощью ядра. Другим полезным следствием ядерного трюка является отсутствие необходи - мости представлять исходные данные как многомерные векторы из веществен - ных чисел1. Вспомним, что скалярное произведение определено на выходных значениях функции ϕ(·), которые не обязаны быть вещественными числами. Таким образом, функцию ϕ(·) и ядро k(·, ·) можно определить на любых входных объектах — множествах, последовательностях, строках, графах, распределениях (Ben-Hur et al., 2008; G ärtner, 2008; Shi et al., 2009; Sriperumbudur et al., 2010; Vishwanathan et al., 2010). 1 2.5. ЧИСЛЕННОЕ РЕШЕНИЕ Мы завершим разговор об SVM обсуждением того, как применять к задачам из данной главы концепции из главы 7. Рассмотрим два подхода к поиску опти - мального решения задачи SVM. Сначала рассмотрим постановку задачи 8.2.2, использующую функцию потерь как задачу оптимизации без ограничений. Затем запишем ограниченные версии прямой и двойственной задач SVM в стан- дартной форме для задач квадратичного программирования 7.3.2. Рассмотрим формулировку SVM (12.31), использующую функцию потерь. Это — задача безусловной оптимизации, однако кусочно-линейная функ - ция (12.28) не дифференцируема. Поэтому надо применять субградиентный метод. Однако заметим, что не дифференцируема кусочно-линейная функция только в одной точке: t = 1. В ней градиентом можно считать множество значений между 0 и −1. Таким образом, субградиент g кусочно-линейной функции потерь задается формулой (12.54) 1 Ядро и его параметры часто выбирают с помощью вложенной кросс-валидации (раз - дел 8.6.1).\n1 2.5. Численное решение 487 Рассмотрим, например, полиномиальное ядро (Sch ölkopf and Smola, 2002), где количество слагаемых в явном разложении растет чрезвычайно быстро (даже для многочленов небольшой степени), в то время как размерность входов вели - ка. Ядерная функция требует числа умножений, равного размерности, что значительно упрощает вычисления. Другим примером может служить гауссова радиально-базисная функция (Sch ölkopf and Smola, 2002; Rasmussen and Williams, 2006), соответствующее которой пространство признаков бесконечномерно. В этом случае мы не можем напрямую работать с пространством признаков, но все еще в состоянии вычислять близость двух объектов с помощью ядра. Другим полезным следствием ядерного трюка является отсутствие необходи - мости представлять исходные данные как многомерные векторы из веществен - ных чисел1. Вспомним, что скалярное произведение определено на выходных значениях функции ϕ(·), которые не обязаны быть вещественными числами. Таким образом, функцию ϕ(·) и ядро k(·, ·) можно определить на любых входных объектах — множествах, последовательностях, строках, графах, распределениях (Ben-Hur et al., 2008; G ärtner, 2008; Shi et al., 2009; Sriperumbudur et al., 2010; Vishwanathan et al., 2010). 1 2.5. ЧИСЛЕННОЕ РЕШЕНИЕ Мы завершим разговор об SVM обсуждением того, как применять к задачам из данной главы концепции из главы 7. Рассмотрим два подхода к поиску опти - мального решения задачи SVM. Сначала рассмотрим постановку задачи 8.2.2, использующую функцию потерь как задачу оптимизации без ограничений. Затем запишем ограниченные версии прямой и двойственной задач SVM в стан- дартной форме для задач квадратичного программирования 7.3.2. Рассмотрим формулировку SVM (12.31), использующую функцию потерь. Это — задача безусловной оптимизации, однако кусочно-линейная функ - ция (12.28) не дифференцируема. Поэтому надо применять субградиентный метод. Однако заметим, что не дифференцируема кусочно-линейная функция только в одной точке: t = 1. В ней градиентом можно считать множество значений между 0 и −1. Таким образом, субградиент g кусочно-линейной функции потерь задается формулой (12.54) 1 Ядро и его параметры часто выбирают с помощью вложенной кросс-валидации (раз - дел 8.6.1).\n--- Страница 488 ---\n488 Глава 1 2. Классификация методом опорных векторов Вычислив субградиент, мы можем применить методы оптимизации из раздела 7.1. Как прямая, так и двойственная задачи SVM сводятся к выпуклой условной задаче квадратичного программирования. Заметим, что в прямой задаче (12.26 a) переменные оптимизации имеют ту же размерность D, что и входные данные. В двойственной же задаче (12.41) они имеют размерность, равную размеру обу- чающей выборки N. Чтобы записать прямую задачу SVM в стандартном виде задачи квадратичного программирования (7.45), будем использовать скалярное произведение (3.5). Перенесем в (12.26 a) все переменные оптимизации в правую часть и приведем ограничение-неравенство к стандартному виду. Получаем задачу (12.55) n = 1, , N. Объединяя переменные w, b, xn в один вектор и группируя слагаемые, получаем матричный вид задачи SVM с мягким зазором: (12.56) В этой задаче минимизация происходит по переменным [ wT, b, ξT]T ∈ D + 1 + N. Мы используем обозначения Im для единичной матрицы размера m × m, 0m, n для матрицы из нулей размера m × n и 1m, n для матрицы из единиц размера m × n. Кроме того, обозначим через y вектор меток [ y1, , yN]T, Y = diag( y) будет матри - цей размера N × N, у которой на диагонали расположены элементы y, а X ∈ N×D — матрица, содержащая все примеры из обучающей выборки. Можно аналогичным образом сгруппировать слагаемые и в двойственной за - даче SVM (12.41). Чтобы записать ее в стандартном виде, сначала введем ма - трицу K с элементами Kij = k(xi, xj). Если xi записан в виде вектора признаков, то мы определяем . Для удобства обозначений введем также матрицу Y = diag( y), у которой на диагонали стоят метки классов, а вне диагонали — нули. Двойственная задача SVM запишется как",
          "debug": {
            "start_page": 487,
            "end_page": 488
          }
        },
        {
          "name": "12.6. Для дальнейшего чтения",
          "content": "--- Страница 489 --- (продолжение)\n1 2.6. Для дальнейшего чтения 489 (12.57) ПРИМЕЧАНИЕ В разделах 7.3.1 и 7.3.2 мы договорились, что стандартной формой ограничений являются ограничения-неравенства. Выразим ограниче - ние-равенство в двойственной задаче SVM как два неравенства: Ax = b заменяется на Ax ≤ b и Ax ≥ b. (12.58) Некоторые программные реализации выпуклой оптимизации допускают и на- личие ограничений-равенств.  Так как существует множество способов рассматривать задачу SVM, существу - ет и множество подходов к решению полученной задачи оптимизации. Подход, изложенный здесь, — представление задачи SVM в стандартном виде задач выпуклой оптимизации — нечасто используется на практике. Две самые попу - лярные реализации поиска решений SVM принадлежат Чангу и Линю (Chang and Lin, 2011) (программа с открытым исходным кодом) и Йоахимсу (Joachims, 1999). Так как SVM сводится к четко поставленной задаче оптимизации, можно применять различные методы численной оптимизации (Nocedal and Wright, 2006, Shawe-Taylor and Sun, 2011). 1 2.6. ДЛЯ ДАЛЬНЕЙШЕГО ЧТЕНИЯ Метод опорных векторов — один из множества методов бинарной классифика - ции. К другим методам относятся перцептроны, логистическая регрессия, дис - криминант Фишера, метод ближайшего соседа, наивный Байес и случайный лес (Bishop, 2006; Murphy, 2012). Краткий справочник по использованию опорных векторов и ядер для дискретных последовательностей можно найти у Бен-Гура и др. (Ben-Hur et al., 2008). Развитие метода опорных векторов связано с задачей минимизации эмпирического риска, которую мы обсуждали в разделе 8.2. По - этому SVM обладает свойствами, доказанными теоретически строго (Vapnik, 2000; Steinwart and Christmann, 2008). В книге по ядерным методам (Sch ölkopf and Smola, 2002) содержится много материала о методе опорных векторов и оп- тимизации. Более развернутое изложение ядерных методов (Shawe-Taylor and Cristianini, 2004) также содержит немало линейно-алгебраических подходов к задачам машинного обучения.\n1 2.6. Для дальнейшего чтения 489 (12.57) ПРИМЕЧАНИЕ В разделах 7.3.1 и 7.3.2 мы договорились, что стандартной формой ограничений являются ограничения-неравенства. Выразим ограниче - ние-равенство в двойственной задаче SVM как два неравенства: Ax = b заменяется на Ax ≤ b и Ax ≥ b. (12.58) Некоторые программные реализации выпуклой оптимизации допускают и на- личие ограничений-равенств.  Так как существует множество способов рассматривать задачу SVM, существу - ет и множество подходов к решению полученной задачи оптимизации. Подход, изложенный здесь, — представление задачи SVM в стандартном виде задач выпуклой оптимизации — нечасто используется на практике. Две самые попу - лярные реализации поиска решений SVM принадлежат Чангу и Линю (Chang and Lin, 2011) (программа с открытым исходным кодом) и Йоахимсу (Joachims, 1999). Так как SVM сводится к четко поставленной задаче оптимизации, можно применять различные методы численной оптимизации (Nocedal and Wright, 2006, Shawe-Taylor and Sun, 2011). 1 2.6. ДЛЯ ДАЛЬНЕЙШЕГО ЧТЕНИЯ Метод опорных векторов — один из множества методов бинарной классифика - ции. К другим методам относятся перцептроны, логистическая регрессия, дис - криминант Фишера, метод ближайшего соседа, наивный Байес и случайный лес (Bishop, 2006; Murphy, 2012). Краткий справочник по использованию опорных векторов и ядер для дискретных последовательностей можно найти у Бен-Гура и др. (Ben-Hur et al., 2008). Развитие метода опорных векторов связано с задачей минимизации эмпирического риска, которую мы обсуждали в разделе 8.2. По - этому SVM обладает свойствами, доказанными теоретически строго (Vapnik, 2000; Steinwart and Christmann, 2008). В книге по ядерным методам (Sch ölkopf and Smola, 2002) содержится много материала о методе опорных векторов и оп- тимизации. Более развернутое изложение ядерных методов (Shawe-Taylor and Cristianini, 2004) также содержит немало линейно-алгебраических подходов к задачам машинного обучения.\n--- Страница 490 ---\n490 Глава 1 2. Классификация методом опорных векторов Альтернативный способ вывести двойственную задачу SVM основан на идее преобразования Лежандра — Фенхеля (раздел 7.3.3). При этом каждое слагаемое в безусловной формулировке задачи SVM (12.31) рассматривается отдельно и вычисляется его выпуклое сопряжение (Rifkin and Lippert, 2007). Читателей, которым интересен подход к SVM с точки зрения функционального анализа (а также методов регуляризации), отсылаем к книге Вахбы (Wahba, 1990). Изу - чение теории ядерных функций (Aronszajn, 1950; Schwartz, 1964; Saitoh, 1988; Manton and Amblard, 2015) требует знаний о линейных операторах (Akhiezer and Glazman, 1993). Понятие ядер обобщается на банаховы пространства (Zhang et al., 2009) и крейновы пространства (Ong et al., 2004; Loosli et al., 2016). Заметим, что у кусочно-линейной функции существует три эквивалентных представления, как видно из формул (12.28) и (12.29) и условной задачи опти - мизации (12.33). Вид (12.28) часто используется для сравнения функции потерь в SVM с другими функциями потерь (Steinwart, 2007). Кусочно-линейный вид (12.29) удобен для вычисления субградиентов. Вид (12.33), как можно увидеть из раздела 12.5, позволяет использовать методы выпуклого квадратичного про - граммирования (раздел 7.3.2). Бинарная классификация — очень распространенная и хорошо изученная за - дача машинного обучения, также известная под названиями задачи разделения и задачи принятия решений. Ответы бинарного классификатора могут быть трех типов. Первый тип — значение линейной функции, которое может быть любым вещественным числом. Эти числа можно использовать для ранжирова - ния объектов, а классификация будет состоять в выборе порогового значения на объектах обучающей выборки (Shawe-Taylor and Cristianini, 2004). Во втором случае ответ бинарного классификатора пропущен через некоторую нелинейную функцию, которая загоняет ответ в заданный интервал, например [0, 1]. Типич - ным примером такой нелинейной функции является сигмоида (Bishop, 2006). Порой применение нелинейной функции дает нам вероятность (Gneiting and Raftery, 2007; Reid and Williamson, 2011), тогда мы говорим о задаче оценки вероятности класса. Третий тип ответа бинарного классификатора (чаще всего, говоря о задаче классификации, имеют в виду именно его) — просто итоговая метка класса {+1, −1}. SVM относится к бинарным классификаторам, не имеющим естественной ве - роятностной интерпретации. Однако существует несколько методов, перево - дящих значения линейной функции в оценки вероятности классов ( P(Y = 1 | X = x)), включающих дополнительную калибровку (Platt, 2000; Zadrozny and Elkan, 2001; Lin et al., 2007). С точки зрения обучения, существуют родственные вероятностные методы. В конце раздела 12.2.5 мы упоминали о связи между функцией потерь и правдоподобием (сравните также с разделами 8.2 и 8.3). Подход максимального правдоподобия с удачным преобразованием во время\n--- Страница 491 ---\n1 2.6. Для дальнейшего чтения 491 обучения называется логистической регрессией и принадлежит к классу обоб - щенных линейных моделей. Подробнее о логистической регрессии с этой точки зрения можно прочитать у Агрести (Agresti, 2002, глава 5) и у Маккаллаха и Нелдера (McCullagh and Nelder, 1989, глава 4). Можно принять байесовскую точку зрения на классификацию и оценить апостериорное распределение от - ветов с помощью байесовской линейной регрессии. Байесовский подход также включает оценку априорного распределения, для чего требуется выбирать, на - пример, сопряженную функцию правдоподобия (раздел 6.6.1). Можно также рассматривать скрытые функции как априорные распределения и прийти к классификации на основе гауссовых процессов (Rasmussen and Williams, 2006, глава 3).\n--- Страница 492 ---\nБиблиография Abel, Niels H. 1826. D émonstration de l’Impossibilit é de la R ésolution Alg ébrique des Équations G énérales qui Passent le Quatri ème Degr é. Grøndahl & S øn. Adhikari, Ani, and DeNero, John. 2018. Computational and Inferential Thinking: The Foundations of Data Science. Gitbooks. Agarwal, Arvind, and Daum é III, Hal. 2010. A Geometric View of Conjugate Priors. Machine Learning, 81(1), 99–113. Agresti, A. 2002. Categorical Data Analysis. Wiley. Akaike, Hirotugu. 1974. A New Look at the Statistical Model Identification. IEEE Transactions on Automatic Control, 19(6), 716–723. Akhiezer, Naum I., and Glazman, Izrail M. 1993. Theory of Linear Operators in Hilbert Space. Dover Publications. Alpaydin, Ethem. 2010. Introduction to Machine Learning. MIT Press. Amari, Shun- ichi. 2016. Information Geometry and Its Applications. Springer. Amari, Shun-ichi. 2016. Information Geometry and Its Applications. Springer. Argyriou, Andreas, and Dinuzzo, Francesco. 2014. A Unifying View of Representer Theorems. In: Proceedings of the International Conference on Machine Lear ning. Aronszajn, Nachman. 1950. Theory of Reproducing Kernels. Transactions of the American Mathematical Society, 68, 337–404. Axler, Sheldon. 2015. Linear Algebra Done Right. Springer. Bakir, G ökhan, Hofmann, Thomas, Sch ölkopf, Bernhard, Smola, Alexander J., Taskar, Ben, and Vishwanathan, S. V. N. (eds). 2007. Predicting Structured Data. MIT Press. Barber, David. 2012. Bayesian Reasoning and Machine Learning. Cambridge Uni - versity Press. Barndorff-Nielsen, Ole. 2014. Information and Exponential Families: In Statistical Theory. Wiley. Bartholomew, David, Knott, Martin, and Moustaki, Irini. 2011. Latent Variable Models and Factor Analysis: A Unified Approach. Wiley.\n--- Страница 493 ---\n493 Библиография Baydin, At ılım G., Pearlmutter, Barak A., Radul, Alexey A., and Siskind, Jeffrey M. 2018. Automatic Differentiation in Machine Learning: A Survey. Journal of Machine Learning Research, 18, 1–43. Beck, Amir, and Teboulle, Marc. 2003. Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization. Operations Research Letters, 31(3), 167–175. Belabbas, Mohamed-Ali, and Wolfe, Patrick J. 2009. Spectral Methods in Machine Learning and New Strategies for Very Large Datasets. Proceedings of the Na - tional Academy of Sciences, 0810600105. Belkin, Mikhail, and Niyogi, Partha. 2003. Laplacian Eigenmaps for Dimensionality Reduction and Data Representation. Neural Computation, 15(6), 1373–1396. Ben-Hur, Asa, Ong, Cheng Soon, Sonnenburg, S ören, Sch ölkopf, Bernhard, and Rätsch, Gunnar. 2008. Support Vector Machines and Kernels for Computation - al Biology. PLoS Computational Biology, 4(10), e1000173. Bennett, Kristin P., and Bredensteiner, Erin J. 2000a. Duality and Geometry in SVM Classifiers. In: Proceedings of the International Conference on Machine Lear- ning. Bennett, Kristin P., and Bredensteiner, Erin J. 2000b. Geometry in Learning. Pa- ges 132–145 in Geometry at Work. Mathematical Association of America. Berlinet, Alain, and Thomas-Agnan, Christine. 2004. Reproducing Kernel Hilbert Spaces in Probability and Statistics. Springer. Bertsekas, Dimitri P. 1999. Nonlinear Programming. Athena Scientific. Bertsekas, Dimitri P. 2009. Convex Optimization Theory. Athena Scientific. Bickel, Peter J., and Doksum, Kjell. 2006. Mathematical Statistics, Basic Ideas and Selected Topics. Vol. 1. Prentice Hall. Bickson, Danny, Dolev, Danny, Shental, Ori, Siegel, Paul H., and Wolf, Jack K. 2007. Linear Detection via Belief Propagation. In: Proceedings of the Annual Allerton Conference on Communication, Control, and Computing. Billingsley, Patrick. 1995. Probability and Measure. Wiley. Bishop, Christopher M. 1995. Neural Networks for Pattern Recognition. Clarendon Press. Bishop, Christopher M. 1999. Bayesian PCA. In: Advances in Neural Information Processing Systems. Bishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer. Blei, David M., Kucukelbir, Alp, and McAuliffe, Jon D. 2017. Variational Inference: A Review for Statisticians. Journal of the American Statistical Association, 112(518), 859–877.\n--- Страница 494 ---\n494 Библиография Blum, Arvim, and Hardt, Moritz. 2015. The Ladder: A Reliable Leaderboard for Machine Learning Competitions. In: International Conference on Machine Learning. Bonnans, J. Fr édéric, Gilbert, J. Charles, Lemar échal, Claude, and Sagastiz ábal, Claudia A. 2006. Numerical Optimization: Theoretical and Practical Aspects. Springer. Borwein, Jonathan M., and Lewis, Adrian S. 2006. Convex Analysis and Nonlinear Optimization. 2nd edn. Canadian Mathematical Society. Bottou, L éon. 1998. Online Algorithms and Stochastic Approximations. Pages 9–42 in Online Learning and Neural Networks. Cambridge University Press. Bottou, L éon, Curtis, Frank E., and Nocedal, Jorge. 2018. Optimization Methods for Large-Scale Machine Learning. SIAM Review, 60(2), 223–311. Boucheron, Stephane, Lugosi, Gabor, and Massart, Pascal. 2013. Concentration Inequalities: A Nonasymptotic Theory of Independence. Oxford University Press. Boyd, Stephen, and Vandenberghe, Lieven. 2004. Convex Optimization. Cambridge University Press. Boyd, Stephen, and Vandenberghe, Lieven. 2018. Introduction to Applied Linear Algebra. Cambridge University Press. Brochu, Eric, Cora, Vlad M., and de Freitas, Nando. 2009. A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning. Tech. rept. TR-2009-023. Department of Computer Science, University of British Columbia. Brooks, Steve, Gelman, Andrew, Jones, Galin L., and Meng, Xiao-Li (eds). 2011. Handbook of Markov Chain Monte Carlo. Chapman and Hall/CRC. Brown, Lawrence D. 1986. Fundamentals of Statistical Exponential Families: With Applications in Statistical Decision Theory. Institute of Mathematical Statis - tics. Bryson, Arthur E. 1961. A Gradient Method for Optimizing Multi-Stage Allocation Processes. In: Proceedings of the Harvard University Symposium on Digital Computers and Their Applications. Bubeck, S ébastien. 2015. Convex Optimization: Algorithms and Complexity. Foun - dations and Trends in Machine Learning, 8(3-4), 231–357. Bühlmann, Peter, and Van De Geer, Sara. 2011. Statistics for High-Dimensional Data. Springer. Burges, Christopher. 2010. Dimension Reduction: A Guided Tour. Foundations and Trends in Machine Learning, 2(4), 275–365.\n--- Страница 495 ---\n495 Библиография Carroll, J Douglas, and Chang, Jih-Jie. 1970. Analysis of Individual Differences in Multidimensional Scaling via an N -Way Generalization of “Eckart–Y oung” Decomposition. Psychometrika, 35(3), 283–319. Casella, George, and Berger, Roger L. 2002. Statistical Inference. Duxbury. Çinlar, Erhan. 2011. Probability and Stochastics. Springer. Chang, Chih-Chung, and Lin, Chih-Jen. 2011. LIBSVM: A Library for Support Vector Machines. ACM Transactions on Intelligent Systems and Technology, 2, 27:1–27:27. Software available at www.csie.ntu.edu.tw/cjlin/libsvm . Cheeseman, Peter. 1985. In Defense of Probability. In: Proceedings of the Interna - tional Joint Conference on Artificial Intelligence. Chollet, Francois, and Allaire, J. J. 2018. Deep Learning with R. Manning Publica - tions. Codd, Edgar F. 1990. The Relational Model for Database Management. Addison-Wesley Longman Publishing. Cunningham, John P., and Ghahramani, Zoubin. 2015. Linear Dimensionality Re - duction: Survey, Insights, and Generalizations. Journal of Machine Learning Research, 16, 2859–2900. Datta, Biswa N. 2010. Numerical Linear Algebra and Applications. SIAM. Davidson, Anthony C., and Hinkley, David V. 1997. Bootstrap Methods and Their Application. Cambridge University Press. Dean, Jeffrey, Corrado, Greg S., Monga, Rajat, et al. 2012. Large Scale Distributed Deep Networks. In: Advances in Neural Information Processing Systems. Deisenroth, Marc P., and Mohamed, Shakir. 2012. Expectation Propagation in Gaus- sian Process Dynamical Systems. In: Advances in Neural Information Process - ing Systems. Deisenroth, Marc P., and Ohlsson, Henrik. 2011. A General Perspective on Gaussian Filtering and Smoothing: Explaining Current and Deriving New Algorithms. In: Proceedings of the American Control Conference. Deisenroth, Marc P., Fox, Dieter, and Rasmussen, Carl E. 2015. Gaussian Processes for Data-Efficient Learning in Robotics and Control. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(2), 408–423. Dempster, Arthur P., Laird, Nan M., and Rubin, Donald B. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, 39(1), 1–38. Deng, Li, Seltzer, Michael L., Yu, Dong, Acero, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey E. 2010. Binary Coding of Speech Spectrograms Using a Deep Auto-Encoder. Proceedings of Interspeech. Devroye, Luc. 1986. Non-Uniform Random Variate Generation. Springer.\n--- Страница 496 ---\n496 Библиография Donoho, David L., and Grimes, Carrie. 2003. Hessian Eigenmaps: Locally Linear Embedding Techniques for High-Dimensional Data. Proceedings of the Natio- nal Academy of Sciences, 100(10), 5591–5596. Dost ál, Zden ěk. 2009. Optimal Quadratic Programming Algorithms: With Applica - tions to Variational Inequalities. Springer. Douven, Igor. 2017. Abduction. In: The Stanford Encyclopedia of Philosophy. Met - aphysics Research Lab, Stanford University. https://plato.stanford.edu/cgi-bin/ encyclopedia/archinfo.cgi?entry=abduction&archive=sum2017 . Downey, Allen B. 2014. Think Stats: Exploratory Data Analysis. 2nd edn. O’Reilly Media. Dreyfus, Stuart. 1962. The Numerical Solution of Variational Problems. Journal of Mathematical Analysis and Applications, 5(1), 30–45. Drumm, Volker, and Weil, Wolfgang. 2001. Lineare Algebra und Analytische Geo- metrie. Lecture Notes, Universit ät Karlsruhe (TH). Dudley, Richard M. 2002. Real Analysis and Probability. Cambridge University Press. Eaton, Morris L. 2007. Multivariate Statistics: A Vector Space Approach. Insti - tute of Mathematical Statistics Lecture Notes. Eckart, Carl, and Y oung, Gale. 1936. The Approximation of One Matrix by Another of Lower Rank. Psychometrika, 1(3), 211–218. Efron, Bradley, and Hastie, Trevor. 2016. Computer Age Statistical Inference: Algo - rithms, Evidence and Data Science. Cambridge University Press. Efron, Bradley, and Tibshirani, Robert J. 1993. An Introduction to the Bootstrap. Chapman and Hall/CRC. Elliott, Conal. 2009. Beautiful Differentiation. In: International Conference on Functional Programming. Evgeniou, Theodoros, Pontil, Massimiliano, and Poggio, Tomaso. 2000. Statistical Learning Theory: A Primer. International Journal of Computer Vision, 38(1), 9–13. Fan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang, Xiang-Rui, and Lin, Chih-Jen. 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9, 1871–1874. Gal, Yarin, van der Wilk, Mark, and Rasmussen, Carl E. 2014. Distributed Variatio- nal Inference in Sparse Gaussian Process Regression and Latent Variable Mod - els. In: Advances in Neural Information Processing Systems. Gärtner, Thomas. 2008. Kernels for Structured Data. World Scientific. Gavish, Matan, and Donoho, David L. 2014. The Optimal Hard Threshold for Sin - gular Values is 4 √3. IEEE Transactions on Information Theory, 60(8), 5040–5053.\n--- Страница 497 ---\n497 Библиография Gelman, Andrew, Carlin, John B., Stern, Hal S., and Rubin, Donald B. 2004. Baye sian Data Analysis. Chapman & Hall/CRC. Gentle, James E. 2004. Random Number Generation and Monte Carlo Methods. Springer. Ghahramani, Zoubin. 2015. Probabilistic Machine Learning and Ar - tificial Intelligence. Nature, 521, 452–459. Ghahramani, Zoubin, and Roweis, Sam T. 1999. Learning Nonlinear Dynamical Systems Using an EM Algorithm. In: Advances in Neural Information Proces- sing Systems. Gilks, Walter R., Richardson, Sylvia, and Spiegelhalter, David J. 1996. Markov Chain Monte Carlo in Practice. Chapman and Hall/CRC. Gneiting, Tilmann, and Raftery, Adrian E. 2007. Strictly Proper Scoring Rules, Pre - diction, and Estimation. Journal of the American Statistical Association, 102(477), 359–378. Goh, Gabriel. 2017. Why Momentum Really Works. Distill. Gohberg, Israel, Goldberg, Seymour, and Krupnik, Nahum. 2012. Traces and Deter - minants of Linear Operators. Birkh äuser. Golan, Jonathan S. 2007. The Linear Algebra a Beginning Graduate Student Ought to Know. Springer. Golub, Gene H., and Van Loan, Charles F. 2012. Matrix Computations. JHU Press. Goodfellow, Ian, Bengio, Y oshua, and Courville, Aaron. 2016. Deep Learning. MIT Press. Graepel, Thore, Candela, Joaquin Qui˜ nonero-Candela, Borchert, Thomas, and Her - brich, Ralf. 2010. Web-Scale Bayesian Click-through Rate Prediction for Spon - sored Search Advertising in Microsoft’s Bing Search Engine. In: Procee dings of the International Conference on Machine Learning. Griewank, Andreas, and Walther, Andrea. 2003. Introduction to Automatic Diffe- rentiation. In: Proceedings in Applied Mathematics and Mechanics. Griewank, Andreas, and Walther, Andrea. 2008. Evaluating Derivatives, Principles and Techniques of Algorithmic Differentiation. SIAM. Grimmett, Geoffrey R., and Welsh, Dominic. 2014. Probability: An Introduction. Oxford University Press. Grinstead, Charles M., and Snell, J. Laurie. 1997. Introduction to Probability. Ame- rican Mathematical Society. Hacking, Ian. 2001. Probability and Inductive Logic. Cambridge University Press. Hall, Peter. 1992. The Bootstrap and Edgeworth Expansion. Springer. Hallin, Marc, Paindaveine, Davy, and Šiman, Miroslav. 2010. Multivariate Quantiles and Multiple-Output Regression Quantiles: From ℓ1 Optimization to Halfspace Depth. Annals of Statistics, 38, 635–669.\n--- Страница 498 ---\n498 Библиография Hasselblatt, Boris, and Katok, Anatole. 2003. A First Course in Dynamics with a Panorama of Recent Developments. Cambridge University Press. Hastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. 2001. The Elements of Statistical Learning — Data Mining, Inference, and Prediction. Springer. Hausman, Karol, Springenberg, Jost T., Wang, Ziyu, Heess, Nicolas, and Riedmiller, Martin. 2018. Learning an Embedding Space for Transferable Robot Skills. In: Proceedings of the International Conference on Learning Representations. Hazan, Elad. 2015. Introduction to Online Convex Optimization. Foundations and Trends in Optimization, 2(3–4), 157–325. Hensman, James, Fusi, Nicol ò, and Lawrence, Neil D. 2013. Gaussian Processes for Big Data. In: Proceedings of the Conference on Uncertainty in Artificial Intel - ligence. Herbrich, Ralf, Minka, Tom, and Graepel, Thore. 2007. TrueSkill(TM): A Bayesian Skill Rating System. In: Advances in Neural Information Processing Systems. Hiriart-Urruty, Jean-Baptiste, and Lemar échal, Claude. 2001. Fundamentals of Convex Analysis. Springer. Hoffman, Matthew D., Blei, David M., and Bach, Francis. 2010. Online Learning for Latent Dirichlet Allocation. In: Advances in Neural Information Processing Systems. Hoffman, Matthew D., Blei, David M., Wang, Chong, and Paisley, John. 2013. Sto - chastic Variational Inference. Journal of Machine Learning Research, 14(1), 1303–1347. Hofmann, Thomas, Sch ölkopf, Bernhard, and Smola, Alexander J. 2008. Kernel Methods in Machine Learning. Annals of Statistics, 36(3), 1171–1220. Hogben, Leslie. 2013. Handbook of Linear Algebra. Chapman and Hall/CRC. Horn, Roger A., and Johnson, Charles R. 2013. Matrix Analysis. Cambridge Univer - sity Press. Hotelling, Harold. 1933. Analysis of a Complex of Statistical Variables into Principal Components. Journal of Educational Psychology, 24, 417–441. Hyvarinen, Aapo, Oja, Erkki, and Karhunen, Juha. 2001. Independent Component Analysis. Wiley. Imbens, Guido W., and Rubin, Donald B. 2015. Causal Inference for Statistics, Social and Biomedical Sciences. Cambridge University Press. Jacod, Jean, and Protter, Philip. 2004. Probability Essentials. Springer. Jaynes, Edwin T. 2003. Probability Theory: The Logic of Science. Cambridge Uni - versity Press.\n--- Страница 499 ---\n499 Библиография Jefferys, William H., and Berger, James O. 1992. Ockham’s Razor and Bayesian Analysis. American Scientist, 80, 64–72. Jeffreys, Harold. 1961. Theory of Probability. Oxford University Press. Jimenez Rezende, Danilo, and Mohamed, Shakir. 2015. Variational Inference with Normalizing Flows. In: Proceedings of the International Conference on Machine Learning. Jimenez Rezende, Danilo, Mohamed, Shakir, and Wierstra, Daan. 2014. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In: Proceedings of the International Conference on Machine Learning. Joachims, Thorsten. 1999. Making Large-Scale SVM Learning Practical. Pages 169– 184 in: Advances in Kernel Methods – Support Vector Learning. MIT Press. Jordan, Michael I., Ghahramani, Zoubin, Jaakkola, Tommi S., and Saul, Lawrence K. 1999. An Introduction to Variational Methods for Graphical Models. Machine Learning, 37, 183–233. Julier, Simon J., and Uhlmann, Jeffrey K. 1997. A New Extension of the Kalman Filter to Nonlinear Systems. In: Proceedings of AeroSense Symposium on Aero- space/Defense Sensing, Simulation and Controls. Kaiser, Marcus, and Hilgetag, Claus C. 2006. Nonoptimal Component Placement, but Short Processing Paths, Due to Long-Distance Projections in Neural Systems. PLoS Computational Biology, 2(7), e95. Kalman, Dan. 1996. A Singularly Valuable Decomposition: The SVD of a Matrix. College Mathematics Journal, 27(1), 2–23. Kalman, Rudolf E. 1960. A New Approach to Linear Filtering and Prediction Prob - lems. Transactions of the ASME – Journal of Basic Engineering, 82(Series D), 35–45. Kamthe, Sanket, and Deisenroth, Marc P. 2018. Data-Efficient Reinforcement Lear - ning with Probabilistic Model Predictive Control. In: Proceedings of the Inter - national Conference on Artificial Intelligence and Statistics. Katz, Victor J. 2004. A History of Mathematics. Pearson/Addison-Wesley. Kelley, Henry J. 1960. Gradient Theory of Optimal Flight Paths. Ars Journal, 30(10), 947–954. Kimeldorf, George S., and Wahba, Grace. 1970. A Correspondence between Bayes - ian Estimation on Stochastic Processes and Smoothing by Splines. Annals of Mathematical Statistics, 41(2), 495–502. Kingma, Diederik P., and Ba, Jimmy. 2014. Adam: A Method for Stochastic Optimi - zation. In: Proceedings of the International Conference on Learning Representa - tions.\n--- Страница 500 ---\n500 Библиография Kingma, Diederik P., and Welling, Max. 2014. Auto-Encoding Variational Bayes. In: Proceedings of the International Conference on Learning Representations. Kittler, Josef, and F öglein, Janos. 1984. Contextual Classification of Multispectral Pixel Data. Image and Vision Computing, 2(1), 13–29. Kolda, Tamara G., and Bader, Brett W. 2009. Tensor Decompositions and Applications. SIAM Review, 51(3), 455–500. Koller, Daphne, and Friedman, Nir. 2009. Probabilistic Graphical Models. MIT Press. Kong, Linglong, and Mizera, Ivan. 2012. Quantile Tomography: Using Quantiles with Multivariate Data. Statistica Sinica, 22, 1598–1610. Lang, Serge. 1987. Linear Algebra. Springer. Lawrence, Neil D. 2005. Probabilistic Non-Linear Principal Component Analysis with Gaussian Process Latent Variable Models. Journal of Machine Learning Research, 6(Nov.), 1783–1816. Leemis, Lawrence M., and McQueston, Jacquelyn T. 2008. Univariate Distribution Relationships. American Statistician, 62(1), 45–53. Lehmann, Erich L., and Romano, Joseph P. 2005. Testing Statistical Hypotheses. Springer. Lehmann, Erich Leo, and Casella, George. 1998. Theory of Point Estimation. Sprin- ger. Liesen, J örg, and Mehrmann, Volker. 2015. Linear Algebra. Springer. Lin, Hsuan-Tien, Lin, Chih-Jen, and Weng, Ruby C. 2007. A Note on Platt’s Prob - abilistic Outputs for Support Vector Machines. Machine Learning, 68, 267–276. Ljung, Lennart. 1999. System Identification: Theory for the User. Prentice Hall. Loosli, Gaёlle, Canu, St éphane, and Ong, Cheng Soon. 2016. Learning SVM in Kre ĭn Spaces. IEEE Transactions of Pattern Analysis and Machine Intelligence, 38(6), 1204–1216. Luenberger, David G. 1969. Optimization by Vector Space Methods. Wiley. MacKay, David J. C. 1992. Bayesian Interpolation. Neural Computation, 4, 415–447. MacKay, David J. C. 1998. Introduction to Gaussian Processes. Pages 133–165 of: Neural Networks and Machine Learning. Springer. MacKay, David J. C. 2003. Information Theory, Inference, and Learning Algorithms. Cambridge University Press. Magnus, Jan R., and Neudecker, Heinz. 2007. Matrix Differential Calculus with Applications in Statistics and Econometrics. Wiley.\n--- Страница 501 ---\n501 Библиография Manton, Jonathan H., and Amblard, Pierre-Olivier. 2015. A Primer on Reproducing Kernel Hilbert Spaces. Foundations and Trends in Signal Processing, 8(1–2), 1–126. Markovsky, Ivan. 2011. Low Rank Approximation: Algorithms, Implementation, Applications. Springer. Maybeck, Peter S. 1979. Stochastic Models, Estimation, and Control. Academic Press. McCullagh, Peter, and Nelder, John A. 1989. Generalized Linear Models. CRC Press. McEliece, Robert J., MacKay, David J. C., and Cheng, Jung-Fu. 1998. Turbo Decod - ing as an Instance of Pearl’s “Belief Propagation” Algorithm. IEEE Journal on Selected Areas in Communications, 16(2), 140–152. Mika, Sebastian, R ätsch, Gunnar, Weston, Jason, Sch ölkopf, Bernhard, and M üller, Klaus-Robert. 1999. Fisher Discriminant Analysis with Kernels. Pages 41–48 of: Proceedings of the Workshop on Neural Networks for Signal Processing. Minka, Thomas P. 2001a. A Family of Algorithms for Approximate Bayesian Inference. Ph.D. thesis, Massachusetts Institute of Technology. Minka, Tom. 2001b. Automatic Choice of Dimensionality of PCA. In: Advances in Neural Information Processing Systems. Mitchell, Tom. 1997. Machine Learning. McGraw-Hill. Mnih, Volodymyr, Kavukcuoglu, Koray, & Silver, David, et al. 2015. Human-Level Control through Deep Reinforcement Learning. Nature, 518, 529–533. Moonen, Marc, and De Moor, Bart. 1995. SVD and Signal Processing, III: Algorithms, Architectures and Applications. Elsevier. Moustaki, Irini, Knott, Martin, and Bartholomew, David J. 2015. Latent-Variable Modeling. American Cancer Society. Pages 1–10. Müller, Andreas C., and Guido, Sarah. 2016. Introduction to Machine Learning with Python: A Guide for Data Scientists. O’Reilly Publishing. Murphy, Kevin P. 2012. Machine Learning: A Probabilistic Perspective. MIT Press. Neal, Radford M. 1996. Bayesian Learning for Neural Networks. Ph.D. thesis, De - partment of Computer Science, University of Toronto. Neal, Radford M., and Hinton, Geoffrey E. 1999. A View of the EM Algorithm that Justifies Incremental, Sparse, and Other Variants. Pages 355–368 of: Learning in Graphical Models. MIT Press. Nelsen, Roger. 2006. An Introduction to Copulas. Springer. Nesterov, Yuri. 2018. Lectures on Convex Optimization. Springer.\n--- Страница 502 ---\n502 Библиография Neumaier, Arnold. 1998. Solving Ill-Conditioned and Singular Linear Systems: A Tutorial on Regularization. SIAM Review, 40, 636–666. Nocedal, Jorge, and Wright, Stephen J. 2006. Numerical Optimization. Springer. Nowozin, Sebastian, Gehler, Peter V., Jancsary, Jeremy, and Lampert, Christoph H. (eds). 2014. Advanced Structured Prediction. MIT Press. O’Hagan, Anthony. 1991. Bayes–Hermite Quadrature. Journal of Statistical Planning and Inference, 29, 245–260. Ong, Cheng Soon, Mary, Xavier, Canu, St éphane, and Smola, Alexander J. 2004. Learning with Non-Positive Kernels. In: Proceedings of the International Con - ference on Machine Learning. Ormoneit, Dirk, Sidenbladh, Hedvig, Black, Michael J., and Hastie, Trevor. 2001. Learning and Tracking Cyclic Human Motion. In: Advances in Neural Informa - tion Processing Systems. Page, Lawrence, Brin, Sergey, Motwani, Rajeev, and Winograd, Terry. 1999. The PageRank Citation Ranking: Bringing Order to the Web. Tech. rept. Stanford InfoLab. Paquet, Ulrich. 2008. Bayesian Inference for Latent Variable Models. Ph.D. thesis, University of Cambridge. Parzen, Emanuel. 1962. On Estimation of a Probability Density Function and Mode. Annals of Mathematical Statistics, 33(3), 1065–1076. Pearl, Judea. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann. Pearl, Judea. 2009. Causality: Models, Reasoning and Inference. 2nd edn. Cambridge University Press. Pearson, Karl. 1895. Contributions to the Mathematical Theory of Evolution. II. Skew Variation in Homogeneous Material. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 186, 343– 414. Pearson, Karl. 1901. On Lines and Planes of Closest Fit to Systems of Points in Space. Philo sophical Magazine, 2(11), 559–572. Peters, Jonas, Janzing, Dominik, and Sch ölkopf, Bernhard. 2017. Elements of Cau sal Inference: Foundations and Learning Algorithms. MIT Press. Petersen, Kaare B., and Pedersen, Michael S. 2012. The Matrix Cookbook. Tech. rept. Technical University of Denmark. Platt, John C. 2000. Probabilistic Outputs for Support Vector Machines and Com - parisons to Regularized Likelihood Methods. In: Advances in Large Margin Classifiers.\n--- Страница 503 ---\n503 Библиография Pollard, David. 2002. A User’s Guide to Measure Theoretic Probability. Cambridge University Press. Polyak, Roman A. 2016. The Legendre Transformation in Modern Optimization. Pages 437–507 of: Optimization and Its Applications in Control and Data Sciences. Springer. Press, William H., Teukolsky, Saul A., Vetterling, William T., and Flannery, Brian P. 2007. Numerical Recipes: The Art of Scientific Computing. Cambridge Univer - sity Press. Proschan, Michael A., and Presnell, Brett. 1998. Expect the Unexpected from Con - ditional Expectation. American Statistician, 52(3), 248–252. Raschka, Sebastian, and Mirjalili, Vahid. 2017. Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow. Packt Publishing. Rasmussen, Carl E., and Ghahramani, Zoubin. 2001. Occam’s Razor. In: Advances in Neural Information Processing Systems. Rasmussen, Carl E., and Ghahramani, Zoubin. 2003. Bayesian Monte Carlo. In: Advances in Neural Information Processing Systems. Rasmussen, Carl E., and Williams, Christopher K. I. 2006. Gaussian Processes for Machine Learning. MIT Press. Reid, Mark, and Williamson, Robert C. 2011. Information, Divergence and Risk for Binary Experiments. Journal of Machine Learning Research, 12, 731–817. Rifkin, Ryan M., and Lippert, Ross A. 2007. Value Regularization and Fenchel Dua- lity. Journal of Machine Learning Research, 8, 441–479. Rockafellar, Ralph T. 1970. Convex Analysis. Princeton University Press. Rogers, Simon, and Girolami, Mark. 2016. A First Course in Machine Learning. Chapman and Hall/CRC. Rosenbaum, Paul R. 2017. Observation and Experiment: An Introduction to Causal Inference. Harvard University Press. Rosenblatt, Murray. 1956. Remarks on Some Nonparametric Estimates of a Density Function. Annals of Mathematical Statistics, 27(3), 832–837. Roweis, Sam T. 1998. EM Algorithms for PCA and SPCA. In: Advances in Neural Information Processing Systems. Roweis, Sam T., and Ghahramani, Zoubin. 1999. A Unifying Review of Linear Gauss - ian Models. Neural Computation, 11(2), 305–345. Roy, Anindya, and Banerjee, Sudipto. 2014. Linear Algebra and Matrix Analysis for Statistics. Chapman and Hall/CRC.\n--- Страница 504 ---\n504 Библиография Rubinstein, Reuven Y., and Kroese, Dirk P. 2016. Simulation and the Monte Carlo Method. Wiley. Ruffini, Paolo. 1799. Teoria Generale delle Equazioni, in cui si Dimostra Impossibile la Soluzione Algebraica delle Equazioni Generali di Grado Superiore al Quarto. Stamperia di S. Tommaso d’Aquino. Rumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. 1986. Learning Representations by Back-Propagating Errors. Nature, 323(6088), 533–536. Sæmundsson, Steind ór, Hofmann, Katja, and Deisenroth, Marc P. 2018. Meta Rein - forcement Learning with Latent Variable Gaussian Processes. In: Proceedings of the Conference on Uncertainty in Artificial Intelligence. Saitoh, Saburou. 1988. Theory of Reproducing Kernels and Its Applications. Longman Scientific & Technical. Särkkä, Simo. 2013. Bayesian Filtering and Smoothing. Cambridge University Press. Schölkopf, Bernhard, Herbrich, Ralf, and Smola, Alexander J. 2001. A Generalized Representer Theorem. In: Proceedings of the International Conference on Com - putational Learning Theory. Schölkopf, Bernhard, and Smola, Alexander J. 2002. Learning with Kernels – Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press. Schölkopf, Bernhard, Smola, Alexander J., and M üller, Klaus-Robert. 1997. Kernel Principal Component Analysis. In: Proceedings of the International Conference on Artificial Neural Networks. Schölkopf, Bernhard, Smola, Alexander J., and M üller, Klaus-Robert. 1998. Nonli near Component Analysis as a Kernel Eigenvalue Problem. Neural Computation, 10(5), 1299–1319. Schwartz, Laurent. 1964. Sous Espaces Hilbertiens d’Espaces Vectoriels Topologiques et Noyaux Associ és. Journal d’Analyse Math ématique, 13, 115–256. Schwarz, Gideon E. 1978. Estimating the Dimension of a Model. Annals of Statistics, 6(2), 461–464. Shahriari, Bobak, Swersky, Kevin, Wang, Ziyu, Adams, Ryan P., and De Freitas, Nando. 2016. Taking the Human out of the Loop: A Review of Bayesian Opti - mization. Proceedings of the IEEE, 104(1), 148–175. Shalev-Shwartz, Shai, and Ben-David, Shai. 2014. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press. Shawe-Taylor, John, and Cristianini, Nello. 2004. Kernel Methods for Pattern Anal - ysis. Cambridge University Press. Shawe-Taylor, John, and Sun, Shiliang. 2011. A Review of Optimization Methodo- logies in Support Vector Machines. Neurocomputing, 74(17), 3609–3618.\n--- Страница 505 ---\n505 Библиография Shental, Ori, Siegel, Paul H., Wolf, Jack K., Bickson, Danny, and Dolev, Danny. 2008. Gaussian Belief Propagation Solver for Systems of Linear Equations. In: Pro - ceedings of the International Symposium on Information Theory. Shewchuk, Jonathan R. 1994. An Introduction to the Conjugate Gradient Method without the Agonizing Pain. Shi, Jianbo, and Malik, Jitendra. 2000. Normalized Cuts and Image Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8), 888–905. Shi, Qinfeng, Petterson, James, Dror, Gideon, Langford, John, Smola, Alexander J., and Vishwanathan, S. V. N. 2009. Hash Kernels for Structured Data. Journal of Machine Learning Research, 2615–2637. Shiryayev, Albert N. 1984. Probability. Springer. Shor, Naum Z. 1985. Minimization Methods for Non-Differentiable Functions. Springer. Shotton, Jamie, Winn, John, Rother, Carsten, and Criminisi, Antonio. 2006. Texton - Boost: Joint Appearance, Shape and Context Modeling for Multi-Class Object Recognition and Segmentation. In: Proceedings of the European Conference on Computer Vision. Smith, Adrian F. M., and Spiegelhalter, David. 1980. Bayes Factors and Choice Cri - teria for Linear Models. Journal of the Royal Statistical Society B, 42(2), 213–220. Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. 2012. Practical Bayesian Op - timization of Machine Learning Algorithms. In: Advances in Neural Information Processing Systems. Spearman, Charles. 1904. “General Intelligence,” Objectively Determined and Meas - ured. American Journal of Psychology, 15(2), 201–292. Sriperumbudur, Bharath K., Gretton, Arthur, Fukumizu, Kenji, Sch ölkopf, Bernhard, and Lanckriet, Gert R. G. 2010. Hilbert Space Embeddings and Metrics on Probabi lity Measures. Journal of Machine Learning Research, 11, 1517–1561. Steinwart, Ingo. 2007. How to Compare Different Loss Functions and Their Risks. Constructive Approximation, 26, 225–287. Steinwart, Ingo, and Christmann, Andreas. 2008. Support Vector Machines. Springer. Stoer, Josef, and Burlirsch, Roland. 2002. Introduction to Numerical Analysis. Springer. Strang, Gilbert. 1993. The Fundamental Theorem of Linear Algebra. American Mathematical Monthly, 100(9), 848–855. Strang, Gilbert. 2003. Introduction to Linear Algebra. Wellesley–Cambridge Press.\n--- Страница 506 ---\n506 Библиография Stray, Jonathan. 2016. The Curious Journalist’s Guide to Data. Tow Center for Dig - ital Journalism at Columbia’s Graduate School of Journalism. Strogatz, Steven. 2014. Writing about Math for the Perplexed and the Traumatized. Notices of the American Mathematical Society, 61(3), 286–291. Sucar, Luis E., and Gillies, Duncan F. 1994. Probabilistic Reasoning in High-Level Vision. Image and Vision Computing, 12(1), 42–60. Szeliski, Richard, Zabih, Ramin, Scharstein, Daniel, et al. 2008. A Comparative Study of Ener gy Minimization Methods for Markov Random Fields with Smooth - ness-Based Priors. IEEE Transactions on Pattern Analysis and Machine Intel - ligence, 30(6), 1068–1080. Tandra, Haryono. 2014. The Relationship between the Change of Variable Theorem and the Fundamental Theorem of Calculus for the Lebesgue Integral. Teaching of Mathematics, 17(2), 76–83. Tenenbaum, Joshua B., De Silva, Vin, and Langford, John C. 2000. A Global Geomet - ric Framework for Nonlinear Dimensionality Reduction. Science, 290(5500), 2319–2323. Tibshirani, Robert. 1996. Regression Selection and Shrinkage via the Lasso. Journal of the Royal Statistical Society B, 58(1), 267–288. Tipping, Michael E., and Bishop, Christopher M. 1999. Probabilistic Principal Com - ponent Analysis. Journal of the Royal Statistical Society: Series B, 61(3), 611–622. Titsias, Michalis K., and Lawrence, Neil D. 2010. Bayesian Gaussian Process Latent Variable Model. In: Proceedings of the International Conference on Artificial Intelligence and Statistics. Toussaint, Marc. 2012. Some Notes on Gradient Descent. https://ipvs.informatik. uni-stuttgart.de/mlr/marc/notes/gradientDescent.pdf. Trefethen, Lloyd N., and Bau III, David. 1997. Numerical Linear Algebra. SIAM. Tucker, Ledyard R. 1966. Some Mathematical Notes on Three-Mode Factor Analy - sis.Psychometrika, 31(3), 279–311. Vapnik, Vladimir N. 1998. Statistical Learning Theory. Wiley. Vapnik, Vladimir N. 1999. An Overview of Statistical Learning Theory. IEEE Trans - actions on Neural Networks, 10(5), 988–999. Vapnik, Vladimir N. 2000. The Nature of Statistical Learning Theory. Springer. Vishwanathan, S. V. N., Schraudolph, Nicol N., Kondor, Risi, and Borgwardt, Karsten M. 2010. Graph Kernels. Journal of Machine Learning Research, 11, 1201–1242.\n--- Страница 507 ---\n507 Библиография von Luxburg, Ulrike, and Sch ölkopf, Bernhard. 2011. Statistical Learning Theory: Models, Concepts, and Results. Pages 651–706 of: Handbook of the History of Logic, vol. 10. Elsevier. Wahba, Grace. 1990. Spline Models for Observational Data. Society for Industrial and Applied Mathematics. Walpole, Ronald E., Myers, Raymond H., Myers, Sharon L., and Y e, Keying. 2011. Probability and Statistics for Engineers and Scientists. Prentice Hall. Wasserman, Larry. 2004. All of Statistics. Springer. Wasserman, Larry. 2007. All of Nonparametric Statistics. Springer. Whittle, Peter. 2000. Probability via Expectation. Springer. Wickham, Hadley. 2014. Tidy Data. Journal of Statistical Software, 59, 1–23. Williams, Christopher K. I. 1997. Computing with Infinite Networks. In: Advances in Neural Information Processing Systems. Yu, Yaoliang, Cheng, Hao, Schuurmans, Dale, and Szepesv ári, Csaba. 2013. Charac - terizing the Representer Theorem. In: Proceedings of the International Confer - ence on Machine Learning. Zadrozny, Bianca, and Elkan, Charles. 2001. Obtaining Calibrated Probability Esti - mates from Decision Trees and Naive Bayesian Classifiers. In: Proceedings of the International Conference on Machine Learning. Zhang, Haizhang, Xu, Yuesheng, and Zhang, Jun. 2009. Reproducing Kernel Banach Spaces for Machine Learning. Journal of Machine Learning Research, 10, 2741–2775. Zia, Royce K. P., Redish, Edward F., and McKay, Susan R. 2009. Making Sense of the Legendre Transform. American Journal of Physics, 77(7), 614–622.\n--- Страница 508 ---\nМарк Питер Дайзенрот, А. Альдо Фейзал, Чен Сунь Он Математика в машинном обучении Перевел с английского С. Черников Руководитель дивизиона Ю. Сергиенко Ведущий редактор Е. Строганова Научный редактор К. Кноп Литературный редактор А. Алимова Художественный редактор В. Мостипан Корректор М. Молчанова Верстка Л. Егорова Изготовлено в России. Изготовитель: ООО «Прогресс книга». Место нахождения и фактический адрес: 194044, Россия, г. Санкт-Петербург, Б. Сампсониевский пр., д. 29А, пом. 52. Тел.: +78127037373. Дата изготовления: 08.2023. Наименование: книжная продукция. Срок годности: не ограничен. Налоговая льгота — общероссийский классификатор продукции ОК 034-2014, 58.11.12 — Книги печатные профессиональные, технические и научные. Импортер в Беларусь: ООО «ПИТЕР М», 220020, РБ, г. Минск, ул. Тимирязева, д. 121/3, к. 214, тел./факс: 208 80 01. Подписано в печать 12.07.23. Формат 70 ×100/16. Бумага офсетная. Усл. п. л. 41,280. Тираж 700. Заказ 0000.\n--- Страница 509 ---\nМигель Моралес ГРОКАЕМ ГЛУБОКОЕ ОБУЧЕНИЕ С ПОДКРЕПЛЕНИЕМ Мы учимся, взаимодействуя с окружающей средой, и получаемые возна ­ граждения и наказания определяют наше поведение в будущем. Глубокое обучение с подкреплением привносит этот естественный процесс в ис ­ кусственный интеллект и предполагает анализ результатов для выявления наиболее эффективных путей движения вперед. Агенты глубокого обучения с подкреплением могут способствовать успеху маркетинговых кампаний, прогнозировать рост акций и побеждать гроссмейстеров в го и шахматах. Давайте научимся создавать системы глубокого обучения на примере увле ­ кательных упражнений, сопровождаемых кодом на Python с подробными комментариями и понятными объяснениями. Вы увидите, как работают алгоритмы, и научитесь создавать собственных агентов глубокого обучения с подкреплением, используя оценочную обратную связь. КУПИТЬ\n--- Страница 510 ---\nФрансуа Шолле ГЛУБОКОЕ ОБУЧЕНИЕ НА PYTHON 2-е международное издание Глубокое обучение динамично развивается, открывая все новые и новые возможности создания ПО. Это не только автоматический перевод текстов с одного языка на другой, распознавание изображений, но и многое другое. Глубокое обучение превратилось в важный навык, необходимый каждому разработчику. Keras и TensorFlow облегчают жизнь разработчикам и по ­ зволяют легко работать даже тем, кто не имеет фундаментальных знаний в области математики или науки о данных. Настала пора познакомиться с глубоким обучением и мощной библиотекой Keras! В этом расширенном и дополненном издании создатель библиотеки Keras Франсуа Шолле делится знаниями и с новичками, и с опытными специа­ листами. Иллюстрации и наглядные примеры помогут вам разобраться с самыми сложными вопросами и концепциями. Вы быстро приобретете навыки, необходимые для разработки приложений глубокого обучения. КУПИТЬ\n--- Страница 511 ---\nРишал Харбанс ГРОКАЕМ АЛГОРИТМЫ ИСКУСCТВЕННОГО ИНТЕЛЛЕКТА Искусственный интеллект — часть нашей повседневной жизни. Мы встре ­ чаемся с его проявлениями, когда занимаемся шопингом в интернет ­ма­ газинах, получаем рекомендации «вам может понравиться этот фильм», узнаем медицинские диагнозы… Чтобы уверенно ориентироваться в новом мире, необходимо понимать алгоритмы, лежащие в основе ИИ. «Грокаем алгоритмы искусственного интеллекта» объясняет фундаменталь ­ ные концепции ИИ с помощью иллюстраций и примеров из жизни. Все, что вам понадобится, — это знание алгебры на уровне старших классов школы, и вы с легкостью будете решать задачи, позволяющие обнаружить банков ­ ских мошенников, создавать шедевры живописи и управлять движением беспилотных автомобилей. КУПИТЬ\n--- Страница 512 ---\nКори Альтхофф COMPUTER SCIENCE ДЛЯ ПРОГРАММИСТА- САМОУЧКИ. ВСЕ ЧТО НУЖНО ЗНАТЬ О СТРУКТУРАХ ДАННЫХ И АЛГОРИТМАХ Книги Кори Альтхоффа вдохновили сотни тысяч людей на самостоятельное изучение программирования. Чтобы стать профи в программировании, не обязательно иметь диплом в области computer science, и личный опыт Кори подтверждает это: он стал разработчиком ПО в eBay и добился этого самостоятельно. Познакомьтесь с наиболее важными темами computer science, в которых должен разбираться каждый программист ­самоучка, мечтающий о выда ­ ющейся карьере, — это структуры данных и алгоритмы. «Computer Science для программиста ­самоучки» поможет вам пройти техническое интервью, без которого нельзя получить работу в «айти». Книга написана для абсолютных новичков, поэтому у вас не должно возник ­ нуть трудностей, даже если ранее вы ничего не слышали о computer science. КУПИТЬ",
          "debug": {
            "start_page": 489,
            "end_page": 512
          }
        }
      ]
    }
  ]
}
{
  "title": "Элегантный SciPy Искусство научного программирования на Python [2018] Х. Нуньес-Иглесиас, Ш. Уолт Х. Дэшноу",
  "chapters": [
    {
      "name": "Глава 1. Элегантный NumPy: фундамент научного программирования на Python 32",
      "content": "--- Страница 32 --- (продолжение)\nГлава 1 Элегантный NumPy: фундамент научного программирования на Python [Библиотека NumPy] повсюду. Она окружает нас. Даже сейчас она с нами рядом. Ты видишь ее, когда смотришь в окно или включаешь те-левизор. Ты ощущаешь ее, когда работаешь, идешь в церковь, когда платишь налоги. – Морфеус, к/ф «Матрица» Эта глава затрагивает некоторые статистические функции SciPy, однако осо-бое внимание в ней уделено исследованию массива NumPy, структуры данных, которая лежит в основе почти всех численных научных вычислений в Python. Мы увидим, как операции с массивами NumPy позволяют создавать краткий и эффективный исходный код для управления числовыми данными. В нашем случае мы будем использовать данные экспрессии генов из про- екта «Атлас ракового генома» (The Cancer Genome Atlas, TCGA) для предсказа-ния смертности среди больных раком кожи. Мы будем работать в направлении этой цели на протяжении всей этой главы и главы 2, по ходу знакомясь с не-которыми ключевыми понятиями SciPy. Прежде чем мы сможем предсказать смертность, мы должны нормализовать данные экспрессии, используя метод под названием «нормализация RPKM». Он позволяет сравнивать результаты из-мерений между различными образцами и генами. (Мы раскроем смысл поня-тия «экспрессия гена» буквально через мгновение.) Чтобы вас заинтриговать и познакомить с идеями этой главы, давайте начнем с фрагмента кода. Так же, как и в других главах, мы начинаем с при-мера исходного кода, который, по нашему мнению, воплощает элегантность и мощь той или иной функции экосистемы SciPy. В данном случае мы хотим подчеркнуть правила векторизации и транслирования библиотеки NumPy, ко- 6 / 27\nГлава 1 Элегантный NumPy: фундамент научного программирования на Python [Библиотека NumPy] повсюду. Она окружает нас. Даже сейчас она с нами рядом. Ты видишь ее, когда смотришь в окно или включаешь те-левизор. Ты ощущаешь ее, когда работаешь, идешь в церковь, когда платишь налоги. – Морфеус, к/ф «Матрица» Эта глава затрагивает некоторые статистические функции SciPy, однако осо-бое внимание в ней уделено исследованию массива NumPy, структуры данных, которая лежит в основе почти всех численных научных вычислений в Python. Мы увидим, как операции с массивами NumPy позволяют создавать краткий и эффективный исходный код для управления числовыми данными. В нашем случае мы будем использовать данные экспрессии генов из про- екта «Атлас ракового генома» (The Cancer Genome Atlas, TCGA) для предсказа-ния смертности среди больных раком кожи. Мы будем работать в направлении этой цели на протяжении всей этой главы и главы 2, по ходу знакомясь с не-которыми ключевыми понятиями SciPy. Прежде чем мы сможем предсказать смертность, мы должны нормализовать данные экспрессии, используя метод под названием «нормализация RPKM». Он позволяет сравнивать результаты из-мерений между различными образцами и генами. (Мы раскроем смысл поня-тия «экспрессия гена» буквально через мгновение.) Чтобы вас заинтриговать и познакомить с идеями этой главы, давайте начнем с фрагмента кода. Так же, как и в других главах, мы начинаем с при-мера исходного кода, который, по нашему мнению, воплощает элегантность и мощь той или иной функции экосистемы SciPy. В данном случае мы хотим подчеркнуть правила векторизации и транслирования библиотеки NumPy, ко- 6 / 27\n--- Страница 33 ---\nЭлегантный NumPy: фундамент научного программирования на Python  33 торые позволяют нам очень эффективно управлять массивами данных и де- лать о них выводы. def rpkm(counts, lengths): \"\"\"Вычислить прочтения на тысячу оснований экзона на миллион картированных прочтений (reads per kilobase transcript per million reads). RPKM = (10^9 * C) / (N * L) где: C = количество прочтений, картированных на ген N = суммы количеств картированных (выровненных) прочтений в эксперименте L = длина экзона в парах оснований для гена Параметры --------- counts: массив, форма (N_genes, N_samples) РНК-сек (или подобные) количественные данные, где столбцы являются отдельными образцами и строки – генами. lengths: массив, форма (N_genes,) Длины генов в парах оснований в том же порядке, что и строки в counts. Возвращает ---------- normed: массив, форма (N_genes, N_samples) Матрица количеств counts, нормализованная согласно RPKM. «»» N = np.sum(counts, axis=0) # просуммировать каждый столбец, чтобы # получить суммы количеств прочтений на образец L = lengths C = counts normed = 1e9 * C / (N[np.newaxis, :] * L[:, np.newaxis]) return(normed) Этот пример иллюстрирует несколько приемов, благодаря которым массивы NumPy могут сделать ваш код элегантнее: массивы могут быть одномерными, как списки, но могут быть и двумер- ными, как матрицы, и даже более высокой размерности. Это позволяет представлять множество различных видов числовых данных. В нашем случае мы манипулируем двумерной матрицей; с массивами можно выполнять операции вдоль осей. В первой строке мы вычисляем сумму каждого столбца, задав ось axis=0 ; массивы позволяют выражать сразу несколько числовых операций. На-пример, ближе к концу функции мы делим двумерный массив количеств (C) на одномерный массив постолбцовых сумм (N). Такая операция на-зывается транслированием. Дополнительная информация о том, как она работает, буквально через мгновение! Прежде чем мы начнем вникать в мощные возможности NumPy, давайте по- тратим немного времени, чтобы разобраться в биологических данных, с кото-рыми мы будем работать. 7 / 27\n--- Страница 34 ---\n34  Элегантный NumPy: фундамент научного программирования на Python ВВеДение В Данные : что такое экспрессия гена? Мы построим свою работу, опираясь на анализ экспрессии генов, который позво- лит продемонстрировать силу библиотек NumPy и SciPy в решении реальной биологической задачи. Мы воспользуемся библиотекой pandas, которая опира-ется на NumPy, чтобы прочитать и преобразовать наши файлы данных, и затем мы будем эффективно манипулировать нашими данными в массивах NumPy. Так называемая центральная догма молекулярной биологии 1 постулирует, что вся информация, необходимая для анализа клетки (или в данном случае организма), хранится в молекуле, называемой дезоксирибонуклеиновой кис - лотой, или ДНК. Эта молекула имеет периодически повторяющийся остов, на котором лежат химические группы, именуемые основаниями, в последователь- ности (рис. 1.1). Имеется четыре вида оснований, сокращенно A, C, G и T, со-ставляющих алфавит, посредством которого сохраняется информация. Фосфат- дезоксирибозный остовАденинТимин ГуанинЦитозин3’ -конец3’ -конец5’ -конец 5’ -конец Рис. 1.1  Химическая структура ДНК (автор Маделин Прайс Болл, изображение используется в соответствии с лицензией общего пользования CC0) 1 См. https://en.wikipedia.org/wiki/Central_dogma_of_molecular_biology. 8 / 27\n--- Страница 35 ---\nВведение в данные: что такое экспрессия гена?  35 Чтобы получить доступ к этой информации, ДНК транскрибируется в род- ственную молекулу, которая называется матричной рибонуклеиновой кисло- той, или мРНК. Наконец, мРНК транслируется в белки, «рабочие лошадки» клетки (рис. 1.2). Участок ДНК, в котором закодирована информация, дающая белок (через мРНК), называется геном. Центральная догма Трансляция ДНК РНК БелокТранскрипция Рис. 1.2  Центральная догма молекулярной биологии Количество мРНК, получаемых из конкретного гена, называется экспрессией этого гена. В идеальном случае мы хотели бы измерить уровни белка. Однако это намного более трудная задача, чем измерение мРНК. К счастью, уровни экс - прессии мРНК и уровни соответствующего ей белка обычно коррелируются 1. Поэтому мы обычно измеряем уровни мРНК и на этом основании проводим наши исследования. Как вы увидите ниже, зачастую это не имеет значимой разницы, потому что уровни мРНК используются из-за их способности пред-сказывать биологические исходы, избавляя от необходимости делать опреде-ленно сформулированные утверждения о белках. Важно отметить, что ДНК в каждой клетке вашего тела идентична. Поэтому различия между клетками являются результатом дифференциальной экспрес - сии этой ДНК на РНК: в разных клетках разные участки ДНК обрабатывают - ся в нисходящие молекулы (рис. 1.3). Аналогичным образом, как мы увидим в этой и следующей главах, дифференциальная экспрессия может идентифи-цировать различные виды рака. Современная технология измерения количества мРНК основывается на мето- де секвенирования (расшифровки) РНК, который носит сокращенное название РНК-сек (RNA-seq). РНК извлекается из образца ткани (например, в результате взятой у пациента биопсии), обратно транскрибируется в ДНК (чья стабильность выше) и затем прочитывается с использованием химически модифицирован- ных оснований, которые светятся 2 при включении их в после довательность ДНК. 1 См.: Майер Т., Гвелл М., Серрано Л. Корреляция мРНК и белка в составных биоло- гических образцах (Tobias Maier, Marc Güell, and Luis Serrano. Correlation of mRNA and protein in complex biological samples. FEBS Letters 583, no. 24 (2009). https://www. sciencedirect.com/science/article/pii/S0014579309008126). 2 См.: И все-таки ДНК светится // https://22century.ru/biology-and-biotechnology/31442. – Прим. перев. 9 / 27\n--- Страница 36 ---\n36  Элегантный NumPy: фундамент научного программирования на Python В настоящее время высокопроизводительные секвенирующие машины способ- ны прочитать лишь короткие фрагменты (как правило, приблизительно 100 ос - нований). Эти короткие последовательности называются «прочтениями»1. Мы измеряем миллионы прочтений и затем на основе их последовательностей под-считываем, сколько прочтений пришло из каждого гена (рис. 1.4). Мы начнем наш анализ непосредственно с этих количественных данных. Клетка кожи ДНКГен A Ген ВГен СКлетка мозга Клетка печени Рис. 1.3  Экспрессия гена Таблица 1.1 показывает минимальный пример количественных данных об экспрессии генов. Таблица 1.1. Количественные данные об экспрессии генов Тип клетки A Тип клетки B Ген 0 100 200 Ген 1 50 0 Ген 2 350 100 Эти данные представляют собой таблицу количеств прочтений в виде целых чи- сел, показывающих, сколько прочтений наблюдалось относительно каждого гена в каждом типе клетки. Вы можете заметить, насколько эти количества по каждому гену разнятся в зависимости от типов клетки. Эта информация может использо-ваться для того, чтобы узнать различия между этими двумя типами клеток. Одним из способов представить эти данные в Python является список спис - ков: gene0 = [100, 200] gene1 = [50, 0]gene2 = [350, 100]expression_data = [gene0, gene1, gene2] 1 Прочтение, или рид (read) – это отсеквенированная последовательность коротких отрезков, полученных при разбиении молекулы, в данном случае молекулы мРНК. – Прим. перев. 10 / 27\n--- Страница 37 ---\nВведение в данные: что такое экспрессия гена?  37 Ген B, 4 прочтенияИзвлечь РНК из клеток Разбить на небольшие фрагменты Произвольно секвенировать Вычислительно картировать (отобразить) прочтения на гены Ген A, 14 прочтений Рис. 1.4  Секвенирование РНК (РНК-сек) Выше показано, что экспрессия каждого гена по разным типам клеток хра- нится в списке целых чисел Python. Затем мы сохраняем все эти списки в дру - гом списке (метасписке, если быть точнее). При этом отдельные точки данных можно извлекать, используя два уровня индексации списка: expression_data[2][0] 350 Этот способ хранения точек данных является весьма неэффективным вследствие характера работы интерпретатора Python. Прежде всего списки Python всегда являются списками объектов. Поэтому приведенный выше список gene2 является не списком целых чисел, а списком указателей на це- лые числа, т. е. ненужными дополнительными издержками. Кроме того, такое представление означает, что каждый из этих списков и каждое из этих целых чисел в конечном итоге занимают совершенно разные случайные участки оперативной памяти вашего компьютера. Вместе с тем современные про-цессоры на практике предпочитают извлекать данные из памяти блоками, поэтому такое распределение данных по всей оперативной памяти является неэффективным. Как раз эта проблема эффективно решается благодаря массивам NumPy. 11 / 27\n--- Страница 38 ---\n38  Элегантный NumPy: фундамент научного программирования на Python n-мерные массиВы numPy Одним из ключевых типов данных NumPy является N-мерный массив ( ndarray , или просто массив). Массивы ndarray лежат в основе многих потрясающих методов управления данными в SciPy. В частности, мы займемся исследова-нием методов, которые позволяют писать мощный и элегантный программ-ный код для управления данными, конкретно методы векторизации и транс - лирования. Прежде всего давайте разберемся с массивом ndarray. Эти массивы должны быть гомогенными: все значения в массиве должны иметь один и тот же тип. В нашем случае мы должны хранить целые числа. Массивы ndarray называют - ся N-мерными, потому что они могут иметь любое количество размерностей. Одномерный массив примерно эквивалентен списку Python: import numpy as np array1d = np.array([1, 2, 3, 4]) print(array1d) print(type(array1d)) [1 2 3 4] <class 'numpy.ndarray'> Массивы имеют особые атрибуты и методы, к которым можно получить до- ступ, поставив точку после имени массива. Например, форму массива можно получить следующим образом: print(array1d.shape) (4,) Здесь это просто кортеж с единственным числом. Вы можете спросить, поче- му просто не применить функцию len, как это было бы сделано в случае списка. И вы будете правы, это сработает. Однако на двумерные массивы этот прием распространяться не будет. Вот что мы используем для представления данных в табл. 1.1: array2d = np.array(expression_data) print(array2d)print(array2d.shape)print(type(array2d)) [[100 200] [ 50 0] [350 100]](3, 2)<class 'numpy.ndarray'> Теперь вы видите, что атрибут shape обобщает функцию len, создавая отчет о величине многочисленных размерностей массива данных. 12 / 27\n--- Страница 39 ---\nN-мерные массивы NumPy  39 Форма: (4,) Форма: (2, 3) Форма: (4, 3, 2)Одномерный массивДвумерный массивТрехмерный массив Ось 0Ось 0Ось 0 Ось 1Ось 1 Ось 2 Рис. 1.5  Визуализация массивов ndarrays NumPy в одной, двух и трех размерностях Массивы имеют и другие атрибуты, такие как ndim, количество размерностей: print(array2d.ndim) 2 Вы познакомитесь со всеми этими атрибутами, когда начнете широко ис - пользовать NumPy при выполнении своего собственного анализа данных. Массивы NumPy могут представлять данные с еще большим количеством размерностей в таких случаях, как, например, данные магнитно-резонансной томографии (МРТ), которые включают результаты измерений внутри трехмер-ного объема. Если хранить значения МРТ во времени, то нам, возможно, пона-добится четырехмерный массив NumPy. Пока же мы будем придерживаться двумерных данных. В последующих гла- вах будут введены многомерные данные с размерностями числом более двух, и вы научитесь писать программный код, который работает для данных любо-го количества размерностей. Зачем использовать массивы ndarray вместо списков Python? Массивы имеют высокое быстродействие, потому что они задействуют векто-ризованные операции, написанные на низкоуровневом языке C. Эти операции работают на всем массиве в целом. Положим, у вас есть список, и вы хотите умно жить каждый элемент в списке на пять. Стандартный подход Python со- стоит в написании цикла, который перебирает элементы списка и умножает каждый элемент на пять. Однако если вместо этого ваши данные представлены в виде массива, то вы можете одновременно умножить каждый элемент масси-ва на пять. Высокооптимизированная библиотека NumPy максимально быстро выполнит эту итеративную обработку за кадром. 13 / 27\n--- Страница 40 ---\n40  Элегантный NumPy: фундамент научного программирования на Python import numpy as np # Создать массив ndarray целочисленных в диапазоне # от 0 и до (но не включая) 1 000 000 array = np.arange(1e6) # Конвертировать его в список list_array = array.tolist() Давайте сравним, сколько потребуется времени, чтобы умножить все зна- чения в массиве на пять, воспользовавшись для этого волшебной функцией IPython timeit . Сначала возьмем данные, которые находятся в списке: %timeit -n10 y = [val * 5 for val in list_array] 10 loops, average of 7: 102 ms +- 8.77 ms per loop (using standard deviation) Теперь выполним ту же операцию с использованием встроенных векторизо- ванных операций NumPy: %timeit -n10 x = array * 510 loops, average of 7: 1.28 ms +- 206 µs per loop (using standard deviation) Быстродействие более чем в 50 раз быстрее, а команда к тому же короче! Массивы также эффективно экономят объем используемой оперативной памяти. В языке Python каждый элемент в списке является объектом, для ко-торого выделяется порядочный участок оперативной памяти (что выглядит как расточительность). В массивах же каждый элемент занимает лишь необ-ходимый объем оперативной памяти. Например, массив 64-разрядных целых чисел займет ровно 64 бита в расчете на элемент, плюс очень маленький рас - ход на метаданные массива, такие как атрибут shape , который мы упоминали выше. Это, как правило, намного меньше, чем было бы выделено на объекты в списке Python. (Если вам интересно узнать, каким образом в Python работает процедура выделения оперативной памяти, обратитесь к публикации в блоге Джейка Вандерпласа «Почему Python медленный: взгляд изнутри» 1.) Кроме этого, при вычислениях с использованием массивов вы также може- те использовать срезы, которые извлекают подмножество массива, не копируя основные данные. # Создать массив ndarray x x = np.array([1, 2, 3], np.int32)print(x) [1 2 3] # Создать «срез» массива x y = x[:2]print(y) [1 2] 1 См. https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/. 14 / 27\n--- Страница 41 ---\nN-мерные массивы NumPy  41 # Назначить первому элементу среза y значение 6 y[0] = 6print(y) [6 2] Обратите внимание: хотя мы редактировали только срез y, массив x тоже из - менился, так как срез y ссылается на те же самые данные! # Теперь первый элемент в массиве x поменялся на 6! print(x) [6 2 3] Это означает, что во время работы с указателями на массив вам следует про- являть осторожность. Если вы хотите управлять данными, не касаясь оригина- ла, сделайте копию. Это очень легко сделать: y = np.copy(x[:2]) Векторизация Ранее мы говорили о быстродействии операций с массивами. Одной из хит - ростей, которую NumPy использует для ускорения вычислений, является век- торизация. Благодаря векторизации можно применять вычисление к каждому элементу в массиве без необходимости использования цикла for. В дополнение к ускорению вычислений векторизация может приводить к более естественному, удобочитаемому программному коду. Давайте рассмотрим несколько примеров. x = np.array([1, 2, 3, 4]) print(x * 2) [2 4 6 8] В приведенном выше примере у нас массив x из 4 значений, и мы неявным образом умножили каждый элемент в x на одиночное значение, равное 2. y = np.array([0, 1, 2, 1]) print(x + y) [1 3 5 5] Теперь мы сложили каждый элемент в x с соответствующим ему элементом в массиве y той же самой формы. Обе эти операции просты и, надеемся, представляют собой интуитивно по- нятные примеры векторизации. Более того, NumPy выполняет их очень быст - ро, намного быстрее, чем итеративный обход массива в ручном режиме. (Вы можете смело поэкспериментировать с векторизацией самостоятельно, при-менив волшебную функцию IPython %timeit , которую мы упоминали ранее.) Транслирование Одной из самых мощных и часто недооцененных особенностей массивов ndarray является операция транслирования. Транслирование – это прием вы- 15 / 27\n--- Страница 42 ---\n42  Элегантный NumPy: фундамент научного программирования на Python полнения неявных операций между двумя массивами. Этот метод позволяет выполнять операции с массивами совместимых форм, создавать более круп- ные массивы, чем оба исходных массива. Например, мы можем вычислить внешнее векторное произведение 1 двух векторов, изменив их форму соответ - ствующим образом: x = np.array([1, 2, 3, 4]) x = np.reshape(x, (len(x), 1))print(x) [[1] [2] [3] [4]] y = np.array([0, 1, 2, 1]) y = np.reshape(y, (1, len(y)))print(y) [[0 1 2 1]] Две формы совместимы, когда по каждой размерности обе равны единице либо они совпадают друг с другом2. Давайте проверим формы двух этих массивов. print(x.shape) print(y.shape) (4, 1) (1, 4) Оба массива имеют две размерности, и внутренние размерности обоих мас - сивов равняются 1, следовательно, размерности совместимы! outer = x * yprint(outer) [[0 1 2 1] [0 2 4 2] [0 3 6 3] [0 4 8 4]] Внешние размерности говорят о размере результирующего массива. В на- шем случае мы ожидаем получить массив (4, 4): print(outer.shape) (4, 4) Вы можете сами убедиться, что outer[i, j] = x[i] * y[j] для всех (i, j). 1 См. https://en.wikipedia.org/wiki/Outer_product. 2 Мы всегда начинаем со сравнения последних размерностей и продвигаемся вперед, игнорируя избыточные размерности, в случае если размерность одного из массивов больше, чем размерность другого массива (например, (3, 5, 1) и (5, 8) совпадают). 16 / 27\n--- Страница 43 ---\nИсследование набора данных экспрессии генов  43 Это было достигнуто за счет правил транслирования NumPy1, которые не- явно расширяют размерности величиной 1 в одном массиве, чтобы данная размерность совпадала с соответствующей размерностью другого массива. Не волнуйтесь, позже в этой главе мы поговорим об этих правилах подробнее. Как мы убедимся в оставшейся части настоящей главы во время исследо- вания реальных данных, операция транслирования имеет чрезвычайную цен-ность для реальных вычислений, связанных с массивами данных. Она позво-ляет выражать сложные операции сжато и эффективно. исслеД оВание набора Данных экспрессии геноВ Используемый нами набор данных является экспериментом по РНК-секве ни- ро ванию (РНК-сек) образцов рака кожи из проекта «Атлас ракового генома» (TCGA)2. Предварительно мы уже очистили и отсортировали данные, поэтому вы можете использовать файл data/counts.txt в хранилище настоящей книги. В главе 2 мы будем использовать эти данные экспрессии генов, чтобы пред- сказать смертность среди больных раком кожи, и воспроизведем упрощенную версию рис. 5A и 5B 3 исследовательской работы4 консорциума TCGA. Но снача- ла нам нужно разобраться в смещениях в наших данных и подумать о том, как их уменьшить. Чтение данных при помощи библиотеки pandas Сначала мы воспользуемся библиотекой pandas, чтобы прочитать таблицу количеств. Заметим, pandas – это библиотека Python, предназначенная для управления данными и их анализа. При этом в этой библиотеке особый ак - цент делается на табличных данных и данных временных рядов. В нашем слу - чае мы воспользуемся этой библиотекой, чтобы прочитать табличные данные смешанного типа. В этой библиотеке имеется тип DataFrame , представляющий собой гибкий табличный формат, основанный на объекте языка R, в котором данный формат носит название «фрейм данных». Например, данные, которые мы будем читать, включают в себя столбец имен генов (строковых значений) и многочисленные столбцы количеств (целых чисел). Поэтому было бы непра-вильным заносить их в гомогенный массив чисел. Хотя NumPy располагает некоторой поддержкой смешанных типов данных (реализованной за счет так называемых «структурированных массивов»), эта библиотека в основном не рассчитана на применение, усложняющее последующие операции. При чтении данных в виде фрейма данных pandas мы поручаем библиотеке pandas выполнить их разбор, затем извлечь релевантную информацию и со- 1 См. https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html. 2 См. http://cancergenome.nih.gov/. 3 См. http://www.cell.com/action/showImagesData?pii=S0092-8674(15)00634-0. 4 См. http://www.cell.com/cell/fulltext/S0092-8674(15)00634-0?_returnURL=http://linkinghub. elsevier.com/retrieve/pii/S0092867415006340%3Fshowall%3Dtrue. 17 / 27\n--- Страница 44 ---\n44  Элегантный NumPy: фундамент научного программирования на Python хранить ее в более эффективном типе данных. Здесь мы используем pandas для быстрого импорта данных. В последующих главах мы познакомимся с pandas поближе, однако более подробную информацию вы можете получить, прочи-тав книгу «Python для анализа данных» (О’Рейли) 1, написанную создателем биб лиотеки pandas Уэса Маккинни (Wes McKinney). import numpy as np import pandas as pd # Импортировать данные TCGA по меланоме filename = 'data/counts.txt'with open(filename, 'rt') as f: data_table = pd.read_csv(f, index_col=0) # pandas выполняет разбор данных print(data_table.iloc[:5, :5]) 00624286-41dd-476f-a63b-d2a5f484bb45 TCGA-FS-A1Z0 TCGA-D9-A3Z1 \\ A1BG 1272.36 452.96 288.06 A1CF 0.00 0.00 0.00 A2BP1 0.00 0.00 0.00A2LD1 164.38 552.43 201.83A2ML1 27.00 0.00 0.00 02c76d24-f1d2-4029-95b4-8be3bda8fdbe TCGA-EB-A51B A1BG 400.11 420.46A1CF 1.00 0.00 A2BP1 0.00 1.00 A2LD1 165.12 95.75A2ML1 0.00 8.00 Мы видим, что библиотека pandas любезно извлекла строку заголовка и ис - пользовала ее, чтобы назвать столбцы. Первый столбец дает имя каждому гену, а остальные столбцы представляют отдельные образцы. Нам также будут нужны соответствующие метаданные, включая информа- цию об образце и длине генов. # Имена образцов samples = list(data_table.columns) Информация о длине генов нам потребуется для нормализации. Поэтому, чтобы воспользоваться причудливой индексацией, принятой в pandas, мы за- дадим индексирование таблицы pandas по именам генов в первом столбце. # Импортировать длины генов filename = 'data/genes.csv' with open(filename, 'rt') as f: # Разобрать файл при помощи pandas, индексировать по GeneSymbol gene_info = pd.read_csv(f, index_col=0) print(gene_info.iloc[:5, :]) GeneID GeneLength GeneSymbolCPA1 1357 1724 1 См. http://shop.oreilly.com/product/0636920050896.do. 18 / 27\n--- Страница 45 ---\nИсследование набора данных экспрессии генов  45 GUCY2D 3000 3623 UBC 7316 2687C11orf95 65998 5581ANKMY2 57037 2611 Давайте проверим, насколько хорошо наши данные о длине генов совпада- ют с данными о количествах экспрессии. print(\"Гены в data_table: \", data_table.shape[0])print(\"Гены в gene_info: \", gene_info.shape[0]) Гены в data_table: 20500 Гены в gene_info: 20503 В данных о длине генов имеется больше генов, чем фактически было изме- рено в эксперименте. Давайте выполним фильтрацию, чтобы получить толь- ко релевантные гены. При этом мы хотим удостовериться, что они находятся в том же самом порядке, что и в наших количественных данных. Как раз здесь пригодится индексирующий функционал библиотеки pandas! Из наших двух источников данных можно получить пересечение имен генов и их использо-вать для индексации обоих наборов данных, тем самым гарантируя, что они будут иметь одинаковые гены, расположенные в том же самом порядке. # Взять подмножество генной информации, которая # совпадает с количественными даннымиmatched_index = pd.Index.intersection(data_table.index, gene_info.index) Теперь давайте применим пересечение имен генов, чтобы проиндексиро- вать количественные данные. # Двумерный массив ndarray, содержащий количества экспрессии # для каждого гена в каждом индивидуумеcounts = np.asarray(data_table.loc[matched_index], dtype=int)gene_names = np.array(matched_index) # Проверить, сколько генов и индивидуумов измерено print(f'{counts.shape[0]} генов измерено в {counts.shape[1]} индивидуумах.') 20500 генов измерено в 375 индивидуумах. И данные с длинами генов: # Одномерный массив ndarray, содержащий длины каждого гена gene_lengths = np.asarray(gene_info.loc[matched_index]['GeneLength'], dtype=int) И теперь проверим размерности объектов: print(counts.shape)print(gene_lengths.shape) (20500, 375) (20500,) Как и ожидалось, они теперь полностью совпадают! 19 / 27\n--- Страница 46 ---\n46  Элегантный NumPy: фундамент научного программирования на Python нормализация Реальные данные содержат самые разные виды артефактов измерений. Преж - де чем выполнять какой-либо вид анализа данных, очень важно их рассмот - реть и определить, является ли обоснованной какая-либо нормализация. Например, результаты измерения температуры цифровыми термометрами могут систематически отличаться от показаний ртутных термометров, чита-емых человеком. Следовательно, сравнение образцов часто требует выполне-ния своего рода преобразования данных, благодаря которому каждый резуль-тат измерений будет приведен к общей шкале. В нашем случае мы хотим удостовериться, что любые обнаруженные нами различия соответствуют реальным биологическим различиям и не относятся к техническому артефакту. Мы рассмотрим два уровня нормализации, кото-рые часто совместно применяются к набору данных экспрессии генов: нор-мализацию между образцами (столбцами) и нормализацию между генами (строками). Нормализация между образцами Например, количества для каждого индивидуума в экспериментах по РНК-сек могут существенно варьироваться. Давайте взглянем на распределение количеств экспрессии по всем генам. Сначала мы просуммируем столбцы и в результате получим суммы количеств экспрессии всех генов по каждому индивидууму, чтобы мы смогли просто рассмотреть вариацию между инди-видуумами. Для визуализации распределения сумм количеств мы будем ис - пользовать метод ядерной оценки плотности (KDE), широко применяемый для сглаживания гистограмм, потому что этот метод дает более четкую картину лежащего в основе распределения. Прежде чем начать, выполним небольшую настройку графика (которую мы будем делать в каждой главе). Смотрите ниже заметку «Короткое примечание по построению графиков» для получения подробной информации относитель-но каждой строки приведенного далее фрагмента кода. # Заставить все графики в блокноте Jupyter # в дальнейшем появляться локально %matplotlib inline # Применить к графикам собственный стилевой файл import matplotlib.pyplot as plt plt.style.use('style/elegant.mplstyle') Краткое замечание по поводу построения графиков Приведенный выше фрагмент кода делает несколько изящных трюков, которые при- дают нашим графикам более симпатичный вид. 20 / 27\n--- Страница 47 ---\nНормализация  47 Во-первых, строка %matplotlib inline представляет собой волшебную команду блокно- та Jupyter1. Эта команда отображает все графики не во всплывающем окне, а в блокноте. Если блокнот Jupyter работает в интерактивном режиме, то вместо команды %matplotlib inline вы можете применить команду %matplotlib notebook. В результате вместо ста- тического изображения вы получите интерактивное изображение каждого графика. Во-вторых, мы импортируем модуль matplotlib.pyplot и затем сообщаем ему, чтобы он использовал наш собственный стиль создаваемых графиков plt.style.use('style/ elegant.mplstyle'). Аналогичный блок программного кода вы будете встречать в каждой главе перед первым графиком. Вы, возможно, обратили внимание, что иногда разработчики импортируют сущест - вующие стили типа plt.style.use('ggplot'). Однако нам бы хотелось применить не- сколько конкретных параметров настройки. Желательно, чтобы все графики в этой кни- ге соблюдали один и тот же стиль. Поэтому нами был собран наш собственный стиль Matplotlib. Чтобы увидеть, как мы это сделали, взгляните на файл таблицы стилей в хра-нилище книги «Элегантный SciPy»: style/elegant.mplstyle. Для получения дополнительной информации о стилях обратитесь к документации Matplotlib по таблицам стилей 2. Теперь вернемся к построению графика распределения количеств! total_counts = np.sum(counts, axis=0) # просуммировать столбцы # (axis=1 будет суммировать строки) from scipy import stats # Применить гауссово сглаживание для оценки плотности density = stats.kde.gaussian_kde(total_counts) # Создать значения, для которых оценить плотность, с целью построения графика x = np.arange(min(total_counts), max(total_counts), 10000) # Создать график плотностиfig, ax = plt.subplots()ax.plot(x, density(x))ax.set_xlabel(\"Суммы количеств на индивидуум\") ax.set_ylabel(\"Плотность\") plt.show() print(f'Количественная статистика:\\n минимум: {np.min(total_counts)}' f'\\n среднее: {np.mean(total_counts)}' f'\\n максимум: {np.max(total_counts)}') Количественная статистика: минимум: 6231205 среднее: 52995255.33866667 максимум: 103219262 Мы видим, что суммы количеств экспрессии между самым низким и самым высоким индивидуумами разнятся на порядок (рис. 1.6). Это означает, что для 1 См. http://ipython.org/ipython-doc/dev/interactive/tutorial.html#magics-explained. 2 См. https://matplotlib.org/users/style_sheets.html. 21 / 27\n--- Страница 48 ---\n48  Элегантный NumPy: фундамент научного программирования на Python каждого индивидуума было сгенерировано разное число прочтений РНК-сек. Мы говорим, что эти индивидуумы имеют разные размеры библиотеки прочтений. Рис. 1.6  График плотности количеств экспрессии генов в расчете на индивидуум на основе сглаживания по методу ядерной оценки плотности (KDE) Нормализация размера библиотеки между образцами Давайте взглянем поближе на диапазоны экспрессии генов для каждого инди- видуума. При применении нормализации мы сможем увидеть ее в действии. Чтобы не слишком загрязнять результирующий график, извлечем случайную выборку, состоящую всего из 70 столбцов. # Извлечь выборку для построения графика np.random.seed(seed=7) # Задать начальное значение случайного числа, # чтобы получить устойчивые результаты# Случайно отобрать 70 образцов samples_index = np.random.choice(range(counts.shape[1]), size=70, replace=False) counts_subset = counts[:, samples_index] # Индивидуальная настройка меток оси Х, чтобы легче было читать графики def reduce_xaxis_labels(ax, factor): «»»Показать только каждую i-ю метку для предотвращения скапливания на оси Х, например factor = 2 будет наносить каждую вторую метку оси Х, начиная с первой. Параметры --------- ax : ось графика matplotlib, подлежашая корректировке factor : int, коэффициент уменьшения числа меток оси Х «»» plt.setp(ax.xaxis.get_ticklabels(), visible=False) for label in ax.xaxis.get_ticklabels()[factor-1::factor]: label.set_visible(True) # Коробчатая диаграмма количеств экспрессии на индивидуум 22 / 27\n--- Страница 49 ---\nНормализация  49 fig, ax = plt.subplots(figsize=(4.8, 2.4)) with plt.style.context('style/thinner.mplstyle'): ax.boxplot(counts_subset) ax.set_xlabel(\"Индивидуумы\") ax.set_ylabel(\"Количества экспрессии генов\") reduce_xaxis_labels(ax, 5) Совершенно очевидно, что в верхнем конце шкалы экспрессии довольно много выбросов. Имеется большая вариация между индивидуумами, но их трудно заметить, так как все данные кластеризованы вокруг нуля (рис. 1.7). Поэтому давайте переведем наши данные в логарифмическую шкалу log(n + 1), чтобы их было легче рассмотреть (рис. 1.8). Чтобы выполнить функцию log с шагом n + 1, следует использовать операцию транслирования, упрощающую программный код и ускоряющую обработку. # Коробчатая диаграмма количеств экспрессии генов на индивидуум fig, ax = plt.subplots(figsize=(4.8, 2.4)) with plt.style.context('style/thinner.mplstyle'): ax.boxplot(np.log(counts_subset + 1)) ax.set_xlabel(\"Индивидуумы\") ax.set_ylabel(\"Лог-количества экспрессии генов\") # логарифмические кол-ва reduce_xaxis_labels(ax, 5) Рис. 1.7  Коробчатая диаграмма количеств экспрессии генов на индивидуум Рис. 1.8  Коробчатая диаграмма количеств экспрессии генов на индивидуум (логарифмическая шкала) 23 / 27\n--- Страница 50 ---\n50  Элегантный NumPy: фундамент научного программирования на Python Теперь давайте посмотрим, что происходит, когда мы выполняем нормали- зацию по размеру библиотеки (рис. 1.9). # Нормализовать по размеру библиотеки # Разделить количества экспрессии на суммы количеств# для конкретного индивидуума# Умножить на 1 миллион, чтобы вернуться к аналогичной шкалеcounts_lib_norm = counts / total_counts * 1000000# Обратите внимание, как здесь мы применили трансляцию дважды!counts_subset_lib_norm = counts_lib_norm[:,samples_index] # Коробчатая диаграмма количеств экспрессии на индивидуум fig, ax = plt.subplots(figsize=(4.8, 2.4)) with plt.style.context('style/thinner.mplstyle'): ax.boxplot(np.log(counts_subset_lib_norm + 1)) ax.set_xlabel(\"Индивидуумы\") ax.set_ylabel(\"Лог-количества экспрессии генов\") reduce_xaxis_labels(ax, 5) Рис. 1.9  Коробчатая диаграмма количеств экспрессии генов на индивидуум, нормализованных по библиотеке (логарифмическая шкала) Теперь график выглядит намного лучше! Также обратите внимание: мы дважды применили трансляцию. Один раз, чтобы разделить все количества экспрессии генов на сумму для этого столбца, и потом еще раз, чтобы умно-жить все значения на 1 миллион. Теперь давайте сравним наши нормализованные данные с необработанны- ми данными. import itertools as it from collections import defaultdict def class_boxplot(data, classes, colors=None, **kwargs): “””Создать коробчатую диаграмму, в которой коробки расцвечены, согласно классу, к которому они принадлежат. Параметры --------- data : массивоподобный список вещественных значений 24 / 27\n--- Страница 51 ---\nНормализация  51 Входные данные. Один коробчатый график будет сгенерирован для каждого элемента в `data`. classes : список строковых значений той же длины, что и `data` Класс, к которому принадлежит каждое распределение в `data`. Другие параметры ---------------- kwargs : словарь Именованные аргументы для передачи в `plt.boxplot`. “”” all_classes = sorted(set(classes)) colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] class2color = dict(zip(all_classes, it.cycle(colors))) # Отобразить классы на векторы данных # другие классы получают пустой список в этой позиции для смещения class2data = defaultdict(list) for distrib, cls in zip(data, classes): for c in all_classes: class2data[c].append([]) class2data[cls][-1] = distrib # Затем по очереди построить каждый коробчатый график # с соответствующим цветом fig, ax = plt.subplots() lines = [] for cls in all_classes: # задать цвет для всех элементов коробчатого графика for key in ['boxprops', 'whiskerprops', 'flierprops']: kwargs.setdefault(key, {}).update(color=class2color[cls]) # нарисовать коробчатый график box = ax.boxplot(class2data[cls], **kwargs) lines.append(box['whiskers'][0]) ax.legend(lines, all_classes) return ax Теперь можно построить цветную коробчатую диаграмму, противопоставив нормализованные и ненормализованные образцы. Для иллюстрации мы по- казываем только три образца из каждого класса: log_counts_3 = list(np.log(counts.T[:3] + 1)) log_ncounts_3 = list(np.log(counts_lib_norm.T [:3] + 1)) ax = class_boxplot(log_counts_3 + log_ncounts_3, ['сырые количества'] * 3 + ['нормализовано по размеру библиотеки'] * 3, labels=[1, 2, 3, 1, 2, 3]) ax.set_xlabel('номер образца') ax.set_ylabel('лог-количества экспрессии генов'); Заметьте, когда мы учитываем размер библиотеки (сумму этих распреде- лений), нормализованные распределения становятся чуть более похожими (рис. 1.10). Теперь между образцами мы сопоставляем подобное с подобным! Но как быть с различиями между генами? 25 / 27\n--- Страница 52 ---\n52  Элегантный NumPy: фундамент научного программирования на Python Рис. 1.10  Сравнение необработанных и нормализованных по размеру библиотеки количеств экспрессии генов в трех образцах (логарифмическая шкала) Нормализация между генами Мы также можем получить неприятности, пытаясь сравнивать разные гены. Количества по конкретному гену связаны с длиной гена. Предположим, что ген B в два раза длиннее гена A. Оба экспрессируются в образце на аналогичных уровнях (т. е. оба производят аналогичное число молекул мРНК). Напомним, что в эксперименте РНК-сек мы подразделяем транскрипты на фрагменты и выборочно отбираем прочтения из пула фрагментов. Поэтому, если ген бу - дет вдвое длиннее, он произведет вдвое больше фрагментов, и мы с удвоен-ной вероятностью отберем именно его. Следовательно, мы ожидаем, что ген B будет иметь вдвое больше количеств, чем ген A (рис. 1.11). Если мы хотим сравнивать уровни экспрессии разных генов, то нам следует выполнить еще одну нормализацию. Ген B, 4 прочтенияГен A, 14 прочтений Рис. 1.11  Связь между количествами и длиной генов Давайте посмотрим, сработает ли в нашем наборе данных связь между ко- личествами и длиной генов. Сначала мы определим служебную функцию для построения графика: 26 / 27\n--- Страница 53 ---\nНормализация  53 def binned_boxplot(x, y, *, # относится только к Python 3! (*см. совет ниже) xlabel='длина гена (логарифмическая шкала)', ylabel='средние логарифмические количества'): «»»Построить график распределения `y` независимо от `x`, используя большое число коробчатых графиков. Примечание: ожидается, что все входные данные приведены в логарифмическую шкалу. Параметры --------- x: Одномерный массив вещественных значений Значения независимых переменных. y: Одномерный массив вещественных значений Значения зависимых переменных. «»» # Определить интервалы для `x` в зависимости от плотности # результатов наблюдений x_hist, x_bins = np.histogram(x, bins='auto') # Применить `np.digitize` для нумерации интервалов # Отбросить последний край интервала, так как он нарушает допущение # метода `digitize` об открытости справа. Максимальный результат наблюдения # правильно попадает в последний интервал. x_bin_idxs = np.digitize(x, x_bins[:-1]) # Применить эти индексы для создания списка массивов, где каждый содержит # значения`y`, соответствующие значениям `x` в последнем интервале. # Этот формат входных данных ожидается на входе в `plt.boxplot` binned_y = [y[x_bin_idxs == i] for i in range(np.max(x_bin_idxs))] fig, ax = plt.subplots(figsize=(4.8,1)) # Создать метки оси Х, используя центры интервалов x_bin_centers = (x_bins[1:] + x_bins[:-1]) / 2 x_ticklabels = np. round(np.exp(x_bin_centers)).astype(int) # Создать коробчатую диаграмму ax.boxplot(binned_y, labels=x_ticklabels) # Показать только каждую 10-ю метку, чтобы # предотвратить скапливание на оси Х reduce_xaxis_labels(ax, 10) # Скорректировать имена осей ax.set_xlabel(xlabel) ax.set_ylabel(ylabel); Теперь мы вычислим длины генов и количества экспрессии: log_counts = np.log(counts_lib_norm + 1)mean_log_counts = np.mean(log_counts, axis=1) # по всем образцамlog_gene_lengths = np.log(gene_lengths) with plt.style.context('style/thinner.mplstyle'): binned_boxplot(x=log_gene_lengths, y=mean_log_counts) Powered by TCPDF (www.tcpdf.org) 27 / 27\n--- Страница 54 ---\n54  Элегантный NumPy: фундамент научного программирования на Python Совет по Python 3: использование символа * для создания только именованных аргументов Начиная с версии 3.0 Python допускает использование «только именованных аргументов»1. Это такие аргументы, которые необходимо вызывать лишь с использова- нием ключевого слова, не полагаясь только на позицию. Например, только что написан- ную функцию binned_boxplot можно вызвать следующим образом: >>> binned_boxplot(x, y, xlabel='my x label', ylabel='my y label') но не так, как показано ниже. Это было бы допустимо в Python 2, но вызовет ошибку в Python 3: >>> binned_boxplot(x, y, 'my x label', 'my y label') ---------------------------------------------------------------------------TypeError Traceback (most recent call last) <ipython-input-58-7a118d2d5750in <module>() 1 x_vals = [1, 2, 3, 4, 5] 2 y_vals = [1, 2, 3, 4, 5] ----3 binned_boxplot(x, y, 'my x label', 'my y label') TypeError: binned_boxplot() takes 2 positional arguments but 4 were given Идея состоит в том, чтобы оградить вас от чего-то, похожего на это: binned_boxplot(x, y, 'моя метка') Данная команда приведет к тому, что ваша метка y будет на оси Х, т. е. вызовет распро- страненную ошибку для сигнатур со многими необязательными параметрами, которые не имеют очевидного порядка следования. На приведенном ниже изображении мы видим, чем длиннее ген, тем выше его измеренные количества! Как отмечалось ранее, это артефакт метода, а не биологический сигнал! Как это объяснить? Нормализация по образцам и генам: RPKM Одним из самых простых методов нормализации, применяемых для данных РНК-сек, является метод определения показателя RPKM, т. е. определение ко-личества прочтений на тысячу оснований экзона на миллион картированных 1 См. https://www.python.org/dev/peps/pep-3102/. 1 / 27\n--- Страница 55 ---\nНормализация  55 прочтений (reads per kilobase transcript per million reads)1. Показатель RPKM со- единяет идеи нормализации по образцу и по гену. Когда мы вычисляем по- казатель RPKM, мы нормализуем размер библиотеки (сумму каждого столбца) и длину гена. Чтобы разобраться в том, каким образом рассчитывается показатель RPKM, давайте определим следующие ниже величины: C = количества прочтений, картированных на ген; L = длина экзона в парах оснований для гена; N = суммы количеств картированных прочтений в эксперименте. Давайте сначала вычислим прочтения на килобазу, или на тысячу основа- ний экзона. Прочтения в расчете на основание выполняются следующим образом: Эта формула запрашивает прочтения в расчете на тысячу оснований вместо прочтений в расчете на основание. Одна килобаза = 1000 оснований, поэтому нам нужно разделить длину (L) на 1000. Прочтения в расчете на тысячу оснований экзона вычисляются следующим образом: Далее нам нужно выполнить нормализацию по размеру библиотеки. Если просто поделить на количество картированных прочтений, мы получим: Но биологи предпочитают размышлять в миллионах прочтений, с тем чтобы числа не становились слишком большими. Рассчитав на миллион прочтений, мы получим: Таким образом, формула вычисления прочтений на тысячу оснований экзо- на на миллион картированных прочтений будет такой: 1 Показатель RPKM получают при помощи нормирования в пределах образца, кото- рый удаляет эффекты длины гена и размера библиотеки. – Прим. перев. 2 / 27\n--- Страница 56 ---\n56  Элегантный NumPy: фундамент научного программирования на Python Теперь давайте реализуем вычисление показателя RPKM по всему массиву количеств. # Создать переменные в соответствии с формулой RPKM, чтобы легче было сравнивать C = countsN = counts.sum(axis=0) # просуммировать каждый столбец, чтобы получить суммы # прочтений на образецL = gene_lengths # длины для каждого гена, совпадающего со строками в `C` Сначала мы умножаем на 10^9. Поскольку переменная C является массивом ndarray, мы можем применить операцию транслирования. Если умножить мас - сив ndarray на одиночное значение, то это значение будет транслировано по всему массиву. # Умножить все количества на 10^9 C_tmp = 10^9 * C Далее нам нужно разделить на длину гена. Процесс транслирования оди- ночного значения по всему двумерному массиву был довольно понятным. Мы просто умножали каждый элемент в массиве на заданное значение. Но что происходит, когда нам нужно разделить двумерный массив на одномерный? Правила транслирования Операция транслирования позволяет выполнять вычисления между массива-ми ndarray. Как говорилось ранее, у этих массивов разные формы. Для того чтобы сделать эти манипуляции немного легче, в библиотеке Numpy использу - ются правила транслирования. Когда два массива имеют одинаковые размер-ности, операция транслирования может выполняться, если размеры каждой размерности совпадают либо одна из них равна 1. Если массивы имеют разные размерности, то в начало более короткого массива добавляется (1,) до тех пор, пока размерности не будут совпадать, и затем применяются стандартные пра-вила транслирования. Например, предположим, что у нас два массива ndarray, A и B, с формами (5,2) и (2,). Мы определяем произведение A * B, используя транслирование. Раз- мерность массива В меньше, чем массива A. Поэтому во время вычисления новая размерность добавляется в начало B со значением 1, в результате чего новая форма массива B становится (1,2). Наконец, там, где форма массива B не совпадает с формой массива А, она умножается путем накапливания достаточ- ного количества версий массива B, давая в итоге форму (5,2). Это действие вы-полняется «виртуально», не расходуя дополнительную оперативную память. В этой точке произведение представляет собой поэлементное умножение, да-вая выходной массив той же самой формы, что и у массива A. Теперь предположим, у нас есть еще один массив, C, с формой (2,5). Чтобы умножить C на B (или их сложить), мы можем попытаться добавить (1, ) в на-чало формы массива B, но в этом случае мы по-прежнему в итоге получим не-совместимые формы: (2,5) и (1,2). Чтобы выполнить транслирование массивов, мы должны вручную добавить размерность в конец массива B. Тогда мы в итоге 3 / 27\n--- Страница 57 ---\nНормализация  57 получим формы (2,5) и (2,1), в результате чего может быть осуществлена опе- рация транслирования. В NumPy можно явным образом добавить новую размерность в массив B, применив np.newaxis . Давайте посмотрим, как это делается в нашей нормали- зации на основе показателя RPKM. Сначала взглянем на размеры наших массивов. print('C_tmp.shape', C_tmp.shape) print('L.shape', L.shape) C_tmp.shape (20500, 375) L.shape (20500,) Мы видим, что массив C_tmp имеет две размерности, тогда как L имеет одну. Поэтому во время транслирования в начало массива L будет добавлена еще одна размерность. В результате мы получим: C_tmp.shape (20500, 375)L.shape (1, 20500) Размеры не будут совпадать! Мы же хотим транслировать массив L на пер- вую размерность массива C_tmp , поэтому нам следует скорректировать размер- ности массива L самостоятельно. L = L[:, np.newaxis] # добавить размерность в L со значением 1print('C_tmp.shape', C_tmp.shape)print('L.shape', L.shape) C_tmp.shape (20500, 375) L.shape (20500, 1) Теперь, когда наши размерности совпадают или равны единице, мы можем выполнить транслирование. # Разделить каждую строку на длину гена для этого гена (L)C_tmp = C_tmp / L Наконец, мы должны выполнить нормализацию по размеру библиотеки, т. е. по сумме количеств для этого столбца. Напомним, что мы уже вычислили N при помощи: N = counts.sum(axis=0) # просуммировать каждый столбец, чтобы получить суммы # количеств прочтений на образец # Проверить формы массивов C_tmp и N print('C_tmp.shape', C_tmp.shape)print('N.shape', N.shape) C_tmp.shape (20500, 375) N.shape (375,) При запуске трансляции в начало массива N будет добавлена дополнитель- ная размерность: N.shape (1, 375) 4 / 27\n--- Страница 58 ---\n58  Элегантный NumPy: фундамент научного программирования на Python Размерности будут совпадать, и поэтому нам ничего делать не нужно. Тем не менее для удобочитаемости полезно добавить в N дополнительную размерность. # Добавить в N дополнительную размерность N = N[np.newaxis, :]print('C_tmp.shape', C_tmp.shape) print('N.shape', N.shape) C_tmp.shape (20500, 375) N.shape (1, 375) # Разделить каждый столбец на суммы количеств прочтений для этого столбца (N) rpkm_counts = C_tmp / N Давайте поместим этот программный код в функцию, чтобы его можно было использовать повторно. def rpkm(counts, lengths): «»»Вычислить прочтения на тысячу оснований экзона на миллион картированных прочтений. RPKM = (10^9 * C) / (N * L) где: C = количества прочтений, картированных на ген N = суммы количеств картированных (выровненных) прочтений в эксперименте L = длина экзона в парах оснований для гена Параметры --------- counts: массив, форма (N_genes, N_samples) РНК-сек (или подобные) количественные данные, где столбцы являются отдельными образцами и строки являются генами. lengths: массив, форма (N_genes,) Длины генов в парах оснований в том же порядке, что и строки в counts. Возвращает ---------- normed: массив, форма (N_genes, N_samples) Матрица counts, нормализованная согласно RPKM. «»» N = np.sum(counts, axis=0) # просуммировать каждый столбец, чтобы # получить суммы количеств прочтений на образец L = lengths C = counts normed = 1e9 * C / (N[np.newaxis, :] * L[:, np.newaxis]) return(normed)counts_rpkm = rpkm(counts, gene_lengths) RPKM между нормализацией генов Давайте посмотрим на влияние нормализации RPKM в действии. Сначала, в качестве напоминания, посмотрите распределение средних логарифмиче-ских количеств как функции длины генов (см. рис. 1.12): 5 / 27\n--- Страница 59 ---\nНормализация  59 log_counts = np.log(counts + 1) mean_log_counts = np.mean(log_counts, axis=1)log_gene_lengths = np.log(gene_lengths) with plt.style.context('style/thinner.mplstyle'): binned_boxplot(x=log_gene_lengths, y=mean_log_counts) Срд. лог-количества Длина гена (логарифмическая шкала) Рис. 1.12  Связь между длиной гена и средней экспрессией до нормализации RPKM (логарифмическая шкала) Теперь тот же самый график с нормализованными по RPKM значениями: log_counts = np.log(counts_rpkm + 1) mean_log_counts = np.mean(log_counts, axis=1)log_gene_lengths = np.log(gene_lengths) with plt.style.context('style/thinner.mplstyle'): binned_boxplot(x=log_gene_lengths, y=mean_log_counts) Срд. лог-количества Длина гена (логарифмическая шкала) Рис. 1.13  Связь между длиной гена и средней экспрессией после нормализации RPKM (логарифмическая шкала) Вы видите, что средние количества экспрессии значительно сгладились, в особенности для генов длиной более 3000 пар оснований. (Гены меньшей длины, похоже, по-прежнему имеют низкую экспрессию – она может быть слишком малой для статистической мощности метода RPKM.) Нормализация RPKM может быть полезной для сравнения профиля экспрес - сии разных генов. Мы уже видели, что более длинные гены имеют на графике более высокие количества, но это не означает, что уровень их экспрессии на самом деле выше. Давайте выберем короткий ген и длинный ген, сравним их количества до и после нормализации RPKM и покажем, что мы имеем в виду. gene_idxs = np.array([80, 186]) gene1, gene2 = gene_names[gene_idxs] 6 / 27\n--- Страница 60 ---\n60  Элегантный NumPy: фундамент научного программирования на Python len1, len2 = gene_lengths[gene_idxs] gene_labels = [f'{gene1}, {len1}bp', f'{gene2}, {len2}bp'] log_counts = list(np.log(counts[gene_idxs] + 1)) log_ncounts = list(np.log(counts_rpkm[gene_idxs] + 1)) ax = class_boxplot(log_counts, ['сырые количества'] * 3, labels=gene_labels)ax.set_xlabel('Гены')ax.set_ylabel('лог-количества экспрессии генов по всем образцам'); Если мы посмотрим только на необработанные количества, то, по-видимому, более длинный ген, TXNDC5, экспрессируется немного больше, чем более ко- роткий, RPL24 (рис. 1.14). Однако после нормализации RPKM проявляется дру - гая картина: ax = class_boxplot(log_ncounts, ['RPKM-нормализовано'] * 3, labels=gene_labels)ax.set_xlabel('Гены')ax.set_ylabel('лог-количества экспрессии генов после RPKM'); Лог-количества экспрессии генов ГеныНеобработанные количества Рис. 1.14  Сравнение экспрессии двух генов до нормализации RPKM Теперь похоже, что RPL24 фактически экспрессируется на гораздо более вы- соком уровне, чем TXNDC5 (см. рис. 1.15). Это вызвано тем, что RPKM содержит нормализацию длины гена. Теперь мы можем непосредственно выполнить сравнение между генами разной длины. 7 / 27\n--- Страница 61 ---\nПодведение итогов  61 RPKM-нормализовано ГеныRPKM-нормализованные лог-количества экспрессии генов Рис. 1.15  Сравнение экспрессии двух генов после нормализации RPKM поДВеДение итогоВ На данный момент мы сделали следующее: импортировали данные, используя библиотеку pandas; познакомились с ключевым классом объектов NumPy – массивом ndarray; применили силу операции трансляции, которая сделала наши вычисле- ния элегантнее. В главе 2 мы продолжим работать с тем же самым набором данных, вы- полнив реализацию более сложного метода нормализации, затем применим кластеризацию, чтобы сделать предсказания относительно смертности среди больных раком кожи. 8 / 27",
      "debug": {
        "start_page": 32,
        "end_page": 61
      }
    },
    {
      "name": "Глава 2. Квантильная нормализация с NumPy и SciPy 62",
      "content": "--- Страница 62 --- (продолжение)\nГлава 2 Квантильная нормализация с NumPy и SciPy Не печальтесь, если вы не сможете сразу по- стичь более глубокие тайны Трехмерия. Посте-пенно они откроются перед вами. – Эдвин Э. Эбботт. «Флатландия: роман о четвертом измерении» В этой главе мы продолжим анализировать данные экспрессии генов из главы 1, но немного с другой целью: мы хотим использовать профиль экспрессии генов каждого пациента (полный вектор замеров экспрессии его генов) для предска-зания ожидаемой выживаемости. Чтобы использовать полные профили, нам нужна более глубокая нормализация, чем та, которую обеспечивает показатель RPKM главы 1. Вместо применения показателя RPKM мы выполним квантиль-ную нормализацию 1, т. е. прием, обеспечивающий укладку замеров в опреде- ленное распределение. В этом методе принимается большое допущение: если данные не распределены согласно желаемой форме, то мы просто заставляем их укладываться в метод! Это немного похоже на обман, но на деле такой под-ход оказывается простым и полезным в случаях, где конкретное распределе-ние не имеет значения, и важность представляют относительные изменения значений внутри популяции. Например, Болстад (Bolstad) и его коллеги про-демонстрировали 2, что данный метод показывает превосходные результаты в восстановлении известных уровней экспрессии в данных ДНК-микрочипов. По ходу главы мы воспроизведем упрощенную версию рис. 5A и 5B3 из ра- боты4 «Геномная классификация кожной меланомы» проекта «Атлас ракового генома» (TCGA). 1 См. https://en.wikipedia.org/wiki/Quantile_normalization. 2 См. https://academic.oup.com/bioinformatics/article/19/2/185/372664. 3 См. http://www.cell.com/action/showImagesData?pii=S0092-8674(15)00634-0. 4 См. http://www.cell.com/cell/fulltext/S0092-8674(15)00634-0?_returnURL=http://linkinghub. elsevier.com/retrieve/pii/S0092867415006340%3Fshowall%3Dtrue. 9 / 27\nГлава 2 Квантильная нормализация с NumPy и SciPy Не печальтесь, если вы не сможете сразу по- стичь более глубокие тайны Трехмерия. Посте-пенно они откроются перед вами. – Эдвин Э. Эбботт. «Флатландия: роман о четвертом измерении» В этой главе мы продолжим анализировать данные экспрессии генов из главы 1, но немного с другой целью: мы хотим использовать профиль экспрессии генов каждого пациента (полный вектор замеров экспрессии его генов) для предска-зания ожидаемой выживаемости. Чтобы использовать полные профили, нам нужна более глубокая нормализация, чем та, которую обеспечивает показатель RPKM главы 1. Вместо применения показателя RPKM мы выполним квантиль-ную нормализацию 1, т. е. прием, обеспечивающий укладку замеров в опреде- ленное распределение. В этом методе принимается большое допущение: если данные не распределены согласно желаемой форме, то мы просто заставляем их укладываться в метод! Это немного похоже на обман, но на деле такой под-ход оказывается простым и полезным в случаях, где конкретное распределе-ние не имеет значения, и важность представляют относительные изменения значений внутри популяции. Например, Болстад (Bolstad) и его коллеги про-демонстрировали 2, что данный метод показывает превосходные результаты в восстановлении известных уровней экспрессии в данных ДНК-микрочипов. По ходу главы мы воспроизведем упрощенную версию рис. 5A и 5B3 из ра- боты4 «Геномная классификация кожной меланомы» проекта «Атлас ракового генома» (TCGA). 1 См. https://en.wikipedia.org/wiki/Quantile_normalization. 2 См. https://academic.oup.com/bioinformatics/article/19/2/185/372664. 3 См. http://www.cell.com/action/showImagesData?pii=S0092-8674(15)00634-0. 4 См. http://www.cell.com/cell/fulltext/S0092-8674(15)00634-0?_returnURL=http://linkinghub. elsevier.com/retrieve/pii/S0092867415006340%3Fshowall%3Dtrue. 9 / 27\n--- Страница 63 ---\nКвантильная нормализация с NumPy и SciPy  63 Наша реализация квантильной нормализации эффективно использует воз- можности библиотек NumPy и SciPy, производя быструю, эффективную и эле- гантную функцию. Квантильная нормализация включает в себя три шага: 1) отсортировать значения по каждому столбцу; 2) найти среднее каждой результирующей строки; 3) заменить квантиль каждого столбца на квантиль среднего столбца. import numpy as np from scipy import stats def quantile_norm(X): «»»Нормализовать столбцы X, чтобы каждый имел одинаковое распределение. При заданной матрице экспрессии (данных микрочипов, количеств прочтений и пр.), состоящей из M генов на N образцов, квантильная нормализация обеспечивает, что все образцы будут иметь одинаковый разброс данных (в силу своей конструкции). Данные по каждой строке усредняются, чтобы получить среднее значение столбца. Каждый квантиль столбца заменяется соответствующим квантилем среднего столбца. Параметры --------- X : двумерный массив вещественных, форма (M, N) Входные данные с M строками (гены/признаки) и N столбцами (образцы). Возвращает ---------- Xn : двумерный массив вещественных, форма (M, N) Нормализованные данные. «»» # Вычислить квантили quantiles = np.mean(np.sort(X, axis=0), axis=1) # Вычислить ранги по столбцам. Каждое наблюдение заменяется на его ранг # в этом столбце: наименьшее наблюдение заменяется на 1, следующее # наименьшее на 2, , и наибольшее на M, т. е. на количество строк. ranks = np.apply_along_axis(stats.rankdata, 0, X) # Преобразовать ранги в целочисленные индексы от 0 до M-1 rank_indices = ranks.astype(int) - 1 # Проиндексировать квантили для каждого ранга ранговой матрицей Xn = quantiles[rank_indices] return( Xn) По причине характера вариабельности, присутствующей в количественных данных экспрессии генов, общепринято перед квантильной нормализацией ло- гарифмически преобразовывать данные. Поэтому мы напишем дополнитель-ную вспомогательную функцию, которая будет выполнять это преобразование: def quantile_norm_log(X): logX = np.log(X + 1) 10 / 27\n--- Страница 64 ---\n64  Квантильная нормализация с NumPy и SciPy logXn = quantile_norm(logX) return logXn Обе эти функции иллюстрируют многое из того, что превращает библиотеку NumPy в мощный инструмент (первые три шага вам известны из главы 1): массивы могут быть одномерными, подобно спискам, но они также мо- гут быть двумерными, как матрицы, и даже более многомерными. Этот прием дает возможность представлять многообразные виды числовых данных. В нашем случае мы представляем двумерную матрицу; массивы позволяют одновременно выражать многие числовые опера-ции. В первой строке функции quantile_norm_log мы одним вызовом до- бавляем единицу и берем логарифм по каждому значению в X. Такая операция называется векторизацией; операции с массивами выполняются вдоль осей. В первой строке функ - ции quantile_norm мы сортируем данные вдоль каждого столбца, задав в методе сортировки np.sort параметр оси axis. Затем мы берем среднее значение вдоль каждой строки, указав другую ось axis; в основе научной экосистемы Python лежат массивы. Функция scipy. stats.rankdata оперирует не списками Python, а массивами NumPy. Это же относится ко многим научным библиотекам в Python; даже те функции, которые не имеют ключевого слова axis= , можно за- ставить оперировать вдоль осей путем применения функции NumPy apply_along_axis ; массивы поддерживают многочисленные виды манипуляций с данны-ми посредством чудесной индексации: Xn = quantiles[ranks] . Она, возмож - но, представляет собой самую хитроумную часть библиотеки NumPy, которая к тому же является одной из самых полезных. Мы займемся ее подроб ным исследованием в последующих разделах. получение Данных Как и в главе 1, мы будем работать с набором данных РНК-сек рака кожи из проекта TCGA. Наша цель состоит в том, чтобы предсказать смертность среди больных раком кожи, используя данные этого проекта об экспрессии РНК. Как отмечалось ранее, к концу этой главы мы воспроизведем упрощенную версию рис. 5A и 5B работы «Геномная классификация кожной меланомы» консор-циума TCGA. Как и в главе 1, мы сначала воспользуемся библиотекой pandas, чтобы на- много упростить работу по считыванию данных в оперативную память. Пре-жде всего мы прочитаем количественные данные как таблицу pandas. import numpy as np import pandas as pd # Импортировать данные TCGA о меланоме filename = 'data/counts.txt' 11 / 27\n--- Страница 65 ---\nРазница в распределении экспрессии генов между индивидуумами  65 data_table = pd.read_csv(filename, index_col=0) # Выполнить разбор файла print(data_table.iloc[:5, :5]) 00624286-41dd-476f-a63b-d2a5f484bb45 TCGA-FS-A1Z0 TCGA-D9-A3Z1 \\ A1BG 1272.36 452.96 288.06A1CF 0.00 0.00 0.00A2BP1 0.00 0.00 0.00A2LD1 164.38 552.43 201.83A2ML1 27.00 0.00 0.00 02c76d24-f1d2-4029-95b4-8be3bda8fdbe TCGA-EB-A51B A1BG 400.11 420.46 Глядя на строки и столбцы таблицы data_table , мы видим, что столбцы явля- ются образцами, а строки – генами. Теперь давайте поместим наши количества в массив NumPy. # Двумерный массив, содержащий количества экспрессии по каждому гену # в каждом индивидуумеcounts = data_table.values разница В распреДелении экспрессии геноВ меж Ду инДиВиД уумами Теперь давайте получим представление о наших количественных данных, построив график распределения количеств по каждому индивидууму. Чтобы сгладить неровности в наших данных, мы будем использовать гауссово ядро. Тем самым мы получим более оптимальное представление об общей форме данных. Сначала, как обычно, мы настроем стиль построения графиков: # Заставить графики появляться локально, задать стиль графиков %matplotlib inlineimport matplotlib.pyplot as pltplt.style.use('style/elegant.mplstyle') Затем напишем функцию построения графика, применяющую функцию биб лиотеки SciPy gaussian_kde для построения сглаженных распределений: from scipy import stats def plot_col_density(data): «»»По каждому столбцу произвести график плотности по всем строкам.»»» # Применить гауссово сглаживание с целью получения оценки плотности density_per_col = [stats.gaussian_kde(col) for col in data.T] x = np.linspace(np.min(data), np.max(data), 100) fig, ax = plt.subplots() for density in density_per_col: ax.plot(x, density(x)) ax.set_xlabel('Значения данных (в расчете на столбец)') ax.set_ylabel('Плотность') 12 / 27\n--- Страница 66 ---\n66  Квантильная нормализация с NumPy и SciPy Теперь мы можем применить эту функцию для построения графика распре- деления необработанных данных, т. е. до выполнения какой-либо нормализа- ции: # До нормализации log_counts = np.log(counts + 1)plot_col_density(log_counts) Рис. 2.1  График распределения до квантильной нормализации Мы видим, что, в то время как распределения количеств в основном по- хожи, некоторые индивидуумы имеют более плоские распределения, а неко- торые смещены влево. На самом деле если учесть, что мы имеем логариф-мическую шкалу, то положение пика распределений фактически варьируется более чем на порядок! Далее в этой главе, во время выполнения анализа ко-личественных данных, мы будем исходить из того, что изменения в экспрес - сии генов происходят из-за биологических различий между образцами. Но главный сдвиг в распределении, который мы наблюдаем, говорит о том, что эти различия технические. Иными словами, наличие изменений, скорее всего, вызвано различиями в обработке каждого образца, а не биологической ва-риацией. Поэтому мы попытаемся нормализовать эти глобальные различия между индивидуумами. Чтобы выполнить эту нормализацию, мы задействуем квантильную норма- лизацию, как было описано в начале главы. Идея состоит в том, что все наши образцы должны иметь похожее распределение, поэтому любые различия в форме, скорее всего, происходят из-за некой технической вариации. Если выражаться более формально, то при заданной матрице экспрессии (данных микрочипов, количеств прочтений и пр.) с формой (n_genes, n_samples) кван- тильная нормализация гарантирует одинаковый разброс данных образцов (столбцов) в силу своей конструкции. 13 / 27\n--- Страница 67 ---\nРазница в распределении экспрессии генов между индивидуумами  67 В NumPy и SciPy квантильная нормализация делается легко и эффективно. Ниже приведена наша реализация квантильной нормализации, которую мы ввели в начале данной главы. Давайте предположим, что мы прочитали входную матрицу как X: import numpy as np from scipy import stats def quantile_norm(X): «»»Нормализовать столбцы X, чтобы каждый имел одинаковое распределение. При заданной матрице экспрессии (данных микрочипов, количеств прочтений и пр.), состоящей из M генов на N образцов, квантильная нормализация обеспечивает, что все образцы будут иметь одинаковый разброс данных (в силу своей конструкции). Данные по каждой строке усредняются, чтобы получить среднее значение столбца. Каждый квантиль столбца заменяется соответствующим квантилем среднего столбца. Параметры --------- X : двумерный массив вещественных, форма (M, N) Входные данные с M строками (гены/признаки) и N столбцами (образцы). Возвращает ---------- Xn : двумерный массив вещественных, форма (M, N) Нормализованные данные. «»» # Вычислить квантили quantiles = np.mean(np.sort(X, axis=0), axis=1) # Вычислить ранги по столбцам. Каждое наблюдение заменяется на его ранг # в этом столбце: наименьшее наблюдение заменяется на 1, следующее # наименьшее на 2, , и наибольшее на M, т. е. количество строк. ranks = np.apply_along_axis(stats.rankdata, 0, X) # Преобразовать ранги в целочисленные индексы от 0 до M-1 rank_indices = ranks.astype(int) - 1 # Проиндексировать квантили для каждого ранга ранговой матрицей Xn = quantiles[rank_indices] return(Xn)def quantile_norm_log( X): logX = np.log(X + 1) logXn = quantile_norm(logX) return logXn Посмотрим, как будут выглядеть наши распределения после квантильной нормализации: # После нормализации log_counts_normalized = quantile_norm_log(counts) plot_col_density(log_counts_normalized) 14 / 27\n--- Страница 68 ---\n68  Квантильная нормализация с NumPy и SciPy Рис. 2.2  График распределения после квантильной нормализации Как и следовало ожидать, теперь распределения выглядят фактически иден- тичными! (Различающиеся левые хвосты распределения связаны с разным числом совпадений для значений с низкими количествами – 0, 1, 2, … – в раз-ных столбцах данных.) Далее, когда мы нормализовали количества, начнем использовать данные экспрессии генов для предсказания дальнейшего течения болезни у пациентов. бикластеризация количестВенных Данных Кластеризация образцов сообщает, какие образцы имеют похожие профили экспрессии генов. Это может свидетельствовать о похожих характеристиках образцов в других шкалах. После нормализации данных мы можем сгруппи-ровать гены (строки) и образцы (столбцы) матрицы экспрессии. Кластериза-ция строк говорит о том, какие значения экспрессии генов связаны между со-бой. Это является признаком, что в изучаемом процессе они работают вместе. Под бикластеризацией подразумевается одновременное группирование строк и столбцов данных. В результате кластеризации вдоль строк мы обнаружива-ем, какие гены взаимодействуют, и в итоге кластеризации вдоль столбцов бу - дет показана схожесть образцов. Поскольку операция кластеризации может быть дорогостоящей, мы ограни- чим наш анализ наиболее изменчивыми генами в количестве 1500, поскольку они будут объяснять большую часть сигнала корреляции в любой размерности. def most_variable_rows(data, *, n=1500): «»»Извлечь подмножество n наиболее изменчивых строк В данном случае нам нужны n наиболее изменчивых генов. Параметры --------- 15 / 27\n--- Страница 69 ---\nБикластеризация количественных данных  69 data : двумерный массив вещественных Данные, из которых будет извлечено подмножество n : целое, необязательный Количество возвращаемых строк. Возвращает ---------- variable_data : двумерный массив вещественных `n` строк данных `data`, которые проявляют наибольшую дисперсию. «»» # Вычислить дисперсию вдоль оси столбцов rowvar = np.var(data, axis=1) # Получить отсортированные индексы (в порядке возрастания), взять последние n sort_indices = np.argsort(rowvar)[-n:] # Использовать в качестве индекса для данных variable_data = data[sort_indices, :] return variable_data Далее нам нужна функция для выполнения бикластеризации данных. Обыч- но для этого используется сложный алгоритм кластеризации из библиотеки scikit-learn1. В нашем случае для простоты и удобства отображения результатов мы применим иерархическую кластеризацию. Как оказалось, библиотека SciPy располагает вполне приличным модулем иерархической кластеризации, хотя потребуется немного времени, чтобы разобраться в его интерфейсе. Напомним, иерархическая кластеризация – это метод группирования наблю- дений с использованием последовательного слияния кластеров. Первоначаль-но каждое наблюдение принадлежит своему собственному кластеру. Затем два ближайших кластера неоднократно объединяются в один, далее приходит оче-редь следующих двух кластеров, и т. д. до тех пор, пока каждое наблюдение не будет находиться в единственном кластере. Эта последовательность операций слияния формирует дерево слияния. Подрезая дерево на определенной высоте, можно получить более мелкозернистую или более крупнозернистую кластери-зацию наблюдений. Функция связи linkage в модуле scipy.cluster.hierarchy выполняет иерархи- ческую кластеризацию строк матрицы, используя конкретный метрический показатель (например, евклидово расстояние, манхэттенское расстояние или другие) и конкретный метод связи, т. е. расстояние между двумя кластерами (например, среднее расстояние между всеми наблюдениями в паре кластеров). Функция возвращает дерево слияния в виде «матрицы связей», содержащей каждую операцию слияния вместе с вычисленным для слияния расстоянием и количеством наблюдений в результирующем кластере. Из документации по функции linkage : Кластер с индексом меньше n соответствует одному из n исходных наблюдений. Расстояние между кластерами Z[i, 0] и Z[i, 1] дает Z[i, 2]. Четвертое значение Z[i, 3] представляет количество исходных наблюдений во вновь сформирован- ном кластере. 1 См. http://scikit-learn.org/. 16 / 27\n--- Страница 70 ---\n70  Квантильная нормализация с NumPy и SciPy Целый кладезь информации! Однако приступим к делу. Надеемся, что вы быстро освоитесь. Сначала мы определим функцию bicluster , кластеризирую- щую строки и столбцы матрицы: from scipy.cluster.hierarchy import linkage def bicluster(data, linkage_method='average', distance_metric='correlation'): “””Кластеризовать строки и столбцы матрицы. Параметры --------- data : двумерный массив ndarray Входные данные, подлежащие бикластеризации. linkage_method : строковый, необязательный Метод связи, передаваемый в `linkage`. distance_metric : строковый, необязательный Метрический показатель расстояния, применяемый для кластеризации. См. документацию по ``scipy.spatial.distance.pdist`` относительно допустимых метрических показателей расстояния Возвращает ---------- y_rows : матрица связей Кластеризация строк входных данных. y_cols : матрица связей Кластеризация столбцов входных данных. «»» y_rows = linkage(data, method=linkage_method, metric=distance_metric) y_cols = linkage(data.T, method=linkage_method, metric=distance_metric) return y_rows, y_cols Все просто: мы вызываем функцию linkage для входной матрицы и для транспонированной входной матрицы, в которой столбцы становятся строка- ми, а строки – столбцами. Визуализация кластероВ Далее определим функцию, визуализирующую результат кластеризации. Мы перестроим строки и столбцы входных данных так, чтобы похожие строки были вместе. Таким же способом выполним перестроение столбцов, чтобы по-хожие столбцы были вместе. Дополнительно к этому мы собираемся показать дерево слияния для строк и для столбцов, демонстрирующее, какие наблюде-ния подходят друг другу. Деревья слияния представляются в виде дендограмм, в которых длины ответвлений говорят, насколько наблюдения похожи друг на друга (чем короче дендограмма, тем больше сходства). Заметьте, здесь используется много жестко запрограммированных пара- метров. Этого при построении графиков трудно избежать, так как дизайн гра-фиков нередко требует визуального осмотра с целью поиска правильных про-порций. 17 / 27\n--- Страница 71 ---\nВизуализация кластеров  71 from scipy.cluster.hierarchy import dendrogram, leaves_list def clear_spines(axes): for loc in ['left', 'right', 'top', 'bottom']: axes.spines[loc].set_visible(False) axes.set_xticks([]) axes.set_yticks([]) def plot_bicluster(data, row_linkage, col_linkage, row_nclusters=10, col_nclusters=3): “””Выполнить бикластеризацию, пострить тепловую карту с дендограммами на каждой оси. Параметры --------- data : массив вещественных, форма (M, N) Входные данные, подлежащие бикластеризации. row_linkage : массив, форма (M-1, 4) Матрица связей для строк данных `data`. col_linkage : массив, форма (N-1, 4) Матрица связей для столбцов данных `data`. n_clusters_r, n_clusters_c : целое, необязательный Количество кластеров для строк и столбцов. «»» fig = plt.figure(figsize=(4.8, 4.8)) # Вычислить и построить дендограмму по строкам # `add_axes` принимает «прямоугольный» вход и добавляет подграфик # в изображение. # Принимается, что изображение имеет длину стороны 1 на каждой стороне, # и его левый нижний угол находится в (0, 0). # В `add_axes` передаются измерения со значениями: левая сторона, # нижняя сторона, ширина и высота подграфика. Таким образом, чтобы # построить левую дендограмму (для строк), мы создаем прямоугольник, чей # левый нижний угол находится в (0.09, 0.1) с шириной 0.2 и высотой 0.6. ax1 = fig.add_axes([0.09, 0.1, 0.2, 0.6]) # Для заданного количества кластеров мы можем получить отрезок дерева # связей, обратившись к соответствующей аннотации расстояния в # матрице связи. threshold_r = (row_linkage[-row_nclusters, 2] + row_linkage[-row_nclusters+1, 2]) / 2 with plt.rc_context({'lines.linewidth': 0.75}): dendrogram(row_linkage, orientation='left', color_threshold=threshold_r, ax=ax1) clear_spines(ax1) # Вычислить и построить дендограмму по столбцам # См. примечания выше, где дается объяснение параметров для `add_axes` ax2 = fig.add_axes([0.3, 0.71, 0.6, 0.2]) threshold_c = (col_linkage[-col_nclusters, 2] + col_linkage[-col_nclusters+1, 2]) / 2 with plt.rc_context({'lines.linewidth': 0.75}): dendrogram(col_linkage, color_threshold=threshold_c, ax=ax2) 18 / 27\n--- Страница 72 ---\n72  Квантильная нормализация с NumPy и SciPy clear_spines(ax2) # Построить тепловую карту данных ax = fig.add_axes([0.3, 0.1, 0.6, 0.6]) # Отсортировать данные в соответствии с листьями дендограммы idx_rows = leaves_list(row_linkage) data = data[idx_rows, :] idx_cols = leaves_list(col_linkage) data = data[:, idx_cols] im = ax.imshow(data, aspect='auto', origin='lower', cmap='YlGnBu_r') clear_spines(ax) # Метки осей ax.set_xlabel('Образцы') ax.set_ylabel('Гены', labelpad=125) # Нанести пояснительную надпись axcolor = fig.add_axes([0.91, 0.1, 0.02, 0.6]) plt.colorbar(im, cax=axcolor) # Показать график plt.show() Теперь мы применим эти функции к нормализованной матрице количеств, чтобы показать кластеризации по строкам и по столбцам (рис. 2.3). counts_log = np.log(counts + 1)counts_var = most_variable_rows(counts_log, n=1500) yr, yc = bicluster(counts_var, linkage_method='ward', distance_metric='euclidean')with plt.style.context('style/thinner.mplstyle'): plot_bicluster(counts_var, yr, yc) преДсказание ВыжиВаемости Мы видим, что данные образцов естественным образом попадают как мини- мум в два или три кластера. В чем смысл этих кластеров? Чтобы ответить на этот вопрос, мы можем обратиться к данным пациентов, доступным в хра-нилище данных, прилагаемых к исследовательской работе. После небольшой предварительной обработки мы получим таблицу пациентов, содержащую ин-формацию о жизни каждого пациента. Затем ее можно сопоставить с класте-рами количеств и понять, сможет ли экспрессия генов пациентов предсказать различия в их патологии. patients = pd.read_csv('data/patients.csv', index_col=0) patients.head() 19 / 27\n--- Страница 73 ---\nПредсказание выживаемости  73 Гены Образцы Рис. 2.3  Эта тепловая карта показывает уровень экспрессии генов по всем образцам и генам. Цвет говорит об уровне экспрессии. Строки и столбцы сгруппированы в соответствии с кластерами. Мы видим кластеры генов вдоль оси Y и кластеры образцов поверх оси X Таблица 2.1. Таблица пациентов Сигнатура в УФ-области спектраИсходные кластерыВремя жизни с меланомойСмерть от меланомы TCGA-BF-A1PU УФ-сигнатура кератин NaN NaN TCGA-BF-A1PV УФ-сигнатура кератин 13.0 0.0 TCGA-BF-A1PX УФ-сигнатура кератин NaN NaN TCGA-BF-A1PZ УФ-сигнатура кератин NaN NaN TCGA-BF-A1Q0 не УФ резистентный 17.0 0.0 По каждому пациенту (строкам) мы имеем: Сигнатура в ультрафиолетовой области спектра Ультрафиолетовый свет имеет тенденцию вызывать определенные мутации ДНК. Отыскивая эту сигнатуру мутации, исследователи могут заключить, смог ли ультрафиолетовый свет вызвать мутацию (мутации), приведшую к появлению рака у этих пациентов. 20 / 27\n--- Страница 74 ---\n74  Квантильная нормализация с NumPy и SciPy Исходный кластер В исследовательской работе пациенты были кластеризованы на основе дан- ных экспрессии генов. Эти кластеры были классифицированы в соответ - ствии с типами генов, типичными для этого кластера. Главными кластера-ми были «резистентный» (immune) (n = 168; 51%), «кератин» (n = 102; 31%) и «MITF-low» (n = 59; 18%) 1. Время жизни с меланомой Количество дней, которые пациент прожил. Смерть от меланомы Один (1), если пациент умер от меланомы, ноль (0), если они жив или умер от чего-то другого. Теперь для каждой группы пациентов, определенной в результате класте- ризации, нам нужно начертить кривые выживания. Этот график показывает доли популяции, остающейся живой в течение некоторого времени. Обратите внимание, некоторые данные цензуированы справа. Это означает, что в неко- торых случаях мы фактически не знаем дату смерти пациента, либо причины смерти пациента не связаны с меланомой. Мы рассматриваем этих пациентов как «живых» на всем протяжении кривой выживаемости, однако более тща-тельные исследования могут попытаться оценить их вероятное время смерти. Чтобы получить кривую выживания из времен жизни, мы создадим ступен- чатую функцию, уменьшающуюся на 1/n, где n – это количество пациентов в группе. Затем мы сопоставим эту функцию относительно нецензуированных времен жизни. def survival_distribution_function(lifetimes, right_censored=None): «»»Вернуть функцию распределения выживаемости из набора времен жизни. Параметры --------- lifetimes : массив вещественных либо целых Наблюдавшиеся времена жизни популяции. Они должны быть неотрицательными. right_censored : массив булевых, такой же формы, что и `lifetimes` Здесь значение `True` говорит о том, что это время жизни не наблюдалось. Значения `np.nan` в `lifetimes` также рассматриваются как цензуированные справа. Возвращает ---------- sorted_lifetimes : массив вещественных Отсортированные времена жизни sdf : array of float Значения, начинающиеся с 1 и прогрессивно уменьшающиеся, на один уровень для каждого наблюдения в `lifetimes`. 1 MITF – транскрипционный фактор, ассоциированный с микрофтальмией. – Прим. перев. 21 / 27\n--- Страница 75 ---\nПредсказание выживаемости  75 Примеры ------- В этом примере в популяции из четырех человек двое умирают во время 1, третий умерает во время 2, и последний умирает в неизвестное время. (Отсюда ``np.nan``.) >>> lifetimes = np.array([2, 1, 1, np.nan]) >>> survival_distribution_function(lifetimes) (array([ 0., 1., 1., 2.]), array([ 1. , 0.75, 0.5 , 0.25])) “”” n_obs = len(lifetimes) rc = np.isnan(lifetimes) if right_censored is not None: rc |= right_censored observed = lifetimes[~rc] xs = np.concatenate( ([0], np.sort(observed)) ) ys = np.linspace(1, 0, n_obs + 1) ys = ys[:len(xs)] return xs, ys Теперь, когда можно легко получить кривые выживания из данных о жизни пациентов, мы сможем вывести их на график. Мы напишем функцию, которая группирует времена жизни по идентификатору (identity) кластера и выводит каждую группу на графике как отдельную прямую: def plot_cluster_survival_curves(clusters, sample_names, patients, censor=True): “””Вывести на график данные о жизни из набора кластеров образцов. Параметры --------- clusters : массив целых либо серия pd.Series категориальных значений Идентификатор кластера каждого образца, кодированный как простое целое либо как категориальная переменная pandas. sample_names : list of string Имя, соответствующее каждому образцу. Должно иметь такую же длину, что и массив `clusters`. patients : pandas.DataFrame Фрейм данных DataFrame, содержащий информацию о жизни каждого пациента. Индексы этого объекта DataFrame должны соответствовать именам `sample_names`. Образцы, не представленные в этом списке, будут проигнорированы. censor : булев, необязательный Если `True`, то использовать `patients[‘melanoma-dead’]`, чтобы цензуировать данные о жизни справа. “”” fig, ax = plt.subplots() if type(clusters) == np.ndarray: cluster_ids = np.unique(clusters) cluster_names = ['cluster {}'.format(i) for i in cluster_ids] elif type(clusters) == pd.Series: cluster_ids = clusters.cat.categories cluster_names = list(cluster_ids) 22 / 27\n--- Страница 76 ---\n76  Квантильная нормализация с NumPy и SciPy n_clusters = len(cluster_ids) for c in cluster_ids: clust_samples = np.flatnonzero(clusters == c) # Отбросить пациентов, которые не представлены в данных о жизни clust_samples = [sample_names[i] for i in clust_samples if sample_names[i] in patients.index] patient_cluster = patients.loc[clust_samples] survival_times = patient_cluster['melanoma-survival-time'].values if censor: censored = ~patient_cluster['melanoma-dead'].values.astype(bool) else: censored = None stimes, sfracs = survival_distribution_function(survival_times, censored) ax.plot(stimes / 365, sfracs) ax.set_xlabel('время жизни (годы)') ax.set_ylabel('доля живых') ax.legend(cluster_names) Теперь можно применить функцию fcluster , чтобы получить идентифи- каторы кластеров для образцов (столбцы данных о количествах) и вывести на график каждую кривую выживания по отдельности. Функция fcluster принимает матрицу связей, возвращаемую функцией linkage, и порог и воз- вращает идентификаторы кластеров. Очень трудно узнать априори, каким должен быть порог, но мы можем получить соответствующий порог для фик - сированного количества кластеров в результате проверки расстояний в мат - рице связей. from scipy.cluster.hierarchy import fcluster n_clusters = 3threshold_distance = (yc[-n_clusters, 2] + yc[-n_clusters+1, 2]) / 2clusters = fcluster(yc, threshold_distance, 'distance') plot_cluster_survival_curves(clusters, data_table.columns, patients) Как показано на рис. 2.4, кластеризация профилей экспрессии генов, по всей видимости, идентифицировала высокорисковый подтип меланомы (группа 2). Исследовательская работа TCGA поддерживает это утверждение более робастной кластеризацией и проверкой статистических гипотез. Это совсем недавнее исследование показывает полученный здесь результат, а также другие результаты, в которых идентифицируются такие подтипы рака, как лейкемия (рак крови), рак пищеварительного тракта и другие. Вы-шеупомянутый метод кластеризации не надежен. Вместе с тем имеются дру - гие, более надежные способы исследования этого и подобных ему наборов данных 1. 1 См. сеть «Атласа генома рака» // Геномная классификация кожной меланомы. Cell 161, no. 7 (2015): 1681–1696 (http://dx.doi.org/10.1016/j.cell.2015.05.044). 23 / 27\n--- Страница 77 ---\nПредсказание выживаемости  77 Фракция выживших Продолжительность жизниКластер 1 Кластер 2 Кластер 3 Рис. 2.4  Кривые выживания для пациентов, кластеризованных с использованием данных экспрессии генов Дальнейшая работа: использование кластеров пациентов TCGA Предсказывают ли наши кластеры выживание более эффективно, чем исход- ные кластеры, приведенные в исследовательской работе? А что относительно УФ-сигнатуры? Постройте график кривых выживания с использованием ис - ходных кластеров и столбцов с УФ-сигнатурами данных пациентов. Как они соотносятся с нашими кластерами? Дальнейшая работа: воспроизведение кластеров TCGA В качестве упражнения мы оставляем вам реализацию подхода, описанного в исследовательской работе 1. 1. Возьмите бутстраповские выборки генов (случайный отбор с возвра-том), которые используются для кластеризации образцов. 2. По каждому образцу произведите иерархическую кластеризацию. 3. При бутстраповской кластеризации в матрице формы (n_samples, n_ samples) сохраните частотность (количество) одновременного появления пары образцов. 4. На результирующей матрице выполните иерархическую кластеризацию. Это позволит идентифицировать группы образцов, которые часто появля- ются в кластерах вместе, независимо от выбранных генов. В результате можно будет считать, что эти образцы надежно кластеризуются вместе.  Подсказка. Для создания бутстраповских выборок индексов строк следует применить функцию np.random.choice с аргументом replacement=True . 1 См. сеть «Атласа генома рака» // Геномная классификация кожной меланомы. Cell 161, no. 7 (2015): 1681–1696 (http://dx.doi.org/10.1016/j.cell.2015.05.044). 24 / 27",
      "debug": {
        "start_page": 62,
        "end_page": 77
      }
    },
    {
      "name": "Глава 3. Создание сетей из областей изображений при помощи ndimage 78",
      "content": "--- Страница 78 --- (продолжение)\nГлава 3 Создание сетей из областей изображений при помощи ndimage Тигр, о тигр, светло горящий В глубине полночной чащи, Кем задуман огневой Симметричный образ твой? – Уильям Блэйк. «Тигр» Скорее всего, вам известно, что цифровые изображения состоят из пикселов. В целом их следует представлять не как крошечные квадраты, а как точечные образцы светового сигнала, вычисленные на регулярной сетке 1. Более того, во время обработки изображений мы часто имеем дело с объ- ектами, которые намного крупнее отдельных пикселов. Пейзаж, небо, земля, деревья и скалы – все эти объекты состоят из большого количества пикселов. Для их представления используется универсальная структура под названием «граф смежности областей», или RAG (region adjacency graph, лоскутный граф). Его узлы содержат свойства каждой области в изображении, а связи – простран- ственные отношения между областями. Во входном изображении два узла связаны между собой, когда их соответствующие области соприкасаются друг с другом. Создание такой структуры может представлять сложную задачу, и эта зада- ча становится еще сложнее, когда изображения не двумерные, а трехмерные и даже четырехмерные, что, помимо прочего, широко распространено в мик - роскопии, материаловедении и климатологии. Тем не менее здесь мы вам по-кажем метод генерирования RAG-графа всего в нескольких строках программ- 1 См. Смит А. Р. Пиксель – это не крошечный квадрат: техническая записка. 17 июля 1995 г. (Alvy Ray Smith. A Pixel Is Not A Little Square // http://alvyray.com/Memos/CG/ Microsoft/6_pixel.pdf). 25 / 27\nГлава 3 Создание сетей из областей изображений при помощи ndimage Тигр, о тигр, светло горящий В глубине полночной чащи, Кем задуман огневой Симметричный образ твой? – Уильям Блэйк. «Тигр» Скорее всего, вам известно, что цифровые изображения состоят из пикселов. В целом их следует представлять не как крошечные квадраты, а как точечные образцы светового сигнала, вычисленные на регулярной сетке 1. Более того, во время обработки изображений мы часто имеем дело с объ- ектами, которые намного крупнее отдельных пикселов. Пейзаж, небо, земля, деревья и скалы – все эти объекты состоят из большого количества пикселов. Для их представления используется универсальная структура под названием «граф смежности областей», или RAG (region adjacency graph, лоскутный граф). Его узлы содержат свойства каждой области в изображении, а связи – простран- ственные отношения между областями. Во входном изображении два узла связаны между собой, когда их соответствующие области соприкасаются друг с другом. Создание такой структуры может представлять сложную задачу, и эта зада- ча становится еще сложнее, когда изображения не двумерные, а трехмерные и даже четырехмерные, что, помимо прочего, широко распространено в мик - роскопии, материаловедении и климатологии. Тем не менее здесь мы вам по-кажем метод генерирования RAG-графа всего в нескольких строках программ- 1 См. Смит А. Р. Пиксель – это не крошечный квадрат: техническая записка. 17 июля 1995 г. (Alvy Ray Smith. A Pixel Is Not A Little Square // http://alvyray.com/Memos/CG/ Microsoft/6_pixel.pdf). 25 / 27\n--- Страница 79 ---\nИзображения – это просто массивы NumPy  79 ного кода с использованием библиотеки NetworkX (библиотеки Python для анализа графов и сетей) и фильтра из подмодуля SciPy ndimage для обработки N-мерных изображений. import networkx as nx import numpy as np from scipy import ndimage as ndi def add_edge_filter(values, graph): center = values[len(values) // 2] for neighbor in values: if neighbor != center and not graph.has_edge(center, neighbor): graph.add_edge(center, neighbor) return 0.0 def build_rag(labels, image): g = nx.Graph() footprint = ndi.generate_binary_structure(labels.ndim, connectivity=1) _ = ndi.generic_filter(labels, add_edge_filter, footprint=footprint, mode='nearest', extra_arguments=(g,)) return g Истоки книги «Элегантный SciPy» (Примечание от Хуана Нуньес-Иглесиаса) Эта глава требует особого упоминания, так как она вдохновила авторов на написа- ние всей книги. Винеш Биркодар (Vighnesh Birodkar) написал данный фрагмент кода в 2014 г., когда, будучи студентом Университета, участвовал в инициативной программе компании Google, известной как Google Summer of Code (GSoC). Когда я увидел этот про-граммный код, он поразил мое воображение. Если говорить о целях настоящей книги, этот код затрагивает многие аспекты научного программирования на Python. Когда вы закончите читать данную главу, вы научитесь обрабатывать массивы любой размерности и прекратите о них думать только как об одномерных списках или двумерных таблицах. Более того, вы поймете основы фильтрации изображений и обработки сетей. Здесь будет происходить следующее: мы представим изображения как мас - сивы NumPy, фильтрацию этих изображений выполним с использованием мо- дуля scipy.ndimage . А для превращения областей изображений в граф (сеть) ис - пользуем библиотеку NetworkX. изображения – это просто массиВы numPy В предыдущей главе было показано, что массивы NumPy могут эффективно представлять табличные данные и являются удобным инструментом выпол-нения вычислений. Как оказалось, массивы одинаково эффективно способны представлять изображения. Ниже показано, как можно создать изображение белого шума, используя только библиотеку NumPy, и с помощью библиотеки Matplotlib вывести его на 26 / 27\n--- Страница 80 ---\n80  Создание сетей из областей изображений при помощи ndimage экран. Мы сначала импортируем необходимые пакеты и применим волшеб- ную команду IPython matplotlib inline , чтобы наши изображения появлялись внизу программного кода: # Заставить графики появляться локально, задать индивидуальный стиль графиков %matplotlib inlineimport matplotlib.pyplot as pltplt.style.use('style/elegant.mplstyle') Затем мы «добавим немного шума» и выведем его как изображение: import numpy as nprandom_image = np.random.rand(500, 500)plt.imshow(random_image); Функция imshow показывает массив NumPy как изображение: Также справедливо и обратное: изображение может рассмотриваться как массив NumPy. Для этого примера мы используем библиотеку scikit-image, представляющую собой коллекцию инструментов обработки изображений, надстроенных поверх библиотек NumPy и SciPy. Вот изображение в формате PNG из хранилища scikit-image. Это черно-белая (иногда именуемая «полутоновой») фотография нескольких древних римских монет из Помпеи, снятая в Бруклинском музее: Powered by TCPDF (www.tcpdf.org) 27 / 27\n--- Страница 81 ---\nИзображения – это просто массивы NumPy  81 Ниже показано, как загрузить изображение монет при помощи библиотеки scikit-image: from skimage import io url_coins = ('https://raw.githubusercontent.com/scikit-image/scikit-image/' 'v0.10.1/skimage/data/coins.png')coins = io.imread(url_coins)print(\"Тип:\", type(coins), \"Форма:\", coins.shape, \"Тип данных:\", coins.dtype) plt.imshow(coins); Тип: <class 'numpy.ndarray'> Форма: (303, 384) Тип данных: uint8 Полутоновое изображение может быть представлено как двумерный массив, в котором каждый элемент массива в конкретной позиции содержит полутоно- вую интенсивность. Одним словом, изображение – это всего лишь массив NumPy. 1 / 27\n--- Страница 82 ---\n82  Создание сетей из областей изображений при помощи ndimage Цветные изображения являются трехмерными массивами, где первые две размерности представляют пространственные позиции изображения, в то время как заключительная размерность представляет цветовые каналы, как правило, три основных аддитивных цвета: красный, зеленый и синий. Чтобы показать, что можно сделать с этими размерностями, давайте поэксперимен-тируем с приведенной ниже фотографией астронавта Айлин Коллинз (Eileen Collins): url_astronaut = ('https://raw.githubusercontent.com/scikit-image/scikit-image/' 'master/skimage/data/astronaut.png')astro = io.imread(url_astronaut)print(\"Тип:\", type(astro), \"Форма:\", astro.shape, \"Тип данных:\", astro.dtype) plt.imshow(astro); Тип: <class 'numpy.ndarray'> Форма: (512, 512, 3) Тип данных: uint8 Это изображение является простым массивом NumPy. Как только вы это пойме те, вы очень легко сможете в изображение добавить зеленый квадрат, используя простой срез массива NumPy: astro_sq = np.copy(astro) astro_sq[50:100, 50:100] = [0, 255, 0] # красный, зеленый, синийplt.imshow(astro_sq); 2 / 27\n--- Страница 83 ---\nИзображения – это просто массивы NumPy  83 Кроме того, вы можете применить булеву маску, т. е. массив, состоящий из значений True или False . Мы эти значения встречали в главе 2 как способ отбора строк таблицы. В данном случае для отбора пикселов можно применить массив такой же формы, что и изображение: astro_sq = np.copy(astro) sq_mask = np.zeros(astro.shape[:2], bool)sq_mask[50:100, 50:100] = Trueastro_sq[sq_mask] = [0, 255, 0]plt.imshow(astro_sq); 3 / 27\n--- Страница 84 ---\n84  Создание сетей из областей изображений при помощи ndimage Задача: добавление сеточного наложения Только что было показано, каким образом можно выбрать квадрат и окрасить его в зеленый цвет. Есть ли возможность ли расширить этот способ на другие формы и цвета? Давайте создадим функцию, рисующую синюю сетку на цвет - ном изображении, и применим ее к приведенной выше фотографии. Ваша функция должна содержать два параметра: входное изображение и интервал сетки. Мы предлагаем использовать шаблон, приведенный ниже. Этот шаблон поможет вам начать: def overlay_grid(image, spacing=128): «»»Вернуть изображение с сеточным наложением, используя предоставленный интервал. Параметры --------- image : массив, форма (M, N, 3) Входное изображение. spacing : целое Интервал между линиями сетки. Возвращает ---------- image_gridded : маасив, форма (M, N, 3) Исходное изображение с наложенной синей сеткой. «»» image_gridded = image.copy() pass # замените эту строку своим программным кодом return image_gridded# plt.imshow(overlay_grid(astro, 128)); # раскомментировать эту строку, чтобы # протестировать вашу функцию Обратите внимание: решение задачи «Добавление сеточного наложения» вы найдете в конце книги. фильтры В обработке сигналоВ Фильтрация – это одна из самых фундаментальных и распространенных опе- раций в обработке изображений. Вы можете отфильтровать изображение, что-бы удалить шум, усилить его свойства или обнаружить края между объектами в изображении. Чтобы понять, как работают фильтры, проще всего начать не с изображения, а с одномерного сигнала. Например, вы можете измерить свет, поступающий в ваш конец оптоволоконного кабеля. Если вы будете отбирать (сэмплиро- вать) сигнал каждую миллисекунду (мс) в течение 100 мс, то в итоге вы полу - чите массив длиной 100 элементов. Предположим, что на 30-й миллисекунде световой сигнал был включен, и через 30 миллисекунд снова выключен. В ре-зультате вы получите сигнал, как показано ниже: 4 / 27\n--- Страница 85 ---\nФильтры в обработке сигналов  85 sig = np.zeros(100, np.float) # sig[30:60] = 1 # сигнал = 1 в течение периода 30–60 мс, # так как свет наблюдаетсяfig, ax = plt.subplots()ax.plot(sig); ax.set_ylim(-0.1, 1.1); Чтобы найти точку, когда свет был включен, можно задержать сигнал на 1 мс, затем вычесть исходный сигнал из задержанного сигнала. Тем самым, когда между текущей миллисекундой и следующей сигнал остается без изме- нения, вычитание будет давать 0, но, когда сигнал увеличится, вы получите по- ложительный сигнал. Когда сигнал уменьшится, мы получим отрицательный сигнал. Если мы за- интересованы только в точном определении времени включения света, мы мо-жем обрезать разностный сигнал, чтобы любые отрицательные значения были преобразованы в 0: sigdelta = sig[1:] # sigdelta[0] равняется sig[1] и т.д. sigdiff = sigdelta - sig[:-1]sigon = np.clip(sigdiff, 0, np.inf) # сигнал включенfig, ax = plt.subplots()ax.plot(sigon) ax.set_ylim(-0.1, 1.1) print('Сигнал включен на:', 1 + np.flatnonzero(sigon)[0], 'мс') Сигнал включен на: 30 мс 5 / 27\n--- Страница 86 ---\n86  Создание сетей из областей изображений при помощи ndimage (Здесь мы применили функцию NumPy flatnonzero , чтобы получить первый индекс, где массив sigon не равен 0.) Оказывается, это может быть реализовано операцией обработки сигналов, именуемой конволюцией, или сверткой. В каждой точке сигнала мы вычисляем скалярное произведение между окружающими эту точку значениями и ядром, или фильтром, который представляет собой предопределенный вектор значе- ний. Затем в зависимости от ядра конволюция выявляет различные свойства сигнала. Теперь подумайте, что произойдет, когда для сигнала s ядро равняет - ся (1, 0, –1), т. е. представляет собой разностный фильтр. В любой позиции i результат конволюции равняется 1*s[i+1] + 0*s[i] – 1*s[i-1] , что сводится к s[i+1] – s[i–1] . Таким образом, когда смежные с s[i] значения идентичны, конволюция дает 0, но когда s[i+1] > s[i–1] (сигнал увеличивается), конволюция дает положительное значение, и наоборот, когда s[i+1] < s[i–1] , конволюция дает отрицательное значение. Это можно представить как вычисление произ-водной из входной функции. В целом формула конволюции имеет вид s′(t) = ∑ t i=1–τs(j)f(t – j), где s – это сиг - нал, s′ – отфильтрованный сигнал, f – фильтр и τ – длина фильтра. В библиотеке SciPy для работы с конволюцией можно использовать функ - цию scipy.ndimage.convolve : diff = np.array([1, 0, -1]) from scipy import ndimage as ndidsig = ndi.convolve(sig, diff)plt.plot(dsig); 6 / 27\n--- Страница 87 ---\nФильтры в обработке сигналов  87 Правда, приведенные выше сигналы обычно зашумлены и не идеальны: np.random.seed(0) sig = sig + np.random.normal(0, 0.3, size=sig.shape)plt.plot(sig); Простой разностный фильтр может этот шум усилить: plt.plot(ndi.convolve(sig, diff)); 7 / 27\n--- Страница 88 ---\n88  Создание сетей из областей изображений при помощи ndimage В таких случаях в фильтр можно добавить сглаживание. Наиболее распро- страненной формой сглаживания является гауссово сглаживание, которое бе- рет взвешенное среднее соседних точек в сигнале, используя гауссову функ - цию1. Мы можем написать функцию, создающую гауссово ядро сглаживания, которая будет выглядеть следующим образом: def gaussian_kernel(size, sigma): «»»Создать одномерное гауссово ядро заданного размера и стандартного отклонения. Размер должен иметь нечетное число и стандартное отклонение (сигма) как минимум в ~6 раз больше, чтобы обеспечить достаточное покрытие. “”” positions = np.arange(size) - size // 2 kernel_raw = np.exp(-positions**2 / (2 * sigma**2)) kernel_normalized = kernel_raw / np.sum(kernel_raw) return kernel_normalized Поистине замечательная особенность операции конволюции состоит в том, что она ассоциативна. То есть если вы хотите найти производную сглаженного сигнала, то в равной степени можете свернуть сигнал сглаженным разностным фильтром! Это может сэкономить уйму вычислительного времени, так как бу - дет сглаживаться только фильтр, размер которого обычно намного меньше данных. smooth_diff = ndi.convolve(gaussian_kernel(25, 3), diff) plt.plot(smooth_diff); 1 См. https://ru.wikipedia.org/wiki/Гауссова_функция. 8 / 27\n--- Страница 89 ---\nФильтры в обработке сигналов  89 Этот сглаженный разностный фильтр отыскивает край в центральной по- зиции и выполняет проверку на продолжение разницы. Это продолжение про- исходит не на «паразитных» краях, вызванных шумом, а в истинном крае. Об-ратите внимание на результат (рис. 3.1): sdsig = ndi.convolve(sig, smooth_diff) plt.plot(sdsig); Рис. 3.1  Сглаженный разностный фильтр, примененный к зашумленному сигналу Хотя он все еще выглядит неустойчивым, соотношение сигнал-шум (SNR) в этой версии намного больше, чем тогда, когда мы используем простой раз- ностный фильтр. 9 / 27\n--- Страница 90 ---\n90  Создание сетей из областей изображений при помощи ndimage Фильтрация. Эта операция называется фильтрацией, потому что в физических электри- ческих цепях большое число таких операций реализуются аппаратными средствами, по-зволяющими пропускать определенные виды тока, блокируя другие. Такие аппаратные компоненты называются фильтрами. Например, распространенный фильтр, удаляющий из тока высокочастотные колебания напряжения, называется фильтром нижних частот. фильтрация изображений (ДВумерные фильтры ) Теперь, когда вы увидели фильтрацию в одном измерении, мы надеемся, что вам удастся легко расширить эти понятия на двумерные сигналы, такие как изображения. Вот двумерный разностный фильтр для нахождения краев на изображении монет: coins = coins.astype(float) / 255 # предотвращает ошибки переполнения diff2d = np.array([[0, 1, 0], [1, 0, -1], [0, -1, 0]])coins_edges = ndi.convolve(coins, diff2d)io.imshow(coins_edges); Принцип тот же, что с одномерным фильтром: в каждой точке в изображе- нии поместить фильтр, вычислить скалярное произведение значений фильт - ра со значениями изображения и поместить результат в выходном изображе- нии в той же самой позиции. Подобно одномерному разностному фильтру, когда фильтр помещается в позицию с небольшой вариацией, а скалярное произведение сводится к нулю, при помещении фильтра в позицию с изме-няемой яркостью изображения значения, умноженные на 1, будут отличаться от значений, умноженных на –1. При этом отфильтрованный результат ста-нет положительной или отрицательной величиной (в зависимости от того, является ли изображение, находящееся справа внизу или слева верху от этой точки, ярче). 10 / 27\n--- Страница 91 ---\nФильтрация изображений (двумерные фильтры)  91 Точно так, как с одномерным фильтром, вы можете усложнить задачу и сгла- дить шум прямо в фильтре. Фильтр Собела предназначен как раз для этой цели. Данный фильтр существует как в горизонтальном, так и в вертикальном вари- анте и отыскивает края в одной из указанных в данных ориентаций. Давайте начнем с горизонтального фильтра. Чтобы на фотографии найти горизонталь-ный край, вы можете попробовать следующий ниже фильтр: # столбцовый (вертикальный) вектор для нахождения горизонтальных краев hdiff = np.array([[1], [0], [-1]]) Однако, как мы убедились на примере одномерных фильтров, это приво- дит к зашумленности оценки краев в изображении. Но вместо применения га- уссового сглаживания, вызывающего во многих случаях расплывчатые края, фильтр Собела использует свойство, демонстрирующее тенденцию непрерыв-ности краев. Например, фотография океана будет содержать горизонтальный край вдоль всей линии, а не только в отдельных точках изображения. Поэто-му фильтр Собела сглаживает вертикальный фильтр горизонтально: он ищет сильный край в центральной позиции, подкрепляемый смежными позициями: hsobel = np.array([[ 1, 2, 1], [ 0, 0, 0], [-1, -2, -1]]) Вертикальный фильтр Собела попросту является результатом транспониро- вания горизонтального: vsobel = hsobel.T В результате в изображении монет мы можем найти горизонтальные и вер- тикальные края: # Нанесение индивидуальных меток на оси Х, чтобы графики легче читалисьdef reduce_xaxis_labels(ax, factor): «»»Показывать только каждую i-ю метку, чтобы предотвратить скапливание на оси X, например factor = 2 будет выводить каждую вторую метку оси X, начиная с первой. Параметры --------- ax : ось графика matplotlib, подлежащая корректировке factor : целое, коэффициент сокращения количества меток оси X «»» plt.setp(ax.xaxis.get_ticklabels(), visible=False) for label in ax.xaxis.get_ticklabels()[::factor]: label.set_visible(True) coins_h = ndi.convolve(coins, hsobel) coins_v = ndi.convolve(coins, vsobel) fig, axes = plt.subplots(nrows=1, ncols=2) axes[0].imshow(coins_h, cmap=plt.cm.RdBu) axes[1].imshow(coins_v, cmap=plt.cm.RdBu) for ax in axes: reduce_xaxis_labels(ax, 2) 11 / 27\n--- Страница 92 ---\n92  Создание сетей из областей изображений при помощи ndimage И наконец, вы можете констатировать, что, как в теореме Пифагора, вели- чина края в любом направлении равна квадратному корню суммы квадратов горизонтальных и вертикальных составляющих: coins_sobel = np.sqrt(coins_h**2 + coins_v**2) plt.imshow(coins_sobel, cmap='viridis'); униВерсальные фильтры : произВольные функции от сосеДних значений В дополнение к скалярным произведениям, реализованным в функции ndi. convolve , библиотека SciPy позволяет определять фильтр, являющийся произ- вольной функцией точек в окрестности. Этот фильтр реализован в функции ndi. generic_filter , позволяющей выражать произвольно сложные фильтры. Предположим, что изображение представляет собой медианную стоимость домов в административном округе с пространственной разрешающей способ- ностью 100 м в продольном и поперечном направлениях (100×100 м). Местный 12 / 27\n--- Страница 93 ---\nУниверсальные фильтры: произвольные функции от соседних значений  93 совет принимает решение облагать налогом продажи домов на уровне $10 000 плюс 5% от 90-й процентили цен на дома в радиусе 1 километр. (Поэтому про-дажа дома в дорогом районе обходится дороже.) При помощи универсально-го (генерического) фильтра generic_filter мы можем создать карту налоговой ставки по всей географической карте округа: from skimage import morphology def tax(prices): return 10000 + 0.05 * np.percentile(prices, 90)house_price_map = (0.5 + np.random.rand(100, 100)) * 1e6footprint = morphology.disk(radius=10) tax_rate_map = ndi.generic_filter(house_price_map, tax, footprint=footprint) plt.imshow(tax_rate_map)plt.colorbar(); Задача: игра «“Жизнь ” Конуэя» Предложено Николасом Ругиром Игра «Жизнь» (англ. Conway’s Game of Life) – это на вид простая конструкция, в которой «клетки» на регулярной квадратной сетке живут или умирают в за-висимости от непосредственно окружающих их клеток. В каждый такт времени мы определяем состояние позиции (i, j) согласно его предыдущему состоянию и состояния его восьми соседей (выше, ниже, слева, справа и по диагоналям): живая клетка умирает, если она имеет всего одного живого соседа или ни одного; живая клетка продолжает жить в течение еще одного поколения, если она имеет двух или трех живых соседей; живая клетка умирает от перенаселенности, если она имеет четырех или более живых соседей; 13 / 27\n--- Страница 94 ---\n94  Создание сетей из областей изображений при помощи ndimage мертвая клетка оживает вследствие воспроизводства, если она имеет ровно трех живых соседей. Хотя эти правила смотрятся как надуманная математическая задача, они на самом деле дают начало невероятным шаблонам, начиная с планеров (небольших скоплений живых клеток, которые медленно перемещаются в каждое поколение) и орудий планеров (постоянных скоплений, которые вырастают из планеров), вплоть до генераторов простых чисел (например, почитайте статью «Генерирование последовательностей простых чисел в игре “«Жизнь» Конуэя”» Натаниеля Джонстона 1) и даже симулирование са- мой игры «Жизнь»2! Сможете ли вы реализовать игру «Жизнь» при помощи ndi.generic_filter ? Обратите внимание: решение задачи «Игра “«Жизнь» Конуэя”» находится в конце книги. Задача: магнитуда градиента Собела Мы недавно увидели, каким образом можно объединять результаты двух раз-ных фильтров: горизонтального фильтра Собела с вертикальным. Сможете ли вы написать функцию, которая делает это за один проход при помощи ndi. generic_filter ? Обратите внимание: решение задачи «Магнитуда градиента Собела» нахо- дится в конце книги. графы и библиотека network X Графы – это естественная форма описания, подходящая для удивительного разнообразия данных. Например, страницы в сети могут содержать узлы, тог - да как связи между этими страницами могут быть, ну, в общем, связями. Или же в биологии в так называемых транскрипционных сетях гены представлены вершинами. При этом гены, имеющие непосредственное влияние на экспрес - сию друг друга, соединены ребрами.  Графы и сети. В данном контексте термин «граф» синонимичен с термином «сеть» и не имеет никакого отношения к слову «график». Математики и специалисты в области ин-форматики используют слегка отличающиеся слова, которые относятся к данной теме: граф = сеть, вершина = узел, ребро = связь = дуга. Как и большинство людей, мы будем использовать эти термины взаимозаменяемо.Возможно, вам чуть больше знакома терминология сетей: сеть состоит из узлов и связей между узлами. Эквивалентно, граф состоит из вершин и ребер между вершинами. В биб- лиотеке NetworkX имеются графовые объекты Graph, состоящие из узлов node и ребер edge между узлами. Такое использование терминов, вероятно, является наиболее рас - пространенным. 1 См. http://www.njohnston.ca/2009/08/generating-sequences-of-primes-in-conways-game-of- life/. 2 См. https://youtu.be/xP5-iIeKXE8. 14 / 27\n--- Страница 95 ---\nГрафы и библиотека NetworkX  95 Чтобы познакомить вас с графами, мы воспроизведем несколько результа- тов из исследовательской работы Лэва Варшни (Lav Varshney) и др. «Структур- ные свойства нейронной сети свободно живущей нематоды»1. В этом примере мы представим нейроны в нервной системе червя нематоды в виде узлов и поместим ребро между двумя узлами, когда нейрон образует синапс с другим узлом. (Синапсы – это химические контакты, посредством ко-торых нейроны передают информацию.) Червь предоставляет потрясающий пример анализа структуры контактов между нейронами, потому что каждый червь (этого вида) имеет одинаковое количество нейронов (302), и все контак - ты между ними известны. Данный факт привел к созданию фантастического проекта Openworm 2, о котором мы рекомендуем почитать подробнее. Вы можете скачать нейронный набор данных в формате Excel из базы дан- ных WormAtlas3. Библиотека pandas позволяет читать таблицу Excel по сети, по- этому для прочтения данных мы воспользуемся этой библиотекой. Затем пере-дадим эти данные в NetworkX. import pandas as pd connectome_url = 'http://www.wormatlas.org/images/NeuronConnect.xls'conn = pd.read_excel(connectome_url) Переменная conn теперь содержит объект библиотеки pandas DataFrame со строками в следующем формате: [Нейрон1, Нейрон2, тип контакта, сила] Мы собираемся исследовать только коннектом4 химических синапсов, по- этому мы отфильтруем другие типы синапсов следующим образом: conn_edges = [(n1, n2, {'weight': s}) for n1, n2, t, s in conn.itertuples(index=False, name=None) if t.startswith('S')] (Обратитесь к странице WormAtlas относительно описания различных типов контактов.) В приведенном выше словаре мы используем слово weight , так как это специальное ключевое слово для свойств ребер в библиотеке NetworkX. За-тем мы строим граф, используя класс библиотеки NetworkX DiGraph : import networkx as nx wormbrain = nx.DiGraph()wormbrain.add_edges_from(conn_edges) Теперь можно приступить к исследованию некоторых свойств этой сети. Один из главных вопросов, который задают исследователи в отношении на- 1 См. http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001066. 2 См. http://www.openworm.org/. 3 См. http://www.wormatlas.org/neuronalwiring.html#Connectivitydata. 4 Коннектом (connectome) – это полное описание структуры связей в нервной системе организма. Они определяют поведение, с их помощью преобразуется вся информа-ция из внутренней и внешней среды. – Прим. перев. 15 / 27\n--- Страница 96 ---\n96  Создание сетей из областей изображений при помощи ndimage правленных сетей, – это какие узлы имеют внутри нее критическое значение для информационного потока. Узлы с высокой центральностью по посредни- честву характеризуются тем, что находятся на кратчайшем пути между много-численными отличающимися парами узлов. Представьте железнодорожную сеть: определенные станции будут связывать множество железнодорожных ве-ток в таком порядке, что для различных поездок вам придется менять линии. Эти станции имеют высокую центральность по посредничеству. При помощи библиотеки NetworkX мы можем легко находить не менее важ - ные нейроны. В документации по API библиотеки NetworkX 1 раздел «centrality», строка документации betweenness_centrality2, определена функция, принима- ющая на входе граф и возвращающая словарь, в котором идентификаторам узлов поставлены в соответствие значения центральности по посредничеству (значения с плавающей запятой). centrality = nx.betweenness_centrality(wormbrain) Теперь мы можем найти нейроны с самой высокой центральностью, исполь- зуя встроенную функцию Python sorted : central = sorted(centrality, key=centrality.get, reverse=True) print(central[:5]) ['AVAR', 'AVAL', 'PVCR', 'PVT', 'PVCL'] В результате будут возвращены нейроны AVAR, AVAL, PVCR, PVT и PVCL, при- частные к тому, как червь отвечает на нажатие: нейроны AVA (среди прочего) связывают передние рецепторы прикосновения червя с нейронами, которые отвечают за обратное движение, в то время как нейроны PVC связывают зад-ние рецепторы прикосновения с движением вперед. Варшни и др. провели исследование свойств компонент сильной связности 237 нейронов из общего числа 279 нейронов. В графах компонента связности представляет собой множество узлов, достижимых благодаря некому пути, проходящему через все связи. Коннектом представляется направленный граф, характеризуемый, что его ребра не просто соединяют узлы между собой, а на-правлены из одного узла в другой узел. В этом случае компонента сильной связ-ности отличается достижимостью всех узлов друг из друга путем обхода связей в правильном направлении. Таким образом, путь A→B→C не характеризуется сильной связностью, так как отсутствует путь в A из B или C. И напротив, путь A→B→C→A обладает сильной связностью. В нейронной цепи компонента сильной связности может рассматриваться как «мозг» цепи, в котором происходит обработка, в то время как вышестоя-щие узлы являются входами, а нижестоящие узлы – выходами. 1 См. https://networkx.readthedocs.io/en/stable/. 2 См. https://networkx.github.io/documentation/stable/reference/algorithms/centrality.html?- highlight=centrality. 16 / 27\n--- Страница 97 ---\nГрафы и библиотека NetworkX  97 Циклы в нейронных сетях. Идея циклических нейронных цепей восходит к 1950-м го- дам. Вот прекрасный абзац из статьи Аманды Джефтер в научном журнале Nautilus «Че- ловек, который попытался искупить мир логикой»1 с описанием этой идеи: Если человек увидит вспышку молнии на небе, то его глаза пошлют сигнал в головной мозг, пронеся его через цепь нейронов. Начиная с любого заданного нейрона в цепи вы можете проследить шаги сигнала и выяснить, сколько времени прошло от вспышки молнии. С одним исключением – если только эта цепь не является циклом. В случае цикла информация, кодирующая вспышку молнии, будет до бесконечности вращаться в цикле. И она не будет иметь никакой связи со временем вспышки самой молнии. Как выразился Маккалок, она становится «идеей, вырванной из временного пространства». Другими словами, памятью. Библиотека NetworkX облегчает работу по получению из сети wormbrain са- мой большой компоненты сильной связности: sccs = nx.strongly_connected_component_subgraphs(wormbrain) giantscc = max(sccs, key=len) print(f'Самая большая компонента сильной связности имеет ' f'{giantscc.number_of_nodes()} узлов из общего числа ' f'{wormbrain.number_of_nodes()} узлов.') Самая большая компонента сильной связности имеет 237 узлов из общего числа 279 узлов. Как отмечается в исследовательской работе, размер этой компоненты мень- ше, чем ожидалось вследствие случайности. Это демонстрирует следующее: сеть подразделяется на три слоя: входной, центральный и выходной. Теперь мы воспроизведем рис. 6B из исследовательской работы: функцию выживания распределения полустепеней захода. Сначала вычислим соответ - ствующие количества: in_degrees = list(dict(wormbrain.in_degree()).values()) in_deg_distrib = np.bincount(in_degrees)avg_in_degree = np.mean(in_degrees)cumfreq = np.cumsum(in_deg_distrib) / np.sum(in_deg_distrib)survival = 1 - cumfreq Затем, используя библиотеку Matplotlib, построим график: fig, ax = plt.subplots()ax.loglog(np.arange(1, len(survival) + 1), survival)ax.set_xlabel('распределение полустепеней захода')ax.set_ylabel('доля нейронов с более высоким распределением полустепеней захода')ax.scatter(avg_in_degree, 0.0022, marker='v') ax.text(avg_in_degree - 0.5, 0.003, 'среднее=%.2f' % avg_in_degree) ax.set_ylim(0.002, 1.0); 1 См. http://nautil.us/issue/21/information/the-man-who-tried-to-redeem-the-world-with-logic. 17 / 27\n--- Страница 98 ---\n98  Создание сетей из областей изображений при помощи ndimage Распределение полустепеней заходаДоля нейронов с более высоким распределением полустепеней захода А вот и результат использования SciPy: воспроизведение научного анали- за. Мы пропускаем подбор кривой …, именно для этого и существуют упраж - нения. Задача: подбор кривой при помощи SciPy Это упражнение является чем-то вроде предварительного ознакомления с ма- териалом главы 7 (посвященной оптимизации): используйте функцию scipy. optimize.curve_fit , чтобы подогнать хвост функции выживания полустепеней захода под степенной закон, f(d) ~ d–y, d > d0, для d = 10, для d0 = 10 (красная прямая на рис. 6B указанной работы), и видоизмените график, чтобы включить эту прямую. Обратите внимание: решение задачи «Подбор кривой при помощи SciPy» находится в конце книги. Теперь у вас должно быть фундаментальное понимание не только графов как научной абстракции, но и как с помощью языка Pyton и библиотеки NetworkX можно легко этими графами манипулировать и анализировать их. Теперь мы пойдем дальше и обратимся к конкретному виду графов, используемых в об-работке изображений и компьютерном зрении. графы смежности областей Граф смежности областей (RAG) представляет изображение в виде, широко применяемом для сегментации, т. е. для подразделения изображений на со- держательные области (или сегменты). Если вы видели к/ф «Терминатор-2», значит, вы видели и пример сегментации (рис. 3.2). 18 / 27\n--- Страница 99 ---\nГрафы смежности областей  99 Рис. 3.2  Зрение Терминатора Сегментация – одна из задач, которые люди выполняют, не задумываясь. Компьютерам же приходится нелегко. Чтобы понять эту трудность, взгляните на следующее ниже изображение: В то время как вы видите лицо, компьютер видит только группу чисел: 58688888888888899998898988888666532121 66888886888998999999899998888888865421666655665666899999999999988888888886536666889999865568899989998888866866555466888899998888888889988888665666666543 66888888886868868889998888666688888865 6666644333455668888998886666666666886666884235221446588889988665644644444666868644862336646668898866554643212423458666665833368558888886665565938136632488866686688666866888886658588422485434888888888886886888888665666866665654448888888886866688888886655668866668655588888988888888888888886656888688886666 88889999989998888888886666888888868886 8888999888888888888888656688888888886688888998888888688888666566868868888888688889998888888888688866568888888888666888899999888888868888865568888888886668888999886686668886888656566888888886888888888866688888888886565588888888866888888666566888888988855555568888888686868868658668868688886555555588886866 66688866468866855566655445555656888866 66688654888886868666555554556666666865 19 / 27\n--- Страница 100 ---\n100  Создание сетей из областей изображений при помощи ndimage 88688658688888888886666655556686688665 68888886666888888988888866666656686665668888888456868889998888866665568666556668888886245666886666665443126868665568688898886689696666655655313668688655 68888898888668998998998885356888986655 68688889888866899999999866666668986655688888888888666668888666666666888666555688888888868688998686865556668888655536668888888868888868688666686688866655266868888888888888888886666886888656542868888888888888888866866666868666655528666688888888888868668668688886665548 Наша визуальная система оптимизирована определять лица так, что вы смо- жете увидеть лицо даже в этом скоплении чисел! Но мы надеемся, что донесли свою точку зрения. К тому же вы можете взглянуть на примеры лиц в Twitter1, которые с юмором демонстрируют оптимизацию обнаружения лиц нашими визуальными системами. Во всяком случае, проблема состоит в придании этим числам смысла и в на- хождении расположения границ, делящих различные части изображения. По-пулярный подход состоит в отыскании небольших областей (так называемых суперпикселов), принадлежащих тому же самому сегменту, и их объединения в соответствии с неким более сложным правилом. В качестве простого примера предположим, что вам необходимо вычленить в приведенном ниже изображении из набора данных эталонных сегментаций Университета Беркли (Berkeley Segmentation Dataset, BSDS) тигра. Алгоритм кластеризации под названием «Простая линейная итеративная кластеризация» (simple linear iterative clustering, SLIC) может предоставить нам хорошую отправную точку. Этот алгоритм находится в библиотеке scikit-image. 1 См. https://twitter.com/facespics. 20 / 27\n--- Страница 101 ---\nГрафы смежности областей  101 url = ('http://www.eecs.berkeley.edu/Research/Projects/CS/vision/' 'bsds/BSDS300/html/images/plain/normal/color/108073.jpg')tiger = io.imread(url)from skimage import segmentationseg = segmentation.slic(tiger, n_segments=30, compactness=40.0, enforce_connectivity=True, sigma=3) Библиотека scikit-image также имеет функцию для вывода сегментации на экран. Мы применим ее для визуализации результата работы алгоритма SLIC: from skimage import color io.imshow(color.label2rgb(seg, tiger)); Результат показывает, что тело тигра будет разбито на три части, а остальная часть изображения находится в других сегментах. Граф смежности областей (RAG) – это граф, в котором каждый узел пред- ставляет одну из вышеупомянутых областей, а ребро соединяет два узла при их соприкосновении. Чтобы до построения собственного графа посмотреть, как это работает, мы используем функцию show_rag из библиотеки scikit-image. На самом деле эта библиотека содержит фрагмент кода данной главы! from skimage.future import graph g = graph.rag_mean_color(tiger, seg) graph.show_rag(seg, g, tiger); 21 / 27\n--- Страница 102 ---\n102  Создание сетей из областей изображений при помощи ndimage Здесь вы видите узлы, соответствующие каждому сегменту, и ребра между смежными сегментами. Они, в соответствии с разницей в цвете между двумя узлами, окрашены в палитру YlGnBu (желтый-зеленый-синий) из библиотеки Matplotlib. Приведенный выше рисунок также показывает чудодейственность взгляда на сегментацию как на графы: вы видите, что ребра между узлами внутри тиг - ра и ребра за его пределами ярче (т. е. с более высокой величиной), чем ребра внутри самого объекта. Таким образом, если мы сможем вырезать граф вдоль этих ребер, получим нашу сегментацию. Мы выбрали простой пример сегмен-тации на основе цвета, однако те же самые принципы сохраняются и для гра-фов с более сложными попарными связями. элегантный пакет ndimage : как строить графы из областей изображений Теперь все части на месте: вы знакомы с массивами NumPy, фильтрацией изо-бражений, универсальными фильтрами и графами, в частности с графами RAG. Давайте построим граф RAG, чтобы выдернуть тигра из этой фотографии! Очевидный подход состоит в применении двух вложенных циклов for для перебора всех пикселов изображения, просмотра соседних пикселов и провер-ки на разницу меток: import networkx as nx def build_rag(labels, image): g = nx.Graph() nrows, ncols = labels.shape for row in range(nrows): for col in range(ncols): current_label = labels[row, col] 22 / 27\n--- Страница 103 ---\nЭлегантный пакет ndimage: как строить графы из областей изображений  103 if not current_label in g: g.add_node(current_label) g.node[current_label]['total color'] = np.zeros(3, dtype=np.float) g.node[current_label]['pixel count'] = 0 if row < nrows - 1 and labels[row + 1, col] != current_label: g.add_edge(current_label, labels[row + 1, col]) if col < ncols - 1 and labels[row, col + 1] != current_label: g.add_edge(current_label, labels[row, col + 1]) g.node[current_label]['total color'] += image[row, col] g.node[current_label]['pixel count'] += 1 return g Отлично! Все работает. Однако если вы захотите сегментировать трехмер- ное изображение, вам придется написать другую версию: import networkx as nx def build_rag_3d(labels, image): g = nx.Graph() nplns, nrows, ncols = labels.shape for pln in range(nplns): for row in range(nrows): for col in range(ncols): current_label = labels[pln, row, col] if not current_label in g: g.add_node(current_label) g.node[current_label]['total color'] = np.zeros(3, dtype=np.float) g.node[current_label]['pixel count'] = 0 if pln < nplns - 1 and labels[pln + 1, row, col] != \\ current_label: g.add_edge(current_label, labels[pln + 1, row, col]) if row < nrows - 1 and labels[pln, row + 1, col ] != \\ current_label: g.add_edge(current_label, labels[pln, row + 1, col]) if col < ncols - 1 and labels[pln, row, col + 1] != \\ current_label: g.add_edge(current_label, labels[pln, row, col + 1]) g.node[current_label]['total color'] += image[pln, row, col] g.node[current_label]['pixel count'] += 1 return g Обе версии довольно страшные и громоздкие. И более того, их трудно рас - ширить. Если мы захотим подсчитать диагонально соседние пикселы как смежные (т. е. [row, col] «смежен с» [row + 1, col + 1]), то программный код станет еще запутаннее. И если мы захотим проанализировать трехмерное видео, то потребуется еще одна размерность и еще один уровень вложенности. Полный бардак! Взгляните на эту проблему с точки зрения Винеша: функция универсальной фильтрации SciPy generic_filter уже делает эту итерацию за нас! Мы исполь- 23 / 27\n--- Страница 104 ---\n104  Создание сетей из областей изображений при помощи ndimage зовали ее выше, чтобы вычислить произвольно сложную функцию на окрест - ности каждого элемента массива NumPy. Только теперь от этой функции нам требуется не отфильтрованное изображение, а граф. Оказывается, функция generic_filter позволяет передавать в функцию фильтрации дополнительные аргументы. И мы можем это задействовать для построения графа: import networkx as nx import numpy as np from scipy import ndimage as nd def add_edge_filter(values, graph): center = values[len(values) // 2] for neighbor in values: if neighbor != center and not graph.has_edge(center, neighbor): graph.add_edge(center, neighbor) # Возвращаемое вещественное значение не используется, но # требуется функцией `generic_filter` return 0.0 def build_rag(labels, image): g = nx.Graph() footprint = ndi.generate_binary_structure(labels.ndim, connectivity=1) _ = ndi.generic_filter(labels, add_edge_filter, footprint=footprint, mode='nearest', extra_arguments=(g,)) for n in g: g.node[n]['total color'] = np.zeros(3, np.double) g.node[n]['pixel count'] = 0 for index in np.ndindex(labels.shape): n = labels[index] g.node[n]['total color'] += image[index] g.node[n]['pixel count'] += 1 return g Вот несколько причин, почему это действительно прекрасный фрагмент кода: функция ndi.generic_filter перебирает все элементы массива вместе с их соседями (используйте numpy.ndindex , чтобы перебирать только индексы массива); из функции фильтрации мы возвращаем «0.0», так как generic_filter тре- бует, чтобы функция фильтрации возвращала вещественное значение. Однако мы игнорируем результат фильтрации (который повсюду рав-няется нулю) и используем его только из-за его «побочного эффекта», заключающегося в добавлении в граф ребер; циклы не вложены на несколько уровней в глубину. Это делает про-граммный код компактнее и проще для восприятия; этот программный код работает не только для одно-, двух-, трех-, но даже и для восьмимерных изображений! чтобы добавить поддержку диагональной связности, достаточно поме-нять параметр connectivity на ndi.generate_binary_structure . 24 / 27\n--- Страница 105 ---\nСобираем все вместе: сегментация по среднему цвету  105 собираем Все Вместе : сегментация по среДнему цВету Теперь, чтобы сегментировать тигра на приведенном выше изображении, мы можем применить все, чему научились: g = build_rag(seg, tiger) for n in g: node = g.node[n] node['mean'] = node['total color'] / node['pixel count']for u, v in g.edges: d = g.node[u]['mean'] - g.node[v]['mean'] g[u][v]['weight'] = np.linalg.norm(d) Каждое ребро содержит разность между средним цветом каждого сегмента. Сейчас мы зададим порог для графа: def threshold_graph(g, t): to_remove = [(u, v) for (u, v, d) in g.edges(data=True) if d['weight'] > t] g.remove_edges_from(to_remove)threshold_graph(g, 80) И применим трюк NumPy с индексацией в массиве, с которым мы познако- мились в главе 2: map_array = np.zeros(np.max(seg) + 1, int)for i, segment in enumerate(nx.connected_components(g)): for initial in segment: map_array[int(initial)] = isegmented = map_array[seg] plt.imshow(color.label2rgb(segmented, tiger)); Ой! Похоже, кошак потерял хвост! 25 / 27\n--- Страница 106 ---\n106  Создание сетей из областей изображений при помощи ndimage И тем не менее мы считаем, что это была прекрасная демонстрация возмож - ностей графов RAG и красоты, с которой библиотеки SciPy и NetworkX их дела- ют выполнимыми. Многие из этих функций имеются в библиотеке scikit-image. Если вы интересуетесь анализом изображений, то с этой библиотекой следует познакомиться поближе! 26 / 27\n",
      "debug": {
        "start_page": 78,
        "end_page": 106
      }
    },
    {
      "name": "Глава 4 Частота и быстрое преобразование Фурье",
      "content": "Глава 4 Частота и быстрое преобразование Фурье Если вы хотите раскрыть тайны вселенной, ду - майте с точки зрения энергии, частоты и виб- рации. – Никола Тесла Эта глава была написана в сотрудничестве с отцом Штефана, ПВ ван дер Уол-том (PW van der Walt).Данная глава немного отойдет от формата остальной части книги. Здесь, в част - ности, вы обнаружите не очень большой программный код. Цель этой главы – показать элегантный и очень полезный алгоритм под названием «быст рое пре - образование Фурье» (БПФ, Fast Fourier Transform, FFT), реализованный в SciPy и, разумеется, работающий с массивами NumPy. ВВеДение В частоту Как всегда, мы начнем с настройки стиля графиков и импорта обычных со- ставляющих: # Заставить графики появляться локально, задать индивидуальный стиль графиков %matplotlib inlineimport matplotlib.pyplot as pltplt.style.use('style/elegant.mplstyle') import numpy as np Дискретное1 преобразование Фурье (ДПФ, Decrete Fourier Transform, DFT) – это математический метод, используемый для преобразования временных или пространственных данных в данные частотной области. Понятие час- тоты широко известно. Благодаря звуковым колебаниям разной частоты 1 ДПФ оперирует с выборочными данными, в отличие от стандартного преобразова- ния Фурье, которое определено для непрерывных данных. Powered by TCPDF (www.tcpdf.org) 27 / 27\n--- Страница 108 ---\n108  Частота и быстрое преобразование Фурье мы слышим друг друга. Частоты, которые слышит человек, лежат в пределах от 20 и до 16 000 Гц. Но способность слышать крайние частоты у каждого человека строго индивидуальна. Самые низкие ноты, воспроизводимые го-ловными телефонами, имеют частоту порядка 20 Гц, а средняя нота «до» на фортепьяно имеет частоту примерно 261.6 Гц. Герцы, или колебания в се-кунду, в данном случае буквально означают количество движений мембраны в наушнике из стороны в сторону в секунду. Эти движения, в свою очередь, создают сжатые импульсы воздуха, которые, достигнув вашей барабанной перепонки, вызывают вибрацию с той же самой частотой. Так, если взять простую периодическую функцию, sin(10 ×2πt), то ее можно представить как волну: f = 10 # Частота колебаний в циклах в секунду, или в герцах f_s = 100 # Частота дискретизации (количество замеров в секунду) t = np.linspace(0, 2, 2 * f_s, endpoint=False)x = np.sin(f * 2 * np.pi * t) fig, ax = plt.subplots() ax.plot(t, x) ax.set_xlabel('Время [с]')ax.set_ylabel('Амплитуда сигнала'); Амплитуда сигнала Время [с] Или как повторяющийся сигнал частотой 10 Гц (он повторяется один раз в 1/10 секунды – этот отрезок времени мы называем периодом). Хотя мы, впол- не есественно, связываем частоту со временем, она может одинаково хорошо применяться и к пространству. Например, фотография текстильных узоров показывает высокую пространственную частоту, тогда как небо или другие гладкие объекты имеют низкую пространственную частоту. Теперь давайте исследуем нашу синусоиду, применив БПФ: 1 / 27\n--- Страница 109 ---\nВведение в частоту  109 from scipy import fftpack X = fftpack.fft(x) freqs = fftpack.fftfreq(len(x)) * f_s fig, ax = plt.subplots()ax.stem(freqs, np.abs(X)) ax.set_xlabel('Частота в герцах [Гц]') ax.set_ylabel('Величина частотной области (или спектра)') ax.set_xlim(-f_s / 2, f_s / 2) ax.set_ylim(-5, 110) (-5, 110) Частота в герцах [Гц]Величина частотной области (или спектра) Мы видим, что на выходе из БПФ мы получим одномерный массив той же самой формы, содержащий комплексные величины, что и на входе. Все вели- чины, за исключением двух записей, равны нулю. По традиции мы визуализи-руем магнитуду результата в виде графика стебель–листья, в котором высота каждого стебля соответствует базовой величине. (Позже в разделе «Дискретные преобразования Фурье» мы объясним, поче- му вы видите положительные и отрицательные колебания частоты. Вы также можете обратиться к упомянутому разделу для получения более глубокого об-зора лежащих в основе математических выкладок.) Преобразование Фурье уводит нас из временной области в частотную, и это влечет за собой огромное количество применений. Быстрое преобразование Фурье (БПФ) представляет собой алгоритм вычисления ДПФ. Его высокое быст родействие достигается за счет сохранения и повторного использования результатов вычислений по мере продвижения. 2 / 27\n--- Страница 110 ---\n110  Частота и быстрое преобразование Фурье В этой главе мы исследуем несколько применений ДПФ, которые продемон- стрируют, что БПФ может применяться к многомерным (а не только к одно- мерным) данным, достигая при этом самых разнообразных целей. иллюстрация : спектрограмма пения птиц Давайте начнем с одного из наиболее распространенных применений – пре-образования аудиосигнала (состоящего из вариаций давления воздуха во вре-мени) в спектрограмму. Вы, возможно, уже встречали спектрограммы в окне эквалайзера своего аудиоплеера или даже в старомодном стереопроигрывате-ле (рис. 4.1). Рис. 4.1  Стереоэквалайзер Numark EQ26001 (изображение используется с разрешения автора Сергея Герасимука) Прослушайте этот отрывок пения соловья2 (выпущенный в соответствии с лицензией CC 4.0): from IPython.display import Audio Audio('data/nightingale.wav') Если вы читаете бумажную версию этой книги, то вам придется применить свое воображение! Он звучит примерно так: чи-чи-вурррр-хи-хи чиит-виит- хуррр-чирр-ви-вео-вео-вео-вео-вео-вео. 1 См. https://sgerasimuk.blogspot.ru/2014/06/numark-eq-2600-10-band-stereo-graphic.html. 2 См. http://www.orangefreesounds.com/nightingale-sound/. 3 / 27\n--- Страница 111 ---\nИллюстрация: спектрограмма пения птиц  111 Поскольку мы понимаем, что не все бегло говорят на птичьем языке, будет лучше выполнить визуализацию всех замеров – именуемых как «сигнал». Мы загрузим аудиофайл, который даст нам частоту дискретизации (коли- чество замеров в секунду), а также аудиоданные в виде массива формы (N, 2) – два стобца, потому что это стереозапись. from scipy.io import wavfile rate, audio = wavfile.read('data/nightingale.wav') Далее преобразуем этот файл в моно, усреднив левый и правый каналы. audio = np.mean(audio, axis=1) Затем мы вычислим длину отрывка и выведем аудио на график (рис. 4.2). N = audio.shape[0] L = N / rate print(f'Длина аудио: {L:.2f} секунд')f, ax = plt.subplots() ax.plot(np.arange(N) / rate, audio) ax.set_xlabel('Время [с]') ax.set_ylabel('Амплитуда [неизвестно]'); Длина аудио: 7.67 секунды Время [c]Амплитуда [неизвестно] Рис. 4.2  График формы аудиосигнала пения соловья Надо сказать, его вид не очень-то вдохновляет, не правда ли? Если бы я на- правил этот сигнал в громкоговоритель, то смог бы услышать щебет птицы. Но я не смогу достаточно хорошо представить, как оно будет звучать в моей голове. Есть ли более подходящий способ увидеть, что происходит? Да, есть. И этот метод называется дискретным преобразованием Фурье, или ДПФ. Слово «дискретный» относится к записи, состоящей, в отличие от непре- 4 / 27\n--- Страница 112 ---\n112  Частота и быстрое преобразование Фурье рывной записи (как, например, на магнитной ленте катушечного магнитофона или аудиокассете), из разделенных во времени замеров звука. ДПФ часто вы-числяется с использованием алгоритма БПФ, название которого неофициаль-но используется для обозначения ДПФ как такового. ДПФ сообщает нам о том, какие частоты или «ноты» можно ожидать в нашем сигнале. Конечно же, в пении птицы содержится большое количество нот, поэтому мы также хотели бы знать, когда появляется каждая нота. Преобразование Фу - рье берет сигнал во временном интервале (т. е. серию замеров во времени) и превращает его в спектр – серию частот с соответствующими (комплексны-ми 1) величинами. Спектр не содержит какой-либо информации о времени2! Так вот, чтобы найти частоты и временную точку, когда они были спеты, нам придется применить неформальный подход. Наша стратегия будет сле-дующей: взять аудиосигнал, разделить его на небольшие накладывающиеся фрагменты и применить преобразование Фурье к каждому из них (такой при-ем называется кратковременным преобразованием Фурье). Мы разделим сигнал на фрагменты, состоящие из 1024 выборок (сэмплов), – продолжительность каждого составит порядка 0.02 секунды аудио. Почему мы выбрали именно 1024, а не 1000, мы объясним через секунду, когда займем-ся исследованием производительности. Фрагменты будут накладываться на 100 выборок, как показано ниже: Начнем с нарезки сигнала на фрагменты по 1024 выборки, причем каждый фрагмент будет накладываться на предыдущие 100 выборок. Результирующий объект slices содержит один фрагмент в расчете на строку. 1 Преобразование Фурье, по существу, говорит о том, как объединить серию синусо- ид переменной частоты, чтобы сформировать входной сигнал. Спектр состоит из комплексных чисел – одно число для каждой синусоиды. В комплексном числе за-кодированы две вещи: магнитуда и угол. Магнитуда – это сила синусоиды в сигнале, и угол – насколько она смещена во времени. На данном этапе нас интересует только магнитуда, которую мы вычисляем с использованием np.abs. 2 Для получения дополнительной информации о методах вычисления (приблизитель- ных) частот и времени появления почитайте материалы по анализу формы сигнала, или по вейвлет-анализу. 5 / 27\n--- Страница 113 ---\nИллюстрация: спектрограмма пения птиц  113 from skimage import util M = 1024slices = util.view_as_windows(audio, window_shape=(M,), step=100) print(f'Форма аудио: {audio.shape}, форма нарезанного аудио: {slices.shape}') Форма аудио: (338081,), форма нарезанного аудио: (3371, 1024) Сгенерируем оконную функцию (см. ниже раздел «Оконное преобразова- ние», в котором обсуждаются лежащие в основе допущения и интерпретации) и умножим ее на сигнал: win = np.hanning(M + 1)[:-1] slices = slices * win Удобнее иметь один фрагмент в расчете на столбец. Поэтому мы транспо- нируем массив: slices = slices.Tprint('Форма объекта `slices`:', slices.shape) Форма объекта `slices`: (1024, 3371) Для каждого фрагмента вычислим ДПФ-преобразование, возвращающее положительные и отрицательные частоты (подробнее об этом ниже в разделе «Частоты и их упорядочивание»). На данный момент мы нарежем положитель-ные M2-частоты. spectrum = np.fft.fft(slices, axis=0)[:M // 2 + 1:-1] spectrum = np.abs(spectrum) (В примечании заметим, что используются взаимозаменяемые функции scipy. fftpack.fft и np.fft . Библиотека NumPy обеспечивает базовый функционал БПФ, расширяемый библиотекой SciPy. При этом обе библиотеки содержат функцию fft, основанную на динамической библиотеке FFTPACK языка Fortran.) Спектр может содержать очень большие и очень малые значения. Если взять логарифм этого спектра, то этот диапазон значений будет существенно сжат. Ниже мы строим логарифмический график соотношения сигнала, деленного на максимальный сигнал (график показан на рис. 4.3). Специфической еди-ницей измерения, используемой для данного соотношения, является децибел, 20log 10 (амплитудный коэффициент). f, ax = plt.subplots(figsize=(4.8, 2.4)) S = np.abs(spectrum) S = 20 * np.log10(S / np.max(S)) ax.imshow(S, origin='lower', cmap='viridis', extent=(0, L, 0, rate / 2 / 1000))ax.axis('tight')ax.set_ylabel('Частота [кГц]')ax.set_xlabel('Время [с]'); 6 / 27\n--- Страница 114 ---\n114  Частота и быстрое преобразование Фурье Частота [кГц] Время [c] Рис. 4.3  Спектрограмма пения птицы Теперь намного лучше! Мы видим, что частоты варьируются во времени, и спектрограмма соответствует тому, как звучит аудио. Проверьте, можно ли сопоставить наше предыдущее описание: чи-чи-вурррр-хи-хи чиит-виит-хуррр-чирр-ви-вео-вео-вео-вео-вео-вео. (Я не расшифровал вторую отметку с 3 по 5 – это другая птица.) Библиотека SciPy уже содержит реализацию этой процедуры как scipy. signal.spectrogram (рис. 4.4), которую можно вызвать следующим образом: from scipy import signal freqs, times, Sx = signal.spectrogram(audio, fs=rate, window='hanning', nperseg=1024, noverlap=M - 100, detrend=False, scaling='spectrum') f, ax = plt.subplots(figsize=(4.8, 2.4)) ax.pcolormesh(times, freqs / 1000, 10 * np.log10(Sx), cmap='viridis')ax.set_ylabel('Частота [кГц]')ax.set_xlabel('Время [с]'); Частота [кГц] Время [c] Рис. 4.4  Встроенное в SciPy исполнение спектрограммы пения птицы 7 / 27\n--- Страница 115 ---\nРеализация  115 Единственная разница между спектрограммой, созданной нами вручную, и встроенной в SciPy функцией состоит в том, что SciPy возвращает квадрат магнитуды спектра (превращающий измеренное напряжение в измеренную энергию) и умножает его на коэффициенты нормализации 1. история Отследить точное происхождение преобразования Фурье довольно сложно. Некоторые связанные с ним процедуры восходят еще к временам Вавилона. Вместе с тем это преобразование было актуальным для орбит астероидов и ре-шения уравнения теплопроводности, что в итоге привело к нескольким про-рывам в начале 1800-х годов. Кого мы должны благодарить: Клеро, Лагранжа, Эйлера, Гаусса и Д’ Аламбера, – не совсем понятно. Но Гаусс был первым, кто описал быстрое преобразование Фурье (т. е. алгоритм вычисления ДПФ, по-пуляризированный Кули и Тьюки в 1965 г.). Жан Батист Жозеф Фурье, в честь которого это преобразование названо, был первым, кто утверждал, что произ- вольные периодические 2 функции могут быть выражены как сумма тригоно- метрических функций. реализация Функционал ДПФ библиотеки SciPy расположен в модуле scipy.fftpack . Кроме того, он обеспечивает следующую связанную с ДПФ функциональность: fft, fft2, fftn Вычисляют ДПФ, используя алгоритм БПФ в 1, 2 или n размерностях. ifft, ifft2, ifftn Вычисляют инверсию ДПФ. dct, idct, dst, idst Вычисляют косинуc- и синус-преобразования и их инверсии. fftshift, ifftshift Смещают постоянную компоненту (компоненту с нулевой частотой) соот - ветственно в центр спектра и назад (подробнее об этом ниже). 1 Библиотека SciPy прилагает некоторые усилия по сохранению энергии в спектре. Поэто- му, беря только половину компонент (для N четного), она умножает оставшиеся компо- ненты, кроме первой и последней, на два (эти две компоненты используются «совмест - но» двумя половинами спектра). Она также нормализует окно, деля его на их сумму. 2 На самом деле период тоже может быть бесконечным! Обобщенное непрерывное преобразование Фурье предусматривает эту возможность. ДПФ-преобразования, как правило, определяются на конечном интервале, и этот интервал неявно явля-ется периодом функции преобразуемой временной области. Другими словами, если вы выполняете обратное дискретное преобразование Фурье, то на выходе вы всегда получите периодический сигнал. 8 / 27\n--- Страница 116 ---\n116  Частота и быстрое преобразование Фурье fftfreq Возвращает выборочные частоты ДПФ. rfft Для увеличения производительности вычисляет ДПФ вещественной после- довательности, эксплуатируя симметрию результирующего спектра. В за-висимости от ситуации также используется функцией fft для внутренних целей. Этот список дополняется следующими ниже функциями библиотеки NumPy: np.hanning, np.hamming, np.bartlett, np.blackman, np.kaiser Функции суженного оконного преобразования. ДПФ также применяется для выполнения быстрой свертки больших вход- ных данных функцией scipy.signal.fftconvolve . Библиотека SciPy служит оберткой для динамической библиотеки FFTPACK языка Fortran. Это не самая быстрая библиотека, но, в отличие от таких паке- тов, как FFTW, она имеет разрешительную лицензию бесплатного программ-ного обеспечения. Выбор Длины Дпф Для простого вычисления ДПФ требуется (N2) операций1. Почему? Дело в том, что у вас N (комплексных) синусоид разных частот (2π f×0, 2πf×1; 2πf×3, , 2πf×(N – 1)), и вы хотите увидеть, насколько ваш сигнал соответ - ствует каждой из них. Начиная с первой вы берете скалярное произведение с сигналом (который сам по себе влечет за собой N операций умножения). Тогда повторение этой операции N раз, один раз для каждой синусоиды, дает N 2 операций. Теперь сопоставьте это с алгоритмом БПФ, имеющим в идеальном случае вычислительную сложность (N logN). Это достигнуто благодаря грамотному повторному использованию результатов вычислений. Громадное улучшение! Однако классический алгоритм Кули-Тьюки, реализованный в динамической библиотеке FFTPACK (и используемый библиотекой SciPy), рекурсивно раз- 1 В информатике вычислительная сложность алгоритма часто выражается в форме ма- тематического обозначения «O» большое. Это обозначение указывает, как масшта-бируется время выполнения алгоритма с ростом количества элементов. Если алго-ритм имеет сложность (N), то время его выполнения увеличивается линейно вместе с количеством входных элементов (например, поиск заданного значения в неот - сортированном списке имеет сложность (N). Сортировка пузырьком является при- мером алгоритма со сложностью O(N 2). Точное количество выполненных операций теоретически может равняться . При этом считается, что вычислительная сложность растет квадратически вместе с количеством входных элементов. 9 / 27\n--- Страница 117 ---\nВыбор длины ДПФ  117 бивает преобразование на меньшие фрагменты (имеющие размер, равный простому числу) и показывает это улучшение только для «гладких» входных длин (входная длина считается гладкой, когда ее самый большой простой множитель является малым, как показано на рис. 4.5). Для фрагментов с раз-мером, равным большому простому числу, вместе с алгоритмом Кули-Тьюки могут применяться алгоритмы Блуштайна или Рейдера. Но такая оптимизация в FFTPACK не реализована 1. Давайте посмотрим на примере: import time from scipy import fftpack from sympy import factorint K = 1000 lengths = range(250, 260) # Вычислить гладкость для всех входных длин smoothness = [max(factorint(i).keys()) for i in lengths] exec_times = [] for i in lengths: z = np.random.random(i) # Для каждой входной длины i исполнить БПФ K раз и # сохранить время исполнения times = [] for k in range(K): tic = time.monotonic() fftpack.fft(z) toc = time.monotonic() times.append(toc - tic) # Для каждой входной длины запомнить *минимальное* время исполнения exec_times.append(min(times)) f, (ax0, ax1) = plt.subplots(2, 1, sharex=True) ax0.stem(lengths, np.array(exec_times) * 10**6) ax0.set_ylabel('Время исполнения (µсек)') ax1.stem(lengths, smoothness) ax1.set_ylabel('Гладкость входной длины\\n(чем ниже, тем лучше)') ax1.set_xlabel('Длина входа'); 1 Хотя в идеальном случае нам бы не хотелось повторно реализовывать существую- щие алгоритмы, иногда возникает необходимость получить наилучшие возможные скорости исполнения, и такие инструменты, как Cython, компилирующий Python на C, и Numba, производящий JIT-компиляцию программного кода Python, намного облегчают жизнь (и ускоряют работу алгоритмов!). Если у вас есть возможность ис - пользовать программное обеспечение с общедоступной GPL-лицензией, то вы могли бы рассмотреть использование библиотеки PyFFTW, в которой применяются более быстрые алгоритмы БПФ. 10 / 27\n--- Страница 118 ---\n118  Частота и быстрое преобразование Фурье Гладкость входной длины (чем ниже, тем лучшеВремя исполнения (µсек) Длина входа Рис. 4.5  Время исполнения БПФ относительно гладкости разных входных длин Интуитивный вывод состоит в том, что для гладких количеств преобразо- вание БПФ может быть разбито на множество небольших фрагментов. После выполнения БПФ на первом фрагменте мы можем повторно воспользовать-ся этими результатами в последующих вычислениях. Это объясняет, поче-му ранее для наших кусков аудио мы выбрали длину 1024, – она имеет глад-кость, равную всего 2, что приводит к оптимальному алгоритму Кули-Тьюки для «корня степени 2», вычисляемому БПФ, используя всего (N /2)log 2N = 5120 комплексных умножений вместо N2 = 1 048 576. Выбор N = 2m всегда га- рантирует максимально гладкое N (и, значит, самое быстрое исполнение ал- горитма БПФ). Дополнительные понятия Дпф Далее мы представим несколько общих понятий, которые стоит узнать, прежде чем приступать к работе с тяжелыми механизмами преобразования Фурье. Мы займемся решением еще одной практической задачи: анализом обнаружения цели в радиолокационных данных. Частоты и их упорядочивание По историческим причинам большинство реализаций возвращает массив, в ко-тором частоты варьируются от низких до высоких и снова низких (см. раздел «Дискретные преобразования Фурье» относительно более подробного объяс - нения частот). Например, когда мы выполняем вещественное преобразование сигнала, состоящего только из единиц, вход не имеет вариации. Поэтому на 11 / 27\n--- Страница 119 ---\nДополнительные понятия ДПФ  119 входе в качестве первой записи появляется лишь самая медленная постоянная компонента Фурье (так называемая «DC-компонента», или постоянно токовая компонента – жаргон из радиоэлектроники, обозначающий просто «среднее значение сигнала»): from scipy import fftpack N = 10 fftpack.fft(np.ones(N)) # Первая компонента равняется np.mean(x) * N array([ 10.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.-0.j, 0.-0.j, 0.-0.j, 0.-0.j]) Когда мы проверяем БПФ на быстро изменяющемся сигнале, видим появле- ние высокочастотной компоненты: z = np.ones(10)z[::2] = -1 print(f'Применение БПФ к {z}') fftpack.fft(z) Применение БПФ к [-1. 1. -1. 1. -1. 1. -1. 1. -1. 1.]array([ 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, -10.+0.j, 0.-0.j, 0.-0.j, 0.-0.j, 0.-0.j]) Обратите внимание, БПФ возвращает комплексный спектр, являющийся в случае вещественных входных данных сопряженно-симметричным (т. е. симметричным в вещественной части и антисимметричным в мнимой части ): x = np.array([1, 5, 12, 7, 3, 0, 4, 3, 2, 8]) X = fftpack.fft(x) np.set_printoptions(precision=2) print(\"Вещественная часть: \", X.real) print(\"Мнимая часть:\", X.imag)np.set_printoptions() Вещественная часть: [ 45. 7.09 -12.24 -4.09 -7.76 -1. -7.76 -4.09 -12.24 7.09] Мнимая часть: [ 0. -10.96 -1.62 12.03 6.88 0. -6.88 -12.03 1.62 10.96] (Напоминаем: первая компонента равняется np.mean (x) * N.) Функция fftfreq показывает, на какие конкретно частоты мы смотрим: fftpack.fftfreq(10) array([ 0. , 0.1, 0.2, 0.3, 0.4, -0.5, -0.4, -0.3, -0.2, -0.1]) Этот результат показывает, что наша максимальная компонента произошла на частоте 0.5 цикла в расчете на выборку. Результат согласуется со входом, где цикл «минус один плюс один» повторялся каждую вторую выборку. Иногда удобно рассматривать спектр, организованный немного по-другому: от высокоотрицательного до «от низко- до высокоположительного» (на дан- 12 / 27\n--- Страница 120 ---\n120  Частота и быстрое преобразование Фурье ном этапе мы не будем детально разбирать понятие отрицательной частоты и прос то скажем, что реальная синусоидальная волна порождается комбина- цией положительных и отрицательных частот). Мы перетасовываем спектр, используя функцию fftshift . Дискретные преобразования Фурье ДПФ преобразовывает последовательность из N равномерно расположенных ве- щественных или комплексных выборок x0, x1, , xn–1 функции x(t) времени (либо другой переменной, в зависимости от приложения) в последовательность из N комплексных чисел Xk путем приведенного ниже суммирования: Если числа Xk известны, то обратное ДПФ-преобразование с помощью приведенного ниже суммирования в точности восстанавливает выборочные значения xn: Учитывая, что ejθ = cosθ + jsinθ, последнее уравнение показывает, что ДПФ-преобра- зование разложило последовательность xn в комплексный дискретный ряд Фурье с ко- эффициентами Xk. Сравним ДПФ с непрерывным комплексным рядом Фурье: ДПФ представляет собой конечный ряд с N членами, определенными в равномерно расположенных дискретных экземплярах угла (ω0tn) = 2π(k/N ) в интервале [0, 2π) – т. е. включая 0 и исключая 2 π. Это автоматически нормализует ДПФ так, что в прямом или обратном преобразовании время явным образом не появляется. Если исходная функция x(t) будет ограничиваться по частоте менее половиной час - тоты дискретизации (так называемой частотой Найквиста), то интерполяция между выборочными значениями, производимая обратным ДПФ-преобразованием, обычно будет давать верную реконструкцию x(t). Если x(t) как таковая не ограничивается, то обратное ДПФ-преобразование не может в целом путем интерполяции использоваться для реконструкции x(t). Обратите внимание, данное ограничение не подразумевает от - сутствия методов, позволяющих выполнять такую реконструкцию. Возьмем, например, методы восстановления сигнала с использованием знаний о его предыдущих разрежен-ных или сжатых значениях (compressed sensing) или методы выборки сигналов с конеч-ной интенсивностью обновления (FRI-сигналов) 1. Функция e(j2πk|N) = (e(j2π|N))k = wk принимает дискретные значения между 0 и на еди- ничном круге в комплексной плоскости. Функция e(j2πkn|N) = wkn окружает источник n[(N – 1)/N ] раз, в результате генерируя гармонику фундаментальной синусоиды, для которой n = 1. 1 Интенсивность обновления сигнала (rate of innovation, FRI) – это количество сте- пеней свободы на единицу времени. См. https://infoscience.epfl.ch//record/34271/files/ VetterliMB01.pdf. – Прим. перев. 13 / 27\n--- Страница 121 ---\nДополнительные понятия ДПФ  121 То, чем мы определили ДПФ, приводит к нескольким тонкостям при n > (N/2) для N четных1. На рис. 4.6 график функции e(j2πkn |N) построен для увеличивающихся значений k для случаев от n = 1 до n = N – 1 для N = 16. При увеличении k от k до k + 1 угол уве- личивается на 2πn/N. При n = 1 шаг равняется 2π/N. При n = N – 1 угол увеличивается на 2π[(N – 1)/N] = 2 π – 2π/N. Поскольку 2π равно одному обороту вокруг круга, шаг будет равен –(2π/N), т. е. в направлении отрицательной частоты. Компоненты до N/2 пред- ставляют положительные компоненты частоты. Компоненты частоты, что выше N/2, и до N – 1 представляют отрицательные частоты. Угловое приращение для компоненты N/2 для N четного занимает половину круга для каждого приращения в k. Поэтому может ин- терпретироваться как положительная или же как отрицательная частота. Эта компонента ДПФ представляет частоту Найквиста (т. е. половину частоты дискретизации) и служит ориентиром при рассмотрении графика ДПФ. БПФ, в свою очередь, просто является специальным и очень эффективным алгорит - мом вычисления ДПФ. В отличие от прямого вычисления ДПФ, занимающего порядка N 2 вычислений, алгоритм БПФ занимает порядка NlogN вычислений. БПФ стал ключевым в широком распространении ДПФ в приложениях, работающих в режиме реального времени, и в 2000 г. журналом IEEE Computing Science & Engineering он был включен в список лучших 10 алгоритмов XX века. k = 0 n = 1 n = N – 1 N = 16k = 0 Рис. 4.6  Образцы единичного круга Давайте исследуем частотные компоненты в зашумленном изображении (рис. 4.7). Обратите внимание: когда статическое изображение не имеет меня-ющейся во времени компоненты, его значения варьируются в пространстве. ДПФ применяется одинаково к любому случаю. Сначала загрузим и покажем изображение: from skimage import io image = io.imread('images/moonlanding.png') M, N = image.shape f, ax = plt.subplots(figsize=(4.8, 4.8)) 1 Как упражнение мы оставляем читателям задачу изобразить ситуацию для N. Об - ратите внимание: в этой главе все примеры используют ДПФ-преобразования с чет - ным порядком. 14 / 27\n--- Страница 122 ---\n122  Частота и быстрое преобразование Фурье ax.imshow(image) print((M, N), image.dtype)(474, 630) uint8 Рис. 4.7  Зашумленное изображение посадки на Луну Не пытайтесь регулировать свой монитор! Показанное изображение настоя- щее, хотя искажено передающим или приемным оборудованием. Для исследования спектра, поскольку изображение имеет более одной раз- мерности, чтобы вычислить ДПФ, применим вместо функции fft функцию fftn. Двумерное БПФ-преобразование эквивалентно взятию одномерного БПФ в строках и затем в столбцах, или наоборот. F = fftpack.fftn(image)F_magnitude = np.abs(F) F_magnitude = fftpack.fftshift(F_magnitude) Снова, чтобы сжать диапазон значений перед выводом на экран, возьмем логарифм спектра: f, ax = plt.subplots(figsize=(4.8, 4.8)) ax.imshow(np.log(1 + F_magnitude), cmap='viridis', extent=(-N // 2, N // 2, -M // 2, M // 2)) ax.set_title('Магнитуда спектра'); 15 / 27\n--- Страница 123 ---\nДополнительные понятия ДПФ  123 Магнитуда спектра Обратите внимание на высокие значения вокруг источника (середины) спектра. Эти коэффициенты описывают низкие частоты или сглаживают части изображения, размывшие полотно фотографии. Более высокочастотные ком-поненты, распространенные по всему спектру, заполняют края и детализацию. Пики вокруг более высоких частот соответствуют периодическому шуму. Из фотографии мы видим, что шум (артефакты измерения) имеет высоко- периодический характер. Поэтому попробуем удалить его, обнулив соответ - ствующие части спектра (рис. 4.8). Изображение с этими подавленными пиками действительно выглядит лучше! # Назначить блоку вокруг центра спектра значение ноль K = 40F_magnitude[M // 2 - K: M // 2 + K, N // 2 - K: N // 2 + K] = 0 # Найти все пики выше 98-го процентиляpeaks = F_magnitude < np.percentile(F_magnitude, 98) # Сдвинуть пики назад, чтобы выровнять с исходным спектром peaks = fftpack.ifftshift(peaks) # Сделать копию исходного (комплексного) спектра F_dim = F.copy() # Установить эти пиковые коэффициенты в ноль F_dim = F_dim * peaks.astype(int) # Выполнить обратную трансформацию Фурье, чтобы вернуться к изображению. # Поскольку мы начали с вещественного изображения, то обратимся только # к вещественной части результата.image_filtered = np.real(fftpack.ifft2(F_dim)) f, (ax0, ax1) = plt.subplots(2, 1, figsize=(4.8, 7)) 16 / 27\n--- Страница 124 ---\n124  Частота и быстрое преобразование Фурье ax0.imshow(np.log10(1 + np.abs(F_dim)), cmap='viridis') ax0.set_title('Спектр после подавления') ax1.imshow(image_filtered) ax1.set_title('Реконструированное изображение'); Спектр после подавления Реконструированное изображение Рис. 4.8  Отфильтрованное изображение посадки на Луну и его спектр Оконное преобразование Если исследовать преобразование Фурье прямоугольного импульса, то мы уви- дим значительные боковые лепестки в спектре: x = np.zeros(500) x[100:150] = 1 X = fftpack.fft(x)f, (ax0, ax1) = plt.subplots(2, 1, sharex=True) ax0.plot(x) ax0.set_ylim(-0.1, 1.1) 17 / 27\n--- Страница 125 ---\nДополнительные понятия ДПФ  125 ax1.plot(fftpack.fftshift(np.abs(X))) ax1.set_ylim(-5, 55); В теории для представления любого резкого перехода вам потребуется ком- бинация бесконечно многочисленных синусоид (частот). Коэффициенты, как правило, имеют ту же самую структуру боковых лепестков, как показано в слу - чае с пульсом. Немаловажно, что ДПФ исходит из периодического характера входного сиг - нала. Если сигнал не периодический, принимается допущение, что в конце сигнала он отскакивает к своему начальному значению. Рассмотрим функцию x(t), показанную ниже: Мы измеряем сигнал в течение короткого времени, помеченного как Teff. Преобразование Фурье строится на основании, что x(8) = x(0), и сигнал продол- жается как прерывистая, а не сплошная линия. Это вносит большой скачок на краю с ожидаемой осцилляцией в спектре: t = np.linspace(0, 1, 500) x = np.sin(49 * np.pi * t) X = fftpack.fft(x) 18 / 27\n--- Страница 126 ---\n126  Частота и быстрое преобразование Фурье f, (ax0, ax1) = plt.subplots(2, 1) ax0.plot(x) ax0.set_ylim(-1.1, 1.1) ax1.plot(fftpack.fftfreq(len(t)), np.abs(X)) ax1.set_ylim(0, 190); Вместо ожидаемых двух линий пики распределены по спектру. Этому эффекту можно противопоставить процесс, называемый оконным преобразованием. Исходная функция умножается на функцию окна, такую как окно Кайзера K(N, β). Ниже мы его визуализируем для β в пределах от 0 до 100: f, ax = plt.subplots() N = 10 beta_max = 5colormap = plt.cm.plasma norm = plt.Normalize(vmin=0, vmax=beta_max) lines = [ ax.plot(np.kaiser(100, beta), color=colormap(norm(beta))) for beta in np.linspace(0, beta_max, N) ] sm = plt.cm.ScalarMappable(cmap=colormap, norm=norm) sm._A = [] plt.colorbar(sm).set_label(r'Кайзер $\\beta$'); 19 / 27\n--- Страница 127 ---\nДополнительные понятия ДПФ  127 Кайзер β Изменяя параметр β, мы можем изменять форму окна из прямоугольного (β = 0, т. е. оконное преобразование отсутствует) в окно, производящее сигна- лы, гладко увеличивающиеся от нуля и уменьшающиеся до нуля в конечных точках выборочного интервала. При этом будут производиться очень низкие боковые лепестки (β, как правило, между 5 и 10) 1. Применяя окно Кайзера, мы видим, что боковые лепестки были сильно со- кращены за счет небольшого расширения в главном лепестке. Эффект оконного преобразования нашего предыдущего примера примеча- телен: win = np.kaiser(len(t), 5) X_win = fftpack.fft(x * win) plt.plot(fftpack.fftfreq(len(t)), np.abs(X_win)) plt.ylim(0, 190); 1 Классические оконные функции включают функции Ханна, Хемминга и Блэкмана. Они различаются по их уровням боковых лепестков и расширением главного ле-пестка (в пространстве Фурье). Современной и гибкой оконной функцией, близкой к оптимальной для большинства приложений, является оконная функция Кайзера – хорошая аппроксимация оптимального вытянутого сферического окна, концентри- рующего большинство энергии в главном лепестке. Путем корректировки параметра b мы можем выполнить тонкую настройку окна Кайзера, чтобы приспособить под конкретное приложение, как проиллюстрировано в основном тексте. 20 / 27\n--- Страница 128 ---\n128  Частота и быстрое преобразование Фурье практическое применение : анализ раДарных Данных В линейно модулированных РЛС непрерывного излучения с частотной моду - ляцией (Frequency-Modulated Continuous-Wave Radars), или FMCW-радарах, алгоритм БПФ широко применяется для обработки сигналов. FMCW-радары обеспечивают примеры самых разнообразных применений БПФ. Мы восполь-зуемся фактическими данными FMCW-радара, чтобы продемонстрировать об-наружение цели (целеуказание). В общем и целом FMCW-радар работает следующим образом (для получения более подробной информации см. раздел «Простая радарная система FMCW» и рис. 4.9): 1. Генерируется сигнал с изменяющейся частотой. Этот сигнал передается антенной, посылающей его от радара вовне. Когда сигнал попадает на объект, часть сигнала отражается обратно на радар. Радар этот сигнал и умножает на копию переданного сигнала. Далее полученный результат сэмплируется (превращается в упакованные в массив числа). Наша зада-ча состоит в том, чтобы проинтерпретировать эти числа и сформировать содержательные результаты. 2. Предыдущий шаг умножения имеет большое значение. Вспомните три-гонометрическое тождество из школьной программы: 3. Так, если умножить принятый на переданный сигнал, получим в спектре две частотные компоненты: разностную между частотами переданного и принятого сигналов и суммарную, состоящую из переданного и при-нятого сигналов. 21 / 27\n--- Страница 129 ---\nПрактическое применение: анализ радарных данных  129 4. Нас в особенности интересует первая, разностная, показывающая время, которое потребуется сигналу для отражения от цели на радар. Мы с по-мощью фильтра нижних частот (фильтр, отсекающий высокочастотную компоненту) отбрасываем другую, суммирующую компоненту. Приемная антеннаГенератор формы волны Компьютер Аналогово- цифровой преобразовательУсилитель передачи Усилитель приемаФильтр нижних частотБлок сопряжения Передача реплики сигнала МикшерПередающая антенна Рис. 4.9  Блок-схема простой радарной системы FMCW Простая радарная система FMCW Блок-схема простой радарной системы FMCW, использующей раздельно передаю- щую и приемную антенны, показана выше. Радар состоит из генератора волны, произ-водящей синусоидальный сигнал. Частота этого сигнала линейно варьируется вокруг требуемой частоты передачи. Сгенерированный сигнал усиливается до нужного уров-ня усилителем передачи и через цепь блока сопряжения направляется в передающую антенну. Обратите внимание, в блоке сопряжения мы отделяем копию сигнала, иду-щего в передающую антенну. Передающая антенна излучает полученный сигнал в на-правлении цели, подлежащей обнаружению. Сигнал излучается в виде узконаправ-ленной электромагнитной волны. Заметьте, любая цель в той или иной мере частично отражает электромагнитную волну, которой он облучается. После встречи электро-магнитной волны с объектом часть облучающей цель энергии отразится и вернется назад в виде вторичной электромагнитной волны. После встречи отраженной от цели электромагнитной волны с приемной антенной эта волна преобразуется в электриче-ский сигнал и направится в микшер. Микшер после умножения отраженного сигнала на копию переданного сигнала произведет синусоидальный сигнал с частотой, равной разнице частот между переданным и приемным сигналами. Этот разностный сигнал поступит на фильтр нижних частот. Фильтр нижних частот отсечет частоты, в которых мы не заинтересованы. Приемный усилитель усилит сигнал до амплитуды, необходи-мой для аналого-цифрового преобразователя (ADC). ADC, в свою очередь, передаст данные в компьютер. 22 / 27\n--- Страница 130 ---\n130  Частота и быстрое преобразование Фурье Подводя итоги, мы должны отметить, что: данные, достигающие компьютера, состоят из N выборок (из умножен- ного и отфильтрованного сигнала) на частоте дискретизации fs; амплитуда возвращенного сигнала варьируется в зависимости от силы отражения (т. е. является свойством целевого объекта и расстояния меж - ду целью и радаром); измеренная частота является признаком расстояния целевого объекта от радара. Чтобы начать анализ радарных (радиолокационных) данных, сгенерируем несколько синтетических сигналов, после чего сосредоточим наше внимание на сигнале, исходящем из фактического радара. Напомним, радар увеличивает свою частоту по мере передачи в размере S Гц/с. По прошествии определенного временного промежутка t частота будет на tS выше (рис. 4.10). В тот же отрезок времени радарный сигнал прошел рас - стояние d = t /v метров, где v – это скорость переданной по воздуху электромаг - нитной волны (примерно такая же, что и скорость света, 3 × 108 м/с). ЧастотаУгол наклона S Гц/с N взятых выборок Время Рис. 4.10  Частотные связи в радаре FMCW с линейной частотной модуляцией Объединяя приведенные выше наблюдения, мы можем вычислить времен- ной промежуток, требующийся сигналу, чтобы пройти, отразиться и вернуться от цели, находящейся на расстоянии R: pi = np.pi # Параметры радара 23 / 27\n--- Страница 131 ---\nПрактическое применение: анализ радарных данных  131 fs = 78125 # Частота дискретизации в герцах, т. е. мы извлекаем # 78125 выборок в секунду ts = 1 / fs # Период дискретизации, т. е. одна выборка # извлекается ts секунд Teff = 2048.0 * ts # Общее время дискретизации для 2048 выборок # (т. н. эффективная длительность развертки) в секундах. Beff = 100e6 # Диапазон частоты проходящего сигнала во время взятия радаром выборок, # т. н. «эффективная полоса частот» (задан в герцах) S = Beff / Teff # Скорость качания частоты в Гц/с # Описания целей. Это выдуманные цели, которые представляют собой # видимые радаром объекты с заданной дальностью и размером. R = np.array([100, 137, 154, 159, 180]) # Дальности (в метрах)M = np.array([0.33, 0.2, 0.9, 0.02, 0.1]) # Размер целиP = np.array([0, pi / 2, pi / 3, pi / 5, pi / 6]) # Случайно отобранные фазовые сдвиги t = np.arange(2048) * ts # Периоды дискретизации fd = 2 * S * R / 3E8 # Разности частот для этих целей # Сгенерировать пять целей signals = np.cos(2 * pi * fd * t[:, np.newaxis] + P) # Сохранить сигнал, связанный с первой целью, в качестве примера # для обследования в дальнейшем v_single = signals[:, 0] # Взвесить сигналы в соответствии с размером и суммы целей, чтобы # сгенерировать комбинированный сигнал, видимый радаром.v_sim = np.sum(M * signals, axis=1) ## Приведенный выше программный код эквивалентен следующему: ## v0 = np.cos(2 * pi * fd[0] * t)# v1 = np.cos(2 * pi * fd[1] * t + pi / 2)# v2 = np.cos(2 * pi * fd[2] * t + pi / 3)# v3 = np.cos(2 * pi * fd[3] * t + pi / 5)# v4 = np.cos(2 * pi * fd[4] * t + pi / 6)### Смешать их вместе# v_single = v0# v_sim = (0.33 * v0) + (0.2 * v1) + (0.9 * v2) + (0.02 * v3) + (0.1 * v4) Здесь мы сгенерировали синтетический сигнал, полученный при рассмо т- рении одиночной цели (см. рис. 4.11). Подсчитав количество циклов, обнару - женных в заданном периоде времени, мы можем вычислить частоту сигнала и, следовательно, расстояние до цели. Вместе с тем реальный радар редко будет получать только одно отражен- ное эхо. Симулируемый сигнал показывает, как выглядит радарный сигнал с пятью целями с различными дальностями (включая две, находящиеся рядом друг с другом на расстоянии 154 и 159 м); показывает выходной сигнал, по-лученный фактическим радаром. Когда мы складываем многочисленные эхо 24 / 27\n--- Страница 132 ---\n132  Частота и быстрое преобразование Фурье вместе, до тех пор, пока мы не проанализируем его более тщательно через линзу ДПФ, результат не будет иметь какого-то интуитивно понятного смысла (рис. 4.11). Время, мс Vфакт(t), В Vсим(t), В Vодин(t), В Рис. 4.11  Выходные сигналы приемника: (a) одиночная симулированная цель, (b) пять симулированных целей и (c) фактические радарные данные Реальные радарные данные читаются в формате NumPy из файла .npz. (Фор- мат NumPy представляет собой легковесный, кросс-платформенный и версион-но-совместимый формат хранения.) Эти файлы могут быть сохранены функция-ми np.savez_compressed или np.savez . Обратите внимание, что подмодуль SciPy io также легко может читать другие форматы, такие как файлы NetCDF и MATLAB. data = np.load('data/radar_scan_0.npz') # Загрузить переменную ‘scan’ из ‘radar_scan_0.npz’ scan = data['scan'] # Набор данных содержит многочисленные замеры, при этом каждый взят, # когда радар был направлен в разные стороны. Здесь мы берем один# такой замер с заданным азимутом (позицией лево-право) и высотой # (позицией верх-низ). Замер имеет форму (2048,). v_actual = scan['samples'][5, 14, :]# Амплитуда сигнала варьируется от –2.5V до +2.5V. 14-разрядный # аналогово-цифровой преобразователь в радаре выдает целые числа # между –8192 и 8192. Мы преобразовываем назад в напряжение путем # умножения $(2.5 / 8192)$. v_actual = v_actual * (2.5 / 8192) 25 / 27\n--- Страница 133 ---\nПрактическое применение: анализ радарных данных  133 Поскольку файлы .npz могут хранить многочисленные переменные, нам не- обходимо отобрать одну, которая нам нужна: data['scan'] . В результате будет возвращен структурированный массив NumPy со следующими полями: time Беззнаковое 64-разрядное (8-байтовое) целое число ( np.uint64 ). size Беззнаковое 32-разрядное (4-байтовое) целое число ( np.uint32 ). position az 32-разрядное вещественное ( np.float32 ). el 32-разрядное вещественное ( np.float32 ). region_type Беззнаковое 8-разрядное (1-байтовое) целое число ( np.uint8 ). region_ID Беззнаковое 16-разрядное (2-байтовое) целое число ( np.uint16 ). gain Беззнаковое 8-разрядное (1-байтовое) целое число ( np.uin8 ). samples 2048 беззнаковых 16-разрядных (2-байтовых) целых чисел ( np.uint16 ). Хотя утверждение, что массивы NumPy гомогенные, верно (т. е. тип всех эле- ментов внутри него одинаков), это не означает, что это не составные элементы, как было здесь. Доступ к отдельному полю осуществляется на основе словарного синтак - сиса: azimuths = scan['position']['az'] # Получить все замеры азимута Резюмируем: все, что мы видели до сих пор, – показанные замеры vсим (и vфакт) являются суммой синусоидальных сигналов, отраженных каждым из несколь-ких объектов. Нам нужно определить каждую составляющую компоненту этих составных радарных сигналов. БПФ – это как раз тот инструмент, который сде-лает это за нас. Свойства сигнала в частотной области Прежде всего мы выполняем БПФ-преобразование наших трех сигналов (синтетического с единственной целью, синтетического с мультицелью и реального) и затем показываем компоненты с положительной частотой (т. е. компоненты от 0 до N/2; см. рис. 4.12). В терминологии радаров они на- зываются трассировками дальности (range traces, или сопровождениями по дальности). 26 / 27\n--- Страница 134 ---\n134  Частота и быстрое преобразование Фурье fig, axes = plt.subplots(3, 1, sharex=True, figsize=(4.8, 2.4)) # Взять БПФ наших сигналов. Обратите внимание на правило именования # БПФ-преобразований с заглавной буквы. V_single = np.fft.fft(v_single) V_sim = np.fft.fft(v_sim)V_actual = np.fft.fft(v_actual) N = len(V_single) with plt.style.context('style/thinner.mplstyle'): axes[0].plot(np.abs(V_single[:N // 2])) axes[0].set_ylabel(\"$|V_\\mathrm{один}|$\") axes[0].set_xlim(0, N // 2) axes[0].set_ylim(0, 1100) axes[1].plot(np.abs(V_sim[:N // 2])) axes[1].set_ylabel(\"$|V_\\mathrm{сим} |$\") axes[1].set_ylim(0, 1000) axes[2].plot(np.abs(V_actual[:N // 2])) axes[2].set_ylim(0, 750) axes[2].set_ylabel(\"$|V_\\mathrm{факт}|$\") axes[2].set_xlabel(\"БПФ-компоненты $n$\") for ax in axes: ax.grid(False) |Vфакт| |Vсим| |Vодин| БПФ-компоненты n Рис. 4.12  Трассировки дальности для: (a) одиночной симулируемой цели, (b) многочисленных симулируемых целей и (c) реальных целей И внезапно информация приобретает смысл! График для |V0| ясно показывает цель в компоненте 67 и для |Vсим| показывает цели, созданные сигналом, которые не поддавались интерпретации во времен-ной области. Реальный радарный сигнал |V факт| показывает большое количество целей между компонентами 400 и 500 с большим пиком в компоненте 443. Это оказывается эхо-сигналом из радара, просвечивающего откос открытой гор-норудной разработки. Powered by TCPDF (www.tcpdf.org) 27 / 27\n--- Страница 135 ---\nПрактическое применение: анализ радарных данных  135 Чтобы получить полезную информацию из графика, мы должны определить дальность! И снова мы используем формулу: В терминологии радаров каждая компонента ДПФ называется интервалом дальности (range bin, или элементом разрешения по дальности). Это уравнение также определяет разрешающую способность радара по даль- ности: цели будут различаться, только если они отстоят более чем на два ин- тервала дальности. Например: Это фундаментальное свойство всех типов радаров.Мы получили удовлетворительный результат – но динамический диапазон столь большой, что мы очень легко могли пропустить некоторые пики. Давай-те, как и ранее, возьмем логарифм спектрограммы: c = 3e8 # Приблизительно скорость света и # электромагнитных волн в воздухе fig, (ax0, ax1, ax2) = plt.subplots(3, 1) def dB(y): \"Вычислить логарифмическое соотношение y / max(y) в децибелах.\" y = np.abs(y) y /= y.max() return 20 * np.log10(y) def log_plot_normalized(x, y, ylabel, ax): ax.plot(x, dB(y)) ax.set_ylabel(ylabel) ax.grid(False) rng = np.arange(N // 2) * c / 2 / Beff with plt.style.context('style/thinner.mplstyle'): log_plot_normalized(rng, V_single[:N // 2], \"$|V_0|$ [дБ]\", ax0) log_plot_normalized(rng, V_sim[:N // 2], \"$|V_5|$ [дБ]\", ax1) log_plot_normalized(rng, V_actual[:N // 2], \"$|V_{\\mathrm{факт}}|$ [дБ]\", ax2) ax0.set_xlim(0, 300) # Для этих графиков изменить границы x, чтобы ax1.set_xlim(0, 300) # можно было лучше увидеть форму пиков.ax2.set_xlim(0, len(V_actual) // 2)ax2.set_xlabel('дальность') 1 / 27\n--- Страница 136 ---\n136  Частота и быстрое преобразование Фурье дальность|Vфакт| [дБ] |V5| [дБ] |V0| [дБ] На этих графиках наблюдаемый динамический диапазон стал намного луч- ше. Например, в реальном радарном сигнале уровень собственных шумов ра- дара стал видимым (т. е. уровень, где электронный шум в системе начинает ограничивать способность радара обнаруживать цель). Оконное преобразование на практике Мы почти у цели. Однако в спектре симулируемого сигнала мы по-прежнему не можем различить пики на 154 и 159 метрах. Кто знает, что бы мы пропусти-ли в реальном сигнале! Чтобы заострить пики, мы вернемся к нашему ком-плекту инструментов и применим оконное преобразование. Вот сигналы, которые до настоящего времени использовались в этом при- мере, преобразованные окном Кайзера при β = 6.1: f, axes = plt.subplots(3, 1, sharex=True, figsize=(4.8, 2.8)) t_ms = t * 1000 # Периоды дискретизации в миллисекундах w = np.kaiser(N, 6.1) # Окно Кайзера при beta = 6.1 for n, (signal, label) in enumerate([(v_single, r'$v_0 [V]$'), (v_sim, r'$v_5 [V]$'), (v_actual, r'$v_{\\mathrm{факт}} [V]$')]): with plt.style.context('style/thinner.mplstyle'): axes[n].plot(t_ms, w * signal) 2 / 27\n--- Страница 137 ---\nПрактическое применение: анализ радарных данных  137 axes[n].set_ylabel(label) axes[n].grid() axes[2].set_xlim(0, t_ms[-1]) axes[2].set_xlabel('Время [мс]'); Время [мс]|Vфакт| [дБ] |V5| [дБ] |V0| [дБ] И соответствующие БПФ-преобразования, или «трассировки дальности» в терминах радаров: V_single_win = np.fft.fft(w * v_single)V_sim_win = np.fft.fft(w * v_sim)V_actual_win = np.fft.fft(w * v_actual) fig, (ax0, ax1,ax2) = plt.subplots(3, 1) with plt.style.context('style/thinner.mplstyle'): log_plot_normalized(rng, V_single_win[:N // 2], r\"$|V_{0,\\mathrm{win}}|$ [dB]\", ax0) log_plot_normalized(rng, V_sim_win[:N // 2], r\"$|V_{5,\\mathrm{win}}|$ [dB]\", ax1) log_plot_normalized(rng, V_actual_win[:N // 2], r\"$|V_\\mathrm{actual,win}|$ [dB]\", ax2) ax0.set_xlim(0, 300) # Для этих графиков изменить границы x, чтобы ax1.set_xlim(0, 300) # можно было лучше увидеть форму пиков. ax1.annotate(\"Новый, ранее не встречавшийся!\", (160, -45), xytext=(10, 15), textcoords=\"offset points\", color='red', size='x-small', arrowprops=dict(width=0.5, headwidth=3, headlength=4, fc='k', shrink=0.1)); 3 / 27\n--- Страница 138 ---\n138  Частота и быстрое преобразование Фурье Новый, ранее не встречавшийся!|Vфакт| [дБ] |V5| [дБ] |V0| [дБ] Сравните их с более ранними трассировками дальности. Имеется сильней- шее понижение в уровне боковых лепестков. Но за счет изменения пиков по форме пики расширились и стали более тупыми. Следовательно, снизилась разрешающая способность радара, т. е. способность радара различать между двумя близко расположенными целями. Выбор окна является компромиссом между уровнем боковых лепестков и разрешающей способностью. Несмотря на это, обращаясь к трассировке для V сим, оконное преобразование сущест - венно увеличило нашу способность отличать небольшую цель от ее крупного соседа. В трассировке дальности реальных радарных данных оконное преобразова- ние также уменьшило боковые лепестки. Это особенно хорошо видно по глу - бине зазубрины между двумя группами целей. Радарные изображения Сведения о том, как проводить анализ одиночной трассировки, позволяют рас - ширить эту методологию до рассмотрения радарных изображений. Данные производятся радаром с параболической отражающей антенной. Она производит остронаправленный круговой карандашный луч с двухградус - ным углом расходимости между точками половинной мощности. При направ-ленности с нормальным падением на поверхность радар будет просвечивать пятно диаметром порядка 2 м на расстоянии 60 м. Вне этого пятна мощность довольно быстро понижается, однако сильные эхо-сигналы снаружи пятна тем не менее будут по-прежнему видимы. Изменяя азимут карандашного луча (позицию лево-право) и высоту (по- зицию верх-низ), мы можем распространить его на интересующую целевую 4 / 27\n--- Страница 139 ---\nПрактическое применение: анализ радарных данных  139 область. Поймав отражения, получим возможность вычислить расстояние до отражателя (объекта, в который попал радарный сигнал). Текущий азимут и высота карандашного луча определяют положение отражателя в трехмерном измерении. Склон скалы состоит из тысяч отражателей. Интервал дальности можно представить в виде большой сферы с радаром в центре, пересекающем склон вдоль рваной линии. Рассеиватели на данной линии будут производить от - ражения в этом интервале дальности. Длина волны радара (расстояние, ко-торое передаваемая волна проходит за одну секунду осциляции) составляет приблизительно 30 мм. Отражения от рассеивателей, отделенных нечетными кратными четверти длины волны, порядка 7.5 мм будут создавать деструктив-ную интерференцию в радаре, в то время как от рассеивателей, отделенных кратными половины длины волны, будут создавать конструктивную интерфе-ренцию. Чтобы произвести различимые пятна сильных отражений, отражения объединяются. Этот конкретный радар перемещает свою антенну так, чтобы сканировать малые области, состоящие из интервалов с 20-градусным азиму - том и 30-градусной высотой, сканируемых с шагом по 0.5 градуса. Теперь мы создадим несколько контурных графиков, результирующих ра- дарные данные. Обратитесь к рис. 4.13, чтобы увидеть, каким образом берутся различные срезы. Первый срез с фиксированной дальностью показывает мощ-ность эхо-сигналов относительно высоты и азимута. Еще два среза при фикси-рованных высоте и азимуте, соответственно, показывают склон (см. рис. 4.13 и 4.14). Ступенчатая конструкция откоса открытой горнорудной разработки видима в азимутной плоскости. Высота АзимутДальностьСрез дальности Срез азимута Срез высоты Рис. 4.13  Диаграмма, показывающая срезы азимута, высоты и дальности через объем данных 5 / 27\n--- Страница 140 ---\n140  Частота и быстрое преобразование Фурье data = np.load('data/radar_scan_1.npz') scan = data['scan'] # Амплитуда сигнала варьируется от –2.5V до +2.5V. 14-разрядный # аналогово-цифровой преобразователь в радаре выдает целые числа # между –8192 и 8192. Мы преобразуем назад в напряжение путем # умножения $(2.5 / 8192)$. v = scan['samples'] * 2.5 / 8192 win = np.hanning(N + 1)[:-1] # Взять БПФ для каждого замера V = np.fft.fft(v * win, axis=2)[::-1, :, :N // 2] contours = np.arange(-40, 1, 2) # игнорировать предупреждения matplotlib import warningswarnings.filterwarnings('ignore', '.*Axes.*compatible.*tight_layout.*') f, axes = plt.subplots(2, 2, figsize=(4.8, 4.8), tight_layout=True) labels = ('Дальность', 'Азимут', 'Высота') def plot_slice(ax, radar_slice, title, xlabel, ylabel): ax.contourf(dB(radar_slice), contours, cmap='magma_r') ax.set_title(title) ax.set_xlabel(xlabel) ax.set_ylabel(ylabel) ax.set_facecolor(plt.cm.magma_r(-40)) with plt.style.context('style/thinner.mplstyle'): plot_slice(axes[0, 0], V[:, :, 250], 'Дальность=250', 'Азимут', 'Высота') plot_slice(axes[0, 1], V[:, 3, :], 'Азимут=3', 'Дальность', 'Высота') plot_slice(axes[1, 0], V[6, :, :].T, 'Высота=6', 'Азимут', 'Дальность') axes[1, 1].axis('off') Трехмерная визуализация Мы также можем визуализировать объем в трех измерениях (рис. 4.15). Сначала вычисляем argmax (индекс максимального значения) в направ- лении дальности. Это должно дать представление о дальности, с которой ра- дарный луч попал на скальный склон. Каждый индекс argmax преобразуется в трехмерную координату (дальности-азимута-высоты): r = np.argmax(V, axis=2) el, az = np.meshgrid(*[ np.arange(s) for s in r.shape], indexing='ij') axis_labels = ['Высота', 'Азимут', 'Дальность'] coords = np.column_stack((el.flat, az.flat, r.flat)) Беря эти координаты, мы проецируем их на плоскость азимута-высоты (пропуская координату дальности) и выполняем тесселяцию Делоне. Тессе- ляция возвращает набор индексов на наши координаты, которые определяют треугольники (или симплексы). Хотя треугольники, строго говоря, определены 6 / 27\n--- Страница 141 ---\nПрактическое применение: анализ радарных данных  141 на спроецированных координатах, мы используем наши исходные координа- ты для реконструкции, для чего добавляем назад компонент дальности: from scipy import spatial d = spatial.Delaunay(coords[:, :2]) simplexes = coords[d.vertices] Дальность = 250 Азимут АзимутАзимут = 3 ДальностьДальностьВысота Высота Высота = 6 Рис. 4.14  Контурные графики трассировок дальностей вдоль различных осей (см. рис. 4.13) Для демонстрационных целей мы меняем местами ось диапазона, делая его первым: coords = np.roll(coords, shift=-1, axis=1) axis_labels = np.roll(axis_labels, shift=-1) Теперь функция Matplotlib trisurf может использоваться для визуализации результата: # Эта строка импорта инициализирует трехмерный механизм Matplotlibfrom mpl_toolkits.mplot3d import Axes3D 7 / 27\n--- Страница 142 ---\n142  Частота и быстрое преобразование Фурье # Задать трехмерную ось f, ax = plt.subplots(1, 1, figsize=(4.8, 4.8), subplot_kw=dict(projection='3d')) with plt.style.context('style/thinner.mplstyle'): ax.plot_trisurf(*coords.T, triangles=d.vertices, cmap='magma_r') ax.set_xlabel(axis_labels[0]) ax.set_ylabel(axis_labels[1]) ax.set_zlabel(axis_labels[2], labelpad=-3) ax.set_xticks([0, 5, 10, 15]) # Скорректировать позицию камеры, чтобы она совпадала с нашей диаграммой выше ax.view_init(azim=-50); Азимут Дальность Высота Рис. 4.15  Трехмерная визуализация положения предполагаемого скального склона Дополнительные применения БПФ Предыдущие примеры показывают только одно из применений БПФ в радаре. Кроме него, существует целый ряд других, таких как (допплеровское) изме-рение движения и распознавание цели. БПФ вездесущ и встречается везде, от МРТ до статистики. Имея на руках базовые методы, которые кратко были об-рисованы в этой главе, вы теперь в достаточной мере оснащены, чтобы начать его использовать! 8 / 27\n--- Страница 143 ---\nПрактическое применение: анализ радарных данных  143 Дополнительные материалы для чтения По преобразованию Фурье: Популис А. Интеграл Фурье и его применения (Athanasios Papoulis. The Fourier Integral and Its Applications. New York: McGraw-Hill, 1960); Брейсуэлл Р. А. Преобразование Фурье и его применения (Ronald A. Bra- ce well. The Fourier Transform and Its Applications. New York: McGraw-Hill, 1986). По обработке радарных сигналов: Ричардс М. А., Шир Дж. А., Холм У. А. Принципы современного радара: Основные принципы (Mark A. Richards, James A. Scheer, and William A. Holm, eds. Principles of Modern Radar: Basic Principles, Raleigh. NC: SciTech, 2010); Ричардс М. А. Основные принципы обработки радарных сигналов (Mark A. Ri chards. Fundamentals of Radar Signal Processing. New York: McGraw Hill, 2014). Задача: свертывание изображения БПФ часто используется для ускорения свертывания изображения (свертыва-ние, или конволюция, связано с применением скользящего фильтра). Свер-ните изображение при помощи np.ones((5, 5)), используя a) функцию NumPy np.convolve и b) функцию np.fft.fft2 . Подтвердите, что результаты идентичны. Подсказки: свертывание x и y эквивалентно ifft2(X * Y), где X и Y – это БПФ- преобразования соответственно x и y; для того чтобы умножить X и Y, они должны иметь одинаковый раз- мер. Примените функцию np.pad , чтобы расширить x и y нулями (вправо и вниз) перед взятием их БПФ-преобразования; вы можете увидеть некоторые краевые эффекты. Их можно удалить, уве-личив размер дополнения так, чтобы x и у имели размерности shape(x) + shape(y) – 1. Обратите внимание: решение задачи «Свертывание изображений» вы най- дете в конце книги. 9 / 27\n",
      "debug": {
        "start_page": 107,
        "end_page": 143
      }
    },
    {
      "name": "Глава 5 Таблицы сопряженности на основе разреженных координатных матриц",
      "content": "Глава 5 Таблицы сопряженности на основе разреженных координатных матриц Мне нравится разреженность. Что-то есть в этой минималистской атмосфере, которая заставляет нечто оказывать непосредственное влияние и делает его уникальным. Я бы, навер-ное, всегда работал с этой формулой. Я просто не знаю, как. – Бритт Дэниел, солист группы Spoon Многие матрицы, используемые в реальных условиях, являются разреженны-ми. Имеется в виду, что большинство их значений равняется нулю. При использовании массивов NumPy для управления разреженными матрица- ми много времени и энергии тратится впустую на умножение огромного количе-ства значений на 0. Вместо этого мы можем воспользоваться модулем SciPy sparse , позволяющим, исследуя только ненулевые значения, эффективно вычислять та-кие матрицы. В дополнение, что этот модуль способствует разрешению таких «канонических» разреженно-матричных проблем, модуль sparse может исполь- зоваться для задач, связанных очевидным образом с разреженными матрицами. Сравнение сегментаций изображения является одной из таких задач. (Об- ратитесь к главе 3 относительно определения понятия «сегментация».) Пример программного кода, мотивирующий эту главу, дважды использует разреженные матрицы. Сначала для вычисления матрицы сопряженности мы используем программный код, предложенный Андреасом Мюллером (Andreas Mueller). Эта матрица подсчитывает соответствие меток между двумя сегмен-тациями. Затем, по предложению Хайме Фернандеса дель Рио и Уоррена Ве-кессера (Jaime Fernández del Río и Warren Weckesser), мы для вычисления из- менчивости информации используем матрицу сопряженности, измеряющей разницу между сегментациями. 10 / 27\n--- Страница 145 ---\nТаблицы сопряженности на основе разреженных координатных матриц  145 def variation_of_information(x, y): # Вычислить матрицу сопряженности, т. н. матрицу совместной вероятности n = x.size Pxy = sparse.coo_matrix((np.full(n, 1/n), (x.ravel(), y.ravel())), dtype=float).tocsr() # Вычислить маргинальные вероятности, преобразовав в одномерный массив px = np.ravel(Pxy.sum(axis=1)) py = np.ravel(Pxy.sum(axis=0)) # Использовать линейную алгебру разреженных матриц, чтобы сначала # вычислить изменчивость информации (VI), # вычислить обратные диагональные матрицы Px_inv = sparse.diags(invert_nonzero(px)) Py_inv = sparse.diags(invert_nonzero(py)) # затем вычислить энтропии hygx = px @ xlog1x(Px_inv @ Pxy).sum(axis=1) hxgy = xlog1x(Pxy @ Py_inv).sum(axis=0) @ py # Вернуть их сумму return float(hygx + hxgy) Профессиональный совет по Python 3.5! Символы @ в приведенном выше абзаце представляют собой оператор умножения матриц и были введены в Python 3.5 в 2015 г. Для научных программистов, чтобы ис - пользовать Python 3, это один из самых востребованных аргументов: он позволяет про- граммировать линейно-алгебраические алгоритмы, используя программный код, кото-рый остается очень близким к исходному математическому аппарату. Сравните строку из приведенного выше программного кода: hygx = px @ xlog1x(Px_inv @ Pxy).sum(axis=1) с эквивалентом на Python 2: hygx = px.dot(xlog1x(Px_inv.dot(Pxy)).sum(axis=1)) При помощи оператора @, позволяющего оставаться максимально близко к форме математической записи, мы избегаем ошибок реализации и производим программный код, который намного легче читается. На самом деле авторы библиотеки SciPy учли это задолго до того, как оператор @ был введен, и фактически изменили значение оператора *, когда входными данными являются матрицы SciPy. Этот оператор существует в Python 2.7 и позволяет создавать хороший, удобочитаемый программный код, как и показанный выше: hygx = -px * xlog(Px_inv * Pxy).sum(axis=1) Однако есть одна большая сложность: этот фрагмент кода будет вести себя по-другому, когда px или Px_inv станут матрицами SciPy, когда они таковыми не являются! Если Px_inv и Pxy являются массивами NumPy, то оператор * производит поэлементное умножение, а если они являются матрицами SciPy, то он производит матричное умножение! Как вы можете справедливо предположить, эта разница является источником очень многих ошибок, и большая часть сообщества SciPy отказалась от его использования в пользу более ужасного, но однозначного метода .dot. Оператор @ языка Python версии 3.5 предлагает нам лучший из обоих методов! 11 / 27\n--- Страница 146 ---\n146  Таблицы сопряженности на основе разреженных координатных матриц таблицы сопряженности Но давайте начнем с простого и займемся сегментациями. Предположим, что вы только что начали работать аналитиком данных в стартапе Spam-o-matic, занимающимся электронными сообщениями. Вам поручена разработка детектора спама. Вы кодируете выход из детектора в виде числового значения, назначая 0 для неспамных сообщений и 1 для спамных. Если вы хотите разбить набор из 10 электронных сообщений на классы, то в итоге вы получите вектор предсказаний: import numpy as np pred = np.array([0, 1, 0, 0, 1, 1, 1, 0, 1, 1]) Вы можете проверить результативность работы детектора, сравнив этот век - тор с контрольным вектором, т. е. результатом классификации, полученным в результате обследования каждого сообщения вручную1. gt = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) Вообще, для компьютеров задача классификации представляет трудность. Так, значения в массивах pred и gt не полностью совпадают. В тех позициях, где pred равняется 0, и gt тоже равняется 0, предиктор правильно идентифициро- вал сообщение как неспамное. Такой исход называется истинно отрицатель- ным. С другой стороны, в позициях, где оба значения равняются 1, предиктор правильно идентифицировал спамное сообщение и получил истинно положи- тельный исход. Далее. Есть два вида ошибок. Если мы допустили попадание спамного со- общения (где gt равняется 1) в почтовый ящик входящих сообщений поль- зователя ( pred равняется 0), то мы совершили ложноотрицательную ошиб- ку. При предсказании, что допустимое сообщение ( gt 0) является спамным (pred равняется 1), будет сделано ложноположительное предсказание. (Электронное сообщение, пришедшее от директора моего научно-исследо-вательского института, однажды попало в мою папку для спама. И причина состояла в том, что его объявление о конкурсе работ, участвовавших в за-щите кандидатской диссертации, начиналось со слов «Вы можете получить приз $500!».) Чтобы измерить результативность работы детектора, следует подсчитать приведенные выше виды ошибок, используя матрицу сопряженности. (Иногда ее также называют матрицей ошибок, что вполне соответствует ее сути.) Для этого мы помещаем метки предсказания вдоль строк и контрольные метки вдоль столбцов. Затем подсчитываем число совпадений. Например, если име-ется 4 истинно положительных исхода (где pred и gt оба равняются 1), матрица будет иметь значение 3 в позиции (1, 1). 1 В англоязычной терминологии vector of ground truth, т. е. вектор данных полевых ис - следований. – Прим. перев. 12 / 27\n--- Страница 147 ---\nТаблицы сопряженности  147 В общем случае: Ci,j = ∑k(pk = i)(gk = j). Вот интуитивно понятный, но неэффективный способ реализовать приве- денное выше уравнение: def confusion_matrix(pred, gt): cont = np.zeros((2, 2)) for i in [0, 1]: for j in [0, 1]: cont[i, j] = np.sum((pred == i) & (gt == j)) return cont Можно проверить, что эта функция дает правильные количества: confusion_matrix(pred, gt) array([[ 3., 1.], [ 2., 4.]]) Задача: вычислительная сложность матриц ошибок Почему мы назвали этот программный код неэффективным? Смотрите реше- ние задачи «Вычислительная сложность матриц ошибок» в конце книги. Задача: альтернативный алгоритм вычисления матрицы ошибок Напишите альтернативный способ вычисления матрицы ошибок, который вы-полняет всего один обход векторов pred и gt. def confusion_matrix1(pred, gt): cont = np.zeros((2, 2)) # здесь идет ваш программный код return cont Обратите внимание на решение задачи «Альтернативный алгоритм вычис - ления матрицы ошибок» в конце книги. Этот пример можно слегка обобщить. Вместо классификации на спам и не спам мы можем классифицировать спам, информационные бюллетени, акции по распродаже и стимулированию продаж, списки рассылок и личные сообще-ния. Получим 5 категорий, которые пометим от 0 до 4. Матрица ошибок теперь будет иметь размер 5 на 5, в которой совпадения будут подсчитываться в диа-гональных ячейках, а ошибки – во внедиагональных ячейках. Определение приведенной выше функции confusion_matrix плохо масштаби- руется на более крупную матрицу, так как теперь мы должны выполнить 25 об-ходов массивов результирующих и контрольных данных. Эта проблема будет только нарастать по мере добавления новых категорий почтовых сообщений, таких как уведомления социальных сетей. 13 / 27\n--- Страница 148 ---\n148  Таблицы сопряженности на основе разреженных координатных матриц Задача: мультиклассовая матрица ошибок Как и в задании выше, напишите функцию вычисления матрицы ошибок за один проход. Но теперь вместо принятия двух категорий она должна логически выводить количество категорий на основе входных данных. def general_confusion_matrix(pred, gt): n_classes = None # заменить `None` на что-то полезное # здесь идет ваш программный код return cont Ваше однопроходное решение хорошо масштабируется в зависимости от ко- личества классов. Но, так как цикл for выполняется в интерпретаторе Python, при большом количестве документов это решение станет медленным. Учиты- вая, что некоторые классы легко спутать с другими, матрица будет разреженной и иметь много нулевых записей. И действительно, по мере увеличения коли-чества классов выделение растущего пространства оперативной памяти под нулевые записи матрицы сопряженности станет наиболее расточительным. Вместо этого мы можем воспользоваться модулем SciPy sparse , который содер- жит объекты для эффективного представления разреженных матриц. форматы Данных моДуля SciP y.SParSe Мы в главе 1 затронули внутренний формат данных массивов NumPy и наде-емся, что он интуитивно понятен и в некотором смысле представляет собой неизбежный формат для хранения данных n-мерных массивов. На самом деле для разреженных матриц имеется огромное количество возможных форма-тов, и «правильный» формат зависит от решаемой задачи. Мы рассмотрим два широко применяемых формата. Чтобы получить полный список, обратитесь к сравнительной таблице, приводимой далее в этой главе, или к онлайн-до-кументации по модулю scipy.sparse . Формат COO Возможно, наиболее интуитивно понятным является координатный формат, или формат COO. Для представления двумерной матрицы A в нем использу - ется три одномерных массива. А именно каждый такой массив имеет длину, равную количеству ненулевых значений в A, и вместе они формируют пере- чень координат в формате (i, j, значение) каждой записи, которая не равна 0. Массивы row и col, вместе задающие позицию каждой ненулевой записи (индексы соответствуют индексам строки и столбца). Массив data, задающий значение в каждой из этих позиций. Каждая часть матрицы, которая не представлена парами (row, col), считает - ся равной 0. Это гораздо эффективнее! Поэтому, чтобы представить матрицу s = np.array([[ 4, 0, 3], [ 0, 32, 0]], dtype=float) 14 / 27\n--- Страница 149 ---\nФорматы данных модуля scipy.sparse  149 мы можем сделать следующее: from scipy import sparse data = np.array([4, 3, 32], dtype=float) row = np.array([0, 0, 1])col = np.array([0, 2, 1]) s_coo = sparse.coo_matrix((data, (row, col))) Метод .toarray() каждого разреженного формата в модуле scipy.sparse воз- вращает представление разреженных данных в виде массива NumPy. Его можно применять для проверки правильности создания разреженного массива s_coo : s_coo.toarray() array([[ 4., 0., 3.], [ 0., 32., 0.]]) Таким же образом мы можем воспользоваться свойством .A, которое очень похоже на атрибут, но на самом деле исполняет функцию. Свойство .A пред- ставляет собой исключительно опасное свойство, потому что за ним может скрываться потенциально очень емкая операция: плотная версия разрежен-ной матрицы может быть на порядок больше разреженной матрицы как тако-вой, ставя компьютер на колени всего тремя нажатиями клавиш! s_coo.A array([[ 4., 0., 3.], [ 0., 32., 0.]]) В этой главе, как и в других случаях, если он не ухудшает удобочитаемость, мы рекомендуем использовать метод toarray() , так как он яснее сигнализиру - ет о потенциально дорогостоящей операции. Однако мы будем использовать свойство .A там, где оно упрощает восприятие программного кода благодаря своей краткости (например, при реализации последовательности математи-ческих уравнений). Задача: представление в формате COO Напишите следующую ниже матрицу в формате COO: s2 = np.array([[0, 0, 6, 0, 0], [1, 2, 0, 4, 5], [0, 1, 0, 0, 0], [9, 0, 0, 0, 0], [0, 0, 0, 6, 7]]) К сожалению, несмотря на то что формат COO интуитивно понятен, он не очень оптимизирован для минимизации потребляемого объема оперативной памяти или для оптимизации скорости обхода массива во время вычисления. (Из главы 1 вы помните, что для эффективных вычислений очень важное зна-чение имеет сосредоточенность данных!) Однако вы можете взглянуть на ваше 15 / 27\n--- Страница 150 ---\n150  Таблицы сопряженности на основе разреженных координатных матриц представление в формате COO и идентифицировать избыточную информа- цию. Обратите внимание на повторяющиеся единицы. Формат сжатой разреженной строки Если мы используем COO не для того, чтобы перечислить ненулевые записи в произвольном порядке (которое этот формат допускает), а для того, чтобы их перечислить построчно, мы в итоге получим множество последовательных повторяющихся значений в массиве row. Вместо того чтобы неоднократно за- писывать индекс строки, эти значения можно сжать, указывая в col индексы, где начинается следующая строка. На этом основывается формат сжатой раз- реженной строки, или формат CSR (Compressed Sparse Row). Давайте проанализируем приведенный выше пример. В формате CSR мас - сивы col и data неизменны (но col переименовывается в indices ). Вместе с тем массив row, вместо чтобы указывать на строки, указывает место, где в col начи- нается каждая строка, и переименовывается в indptr , т. е. в «указатель индекса». Рассмотрим row и col в формате COO без учета data: row = [0, 1, 1, 1, 1, 2, 3, 4, 4] col = [2, 0, 1, 3, 4, 1, 0, 3, 4] Каждая новая строка начинается в индексе, где изменяется значение row. Ну- левая строка начинается в индексе 0, а первая строка начинается в индексе 1, но вторая строка начинается там, где в строке row в первый раз появляется «2», т. е. в индексе 5. Затем индексы увеличиваются на 1 для строк 3 и 4 до 6 и 7. За-ключительный индекс, указывающий на конец матрицы, представляет собой общее количество ненулевых значений (9). Поэтому: indptr = [0, 1, 5, 6, 7, 9] Давайте применим эти вычисленные вручную массивы, чтобы построить матрицу CSR в SciPy. Мы можем проверить нашу работу, сравнив свойства .A наших представлений в формате COO и CSR с массивом NumPy s2, который мы определили ранее. data = np.array([6, 1, 2, 4, 5, 1, 9, 6, 7]) coo = sparse.coo_matrix((data, (row, col))) csr = sparse.csr_matrix((data, col, indptr)) print('Массивы COO и CSR эквивалентны: ', np.all(coo.A == csr.A))print('Массивы CSR и NumPy эквивалентны: ', np.all(s2 == csr.A)) Массивы COO и CSR эквивалентны: True Массивы CSR и NumPy эквивалентны: True Способность хранить большие разреженные матрицы и выполнять с ними вычисления имеет невероятную силу и может применяться во многих пред- метных областях. 16 / 27\n--- Страница 151 ---\nФорматы данных модуля scipy.sparse  151bsr_matrix coo_matrix csc_matrix csr_matrix dia_matrix dok_matrix lil_matrixПолное имяБлочная разреженная строка (BSR)Координатная матрица (COO)Сжатый разреженный столбец (CSC)Сжатая разреженная строка (CSR)Диагональная матрица (DIA)Словарь ключей (DOK)Построчный связный список (LIL)Случаи примененияХранение плотных подматриц.Часто применя-ется в численном анализе дискрети-зированных задач, таких как конеч-ные элементы, дифференциаль-ные уравненияБыстрый и прямо-линейный способ создания разре-женных матриц.Во время созда-ния дублирующие координаты сум-мируются – напри-мер, полезно для анализа конечных элементовАрифметические операции(поддержка сло-жения, вычита-ния, умножения, деления и степени матрицы).Гибкая нарезка столбцов.Быстрые матрич-но-векторные произведения (CSR, BSR могут быть быстрее, в зависимости от задачи)Арифметические операции.Эффективная на-резка строк.Быстрые матрич-но-векторные произведенияАрифметические операцииНедорогостоящее внесение изме-нений в структуру разреженности.Арифметиче-ские операции. Быстрый доступ к индивидуаль-ным элементам.Эффективное преобразование в COO (но дубли-каты недопустимы)Недорогостоящее внесение изме-нений в структуру разреженности.Гибкая нарезкаНедостаткиНет арифметиче-ских операций.Нет срезовМедленная нарез-ка строк (см. CSR).Дорогостоящее внесение изме-нений в структуру разреженности (см. LIL, DOK)Медленная на-резка столбцов (см. CSC).Дорогостоящее внесение изме-нений в структуру разреженности (see LIL, DOK)Структура разре-женности ограни-чена значениями по диагоналямДорогостоящие арифметические операции.Медленное мат - рично-векторное произведениеДорогостоящие арифметические операции.Медленная нарез-ка столбцов.Медленное мат - рично-векторное произведение 17 / 27\n--- Страница 152 ---\n152  Таблицы сопряженности на основе разреженных координатных матриц Например, всю Мировую паутину можно представить как большую раз- реженную матрицу размера N × N. Каждая запись Xij указывает на то, связана ли веб-страница i со страницей j. Нормализовав эту матрицу и найдя ее до- минирующий собственный вектор, можно получить так называемую меру PageRank – одно из чисел, которое поисковик Google использует для упорядо-чивания результатов вашего поискового запроса. (В следующей главе вы узна-ете о ней подробнее.) В качестве еще одного примера мы можем представить человеческий мозг как большой граф размера m × m, где есть m узлов (позиций), в которых вы из- меряете активность, используя МРТ-сканер. Через какое-то время после сня-тия измерений могут быть вычислены корреляции и введены в матрицу C ij. Пороговая обработка этой матрицы порождает разреженную матрицу, со-стоящую из единиц и нулей. Собственный вектор, соответствующий второму самому маленькому собственному значению этой матрицы, подразделяет m мозговых областей в подгруппы, которые, как оказывается, часто связаны с функциональными областями мозга 1! применения разреженных матриц : преобразоВания изображений Такие библиотеки, как scikit-image и SciPy, уже содержат алгоритмы эффек - тивного преобразования (поворота и деформирования) изображений, но что, если вы являетесь главой Агентства NumPy по космическим делам и должны вращать миллионы изображений, поступающих потоком от недавно запущен-ного орбитального аппарата Jupyter? В таких случаях вы захотите выжать из своего компьютера каждую каплю производительности. Оказывается, что мы можем получить намного лучшую результативность, чем даже оптимизированный код на C в модуле SciPy ndimage , если будем многократно применять одинаковое преобразование. В качестве примера данных мы используем приведенное ниже тестовое изо- бражение кионооператора из scikit-image: # Заставить графики появляться локально, задать индивидуальный стиль графиков %matplotlib inlineimport matplotlib.pyplot as pltplt.style.use('style/elegant.mplstyle') from skimage import data image = data.camera()plt.imshow(image); 1 Ньюман М. Э. Дж. Модульность и структура сообществ в сетях // PNAS 103. № 23 (2006):8577-8582 (http://dx.doi.org/DOI:10.1073/pnas.0601602103). 18 / 27\n--- Страница 153 ---\nПрименения разреженных матриц: преобразования изображений  153 В качестве тестовой операции будем поворачивать изображение на 30 гра- дусов. Мы начнем с определения матрицы преобразования, H, которая при умножении с координатой из входного изображения, [r, c, 1], даст нам соответ - ствующую координату на выходе, [r’, c’, 1]. (Обратите внимание: мы использу - ем однородные координаты1, где к координатам прибавляется 1 и которые при определении линейных преобразований предоставляют нам бóльшую гиб- кость.) angle = 30 c = np.cos(np.deg2rad(angle))s = np.sin(np.deg2rad(angle)) H = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]]) Вы можете проверить, все работает. Для этого умножьте H на точку (1, 0). 30-градусный поворот против часовой стрелки вокруг начала системы коорди- нат (0, 0) должен нас привести к точке (√_ 3/2, 1/2): point = np.array([1, 0, 1]) print(np.sqrt(3) / 2)print(H @ point) 1 См. https://en.wikipedia.org/wiki/Homogeneous_coordinates и https://ru.wikipedia.org/wiki/ Однородная_система_координат. 19 / 27\n--- Страница 154 ---\n154  Таблицы сопряженности на основе разреженных координатных матриц 0.866025403784 [ 0.8660254 0.5 1. ] И точно так же трехкратное применение 30-градусного поворота должно нас привести к оси столбцов в точке (0, 1). Мы видим, что за минусом погрешности аппроксимации чисел с плавающей точкой все работает: print(H @ H @ H @ point) [ 2.77555756e-16 1.00000000e+00 1.00000000e+00] Теперь мы построим функцию, которая определяет «разреженный опера- тор» (sparse operator). Задача разреженного оператора состоит в том, чтобы взять все пикселы выходного изображения, выяснить, где они находятся во входном изображении, и, чтобы вычислить их значения, выполнить соответ - ствующую (билинейную) интерполяцию (см. рис. 5.1). Этот оператор работает, применяя к значениям изображения только умножение матриц. Поэтому его быстродействие чрезвычайно большое. Рис. 5.1  Диаграмма, объясняющая билинейную интерполяцию, – значение в точке P оценивается как взвешенная сумма значений в Q11, Q12, Q21, Q22 Давайте рассмотрим функцию, которая строит наш разреженный оператор: from itertools import product def homography(tf, image_shape): «»»Проективное (гомографическое) преобразование и интерполяция в качестве линейного оператора. Параметры --------- tf : (3, 3) массив ndarray Матрица преобразования. image_shape : (M, N) Форма входного полутонового изображения. 20 / 27\n--- Страница 155 ---\nПрименения разреженных матриц: преобразования изображений  155 Возвращает ---------- A : (M * N, M * N) разреженная матрица Линейный оператор, представляющий преобразование + билинейная интерполяция. «»» # Инвертировать матрицу. Сообщает по каждому выходному пикселу, # где искать соответствующий ему входной пиксел. H = np.linalg.inv(tf) m, n = image_shape # Мы построим COO-матрицу, так называемую IJK-матрицу, для # которой нам потребуются координаты строк (I), # координаты столбцов (J) и значения (K). row, col, values = [], [], [] # Для каждого пиксела в выходном изображении for sparse_op_row, (out_row, out_col) in \\ enumerate(product(range(m), range(n))): # Вычислить, где он находится во входном изображении in_row, in_col, in_abs = H @ [out_row, out_col, 1] in_row /= in_abs in_col /= in_abs # Если координаты лежат за пределами исходного изображения, то # проигнорировать эту координату; в этой позиции у нас будет 0 if (not 0 <= in_row < m - 1 or not 0 <= in_col < n - 1): continue # Мы хотим найти четыре окружающих пиксела и интерполировать их # значения, чтобы рассчитать точное значение выходного пиксела. # Мы начинаем с левого верхнего угла, отмечая, что остальные # точки отстоят на 1 в каждом направлении. top = int(np.floor(in_row)) left = int(np.floor(in_col)) # Вычислить позицию выходного пиксела, отображенного на # входное изображение, в пределах четырех отобранных пикселов. # https://commons.wikimedia.org/wiki/File:BilinearInterpolation.svg t = in_row - top u = in_col - left # Текущая строка разреженно-операторной матрицы задается # развернутыми (т. е. обработанными функцией ravel) координатами # выходных пикселов, содержащимися в sparse_op_row. # Мы возьмем взвешенное среднее четырех окружающих входных # пикселов, соответствующих четырем столбцам. Поэтому нам нужно # повторить индекс строки четыре раза. row.extend([sparse_op_row] * 4) # Фактические веса вычисляются в соответствии с алгоритмом # билинейной интерполяции, как показано в Википедии # https://en.wikipedia.org/wiki/Bilinear_interpolation 21 / 27\n--- Страница 156 ---\n156  Таблицы сопряженности на основе разреженных координатных матриц sparse_op_col = np.ravel_multi_index( ([top, top, top + 1, top + 1 ], [left, left + 1, left, left + 1]), dims=(m, n)) col.extend(sparse_op_col) values.extend([(1-t) * (1-u), (1-t) * u, t * (1-u), t * u]) operator = sparse.coo_matrix((values, (row, col)), shape=(m*n, m*n)).tocsr() return operator Напомним, что мы применяем разреженный оператор следующим образом: def apply_transform(image, tf): return (tf @ image.flat).reshape(image.shape) Давайте испытаем! tf = homography(H, image.shape)out = apply_transform(image, tf)plt.imshow(out); А вот и поворот, о котором идет речь! Задача: поворот изображения Поворот происходит вокруг начала системы координат, т. е. координаты (0, 0). Сможете ли вы выполнить поворот изображения вокруг его центра? Совет: матрица преобразования для трансляции (т. е. сдвига изображения вверх/вниз или налево/направо) задается следующим образом: 22 / 27\n--- Страница 157 ---\nНазад к таблицам сопряженности  157 когда вы хотите переместить изображение на tr пикселов вниз и tc пикселов вправо. Как было отмечено ранее, подход к преобразованию изображения на осно- ве разреженного линейного оператора имеет очень высокое быстродействие. Давайте измерим его производительность по сравнению с ndimage . Чтобы со- поставление было справедливым, мы должны сообщить ndimage , что нам тре- буется выполнить линейную интерполяцию с order=1 , а пикселы за пределами исходной формы с reshape=False проигнорируем. %timeit apply_transform(image, tf) 100 loops, average of 7: 3.35 ms +- 270 µs per loop (using standard deviation)from scipy import ndimage as ndi %timeit ndi.rotate(image, 30, reshape=False, order=1) 100 loops, average of 7: 19.7 ms +- 988 µs per loop (using standard deviation) На наших машинах мы видим приблизительно 10-кратное ускорение. Хотя этот пример выполняет только поворот, мы можем выполнять и более сложные операции по деформированию, такие как корректировка искаженных линз во время формирования изображения или для придания людям забавных лиц. После вычисления преобразования его многократное применение будет быст - рым благодаря разреженно-матричной алгебре. Итак, мы увидели «стандартное» применение разреженных матриц SciPy. Теперь рассмотрим инновационное применение, вдохновившее нас на напи-сание этой главы. назаД к таблицам сопряженности Следует напомнить, что мы пытаемся быстро построить разреженную матри-цу совместных вероятностей с использованием разреженных форматов SciPy. Мы знаем, что формат COO хранит разреженные данные в виде трех массивов, содержащих координаты строк и столбцов с ненулевыми записями, а также их значения. Однако мы можем воспользоваться малоизвестным свойством фор-мата COO, которое позволяет чрезвычайно быстро получить нашу матрицу. Взгляните на эти данные: row = [0, 0, 2] col = [1, 1, 2]dat = [5, 7, 1]S = sparse.coo_matrix((dat, (row, col))) Обратите внимание, запись в формате (строка, столбец) в позиции (0, 1) появляется дважды: сначала как 5, далее как 7. Каким должно быть значение 23 / 27\n--- Страница 158 ---\n158  Таблицы сопряженности на основе разреженных координатных матриц матрицы в позиции (0, 1)? Как вариант может быть выбрана самая первая встретившаяся запись или самая последняя, однако в действительности была выбрана сумма первой и последней записей: print(S.toarray()) [[ 0 12 0] [ 0 0 0] [ 0 0 1]] Иными словами, формат COO суммирует повторяющиеся записи. Это имен- но то, что нам нужно для создания матрицы сопряженности! И действительно, наша задача в значительной степени решена: массиву pred можно назначить строки, массиву gt назначить столбцы и в качестве значения использовать 1. Затем мы просуммируем единицы и подсчитаем количество раз, когда метка i из pred встречается вместе с меткой j из gt в позиции i, j матрицы! Давайте это проверим: from scipy import sparse def confusion_matrix(pred, gt): cont = sparse.coo_matrix((np.ones(pred.size), (pred, gt))) return cont Чтобы взглянуть на небольшую матрицу, применим метод .toarray , как было показано выше: cont = confusion_matrix(pred, gt)print(cont) (0, 0) 1.0 (1, 0) 1.0 (0, 0) 1.0 (0, 0) 1.0 (1, 0) 1.0 (1, 1) 1.0 (1, 1) 1.0 (0, 1) 1.0 (1, 1) 1.0 (1, 1) 1.0 print(cont.toarray()) [[ 3. 1.] [ 2. 4.]] Работает! Задача: сокращение объема потребляемой оперативной памяти В главе 1 было сказано, что NumPy имеет встроенные инструменты для повто- рения массивов на основе операции транслирования. Каким образом можно 24 / 27\n--- Страница 159 ---\nТаблицы сопряженности в сегментации изображений  159 уменьшить объем потребляемой оперативной памяти, требуемой для вычис - ления матрицы сопряженности? Совет: обратитесь к документации по функции np.broadcast_to . таблицы сопряженности В сегментации изображений Сегментацию изображения можно представить в том же виде, что и выше за- дачу классификации: сегментная метка в каждом пикселе представляет собой предсказание, к какому классу принадлежит пиксел. И массивы NumPy позво-ляют нам делать это прозрачно, так как их метод .ravel() возвращает одномер- ное представление лежащих в основе данных. В качестве примера – сегментация крошечного изображения размером 3 × 3: seg = np.array([[1, 1, 2], [1, 2, 2], [3, 3, 3]], dtype=int) Вот контрольные данные со слов некого человека о том, как правильно сег - ментировать это изображение: gt = np.array([[1, 1, 1], [1, 1, 1], [2, 2, 2]], dtype=int) Эти две классификации можно представить точно так же, как и прежде. Каж - дый пиксел является другим предсказанием. print(seg.ravel()) print(gt.ravel()) [1 1 2 1 2 2 3 3 3] [1 1 1 1 1 1 2 2 2] Затем, как и прежде, получаем матрицу сопряженности: cont = sparse.coo_matrix((np.ones(seg.size), (seg.ravel(), gt.ravel()))) print(cont) (1, 1) 1.0 (1, 1) 1.0 (2, 1) 1.0 (1, 1) 1.0 (2, 1) 1.0 (2, 1) 1.0 (3, 2) 1.0 (3, 2) 1.0 (3, 2) 1.0 Некоторые индексы появляются несколько раз, но мы можем воспользо- ваться свойством суммирования формата COO и подтвердить, что этот резуль- тат представляет нужную нам матрицу: 25 / 27\n--- Страница 160 ---\n160  Таблицы сопряженности на основе разреженных координатных матриц print(cont.toarray()) [[ 0. 0. 0.] [ 0. 3. 0.] [ 0. 3. 0.] [ 0. 0. 3.]] Как преобразовать эту таблицу в меру того, насколько хорошо массив seg представляет массив gt? Сегментация представляет собой сложную задачу, по- этому важно измерить, как хорошо алгоритм сегментации с ней справляется. Для этого сравним его результат с «контрольной» сегментацией, выполненной человеком вручную. Но даже такое сравнение не является легкой задачей. Тогда как определить, насколько автоматическая сегментация будет «близкой» по сравнению с конт - рольной? Мы проиллюстрируем один замечательный метод под названием « из- менчивость информации» (variation of information, VI), или ВИ (Meila, 2005). Он определяется как ответ на следующий вопрос: в среднем, если для случайного пик - села дан его сегментный идентификатор в одной сегментации, сколько еще потре-буется информации для определения его идентификатора в другой сегментации? На интуитивном уровне, если эти две сегментации полностью одинаковы, сведения о сегментном идентификаторе в одной сегментации говорят о сег - ментном идентификаторе в другой без дополнительной информации. Но по мере того, как сегментации все больше отличаются, сведения об идентифика-торе в одной сегментации не говорят об идентификаторе в другой без допол-нительной информации. теория информации Вкратце Чтобы ответить на этот вопрос, нам потребуется оперативная сводка о теории ин-формации. Эта сводка будет краткой. Однако если вам требуется более подроб ная информация, то (увы) вам следует обратиться к звездной пуб ликации в блоге Кристофера Олаха (Christopher Olah) «Визуальная теория информации» 1. Базовой единицей информации является бит. Бит представляется как 0 или 1, равновероятностный выбор между двумя вариантами. Это прямолиней- но просто: если я хочу вам сказать, повернулась ли монета в результате броска орлом либо решкой, мне потребуется один бит, и такой вариант может при-нимать много разных форм: длинный или короткий сигнал по телеграфному проводу (как в азбуке Морзе), луч света, сверкающий одним цветом из двух, или же одиночное число, принимающее значения 0 либо 1. Важным является то, что мне всегда нужен один бит, так как исход броска монеты случаен. Оказывается, что это понятие можно расширить до дробных битов для менее случайных событий. Предположим, вам требуется передать, шел сегодня в Лос-Анджелесе дождь или нет. На первый взгляд кажется, что это тоже потребует 1 бит: 0 для того, что дождя не было, 1 для того, что лил дождь. Однако дождь 1 См. https://colah.github.io/posts/2015-09-Visual-Information/. 26 / 27\n--- Страница 161 ---\nТеория информации вкратце  161 в Лос-Анджелесе является редким событием. Поэтому со временем мы можем обойтись передачей намного меньшей информации: изредка будем пере-давать 0, чтобы удостовериться, что наш канал связи по-прежнему работает. В противном случае примем как допущение, что сигнал равняется 0, и отправ-лять 1 только в редких случаях, когда идет дождь. Следовательно, когда два события не равновероятны, для их представления потребуется меньше одного бита. Обычно мы это измеряем для любой случай-ной величины X (которая может иметь больше двух возможных значений) при помощи функции энтропии H: , где значения x – это возможные значения X и px – вероятность, что X примет значение x. Поэтому энтропия броска монеты T, могущего принимать значе- ния «орел» (h) и «решка» (t), равняется: H(T) = phlog2(1/ph) + ptlog2(1/pt) = 1/2 log2(2) + 1/2log2(2) = 1/2·1 + 1/2·1 = 1. Долгосрочная вероятность дождя в любой конкретный день в Лос-Анджелесе равняется приблизительно 1 из 6. Поэтому энтропия дождя в Лос-Анджелесе, R, принимающая значения дождливо (r) либо солнечно (s), равняется: H(R) = p rlog2(1/pr) + pslog2(1/ps) = 1/6log2(6) + 5/6log2(6/5) ≈ 0.65 бита. Особым видом энтропии является условная энтропия. Это энтропия величи- ны при условии, что об этой величине вы также знаете что-то еще. Например, какова энтропия дождя, если известен месяц? Она записывается так: H(R|M ) = ∑12 m=1p(m)H(R|M = m) и: Powered by TCPDF (www.tcpdf.org) 27 / 27\n--- Страница 162 ---\n162  Таблицы сопряженности на основе разреженных координатных матриц Теперь вы располагаете достаточными сведениями обо всей теории ин- формации, которая вам потребуется для понятия изменчивости информации. В предыдущем примере событиями являются дни, и они имеют два свойства: дождливо/солнечно; месяц. Проведя наблюдения в течение многих дней, мы можем построить матри- цу сопряженности, которая будет точно такой, что и в примерах классифика-ции, с показаниями за определенный день месяца, была ли в этот день погода дожд ливой. Чтобы это сделать, мы не собираемся ехать в Лос-Анджелес (как бы весело это не было) и вместо этого будем использовать приведенную ниже историческую таблицу, составленную на глаз на основе данных метеорологи-ческого веб-сайта WeatherSpark 1: Месяц P(дождливо) P(солнечно) 1 0.25 0.75 2 0.27 0.73 3 0.24 0.76 4 0.18 0.82 5 0.14 0.86 6 0.11 0.89 7 0.07 0.93 8 0.08 0.92 9 0.10 0.90 10 0.15 0.85 11 0.18 0.82 12 0.23 0.77 Тогда условная энтропия дождя при заданном месяце будет следующей: Итак, используя месяц, мы уменьшили хаотичность сигнала, но ненамного!Мы также можем вычислить условную энтропию месяца при наличии дож - дя, которая измеряет количество информации, требующейся для определения месяца, если мы знаем, что шел дождь. На интуитивном уровне мы знаем, это лучше, чем идти вслепую, так как вероятность дождливой погоды гораздо выше в зимние месяцы. 1 См. https://weatherspark.com/y/1705/Average-Weather-in-Los-Angeles-California-United-States- Year-Round. 1 / 27\n--- Страница 163 ---\nТеория информации применительно к сегментации  163 Задача: вычисление условной энтропии Вычислите условную энтропию месяца при наличии дождя. Какова энтропия переменной месяца? (Не учитывайте разное количество дней в месяце.) Какая из них больше?  Представленные в таблице вероятности являются условными вероятностями дождя при наличии месяца. prains = np.array([25, 27, 24, 18, 14, 11, 7, 8, 10, 15, 18, 23]) / 100 pshine = 1 - prainsp_rain_g_month = np.column_stack([prains, pshine])# Замените ‘None’ на выражение для таблицы безусловной сопряженности# Совет: сумма значений в таблице должна равняться 1. p_rain_month = None# Добавьте свой собственный исходный код ниже, чтобы вычислить# H(M|R) и H(M) Эти два значения вместе определяют изменчивость информации (VI): VI(A, B) = H (A|B) + H (B|A). теория информации применительно к сегментации : изменчиВость информации Если вернуться назад в контекст сегментации изображений, то «дни» стано- вятся «пикселами», а «дождь» и «месяц» становятся «меткой в автоматиче-ской сегментации (S)» и «контрольной меткой (T)». Затем условная энтропия автоматической сегментации при наличии контрольных данных дает меру коли чества дополнительной информации, которая нам потребуется, для опре- деления идентичности пиксела в S, если нам известна его идентичность в T. Например, если каждый сегмент g в T разбивается на два равноразмерных сег - мента a 1 и a2 в S, то H(S|T) = 1. Если пиксел находится в g, чтобы узнать, принад- лежит ли он a1 или a2, вам по-прежнему нужен 1 дополнительный бит. Однако H(T|S) = 0, потому что, независимо от того, находится ли пиксел в a1 или a2, он гарантированно находится в g, и поэтому, в отличие от сегмента в S, дополни- тельная информация не нужна. Поэтому в данном случае все вместе: VI(S, T) = H (S|T) + H (T|S) = 1 + 0 = 1 бит. Вот простой пример: S = np.array([[0, 1], [2, 3]], int)T = np.array([[0, 1], [0, 1]], int) 2 / 27\n--- Страница 164 ---\n164  Таблицы сопряженности на основе разреженных координатных матриц Здесь мы имеем две сегментации четырехпиксельных изображений: S и T. Сегментация S помещает каждый пиксел в свой собственный сегмент, в то вре- мя как сегментация T помещает левые два пиксела в сегмент 0 и правые два пиксела – в сегмент 1. Теперь точно так же, как мы сделали с метками предска- зания спама, мы создадим таблицу сопряженности пиксельных меток. Един-ственная разница – в том, что массивы меток, в отличие от одномерных мас - сивов предсказаний, являются двумерными. По сути, это не имеет значения: вспомните, что на самом деле массивы NumPy представляют собой линейные (одномерные) блоки данных, за которыми закреплена информация об их фор-ме и другие метаданные. Как мы отмечали ранее, мы можем проигнорировать форму при помощи метода .ravel() для массивов : S.ravel() array([0, 1, 2, 3]) Теперь можно легко создать таблицу сопряженности, точно так, как мы дела- ли, когда предсказывали спам: cont = sparse.coo_matrix((np.broadcast_to(1., S.size), (S.ravel(), T.ravel())))cont = cont.toarray()cont array([[ 1., 0.], [ 0., 1.], [ 1., 0.], [ 0., 1.]]) Чтобы вместо количеств эта таблица содержала вероятности, мы просто вы- полним деление на общее количество пикселов: cont /= np.sum(cont) Наконец, эту таблицу можно применить для вычисления вероятности меток в S либо в T, используя суммы вдоль осей: p_S = np.sum(cont, axis=1)p_T = np.sum(cont, axis=0) В написании исходного кода Python для вычисления энтропии имеется небольшое отклонение: хотя 0log(0) по определению равняется 0, в Python это выражение не определено и возвращает значение nan (not a number, не цифра): print('Логарифм 0 равняется: ', np.log2(0)) print('Произведение 0 на логарифм 0 равняется: ', 0 * np.log2(0)) Логарифм 0 равняется: -inf Произведение 0 на логарифм 0 равняется: nan Поэтому, чтобы наложить маску на нулевые значения, мы должны вос - пользоваться индексацией NumPy. Кроме того, в зависимости, является ли 3 / 27\n--- Страница 165 ---\nТеория информации применительно к сегментации  165 вход массивом NumPy или разреженной матрицей SciPy, нам потребуется не- много другая стратегия. Мы напишем приведенную ниже вспомогательную функцию: def xlog1x(arr_or_mat): «»»Вычислить поэлементную функцию энтропии для массива или матрицы. Параметры --------- arr_or_mat : массив numpy или разреженная матрица scipy Входной массив вероятностей. Поддерживаются только форматы разреженных матриц с атрибутом `data`. Возвращает ---------- out : массив или разреженная матрица, тот же тип, что и на входе Результирующий массив. Нулевые записи во входных данных остаются нулевыми; все другие записи умножаются на логарифм (по основанию 2) их инверсии. “”” out = arr_or_mat.copy() if isinstance(out, sparse.spmatrix): arr = out.data else: arr = out nz = np.nonzero(arr) arr[nz] *= -np.log2(arr[nz]) return out Давайте проверим, работает ли эта функция: a = np.array([0.25, 0.25, 0, 0.25, 0.25])xlog1x(a) array([ 0.5, 0.5, 0. , 0.5, 0.5]) mat = sparse.csr_matrix([[0.125, 0.125, 0.25, 0], [0.125, 0.125, 0, 0.25]]) xlog1x(mat).A array([[ 0.375, 0.375, 0.5 , 0. ], [ 0.375, 0.375, 0. , 0.5 ]]) Поэтому условная энтропия S при наличии T: H_ST = np.sum(np.sum(xlog1x(cont / p_T), axis=0) * p_T) H_ST 1.0 И обратное: H_TS = np.sum(np.sum(xlog1x(cont / p_S[:, np.newaxis]), axis=1) * p_S) H_TS 0.0 4 / 27\n--- Страница 166 ---\n166  Таблицы сопряженности на основе разреженных координатных матриц конВертироВание программного коДа массиВоВ numPy поД использоВание разреженных матриц В приведенных выше примерах мы использовали массивы NumPy и операцию транслирования. Мы уже не раз убеждались, это мощные инструменты анали-за данных в Python. Однако для сегментации сложных изображений, содержа-щих тысячи сегментов, этот способ не эффективен. Лучше во время вычисле-ний воспользоваться модулем sparse и придать части чудесных особенностей NumPy форму линейно-алгебраических операций. Это было предложено1 Уор - реном Векессером (Warren Weckesser) на StackOverflow. Линейно-алгебраическая версия элегантно короткая и эффективно вычис - ляет матрицу сопряженности для очень больших объемов данных, порядка миллиардов точек. import numpy as np from scipy import sparse def invert_nonzero(arr): arr_inv = arr.copy() nz = np.nonzero(arr) arr_inv[nz] = 1 / arr[nz] return arr_inv def variation_of_information(x, y): # Вычислить матрицу сопряженности, т. н. матрицу совместной вероятности n = x.size Pxy = sparse.coo_matrix((np.full(n, 1/n), (x.ravel(), y.ravel())), dtype=float).tocsr() # Вычислить маргинальные вероятности, преобразовав в одномерный массив px = np.ravel(Pxy.sum(axis=1)) py = np.ravel(Pxy.sum(axis=0)) # Использовать разреженно-матричную линейную алгебру, чтобы сначала # вычислить изменчивость информации (VI), # вычислить обратные диагональные матрицы Px_inv = sparse.diags(invert_nonzero(px)) Py_inv = sparse.diags(invert_nonzero(py)) # затем вычислить энтропии hygx = px @ xlog1x(Px_inv @ Pxy).sum(axis=1) hxgy = xlog1x(Pxy @ Py_inv).sum(axis=0) @ py # вернуть их сумму return float(hygx + hxgy) Мы можем проверить, что эта функция дает правильное значение (1) для изменчивости информации наших игрушечных S и T: 1 См. https://stackoverflow.com/questions/16043299/substitute-for-numpy-broadcasting-using- scipy-sparse-csc-matrix. 5 / 27\n--- Страница 167 ---\nПрименение изменчивости информации  167 variation_of_information(S, T) 1.0 Вы видите, каким образом мы используем три типа разреженных матриц (COO, CSR и диагональную) для эффективного вычисления энтропии разрежен- ных матриц сопряженности, в которых библиотека NumPy неэффективна. (На са-мом деле появление этого подхода стало следствием ошибки Python MemoryError !) применение изменчиВости информации В заключение продемонстрируем применение изменчивости информации для вычисления наилучшей автоматической сегментации изображения из воз-можных. Вы, вероятно, помните нашего дружелюбного тигра из главы 3 (см. рис. 5.2). (Если нет, то вам следует поработать над своими навыками оценива-ния угроз!) Используя навыки, полученные в главе 3, сгенерируем ряд возмож - ных вариантов сегментирования изображения тигра и затем выясним, какой из них самый лучший. from skimage import io url = ('http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds' '/BSDS300/html/images/plain/normal/color/108073.jpg') tiger = io.imread(url) plt.imshow(tiger); Рис. 5.2  Изображение тигра из набора данных BSDS, номер 108073 Чтобы проверить нашу сегментацию изображения, нам потребуется немно- го контрольных данных. Оказывается, люди обладают удивительной способно- стью идентифицировать тигров (естественный отбор, знаете ли!), поэтому от нас требуется только попросить человека найти тигра на фотографии. К сча- 6 / 27\n--- Страница 168 ---\n168  Таблицы сопряженности на основе разреженных координатных матриц стью, исследователи в Беркли уже попросили десятки людей посмотреть на это изображение и вручную его сегментировать1. Давайте возьмем одно из отсегментированных изображений из набора данных эталонных сегментаций университета Беркли (Berkeley Segmentation Dataset and Benchmark, BSDS) (см. рис. 5.3) 2. Следует отметить, между сегмен- тациями, выполненными людьми, имеется довольно существенная вариация. Если вы просмотрите различные сегментации тигра 3, то обнаружите, что не- которые люди педантичнее других в прорисовке тростников. Другие же счита-ют, что водяные блики заслуживают выделения в виде сегментов из остальной части воды. Мы выбрали понравившуюся нам сегментацию (с педантичной прорисовкой тростника, потому что мы принадлежим к тому типу ученых, ко-торых называют перфекционистами). Однако сразу же внесем ясность, у нас нет ни одного фрагмента контрольных данных! from scipy import ndimage as ndi from skimage import color human_seg_url = ('http://www.eecs.berkeley.edu/Research/Projects/CS/' 'vision/bsds/BSDS300/html/images/human/normal/' 'outline/color/1122/108073.jpg')boundaries = io.imread(human_seg_url)plt.imshow(boundaries); Рис. 5.3  Сегментация изображения тигра, выполненная человеком 1 См.: Арбелаес П., Мэр М., Фолкс Ч., Малик Дж. Обнаружение контура и иерархическая сегментация изображения (Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour Detection and Hierarchical Image Segmentation // IEEE TPAMI 33. № 5 (2011): 898–916). 2 См. https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/. 3 См. https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300/html/dataset/ images/color/108073.html. 7 / 27\n--- Страница 169 ---\nПрименение изменчивости информации  169 Совместив изображение тигра с сегментацией, выполненной человеком, мы видим, что (как и следовало ожидать) этот человек довольно хорошо справился с работой по нахождению тигра (см. рис. 5.4). Он также сегментировал берег реки и заросли тростника. Отлично, человек № 1122! human_seg = ndi.label(boundaries > 100)[0] plt.imshow(color.label2rgb(human_seg, tiger)); Рис. 5.4  Сегментация изображения тигра, выполненная человеком, с совмещением Теперь возьмем наш программный код сегментации изображения из гла- вы 3 и посмотрим, насколько хорошо Python справится с распознаванием тиг - ра (рис. 5.5)! # Нарисовать граф смежности областей (RAG) – весь исходный код из главы 3import networkx as nximport numpy as np from skimage.future import graph def add_edge_filter(values, graph): current = values[0] neighbors = values[1:] for neighbor in neighbors: graph.add_edge(current, neighbor) return 0. # обобщенный фильтр generic_filter требует возвращаемое # значение, которое мы игнорируем! def build_rag(labels, image): g = nx.Graph() footprint = ndi.generate_binary_structure(labels.ndim, connectivity=1) for j in range(labels.ndim): fp = np.swapaxes(footprint, j, 0) fp[0, ] = 0 # обнулить вершину покрытия на каждой оси _ = ndi.generic_filter(labels, add_edge_filter, footprint=footprint, mode='nearest', extra_arguments=(g,)) 8 / 27\n--- Страница 170 ---\n170  Таблицы сопряженности на основе разреженных координатных матриц for n in g: g.node[n]['total color'] = np.zeros(3, np.double) g.node[n]['pixel count'] = 0 for index in np.ndindex(labels.shape): n = labels[index] g.node[n]['total color'] += image[index] g.node[n]['pixel count'] += 1 return g def threshold_graph(g, t): to_remove = [(u, v) for (u, v, d) in g.edges(data=True) if d['weight'] > t] g.remove_edges_from(to_remove) # Базовая сегментация from skimage import segmentationseg = segmentation.slic(tiger, n_segments=30, compactness=40.0, enforce_connectivity=True, sigma=3) plt.imshow(color.label2rgb(seg, tiger)); Рис. 5.5  Базовая сегментация изображения тигра на основе SLIC-алгоритма (простая линейная итеративная кластеризация) В главе 3 мы назначили графу порог величиной 80 и для ясности опустили технические детали. Теперь мы собираемся рассмотреть внимательней, как этот порог влияет на точность нашей сегментации. Давайте вставим исходный код сегментации в функцию, чтобы с ним поэкспериментировать. def rag_segmentation(base_seg, image, threshold=80): g = build_rag(base_seg, image) for n in g: node = g.node[n] node['mean'] = node['total color'] / node['pixel count'] for u, v in g.edges(): d = g.node[u]['mean'] - g.node[v]['mean'] g[u][v]['weight'] = np.linalg.norm(d) 9 / 27\n--- Страница 171 ---\nПрименение изменчивости информации  171 threshold_graph(g, threshold) map_array = np.zeros(np.max(seg) + 1, int) for i, segment in enumerate(nx.connected_components(g)): for initial in segment: map_array[int(initial)] = i segmented = map_array[seg] return(segmented) Теперь попробуем несколько порогов и посмотрим, что произойдет (см. рис. 5.6 и 5.7): auto_seg_10 = rag_segmentation(seg, tiger, threshold=10)plt.imshow(color.label2rgb(auto_seg_10, tiger)); Рис. 5.6  Сегментация тигра на основе графа RAG с порогом 10 auto_seg_40 = rag_segmentation(seg, tiger, threshold=40)plt.imshow(color.label2rgb(auto_seg_40, tiger)); Рис. 5.7  Сегментация тигра на основе графа RAG с порогом 40 10 / 27\n--- Страница 172 ---\n172  Таблицы сопряженности на основе разреженных координатных матриц На самом деле в главе 3 мы выполнили сегментацию несколько раз с разны- ми порогами и затем (потому что мы – люди, поэтому имеем право) выбрали ту, который произвел хорошую сегментацию. Это абсолютно неудовлетвори-тельный подход к программированию сегментации изображения. Очевидно, нам нужно этот процесс как-то автоматизировать. Мы видим, что более высокий порог порождает более оптимальную сег - ментацию. Но у нас есть контрольные данные, поэтому в действительности мы можем назначить сегментации число! Используя все наши навыки работы с разреженной матрицей, можем вычислить изменчивость информации для каждой сегментации. variation_of_information(auto_seg_10, human_seg) 3.44884607874861 variation_of_information(auto_seg_40, human_seg) 1.0381218706889725 Высокий порог имеет более низкую изменчивость информации и, значит, бо- лее оптимальную сегментацию! Теперь можем вычислить изменчивость инфор- мации для диапазона возможных порогов и увидеть, какой из них дает сегмен-тацию, ближе всего соответствующую контрольным данным человека (рис. 5.8). # Проверить несколько порогов def vi_at_threshold(seg, tiger, human_seg, threshold): auto_seg = rag_segmentation(seg, tiger, threshold) return variation_of_information(auto_seg, human_seg) thresholds = range(0, 110, 10) vi_per_threshold = [vi_at_threshold(seg, tiger, human_seg, threshold) for threshold in thresholds] plt.plot(thresholds, vi_per_threshold); Рис. 5.8  Сегментация на основе изменчивости информации как функции порога 11 / 27\n--- Страница 173 ---\nПрименение изменчивости информации  173 Как и следовало ожидать, оказывается, визуальный осмотр и выбор порога threshold=80 действительно дали одну из лучших сегментаций (рис. 5.9). Одна- ко теперь у нас есть способ автоматизации этого процесса для любого изобра- жения! auto_seg = rag_segmentation(seg, tiger, threshold=80) plt.imshow(color.label2rgb(auto_seg, tiger)); Рис. 5.9  Оптимальная сегментация тигра на основе кривой изменчивости информации Дальнейшая работа: сегментация на практике Попытайтесь найти наилучший порог для подборки других изображений из набора данных эталонных сегментаций университета Беркли1. Используя среднее или медиану этих порогов, попробуйте сегментировать новое изобра-жение. Насколько удачной будет полученная вами сегментация? Разреженные матрицы являются эффективным способом представления данных со многими промежутками – такая ситуация происходит довольно час то. После прочтения этой главы вы, вероятно, начнете постоянно замечать возможности для их применения , и вы будете знать, как это делать. Один из конкретных случаев, где разреженные матрицы по-настоящему приходят на выручку, лежит в области линейной алгебры. Читайте дальше, и в следующей главе вы узнаете об этом больше! 1 См.: Арбелаес П., Мэр М., Фолкс Ч., Малик Дж. Обнаружение контура и иерархическая сегментация изображения (Contour Detection and Hierarchical Image Segmentation // IEEE TPAMI 33. № 5 (2011): 898–916). 12 / 27\n",
      "debug": {
        "start_page": 144,
        "end_page": 173
      }
    },
    {
      "name": "Глава 6 Линейная алгебра в SciPy",
      "content": "Глава 6 Линейная алгебра в SciPy Увы, невозможно объяснить, что такое Матри- ца… Ты должен увидеть это сам. – Морфеус, к/ф «Матрица» Точно так же, как и в главе 4, посвященной быстрому преобразованию Фурье (БПФ), в центре внимания настоящей главы будет элегантный метод. Мы хотим выделить имеющиеся в SciPy программные пакеты, позволяющие применять линейную алгебру, формирующую основу большинства научных вычислений. осноВы линейной алгебры Глава в книге по программированию является не совсем подходящим местом, где можно изучить линейную алгебру как таковую. Поэтому мы исходим из того, что читатель знаком с понятиями линейной алгебры. Как минимум, вы должны знать, что линейная алгебра связана с векторами (упорядоченной кол-лекцией чисел) и их преобразованием путем умножения на матрицы (коллек - ции векторов). Если все это для вас выглядит как тарабарщина, прежде чем приступить к чтению этой главы, вам, вероятно, следует подобрать учебник с вводным курсом линейной алгебры. Мы настоятельно рекомендуем учебник «Линейная алгебра и ее применения» Джила Стрэнга (Linear Algebra and Its Applications, Gil Strang, Pearson, 1994). Причем вам потребуется только введе-ние – мы надеемся передать силу методов линейной алгебры, сохранив опера-ции относительно простыми! Попутно заметим: мы нарушим общепринятые правила написания про- граммного кода Python в угоду соблюдения линейно алгебраической формы записи. В Python имена переменных обычно начинаются буквами в нижнем регистре. Однако в линейной алгебре матрицы обозначаются прописной бук - вой, а векторы и скалярные величины – буквами в нижнем регистре. Поскольку мы собираемся работать с большим количеством матриц и векторов, соблю-дение линейно-алгебраической формы записи помогает сохранить прямоли-нейное соответствие. Поэтому переменные, представляемые матрицами, бу - дут начинаться с прописной буквы, в то время как векторы и числа – с букв в нижнем регистре: 13 / 27\n--- Страница 175 ---\nЛапласова матрица графа  175 import numpy as np m, n = (5, 6) # скаляры M = np.ones((m, n)) # матрица v = np.random.random((n,)) # вектор w = M @ v # еще один вектор В математической форме записи векторы, в отличие от скалярных величин, как правило, записываются полужирным шрифтом, как v и w. Скалярные ве- личины записываются как m и n. Мы не сможем поддерживать это различие в программном коде Python. Поэтому, чтобы различать скаляры и векторы, мы будем опираться на контекст. лапласоВа матрица графа Графы мы рассмотрели в главе 3, где представляли области изображения как узлы, связанные между собой ребрами. Однако мы использовали довольно простой метод анализа: выполняли пороговую обработку графа, удаляя все реб ра, которые были выше некоторого значения. Пороговая обработка хорошо работает лишь в простых случаях. Но достаточно одного значения, попавшего на неправильную сторону порога, чтобы этот подход не сработал. Как пример предположим, что вы находитесь в состоянии войны и враже- ские войска дислоцировались на другом берегу реги напротив ваших сил. Вы хотите отрезать все подходы и поэтому решаете взорвать все мосты между вами. Разведка предлагает, что t килограмм тротиловой взрывчатки будет до- статочно для подрыва всех мостов через реку. Но мосты на вашей собственной территории могут выдержать t + 1 кг. Прочитав главу 3, вы можете отдать своей диверсионно-разведывательной группе приказ взорвать t кг тротила под каж - дым мостом в этом районе. Но если сведения разведки оказались верными ко всем мостам, кроме одного, и он останется стоять, то вражеская армия сможет переправиться! И тогда беда! Поэтому в этой главе мы займемся исследованием некоторых альтернатив- ных подходов к анализу графов на основе линейной алгебры. Оказывается, можно представить граф G как матрицу смежности, в которой мы пронумеру - ем узлы графа от 0 до n – 1, и помещаем 1 в строке i, столбце j матрицы всякий раз, когда имеется ребро из узла i в узел j. Другими словами, если мы назовем матрицу смежности A, тогда A i,j = 1, если и только если ребро (i, j) находится в G. И тогда мы можем применить линейно-алгебраические методы исследования этой матрицы, часто получая поразительные результаты. Степень, или валентность, узла (вершины) графа определяется как коли- чество ребер, инцидентных этому узлу. Например, если узел связан с пятью другими узлами в графе, то его степень равняется 5. (Позже мы будем разли-чать между полустепенью захода и полустепенью исхода, когда ребра имеют «вход» и «выход».) В терминах матриц степень соответствует сумме значений в строке или столбце матрицы. 14 / 27\n--- Страница 176 ---\n176  Линейная алгебра в SciPy Лапласова матрица графа (для краткости иногда «лапласиан») определяется как степенная матрица, D, содержащая степень каждого узла вдоль диагонали и нуль в остальных ячейках минус матрица смежности A: L = D – A. Мы определенно не сможем втиснуть всю теорию линейной алгебры, чтобы разобраться в свойствах этой матрицы. Достаточно сказать, что она имеет не- сколько замечательных свойств. И мы задействуем пару таких свойств в следу - ющих абзацах. Прежде всего мы рассмотрим собственные векторы L. Собственный вектор v матрицы M является вектором, который удовлетворяет свойству Mv = λv для некоторого числа λ, которое называется собственным значением. Другими словами, v – это особый вектор относительно M, потому что Mv просто изме- няет размер вектора, не изменяя его направления. Как мы вскоре увидим, соб-ственные векторы имеют много полезных и волшебных свойств! Например, 3 × 3 – матрица R поворота при умножении на любой трехмерный вектор p поворачивает его на 30 градусов вокруг оси z. Матрица R будет повора- чивать все векторы, за исключением лежащих на оси z. Для них мы не увидим эффекта, или Rp = p (т. е. Rp = λp) с собственным значением λ = 1. Задача: матрица поворота Рассмотрим матрицу поворота: При умножении R на трехмерный столбцовый вектор p = [ x y z]T результи- рующий вектор Rp поворачивается на θ градусов вокруг оси z. 1. Для θ = 45° следует подтвердить (путем проверки на нескольких произ- вольных векторах), что R поворачивает эти векторы вокруг оси z. Напом- ним, что матричное умножение в Python обозначается символом @. 2. Что делает матрица S = RR? Проверить это в Python. 3. Подтвердить, что умножение на R оставляет вектор [0 0 1]T неизмен- ным. Другими словами, Rp = 1p, т. е. p – это собственный вектор матрицы R с собственным значением 1. 4. Применить функцию np.linalg.eig для нахождения собственных зна- чений и собственных векторов матрицы R и подтвердить, что [0 0 1]T действительно находится среди них и соответствует собственному зна-чению 1. Вернемся к лапласовой матрице. Типичной задачей в сетевом анализе явля- ется визуализация. Как нарисовать узлы и ребра так, чтобы не получить пол-ный беспорядок, как, например, на рис. 6.1? 15 / 27\n--- Страница 177 ---\nЛапласова матрица графа  177 Рис. 6.1  Визуализация структуры Википедии (созданной Крисом Дэвисом и выпущенной согласно лицензии CC SA 3.0) Один из способов состоит в том, чтобы разместить узлы, делящие между со- бой много ребер, близких друг к другу. Оказывается, это можно сделать при помощи второго наименьшего собственного значения лапласовой матрицы и соответствующего ей собственного вектора, важность которого настолько высока, что он имеет свое собственное имя: вектор Фидлера 1. Для иллюстрации давайте воспользуемся минимальной сетью. Мы начнем с создания матрицы смежности: import numpy as np A = np.array([[0, 1, 1, 0, 0, 0], [1, 0, 1, 0, 0, 0], [1, 1, 0, 1, 0, 0], [0, 0, 1, 0, 1, 1], [0, 0, 0, 1, 0, 1], [0, 0, 0, 1, 1, 0]], dtype=float) Для изображения сети можно воспользоваться библиотекой NetworkX. Сна- чала, как обычно, мы инициализируем Matplotlib: 1 См. https://en.wikipedia.org/wiki/Algebraic_connectivity#The_Fiedler_vector или https://ru. wikipedia.org/wiki/Алгебраическая_связность#Вектор_Фидлера. 16 / 27\n--- Страница 178 ---\n178  Линейная алгебра в SciPy # Заставить графики появляться локально, задать индивидуальный стиль графиков %matplotlib inlineimport matplotlib.pyplot as pltplt.style.use('style/elegant.mplstyle') Теперь можно изобразить сеть: import networkx as nxg = nx.from_numpy_matrix(A)layout = nx.spring_layout(g, pos=nx.circular_layout(g))nx.draw(g, pos=layout, with_labels=True, node_color='white') Вы видите, что узлы естественным образом попадают в две группы: 0, 1, 2 и 3, 4, 5. Сможет ли вектор Фидлера нам об этом сообщить? Прежде всего мы должны вычислить степенную матрицу и лапласову матрицу. Сначала мы по-лучаем степени, выполняя суммирование вдоль любой оси A. (Подойдет любая из осей, потому что матрица A симметрична.) d = np.sum(A, axis=0) print(d) [ 2. 2. 3. 3. 2. 2.] Затем помещаем эти степени в диагональную матрицу такой же формы, что и A, в степенную матрицу. Для этого можно применить функцию np.diag : D = np.diag(d) print(D) 17 / 27\n--- Страница 179 ---\nЛапласова матрица графа  179 [[ 2. 0. 0. 0. 0. 0.] [ 0. 2. 0. 0. 0. 0.] [ 0. 0. 3. 0. 0. 0.] [ 0. 0. 0. 3. 0. 0.] [ 0. 0. 0. 0. 2. 0.] [ 0. 0. 0. 0. 0. 2.]] Наконец, из этого определения мы получаем лапласову матрицу: L = D – A print(L) [[ 2. -1. -1. 0. 0. 0.] [-1. 2. -1. 0. 0. 0.] [-1. -1. 3. -1. 0. 0.] [ 0. 0. -1. 3. -1. -1.] [ 0. 0. 0. -1. 2. -1.] [ 0. 0. 0. -1. -1. 2.]] Поскольку матрица L симметрична, для вычисления собственных значений и собственных векторов мы можем применить функцию np.linalg.eigh : val, Vec = np.linalg.eigh(L) Вы можете проверить, что возвращенные значения удовлетворяют опре- делению собственных значений и собственных векторов. Например, одно из собственных значений равняется 3: np.any(np.isclose(val, 3)) True И мы можем проверить, что умножение матрицы L на соответствующий соб- ственный вектор действительно умножает вектор на 3: idx_lambda3 = np.argmin(np.abs(val - 3)) v3 = Vec[:, idx_lambda3] print(v3) print(L @ v3) [ 0. 0.37796447 -0.37796447 -0.37796447 0.68898224 -0.31101776] [ 0. 1.13389342 -1.13389342 -1.13389342 2.06694671 -0.93305329] Как было отмечено ранее, вектор Фидлера – это вектор, который соответ - ствует второму наименьшему собственному значению матрицы L. Сортиров- ка собственных значений сообщает, который из них является вторым наи- меньшим: 18 / 27\n--- Страница 180 ---\n180  Линейная алгебра в SciPy plt.plot(np.sort(val), linestyle='-', marker='o'); Это первое ненулевое собственное значение, находящееся рядом с 0.4. Век - тор Фидлера является соответствующим собственным вектором (см. рис. 6.2): f = Vec[:, np.argsort(val)[1]] plt.plot(f, linestyle='-', marker='o'); Рис. 6.2  Вектор Фидлера матрицы L Поразительно! Глядя на знак элементов вектора Фидлера, мы можем разде- лить узлы на две группы, которые были определены на рисунке (см. рис. 6.3)! 19 / 27\n--- Страница 181 ---\nЛапласовы матрицы с данными о мозге  181 colors = ['orange' if eigv > 0 else 'gray' for eigv in f] nx.draw(g, pos=layout, with_labels=True, node_color=colors) Рис. 6.3  Узлы, окрашенные по их знаку в векторе Фидлера матрицы L лапласоВы матрицы с Данными о мозге Продемонстрируем этот процесс на реальном примере, расположив клетки головного мозга червя так, как показано на рис. 2 исследовательской работы Варшни и др. 1 С этой работой мы познакомили вас в главе 3. (Информация о том, как это сделать, находится в дополнительном материале, прилагаемом к указанной работе 2.) Чтобы получить полученное исследователями располо- жение нейронов мозга червя, была применена сходная матрица, называемая лапласовой матрицей, нормализованной по степени. Поскольку в этом анализе важен порядок следования нейронов, чтобы не за- громождать эту главу очисткой данных, мы используем предварительно обра-ботанный набор данных. Исходные данные были получены на веб-сайте Лэва Варшни 3, а обработанные данные находятся в нашем каталоге data/. Сначала загрузим данные. Есть четыре компонента: сеть химических синапсов, через которые предсинаптический нейрон по - сылает химический сигнал в постсинаптический нейрон; сеть щелевых контактов, содержащая прямые электрические контакты между нейронами; 1 См. http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001066. 2 См. http://journals.plos.org/ploscompbiol/article/file?id=info:doi/10.1371/journal.pcbi.1001066. s001&type=supplementary. 3 См. http://www.ifp.illinois.edu/~varshney/elegans. 20 / 27\n--- Страница 182 ---\n182  Линейная алгебра в SciPy идентификаторы нейронов (имена); три типа нейронов: – сенсорные (рецепторные) нейроны – это нейроны, которые обнаружи-вают сигналы, поступающие из внешней среды; закодированы как 0; – моторные (эффекторные) нейроны – это нейроны, активизирующие мышцы, позволяя червю перемещаться; закодированы как 2; – промежуточные (вставочные) нейроны – это нейроны, выполняющие сложную обработку сигналов между сенсорными и моторными нейро-нами; закодированы как 1. import numpy as np Chem = np.load('data/chem-network.npy') Gap = np.load('data/gap-network.npy')neuron_ids = np.load('data/neurons.npy')neuron_types = np.load('data/neuron-types.npy') Затем мы упрощаем сеть, т. е. складываем два вида связей и удаляем из сети направленность, беря среднее значение входящих и исходящих связей ней- ронов. Это слегка напоминает обман, но, поскольку мы стремимся получить только расположение нейронов в графе, нас интересует не направленность связи между нейронами, а факт наличия этой связи. Мы называем результи-рующую матрицу матрицей связности, C, являющейся другой разновидностью матрицы смежности. A = Chem + Gap C = (A + A.T) / 2 Чтобы получить лапласову матрицу L, нам потребуется матрица степени D, которая содержит степень узла i в позиции [i, i] и нули в остальных позициях. degrees = np.sum(C, axis=0)D = np.diag(degrees) Теперь можно получить лапласову матрицу: L = D – C Вертикальные координаты на рис. 2 из исследовательской работы получены путем расположения узлов так, что в среднем нейроны находятся максималь- но близко и «чуть выше» своих нисходящих соседей. Варшни и др. называют эту меру «глубиной обработки» (processing depth, вычислительной глубиной). Она получена в результате решения линейного уравнения с участием лапласо-вой матрицы. Для его решения мы используем псевдоинверсию 1, т. е. функцию scipy.linalg.pinv : from scipy import linalg b = np.sum(C * np.sign(A - A.T), axis=1)z = linalg.pinv(L) @ b 1 См. https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse. 21 / 27\n--- Страница 183 ---\nЛапласовы матрицы с данными о мозге  183 (Обратите внимание на использование символа @, который был введен в Py - thon 3.5 для обозначения операции умножения матриц. Как мы отмечали в предисловии и в главе 5, в предыдущих версиях Python использовалась функ - ция np.dot .) Чтобы получить нормализованную по степени лапласову матрицу, Q, нам потребуется обратный квадратный корень матрицы D: Dinv2 = np.diag(1 / np.sqrt(degrees)) Q = Dinv2 @ L @ Dinv2 Наконец, мы можем извлечь координаты x нейронов, тем самым гаранти- руя, что сильно связанные нейроны остаются рядом: собственный вектор Q, соответствующий своему второму наименьшему собственному значению, нормализованному по степени: val, Vec = linalg.eig(Q) Обратите внимание: согласно документации по функции numpy.linalg.eig : Собственные значения не обязательно упорядочены. Хотя документация SciPy по функции eig такого предупреждения не со- держит, в данном случае оно остается верным. Следовательно, мы должны самостоятельно отсортировать собственные значения и соответствующие им столбцы собственных векторов: smallest_first = np.argsort(val) # наименьший первый val = val[smallest_first]Vec = Vec[:, smallest_first] Теперь найдем собственный вектор, требующийся для вычисления аффин- ных координат (координат аффинного подобия): x = Dinv2 @ Vec[:, 1] (Объяснение причин использования этого вектора потребует слишком много времени и места. Эти объяснения можно найти в дополнительном материале, прилагаемом к исследовательской работе, ссылка на которую приведена выше. Если коротко, то выбор такого вектора позволяет минимизировать общую дли-ну связей между нейронами.) Есть одна небольшая сложность, которую мы должны решить, перед тем как пойти дальше: собственные векторы определены только до мультипликатив-ной константы. Это следует из определения собственного вектора. Предпо-ложим, что v – это собственный вектор матрицы M с соответствующим соб- ственным значением λ. Тогда αv – это тоже собственный вектор матрицы M для любого скалярного числа α, потому что Mv = λv влечет M(αv) = λ(αv). Поэтому, когда у программного пакета запрашиваются собственные векторы матрицы M, не важно, возвращает он v или –v. Чтобы убедиться, что мы воспроизводим то расположение узлов, которое было получено в работе Варшни и др., нам не-обходимо удостовериться, что вектор указывает в том же самом направлении, 22 / 27\n--- Страница 184 ---\n184  Линейная алгебра в SciPy что и у них, а не в противоположном. Это делается путем выбора произволь- ного нейрона из рис. 2 работы Варшни и др. и проверки знака переменной x в этой позиции. Далее, если знак не совпадает со своим знаком на рис. 2 ис - следовательской работы, мы меняем его на обратный. vc2_index = np.argwhere(neuron_ids == 'VC02') if x[vc2_index] < 0: x = -x Теперь все дело сводится к отрисовке узлов и ребер. Мы окрашиваем их со- гласно типу, хранящемуся в neuron_types , используя симпатичную и функцио- нально «нечувствительную к цвету» палитру colorbrewer1: from matplotlib.colors import ListedColormapfrom matplotlib.collections import LineCollection def plot_connectome(x_coords, y_coords, conn_matrix, *, labels=(), types=None, type_names=('',), xlabel='', ylabel=''): “””Вывести нейроны в виде точек, соединенных линиями. Нейроны могут иметь разные типы (до 6 разных цветов). Параметры --------- x_coords, y_coords : массив вещественных, форма (N,) Координаты x и y нейронов. conn_matrix : массив или разреженная матрица вещественных, форма (N, N) Матрица связности с ненулевыми записями (i, j), если и только если узел i и узел j связаны. labels : массивоподобный с типом string, форма (N,), необязательный Имена узлов. types : массив целочисленных, форма (N,), необязательный Тип (т. е. сенсорный нейрон, промежуточный нейрон) каждого узла. type_names : массивоподобный с типом string, необязательный Имя каждого значения параметра `types`. Например, если a 0 в `types` означает «сенсорный нейрон», то `type_names[0]` должен быть «сенсорный нейрон». xlabel, ylabel : строковый, необязательный Метки для осей. «»» if types is None: types = np.zeros(x_coords.shape, dtype=int) ntypes = len(np.unique(types)) colors = plt.rcParams['axes.prop_cycle'][:ntypes].by_key()['color'] cmap = ListedColormap( colors) fig, ax = plt.subplots() # Вывести на график позиции нейронов: for neuron_type in range(ntypes): plotting = (types == neuron_type) 1 См. http://chrisalbon.com/python/seaborn_color_palettes.html. 23 / 27\n--- Страница 185 ---\nЛапласовы матрицы с данными о мозге  185 pts = ax.scatter(x_coords[plotting], y_coords[plotting], c=cmap(neuron_type), s=4, zorder=1) pts.set_label(type_names[neuron_type]) # Добавить текстовые метки: for x, y, label in zip(x_coords, y_coords, labels): ax.text(x, y, ' ' + label, verticalalignment='center', fontsize=3, zorder=2) # Вывести ребра pre, post = np.nonzero(conn_matrix) links = np.array([[x_coords[pre], x_coords[post]], [y_coords[pre], y_coords[post]]]).T ax.add_collection(LineCollection(links, color='lightgray', lw=0.3, alpha=0.5, zorder=0)) ax.legend(scatterpoints=3, fontsize=6) ax.set_xlabel(xlabel, fontsize=8) ax.set_ylabel(ylabel, fontsize=8) plt.show() Теперь, чтобы вывести нейроны на график, применим эту функцию: plot_connectome(x, z, C, labels=neuron_ids, types=neuron_types, type_names=['сенсорные нейроны', 'промежуточные нейроны', 'моторные нейроны'], xlabel='Аффинный собственный вектор 1', ylabel='Глубина обработки') сенсорные нейроны промежуточные нейронымоторные нейроны Аффинный собственный вектор 1Глубина обработки 24 / 27\n--- Страница 186 ---\n186  Линейная алгебра в SciPy Извольте получить: мозг червя! Как отмечалось в оригинальной исследова- тельской работе, вы видите нисходящую обработку от сенсорных нейронов до моторных нейронов через сеть промежуточных нейронов. Здесь показаны две разные группы моторных нейронов: соответствующие шейному (слева) и те-лесному (справа) сегментам червя. Задача: изображение аффинного подобия Как видоизменить приведенный выше программный код, чтобы показать аф-финное подобие, показанное на рис. 2B исследовательской работы? Задача: линейная алгебра с разреженными матрицами В приведенном выше программном коде массивы NumPy используются для хранения матриц и выполнения необходимых вычислений. Это оправдано, по-тому что мы используем небольшой граф, состоящий из менее чем 300 узлов. Однако в случае более крупных графов этот подход потерпит неудачу. Например, можно было бы проанализировать связи между библиотеками, перечисленными в Каталоге пакетов Python, или PyPI, который содержит более 10 тысяч пакетов. Хранение лапласовой матрицы для этого графа заняло бы 8(100 × 10 3)2 = 8 × 1010 байт, или 80 Гб ОЗУ. Если к этому добавить матрицы смеж - ности, симметрической смежности, псевдоинверсии и, скажем, две временные матрицы, используемые во время вычислений, то в результате вы получите 480 Гб, что будет вне досягаемости большинства настольных компьютеров. Многие из вас могут воскликнуть: «Ха! Да, в моем настольном компьюте- ре ОЗУ 512 Гб! Ему ничего не стоит вмиг справиться с этим так называемым “большим” графом!» Может быть. Но вы, возможно, также захотите проанализировать цитат - ный граф Ассоциации вычислительной техники (Association for Computing Machinery, ACM), а это сеть, состоящая из более чем двух миллионов научных работ и ссылок. Такая лапласова матрица заняла бы ОЗУ объемом 32 терабайта. Однако нам известно, что графы зависимостей и ссылок являются разре- женными: программные пакеты обычно зависят лишь от нескольких других пакетов, а не всего каталога PyPI в целом. А научные работы и книги обычно ссылаются лишь на некоторые другие. Поэтому мы можем хранить выше-упомянутые матрицы, используя разреженные структуры данных из модуля scipy.sparse (см. главу 5), и для вычисления требующихся для нас значений применять линейно-алгебраические функции из подмодуля scipy.sparse. linalg . Попробуйте исследовать документацию по scipy.sparse.linalg , чтобы разра- ботать разреженную версию вышеупомянутого вычисления.  Псевдоинверсия разреженной матрицы в целом не является разреженной, поэтому она здесь не применима. Аналогичным образом вы не сможете получить все собственные векторы разреженной матрицы, потому что все вместе они образуют плотную матрицу. 25 / 27\n--- Страница 187 ---\nPageRank: линейная алгебра для репутации и важности  187 Вы найдете части решения ниже (и, разумеется, в приложении в конце кни- ги), однако мы настоятельно рекомендуем попробовать вам это сделать само- стоятельно. Решатели SciPy располагает несколькими разреженными итеративными решателями, и не всег - да понятно, какой из них применять. К сожалению, на этот вопрос тоже нет простого ответа: у разных алгоритмов есть много сильных сторон с точки зрения скорости схо-димости, стабильности, точности и использования оперативной памяти (среди прочих). Кроме того, глядя на входные данные, невозможно предсказать, какой алгоритм покажет наилучшую результативность. Вот грубый ориентир для выбора итеративного решателя: • если входная матрица A симметрична и положительно-определенная, лучше ис - пользовать решатель сопряженных градиентов cg. Если A симметричная, но почти сингулярная или неопределенная, попробуйте итеративный метод минимальных остатков minres; • для несимметричных систем воспользуйтесь стабилизированным методом бисо-пряженных градиентов bicgstab. Квадратичный метод сопряженных градиентов, cgs, выполняется немного быстрее, но имеет более неустойчивую сходимость; • если решается много аналогичных систем, используйте LGMRES-алгоритм lgmres; • если A не является квадратной, воспользуйтесь алгоритмом наименьших квадра- тов lsmr. А вот дополнительные материалы для чтения:• Noël M. Nachtigal, Satish C. Reddy, and Lloyd N. Trefethen. How Fast Are Nonsymmetric Matrix Iterations? (Какова скорость несимметричных матричных итераций) SIAM Journal on Matrix Analysis and Applications13, no. 3 (1992): 778–795; • Донгарра Дж. Обзор новейших методов Крылова 1 (Jack Dongarra. Survey of Recent Krylov Methods. November 20, 1995). Pagerank: линейная алгебра Для репутации и Важности Еще одно применение линейной алгебры и собственных векторов представ- лено алгоритмом компании Google PageRank для обозначения веб-страниц, остроумно названным по имени одного из соучредителей компании, Ларри Пейджа. Чтобы ранжировать веб-страницы по важности, вы можете подсчитать, сколько других веб-страниц на нее ссылаются. В конце концов, если каждая страница связана с конкретной страницей, это хорошо, разве не так? Но этот метрический показатель легко обойти: для повышения ранга вашей собствен-ной веб-страницы просто создайте столько других веб-страниц, сколько смо-жете. И пусть они ссылаются на вашу исходную страницу. Ключевой вывод, стимулировавший ранний успех Google, состоял в том, что на важные веб-страницы ссылаются не просто многие веб-страницы, а важные 1 См. http://www.netlib.org/linalg/html_templates/node50.html. 26 / 27\n--- Страница 188 ---\n188  Линейная алгебра в SciPy веб-страницы. Но как узнать, какие другие страницы важные? Ведь на них са- мих ссылаются другие важные страницы. И т. д. Из этого рекурсивного определения следует, что важность страницы может быть измерена собственным вектором так называемой матрицы переходов, со- держащей связи между веб-страницами. Предположим, у вас есть свой вектор важности r и своя матрица связей M. Вы еще не знаете r, но знаете, что важ - ность страницы пропорциональна сумме важностей страниц, которые на нее ссылаются: r = αMr, или Mr = λr, для λ = 1/α. Но ведь это и есть определение собственного значения! Добившись, что матрица переходов удовлетворяет некоторым особым свой- ствам, далее мы можем определить, что необходимое собственное значение равняется 1 и что оно является наибольшим собственным значением M. Матрица переходов обозначает интернет-пользователя, нередко именуемо- го Вэбстером, который беспорядочно нажимает на ссылку на каждой посещае-мой им веб-странице и затем задается вопросом, какую вероятность он в итоге получит на той или иной заданной странице. Эта вероятность называется ран-гом страницы, или PageRank. С началом роста популярности Google исследователи стали применять меру PageRank ко всем видам сетей. Мы будем использовать пример Стефано Алле-зина и Мерседес Паскуаль (Stefano Allesina и Mercedes Pascual), опубликован-ный 1 в журнале «PLoS Вычислительная биология». Они намеревались применить этот метод в экологических пищевых сетях, т. е. сетях, в которых организмы связаны с теми организмами, которыми они питаются. В простейшем плане, если вам интересно, насколько важным для экосисте- мы является какой-либо организм, то вы посмотрите, сколько организмов им питаются. Если их много и этот организм исчез, то все «зависимые» от него организмы могут исчезнуть вместе с ним. На языке сетей можно сказать, что его полустепень захода определяет его экологическую важность. Может ли PageRank быть оптимальной мерой важности для экосистемы?Профессор Аллезина любезно предоставил нам несколько пищевых сетей для экспериментирования. Мы сохранили одну из них в формате языка раз-метки графов. Эта сеть получена в Природном заповеднике Сент-Марка во Флориде. Данная сеть была описана 2 в 1999 г. Робертом Р . Кристианом и Джо- зефом Й. Луцзовичем (Robert R. Christian и Joseph J. Luczovich). В наборе данных узел i имеет ребро в узел j, если организм i питается организмом j. Начнем с загрузки данных, которые библиотека NetworkX читает тривиаль- ным образом: import networkx as nx stmarks = nx.read_gml('data/stmarks.gml') 1 См. https://doi.org/10.1371/journal.pcbi.1000494. 2 См. https://www.sciencedirect.com/science/article/pii/S0304380099000228. Powered by TCPDF (www.tcpdf.org) 27 / 27\n--- Страница 189 ---\nPageRank: линейная алгебра для репутации и важности  189 Затем получим разреженную матрицу, которая соответствует графу. Посколь- ку матрица содержит только числовую информацию, мы должны поддерживать отдельный список имен пакетов, соответствующих строкам/столбцам матрицы: species = np.array(stmarks.nodes()) # массив для мультииндексации Adj = nx.to_scipy_sparse_matrix(stmarks, dtype=np.float64) Из матрицы смежности мы можем вывести матрицу переходных вероятно- стей, в которой каждое ребро заменяется вероятностью 1 на количестве ребер, исходящих из этого организма. В пищевой сети целесообразнее назвать эту матрицу матрицей обеденных вероятностей. Общее количество организмов в нашей матрице будет использоваться не- однократно, поэтому давайте назовем его n: n = len(species) Затем нам понадобятся степени и, в частности, диагональная матрица, со- держащая инверсию полустепеней исхода каждого узла на диагонали: np.seterr(divide='ignore') # игнорировать ошибки деления на нуль from scipy import sparse degrees = np.ravel(Adj.sum(axis=1)) Deginv = sparse.diags(1 / degrees).tocsr() Trans = (Deginv @ Adj).T Как правило, мера PageRank представлена первым собственным вектором матрицы переходов. Если мы назовем матрицу переходов M и вектор PageRank- значений r, получим: r = Mr. Однако решение с вызовом функции np.seterr не такое простое. Метод на основе меры PageRank работает в случае, если матрица переходов является по- столбцовой стохастической матрицей, где каждый столбец в сумме составляет 1. Кроме того, каждая страница должна быть достижима из любой другой стра-ницы, даже если путь до ее достижения очень длинный. В нашей пищевой сети это вызывает проблемы, потому что основание пи- щевой цепи, т. е. то, что авторы называют органическим детритом (в основном это донные осадки), фактически ничем не питается (несмотря на круговорот жизни), и поэтому из него невозможно достигнуть других организмов. Молодой Симба: Но, папа, разве мы не питаемся антилопой? Муфаса: Да, Симба, только дай-ка я тебе объясню. Когда мы умираем, наши тела становятся травой, и антилопы едят траву. И поэтому мы все связаны между со-бой в большом круговороте жизни. – Король-лев Для решения этой проблемы в алгоритме PageRank используется так на- зываемый «фактор затухания» (или демпфирования), обычно принимаемый 1 / 27\n--- Страница 190 ---\n190  Линейная алгебра в SciPy в размере 0.85. Этот фактор означает, что 85% времени алгоритм переходит по случайно отбираемой ссылке, но для остальных 15% он беспорядочно переска-кивает на любую произвольную страницу. Так, если бы каждая страница имела низковероятностную ссылку на любую страницу. Или если бы креветка изред-ка питалась акулами. Это может показаться бессмысленным, но послушайте! Ведь это вообще-то математическое представление круговорота жизни. Мы назначим фактору затухания величину 0.85, но для анализа его величина со-всем не имеет значения: результаты будут аналогичными для большого диа-пазона возможных факторов затухания. Если мы назовем фактор затухания d, то видоизмененное уравнение Page- Rank будет таким: , и: Мы можем решить это уравнение, используя прямой решатель spsolve под - модуля scipy.sparse.linalg . Правда, в зависимости от структуры и размера ли- нейно-алгебраической задачи применение итеративного решателя может ока- заться эффективнее. По этому поводу обратитесь к документации1 по scipy. sparse.linalg для получения более подробной информации. from scipy.sparse.linalg import spsolve damping = 0.85 beta = 1 - damping I = sparse.eye(n, format='csc') # Такой же разреженный формат, что и Transpagerank = spsolve(I - damping * Trans, np.full(n, beta / n)) Теперь у нас есть «пищевой ранг» пищевой сети заповедника Сент-Марк! Итак, каким образом пищевой ранг организма соотносится с рядом других организмов, которые им питаются? def pagerank_plot(in_degrees, pageranks, names, *, annotations=[], **figkwargs): “””Построит график рангов узлов в сопоставлении с полустепенью захода с отобранными вручную именами узлов.»»» fig, ax = plt.subplots(**figkwargs) ax.scatter(in_degrees, pageranks, c=[0.835, 0.369, 0], lw=0) for name, indeg, pr in zip(names, in_degrees, pageranks): if name in annotations: text = ax.text(indeg + 0.1, pr, name) ax.set_ylim(0, np.max(pageranks) * 1.1) 1 См. https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html#solving-linear-problems. 2 / 27\n--- Страница 191 ---\nPageRank: линейная алгебра для репутации и важности  191 ax.set_xlim(-1, np.max(in_degrees) * 1.1) ax.set_ylabel('PageRank') ax.set_xlabel('Полустепень захода') Теперь построим график. Предварительно изучив набор данных, мы заранее пометили в графике некоторые интересные узлы: interesting = ['detritus', 'phytoplankton', 'benthic algae', 'micro-epiphytes', 'microfauna', 'zooplankton', 'predatory shrimps', 'meiofauna', 'gulls']in_degrees = np.ravel(Adj.sum(axis=0)) pagerank_plot(in_degrees, pagerank, species, annotations=interesting) Полустепень заходаPageRank Донные осадки (органический детрит, detritus) являются самым важным элементом как по количеству организмов, которые им питаются (15), так и по величине меры PageRank (> 0.003). Оданко вторым по важности элементом яв- ляются не донные водоросли (benthic algae), которые кормят 13 других орга-низмов, а фитопланктон (phytoplankton), который кормит всего 7! Все потому, что им питаются другие важные организмы. Слева внизу у нас морские чайки (gulls), которые, как теперь можно подтвердить, являются для экосистемы на-стоящими бездельниками. Злобные хищные креветки (predatory shrimps) (мы это не выдумываем) поддерживают то же самое количество организмов, что и фитопланктон, но они являются менее существенными организмами, и по-этому они в итоге получают более низкий пищевой ранг. Хотя мы это здесь не показываем, Аллесина и Паскуаль продолжили моде- лировать влияние исчезновения организмов на экологию и действительно об-наружили, что мера PageRank предсказывает экологическую важность лучше, чем полустепени захода. Прежде чем закончить, хотелось бы отметить, что мера PageRank может вы- числяться несколькими разными способами. Один способ, комплементарный тому, что мы сделали выше, называется степенным методом, и он действитель- 3 / 27\n--- Страница 192 ---\n192  Линейная алгебра в SciPy но мощный! Этот метод вытекает из теоремы Фробениуса–Перрона1, которая, среди всего прочего, гласит, что стохастическая матрица имеет 1 в качестве собственного значения и что единица является ее наибольшим собственным значением. (Соответствующий собственный вектор является вектором мер PageRank.) Это означает, что всякий раз, когда мы умножаем любой вектор на M, его компонента, указывающая на этот главный собственный вектор, остает - ся прежней, а все другие компоненты сужаются мультипликативным фактором. Вследствие, если мы неоднократно умножаем некий случайный начальный вектор на M, в конечном счете мы должны получить вектор мер PageRank! SciPy делает это очень эффективно при помощи своего модуля sparse для работы с разреженными матрицами: def power(Trans, damping=0.85, max_iter=10**5): n = Trans.shape[0] r0 = np.full(n, 1/n) r = r0 for _iter_num in range(max_iter): rnext = damping * Trans @ r + (1 - damping) / n if np.allclose(rnext, r): break r = rnext return r Задача: обработка висячих узлов Следует отметить, что в приведенной выше итеративной обработке матрица Trans не является постолбцово-стохастической матрицей. Поэтому при каждой итера- ции вектор r сужается. Чтобы сделать матрицу стохастической, мы должны за- менить каждый нулевой столбец на столбец, где все элементы равняются 1/n . Это слишком дорого. С другой стороны, вычисление итераций обходится дешевле. Каким образом модифицировать приведенный выше программный код, чтобы вектор r гарантированно оставался вектором вероятностей на всем протяжении? Задача: эквивалентность разных методов получения собственного вектора Удостоверьтесь, что все три метода производят одинаковое ранжирование уз- лов. Функция numpy.corr.coef может оказаться для этого полезной. заключительные замечания Область линейной алгебры слишком широка, чтобы ее можно было полноцен-но рассмотреть в главе книги. Но эта глава позволила нам увидеть ее силу и как язык Python и библиотеки NumPy и SciPy делают эти элегантные алгоритмы доступными. 1 См. https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem и https://ru.wiki- pedia.org/wiki/Теорема_Фробениуса_–_Перрона. 4 / 27\n",
      "debug": {
        "start_page": 174,
        "end_page": 192
      }
    },
    {
      "name": "Глава 7 Оптимизация функций в SciPy",
      "content": "--- Страница 193 ---\nГлава 7 Оптимизация функций в SciPy «Что новенького?» – интересный и уходящий вширь вечный вопрос, но если пользоваться исключитель-но только им, то он приводит лишь к бесконечно-му параду тривиальностей, формальностей и пыли завт рашнего дня. Напротив, я хотел бы задуматься над вопросом «Что лучше?», над вопросом, который врезается вглубь, а не вширь, над вопросом, ответы на который смывают все наносное вниз по течению. – Роберт М. Пирсиг, «Дзэн и искусство обслуживания мотоцикла» С первого раза вы не повесите картину на стену ровно. Вы ее поправляете, от - ходите назад, оцениваете горизонтальное положение и повторяете все заново. Этот процесс называется процессом оптимизации: мы изменяем ориентацию картины до тех пор, пока она не будет удовлетворять нашему требованию, т. е. пока ориентация картины не составит нулевой угол с горизонтом. В математике наше требование называется « функцией стоимости», а ориен- тация портрета – «параметром». В типичной задаче оптимизации мы изменя-ем параметры до тех пор, пока функция стоимости не будет минимизирована. Например, рассмотрим смещенную параболу, f(x) = ( x – 3) 2. Мы хотели бы найти значение x, минимизирующее функцию стоимости. Нам известно, что эта функция с параметром x имеет минимум в значении 3, так как можем вы- числить производную, уравнять ее нулем и увидеть, что 2(x – 3) = 0 (т. е. x = 3). Но если бы эта функция была намного сложнее (например, если бы выра- жение имело много членов, многочисленные точки нулевой производной, со-держало нелинейности или зависело от большего количества переменных), то ручное вычисление потребовало бы большого напряжения. Функцию стоимости можно представить как рельеф местности, где мы пыта- емся найти самую низкую точку. Такая аналогия непосредственно подчеркива-ет одну из сложных составляющих этой задачи: если вы находитесь в какой-то долине, окруженной горами, то как узнать, что вы находитесь в самой низкой долине, и не выглядит ли эта долина низкой, потому что она окружена очень 5 / 27\n--- Страница 194 ---\n194  Оптимизация функций в SciPy высокими горами? На языке оптимизации: как узнать, не застряли ли вы в ло- кальном минимуме? Большинство алгоритмов оптимизации предпринимает ту или иную попытку решить эту проблему1.  Рисунок 7.1 показывает все доступные в SciPy методы, некоторые из которых мы будем использовать, а другие оставим вам для исследования. Существует целый ряд различных алгоритмов оптимизации (см. рис. 7.1). Вам предстоит решить, примет ли ваша функция стоимости на входе скаляр или вектор (т. е. имеется ли у вас один или несколько оптимизируемых пара-метров?). Существуют функции, которые требуют задание градиента функции стоимости, и функции, вычисляющие его автоматически. Некоторые функции отыскивают только параметры в заданной области (оптимизация при заданных ограничениях), а другие исследуют все параметрическое пространство. Nelder-Mead BFGS TNCPowell Newton-CG SLSQPCG L-BFGS-B trust-ncg Рис. 7.1  Сравнение путей оптимизации, принимаемых разными алгоритмами оптимизации на функции Розенброка (вверху). Метод Пауэлла (Powell) выполняет линейный поиск вдоль первой размерности перед выполнением градиентного спуска. Метод сопряженных градиентов (CG), с другой стороны, выполняет градиентный спуск, начиная с отправной точки 1 Алгоритмы оптимизации решают этот вопрос различными способами, но два обще- принятых подхода представлены линейным (одномерным) поиском и доверитель- ной областью. В случае с линейным поиском вы пытаетесь найти минимум функции стоимости вдоль определенной размерности и затем последовательно делаете ту же самую попытку вдоль остальных размерностей. В случае с доверительными об- ластями мы перемещаем наше предположение относительно минимума в том на-правлении, в котором его ожидаем; если мы видим, что мы ожидаемо приближаемся к минимуму, то будем повторять эту процедуру с увеличенной уверенностью. В про- тивном случае мы понижаем нашу уверенность и ищем более широкую область. 6 / 27\n--- Страница 195 ---\nОптимизация в SciPy: scipy.optimize  195 оптимизация В SciPy: SciP y.oPtimize В оставшейся части настоящей главы мы для выравнивания двух изображений собираемся использовать модуль SciPy optimize . Применения выравнивания изображений, или регистрации изображений, включают сшивку панорамных снимков, совмещение разных сканов головного мозга, создание изображений со сверхвысокой разрешающей способностью и очистку объектов от зашум-ленности (шумоподавление) посредством комбинации многочисленных экс - позиций в астрономии. Как обычно, мы настраиваем нашу среду построения графиков: # Заставить графики появляться локально, задать индивидуальный стиль графиков %matplotlib inline import matplotlib.pyplot as pltplt.style.use('style/elegant.mplstyle') Начнем с самой простой версии задачи: у нас есть два изображения, одно смещено относительно другого. Требуется восстановить смещение, выравни- вающее наилучшим образом наши изображения. Наша функция оптимизации будет «сдвигать» одно из изображений и смот - реть, уменьшает ли сдвиг в том или ином направлении их различие между собой. Выполнив это многократно, мы попробуем отыскать правильное вы-равнивание. Пример: вычисление оптимального сдвига изображения Вы должны помнить нашего астронавта – Айлин Коллинз – из главы 3. Мы бу - дем смещать это изображение на 50 пикселов вправо, затем сравнивать его с оригиналом, пока не найдем смещение, которое совпадает лучше всего. Оче-видно, это глупость, так как мы знаем исходное положение. Но тем самым мы узнаем правду, и мы сможем проверить результативность нашего алгоритма. Вот оригинальное и смещенное изображения: from skimage import data, color from scipy import ndimage as ndi astronaut = color.rgb2gray(data.astronaut()) shifted = ndi.shift(astronaut, (0, 50)) fig, axes = plt.subplots(nrows=1, ncols=2) axes[0].imshow(astronaut) axes[0].set_title('Оригинальное')axes[1].imshow(shifted)axes[1].set_title('Смещенное'); 7 / 27\n--- Страница 196 ---\n196  Оптимизация функций в SciPy Оригинальное Смещенное Чтобы алгоритм оптимизации выполнил свою работу, нам нужен некий спо- соб определения «несхожести», т. е. функции стоимости. Самый легкий способ состоит в вычислении среднего квадрата разностей, который часто называется среднеквадратической ошибкой, или СКО (mean squared error, MSE). import numpy as np def mse(arr1, arr2): «»»Вычислить среднеквадратическую ошибку между двумя массивами.»»» return np.mean((arr1 - arr2)**2) Эта функция вернет 0, когда оба изображения идеально выровнены, и более высокое значение в противном случае. При помощи функции стоимости мы можем проверить, выровнены два изображения или нет: ncol = astronaut.shape[1] # Покрыть расстояние в 90% от длины в столбцах, # с одним значением в расчете на процентный пунктshifts = np.linspace(-0.9 * ncol, 0.9 * ncol, 181)mse_costs = [] for shift in shifts: shifted_back = ndi.shift(shifted, (0, shift)) mse_costs.append(mse(astronaut, shifted_back)) fig, ax = plt.subplots() ax.plot(shifts, mse_costs)ax.set_xlabel('Сдвиг') ax.set_ylabel('СКО'); 8 / 27\n--- Страница 197 ---\nОптимизация в SciPy: scipy.optimize  197 СКО Сдвиг В случае если функция стоимости определена, мы можем запросить функ - цию scipy.optimize.minimize , чтобы отыскать оптимальные параметры: from scipy import optimize def astronaut_shift_error(shift, image): corrected = ndi.shift(image, (0, shift)) return mse(astronaut, corrected) res = optimize.minimize(astronaut_shift_error, 0, args=(shifted,), method='Powell') print(f'Оптимальный сдвиг для коррекции составляет: {res.x}') Оптимальный сдвиг для коррекции составляет: -49.99997565757551 Сработало! Мы сместили его на +50 пикселов, и благодаря нашей мере СКО функция SciPy optimize.minimize выдала правильный размер сдвига (–50), при котором изображение возвращается в свое исходное состояние. Правда, эта чрезвычайно легкая задача оптимизации приводит нас к глав- ной трудности данного вида выравнивания: иногда мера СКО должна ухуд- шиться, чтобы потом улучшиться. Давайте снова взглянем на смещение изображений, начав с неизмененного изображения: ncol = astronaut.shape[1] # Покрыть расстояние в 90% от длины в столбцах, # где одно значение приходится на один процентный пункт shifts = np.linspace(-0.9 * ncol, 0.9 * ncol, 181)mse_costs = [] for shift in shifts: shifted1 = ndi.shift(astronaut, (0, shift)) mse_costs.append(mse(astronaut, shifted1)) 9 / 27\n--- Страница 198 ---\n198  Оптимизация функций в SciPy fig, ax = plt.subplots() ax.plot(shifts, mse_costs)ax.set_xlabel('Сдвиг')ax.set_ylabel('СКО'); СКО Сдвиг Начиная с нулевого сдвига взгляните на значение СКО. Сначала сдвиг стано- вится все более отрицательным. Он последовательно увеличивается прибли- зительно до –300 пикселов сдвига, после чего снова начинает уменьшаться! Незначительно, но тем не менее он уменьшается. Мера СКО достигает нижнего предела приблизительно в –400, после чего снова увеличивается. Это называ-ется локальным минимумом. Из-за того, что методы оптимизации имеют до- ступ только к «соседним» значениям функции стоимости, в ситуациях, когда функция улучшается, перемещаясь в «неправильном» направлении, процесс минимизации все равно будет двигаться в этом направлении. Поэтому если мы начинаем с изображения, сдвинутого на –340 пикселов: shifted2 = ndi.shift(astronaut, (0, -340)) функция minimize сдвинет его приблизительно на лишние 40 пикселов или око- ло того, вместо того чтобы вернуть оригинальное изображение: res = optimize.minimize(astronaut_shift_error, 0, args=(shifted2,), method='Powell') print(f'Оптимальный сдвиг для коррекции составляет {res.x}') Оптимальный сдвиг для коррекции составляет -38.51778619397471 Общепринятым решением этой задачи является сглаживание или пониже- ние масштаба изображения. Это решение имеет побочный результат сглажива- ния целевой функции. Взгляните на тот же график после сглаживания изобра-жения гауссовым фильтром: 10 / 27\n--- Страница 199 ---\nОптимизация в SciPy: scipy.optimize  199 from skimage import filters astronaut_smooth = filters.gaussian(astronaut, sigma=20)mse_costs_smooth = [] shifts = np.linspace(-0.9 * ncol, 0.9 * ncol, 181) for shift in shifts: shifted3 = ndi.shift(astronaut_smooth, (0, shift)) mse_costs_smooth.append(mse(astronaut_smooth, shifted3)) fig, ax = plt.subplots() ax.plot(shifts, mse_costs, label='оригинальное')ax.plot(shifts, mse_costs_smooth, label='сглаженное')ax.legend(loc='lower right') ax.set_xlabel('Сдвиг') ax.set_ylabel('СКО'); СКО Сдвигоригинальное сглаженное Как вы видите, в случае экстремального сглаживания «полоса» функции ошибки становится шире и менее бугристой. Вместо того чтобы сглаживать саму функцию, мы можем получить аналогичный эффект путем размывки изображений перед их сравнением. Поэтому современное программное обес - печение выравнивания использует метод, называемый гауссовой пирамидой. Гауссова пирамида состоит из набора версий с пошагово уменьшающейся раз-решающей способностью одного и того же изображения. Сначала мы выравни-ваем изображения с наиболее низкой разрешающей способностью (наиболее расплывчатые), чтобы получить приблизительное выравнивание. Затем по-следовательно уточняем выравнивание, основываясь на более четких изобра-жениях. def downsample2x(image): offsets = [((s + 1) % 2) / 2 for s in image.shape] slices = [slice(offset, end, 2) 11 / 27\n--- Страница 200 ---\n200  Оптимизация функций в SciPy for offset, end in zip(offsets, image.shape)] coords = np.mgrid[slices] return ndi.map_coordinates(image, coords, order=1) def gaussian_pyramid(image, levels=6): «»»Создать гауссову пирамиду изображения. Параметры --------- image : массив вещественных Входное изображение. max_layer : целочисленный, необязательный Количество уровней в пирамиде. Возвращает ---------- pyramid : итератор массива вещественных Итератор уровней гауссовой пирамиды, начиная с верхнего уровня (наименьшей разрешающей способности). “”” pyramid = [image] for level in range(levels - 1): blurred = ndi.gaussian_filter(image, sigma=2/3) image = downsample2x(image) pyramid.append(image) return reversed(pyramid) Давайте проверим, как выглядит одномерное выравнивание вдоль этой пи- рамиды: shifts = np.linspace(-0.9 * ncol, 0.9 * ncol, 181) nlevels = 8 costs = np.empty((nlevels, len(shifts)), dtype=float) astronaut_pyramid = list(gaussian_pyramid(astronaut, levels=nlevels)) for col, shift in enumerate(shifts): shifted = ndi.shift(astronaut, (0, shift)) shifted_pyramid = gaussian_pyramid(shifted, levels=nlevels) for row, image in enumerate(shifted_pyramid): costs[row, col] = mse(astronaut_pyramid[row], image) fig, ax = plt.subplots() for level, cost in enumerate(costs): ax.plot(shifts, cost, label='Уровень %d' % (nlevels - level)) ax.legend(loc='lower right', frameon=True, framealpha=0.9)ax.set_xlabel('Сдвиг') ax.set_ylabel('СКО'); Как вы видите, на верхнем уровне пирамиды в сдвиге размером порядка –325 пикселов эта выбоина исчезает. Следовательно, на этом уровне мы мо - жем получить приблизительное выравнивание, затем перейти к более низким уровням, чтобы уточнить полученное выравнивание (см. рис. 7.2). 12 / 27\n--- Страница 201 ---\nРегистрация изображения при помощи optimize  201 СКО СдвигУровень 8 Уровень 7Уровень 6Уровень 5Уровень 4Уровень 3Уровень 2Уровень 1 Рис. 7.2  Среднеквадратическая ошибка сдвига на разных уровнях гауссовой пирамиды регистрация изображения при помощи oPtimize Давайте данный метод автоматизируем и попробуем выполнить «реальное» выравнивание с тремя параметрами: поворотом, трансляцией в размерность строк и трансляцией в размерность столбцов. Такое выравнивание называет - ся «жесткой регистрацией», потому что отсутствуют какие-либо деформации (масштабирование, искажение или иное растягивание). Объект считается мо-нолитным и перемещается в разные стороны (включая поворот) до тех пор, пока не будет найдено совпадение. В целях упрощения исходного кода мы воспользуемся модулем библиотеки scikit-image transform , который будет вычислять сдвиг и поворот изображения. Модуль SciPy optimize предполагает на входе наличие вектора параметров. Мы сначала создадим функцию, принимающую такой вектор и производящую жесткое преобразование с правильными параметрами: from skimage import transform def make_rigid_transform(param): r, tc, tr = param return transform.SimilarityTransform(rotation=r, translation=(tc, tr)) rotated = transform.rotate(astronaut, 45) fig, axes = plt.subplots(nrows=1, ncols=2) axes[0].imshow(astronaut)axes[0].set_title('Оригинальное') axes[1].imshow(rotated) axes[1].set_title('Повернутое'); 13 / 27\n--- Страница 202 ---\n202  Оптимизация функций в SciPy Далее нам потребуется функция стоимости. Ею будет простая функция СКО, но SciPy требует определенного формата: первый аргумент должен быть век- тором параметров, который она оптимизирует. Последующие аргументы мо- гут быть переданы через ключевое слово args в виде кортежа, но должны оста- ваться фиксированными. Оптимизируется только вектор параметров. В нашем случае это просто угол поворота и два параметра трансляции: def cost_mse(param, reference_image, target_image): transformation = make_rigid_transform(param) transformed = transform.warp(target_image, transformation, order=3) return mse(reference_image, transformed) Оригинальное Повернутое Наконец, мы напишем функцию выравнивания, которая, используя резуль- тат предыдущего уровня как отправную точку для следующего уровня, опти- мизирует функцию стоимости на каждом уровне гауссовой пирамиды: def align(reference, target, cost=cost_mse): nlevels = 7 pyramid_ref = gaussian_pyramid(reference, levels=nlevels) pyramid_tgt = gaussian_pyramid(target, levels=nlevels) levels = range(nlevels, 0, -1) image_pairs = zip(pyramid_ref, pyramid_tgt) p = np.zeros(3) for n, (ref, tgt) in zip(levels, image_pairs): p[1:] *= 2 res = optimize.minimize(cost, p, args=(ref, tgt), method='Powell') p = res.x # Печатать текущий уровень, всякий раз перезаписывая информацию # (как индикатор выполнения) print(f'Уровень: {n}, Угол: {np.rad2deg(res.x[0]) :.3}, ' f'Сдвиг: ({res.x[1] * 2**n :.3}, {res.x[2] * 2**n :.3}), ' f'Стоимость: {res.fun :.3}', end='\\r') print('') # новая строка, когда выравнивание завершено return make_rigid_transform(p) 14 / 27\n--- Страница 203 ---\nРегистрация изображения при помощи optimize  203 Давайте проверим ее на нашем изображении астронавта. Будем его пово- рачивать на 60 градусов и добавлять немного шума. Сможет ли SciPy восстано- вить правильное преобразование? (См. рис. 7.3.) from skimage import util theta = 60 rotated = transform.rotate(astronaut, theta)rotated = util.random_noise(rotated, mode='gaussian', seed=0, mean=0, var=1e-3) tf = align(astronaut, rotated) corrected = transform.warp(rotated, tf, order=3) f, (ax0, ax1, ax2) = plt.subplots(1, 3) ax0.imshow(astronaut)ax0.set_title('Оригинальное')ax1.imshow(rotated) ax1.set_title('Повернутое') ax2.imshow(corrected)ax2.set_title('Зарегистрированное')for ax in (ax0, ax1, ax2): ax.axis('off') Уровень: 1, Угол: -60.0, Сдвиг: (-1.87e+02, 6.98e+02), Cost: 0.0369 Оригинальное Повернутое Зарегистрированное Рис. 7.3  Применение оптимизации для восстановления выравнивания изображения Теперь мы чувствуем себя гораздо лучше. Однако наш выбор параметров фактически замаскировал трудность оптимизации. Давайте посмотрим, что произойдет после поворота на 50 градусов, который находится ближе к исход-ному изображению: theta = 50 rotated = transform.rotate(astronaut, theta)rotated = util.random_noise(rotated, mode='gaussian', seed=0, mean=0, var=1e-3) tf = align(astronaut, rotated) corrected = transform.warp(rotated, tf, order=3) 15 / 27\n--- Страница 204 ---\n204  Оптимизация функций в SciPy f, (ax0, ax1, ax2) = plt.subplots(1, 3) ax0.imshow(astronaut) ax0.set_title('Оригинальное') ax1.imshow(rotated) ax1.set_title('Повернутое') ax2.imshow(corrected) ax2.set_title('Зарегистрированное')for ax in (ax0, ax1, ax2): ax.axis('off') Уровень: 1, Угол: 0.414, Сдвиг: (2.85, 38.4), Cost: 0.141 Несмотря на то что мы начали обработку ближе к исходному изображению, нам не удалось восстановить правильный поворот (рис. 7.4). Причина неуда- чи в том, что методы оптимизации могут застревать в локальных минимумах и небольших выбоинах. В этом мы уже убедились в случае с выравниванием только для сдвига. И как результат итог может быть довольно чувствительным по отношению к исходным параметрам. Оригинальное Повернутое Зарегистрированное Рис. 7.4  Неудавшаяся оптимизация преДотВращение локальных минимумоВ на осноВе алгоритма baSin hoPPing Алгоритм 1997 года, разработанный Дэвидом Уэйлсом и Джонатаном Дой-лом 1 под названием basin hopping (прыжки по котловине), пытается избежать локальных минимумов. Чтобы проскочить локальный мимниум, следует по-пробовать оптимизацию с несколькими первоначальными параметрами. За-тем уйти от найденного локального минимума в произвольном направлении 1 Уэйлс Д. Дж., Дойл Дж. П. К. Глобальная оптимизация путем отскока от котловины и структуры кластеров с наименьшей энергией Леннарда–Джонса, содержащие до 110 атомов (David J. Wales, Jonathan P . K. Doyle. Global Optimization by Basin-Hopping and the Lowest Energy Structures of Lennard-Jones Clusters Containing up to 110 Atoms // Journal of Physical Chemistry 101, no. 28 (1997): 5111–5116). 16 / 27\n--- Страница 205 ---\n«Что лучше?»: выбор правильной целевой функции  205 и снова оптимизироваться. Выбрав соответствующий размер шага для случай- ных шагов, данный алгоритм может избежать повторного попадания в тот же самый локальный минимум и, по сравнению с простыми методами оптимиза-ции на основе градиента, исследует гораздо большую площадь параметриче-ского пространства. В качестве упражнения мы предлагаем читателям включить реализацию ал- горитма basin hopping средствами SciPy в нашу функцию выравнивания. Эта функция потребуется вам для последующих частей данной главы. Если же вы попадете в тупик, то смело можете подсмотреть решение в конце книги. Задача: модификация функции align Попробуйте модифицировать функцию align для использования функции scipy.optimize.basinhopping , которая имеет четкие стратегии уклонения от ло- кальных минимумов.  Ограничьте использование алгоритма basin hopping только верхними уровнями пира-миды, т. к. этот метод оптимизации более медленный и может занять гораздо больше времени в случае работы с полной разрешающей способностью изображения. «что лучше ?»: Выбор праВильной целеВой функции На данном этапе у нас есть рабочий подход к регистрации, дающий наилучшие результаты. Но оказывается, что мы решили лишь самую простую из регистра-ционных задач: выравнивание изображений той же самой модальности. Это значит, что мы ожидаем совпадения ярких пикселов в опорном изображении с яркими пикселами в тестовом изображении. Теперь выровняем в этом изображении каналы цветности. Обратите внима- ние, здесь мы больше не можем опираться на каналы, имеющие ту же самую модальность. В данной задаче есть историческая ценность: между 1909 и 1915 г., до изобретения цветной фотографии, фотограф Сергей Михайлович Прокудин-Горский уже создавал цветные фотографии Российской империи. Фотографии создавались с помощью трех монохромных снимков, сделанных через поме-щенные перед линзой объектива три фильтра синего, красного и зеленого цвета. В этом случае совместное выравнивание ярких пикселов, которое неявным образом выполняет СКО, работать не будет. Возьмем, к примеру, три снимка витража в церкви Святого Иоанна Богослова, взятые из коллекции Прокудина-Горского Библиотеки конгресса 1 (см. рис. 7.5): from skimage import io stained_glass = io.imread('data/00998v.jpg') / 255 # в [0, 1] использовать # вещественное изображение fig, ax = plt.subplots(figsize=(4.8, 7)) 1 См. http://www.loc.gov/pictures/item/prk2000000263/. 17 / 27\n--- Страница 206 ---\n206  Оптимизация функций в SciPy ax.imshow(stained_glass) ax.axis('off'); Рис. 7.5  Пластина Прокудина-Горского: три фотографии одного и того же витража, снятые с использованием трех разных фильтров Взгляните на одежды Святого Иоанна: на одном изображении они выглядят черными как смола, серыми на другом и ярко-белыми на третьем! Это привело бы к ужасной оценке СКО даже с прекрасным выравниванием. 18 / 27\n--- Страница 207 ---\n«Что лучше?»: выбор правильной целевой функции  207 Посмотрим, что можно с этим сделать. Начнем с разбиения пластины на со- ставляющие ее каналы: nrows = stained_glass.shape[0] step = nrows // 3channels = (stained_glass[:step], stained_glass[step:2*step], stained_glass[2*step:3*step])channel_names = ['синий', 'зеленый', 'красный']fig, axes = plt.subplots(1, 3)for ax, image, name in zip(axes, channels, channel_names): ax.imshow(image) ax.axis('off') ax.set_title(name) синий зеленый красный Сначала наложим все три изображения друг на друга, чтобы убедиться в не- обходимости точной настройки выравнивания между этими тремя каналами: blue, green, red = channelsoriginal = np.dstack((red, green, blue))fig, ax = plt.subplots(figsize=(4.8, 4.8), tight_layout=True)ax.imshow(original)ax.axis('off'); 19 / 27\n--- Страница 208 ---\n208  Оптимизация функций в SciPy По наличию цветных «ореолов» вокруг объектов на изображении вы видите, что цвета близки к выравниванию, но не совсем. Давайте попробуем выров- нять их так же, как мы выравнивали приведенное выше изображение астро-навта, используя СКО. В качестве опорного изображения используем один ка-нал зеленого цвета и по нему выровняем синий и красный каналы. print('*** Выравнивание синего с зеленым ***') tf = align(green, blue) cblue = transform.warp(blue, tf, order=3) print('** Выравнивание красного с зеленым ***') tf = align(green, red)cred = transform.warp(red, tf, order=3) corrected = np.dstack((cred, green, cblue)) f, (ax0, ax1) = plt.subplots(1, 2) ax0.imshow(original) ax0.set_title('Оригинальный')ax1.imshow(corrected)ax1.set_title('Скорректированный')for ax in (ax0, ax1): ax.axis('off') *** Выравнивание синего с зеленым *** Уровень: 1, Угол: -0.0474, Сдвиг: (-0.867, 15.4), Cost: 0.0499** Выравнивание красного с зеленым ***Уровень: 1, Угол: 0.0339, Сдвиг: (-0.269, -8.88), Cost: 0.0311 Выравнивание получилось немного лучше, чем с необработанными изобра- жениями (рис. 7.6), потому что благодаря гигантскому желтому участку неба красный и зеленый каналы выровнены правильно. Однако синий канал по-прежнему смещен, и яркие пятна синего цвета не совпадают с зеленым кана-лом. Это означает, что СКО будет ниже, когда каналы рассогласованы из-за того, что синие участки частично накладываются на ярко-зеленые участки. Оригинальный Скорректированный Рис. 7.6  Выравнивание на основе СКО сокращается, но не устраняет цветных ореолов 20 / 27\n--- Страница 209 ---\n«Что лучше?»: выбор правильной целевой функции  209 Теперь мы обратимся к мере под названием «нормализованная взаимная ин- формация» (НВИ, normalized mutual information, NMI), которая измеряет кор- реляции между разными полосами яркости различных изображений. Когда изображения идеально выровнены, любой объект однородного цвета создаст большую корреляцию между оттенками различных компонентных каналов и соответствующим образом бóльшую величину НВИ. В некотором смысле НВИ измеряет, насколько легко получается предсказать значение пиксела в одном изображении при наличии значения соответствующего пиксела в другом. Эта мера была определена в работе «Инвариантная к наложению мера энтропии выравнивания трехмерного медицинского изображения» 1: где H(X) – это энтропия X и H(X, Y) – совместная энтропия X и Y. Числитель опи- сывает энтропию двух изображений, рассматриваемых раздельно, а знамена-тель – общую энтропию, если они наблюдаются вместе. Величины могут варьи-роваться между 1 (максимально выровнены) и 2 (минимально выровнены) 2. См. главу 5. В программном коде Python получится: from scipy.stats import entropy def normalized_mutual_information(A, B): «»»Вычислить нормализованную взаимную информацию. Нормализованная взаимная информация задается формулой: H(A) + H(B) Y(A, B) = ----------- H(A, B) где H(X) – это энтропия ``- sum(x log x) для x в X``. Параметры --------- A, B : массив ndarray Регистрируемые изображения. Возвращает ---------- nmi : вещественное 1 См.: Стадхолм К., Хилл Д. Л. Г., Хоукс Д. Дж. Инвариантная к наложению мера эн- тропии выравнивания трехмерного медицинского изображения (C. Studholme, D. L. G. Hill, and D. J. Hawkes. An Overlap Invariant Entropy Measure of 3D Medical Image Align-ment // Pattern Recognition 32, no. 1 (1999): 71–86). (https://www.sciencedirect.com/science/article/pii/S0031320398000910). 2 Объяснение на скорую руку состоит в следующем: энтропия вычисляется из рассмат - риваемой гистограммы количества. Если X = Y, то совместная гистограмма (X, Y) – это диагональ, и эта диагональ совпадает с диагональю X или Y. Отсюда H(X) = H(Y) = H(X, Y) и I(X, Y) = 2. 21 / 27\n--- Страница 210 ---\n210  Оптимизация функций в SciPy Нормализованная взаимная информация между двумя массивами, вычисленная с гранулярностью 100 интервалов на ось (всего 10 тыс. интервалов). «»» hist, bin_edges = np.histogramdd([np.ravel(A), np.ravel(B)], bins=100) hist /= np.sum(hist) H_A = entropy(np.sum(hist, axis=0)) H_B = entropy(np.sum(hist, axis=1)) H_AB = entropy(np.ravel(hist)) return (H_A + H_B) / H_AB Теперь мы определяем оптимизируемую функцию стоимости в соответ - ствии с определением cost_mse , приведенным выше: def cost_nmi(param, reference_image, target_image): transformation = make_rigid_transform(param) transformed = transform.warp(target_image, transformation, order=3) return -normalized_mutual_information(reference_image, transformed) Наконец, мы применим ее с нашим оптимизирующим выравнивателем, ра- ботающим на основе алгоритма basin hopping (рис. 7.7): print('*** Выравнивание синего с зеленым ***')tf = align(green, blue, cost=cost_nmi)cblue = transform.warp(blue, tf, order=3) print('** Выравнивание красного с зеленым ***') tf = align(green, red, cost=cost_nmi)cred = transform.warp(red, tf, order=3) corrected = np.dstack((cred, green, cblue)) fig, ax = plt.subplots(figsize=(4.8, 4.8), tight_layout=True) ax.imshow(corrected) ax.axis('off') *** Выравнивание синего с зеленым *** Уровень: 1, Угол: 0.444, Сдвиг: (6.07, 0.354), Cost: -1.08** Выравнивание красного с зеленым ***Уровень: 1, Угол: 0.000657, Сдвиг: (-0.635, -7.67), Cost: -1.11(-0.5, 393.5, 340.5, -0.5) Какое великолепное изображение! Только представьте, что этот экспонат был создан еще до появления цветной фотографии! Обратите внимание на жемчуж - но-белые одежды Господа, белую бороду Иоанна и белые страницы книги в руках Прохора, его писца – все они отсутствовали на выравнивании, основанном на СКО, но выглядят резко и четко после применения НВИ. Также обратите внима-ние на реалистичное золото подсвечников, расположенных на переднем плане. В этой главе мы проиллюстрировали два ключевых понятия функциональ- ной оптимизации: понимание локальных минимумов, как их избежать, и вы-бор правильной оптимизируемой функции для достижения конкретной цели. Решение этих функций позволяет применить оптимизацию к огромному спект ру научных задач! 22 / 27\n--- Страница 211 ---\n«Что лучше?»: выбор правильной целевой функции  211 Рис. 7.7  Каналы Prokudin-Gorskii выровнены с нормализованной взаимной информацией 23 / 27\n",
      "debug": {
        "start_page": 193,
        "end_page": 211
      }
    },
    {
      "name": "Глава 8 Большие данные с T oolz в маленьком ноутбуке ГРЕЙСИ",
      "content": "--- Страница 212 ---\nГлава 8 Большие данные с T oolz в маленьком ноутбуке ГРЕЙСИ: Нож? Парень двенадцать футов ростом! ДЖЕК: Семь. Да не волнуйся ты, думаю, я с ним справлюсь. – Джек Бертон, к/ф «Большой переполох в маленьком Китае» Потоковая обработка не является изюминкой SciPy. Это скорее подход, позво-ляющий эффективно обрабатывать большие наборы данных, подобные тем, с которыми часто сталкиваются в науке. Язык Python содержит несколько по-лезных примитивов для обработки потоковых данных, и они могут быть объ-единены с библиотекой Toolz Мэтта Роклина (Matt Rocklin), предназначенной для генерирования элегантного и сжатого программного кода. Этот код эф-фективно экономит потребление оперативной памяти. В данной главе мы по-кажем, как применять принципы потоковой обработки, чтобы позволить вам работать с более крупными наборами данных, чем те, которые смогут помес - титься в ОЗУ вашего компьютера. Скорее всего, с потоковой обработкой вы уже работали. Самая простая пото- ковая форма состоит в итеративном переборе строк файла и обработке каждой строки без считывания всего файла в оперативную память. Например, подоб-ный цикл будет состоять в вычислении среднего значения каждой строки и их суммировании: import numpy as np with open('data/expr.tsv') as f: sum_of_means = 0 for line in f: sum_of_means += np.mean(np.fromstring(line, dtype=int, sep='\\t')) print(sum_of_means) 1463.0 24 / 27\n--- Страница 213 ---\nБольшие данные с Toolz в маленьком ноутбуке  213 Эта стратегия прекрасно работает, когда ваша задача легко решается по- строчной обработкой. Но при усложнении программного кода она может быст - ро выйти из-под контроля. В потоковых программах некая функция обрабатывает фрагмент вход- ных данных, возвращает обработанный блок, затем, пока нисходящие функ - ции работают с этим блоком, сама функция получает следующий фрагмент и т. д. И все эти вещи происходят одновременно! Как обеспечить их надежную работу ? Для нас это тоже представляло трудность, пока мы не обнаружили библио- теку Toolz. Ее конструкции делают потоковые программы столь элегантными, что написание этой книги просто невозможно представить без включения гла-вы, посвященной данной теме. Давайте уточним, что мы имеем в виду под «потоковой обработкой» и зачем она вам может понадобиться. Допустим, что у вас в текстовом файле есть некие данные и вы хотите вычислить постолбцовое среднее логарифма значений по формуле log(x + 1). В таких случаях принято использовать библиотеку NumPy, при помощи которой загружают значения, вычисляют функцию логарифма для всех значений в полной матрице и затем берут среднее на первой оси: import numpy as np expr = np.loadtxt('data/expr.tsv') logexpr = np.log(expr + 1) np.mean(logexpr, axis=0) array([ 3.11797294, 2.48682887, 2.19580049, 2.36001866, 2.70124539, 2.64721531, 2.43704834, 3.28539133, 2.05363724, 2.37151577, 3.85450782, 3.9488385 , 2.46680157, 2.36334423, 3.18381635, 2.64438124, 2.62966516, 2.84790568, 2.61691451, 4.12513405]) Все хорошо работает, следуя успокаивающе знакомой вычислительной мо- дели ввода-вывода. Однако такой способ работы совершенно неэффективен! Мы загружаем полную матрицу в оперативную память (1), затем делаем ее ко-пию, добавляя 1 в каждое значение (2), потом делаем еще одну копию для вы-числения логарифма (3) и только после этого передаем ее в np.mean . Целых три экземпляра массива данных для выполнения операции, не требующей хране-ния в памяти даже одного экземпляра. Для любого вида операции с «больши-ми данными» такой подход просто не будет работать. Создатели языка Python это хорошо понимали и создали ключевое слово yield , наделяющее функции возможностью обрабатывать всего один «глоток» данных. Потом передавать результат дальше в следующий процесс и завершать цепь обработки одного фрагмента данных, перед тем как перейти к следующе-му. Слово «yield» (производить, передавать) очень хорошо это характеризует: функция передает управление в следующую функцию, ожидая продолжения обработки данных до тех пор, пока все нисходящие шаги не обработают эту точку данных. 25 / 27\n--- Страница 214 ---\n214  Большие данные с Toolz в маленьком ноутбуке потокоВая переДача при помощи yield Описанный выше поток управления довольно сложно отследить. Потряса- ющая особенность языка Python заключается в том, что он абстрагирует - ся от этой сложности, позволяя вам сосредоточиваться на аналитическом функционале. Вот как это можно представить: для каждой обрабатывающей функции, которая обычно принимала список (коллекцию данных) и выпол-няла преобразование этого списка, вы можете переписать эту функцию как принимающую поток и производящую результат из каждого элемента этого потока. Ниже приводится пример, где мы берем логарифм каждого элемента списка, используя два метода: стандартный метод копирования данных и потоковый метод: def log_all_standard(input): output = [] for elem in input: output.append(np.log(elem)) return output def log_all_streaming(input_stream): for elem in input_stream: yield np.log(elem) Давайте проверим, дают ли оба метода одинаковый результат: # Установить начальное значение случайного числа, # чтобы получать воспроизводимые результатыnp.random.seed(seed=7) # Задать настройки печати для отображения только 3 значащих разрядов np.set_printoptions(precision=3, suppress=True) arr = np.random.rand(1000) + 0.5 result_batch = sum(log_all_standard(arr))print('Пакетный результат: ', result_batch)result_stream = sum(log_all_streaming(arr))print('Потоковый результат: ', result_stream) Пакетный результат: -48.2409194561 Потоковый результат: -48.2409194561 Преимущество потокового метода состоит в том, что элементы потока не обрабатываются, пока в них не возникнет необходимость, будь то вычисление нарастающего итога или запись данных на жесткий диск либо что-то еще. Это позволяет сэкономить большой объем оперативной памяти при большом объ-еме входных значений или когда каждое значение очень большое. (Или и то, и другой вместе!) Приведенная ниже цитата из публикации в блоге 1 Мэтта очень кратко резюмирует полезность потокового анализа данных: 1 См. http://matthewrocklin.com/blog/work/2015/02/17/Towards-OOC-Bag. 26 / 27\n--- Страница 215 ---\nПотоковая передача при помощи yield  215 Исходя из моего небольшого опыта, люди редко идут по такому [потоково- му] пути. Они используют однопоточный Python с хранением данных в опера- тивной памяти, пока все не зависнет, и затем ищут инфраструктуру обработки больших данных типа Hadoop/Spark с относительно высокими издержками по производительности. И действительно, эта цитата идеально описывает нашу вычислительную карье ру. Однако промежуточный подход способен продвинуть вас гораздо дальше, чем вы думаете. В некоторых случаях он способен сделать это быстрее, чем подход на основе сверхвысокопроизводительных вычислений, устраняя издержки, связанные со взаимодействием многочисленных ядер и произволь-ным доступом к базам данных. (Обратитесь, например, к публикации в блоге «Большие данные; тот самый ноутбук» 1 Фрэнка Макшерри (Frank McSherry), где он обрабатывает граф с 128 миллиардами ребер на своем ноутбуке быстрее, чем используя графовую базу данных на суперкомпьютере.) Чтобы внести ясность, каким образом при использовании потоковых функ - ций разворачивается поток управления, полезно написать детализированные версии функции, печатающей сообщение вместе с каждой операцией. import numpy as np def tsv_line_to_array(line): lst = [float(elem) for elem in line.rstrip().split('\\t')] return np.array(lst) def readtsv(filename): print('вход в readtsv') with open(filename) as fin: for i, line in enumerate(fin): print(f'чтение строки {i}') yield tsv_line_to_array(line) print('выход из readtsv') def add1(arrays_iter): print('вход в add1') for i, arr in enumerate(arrays_iter): print(f'добавление 1 к строке {i}') yield arr + 1 print('выход из add1') def log(arrays_iter): print('вход в log') for i, arr in enumerate(arrays_iter): print(f'взятие логарифма массива {i}') yield np.log(arr) print('выход из log') def running_mean(arrays_iter): print('вход в running_mean') for i, arr in enumerate(arrays_iter): 1 См. http://www.frankmcsherry.org/graph/scalability/cost/2015/02/04/COST2.html. Powered by TCPDF (www.tcpdf.org) 27 / 27\n--- Страница 216 ---\n216  Большие данные с Toolz в маленьком ноутбуке if i == 0: mean = arr mean += (arr - mean) / (i + 1) print(f'добавление строки {i} к скользящему среднему') print('возвращение среднего значения') return mean Давайте посмотрим на эти функции в действии на примере небольшого де- монстрационного файла: fin = 'data/expr.tsv' print('Создание итератора строк')lines = readtsv(fin)print('Создание итератора логарифма строк')loglines = log(add1(lines))print('Вычисление среднего')mean = running_mean(loglines)print(f'Среднее логарифма строк: {mean}') Создание итератора строк Создание итератора логарифма строкВычисление среднеговход в running_meanвход в logвход в add1 вход в readtsv чтение строки 0добавление 1 к строке 0взятие логарифма массива 0добавление строки 0 к скользящему среднемучтение строки 1добавление 1 к строке 1взятие логарифма массива 1добавление строки 1 к скользящему среднему чтение строки 2 добавление 1 к строке 2взятие логарифма массива 2добавление строки 2 к скользящему среднемучтение строки 3добавление 1 к строке 3взятие логарифма массива 3добавление строки 3 к скользящему среднемучтение строки 4 добавление 1 к строке 4 взятие логарифма массива 4добавление строки 4 к скользящему среднемувыход из readtsvвыход из adding 1выход из logвозвращение среднего значенияСреднее логарифма строк: [ 3.118 2.487 2.196 2.36 2.701 2.647 2.437 3.285 2.054 2.372 3.855 3.949 2.467 2.363 3.184 2.644 2.63 2.848 2.617 4.125] 1 / 27\n--- Страница 217 ---\nВведение в потоковую библиотеку Toolz  217 Примечание: при создании итератора строк и итератора логарифма строк никаких вычислений не происходит. Это связано с характером итераторов – они ленивые, имея в виду, что они не вычисляются (не потребляются) до тех пор, пока результат не станет необходим; когда в результате вызова функции running_mean вычисление наконец запускается, перед тем как перейти дальше к следующей строке, поток управления прыгает туда-сюда между всеми функциями, по мере того как выполняются различные вычисления, связанные с каждой строкой данных. ВВеДение В потокоВую библиотеку toolz В примере программного кода этой главы, предоставленном Мэттом Рокли-ном, мы создаем марковскую модель на основе полного генома плодовой муш-ки, дрозофилы. Эта модель создается на ноутбуке менее чем за пять минут, используя всего несколько строк программного кода. (Мы его немного отре-дактировали, чтобы упростить его нисходящую обработку.) В примере Мэтта используется геном человека, но по вполне понятным причинам быстродей-ствие наших ноутбуков не было достаточным, поэтому вместо него мы собира-емся использовать геном плодовой мушки (его размер составляет 1/20 генома человека). В течение этой главы данный пример будет немного расширен, что-бы можно было начинать обработку со сжатых данных (неужели же кто-то же-лает хранить несжатый набор данных на своем жестком диске?). Данная моди-фикация почти тривиальна, что говорит в пользу элегантности этого примера. import toolz as tz from toolz import curried as cfrom glob import globimport itertools as it LDICT = dict(zip('ACGTacgt', range(8))) PDICT = {(a, b): (LDICT[a], LDICT[b]) for a, b in it.product(LDICT, LDICT)} def is_sequence(line): return not line.startswith('>') def is_nucleotide(letter): return letter in LDICT # ignore ‘N’ @tz.curry def increment_model(model, index): model[index] += 1 def genome(file_pattern): «»»Передать геном потоком, буква за буквой, из списка имен файлов FASTA.»»» return tz.pipe(file_pattern, glob, sorted, # Имена файлов c.map(open), # строки # конкатенировать строки из всех файлов: 2 / 27\n--- Страница 218 ---\n218  Большие данные с Toolz в маленьком ноутбуке tz.concat, # отбросить заголовок из каждой последовательности c.filter(is_sequence), # конкатенировать символы из всех строк tz.concat, # отбросить символы новой строки и ‘N’ c.filter(is_nucleotide)) def markov(seq): «»»Получить марковскую модель первого порядка из последовательности нуклеотидов.»»» model = np.zeros((8, 8)) tz.last(tz.pipe(seq, c.sliding_window(2), # каждый последующий кортеж c.map(PDICT.__getitem__), # местоположение кортежа # в матрице c.map(increment_model(model)))) # прирастить матрицу # преобразовать количества в матрицу вероятностей переходов model /= np.sum(model, axis=1)[:, np.newaxis] return model Затем, чтобы получить марковскую модель последовательностей, повторя- ющихся в геноме дрозофилы, мы можем сделать следующее: %timeit -r 1 -n 1dm = 'data/dm6.fa' model = tz.pipe(dm, genome, c.take(10**7), markov) # Мы используем `take`, чтобы построить модель на первых 10 млн оснований # с целью ускорения обработки. Шаг, связанный с функцией take, # можно пропустить, если вы готовы подождать ~5-10 мин. 1 loop, average of 1: 24.3 s +- 0 ns per loop (using standard deviation) В этом примере происходит целый ряд событий, и мы собираемся его разло- жить по полочкам шаг за шагом. В конце главы мы выполним данный пример. Первое, на что следует обратить внимание, – это ряд функций из библиотеки Toolz1. Например, из библиотеки Toolz мы взяли функции pipe, sliding_window , frequencies и каррированную версию функции map (подробнее об этом позднее). Это вызвано тем, что библиотека Toolz специально написана для мобилизации преимуществ итераторов Python и может легко управлять потоками. Начнем с функционального конвейера pipe. Эта функция представляет со- бой синтаксический сахар, облегчающий чтение вызовов вложенных функ - ций. Данная возможность приобретает особое значение, так как такой функ - циональный шаблон получает все большее распространение, когда приходится иметь дело с итераторами. В качестве простого примера перепишем наше скользящее среднее с ис - пользованием функции pipe: import toolz as tz filename = 'data/expr.tsv' 1 См. http://toolz.readthedocs.org/en/latest/. 3 / 27\n--- Страница 219 ---\nПодсчет k-мер и исправление ошибок  219 mean = tz.pipe(filename, readtsv, add1, log, running_mean) # Это эквивалентно следующему вложению функций: # running_mean(log(add1(readtsv(filename)))) вход в running_mean вход в log вход в add1вход в readtsvчтение строки 0добавление 1 к строке 0взятие логарифма массива 0добавление строки 0 к скользящему среднему чтение строки 1 добавление 1 к строке 1 взятие логарифма массива 1 добавление строки 1 к скользящему среднему чтение строки 2добавление 1 к строке 2взятие логарифма массива 2добавление строки 2 к скользящему среднемучтение строки 3добавление 1 к строке 3взятие логарифма массива 3добавление строки 3 к скользящему среднему чтение строки 4 добавление 1 к строке 4взятие логарифма массива 4добавление строки 4 к скользящему среднемувыход из readtsvвыход из add1 выход из log возвращение среднего значения То, что первоначально представляло собой многочисленные строки или кучу беспорядочных круглых скобок, теперь является четким описанием по- следовательных преобразований входных данных. И его гораздо легче понять! Данная стратегия также имеет преимущество над оригинальной реализа- цией NumPy: если мы масштабируем наши данные до миллионов или милли-ардов строк, нашему компьютеру пришлось бы из последних сил держать все данные в оперативной памяти. Напротив, здесь мы загружаем по одной строке из жесткого диска и обеспечиваем поддержание данных в объеме всего одной строки. поДсчет k-мер и испраВление ошибок Чтобы освежить свою память по информации о ДНК и геномике, вы, возможно, захотите просмотреть главы 1 и 2. Если коротко, то ваша генетическая инфор-мация, т. е. рецепт, по которому вы были созданы, закодирована в вашем гено- ме в виде последовательности химических оснований. Они чрезвычайно малы, 4 / 27\n--- Страница 220 ---\n220  Большие данные с Toolz в маленьком ноутбуке поэтому вы не сможете просто взять микроскоп и их прочитать. Вы также не сможете прочитать их длинную цепочку: ошибки накапливаются, и считыва-ние становится непригодным. (Новые технологии это исправляют, но здесь мы сосредоточимся на наиболее распространенных сегодня коротко прочитанных данных секвенирования.) К счастью, все до единой клетки имеют идентичную копию вашего генома. Поэтому мы можем расщепить эти копии на крошеч-ные сегменты (приблизительно 100 оснований в длину) и затем собрать их как огромный пазл из 30 миллионов фрагментов. Перед выполнением сборки чрезвычайно важно выполнить коррекцию про- чтений. За время секвенирования ДНК некоторые основания прочитываются неправильно и должны быть исправлены, иначе они испортят сборку. (Пред-ставьте фрагменты пазла неправильной формы.) Одна из стратегий исправления состоит в том, чтобы в вашем наборе данных найти схожие прочтения и исправить ошибку, выхватив из этих прочтений правильную информацию. Или, в качестве альтернативы, вы можете полно-стью отказаться от прочтений с ошибками. Однако этот способ очень неэффективный, потому что отыскание схожих прочтений означает необходимость сравнивать каждое прочтение со всеми другими прочтениями. На это потребуется N 2 операций, или 9 × 1014 для 30-мил- лионного набора прочтений! (К тому же эти операции далеко не дешевые.) Существует еще один способ. Павел Певзнер (Pavel Pevzner) и другие1 поня- ли, что прочтения могут быть разбиты на меньшие, накладывающиеся k-меры2, подстроки длины k, которые затем сохраняются в хеш-таблице (в Python это словарь). У этого способа есть масса преимуществ. Главное преимущество – в том, что вместо вычислений на общем количестве прочтений, которое может быть произвольно большим, мы можем вычислять на общем количестве k-мер, равном величине самого генома. Это обычно на один-два порядка меньше ко-личества прочтений. Если мы выбираем довольно большую величину k, чтобы гарантировать появление в геноме любого k-мер только один раз, то количество появлений k-мер в точности равно количеству прочтений, приходящих из этого участка генома. Такое количество называется покрытием данного участка. Если в прочтении есть ошибка, то существует высокая вероятность, что на- кладывающиеся на ошибку k-меры будут в геноме уникальными или близкими к уникальным. Ближайщую аналогию можно найти в английской литературе: если бы вам пришлось брать прочтения из Шекспира и одним из прочтений было «быть или ну быть», то встречаемость 7-мер «ну быть» будет редкой либо вовсе нулевой, тогда как встречаемость «не быть» будет очень частой. 1 См. http://www.pnas.org/content/98/17/9748.full. 2 Термин k-мер (k-mer) обычно относится ко всем возможным подстрокам длины k, содержащимся в строке. В вычислительной геномике k-мер относится ко всем воз-можным подпоследовательностям (длины k) из прочтения, полученного при помо-щи ДНК-секвенирования. – Прим. перев. 5 / 27\n--- Страница 221 ---\nПодсчет k-мер и исправление ошибок  221 В этом основа для исправления ошибок k-мер: разбить прочтения на k-меры, подсчитать количество появлений каждого k-мер и применить логику, чтобы заменить в прочтениях редкие k-меры на схожие общие. (Или же в качестве альтернативы отбросить прочтения с ошибочными k-мерами. Последнее воз-можно благодаря тому, что прочтения присутствуют в таком изобилии, что мы можем позволить себе отбраковать ошибочные данные.) Этот пример ярко демонстрирует чрезвычайную важность потоковой обра- ботки. Как было отмечено ранее, количество прочтений может быть огром-ным, поэтому мы не хотим хранить их в оперативной памяти. Данные последовательности ДНК общепринято представлять в формате FASTA. Это текстовый формат, состоящий из одной или нескольких последо-вательностей ДНК в расчете на файл, при этом каждая имеет имя и саму по-следовательность. Типичный файл FASTA: > sequence_name1TCAATCTCTTTTATATTAGATCTCGTTAAAGTAAAATTTTGGTTTGTGTTAAAGTACAAGGGGTACCTATGACCACGGAACCAACAAAGTGCCTAAATAGGACATCAAGTAACTAGCGGTACGT > sequence_name2 ATGTCCCAGGCGTTCCTTTTGCATTTGCTTCGCATTAACAGAATATCCAGCGTACTTAGGATTGTCGACCTGTCTTGTCGTACGTGGCCGCAACACCAGGTATAGTGCCAATACAAGTCAGACTAAAACTGGTTC Теперь у нас есть требуемая информация для преобразования потока строк из файла FASTA и подсчета k-мер: отфильтровать строки так, чтобы использовались только строки после- довательностей; для каждой строки последовательности произвести поток k-мер; добавить каждый k-мер в счетчик количеств в словаре. Вот как это делается на чистом Python, не используя ничего, кроме встроен- ных функций: def is_sequence(line): line = line.rstrip() # удалить ‘\\n’ в конце строки return len(line) > 0 and not line.startswith('>') def reads_to_kmers(reads_iter, k=7): for read in reads_iter: for start in range(0, len(read) - k): yield read[start : start + k] # обратите внимание на yield, # это генератор def kmer_counter(kmer_iter): counts = {} for kmer in kmer_iter: if kmer not in counts: counts[kmer] = 0 counts[kmer] += 1 return counts 6 / 27\n--- Страница 222 ---\n222  Большие данные с Toolz в маленьком ноутбуке with open('data/sample.fasta') as fin: reads = filter(is_sequence, fin) kmers = reads_to_kmers(reads) counts = kmer_counter(kmers) Этот код прекрасно работает, и данные поступают в потоке – прочтения за- гружаются по одному из жесткого диска и передаются по конвейеру через кон- вертор k-мер в счетчик k-мер. В результате мы можем построить гистограмму количеств и подтвердить, что действительно имеется две хорошо разделенные популяции правильных и ошибочных k-мер: # Заставить графики появляться локально, задать стиль графиков %matplotlib inlineimport matplotlib.pyplot as pltplt.style.use('style/elegant.mplstyle') def integer_histogram(counts, normed=True, xlim=[], ylim=[], *args, **kwargs): hist = np.bincount(counts) if normed: hist = hist / np.sum(hist) fig, ax = plt.subplots() ax.plot(np.arange(hist.size), hist, *args, **kwargs) ax.set_xlabel('количества') ax.set_ylabel('частота') ax.set_xlim(*xlim) ax.set_ylim(*ylim) counts_arr = np.fromiter(counts.values(), dtype=int, count=len(counts)) integer_histogram(counts_arr, xlim=(-1, 250)) частота количества Обратите внимание на замечательное распределение частот k-мер и боль- шую вогнутость k-мер (в левой части графика), которая появляется всего один раз. Такие низкочастотные k-меры, скорее всего, являются ошибками. 7 / 27\n--- Страница 223 ---\nКаррирование: изюминка потоковой обработки  223 Но в приведенном выше программном коде мы на самом деле выполняем слишком много работы. Значительная часть функциональности, которую мы поместили в циклы for и инструкцию yield , на самом деле манипулирует по- током: преобразование потока данных в другой вид данных и его накопление в конце. Библиотека Toolz имеет целый ряд примитивов для управления по- током, которые облегчают написание вышеупомянутого программного кода всего в одном вызове функции; причем по именам преобразующих функций также становится легче визуализировать то, что происходит в каждой точке с вашим потоком данных. Например, функция скользящего окна sliding_window представляет собой именно то, что нам нужно для создания k-мер: print(tz.sliding_window.__doc__) Последовательность накладывающихся последовательностей >>> list(sliding_window(2, [1, 2, 3, 4])) [(1, 2), (2, 3), (3, 4)] Эта функция создает скользящее окно, подходящее для таких преобразований, как скользящие средние / сглаживание >>> mean = lambda seq: float(sum(seq)) / len(seq) >>> list(map(mean, sliding_window(2, [1, 2, 3, 4]))) [1.5, 2.5, 3.5] Кроме того, функция частот frequencies подсчитывает количество появлений отдельных значений в потоке данных. Вместе с конвейером pipe теперь мы мо- жем подсчитать k-меры в одном вызове функции: from toolz import curried as c k = 7 counts = tz.pipe('data/sample.fasta', open, c.filter(is_sequence), c.map(str.rstrip), c.map(c.sliding_window(k)), tz.concat, c.map(''.join), tz.frequencies) Но постойте: что это за вызовы функций с префиксом c из пространства имен toolz.curried ? каррироВание : изюминка потокоВой обработки Ранее мы запросто использовали каррированную версию функции map, которая применяет заданную функцию к каждому элементу в последовательности. Те- перь, когда мы примешали туда еще несколько каррированных вызовов, пора поделиться с вами, что это означает! Операция каррирования получила такое название не по имени пряной приправы «карри» (хотя, надо признать, она действительно приправляет ваш программный код). Она названа в честь Кар- 8 / 27\n--- Страница 224 ---\n224  Большие данные с Toolz в маленьком ноутбуке ри Хаскелла, математика, который изобрел это понятие. Карри Хаскелл также является тезкой языка программирования Haskell, в котором все функции кар- рированы! «Каррирование» означает частичное вычисление функции и возвращение еще одной «меньшей» функции. В обычной ситуации, если в Python не предо-ставить функции все необходимые аргументы, она закатит истерику. Напро-тив, каррированная функция может принимать лишь некоторые из этих аргу - ментов. Если каррированная функция не получает достаточно аргументов, то возвращает новую функцию, принимающую остальные аргументы. Когда эта вторая функция вызывается с остальными аргументами, она может выполнить исходную задачу. Еще одним термином для каррирования является частичное применение. В функциональном программировании каррирование является средством порождения функции, ожидающей остальных аргументов, которые появятся позже. Поэтому, в отличие от вызова функции map(np.log, numbers_list) , применя- ющей функцию np.log ко всем числам в списке numbers_list (возвращая после- довательность логарифмированных чисел), вызов каррированной функции toolz.curried.map(np.log) возвращает функцию, принимающую последователь- ность чисел и возвращающую последовательность логарифмированных чисел. Как оказалось, наличие функции, которая уже знает некоторые аргументы, идеально подходит для потоковой обработки! В приведенном выше фрагменте кода мы уже увидели наметки, показывающие, насколько мощным может быть каррирование совместно с конвейерами. Однако когда вы начинаете впервые, каррирование может заставить вас поднапрячься. Поэтому мы попробуем эту операцию на нескольких простых примерах, чтобы продемонстрировать, как она работает. Давайте начнем с того, что напишем простую некаррированную функцию: def add(a, b): return a + b add(2, 5) 7 Теперь мы пишем похожую функцию, которую прокаррируем вручную: def add_curried(a, b=None): if b is None: # Второй аргумент не задан, поэтому создать функцию и ее вернуть def add_partial(b): return add(a, b) return add_partial else: # Оба аргумента заданы, поэтому просто вернуть значение return add(a, b) Теперь давайте испытаем каррированную функцию, чтобы удостовериться в том, что она делает именно то, что мы ожидаем: 9 / 27\n--- Страница 225 ---\nКаррирование: изюминка потоковой обработки  225 add_curried(2, 5) 7 Замечательно. Когда заданы обе переменные, она действует как нормальная функция. Теперь давайте исключим вторую переменную: add_curried(2)<function __main__.add_curried.<locals>.add_partial> Как мы и ожидали, она вернула функцию. Теперь давайте применим эту воз- вращенную функцию: add2 = add_curried(2) add2(5) 7 Сработало! Правда, читабельность функции add_curried вне всякой критики. При ее использовании в будущем, вероятно, могут возникнуть затруднения, и придется поднапрячься, чтобы вспомнить, зачем мы написали этот фраг - мент кода. К счастью, библиотека Toolz имеет инструменты, которые способны нас выручить. import toolz as tz @tz.curry # Применить curry в качестве декоратора def add(x, y): return x + y add_partial = add(2) add_partial(5) 7 Подводя итог сделанному, функция add теперь является каррированной функцией. Поэтому она может принимать один из аргументов и возвращать еще одну функцию, add_partial , которая «запоминает» этот аргумент. На самом деле все функции библиотеки Toolz также доступны в карриро- ванном виде и находятся в пространстве имен toolz.curried . Библиотека Toolz также включает каррированные версии некоторых вспомогательных функ - ций Python высшего порядка, таких как map, filter и reduce . Мы импортируем пространство имен curried под псевдонимом c, чтобы не загромождать наш программный код. Так, к примеру, каррированной версией функции map будет c.map . Стоит отметить, что каррированные функции (например, c.map ) отлича- ются от декораторов @curry , которые используются для создания каррирован- ных функций. from toolz import curried as c c.map <class 'map'> 10 / 27\n--- Страница 226 ---\n226  Большие данные с Toolz в маленьком ноутбуке В качестве напоминания заметим, функция map встроена в Python. Из до- кументации1: map(функция, итерируемый_объект, ) Возвращает итератор, применяющий функцию к каждому значению итерируемого объекта, производя результат ее применения. Каррированная версия функции map особенно удобна во время работы в кон- вейере библиотеки Toolz. Используя конвейер tz.pipe , вы можете передать свою функцию в каррированную функцию c.map и позже направить потоком итератор. Взгляните еще раз на нашу функцию чтения генома, чтобы увидеть, как все это работает на практике. def genome(file_pattern): «»»Передать геном потоком, буква за буквой, из списка имен файлов FASTA.»»» return tz.pipe(file_pattern, glob, sorted, # Имена файлов c.map(open), # строки # конкатенировать строки из всех файлов: tz.concat, # отбросить заголовок из каждой последовательности c.filter(is_sequence), # конкатенировать символы из всех строк tz.concat, # отбросить символы новой строки и ‘N’ c.filter(is_nucleotide)) ВозВращаясь к поДсчету k-мер Итак, надо полагать, теперь мы ориентируемся в каррировании. Давайте вер- немся к нашему программному коду подсчета k-мер. Ниже еще раз приведем фрагмент кода с использованием этих каррированных функций: from toolz import curried as c k = 7 counts = tz.pipe('data/sample.fasta', open, c.filter(is_sequence), c.map(str.rstrip), c.map(c.sliding_window(k)), tz.concat, c.map(''.join), tz.frequencies) Теперь мы можем увидеть частоту различных k-мер: counts = np.fromiter(counts.values(), dtype=int, count=len(counts)) integer_histogram(counts, xlim=(-1, 250), lw=2) 1 См. https://docs.python.org/3.4/library/functions.html%23map. 11 / 27\n--- Страница 227 ---\nВозвращаясь к подсчету k-мер  227 частота количества Советы для работы с потоками • «Список списков» следует преобразовывать в «длинный список», используя для этого функцию tz.concat. • Не попадитесь на следующем: – итераторы потребляются. Поэтому если вы создаете объект генератора и выпол- няете с ним некую обработку и потом на каком-то последующем шаге терпите неудачу, то вам следует создать генератор заново. Оригинал перестает существо-вать; – итераторы ленивы. Вам необходимо иногда вызывать вычисление. • Когда в конвейере много функций, иногда трудно установить, где работа пошла не так, как надо. Возьмите небольшой поток и добавляйте в свой конвейер по одной функции, начиная с первой/крайней левой, до тех пор, пока вы не найдете ту, которая вызывает ошибку. Вы также можете вставить выражение map(do(print)) (функции map и do взяты из пространства имен toolz.curried) в любую точку в потоке, чтобы распе- чатывать каждый элемент во время его потоковой передачи. Задача: анализ главных компонент потоковых данных В библиотеке scikit-learn существует класс IncrementalPCA , позволяющий выпол- нять анализ главных компонент (principal components analysis, PCA) набора данных, не загружая полный набор данных в оперативную память. Однако при этом вам необходимо разбить свои данные на блоки самостоятельно, что де-лает использование вашего программного кода немного неудобным. Создайте функцию, которая способна принимать поток образцов данных и выполнять PCA. Затем примените эту функцию, чтобы вычислить PCA набора данных iris («Ирисы Фишера»), предназначенного для тестирования алгоритмов машин-ного обучения, который находится в файле data/iris.csv. (Доступ к этому набору данных можно также получить из модуля datasets библиотеки scikit-learn, ис - пользуя datasets.load_iris() .) По желанию вы можете раскрасить точки с номе- рами видов цветков из файла data/iris-target.csv. 12 / 27\n--- Страница 228 ---\n228  Большие данные с Toolz в маленьком ноутбуке Класс IncrementalPCA находится в модуле sklearn.decomposition и для пакетной тре- нировки модели требует размер пакета больше 1. Что касается того, как создавать по-ток пакетов из потока точек данных, следует обратить внимание на функцию toolz. curried.partition. маркоВская моДель на осноВе полного генома Вернемся к нашему первоначальному примеру программного кода. Что такое марковская модель, и в чем ее прелесть? В целом марковская модель, или модель Маркова, исходит из того, что веро- ятность перехода системы в заданное состояние зависит только от состояния, в котором она находилась непосредственно перед этим. Например, если сей-час солнечно, то имеется высокая вероятность, что завтра тоже будет солнечно. И тот факт, что вчера шел дождь, не имеет никакого значения. В этой теории вся информация, необходимая для предсказания будующего, закодирована в текущем состоянии дел. Прошлое не имеет значения. Это допущение полез-но тем, что упрощает решение задач, которые не поддаются решению в других условиях, и часто дает хорошие результаты. Марковские модели, например, лежат в основе большинства методов обработки сигналов в мобильных теле-фонах и спутниковой связи. В контексте геномики, как мы увидим, разные функциональные участки ге- нома имеют разные переходные вероятности между аналогичными состояния- ми. Встречая их в новом геноме, мы можем делать некие предсказания о функ - ции этих участков. Возвращаясь к погодной аналогии, вероятность перехода от солнечного дня к дождливому очень отличается в зависимости от местополо-жения: находитесь вы в Лос-Анджелесе или в Лондоне. Поэтому, если я дам вам последовательность дней (солнечно, солнечно, солнечно, дождливо, солнечно, …), при условии что ваша модель предварительно была натренирована, вы мо-жете предсказать, откуда она поступила: из Лос-Анджелеса или из Лондона. В этой главе мы пока затронем только построение модели.Для этого вам надо скачать файл с геномом дрозофилы фруктовой (плодо- вой мушки) dm6.fa.gz 1. И вам придется его распаковать, применив команду gzip -d dm6.fa.gz . В геномных данных генетическая последовательность, состоящая из букв A, C, G и T, содержит информацию о принадлежности к повторяющимся элемен- там, т. е. особому классу ДНК. Эта информация закодирована регистром букв, т. е. тем, находятся ли буквы последовательности в нижнем регистре (повто-ряющиеся элементы) или в верхнем регистре (неповторяющиеся элементы). Мы можем использовать эту информацию во время построения марковской модели. Запрограммируем марковскую модель в виде массива NumPy. А чтобы ин- дексировать по буквам в индексы в [0, 7] (имя LDICT обозначает «словарь букв») 1 См. http://hgdownload.cse.ucsc.edu/goldenPath/dm6/bigZips/. 13 / 27\n--- Страница 229 ---\nМарковская модель на основе полного генома  229 и по парам букв в двумерные индексы в ([0, 7], [0, 7]) (имя PDICT обозначает «словарь пар»), создадим словари: import itertools as it LDICT = dict(zip('ACGTacgt', range(8))) PDICT = {(a, b): (LDICT[a], LDICT[b]) for a, b in it.product(LDICT, LDICT)} Кроме того, следует отфильтровать данные, которые не принадлежат после- довательностям: имена последовательностей, находящиеся в строках и начи- нающиеся с символа >, и неизвестную последовательность, которая помечена как N. Для этого мы создадим функции с критериями, на основе которых будет выполняться фильтрация: def is_sequence(line): return not line.startswith('>') def is_nucleotide(letter): return letter in LDICT # игнорировать ‘N’ Наконец, всякий раз, когда мы будем получать новую нуклеотидную пару, скажем, (A, T), будем наращивать нашу марковскую модель (матрицу NumPy) в соответствующей позиции. Для выполнения этой работы мы создадим кар- рированную функцию: import toolz as tz @tz.curry def increment_model(model, index): model[index] += 1 Теперь мы можем эти элементы объединить и направить геном потоком в нашу матрицу NumPy. Обратите внимание, если seq является потоком, то нам не придется втискивать в оперативную память ни полный геном, ни даже большой кусок генома! from toolz import curried as c def markov(seq): «»»Получить марковскую модель первого порядка из последовательности нуклеотидов.»»» model = np.zeros((8, 8)) tz.last(tz.pipe(seq, c.sliding_window(2), # каждый последующий кортеж c.map(PDICT.__getitem__), # позиция в матрице кортежей c.map(increment_model(model)))) # нарастить матрицу # Преобразовать матрицу переходных вероятностей model /= np.sum(model, axis=1)[:, np.newaxis] return model Теперь от нас требуется только запустить этот геномный поток и создать нашу марковскую модель: 14 / 27\n--- Страница 230 ---\n230  Большие данные с Toolz в маленьком ноутбуке from glob import glob def genome(file_pattern): «»»Передать геном потоком, буква за буквой, из списка имен файлов FASTA.»»» return tz.pipe(file_pattern, glob, sorted, # Имена файлов c.map(open), # строки # конкатенировать строки из всех файлов: tz.concat, # отбросить заголовок из каждой последовательности c.filter(is_sequence), # конкатенировать символы из всех строк tz.concat, # отбросить символы новой строки и ‘N’ c.filter(is_nucleotide)) Давайте теперь проверим геном дрозофилы (плодовой мушки): # Скачать dm6.fa.gz из ftp://hgdownload.cse.ucsc.edu/goldenPath/dm6/bigZips/ # Перед использованием распаковать: gzip -d dm6.fa.gzdm = 'data/dm6.fa' model = tz.pipe(dm, genome, c.take(10**7), markov) # Мы используем `take`, чтобы построить модель на первых 10 млн оснований # с целью ускорения обработки. Шаг, связанный с функцией take,# можно пропустить, если вы готовы подождать ~5–10 мин. А теперь давайте посмотрим на результирующую матрицу: print(' ', ' '.join('ACGTacgt'), '\\n')print(model) A C G T a c g t [[ 0.348 0.182 0.194 0.275 0. 0. 0. 0. ] [ 0.322 0.224 0.198 0.254 0. 0. 0. 0. ][ 0.262 0.272 0.226 0.239 0. 0. 0. 0. ][ 0.209 0.199 0.245 0.347 0. 0. 0. 0. ][ 0.003 0.003 0.003 0.003 0.349 0.178 0.166 0.296] [ 0.002 0.002 0.003 0.003 0.376 0.195 0.152 0.267] [ 0.002 0.003 0.003 0.002 0.281 0.231 0.194 0.282][ 0.002 0.002 0.003 0.003 0.242 0.169 0.227 0.351]] Вероятно, полученный результат будет яснее, если продемонстрировать его на диаграмме (рис. 8.1): def plot_model(model, labels, figure=None): fig = figure or plt.figure() ax = fig.add_axes([0.1, 0.1, 0.8, 0.8]) im = ax.imshow(model, cmap='magma'); axcolor = fig.add_axes([0.91, 0.1, 0.02, 0.8]) plt.colorbar(im, cax=axcolor) for axis in [ax.xaxis, ax.yaxis]: axis.set_ticks(range(8)) axis.set_ticks_position('none') axis. set_ticklabels(labels) return ax plot_model(model, labels='ACGTacgt'); 15 / 27\n--- Страница 231 ---\nМарковская модель на основе полного генома  231 Рис. 8.1  Матрица переходных вероятностей для генетической последовательности в геноме дрозофилы фруктовой Обратите внимание, насколько переходы C–A и G–C отличаются в повто- ряющихся и неповторяющихся участках генома. Эта информация может ис - пользоваться для классификации ранее не встречавшейся последовательности ДНК. Задача: онлайновая распаковка архива Рекомендуем в начало конвейера добавить шаг, который будет выполнять распаковку данных из архива, избавляя вас от необходимости держать рас - пакованную версию данных на своем жестком диске. Будучи в сжатом виде (модулем gzip), геном дрозофилы, например, занимает меньше одной трети пространства жесткого диска. И разумеется, распаковка архива тоже может вы-полняться в потоке!  Пакет gzip, являющийся частью стандартной библиотеки Python, позволяет открывать файлы .gz , как если бы они были обычными файлами. Мы надеемся, что нам удалось хотя бы дать вам намек, как легко можно вы- полнять потоковую обработку в Python, при использовании всего нескольких абстракций, наподобие тех, которые обеспечивает библиотека Toolz. Потоковая обработка может сделать вашу работу продуктивнее, потому что работа с большим объемом данных занимает линейно более продолжительное время, чем с малым объемом данных. В пакетном анализе работа с большим объемом данных может продолжаться бесконечно, потому что операционной системе придется все время передавать данные из ОЗУ на жесткий диск и об-ратно. Или, что еще хуже, Python и вовсе может отказаться их обрабатывать, 16 / 27\n--- Страница 232 ---\n232  Большие данные с Toolz в маленьком ноутбуке просто показав ошибку MemoryError ! Все это означает, что в случае проведения различных видов анализа больших наборов данных вам не потребуется более мощная машина. И если ваши тесты успешно проходят на малых объемах дан- ных, то они успешно пройдут и на больших объемах данных! Наша ключевая мысль заключается в следующем: при написании алгоритма или исследования подумайте, сможете ли вы применить потоковую обработку. Если да, то делайте это с самого начала. В будущем вы сами скажете себе спаси-бо. Позже сделать это будет сложнее, и в конечном счете приведет к ситуации, как на рис. 8.2. * Рис. 8.2  Списки неотложных дел в истории (карикатурист Ману Корне, использовано с разрешения) * TODO (Джованни): укрепить южный фундамент до наступления зимы 1273 17 / 27\n--- Страница 233 ---\nЭпилог Качество означает делать все пра- вильно, когда никто не смотрит. – Генри Форд В этой книге наша главная задача состояла в том, чтобы помочь в распростра-нении элегантного применения библиотек NumPy и SciPy. Обучив вас тому, как при помощи библиотеки SciPy выполнять эффективный научный анализ, мы надеемся, что вселили в вас ощущение, что качественный программный код заслуживает необходимых для него усилий. что Дальше ? Теперь, когда вы знаете библиотеку SciPy в мере, чтобы проводить анализ лю-бых данных, встающих на вашем пути, встает вопрос: что нужно сделать, что-бы пойти дальше? В самом начале книги мы сказали, что у нас не было никаких замыслов охватить все, что только можно узнать о библиотеке и всех ее ответв-лениях. Прежде чем мы разойдемся, хотим направить вас к многочисленным ресурсам, которые существуют, чтобы вам помочь. Списки рассылок В предисловии мы упомянули, что SciPy является сообществом. Отличный спо-соб продолжить обучение – подписаться на главные списки рассылок на биб-лиотеки NumPy, SciPy, pandas, Matplotlib, scikit-image и другие, в которых вы можете быть заинтересованы, и регулярно их читать. И когда вы по-настоящему попадете в тупик в своей работе, не бойтесь обра- титься туда за помощью! Мы – дружелюбная компания! При поиске помощи от вас требует только одно. Вы должны показать, что вы пытались решить задачу самостоятельно, и предоставить другим участникам минимальный сценарий и образец данных в достаточном объеме, чтобы продемонстрировать вашу проблему и то, как вы попытались ее исправить. Нет: «Мне нужно сгенерировать большой массив случайных гауссиан. Кто-нибудь сможет помочь?» Нет: «У меня вот тут огромная библиотека: https://github.com/ron_obvious. Если вы посмотрите в статистическую библиотеку, то там есть фрагмент, для которого реально требуются случайные гауссианы. Кто-нибудь смо-жет взглянуть???» Да: «Я попытался сгенерировать большой список случайных гаус - сиан, вот так: gauss = [np.random.randn ()] * 10 ** 5. Но когда я вычисляю np.mean(gauss) , у меня практически ни разу не получались значения, 18 / 27\n--- Страница 234 ---\n234  Эпилог близкие к 0, как хотел бы я. Что я делаю неправильно? Полный сценарий прилагается ниже». GitHub В предисловии мы также поговорили о GitHub. Весь рассмотренный нами в этой книге программный код и другие материалы лежат на GitHub: NumPy 1; SciPy2. Когда что-то, вопреки вашим ожиданиям, не работает, возможно, причиной тому является ошибка. Если после небольшого расследования вы убеждены, что действительно обнаружили ошибку, вам следует пройти в закладку «issues» (вопросы) соответствующего хранилища GitHub и создать новый вопрос. Тем самым будет гарантировано, что ошибка попадет на радар разработчиков этой библиотеки и что она (надо надеяться) будет исправлена в следующей ее вер-сии. Между прочим, этот совет также относится и к «ошибкам» в документации: если в документации библиотеки вам что-то не ясно, зарегистрируйте вопрос! Помимо регистрации вопроса, будет еще лучше, если вы отправите запрос на включение внесенных изменений. Такой запрос с улучшением документации библиотеки является отличным способом попробовать свои силы в открытом исходном коде! Мы здесь не будем рассматривать весь этот процесс, однако предложим вам целый ряд книг и ресурсов, которые вам помогут: «Эффективные вычисление в физике» (Anthony Scopatz, Katy Huff. Effec - tive Computation in Physics. O’Reilly, 2015) 3. В этой книге среди многих других тем в области научных вычислений рассматриваются Git и GitHub; «Введение в GitHub» (Peter Bell, Brent Beer. Introducing GitHub. O’Reilly, 2015) 4. В этой книге GitHub рассматривается более подробно; «Организация Software Carpentry»5 содержит уроки по Git и круглый год предлагает бесплатные семинары по всему миру; один из авторов книги, частично опираясь на эти уроки, создал закон-ченное учебное пособие по включению внесенных изменений с участи-ем Git и GitHub «Открытая наука с Git и GitHub» (Open Source Science with Git and GitHub) 6; наконец, многие проекты с открытым исходным кодом на GitHub имеют файл «CONTRIBUTING» 7, который содержит ряд рекомендаций по учас - тию своим программным кодом в проекте. 1 См. https://github.com/numpy/numpy. 2 См. https://github.com/scipy/scipy. 3 См. http://shop.oreilly.com/product/0636920033424.do. 4 См. http://shop.oreilly.com/product/0636920033059.do. 5 См. https://software-carpentry.org/. 6 См. http://jni.github.io/git-tutorial/. 7 См. https://github.com/scikit-image/scikit-image/blob/master/CONTRIBUTING.txt. 19 / 27\n--- Страница 235 ---\nЗа пределами SciPy  235 Одним словом, интеллектуальный голод из-за отсутствия помощи по этой теме вам не грозит! Мы приглашаем вас как можно чаще принимать участие в экосистеме SciPy не только потому, что вы сможете поспособствовать, чтобы эти библиотеки стали лучше для всех, но и потому, что это один из лучших способов развить свои навыки программирования. С каждым отправляемым вами запросом на включение внесенных изменений вы получите отзыв о своем программном коде, который поможет вам стать лучше. Вы сможете ближе познакомиться с процессом участия в GitHub и этикетом, которые являются весьма ценными навыками на сегодняшнем рынке труда. Конференции Если вам понравилась эта книга, мы настоятельно рекомендуем посещать конференции по программированию в этой области. Фантастическая конфе-ренция SciPy каждый год проводится в Остине, шт. Техас. Вероятно, это самая лучшая конференция из возможных вариантов. Также существует европейская версия конференции, EuroSciPy, которая каждые два года проводится в другом городе-организаторе. Наконец, существует более общая конференция PyCon, проводимая в Соединенных Штатах. Но эта конференция имеет ответвления по всему миру, такие как PyCon-AU в Австралии, у которой имеется мини-кон-ференция «Наука и данные», проводимая за день до главной конференции. Какую бы конференцию вы ни выбрали, дождитесь спринтерских состяза- ний в конце конференции. Программистский спринт – это интенсивный сеанс командного программирования и фантастическая возможность изучить про-цесс внесения своего вклада в открытый исходный код, независимо от вашего уровня квалификации. Именно так один из ваших авторов (Хуан) начал свое путешествие по открытому исходному коду. за преДелами SciPy Библиотека SciPy написана не только на Python, но и на высокооптимизиро-ванных языках C и Fortran, которые взаимодействуют с Python. Вместе с NumPy и связанными с ней библиотеками SciPy пытается охватить значительную часть вариантов использования, которые возникают в научном анализе дан-ных, и для этого обеспечивает очень быстрые функции. Тем не менее иногда научная задача совершенно не совпадает с тем, что уже есть в SciPy, и чистое Python’овское решение может оказаться бесполезным, так как не дает доста-точной скорости. Что же делать? Высокоэффективный Python 1 (High Performance Python. O’Reilly, 2014) под ав- торством Мичи Горелика и Иэна Озсвалда (Micha Gorelick, Ian Ozsvald) рассмат - ривает все, что вам нужно знать в таких ситуациях: как найти то место, где вам 1 См. http://shop.oreilly.com/product/0636920028963.do. 20 / 27\n--- Страница 236 ---\n236  Эпилог действительно нужна производительность, и какие существуют варианты по- лучения этой производительности. Мы настоятельно рекомендуем эту книгу. Далее мы кратко упомянем два варианта, которые особенно актуальны для мира SciPy. В первую очередь это Cython. Cython является версией Python, допускающей компилирование в C и затем обратное импортирование в Python. Добавление некоторых аннотаций типов в переменные Python приводит к тому, что скомпи-лированный в C программный код может быть в сотни, а то и тысячи раз быст рее сравнимого программного кода на Python. Cython сегодня является промышлен-ным стандартом и используется в NumPy, SciPy и многих связанных с ними би-блиотеках (таких как scikit-image), обеспечивая быстрые алгоритмы в программ-ном коде на основе массивов. Курт Смит (Kurt Smith) написал книгу с прос тым названием Cython 1 (O’Reilly, 2015), которая научит вас основам этого языка. Нередко более легкой в использовании альтернативой Cython является Numba, JIT-компилятор для программного кода Python на основе массивов. JIT- компиляторы («just-in-time», или динамические, компилирующие «на лету») ждут первого исполнения функции и в точке исполнения логически выводят тип всех аргументов функции и результатов и компилируют программный код в высоко-эффективную форму для этих конкретных типов. В Numba аннотировать типы не нужно: Numba выведет их тип логически, когда впервые будет вызвана функция. Вместо этого вам просто нужно придерживаться правила: использовать только элементарные типы (целые, вещественные и т. д.) и массивы, исключив более сложные объекты Python. В этих случаях Numba сможет скомпилировать про-грамму на Python в очень эффективный код и ускорить вычисления на порядки. Numba по-прежнему остается очень молодым JIT-компилятором Python, но уже очень полезен. Немаловажно: Numba показывает, какие существуют воз-можности при реализации приложений на основе JIT-компиляторов. А они, судя по всему, собираются стать более распространенным явлением: в Py thon 3.6 до - бавлены возможности, облегчающие использование новых JIT-компи ля то ров (Pyjion JIT основан как раз на них). В блоге Хуана 2 вы сможете увидеть неко- торые примеры использования Numba, включая те, которые показывают, как его объединять с SciPy. И разумеется, у Numba есть свой собственный, очень активный и дружественный список рассылки. соДейстВие этой книге Непосредственно сам источник книги размещен на GitHub3 (а также на веб- сайте Elegant SciPy4). Вы точно так, как если бы вы участвовали в любом другом проекте с открытым исходным кодом, можете поднимать любые вопросы или, 1 См. http://shop.oreilly.com/product/0636920033431.do. 2 См. https://ilovesymposia.com/tag/numba/. 3 См. https://github.com/elegant-scipy/elegant-scipy. 4 См. http://elegant-scipy.org/. 21 / 27\n--- Страница 237 ---\nДо следующей встречи  237 если найдете ошибки или опечатки, отправлять любые запросы на включение внесенных изменений. Мы будем вам весьма признательны за это. Мы, чтобы проиллюстрировать различные части библиотек SciPy и NumPy, использовали самые лучшие примеры программного кода, которые смогли найти. Если у вас есть более оптимальный пример, пожалуйста, поднимите во-прос в хранилище. Мы с удовольствием включим его в будущие издания книги. Мы также находимся в Twitter по хештегу @elegantscipy. Напишите нам, если захотите поговорить о книге! Каждого автора по отдельности вы найдете по хештегам @jnuneziglesias, @stefanvdwalt и @hdashnow. Нам очень хотелось бы услышать, используете ли вы какую-либо из идей или какой-либо из фрагментов кода из этой книги для совершенствования своего научного исследования. Ведь в этом и заключается суть SciPy! До слеД ующей Встречи Тем временем мы надеемся, что эта книга вам понравилась и вы нашли ее по-лезной для себя. Если это так, то расскажите об этом всем своим друзьям и за-ходите, чтобы поздороваться в списках рассылок, на конференции, на GitHub и в Twitter. Спасибо за чтение, и пусть будет еще более Элегантный SciPy! 22 / 27\n--- Страница 238 ---\nПриложение Решения задач решение : ДобаВление сеточного наложения Здесь показано решение задачи «Добавление сеточного наложения» главы 3. Мы можем применить сразу NumPy, чтобы отобрать строки сетки, назна- чить им синий цвет и затем отобрать столбцы и тоже назначить им синий цвет (рис. А1): def overlay_grid(image, spacing=128): «»»Вернуть изображение с сеточным наложением, используя предоставленный интервал. Параметры --------- image : массив, форма (M, N, 3) Входное изображение. spacing : целое Интервал между линиями сетки. Возвращает ---------- image_gridded : массив, форма (M, N, 3) Исходное изображение с наложенной синей сеткой. “”” image_gridded = image.copy() image_gridded[spacing:-1:spacing, :] = [0, 0, 255] image_gridded[:, spacing:-1:spacing] = [0, 0, 255] return image_gridded plt.imshow(overlay_grid(astro, 128)); Обратите внимание, согласно стандартной индексации в Python, мы исполь- зовали –1, имея в виду последнее значение оси. Вы можете это значение опус - тить, но тогда смысл будет немного отличаться. Без него (т. е. spacing::spacing ) вы пройдете до самого конца массива, включая заключительную строку/стол- бец. Когда вы используете его в качестве индекса остановки, то препятствуете отбору заключительной строки. В случае наложения сетки такое поведение, вероятно, будет желательным. 23 / 27\n--- Страница 239 ---\nРешение: игра «“Жизнь ” Конуэя»  239 Рис. А1  Изображение астронавта с наложенной сеткой решение : игра «“ж изнь” конуэя » Здесь показано решение задачи «Игра “«Жизнь» Конуэя”» из главы 3. На своей странице со 100 упражнениями NumPy1 в упражнении под номе- ром 79 Николас Ругир2 (@NPRougier) предлагает решение с использованием только NumPy: def next_generation(Z): N = (Z[0:-2,0:-2] + Z[0:-2,1:-1] + Z[0:-2,2:] + Z[1:-1,0:-2] + Z[1:-1,2:] + Z[2: ,0:-2] + Z[2: ,1:-1] + Z[2: ,2:]) # Применить правила birth = (N==3) & (Z[1:-1,1:-1]==0) survive = ((N==2) | (N==3)) & (Z[1:-1,1:-1]==1) Z[ ] = 0 Z[1:-1,1:-1][birth | survive] = 1 return Z Затем мы можем начать доску так: random_board = np.random.randint(0, 2, size=(50, 50))n_generations = 100 1 См. http://www.labri.fr/perso/nrougier/teaching/numpy.100/. 2 См. https://github.com/rougier. 24 / 27\n--- Страница 240 ---\n240  Решения задач for generation in range(n_generations): random_board = next_generation(random_board) Использование универсального фильтра generic_filter делает это еще проще: def nextgen_filter(values): center = values[len(values) // 2] neighbors_count = np.sum(values) - center if neighbors_count == 3 or (center and neighbors_count == 2): return 1. else: return 0. def next_generation(board): return ndi.generic_filter(board, nextgen_filter, size=3, mode='constant') Замечательно то, что в некоторых формулировках игры «Жизнь» исполь- зуются так называемая тороидальная доска, а именно левый и правый концы дос ки «циклически переносятся» и соединяются друг с другом. То же самое происходит с верхним и нижним концами. С участием generic_filter модифи- кация нашего решения для ее включения реализуется обычным образом: def next_generation_toroidal(board): return ndi.generic_filter(board, nextgen_filter, size=3, mode='wrap') Теперь можно просимулировать эту тороидальную доску для нескольких по- колений: random_board = np.random.randint(0, 2, size=(50, 50))n_generations = 100 for generation in range(n_generations): random_board = next_generation_toroidal(random_board) решение : магниту Да граДиента собела Здесь показано решение задачи «Магнитуда градиента Собела» из главы 3: hsobel = np.array([[ 1, 2, 1], [ 0, 0, 0], [-1, -2, -1]]) vsobel = hsobel.T hsobel_r = np.ravel(hsobel) vsobel_r = np.ravel(vsobel) def sobel_magnitude_filter(values): h_edge = values @ hsobel_r v_edge = values @ vsobel_r return np.hypot(h_edge, v_edge) 25 / 27\n--- Страница 241 ---\nРешение: подбор кривой при помощи SciPy  241 Теперь мы можем проверить его на изображении монет: sobel_mag = ndi.generic_filter(coins, sobel_magnitude_filter, size=3) plt.imshow(sobel_mag, cmap='viridis'); решение : поДбор криВой при помощи SciPy Здесь показано решение задачи «Подбор кривой с SciPy» из главы 3. Давайте посмотрим на начало описания функции curve_fit в строке доку - ментации Python docstring: Использует нелинейные наименьшие квадраты для подгонки функции, f, к данным. Принимает ''ydata = f (xdata, *params) + eps''Параметры ---------f: вызываемая функция Модельная функция, f(x, ). Она должна принимать независимую переменную в качестве первого аргумента и подгоняемые параметры в качестве отдельных остальных аргументов.xdata: последовательность длины M или массив формы (k, M) для функций с k предикторами. Независимая переменная, где данные подлежат измерению. ydata: последовательность длины M Зависимые данные --- номинально f(xdata, ) Похоже, что нам просто нужно предоставить функцию, принимающую точку данных и несколько параметров, и вернуть предсказанное значение. В нашем случае мы хотим, чтобы кумулятивная оставшаяся частота, f(d), была пропор- циональной d–y. Это означает, что нам нужно f (d) = αd–gamma: 26 / 27\n--- Страница 242 ---\n242  Решения задач def fraction_higher(degree, alpha, gamma): return alpha * degree ** (-gamma) Затем нам нужны данные X и Y, под которые выполняется подгонка, для d > 10: x = 1 + np.arange(len(survival))valid = x > 10x = x[valid]y = survival[valid] Теперь можно применить функцию curve_fit , чтобы получить подогнанные параметры: from scipy.optimize import curve_fit alpha_fit, gamma_fit = curve_fit(fraction_higher, x, y)[0] Давайте выведем результаты на график, чтобы увидеть состояние дел: y_fit = fraction_higher(x, alpha_fit, gamma_fit)fig, ax = plt.subplots() ax.loglog(np.arange(1, len(survival) + 1), survival)ax.set_xlabel('распределение полустепени захода')ax.set_ylabel('доля нейронов с высоким распределением полустепени захода')ax.scatter(avg_in_degree, 0.0022, marker='v')ax.text(avg_in_degree - 0.5, 0.003, 'среднее=%.2f' % avg_in_degree) ax.set_ylim(0.002, 1.0) ax.loglog(x, y_fit, c='red'); распределение полустепени заходадоля нейронов с высоким распределением полустепени захода mean = 7.86 Вуаля! Полный рисунок 6B и полное соответствие! Powered by TCPDF (www.tcpdf.org) 27 / 27\n--- Страница 243 ---\nРешение: альтернативный алгоритм вычисления матрицы ошибок  243 решение : сВертыВание изображения Здесь показано решение задачи «Свертывание изображения» из главы 4. from scipy import signal x = np.random.random((50, 50)) y = np.ones((5, 5)) L = x.shape[0] + y.shape[0] - 1 Px = L - x.shape[0]Py = L - y.shape[0] xx = np.pad(x, ((0, Px), (0, Px)), mode='constant') yy = np.pad(y, ((0, Py), (0, Py)), mode='constant') zz = np.fft.ifft2(np.fft.fft2(xx) * np.fft.fft2(yy)).real print('Результирующая форма:', zz.shape, ' <-- Почему?') z = signal.convolve2d(x, y) print('Результаты равны?', np.allclose(zz, z)) Результирующая форма: (54, 54) <-- Почему? Результаты равны? True решение : Вычислительная сложность матриц ошибок Здесь показано решение задачи «Вычислительная сложность матриц ошибок» из главы 5. Из главы 1 вы помните, что выражение arr == k создает массив булевых ( True или False ) значений того же самого размера, что и массив arr. Это, как можно ожидать, требует полного обхода массива arr. Поэтому в приведенном выше решении мы выполняем обход массивов pred и gt для каждой комбинации зна- чений в pred и gt. По сути дела, мы можем вычислить cont всего за один обход обоих массивов. Поэтому многократные обходы неэффективны. решение : альтернатиВный алгоритм Вычисления матрицы ошибок Здесь показано решение задачи «Альтернативный алгоритм вычисления мат - рицы ошибок» главы 5. Мы предлагаем два решения, хотя решений может быть много.В первом решении используется встроенная в Python функция zip, чтобы выдавать попарно метки из массивов pred и gt. def confusion_matrix1(pred, gt): cont = np.zeros((2, 2)) for i, j in zip(pred, gt): cont[i, j] += 1 return cont 1 / 24\n--- Страница 244 ---\n244  Решения задач Второе решение состоит в том, чтобы перебрать все возможные индексы массивов pred и gt в цикле и вручную извлечь соответствующее значение из каждого массива: def confusion_matrix2(pred, gt): cont = np.zeros((2, 2)) for idx in range(len(pred)): i = pred[idx] j = gt[idx] cont[i, j] += 1 return cont Первый вариант может рассматриваться как более Python’овский из двух, но второй легче ускорить путем трансляции и компиляции на таких языках или в таких инструментах, как C, Cython и Numba (это тема для еще одной книги). решение : Вычисление матрицы ошибок Здесь показано решение задачи «Мультиклассовая матрица ошибок» главы 5. Чтобы определить максимальную метку, нужно выполнить только первона- чальный обход обоих входных массивов. Затем, чтобы учесть нулевую метку и индексацию Python с отсчетом от 0, мы добавляем в нее 1. Далее создаем матрицу и заполняем ее так, как было выше: def general_confusion_matrix(pred, gt): n_classes = max(np.max(pred), np.max(gt)) + 1 cont = np.zeros((n_classes, n_classes)) for i, j in zip(pred, gt): cont[i, j] += 1 return cont решение : преД стаВление В формате coo Здесь показано решение задачи «Представление в формате COO» из главы 5. Сначала мы формируем перечень ненулевых элементов массива, слева на- право и сверху вниз, как при чтении книги: s2_data = np.array([6, 1, 2, 4, 5, 1, 9, 6, 7]) Затем формируем перечень строчных индексов этих значений в том же са- мом порядке: s2_row = np.array([0, 1, 1, 1, 1, 2, 3, 4, 4]) И наконец, столбцовых индексов: s2_col = np.array([2, 0, 1, 3, 4, 1, 0, 3, 4]) Проверив их равенство в обоих направлениях, можно легко убедиться, что они порождают правильную матрицу: 2 / 24\n--- Страница 245 ---\nРешение: поворот изображения  245 s2_coo0 = sparse.coo_matrix(s2) print(s2_coo0.data)print(s2_coo0.row)print(s2_coo0.col) [6 1 2 4 5 1 9 6 7] [0 1 1 1 1 2 3 4 4] [2 0 1 3 4 1 0 3 4] и: s2_coo1 = sparse.coo_matrix((s2_data, (s2_row, s2_col)))print(s2_coo1.toarray()) [[0 0 6 0 0] [1 2 0 4 5] [0 1 0 0 0] [9 0 0 0 0] [0 0 0 6 7]] решение : поВорот изображения Здесь показано решение задачи «Поворот изображения» главы 5. Мы можем скомпоновать преобразования путем их умножения. Мы знаем, как поворачивать изображение вокруг начала координат, а также как двигать его в разные стороны. Поэтому сдвинем изображение так, чтобы его центр на-ходился в начале координат, повернем его и затем сдвинем назад. def transform_rotate_about_center(shape, degrees): «»»Вернуть гомографическую матрицу для поворота вокруг центра изображения.»»» c = np.cos(np.deg2rad(angle)) s = np.sin(np.deg2rad(angle)) H_rot = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]]) # Вычислить координаты центра изображения center = np.array(image.shape) / 2 # Матрица для центрирования изображения по началу координат H_tr0 = np.array([[1, 0, -center[0]], [0, 1, -center[1]], [0, 0, 1]]) # Матрица для перемещения центра назад H_tr1 = np.array([[1, 0, center[0]], [0, 1, center[1]], [0, 0, 1]]) # Полная матрица преобразования H_rot_cent = H_tr1 @ H_rot @ H_tr0 sparse_op = homography(H_rot_cent, image.shape) return sparse_op 3 / 24\n--- Страница 246 ---\n246  Решения задач Можно проверить, что все работает: tf = transform_rotate_about_center(image.shape, 30) plt.imshow(apply_transform(image, tf)); решение : сокращение объема потребляемой оператиВной памяти Здесь показано решение задачи «Сокращение объема потребляемой оператив- ной памяти» из главы 5. Создаваемый нами массив np.ones предназначен только для чтения: он бу - дет использоваться лишь как значения, суммируемые функцией coo_matrix . Мы можем применить функцию broadcast_to , чтобы создать аналогичный массив всего с одним элементом, «виртуально» повторенным n раз: def confusion_matrix(pred, gt): n = pred.size ones = np.broadcast_to(1., n) # виртуальный массив единиц размера n cont = sparse.coo_matrix((ones, (pred, gt))) return cont Удостоверимся, что все по-прежнему работает ожидаемым образом: cont = confusion_matrix(pred, gt)print(cont.toarray()) 4 / 24\n--- Страница 247 ---\nРешение: матрица поворота  247 [[ 3. 1.] [ 2. 4.]] Ба-бах! Вместо того чтобы создавать такой же большой массив, что и исход- ные данные, мы просто создаем массив размера 1. По мере обработки все более крупных наборов данных такая оптимизация приобретает все более важное значение. решение : Вычисление услоВной энтропии Чтобы получить таблицу совместных вероятностей, мы просто делим таблицу на ее общее количество – в данном случае на 12: Здесь показано решение задачи «Вычисление условной энтропии» из гла- вы 5. print('сумма таблицы:', np.sum(p_rain_g_month)) p_rain_month = p_rain_g_month / np.sum(p_rain_g_month) table total: 12.0 Теперь мы можем вычислить условную энтропию месяца при условии дождя . (Это похоже на вопрос: если мы знаем, что идет дождь, сколько еще нам нужно узнать информации, чтобы выяснить, какой это месяц, в среднем?) p_rain = np.sum(p_rain_month, axis=0) p_month_g_rain = p_rain_month / p_rain Hmr = np.sum(p_rain * p_month_g_rain * np.log2(1 / p_month_g_rain)) print(Hmr) 3.5613602411 Сравним этот результат с энтропией месяцев: p_month = np.sum(p_rain_month, axis=1) # 1/12, но этот метод более общий Hm = np.sum(p_month * np.log2(1 / p_month)) print(Hm) 3.58496250072 Таким образом, мы видим, что сведения о том, шел ли сегодня дождь, при- близили нас на две сотых бита к тому, чтобы угадать, какой это месяц! Не стоит ставить все на эту догадку. решение : матрица поВорота Здесь показано решение задачи «Матрица поворота» из главы 6. Часть 1 import numpy as np theta = np.deg2rad(45) R = np.array([[np.cos(theta), -np.sin(theta), 0], [np.sin(theta), np.cos(theta), 0], [ 0, 0, 1]]) 5 / 24\n--- Страница 248 ---\n248  Решения задач print(\"Произведение R на ось X:\", R @ [1, 0, 0]) print(\"Произведение R на ось Y:\", R @ [0, 1, 0])print(\"Произведение R на 45-градусный вектор:\", R @ [1, 1, 0]) Произведение R на ось X: [ 0.70710678 0.70710678 0. ] Произведение R на ось Y: [-0.70710678 0.70710678 0. ] Произведение R на 45-градусный вектор: [ 1.11022302e-16 1.41421356e+00 0.00000000e+00] Часть 2 Поскольку умножение вектора на R поворачивает его на 45 градусов, повтор- ное умножение результата на R должно привести к тому, что исходный вектор будет повернут на 90 градусов. Умножение матриц ассоциативно, имея в виду, что R(Rv ) = (RR)v, поэтому S = RR должно поворачивать векторы на 90 градусов вокруг оси z. S = R @ R S @ [1, 0, 0]array([ 2.22044605e-16, 1.00000000e+00, 0.00000000e+00]) Часть 3 print(\"R @ ось Z:\", R @ [0, 0, 1])R @ ось Z: [ 0. 0. 1.] R поворачивает ось x и ось y, но не ось z. Часть 4 Если взглянуть на документацию по функции eig, то увидим, что она возвра- щает два значения: одномерный массив собственных значений и двумерный массив, в котором каждый столбец содержит собственный вектор, соответст - вующий каждому собственному значению. np.linalg.eig(R) (array([ 0.70710678+0.70710678j, 0.70710678-0.70710678j, 1.00000000+0.j ]), array([[ 0.70710678+0.j , 0.70710678-0.j , 0.00000000+0.j ], [ 0.00000000-0.70710678j, 0.00000000+0.70710678j, 0.00000000+0.j ], [ 0.00000000-0.j , 0.00000000+0.j , 1.00000000+0.j ]])) В дополнение к нескольким комплексным собственным значениям и векто- рам мы видим значение, связанное с вектором [0, 0, 1]T. решение : изображение аффинного поДобия Здесь показано решение задачи «Изображение аффинного подобия» из гла- вы 6. В аффинном представлении вместо использования обработки на оси y мы используем нормализованный третий собственный вектор Q точно так же, как поступили с x. (И при необходимости мы его инвертируем точно так же, как поступили с x!) 6 / 24\n--- Страница 249 ---\nРешение: линейная алгебра с разреженными матрицами  249 y = Dinv2 @ Vec[:, 2] asjl_index = np.argwhere(neuron_ids == 'ASJL')if y[asjl_index] < 0: y = -y plot_connectome(x, y, C, labels=neuron_ids, types=neuron_types, type_names=['сенсорные нейроны', 'промежуточные нейроны', 'моторные нейроны'], xlabel='Аффинный собственный вектор 1', ylabel='Аффинный собственный вектор 2') Аффинный собственный вектор 1Аффинный собственный вектор 2сенсорные нейроны промежуточные нейронымоторные нейроны решение : линейная алгебра с разреженными матрицами Здесь показано решение задачи «Линейная алгебра с разреженными матрица- ми» из главы 6. Для целей этой задачи мы используем небольшой коннектом, потому что так легче визуализировать то, что происходит. В последующих частях упраж - нения мы будем использовать эти методы для анализа более крупных сетей. Прежде всего начинаем с матрицы смежности, A, в формате разреженной матрицы, в данном случае в формате CSR, т. е. в формате, который наиболее распространен в линейно-алгебраических задачах. К именам всех матриц до-бавим букву «s» (от sparse), тем самым обозначив, что они являются разрежен-ными. from scipy import sparse import scipy.sparse.linalg As = sparse.csr_matrix(A) Мы можем создать матрицу связности таким же образом: Cs = (As + As.T) / 2 7 / 24\n--- Страница 250 ---\n250  Решения задач Чтобы получить степенную матрицу, можем применить разреженный диа- гональный формат «diags», в котором хранятся диагональные и внедиагональ- ные матрицы. degrees = np.ravel(Cs.sum(axis=0)) Ds = sparse.diags(degrees) Матрица Лапласа выводится прямолинейно: Ls = Ds - Cs Теперь следует получить глубину обработки. Напомним, что о получении псевдоинверсии лапласовой матрицы не может быть и речи, так как это будет плотная матрица (инверсия разреженной матрицы в целом не является раз-реженной как таковой). Однако мы фактически использовали псевдоинвер-сию, чтобы вычислить вектор z, удовлетворяющий уравнению Lz = b, где b = c знак(A – AT)1. (Вы можете это увидеть в дополнительном материале, при- лагаемом к работе Варшни и др.) В случае плотных матриц мы можем прос - то применить z = L+b. В случае разреженных, тем не менее, можно применить один из решателей (см. в главе 6 раздел «Решатели») в модуле sparse.linalg. isolve . Чтобы получить вектор z после предоставления L и b, никакой инверсии не требуется! b = Cs.multiply((As - As.T).sign()).sum(axis=1) z, error = sparse.linalg.isolve.cg(Ls, b, maxiter=10000) Наконец, следует найти собственные векторы матрицы Q, нормализованной по степени лапласовой матрицы, соответствующей ее второму и третьему наи- меньшим собственным значениям. Из главы 5 вы, скорее всего, помните, что числовые данные в разреженных матрицах находятся в атрибуте .data . Мы используем его, чтобы инвертиро- вать степенную матрицу: Dsinv2 = Ds.copy() Dsinv2.data = 1 / np.sqrt(Ds.data) Наконец, используем линейно-алгебраические функции из модуля SciPy sparse , чтобы отыскать требующиеся собственные векторы. Матрица Q симмет - рична, поэтому можем применить функцию eigsh , специально предназначен- ную для вычисления симметрических матриц. Используем именованный аргу - мент which , чтобы указать, что нам необходимо получить собственные векторы, которые соответствуют наименьшим собственным значениям, и k и указать, что нам нужны три наименьших: Qs = Dsinv2 @ Ls @ Dsinv2vals, Vecs = sparse.linalg.eigsh(Qs, k=3, which='SM')sorted_indices = np.argsort(vals)Vecs = Vecs[:, sorted_indices] Наконец, чтобы получить координаты x и y (и при необходимости меняем их местами), нормализуем собственные векторы: 8 / 24\n--- Страница 251 ---\nРешение: линейная алгебра с разреженными матрицами  251 _dsinv, x, y = (Dsinv2 @ Vecs).T vc2_index = np.argwhere(neuron_ids == 'VC02') if x[vc2_index] < 0: x = -x if y[asjl_index] < 0: y = -y (Обратите внимание, собственный вектор, соответствующий наименьшему собственному значению, всегда является вектором единиц, который нас не интересует.) Теперь можно воспроизвести следующие ниже графики! plot_connectome(x, z, C, labels=neuron_ids, types=neuron_types, type_names=['сенсорные нейроны', 'промежуточные нейроны', 'моторные нейроны'], xlabel='Аффинный собственный вектор 1', ylabel='Глубина обработки') plot_connectome(x, y, C, labels=neuron_ids, types=neuron_types, type_names=['сенсорные нейроны', 'промежуточные нейроны', 'моторные нейроны'], xlabel='Аффинный собственный вектор 1', ylabel='Аффинный собственный вектор 2') Аффинный собственный вектор 1Глуюина обработкисенсорные нейроны промежуточные нейронымоторные нейроны 9 / 24\n--- Страница 252 ---\n252  Решения задач Аффинный собственный вектор 1Аффинный собственный вектор 2сенсорные нейроны промежуточные нейронымоторные нейроны решение : обработка Висячих узлоВ Здесь показано решение задачи «Обработка висячих узлов» из главы 6. Чтобы получить стохастическую матрицу, в сумме все столбцы матрицы пе- реходов должны составлять 1. Это требование не удовлетворяется, когда орга- низм не служит пищей для других: этот столбец будет состоять из одних нулей. Вместе с тем замена всех этих столбцов на 1/n 1 будет дорогостоящей. Ключ к решению этой задачи состоит в том, чтобы осознать: каждая строка вносит одинаковый вклад в умножение матрицы переходов на вектор текущих вероятностей. Иными словами, сложение этих столбцов добавит единое зна-чение к результату итеративного умножения. Каково это значение? 1/n, умно-женное на элементы r, которые соответствуют висячему узлу. Это может быть выражено как скалярное произведение вектора, содержащего 1/n для позиций, соответствующих висячим узлам, и нуль во всех остальных случаях, на вектор r для текущей итерации. def power2(Trans, damping=0.85, max_iter=10**5): n = Trans.shape[0] dangling = (1/n) * np.ravel(Trans.sum(axis=0) == 0) r0 = np.full(n, 1/n) r = r0 for _ in range(max_iter): rnext = (damping * (Trans @ r + dangling @ r) + (1 - damping) / n) if np.allclose(rnext, r): return rnext else: r = rnext return r 10 / 24\n--- Страница 253 ---\nРешение: модификация функции align  253 Попробуйте это выполнить вручную для нескольких итераций. Обратите внимание, если вы начинаете со стохастического вектора (вектора, все эле- менты которого в сумме составляют 1), то следующий вектор по-прежнему бу - дет стохастическим вектором. Таким образом, получаемая на выходе из этой функции мера PageRank будет вектором истинных вероятностей, и значения представят вероятность, что мы, следуя по ссылкам в пищевой цепи, придем к конкретному организму. решение : мето Ды проВерки Здесь показано решение задачи «Эквивалентность различных методов полу - чения собственного вектора» из главы 6. Функция np.corrcoef дает коэффициент корреляции Пирсона между всеми парами списка векторов. Этот коэффициент будет равен 1, если и только если два вектора являются скалярными кратными друг друга. Поэтому коэффици-ент корреляции, равный 1, будет достаточен, чтобы показать, что вышеупомя-нутые методы порождают одинаковое ранжирование. pagerank_power = power(Trans) pagerank_power2 = power2(Trans)np.corrcoef([pagerank, pagerank_power, pagerank_power2]) array([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]]) решение : моДификация функции align Здесь показано решение задачи «Модификация функции align» главы 7. Мы используем метод basin hopping на более высоких уровнях пирамиды. Однако для более низких уровней используем метод Пауэлла, потому что при полной разрешающей способности выполнение метода basin hopping в вычис - лительном плане обходится слишком дорого: def align(reference, target, cost=cost_mse, nlevels=7, method='Powell'): pyramid_ref = gaussian_pyramid(reference, levels=nlevels) pyramid_tgt = gaussian_pyramid(target, levels=nlevels) levels = range(nlevels, 0, -1) image_pairs = zip(pyramid_ref, pyramid_tgt) p = np.zeros(3) for n, (ref, tgt) in zip(levels, image_pairs): p[1:] *= 2 if method.upper() == 'BH': res = optimize.basinhopping(cost, p, minimizer_kwargs={'args': (ref, tgt)}) 11 / 24\n--- Страница 254 ---\n254  Решения задач if n <= 4: # avoid basin hopping in lower levels method = 'Powell' else: res = optimize.minimize(cost, p, args=(ref, tgt), method='Powell') p = res.x # print current level, overwriting each time (like a progress bar) print(f'Уровень: {n}, Угол: {np.rad2deg(res.x[0]) :.3}, ' f'Сдвиг: ({res.x[1] * 2**n :.3}, {res.x[2] * 2**n :.3}), ' f'Стоимость: {res.fun :.3}', end='\\r') print('') # новая строка, когда выравнивание завершено return make_rigid_transform(p) Теперь попробуем это выравнивание: from skimage import util theta = 50 rotated = transform.rotate(astronaut, theta) rotated = util.random_noise(rotated, mode='gaussian', seed=0, mean=0, var=1e-3) tf = align(astronaut, rotated, nlevels=8, method='BH') corrected = transform.warp(rotated, tf, order=3) f, (ax0, ax1, ax2) = plt.subplots(1, 3) ax0.imshow(astronaut)ax0.set_title('Оригинальное')ax1.imshow(rotated) ax1.set_title('Повернутое') ax2.imshow(corrected)ax2.set_title('Зарегистрированное')for ax in (ax0, ax1, ax2): ax.axis('off') Уровень: 1, Угол: -50.0, Сдвиг: (-2.09e+02, 5.74e+02), Стоимость: 0.0385 Оригинальное Повернутое Зарегистрированное Успешно! Метод basin hopping смог восстановить правильное выравнивание даже в проблематичном случае, где функция minimize попала в тупик. 12 / 24\n--- Страница 255 ---\nРешение: анализ главных компонент потоковых данных  255 решение : анализ глаВных компонент потокоВых Данных при помощи библиотеки Scikit -learn Здесь показано решение задачи «Анализ главных компонент потоковых дан- ных» из главы 8. Сначала пишется функция для тренировки модели. Эта функция долж - на принимать поток образцов и производить модель PCA, которая способна преобразовывать новые образцы, проецируя их из исходного n-мерного про- странства в пространство главной компоненты. import toolz as tz from toolz import curried as cfrom sklearn import decompositionfrom sklearn import datasetsimport numpy as np def streaming_pca(samples, n_components=2, batch_size=100): ipca = decomposition.IncrementalPCA(n_components=n_components, batch_size=batch_size) tz.pipe(samples, # итератор одномерных массивов c.partition(batch_size), # итератор кортежей c.map(np.array), # итератор двумерных массивов c.map(ipca.partial_fit), # частичная подгонка partial_fit по каждому tz.last) # Пропустить поток данных через конвейер return ipca Теперь можно применить эту функцию, чтобы натренировать (или выпол- нить подгонку) модели PCA: reshape = tz.curry(np.reshape) def array_from_txt(line, sep=',', dtype=np.float): return np.array(line.rstrip().split(sep), dtype=dtype) with open('data/iris.csv') as fin: pca_obj = tz.pipe(fin, c.map(array_from_txt), streaming_pca) Наконец, исходные образцы направляются потоком через модельную функ - цию transform . Мы укладываем их воедино, чтобы получить матрицу данных, состоящую из n образцов на n компонент ( n_samples n_components ): with open('data/iris.csv') as fin: components = tz.pipe(fin, c.map(array_from_txt), c.map(reshape(newshape=(1, -1))), c.map(pca_obj.transform), np.vstack) print(components.shape)(150, 2) 13 / 24\n--- Страница 256 ---\n256  Решения задач Теперь компоненты можно вывести на график: iris_types = np.loadtxt('data/iris-target.csv') plt.scatter(*components.T, c=iris_types, cmap='viridis'); Вы можете проверить, что это дает (приблизительно) такой же результат, что и стандартный PCA (сравните рис. А2 и А3): iris = np.loadtxt('data/iris.csv', delimiter=',')components2 = decomposition.PCA(n_components=2).fit_transform(iris)plt.scatter(*components2.T, c=iris_types, cmap='viridis'); Рис. А2  Главные компоненты набора данных цветков ириса, вычисленных при помощи потокового PCA Рис. А3  Главные компоненты набора данных цветков ириса, вычисленные при помощи обычного PCA 14 / 24\n--- Страница 257 ---\nРешение: добавление шага в начало конвейера  257 Разница, конечно же, состоит в том, что потоковый анализ главных компо- нент масштабируется до чрезвычайно больших наборов данных. решение : ДобаВление шага В начало конВейера Здесь показано решение задачи «Онлайновая распаковка данных» из главы 8. Функция open в первоначальном исходном коде genome может быть заменена на каррированную версию gzip.open . Функция open в библиотеке gzip по умол- чанию использует режим rb (read bytes, т. е. читать байты) вместо встроенной в Python функции open, которая использует режим rt (read text, т. е. читать текст). Поэтому мы должны ее предоставить. import gzip gzopen = tz.curry(gzip.open)def genome_gz(file_pattern): «»»Передать геном потоком, буква за буквой, из списка имен файлов FASTA.»»» return tz.pipe(file_pattern, glob, sorted, # Имена файлов c.map(gzopen(mode='rt')), # строки # конкатенировать строки из всех файлов: tz.concat, # отбросить заголовок из каждой последовательности c.filter(is_sequence), # конкатенировать символы из всех строк tz.concat, # отбросить символы новой строки и ‘N’ c.filter(is_nucleotide)) Вы можете испытать эту функцию на сжатом файле генома дрозофилы: dm = 'data/dm6.fa.gz'model = tz.pipe(dm, genome_gz, c.take(10**7), markov)plot_model(model, labels='ACGTacgt') 15 / 24\n--- Страница 258 ---\n258  Решения задач Если вы хотите иметь единую функцию genome , то вы могли бы написать соб- ственную функцию open, которая по имени файла или путем проб и ошибок будет решать, является ли файл архивным файлом gzip. Таким же образом, если у вас есть архив .tar.gz с файлами FASTA, вместо мо- дуля glob вы можете использовать модуль Python tarfile , чтобы прочитать каж - дый файл по отдельности. Единственная оговорка состоит в том, что для деко- дирования каждой строки вам придется использовать функцию bytes.decode , поскольку tarfile возвращает строки как байты, а не как текст. 16 / 24\n--- Страница 259 ---\nПредметный указатель Символы %matplotlib inline, команда, 47 @, оператор умножения матриц, 145 B basin hopping, алгоритм, 204 C conda, 23Cython, 235 F FASTA, формат, 221fcluster, функция, 76 FFTPACK, динамическая библиотека, 113 frequencies, функция, 223 G generic_filter, функция, 92, 103 GitHub, 19, 234 I imshow, функция, 80IncrementalPCA, класс, 227 IPython, 13 J JIT-компиляторы (динамические), 236 Jupyter волшебные команды, 46команда %matplotlib inline, 47 роль в научной экосистеме Python, 13 M Matplotlib %matplotlib inline, 46вывод изображений, 79модуль matplotlib.pyplot, 47 роль в научной экосистеме языка Python, 13 создание таблицы стилей, 47 стилизация, 46 функция trisurf, 141 N ndimage, библиотека, 79, 86, 102 NetworkX, библиотека, 94 Numba, 236NumPy N-мерные массивы, 38квантильная нормализация, 62 метод .ravel(), 159 нормализация данных, 46правила векторизации и транслирования, 32, 38, 56, 64, 158 преобразование в разреженные матрицы, 166роль в научной экосистеме языка Python, 13, 32 создание изображений, 79функциональность, связанная с ДПФ, 115 N-мерные массивы (массивы ndarray) векторизация, 64в сопоставлении со списками Python, 39 метод .ravel(), 159особенности, 38, 63 представление изображений, 79преобразование в разреженные матрицы, 166транслирование, 41, 56, 158 O optimize, модуль, 195 17 / 24\n--- Страница 260 ---\n260  Предметный указатель P PageRank, мера, 187 pandas, библиотека роль в научной экосистеме Python, 14чтение данных, 43, 64 Python 3 анализ экспрессии генов, 36в сопоставлении с Python 2, 14в сопоставлении с массивами ndarray, 39 инсталляция версии 3.6, 22 ключевое слово yield, 214 оператор умножения матриц, 145 только именованные аргументы, 54 R RAG (графы смежности областей), 78. См. Области изображения лапласова матрица графа, 175лапласова матрица, нормализованная по степени, 181порождение, 78сегментация, 98узлы и связи, 78 RPKM (количество прочтений на тысячу оснований экзона на миллион картированных прочтений), 32, 54 S scikit-image алгоритм SLIC, 100роль в научной экосистеме Python, 14 scikit-learn класс IncrementalPCA, 227роль в научной экосистеме Python, 14 SciPy, библиотека альтернативы, 235документация и обучающие пособия, 10значение понятия «элегантный», 9квантильная нормализация, 62 линейная алгебра, 174 модуль scipy.fftpack, 113модуль sparse, 144, 148, 192 модуль иерархической кластеризации, 69 оптимизация функций, 193преимущества, 13 разреженные итеративные решатели, 187 ресурсы GitHub, 19, 234 конференции, 16 списки рассылок, 22, 233 роль в научной экосистеме Python, 13 требуемые предварительные знания, 11функция scipy.signal.fftconvolve, 116 T Toolz, библиотека, 217, 223 Y yield, ключевое слово, 214 А Алгоритм Блуштайна, 117 Алгоритм Кули-Тьюки, 116 Алгоритм Рейдера, 117 Алгоритмы линейного поиска, 193Анализ потоковых данных выигрыш в производительности, 231каррирование данных, 223 ключевое слово yield, 214 основные понятия, 212 подсчет k-мер и исправление ошибок, 219подсчет k-мер при помощи каррированных функций, 226 преимущества библиотеки Toolz, 213, 217 применения, 213советы для работы, 227 18 / 24\n--- Страница 261 ---\nПредметный указатель  261 управление потоком, 223 Анализ экспрессии генов бикластеризация количественных данных, 68визуализация гластеров, 70 квантильная нормализация, 62 марковская модель на основе полных геномов, 228 нормализация, 46 подсчет k-мер и исправление ошибок, 219 предсказание выживаемости, 72пример, 32 разница в распределении между индивидуумами, 65чтение данных при помощи библиотеки pandas, 43, 64 Аргументы, только именованные, 54Атлас ракового генома (TCGA), 32, 43 Б Бесплатное программное обеспечение и программное обеспечение с открытым исходным кодом (FOSS), 16 Бикластеризация, 68 Благодарности, 24 Большие данные, 212, 232. См. Анализ потоковых данных Быстрое преобразование Фурье (FFT) в анализе радарных данных, 128в вычислении дискретного преобразования Фурье, 110 выбор длины, 116 дополнительные применения, 142история, 115 оконное преобразование, 124, 136 преимущества, 109реализация, 115 ресурсы, 143частоты и их упорядочивание, 118 В Векторизация, 41Вектор Фидлера, 177Визуальная теория информации, 160. См. Теория информацииВолшебные команды Jupyter, 46Вопросы и комментарии, 236Временные данные, преобразование, 107 Г Гауссово ядро, 65, 88 Графики, стилизация, 46Графики стебель-листья, 109Графы в сопоставлении с сетями, 94использование библиотеки NetworkX, 94 компоненты связности, 96лапласова матрица графа, 175лапласова матрица, нормализованная по степени, 181построение из областей изображения, 102 Д Данные, цензуированные справа, 74Деревья слияния, 69Дискретное преобразование Фурье (DFT) анализ радарных данных, 128выбор длины, 116дополнительные применения, 142иллюстрация спектрограммы, 110история, 115математические преобразования, 120оконное преобразование, 136преобразование данных, 107реализация, 115ресурсы, 143 частоты и их упорядочивание, 118 ДНК (дезоксирибонуклеиновая кислота), 34Доверительные области, 194 И Иерархическая кластеризация, 69 19 / 24\n--- Страница 262 ---\n262  Предметный указатель Изменчивость информации вычисление, 144, 160 использование, 167 Интервал дальности, 135 Истинно отрицательные/истинно положительные исходы, 146Итеративные решатели, 187 К Каррирование, 223Квантильная нормализация бикластеризация количественных данных, 68 визуализация кластеров, 70логарифмическое преобразование данных, 63преимущества, 62 разница в распределении между индивидуумами, 65чтение данных при помощи библиотеки pandas, 64шаги, 63 Комментарии и вопросы, 23, 236 Компоненты связности, 96Конволюция, 143. См. Свертка Контактная информация, 237 Контрольные данные (ground truth), 146Кривые выживания, 74 Л Лапласова матрица графа анализ графа, 175лапласова матрица, нормализованная по степени, 181 Линейная алгебра лапласова матрица графа, 175 лапласова матрица, нормализованная по степени, 181 основы, 174реализация алгоритма PageRank, 187 Лицензии, 17Лицензии на программное обеспечение, 17 Локальные минимумы предотвращение при помощи алгоритма basin hopping, 204пример, 195 проблемы, 204 решение проблемы разными алгоритмами, 193 М Марковские модели, 228Математическое обозначение O большое, 116 Матрица переходов, 188 Матрица сопряженности, 162Матрицы ошибок, 146. См. Матрицы сопряженностиМатрицы смежности, 175 Молекулярная биология, ее центральная догма, 34 Н Набор данных эталонных сегментаций университета Беркли, 168 Научная экосистема языка Python бесплатное программное обеспечение и программное обеспечение с открытым исходным кодом (FOSS), 16 компоненты, 13 отсылки на шоу «Монти Пайтон», 21поддержка со стороны сообществ, 16, 21. См. Ресурсы роль NumPy, 32участие, 21 Нормализация квантильная нормализация, 62между генами, 52между образцами, 46по образцам и генам (RPKM), 54потребность, 46 20 / 24\n--- Страница 263 ---\nПредметный указатель  263 Нормализованная взаимная информация (NMI), 209 О Области изображения графовые представления, 94двумерные фильтры (изображения), 90изображения в виде массивов NumPy, 79преобразования изображений с использованием разреженных матриц, 152 сегментация по среднему цвету, 105 создание графов, 102универсальные фильтры, 92фильтры в обработке сигналов, 84 Обработка сигналов гауссово сглаживание, 88 двумерные фильтры (изображения), 90зашумленные сигналы, 87свертка, 86соотношение сигнал-шум (SNR), 89универсальный фильтр, 92 фильтры, 84 Однородные координаты, 153 Окно Кайзера, 126Оконное преобразование, 124, 136 Оператор умножения матриц, 145Оптимизация, 193. См. Оптимизация функцииОптимизация при заданных ограничениях, 194Оптимизация функции выбор алгоритма оптимизации функции, 194 выбор целевых функций, 205 вычисление оптимального смещения изображения, 195определение, 193предотвращение локальных минимумов при помощи алгоритма basin hopping, 204регистрация изображения, 201 функции стоимости, 193 П Палитра colorbrewer, 184Переходные вероятности, 228Пиксел, 78. См. Области изображения Подсчет k-мер и исправление ошибок, 219использование каррированных функций, 226 Получение помощи, 22. См. Ресурсы Предсказания, 146 Примеры программного кода, получение и использование, 19, 22, 24, 234 Причудливая индексация, 64Простая линейная итеративная кластеризация (SLIC), 100Пространственная частота, 108 Пространственные данные, преобразование, 107Публичная лицензия GNU, 18 Р Радарные данные, анализ, 128Разработка программного обеспечения с открытым исходным кодом, 16 Разреженные итеративные решатели, 187Разреженная матрица преимущества модуля sparse, 144 преобразование изображений, 152 преобразование массива NumPy, 166сравнение форматов, 152формат COO (координатный), 148, 157 формат сжатых разреженных строк (CSR), 150 Разреженная оптимизация, сравнение методов, 194Разрешительные лицензии, 18 21 / 24\n--- Страница 264 ---\n264  Предметный указатель Ресурсы GitHub, 19, 234 конференции, 16 списки рассылок, 22, 233 Решения задач, 238 С Свертка, 86. См. Конволюция Свободная лицензия (copy-left), 18 Система управления версиями, 19 Собственные векторы, 176, 183 Сосредоточенность данных, 149Спектрограммы, 110Среднеквадратическая ошибка (СКО, MSE), 196 Степенная матрица (D), 176 Степенной метод, 191 Т Таблицы сопряженности измерение результативности, 147пример таблицы с исторической информацией, 160примеры разреженных матриц, 144 сегментация, 146, 159 формат COO (координатный), 157 Теорема Фробениуса-Перрона, 192 Теория информации введение, 160 визуальная теория информации, 160в сегментации, 163 Только именованные аргументы, 54Транскрипционные сети, 94Транслирование массивов, 33, 41, 56, 158 Трассировки дальности, 133 У Управление потоком, 223 Упражнения альтернативный алгоритм вычисления матрицы ошибок, 147, 243 анализ главных компонент (PCA) потоковых данных, 227, 255 вычисление условной энтропии, 163, 247 вычислительная сложность матриц ошибок, 147, 243 добавление сеточного наложения, 84, 238 игра «Жизнь», 93, 239 изображение афинного подобия, 186, 248 линейная алгебра с разреженными матрицами, 186, 249 магнитуда градиента Собела, 94, 240 матрица поворота, 176, 247 модификация функции align, 205, 253 мультиклассовая матрица ошибок, 148, 244 обработка висячих узлов, 192, 252 онлайновая распаковка архива, 231, 257 поворот изображения, 156, 245 подбор кривой при помощи SciPy, 98, 241 предсказание выживаемости, 77 представление в формате COO, 149, 244 решения, 238свертка изображения, 143, 243 сегментация на практике, 173сокращение объема потребляемой оперативной памяти, 158, 246 эквивалентность разных методов получения собственного вектора, 192, 253 Уровень собственных шумов, 136Условная энтропия, 161Условные обозначения, 23 Ф Фильтры в обработке сигналов, 84 22 / 24\n--- Страница 265 ---\nПредметный указатель  265 двумерные фильтры (изображения), 90 универсальные фильтры, 92фильтры Собела, 91, 94 функция generic_filter, 102 Фильтры Собела, 91, 94 Функция скользящего окна, 223Функция стоимости, 193. См. Оптимизация функции Ц Центральная догма молекулярной биологии, 34 Цифровые изображения, 78. См. Области изображенияЧ Частота анализ данных FMCW-радаров, 128 данные частотной области, 107 Найквиста, 120 оконное преобразование, 136понятие, 107пространственная частота, 108упорядочивание частот, 118 Э Элегантный программный код, особенности, 9 Я Ядерная оценка плотности (KDE), 46 23 / 24\n--- Страница 266 ---\nКниги издательства «ДМК Пресс» можно заказать в торгово-издательском холдинге «Планета Альянс» наложенным платежом, выслав открытку или письмо по почтовому адресу: 115487, г. Москва, 2-й Нагатинский пр-д, д. 6А. При оформлении заказа следует указать адрес (полностью), по которому должны быть высланы книги; фамилию, имя и отчество получателя. Желательно также указать свой телефон и электронный адрес. Эти книги вы можете заказать и в интернет-магазине: www.alians-kniga.ru. Оптовые закупки: тел. (499) 782-38-89. Электронный адрес: books@alians-kniga.ru. Хуан Нуньес-Иглесиас, Штефан ван дер Уолт, Харриет Дэшноу Элегантный SciPy Главный редактор Мовчан Д. А. dmkpress@gmail.com Перевод Логунов А. В. Корректор Синяева Г. И. Верстка Чаннова А. А. Дизайн обложки Мовчан А. Г. Формат 70×100 1/16. Гарнитура «PT Serif». Печать офсетная. Усл. печ. л. 24,9375. Тираж 200 экз. Веб-сайт издательства: www.dmkpress.com Powered by TCPDF (www.tcpdf.org) 24 / 24\n--- Страница 267 ---\nИнтернет-магазин: www .dmkpress.comКнига – почтой: orders@alians-kniga.ruОптовая продажа: “Альянс-книга”тел.(499)782-38-89 books@alians-kniga.ru www.дмк.рф Элегантный SciPy Элегантный SciPy Хуан Нуньес-Иглесиас Штефан ван дер Уолт Харриет ДэшноуНаучное программирование на Python ` 9785970 606001ISBN 978-5-97060-600-1Добро пожаловать в научное программирование на Python и его сообщество. Если вы – уче- ный, который программирует на Python, то это практическое руководство для вас! Оно не только познакомит вас с основополагающими компонентами библиотеки SciPy и другими связанными с ней библиотеками, но и даст вам ощущение красоты и удобочитаемости программного кода, который вы сможете применять на практике. Вы научитесь писать элегантный программный код, который ясен, краток и эффективен при исполнении ре-шаемой задачи.На протяжении всей книги вы будете работать с примерами из обширной научной экосистемы Python, используя программный код, который иллюстрирует кратко очерченные принципы. Используя реальные научные данные, вы будете работать с практическими задачами вместе с SciPy , NumPy , Pandas, scikit-image и другими библиотеками Python. Вы будете: •исследовать массивы NumPy , то есть структуры данных, которые лежат в основе численных научных вычислений; •применять квантильную нормализацию, чтобы данные измерений гарантированно укладывались в заданное распределение; •представлять отдельные области изображения при помощи графа смежности областей; •преобразовывать временные и пространственные данные в данные частотной области при помощи быстрого преобразования Фурье; •решать задачи с разреженными матрицами, включая сегментацию изображений при помощи модуля sparse библиотеки SciPy; •выполнять линейно-алгебраические задачи при помощи пакетов SciPy; • исследовать выравнивание (регистрацию) изображений при помощи модуля SciPy optimize;•обрабатывать крупные наборы данных при помощи потоковых примитивов и библиотеки Toolz. Хуан Нуньес-Иглесиас (Juan Nunez-Iglesias) – свободный консультант и научный сотрудник Университета Австралии. Его главные интересы лежат в области нейробиологии и анализа изображений. Он также инте-ресуется графовыми методами в биоинформатике и биостатистике.Штефан ван дер Уолт (Stefan van der Walt) – младший научный сотрудник Института науки о данных в Калифорнийском университете и старший лектор прикладной математики в Университете Штелленбоша, Южная Африка. Он активно участвует в разработке научного программного обеспечения с открытым ис-ходным кодом и преподает Python на семинарах и конференциях, а также является создателем библиотеки scikit-image и одним из разработчиков библиотек NumPy, SciPy и cesium-ml.Харриет Дэшноу (Harriet Dashnow) – биоинформатик, работала в Детском научно-исследовательском институте Мердока, в отделе биохимии в Мельбурнском университете. В настоящее время одновременно с написанием докторской диссертацией она занимается преподаванием на профессиональных семинарах в области геномики, принципов разработки программного обеспечения, Python, R, Unix и системы управления версиями Git.«Настоящая книга восполняет важную потребность: она знакомит студентов с элегантными реализациями классических алгоритмов в области обработки сигналов и изображений, теории сетей и биоинформатики». – Лэв Варшни, Университет шт. Иллинойс, США ` ` 1 / 27",
      "debug": {
        "start_page": 212,
        "end_page": 267
      }
    }
  ]
}
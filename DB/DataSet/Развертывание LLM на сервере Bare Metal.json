{
  "title": "Развертывание LLM на сервере Bare Metal",
  "chapters": [
    {
      "name": "Развертывание LLM на сервере Bare Metal",
      "content": "Практические руководства Evolution    \n\n # Развертывание LLM на сервере Bare Metal   Эта статья полезна?          \nС помощью этого руководства вы развернете большую языковую моделей (LLM) deepseek-r1:32b на сервере Bare Metal и настроите общение с ней из терминала.\nДля этого используются:\n- Ollama — для запуска модели.\n- Open WebUI — для доступа к модели снаружи сервера.\nШаги:\n1. Разверните инфраструктуру.\n2. Настройте и запустите контейнеры.\n3. Настройте Open WebUI и выберите модель.\n4. Настройте работу с моделью из терминала.\n\n## 1. Разверните инфраструктуру\n1. Арендуйте сервер Bare Metal с публичным IP-адресом.\nДля корректной работы модели выбирайте конфигурации с:\n- объемом оперативной памяти не менее 32 ГБ;\n- наличием SSD накопителей;\n- (опционально) поддержкой GPU.\n2. Подключитесь к серверу по SSH или через виртуальную консоль.\n3. Установите Docker.\n4. Установите Docker Compose.\n\n## 2. Настройте и запустите контейнеры\n1. Создайте каталог для проекта и перейдите в него:\n```\nmkdir llm-deploy-testcd llm-deploy-test\n```\n2. Создайте файл «compose.yaml» и поместите в него код:\n```\nservices:ollama:   image: ollama/ollama   container_name: ollama   volumes:   - ollama_data:/root/.ollama  # If you use GPUs, uncomment code below by deleting \"#\" only  # deploy:  #   resources:  #     reservations:  #       devices:  #         - driver: nvidia  #           count: all  #           capabilities: [gpu]\nopen-webui:   # For CPU‑only usage (using the \"main\" tag):   image: ghcr.io/open-webui/open-webui:main   container_name: open-webui   volumes:   - openwebui_data:/app/backend/data   ports:   - \"3000:8080\"   extra_hosts:   - \"host.docker.internal:host-gateway\"   environment:   # If Ollama is running locally, you can set the base URL as follows:   - OLLAMA_BASE_URL=http://ollama:11434   depends_on:   - ollama  # If you use GPUs, uncomment code below by deleting \"#\" only  # deploy:  #   resources:  #     reservations:  #       devices:  #         - driver: nvidia  #           count: all  #           capabilities: [gpu]\nvolumes:ollama_data:openwebui_data:\n```\n3. Запустите контейнеры:\n```\nsudo docker compose up -d\n```\n\nФлаг «-d» используется для запуска контейнеров в фоновом режиме.\nВ этом случае в терминале не отобразятся логи.\nЕсли вам необходимо посмотреть логи, выполните команду:\n```\ndocker compose logs -f\n```\n\n## 3. Настройте Open WebUI и выберите модель\n1. В браузере перейдите на страницу «http://<IP-адрес_сервера>:3000».\n2. Нажмите Get started.\n3. В открывшемся окне настройте аккаунт администратора:\n1. В поле Имя укажите введите имя.\n2. В поле Электронная почта введите ваш e-mail.\n3. В поле Пароль введите пароль.\n4. Нажмите Создать аккаунт администратора.\n4. Справа выберите Настройки → Модели.\n5. Нажмите Manage models и в открывшемся окне:\n1. В поле Загрузить модель с Ollama.com введите «deepseek-r1:32b».\n2. Нажмите Показать.\nОткроется окно для общения с моделью.\n\n## 4. Настройте работу с моделью из терминала\nВы также можете использовать модель напрямую из терминала.\nЭто позволит ускорить работу, а также получить доступ к некоторым дополнительным инструментам, например LangChain.\nВ рамках сценария используется решение aider.\n1. Справа выберите Настройки → Аккаунт.\n2. В поле Ключи API нажмите Сгенерировать и скопируйте ключ.\nОн понадобится в дальнейшем.\n3. В терминале выполните запрос для проверки работы API:\n```\ncurl -X POST http://<IP-адрес_сервера>:3000/api/chat/completions-H \"Authorization: Bearer <API-ключ>\"-H \"Content-Type: application/json\"-d '{   \"model\": \"deepseek-r1:32b\",   \"messages\": [      {      \"role\": \"user\",      \"content\": \"Why is the sky blue?\"      }   ]}'\n```\n\nВ ответ должно отобразиться:\n```\n{\"id\":\"deepseek-r1:7b-d91cdf31-d05d-4185-a512-960753e21239\"...\n```\n4. Установите aider для работы с моделью в терминале:\n```\npython -m pip install aider-installaider-install\n```\n5. Настройте подключение aider к Open WebUI:\n```\nexport OPENAI_API_BASE=http://<IP-адрес_сервера>:3000/apiexport OPENAI_API_KEY=<API-ключ>\n```\n6. Откройте окно для общения с моделью:\n```\naider --model openai/deepseek-r1:32b\n```\nТеперь вы можете задавать модели вопросы и получать ответы напрямую в терминале.\n\n  [© 2025 Cloud.ru](https://cloud.ru)",
      "debug": {
        "start_page": 1,
        "end_page": 1
      }
    }
  ]
}
{
  "title": "Крупномасштабное машинное обучение вместе с Python [2018] Шарден, Массарон, Боскетти,",
  "chapters": [
    {
      "name": "Глава 1. Первые шаги к масштабируемости 20",
      "content": "--- Страница 20 ---\nКомментарий переводчика  19 простая, легковесная и бесплатная интерактивная среда разработки на Python, ко- торая предлагает функционал, аналогичный среде разработки на MATLAB, вклю-чая готовые к использованию виджеты PyQt5 и PySide : редактор исходного кода, редактор массивов данных NumPy , редактор словарей, консоли Python и IPython и многое другое. Чтобы установить среду Spyder в Ubuntu Linux, используя официальный менед- жер библиотек, нужна всего одна команда: sudo apt-get install spyder Чтобы установить с использованием менеджера библиотек pip: sudo apt-get install python-qt5 python-sphinxsudo pip install spyder И чтобы обновить: sudo pip install -U spyder Установка среды Spyder в Fedora 25: dnf install python-spyder Установка среды Spyder в Windows: pip install spyder Примечание: среда Spyder требует обязательной установки библиотеки PyQt5.\n--- Страница 21 ---\nГлава 1 Первые шаги к масштабируемости Добро пожаловать в книгу по масштабируемому машинному обучению на Python. В этой главе мы обсудим способы эффективного обучения на больших данных в среде Python и как это можно осуществить, используя всего одну машину или кластер из других машин, который, к примеру, можно получить в веб-службах об-лачных вычислений Amazon Web Servic es (AWS) или в веб-службах платформы Google-облако. В настоящей книге мы будем использовать реализацию масштабируемых ал- горитмов машинного обучения на языке Python. Иными словами, они смогут ра-ботать с большим объемом данных и не дадут сбоя из-за нехватки оперативной памяти. Кроме того, работа таких алгоритмов будет занимать разумное количест - во времени, достаточно приемлемое для прототипа в области науки о данных и для развертывания проекта в эксплуатационной среде. Главы книги организо-ваны вокруг решений (таких как потоковая передача данных), алгоритмов (таких как нейронные сети или ансамбль деревьев) и платформ (Hadoop или Spark). Мы также предложим небольшой справочник по алгоритмам машинного обучения и объясним, как сделать их масштабируемыми и пригодными для решения задач с крупными наборами данных. С учетом таких стартовых предпосылок вам потребуется изучить основы (что- бы уяснить перспективу, с которой эта книга была написана), а также установить и настроить все основные инструменты, которые позволят незамедлительно при-ступить к чтению глав. В этой главе мы представим следующие темы: что в действительности означает термин «масштабируемость»; на какие узкие места необходимо обратить внимание во время работы с данными; какие задачи эта книга помо жет решать; как испо льзовать Python для эффективного анализа наборов данных в круп- ном масштабе; каким образом быстро настроить свою машину для выполнения представ- ленных в этой книге примеров. Давайте же начнем наше совместное путешествие по масштабируемым реше- ниям в среде Python!\n--- Страница 22 ---\nПодробное объяснение термина масштабируемости  21 пОдрОбнОе Объяснение термина масштабируемОсти Несмотря на весь нынешний ажиотаж вокруг больших данных, большие наборы данных существовали задолго до того, как был введен сам термин. Большое коли-чество текстовых данных, последовательностей ДНК и огромное количество дан-ных с радиотелескопов всегда представляли проблему для ученых и исследовате-лей-аналитиков. Поскольку большинство алгоритмов машинного обучения имеет вычислительную сложность O(n 2) или даже O(n3), где n – это число тренировочных прецедентов, исследователи и аналитики прежде решали поставленные крупны-ми наборами данных сложные задачи, привлекая более эффективные алгоритмы обработки данных. Алгоритм машинного обучения считается масштабируемым, когда после соответствующей настройки он может работать в условиях больших наборов данных. Набор данных может быть большим в силу большого количества прецедентов либо переменных, либо в силу обоих причин, а масштабируемый ал-горитм может справляться с ними эффективно, поскольку его время выполнения увеличивается почти линейно в соответствии с размером задачи. Следователь-но, это просто вопрос обмена в соотношении 1:1 большего количества времени (или большей вычислительной мощи) на большее количество данных. Между тем обычный алгоритм машинного обучения, когда сталкивается с большими объ-емами данных, не масштабируется; он попросту прекращает работать либо ра-ботает со временем выполнения, которое увеличивается нелинейно, например экспоненциально, тем самым делая обучение неосуществимым. Внедрение дешевых систем хранения данных, больших RAM и многоядерных CPU кардинально все изменило, увеличив возможности одиночных ноутбуков по анализу больших объемов данных. Появление в недавнем прошлом еще одного игрока стало переломным моментом, переориентировав внимание с одиночных мощных машин на кластеры серийных компьютеров (более дешевых и легкодо-ступных). Эта серьезная перемена обусловила внедрение вычислительной сетевой парадигмы MapReduce и плат формы с открытым исходным кодом Apache Hadoop с ее распределенной файловой сист емой Hadoop HDFS (Hadoop distributed file system) и в целом параллельных вычислений в компьютерных сетях. Чтобы выяснить, каким образом обе эти перемены глубоко и положительно по- влияли на возможности решения крупномасштабных задач, прежде всего следует выяснить, что на самом деле мешало (и по-прежнему мешает, в зависимости от того, насколько крупной является решаемая задача) выполнять анализ крупных наборов данных. Независимо от того, какую задачу вы решаете, в конечном счете вы обнару - жите, что не можете выполнить анализа своих данных в силу следующих ниже ограничений: вычислит ельная емкость оказывает влияние на время, затрачиваемое на выполнение анализа; емкос ть каналов ввода-вывода данных влияет на то, сколько данных может быть передано за единицу времени из хранилища данных в оперативную память машины; емкос ть оперативной памяти влияет на то, насколько большими будут дан- ные, которые можно обработать за один раз.\n--- Страница 23 ---\n22  Первые шаги к масштабиру емости Ваш компьютер имеет ограничения, которые будут определять, сможете ли вы обучиться на данных и сколько потребуется времени, прежде чем вы упретесь в стену. Вычислительные ограничения бывают во многих вычислительно емких расчетах, проблемы, связанные с вводом-выводом, образуют узкое место для быстрого доступа к данным, и, наконец, ограничения по памяти могут вынудить принимать лишь часть данных, тем самым ограничивая возможности матричных вычислений, к которым можно было бы обратиться, либо прецизионность или даже строгость получаемых оценок. Каждое из этих аппаратных ограничений будет также оказывать разное влия- ние по степени серьезности относительно анализируемых данных: высокие данные, отличительная особенность которых состоит в том, что они имеют большое количество прецедентов; широкие данные, которые характерны тем, что имеют большое количество признаков; высокие и широкие данные, которые имеют одновременно большое коли- чество прецедентов и признаков; разреж енные данные, которые отличаются тем, что имеют большое коли- чество нулевых записей или записей, которые можно преобразовать в нули (т. е. матрица данных может быть высокой и/или широкой, но информатив-ной, при этом не все записи в матрице имеют информационное наполне-ние). И в конце дело сводится к алгоритму, который вы собираетесь использовать, чтобы обучиться на данных. Каждый алгоритм имеет свои собственные свойства и способен преобразовывать данные, используя решение, на которое по-разному воздействует смещение или дисперсия. Следовательно, относительно задачи, ко-торую вы до сих пор решали при помощи машинного обучения, вы рассчитыва-ли, что определенные алгоритмы могут работать лучше других, основываясь при этом на своем опыте или эмпирической проверке. В случае с крупномасштабны-ми задачами при выборе алгоритма необходимо добавить еще несколько совсем других соображений: какова вычислительная сложность алгоритма, т. е. влияет ли число строк и столбцов в данных на число вычислений линейным или нелинейным об-разом. Большинство решений в области машинного обучения основывается на алгоритмах квадратичной или кубической сложности, тем самым строго ограничивая их применимость к большим данным; ско лько в модели параметров; здесь дело не просто в проблеме дисперсии оценок (переподгонке), а во времени, которое может потребоваться на их вычисление; можно ли параллелизировать процессы оптимизации, т. е. можно ли легко разделить вычисления между многочисленными узлами или ядрами CPU, или же приходится опираться на одиночный последовательный процесс оптимизации; долж ен ли алгоритм обучаться сразу на всех данных, или же вместо этого можно использовать одиночные примеры либо небольшие пакеты данных. Если перекрестно оценить аппаратные ограничения и свойства данных, с одной стороны, и подобного рода алгоритмы – с другой, то можно получить множество возможных проблемных сочетаний, которые могут стать препятствием для полу -\n--- Страница 24 ---\nПодробное объяснение термина масштабируемости  23 чения результатов от проведения крупномасштабного анализа. С практической точки зрения все проблемные сочетания можно решить на основе трех подходов: вертикальног о масштабирования, т. е. улучшения производительности оди- ночной машины путем модификации программного обеспечения и/или оборудования (больше оперативной памяти, более быстрые CPU и дисковая память, а также использование модулей GPU); горизонтальног о масштабирования, т. е. распределения вычислений (и про- изводительности) по многочисленным машинам с привлечением внешних ресурсов, а именно другой дисковой памяти и других модулей CPU (либо GPU); вертикальног о и горизонтального масштабирования, т. е. взятия лучшего решения из вертикальных и горизонтальных решений вместе взятых. Приведение крупномасштабных примеров Несколько мотивирующих примеров прояснит ситуацию и сделает ее запоми - нающейся. Возьмем два прос тых примера: способнос ть предсказывать кликабельность (click-through-rate, CTR), т. е. отношение числа щелчков к числу показов. В наши дни, когда интернет-реклама настолько распространилась вширь и вглубь, что отъедает значи-тельные куски у традиционных СМИ, она помогает довольно много зараба-тывать; способнос ть предложить правильную информацию своим клиентам, ког - да они ищут предлагаемые вашим сайтом продукты и услуги, может по-настоящему улучшить ваши возможности их продавать, в случае если вы смо-жете угадывать, что помещать во главу результатов их поискового запроса. В обоих случаях мы располагаем довольно большими наборами данных, кото- рые продуцируются пользователями в результате их взаимодействия в Интернете. В зависимости от бизнеса, который мы имеем в виду (тут можно вообразить некоторых крупных игроков), в обоих указанных случаях мы, очевидно, говорим о миллионах точек данных в день. В случае рекламы данные, разумеется, явля-ются высокими, потому что они представляют собой непрерывный поток инфор-мации с заменой старых данных на новые, в большей мере отражающих рынки и потребителей. В случае поисковой системы данные являются широкими, до-полняясь компонентом, предоставляемым результатами, которые вы предложи-ли своим клиентам: например, если вы занимаетесь туристическим бизнесом, то у вас будет довольно много признаков об отелях, местах посещений и предлагае-мых услугах. Безусловно, масштабируемость создает трудности в обеих этих задачах: необх одимо обучаться на данных, которые растут каждый день, и причем обучаться быстро, потому что, пока вы обучаетесь, продолжают поступать новые данные. При этом вам приходится иметь дело с данными, которые, очевидно, не смогут уместиться в оперативной памяти, потому что матрица слишком высокая или слишком большая; необх одимо часто выполнять обновления модели машинного обучения с целью размещения новых данных. Для этого потребуется алгоритм, кото-рый может обрабатывать информацию в нужные сроки. Вычислительную сложность O(n 2) или O(n3) практически невозможно обработать ввиду ко-\n--- Страница 25 ---\n24  Первые шаги к масштабиру емости личества данных; потребуется такой алгоритм, который сможет работать с пониженной сложностью (такой как O(n)) или путем разделения данных, в результате которого n будет гораздо меньше; необх одимо уметь предсказывать быстро, потому что предсказания долж - ны предоставляться только новым клиентам. И опять-таки, здесь важную роль играет вычислительная сложность используемого алгоритма. Проблема масштабируемости может быть решена одним или несколькими спо- собами: вертикальное масштабирование путем снижения размерности задачи; на- пример, в случае поисковой системы, путем эффективного отбора реле-вантных признаков для использования; вертикальное масштабирование с использованием приемлемого алгорит - ма; например, в случае рекламных данных, существуют соответствующие алгоритмы для эффективного обучения на потоках данных; горизонтальное масштабирование процесса обучения путем привлечения многочисленных машин; вертикальное масштабирование процесса внедрения с эффективным ис - пользованием многопроцессорной обработки и векторизации на одиноч-ном сервере. В настоящей книге мы покажем, какие практические задачи могут решаться каждым из этих предложенных решений или алгоритмов. В результате вы научи-тесь автоматически связывать отдельное ограничение по времени и исполнению (CPU, оперативная память или операции ввода-вывода) с самым подходящим ре-шением среди тех, которые мы предлагаем. Введение в язык Python Поскольку наше исследование будет зависеть от языка Python – общедоступного языка программирования, выбранного в качестве базового для настоящей кни-ги, – мы должны сделать короткую остановку, чтобы познакомиться с языком, прежде чем приступим к выяснению того, каким образом Python способен по-мочь легко масштабировать решение задачи с массивными данными вертикаль-но и горизонтально. Созданный в 1991 г. как общецелевой, интерпретируемый, объектно-ориенти- рованный язык программирования, Python медленно, но верно завоевывал науч-ное сообщество и в конечном итоге превратился в зрелую экосистему, состоящую из специализированных библиотек для обработки и анализа данных. Он позволя-ет проводить бесчисленные и быстрые эксперименты, легко разрабатывать про-граммы, воплощающие теоретические предпосылки, и быстро развертывать на-учные приложения. Как практик машинного обучения вы найдете использование Python интерес - ным по нескольким причинам: он пре длагает большую, зрелую систему библиотек для анализа данных и машинного обучения. Это гарантирует, что вы получите все, в чем вы, воз-можно, нуждаетесь в ходе анализа данных, и иногда даже больше; он о чень универсален. Независимо от опыта или стиля программирования (объектно-ориентированный, функциональный или процедурный), про-граммирование на Python доставит вам удовольствие;\n--- Страница 26 ---\nПодробное объяснение термина масштабируемости  25 если вы еще с ним незнакомы, но хорошо знаете другие языки, такие как C/ C++ или Java, то он очень прост в освоении и использовании. После того как вы уясните основы, лучший способ узнать больше – сразу же приступить к программированию; он являет ся кросс-платформенным; ваши решения будут работать отлично и гладко в операционных системах Windows, Linux и Mac OS. Вам не при-дется беспокоиться о переносимости исходного кода; хо тя он является интерпретируемым, его быстродействие, несомненно, выше, по сравнению с другими основными языками для анализа данных, такими как R и MATLAB (хотя он не сопоставим с C, Java и недавно появив-шимся языком Julia); он мо жет работать с большими данными в оперативной памяти в силу ми- нимального объема потребляемой оперативной памяти и превосходного управления ею. Сборщик «мусора» в оперативной памяти будет часто эко-номить уйму времени, когда вы будете загружать, преобразовывать, наре-зать, группировать, сохранять или отбрасывать данные, используя различ-ные циклы и повторные операции по подготовке данных к их анализу.  Если вы еще не являетесь экспертом в этом языке (на самом деле, чтобы иметь возможность испо льзовать эту книгу максимальным образом, мы требуем лишь элементарных знаний язы- ка), то всю информацию о языке можно почерпнуть (а также найти основные установочные файлы) непосредственно на главном сайте языка Python по адресу https://www.python.org/. Вертикальное масштабирование средствами Python Python – это интерпретируемый язык; он выполняет чтение вашего сценария из оперативной памяти и выполняет его динамически, тем самым получая доступ к необходимым ресурсам (файлам, объектам в памяти и т. п.). Помимо того что он – интерпретируемый, следует учитывать еще один важный аспект при исполь-зовании Python для анализа данных и машинного обучения – Python является од-нопоточным языком. Однопоточность означает, что любая программа на Python выполняется последовательно от начала до конца сценария и что Python не может использовать в своих интересах дополнительных вычислительных возможностей, предлагаемых многочисленными процессами и процессорами, которые могут иметься у компьютера (большинство компьютеров в наше время многоядерные). Учитывая такую ситуацию, вертикальное масштабирование на основе Python может достигаться на основе разных стратегий: компиляция сценариев Python для достижения большей скорости исполне- ния. Несмотря на то что она легко осуществима, например при помощи PyPy – динамическ ого (JIT) компилятора, который можно найти на http:// pypy.org/, мы в нашей книге на самом деле к такому решению не обраща-лись, потому что оно требует написания алгоритмов на Python с нуля; использование Python в качестве оберточного языка, тем самым связы- вая выполняемые на Python операции с выполнением внешних библиотек и программ, некоторые из которых приспособлены к многоядерной обра-ботке. В нашей книге вы найдете много примеров, где вызываются специа-лизированные библиотеки, в частности библиотека для машин опорных вект оров LIBSVM (Librar y for Support Vector Machines), или такие програм-\n--- Страница 27 ---\n26  Первые шаги к масштабиру емости мы, как Vowpal Wabbit (VW), XGBoost или H2O, для выполнения операций по машинному обучению; эффективное использование методов векторизации, т. е. специальных биб - лиот ек для выполнения матричных вычислений. Это может быть достиг - нуто при помощи библиотек NumPy или pandas , в которых задействуют - ся вычисления с использованием модулей GPU. Модули GPU точно так же, как и многоядерные CPU, имеют свою собственную оперативную память и способны выполнять вычисления в параллельном режиме (как вы можете догадаться, они имеют многочисленные крошечные ядра). Методы векто-ризации на основе модулей GPU могут невероятно ускорить вычисления, в особенности во время работы с нейронными сетями. Однако GPU имеют свои собственные ограничения. Прежде всего располагаемая память име-ет определенный канал ввода-вывода для передачи данных в память GPU и возврата результатов назад в CPU, и они требуют параллельного програм-мирования посредством специального программного интерфейса (API), та-кого как CUDA для модулей GPU производства NVIDIA (отсюда следует, что требуется установка надлежащих драйверов и программ); сведение большой задачи к порциям и решение каждой порции поочеред- но в оперативной памяти (алгоритмы парадигмы «разделяй и властвуй»). Это влечет за собой разбиение данных из оперативной памяти или диска на части либо извлечение из них подвыборок и управление приближенными решениями задачи машинного обучения, что позволяет добиваться доволь-но эффективных результатов. Важно отметить, что разбиение на части и из-влечение подвыборок может действовать как для прецедентов, так и для признаков (либо для обоих одновременно). Если исходные данные хранятся в дисковой памяти, то для итоговой производительности определяющими, безусловно, станут ограничения канала ввода-вывода; эффективное привлечение как многопроцессорной, так и многопоточной обработки в зависимости от используемого обучающегося алгоритма. Не-которые алгоритмы естественным образом могут разбивать свои операции на параллельные. В таких случаях единственным ограничением будут CPU и оперативная память (так как данные должны быть реплицированы для каждого параллельного рабочего процесса, который будет использовать-ся). Некоторые другие алгоритмы вместо этого используют преимущества многопоточной обработки, тем самым одновременно управляя бóльшим числом операций на тех же блоках памяти. Горизонтальное масштабирование средствами Python Горизонтальное масштабирование решений состоит в объединении многочис - ленных машин в кластер. При подключении машин (т. е. масштабируя вширь) можно также масштабировать каждую из них вертикально (т. е. вглубь), исполь-зуя более мощные конфигурации (тем самым усиливая CPU, оперативную память и каналы ввода-вывода), применяя методы, упомянутые нами в предыдущем аб-заце, и улучшая их производительность. Подключая многочисленные машины, можно мобилизовать их вычислитель- ную мощь в параллельном виде. Ваши данные будут распределены по много-численным дисковым хранилищам/устройствам памяти, тем самым ограничивая\n--- Страница 28 ---\nPython для крупномасштабного машинного обучения  27 число операций передачи данных по каналу ввода-вывода и заставляя каждую машину обрабатывать только имеющиеся у нее данные (т. е. свое собственное дисковое хранилище или оперативную память). В нашей книге это транслируется в эффективное использование внешних ре- сурсов посредством следующих платформ: платформы H2O; платформы Hadoop и ее компонентов, таких как распределенная файловая система HDFS, вычислительная парадигма MapReduce и менеджер ресурсов YARN; платформы Spark повер х Hadoop. Python будет управлять всеми этими платформами (например, платформа Spark управляется ее Python’овским интерфейсом под названием pySpark). Python для крупнОмасштабнОг О машиннОг О Обучения С учетом наличия многих удобных библиотек для машинного обучения и того факта, что этот язык программирования довольно популярен среди исследова-телей-аналитиков, Python был выбран в качестве базового языка для написания всего представленного в настоящей книге исходного кода. В этой книге, по мере необходимости, мы будем предоставлять дальнейшие ин- струкции по установке других необходимых библиотек или инструментов. Здесь же вместо этого мы приступим к установке самых главных компонентов, т. е. язы-ка Python и наиболее часто используемых библиотек, применяемых для вычисле-ний и машинного обучения. Выбор между Python 2 и Python 3 Перед тем как начинать, важно уяснить, что существуют две основные ветви Python: версии 2 и 3. Поскольку в обеих версиях много базовой функциональ-ности изменилось, сценарии, созданные для одной версии, иногда несовместимы с другой (они будут работать, вызывая сообщения об ошибках и предупреждения). Несмотря на то что третья версия является новейшей, более старая по-прежнему широко используется в научной среде и является версией по умолчанию для мно-гих действующих систем (в основном для совместимости при обновлениях). Ког - да версия 3 была выпущена (в 2008 г.), большинство научных библиотек еще не было готово, и поэтому научное сообщество продолжило использовать предыду - щую версию. К счастью, с тех пор почти все библиотеки были обновлены, за ис - ключением лишь некоторых (см. http://py3readiness.org по поводу обзора совмес - тимости), оставшихся несовместимыми с Python 3. Несмотря на недавний рост популярности Python 3 (который продолжит свое развитие в будущем, и мы не должны это забывать), Python 2 все еще широко используется среди исследователей и аналитиков. Кроме того, в течение дли- тельного времени Python 2 устанавливался по умолчанию (например, в Ubuntu), таким образом, эта версия будет наиболее вероятной, которую большинство чи-тателей будет иметь под рукой. Исходя из всех этих причин, для этой книги мы примем за основу Python 2. Это решение вызвано не какой-то любовью к старым технологиям, а попросту является практичным выбором в пользу того, чтобы\n--- Страница 29 ---\n28  Первые шаги к масштабиру емости сделать книгу «Крупномасштабное машинное обучение на Python» доступной для большей аудитории: исх одный код на Python 2 сразу охватит всю существующую аудиторию экс - пертов в области данных; пользоват ели Python 3 очень легко смогут преобразовать наши сценарии для работы в версии Python, которую они предпочитают, потому что напи- санный нами исходный код легко конвертируем.  В случае если есть необходимость подробно разобраться в различиях между Python 2 и Python 3, мы предлагаем прочитать эту веб-страницу о написании совместимого между Python 2–3 исходного кода: http://python-future.org/compatible_idioms.html. Кроме того, на веб-сайте Python-Future можно найти полезные сведения о том, как преобразовать исход- ный код Python 2 в Python 3: http://python-future.org/automatic_conversion.html. Инсталляция среды Python В качестве первого шага мы создадим рабочую среду для науки о данных, которую можно использовать для репликации и тестирования примеров из книги и созда-ния прототипов собственных больших решений. Независимо от того, на каком языке вы собираетесь разрабатывать свое при- ложение, с Python вы не будете испытывать трудностей при получении своих данных, создании из них модели и извлечении правильных параметров, которые необходимы для выполнения прогнозов в эксплуатационной среде. Python – это общедоступный объектно-ориентированный кросс-платфор мен - ный язык программирования, который, по сравнению с его прямыми конкурен- тами (например, C/C++ и Java), производит очень сжатый и читаемый код. Это позволяет создавать действующий прототип программного обеспечения в очень короткие сроки и тестировать, поддерживать и масштабировать его в будущем. Он завоевал статус наиболее используемого языка среди программного инстру - ментария исследователя-аналитика, потому что, будучи языком общего назначе-ния, он в конечном итоге приобрел гибкость благодаря большому разнообразию имеющихся библиотек, которые способны легко и быстро помочь в решении са-мого широкого спектра как распространенных, так и нишевых задач. Пошаговая установка Если вы никогда не использовали Python (однако это не исключает того, что он на вашей машине может быть уже установлен), то сначала необходимо скачать уста-новщик с основного веб-сайта проекта https://www.python.org/downloads/ (напом- ним, что мы используем версию 2) и затем установить его на локальной машине. Этот раздел предоставляет вам полный контроль над тем, что может быть уста- новлено на вашей машине. Это очень удобно, если вы собираетесь использовать Python одновременно как язык для прототипирования и программирования промышленного кода. Кроме того, этот раздел поможет отслеживать используе-мые версии библиотек. Так или иначе, предупреждаем, что пошаговая установка в действительности потребует некоторого времени и усилий. С другой стороны, установка готового научного дистрибутива облегчит бремя инсталляционной процедуры и хорошо подойдет для первого раза и обучения, потому что экономит довольно много времени, хотя и установит на ваш компьютер сразу большое ко-\n--- Страница 30 ---\nPython для крупномасштабного машинного обучения  29 личество библиотек (которые вы по большей части, возможно, никогда не будете использовать). Поэтому если вы хотите начать немедленно и не желаете слишком уж беспокоиться по поводу контроля над инсталляцией, то просто пропустите эту часть и перейдите к следующему разделу «Научные дистрибутивы». Поскольку Python является мультиплатформенным языком программирова- ния, вы найдете установщики для компьютеров, которые работают под управ-лением операционной системы Windows либо Linux-/Unix-подобных ОС. Напом-ним, что в некоторых дистрибутивах Linux (таких как Ubuntu) язык Python 2 уже включен в репозиторий по умолчанию, что делает процесс установки еще проще. 1. Откройте оболочку Python, введите python в терминале или щелкните по значку Python. 2. Затем для тестирования результата инсталляции выполните следующие ко- манды в интерактивной оболочке Python, или ее стандартной интерактив-ной среде программирования REPL (от англ. Read-Eval-Print Loop, работаю- щей в цикле чтения-вычисления-печати результата), или других решениях, таких как Spyder или PyCharm: >>> import sys>>> print(sys.version) Если была поднята синтаксическая ошибка, то, значит, вы выполняете Python 3 вместо Python 2. Если же ошибка отсутствует и вы можете прочитать, что исполь-зуемой версией является Python 2.7.x (во время написания книги последней была версия 2.7.12), то поздравляем с установкой версии Python, которую мы выбрали для этой книги. Для тех же читателей, кто работает с Python 3, подойдут все версии начиная с Python 3.4 (во время написания последней была версия 3.5.2). Напоминаем, что, когда команда выдается в командой строке терминала, она предваряется префиксом $. В противном случае, если речь о стандартной среде Python REPL, ей предшествует подсказка >>>. Установка библиотек В зависимости от вашей системы и предыдущих инсталляций среда Python мо-жет не оказаться укомплектованной всем тем, в чем вы нуждаетесь, если не уста-новлен дистрибутив (который, с другой стороны, обычно укомплектован гораздо шире, чем вам, возможно, понадобится). Чтобы установить любую нужную библиотеку, можно применить команду pip или easy_install ; однако в будущем поддержка команды easy_install будет пре- кращена, и pip имеет перед ней важные преимущества. Инструмент установки библиотек Python pip непосредс твенно получает доступ к Интернету и выбирает их из каталога библиотек Python PyPI ( https: //pypi.python. org/pypi). PyPI предс тавляет собой репозиторий, содержащий сторонние библио- теки с открытым исходным кодом, которые постоянно поддерживаются в работо-способном состоянии и сохраняются в репозитории их авторами. Устанавливать библиотеки лучше всего при помощи pip по следующим при- чинам: он являет ся предпочтительным диспетчером библиотек Python и начиная с Python 2.7.9 и Python 3.4 по умолчанию включен в двоичные установщики Python;\n--- Страница 31 ---\n30  Первые шаги к масштабиру емости он обеспечивает функциональность по деинсталляции библиотек; он возвращает вашу систему в исходное состояние и оставляет ее чистой, если по какой-либо причине установленная библиотека перестала работать. Команда pip работает в командной строке и ускоряет весь процесс установки, обновления и удаления библиотек Python. Как уже упоминалось, если вы работаете как минимум с Python 2.7.9 или Py - thon 3.4, то команда pip должна уже иметься в наличии. Чтобы удостовериться в том, какие инструменты были установлены на локальной машине, выполните прямую проверку на возможные ошибки при помощи следующей команды: $ pip –V В некоторых инсталляциях в Linux и Mac OS устанавливается Python 3, а не Py - thon 2, в результате чего эта команда может присутствовать как pip3, поэтому при получении ошибки при поиске pip попробуйте выполнить следующую команду: $ pip3 –V Если это так, то напомним, что pip3 подходит только для установки библиотек в Python 3. Поскольку в книге мы работаем с Python 2 (если, разумеется, вы не решили использовать новейшую версию Python 3), то вашим основным установ-щиком библиотек всегда будет pip. Как вариант можно также проверить доступность старой команды easy_install : $ easy_install --version  Использовать easy_install вместо pip целесообразно, если вы работаете под Windows, по- тому что pip не устанавливает двоичных библиотек; поэтому если при установке библиотеки вы испытываете неожиданные трудности, то easy_install может сэкономить вам уйму вре- мени. Если проверка закончилась ошибкой, то вам действительно необходимо уста- новить pip с нуля (и при этом также easy_install ). Для установки pip просто следуйте инструкциям на https://pip.pypa.io/en/stable/ installing/. Самый безопасный путь состоит в том, чтобы скачать сценарий get-pip. py по прямой ссылке с https://bootstrap.pypa.io/get-pip.py и затем выполнить его при помощи следующей команды: $ python get-pip.py Между прочим, этот сценарий также установит настроечный инструмент setuptools с https://pypi.python.org/pypi/setuptools, который содержит easy_install . В качестве альтернативы, если вы работаете под управлением Unix-подобной операционной системы Debian/Ubuntu, укороченный способ установки всего, что нужно, будет состоять в команде apt-get : $ sudo apt-get install python3-pip После проверки этого основного требования теперь все готово к установке всех библиотек, которые потребуются для выполнения примеров, прилагаемых к на-стоящей книге. Чтобы установить типовую библиотеку <lib> , нужно просто вы- полнить следующую команду: $ pip install <lib>\n--- Страница 32 ---\nPython для крупномасштабного машинного обучения  31 Как вариант, если вы предпочитаете использовать команду easy_install , можно также выполнить следующую команду: $ easy_install <lib> После этого библиотека <lib> и все библиотеки, от которых она зависит, будут скачаны и установлены. Если вы не уверены в том, была библиотека установлена или нет, просто по- пробуйте импортировать из нее модуль. Если интерпретатор Python поднимает ошибку импорта ImportError, то можно сделать вывод, что библиотека не была установлена. Посмотрим на примере. Вот что происходит, когда библиотека NumPy была установлена: >>> import numpy >>> А вот что – если она не установлена: >>> import numpyTraceback (most recent call last):File “<stdin>”, line 1, in <module>ImportError: No module named numpy В последнем случае, прежде чем ее импортировать, надо установить библиоте- ку посредством pip или easy_install . Позаботьтесь о том, чтобы не перепутать библиотеки с модулями. При помо- щи pip вы устанавливаете библиотеку, а в среду Python вы импортируете модуль. Иногда библиотека и модуль имеют одинаковое название, но во многих случаях они не совпадают. Например, модуль sklearn вк лючен в библиотеку Scikit -learn. Способы обновления библиотек Как правило, вы окажетесь в ситуации, когда необходимо обновить библиотеку, потому что некая связанная с ней другая библиотека, т. н. зависимость, требует наличия новой версии, либо имеется дополнительный функционал, который тре-буется задействовать. Для этого сначала нужно проверить версию установленной библиотеки, обратившись к атрибуту __version__ , как показано на примере с биб - лиот екой NumPy ниже: >>> import numpy >>> numpy.__version__ # 2 символа подчеркивания перед ним и после него‘1.9.0’ Далее, если нужно обновить ее до более нового выпуска, скажем, в точности до версии 1.9.2, то из командной строки можно выполнить следующую ниже коман- ду: $ pip install -U numpy==1.9.2 Как вариант (но мы его не рекомендуем, только в случае, если это оказывается необходимым), можно также использовать следующую команду: $ easy_install --upgrade numpy==1.9.2\n--- Страница 33 ---\n32  Первые шаги к масштабиру емости Наконец, если вы попросту заинтересованы в ее обновлении до последней до- ступной версии, то просто выполните следующую команду: $ pip install -U numpy Можно также выполнить альтернативную команду easy_install : $ easy_install --upgrade numpy Научные дистрибутивы Из того, что вы прочли до сих пор, следует, что работа по созданию рабочей среды занимает у исследователя-аналитика много времени. Сначала нужно установить Python и затем, одну за другой, можно установить все библиотеки, в которых вы будете нуждаться. (Иногда инсталляционная процедура может пойти не так глад-ко, как вы надеялись.) Если вы хотите сэкономить время и усилия и гарантированно получить полно- стью рабочую, готовую к использованию среду Python, то можно просто скачать, установить и использовать научный дистрибутив Python. Помимо языка Python, такие дистрибутивы также содержат множество предварительно установленных библиотек, и иногда в них даже имеются для использования дополнительные ин-струменты и интегрированные среды разработки (IDE). Несколько из них хорошо известно среди исследователей-аналитиков, и в последующих разделах вы позна-комитесь с некоторыми главными особенности двух таких комплектов программ-ного обес печения, которые мы сочли самыми полезными и практичными. Чтобы немедленно сосредоточиться на содержимом настоящей книги, мы предлагаем сначала быстро скачать и установить научный дистрибутив, в част - ности Anaconda (ко торый, по нашему мнению, является самым полным из всех), и затем после выполнения примеров из этой книги полностью его деинсталлиро-вать, чтобы установить только один Python, дополнив его лишь теми библиотека-ми, в которых вы нуждаетесь для разработки своих проектов. И опять-таки, если это возможно, скачайте и установите версию дистрибутива, содержащую Python 2. В качестве первого комплекта программного обеспечения мы рекомендуем попробовать дистрибутив Python Anaconda (https://www.continuum.io/downloads), предлагаемый компанией Continuum Analytics, который включает почти 200 биб - лиот ек, в т. ч. NumPy, SciPy, pandas, IPython, matplotlib, Scikit-learn и StatsModels. Это кросс-платформенный дистрибутив, который можно установить на машины с другими существующими дистрибутивами и версиями Python, а его базовая вер-сия бесплатна. Дополнительные надстройки, которые содержат расширенные воз-можности, оплачиваются отдельно. Anaconda представляет двоичный диспетчер библиотек conda как инструмент командной строки для управления установкой библиотек. Как утверждается на веб-сайте дистрибутива, задача Anaconda состоит в том, чтобы обеспечить дистрибутив Python, готовый к работе на уровне пред-приятия, для крупномасштабной обработки данных, прогнозной аналитики и на-учных вычислений. Что касается версии 2.7 Python, то мы рекомендуем как ми-нимум дистрибутив Anaconda 4.0.0. (Чтобы взглянуть на устанавливаемые вмес те с Anaconda библиотеки, следует обратиться к списку на https://docs.continuum.io/ anaconda/pkg-docs.)\n--- Страница 34 ---\nPython для крупномасштабного машинного обучения  33 Второе предложение – дистрибутив WinPython (http: //winpython.sourceforge. net/). Этот дистрибутив может оказаться довольно интересной альтернативой, если вы работаете под Windows и желаете, чтобы ваш дистрибутив был перено-симым (извините, но версии под Linux и Mac OS отсутствуют). WinPython – это тоже бесплатный дистрибутив Python с открытым исходным кодом, который под-держивается сообществом. И он тоже разработан, имея в виду исследователей-аналитиков, и включает в себя комплект основных библиотек, таких как NumPy, SciPy, matplotlib и IPython (в основном те же, что и в Anaconda). Он так же, как и Anaconda, содержит инструментальную среду разработки Spyder, которая мо- жет быть полезной, если у вас есть опыт работы в интерфейсе языка MATLAB. Ре-шающее преимущество данного дистрибутива состоит в том, что он переносим (его можно поместить в любом каталоге или даже на флеш-накопитель USB), тем самым на компьютере могут существовать различные версии, их можно пере-мещать с одного компьютера Windows на другой, и более старая версия дистри-бутива легко заменяется на более новую всего лишь путем смены каталога. При выполнении WinPython или его оболочки он автоматически установит все необ-ходимые для выполнения Python переменные окружения так, как будто он был инсталлирован в регулярном режиме и зарегистрирован в системе.  Во время написания настоящей книги последним по времени дистрибутивом WinPython для версии Python 2.7 был созданный в октябре 2015 г. релиз 2.7.10; с тех пор публиковались обновления дистрибутива WinPython только для версии Python 3. После установки дис - трибутива в операционной системе, возможно, потребуется обновить некоторые ключевые библиотеки, необходимые для выполнения примеров из этой книги. Введение в Jupyter Бесплатный проект IPython был запущен Фернандо Пересом в 2001 г. с целью ре-шения проблемы отсутствия в Python стека научных исследований с использова-нием пользовательского программного интерфейса, который мог бы включать в процесс разработки программного обеспечения научный подход (главным об-разом экспериментирование и интерактивный поиск). Научный подход подразумевает проведение быстрых экспериментов в отно- шении разных гипотез в воспроизводимой форме (подобно задаче разведочного анализа в науке о данных). Используя IPython во время написания своего исход-ного кода, вы сможете реализовывать разведочные, итеративные и эмпирические (путем проб и ошибок) исследования более естественным образом. В конце 2015 г. значительная часть проекта IPython переместилась в новый проект под названием Jupyter ( http://jupyter.org/). Этот новый проект расширяет потенциальное удобство использования исходного интерфейса IPython до широ-кого спектра языков программирования 1. (Для получения полного списка язы- ков программирования посетите https://github.com/ipython/ipython/wiki/IPython- kernels-for-other-languages). Благодаря мощной идее ядер языков программирования специальный внеш- ний интерфейс позволяет передавать в них программы, которые выполняют 1 Далее в книге все упоминания интерактивной среды IPython заменены на Jupyter, являю - щуюся бо лее общей и более современной интерактивной средой программирования, куда IPython входит в качестве главной составной части (как нулевое ядро). – Прим. перев.\n--- Страница 35 ---\n34  Первые шаги к масштабиру емости пользовательский программный код, и в качестве отклика получать результат исполненного исходного кода; вы можете пользоваться единым интерфейсом и интерактивным стилем программирования независимо от того, на каком языке пишется программа. Jupyter (IPython является нулевым ядром, запускаемым с самого начала) можно описать как инструмент для выполнения интерактивных задач, пригодных для работы в консоли или веб-ориентированном блокноте, который предлагает спе-циальные команды, помогающие разработчикам лучше понимать и создавать ис - ходный код, пишущийся в настоящий момент. В отличие от интегрированной среды разработки (IDE), которая строится во- круг идеи написания сценария, его последующего выполнения и оценки его ре-зультатов, среда программирования Jupyter позволяет писать свой исходный код порциями, именуемыми ячейками, последовательно выполнять каждую из них и оценивать результаты каждой ячейки отдельно, исследуя одновременно тексто-вые и графические результаты. Помимо графической интеграции, эта среда пре-доставляет дополнительную помощь благодаря настраиваемым командам, бога-той истории (в формате JSON) и вычислительному параллелизму для повышения производительности во время работы с тяжелыми численными вычислениями. Такой подход также в особенности продуктивен для заданий, связанных с раз- работкой исходного кода на основе данных, поскольку он автоматически реа-лизует часто пренебрегаемую обязанность по документированию и иллюстри-рованию того, каким образом анализ данных был проведен, его предпосылок и допущений и его промежуточных и конечных результатов. Если часть вашего задания, кроме того, состоит в презентации работы с целью убедить в проекте внутренние или внешние заинтересованные стороны, то Jupyter может действи-тельно совершать волшебство, выполняя презентацию за вас, при этом требуя лишь небольших дополнительных усилий. На https://github.com/ipython/ipython/ wiki/A-gallery-of-interesting-IPython-Notebooks имеется множество примеров ис - пользования блокнотов, некоторые из которых могут вдохновить вас в вашей работе, как это вышло у нас. На самом деле мы должны признаться, что поддержание чистого, актуального блокнота Jupyter сэкономило нам неисчислимое количество времени, когда не-ожиданно всплыли незапланированные встречи с менеджерами/заинтересован-ными сторонами, которые потребовали от нас торопливо представить состояние нашей работы. Одним словом, Jupyter предлагает вам следующие возможности: видеть промежуточные результаты (и их отладку) для каждого шага ана - лиза; выполнять только некоторые разделы (или ячейки) с исходным кодом; хранить промежуточные результаты в формате JSON и иметь возможность выполнять на них версионный контроль; выполнять презентацию работы (в виде комбинации текста, исходного кода и изображений), делясь ею при помощи средства просмотра блокнотов Jupyter Notebook Viewer (http://nbviewer.jupyter.org/) и легко экспортируя его в PY, HTML, PDF или даже в слайдовые презентации.\n--- Страница 36 ---\nPython для крупномасштабного машинного обучения  35 Блокноты Jupyter являются нашим предпочтительным выбором на протяжении всей книги. Они используются для ясной и эффективной иллюстрации операций, связанных с изложением материала, на основе сценариев и данных, и последую-щих результатов. Хотя мы рекомендуем использовать исключительно Jupyter, если вы исполь- зуете среду интерпретатора REPL или IDE, то вы можете использовать те же са-мые инструкции и ожидать идентичных результатов (за исключением форматов и расширений оператора печати для возвращаемых результатов). Если в вашей операционной системе Jupyter не установлен, то его можно быст - ро ус тановить, воспользовавшись следующей командой: $ pip install jupyter  Полные инструкции по установке Jupyter (с охватом разных операционных систем) можно найти на http://jupyter.readthedocs.io/en/latest/install.html. Если Jupyter уже установлен, то он должен быть обновлен как минимум до вер- сии 4.1. После установки можно сразу начать использовать Jupyter, вызвав его из команд ной строки: $ jupyter notebook Открыв экземпляр Jupyter в браузере, щелкните по кнопке New и в разделе блокнотов Notebooks выберите Python 2 (в разде ле могут присутствовать другие ядра, в зависимости от того, что установлено): В этом месте новый пустой блокнот будет похож на следующий ниже снимок экрана, и можно приступать к вводу команд в ячейки:\n--- Страница 37 ---\n36  Первые шаги к масштабиру емости Например, можно набрать в ячейку следующее: In: print («Это проверка») Набрав в ячейку исходный код, просто нажмите кнопку воспроизведения (под вкладкой Cell), чтобы его выполнить и получить результат. Ниже под ячейкой с введенным кодом появится еще одна ячейка. Если при заполнении ячеек нажать кнопку с плюсом в верхней строке меню, то вы получите новую ячейку, при этом перемещаться от ячейки к ячейке можно при помощи стрелок в меню. Большинство других функций вполне интуитивно понятно, и мы приглаша- ем вас их попробовать. Чтобы лучше познакомиться с тем, как работает Jupyter, можно воспользоваться кратким практическим руководством, в частности http:// jupyter-notebook-beginner-guide.readthedocs.io/en/latest/, либо получить специали-зированную книгу с описанием функционала Jupyter.  Для полного изложения всего спектра функциональных возможностей Jupyter при выпол-нении ядра IPython обратит есь к следующим двум книгам издательства Packt Publishing: • IPy thon Interactive Computing and Visualization Cookbook («Книга рецептов по интерактив- ным вычислениям и визуализации в IPython»), автор Сирилл Россан (Cyrille Rossant), сен-тябрь 2014; • Learning IP ython for Interactive Computing and Data Visualization («Изучение IPython для интерактивных вычислений и визуализации данных»), автор Сирилл Россан, апрель 2013. Следует учитывать, что для большей наглядности каждый блок инструкций Jupyter имеет пронумерованный оператор ввода и оператор вывода, поэтому в этой книге вы увидите, что исходный код структурирован в двух блоках, за ис - ключением случаев, когда результат на выходе тривиален, – в этом случае ожи-дайте увидеть всего один входной оператор: In: <вводимый исходный код>Out: <полученный итоговый результат> Как правило, вам придется лишь набирать исходный код в ячейки после оператора In: и выполнять его. Затем вы будете сравнивать полученные вами выходные данные с результатом в книге после оператора Out:, который мы фактически получили на наших компьютерах, когда тестировали приводимый исходный код.\n--- Страница 38 ---\nБиблиотеки Python  37 библиО теки Python В настоящем разделе мы представим библиотеки, которые в этой книге будут применяться часто. Если вы не используете научного дистрибутива, то мы пред-лагаем пошаговое краткое описание того, на каких версиях необходимо остано-виться и каким образом быстро и успешно их установить 1. NumPy Библиотека NumPy, разработанная Трэвисом Олифантом (Travis Oliphant), нахо-дится в сердцевине каждого аналитического решения на языке Python. Он пред-лагает пользователю многомерные массивы вместе с большим набором функций для работы с многочисленными математическими операциями на этих массивах. Массивы – это блоки данных, расположенные вдоль многочисленных размерно-стей, программно реализующие математические векторы и матрицы. Массивы полезны не только для хранения данных, но и для быстрых матричных операций (векторизации), которые необходимы, когда вы хотите решать оперативные за-дачи науки о данных. Веб-сайт: http: //www.numpy.org/. Версия во время написания: 1.11.1 (1.13.0). Пред лагаемая команда установки: $ pip install numpy  В сообществе разработчиков на Python негласно принято при импорте библиотеки NumPy испо льзовать псевдоним np: import numpy as np SciPy Библиотека SciPy, исходный проект нескольких авторов (Travis Oliphant, Pearu Peterson и Eric Jones), дополняет функциональность библиотеки NumPy, предла-гая более широкое разнообразие научных алгоритмов для линейной алгебры, раз-реженных матриц, обработки сигналов и изображений, оптимизации, быстрого преобразования Фурье и многих других. Веб-сайт: http: //www.scipy.org/. Версия во время написания: 0.17.1 (0.19.0). Пред лагаемая команда установки: $ pip install scipy Pandas Библиотека pandas имеет дело со всем, что не могут делать NumPy и SciPy. В част - ности, благодаря своим специальным объектным структурам данных DataFrame и Series она позволяет обрабатывать сложные таблицы данных различных типов (то, что массивы NumPy не могут делать) и временные ряды. Благодаря создателю Уэс Маккинни (Wes McKinney) вы получите возможность легко и быстро загру - 1 В строке с указанием версии библиотеки в скобках приведена версия на момент публи- кации перево да настоящей книги. – Прим. перев.\n--- Страница 39 ---\n38  Первые шаги к масштабиру емости жать данные из разнообразных источников, а затем нарезать, группировать, об- рабатывать пропущенные элементы, добавлять, переименовывать, агрегировать, видоизменять и, наконец, визуализировать их по своему усмотрению. Веб-сайт: http: //pandas.pydata.org/. Версия во время написания: 0.18.0 (0.20.2). Пред лагаемая команда установки: $ pip install pandas  Традиционно библиотека pandas импортируется как pd: import pandas as pd Scikit-learn Появившись в составе SciKits (SciPy Toolkits, т. е. инструментария библиотеки SciPy), сегодня библиотека Scikit-learn является ключевым элементом операций в области науки о данных на Python. Он предлагает все, в чем вы, возможно, бу - дете нуждаться с точки зрения предобработки данных, обучения с учителем и без учителя, отбора модели, перекрестной проверки и метрик ошибочности. Мы бу - дем подробно говорить об этой библиотеке на протяжении всей книги. Разработка библиотеки Scikit-learn была начата Дэвидом Курнапо (David Cour - napeau) в 2007 г. в рамках ежегодного проекта Google Summer of Code. С 2013 г. она была передана исследователям в Inria (во французском Исследовательском институте в области информатики и автоматизации). Библиотека Scikit-learn предлагает модули для обработки данных ( sklearn. preprocessing и sklearn.feature_extraction ), отбора и перекрестной проверки моде- лей (sklearn.model_selection и sklearn.metrics ) и полный комплект методов ( sklearn. linear_model ), в котором предполагается, что целевое значение, число или вероят - ность, будет линейной комбинацией входных переменных. Веб-сайт: http: //scikit-learn.org/stable/. Версия во время написания: 0.17.1 (0.18.1). Пред лагаемая команда установки: $ pip install scikit-learn  Отметим, что импортированный модуль называется sklearn . Matplotlib Библиотека matplotlib первоначально была разработана Джоном Хартером (John Hunter). Она содержит все стандартные компоненты, необходимые для создания качественных графиков из массивов и их визуализации в интерактивном режиме. Внутри модуля PyLab можно найти все MATLAB-подобные компоненты для по- строения графиков и диаграмм. Веб-сайт: http: //matplotlib.org/. Версия во время написания: 1.5.1 (2.0.2). Пред лагаемая команда установки: $ pip install matplotlib Имеется возможность импортировать только то, что требуется для целей визуа - лизации:\n--- Страница 40 ---\nБиблиотеки Python  39 import matplotlib as mpl from matplotlib import pyplot as plt Gensim Библиотека с открытым исходным кодом Gensim, разработанная Радимом Жехуже-ком (Radim Řehůřek), предназначена для анализа больших текстовых наборов с ис - пользованием параллельно распределяемых онлайновых алгоритмов. Среди прочего продвинутого функционала в ней реализованы латентно-семантический анализ (LSA), тематическое моделирование на основе латентного распреде ления Дирих- ле (LDA) и мощный алг оритм Google word2v ec для преобразования текстов в вектор- ные признаки для использования в машинном обучении с учителем и без учителя. Веб-сайт: http: //radimrehurek.com/gensim/. Версия во время написания: 0.13.1 (2.1.0). Пред лагаемая команда установки: $ pip install gensim H2O Платформа с открытым исходным кодом H2O была создана стартап-компанией H2O.ai (ранее называвшейся 0xdata) и служит для анализа больших данных. Она работает с языками программирования R, Python, Scala и Java. Платформа H2O позволяет легко использовать автономную машину (с привлечением многопро-цессорной обработки) или кластер Hadoop (например, кластер в среде AWS), тем самым помогая достигать вертикальной и горизонтальной масштабируемости. Веб-сайт: http: //www.h2o.ai. Версия во время написания: 3.8.3.3 (3.10.4.8). Чтобы установить эту библиотеку, сначала необходимо скачать и установить в операционную систему среду Java (вам понадобится комплект разработчика приложений на Java Java Dev elopment Kit (сокращенно JDK) версии 1.8, посколь- ку H2O основывается на Java), затем можно обратиться к онлайновым инструкци-ям, предоставленным на http://www.h2o.ai/download/h2o/python. Все шаги по инсталляции платформы можно свести к следующим строкам.При помощи следующих ниже инструкций можно установить одновременно платформу H2O и ее программный интерфейс для Python, который мы использо-вали в нашей книге: $ pip install -U requests$ pip install -U tabulate$ pip install -U future$ pip install -U six Эти команды установят необходимые библиотеки, и затем можно установить платформу, позаботившись об удалении любой предыдущей установки: $ pip uninstall h2o$ pip install h2o Для того чтобы установить ту же самую версию, которую мы применяли в на- шей книге, можно поменять последнюю команду pip install на следующую: $ pip install http://h2o-release.s3.amazonaws.com/h2o/rel-turin/3/Python/h2o-3.8.3.3-py2.py3-none-any.whl\n--- Страница 41 ---\n40  Первые шаги к масштабиру емости Если возникнут какие-либо проблемы, приглашаем посетить страницу плат - формы H2O в Google-группах, где можно получить справку по вашей проблеме: https://groups.google.com/forum/#!forum/h2ostream. XGBoost Масштабируемая, переносимая и распределенная библиотека градиентного бус - тинга XGBoost (древовидный ансамб левый алгоритм машинного обучения) име- ется для языков Python, R, Java, Scala, Julia и C++; она может работать на одиночной машине (с привлечением многопоточной обработки) одновременно в кластерах Hadoop и Spark. Веб-сайт: https: //xgboost.readthedocs.io/en/latest/. Версия во время написания: 0.4 (0.6a2). Подробные инструкции по установке библиотеки XGBoost в вашей операцион- ной системе можно найти на https://github.com/dmlc/xgboost/blob/master/doc/build. md. Установка XGBoost в Linux и Mac OS довольно прямолинейная, но немного бо- лее заковыристая для пользователей Windows. По этой причине ниже мы в поша-говом режиме приводим процедуру ее установки, чтобы получить работающую под Windows версию этой библиотеки: 1. Преж де всего скачать и установить распределенную систему контроля вер- сий Git для Windo ws ( https://git-for-windows.github.io/). 2. Затем потребуется наличие в операционной системе компилятора MinGW (минималистской среды разработки GNU для Windows). Его можно скачать с https://sourceforge.net/projects/mingw-w64/ либо https://wiki.qt.io/MinGW-64- bit или http://www.mingw.org/ в соответствии с характеристиками вашей си- стемы. Во время инсталляции можно выбрать следующие настройки (на ри-сунке показано окно настроек среды MinGW из первой ссылки):\n--- Страница 42 ---\nБиблиотеки Python  41 3. Из командной с троки выполнить следующее: $ git clone --recursive https://github.com/dmlc/xgboost $ cd xgboost$ git submodule init$ git submodule update 4. Затем из командной строки, находясь в каталоге xgboost, скопировать кон- фиг урацию для 64-разрядных систем, сделав ее конфигурацией по умолча- нию: $ copy make\\mingw64.mk config.mk Как вариант можно скопировать простую 32-разрядную версию: $ copy make\\mingw.mk config.mk 5. После копирования конфигурационного файла запустить компилятор, уста-новив число потоков равным 4 для ускорения процедуры компиляции1: $ make -j4 6. В заключение, если компилятор завершил свою работу без ошибок, устано-вить библиотеку в свою среду Python, выполнив следующие команды: $ cd python-package $ python setup.py install Theano Библиотека Python Theano позволяет эффективным образом определять, опти- мизировать и вычислять математические выражения, в т. ч. многомерные масси- вы. В сущности, она предоставляет все структурные компоненты, необходимые для создания глубоких нейронных сетей. Веб-сайт: http: //deeplearning.net/software/theano/. Версия во время написания: 0.8.2 (0.10.0.dev1). У становка Theano должна быть прямолинейной, поскольку теперь она находит - ся в каталоге библиотек PyPI: $ pip install Theano Если нужно установить самую последнюю обновленную версию библиотеки, то ее можно склонировать с GitHub: $ git clone git://github.com/Theano/Theano.git Затем можно перейти непосредственно к установке в среде Python: $ cd Theano $ python setup.py install Чтобы проверить результат установки, можно выполнить следующие команды в терминале/командной оболочке и проверить отчеты по установке: $ pip install nose $ pip install parameterized$ nosetests theano 1 Компоновщик make в MinGW имеет имя mingw32-make.exe , поэтому следующая команда должна быть: mingw32-make -j4. – Прим. перев.\n--- Страница 43 ---\n42  Первые шаги к масштабиру емости Если вы работаете в Windows и предыдущие инструкции не срабатывают, то можно попробовать следующее: 1. Ус тановить 64-разрядный комплект компиляторов TDM-GCC x64 (http://tdm- gcc.tdragon.net/). 2. Открыть к омандную строку Anaconda и выполнить следующее: $ conda update conda $ conda update –all$ conda install mingw libpython$ pip install git+git://github.com/Theano/Theano.git  Библиотека Theano требует наличия библиотеки libpython, которая пока не совместима с версией Python 3.5, поэтому, будучи установленным в Windows, он не работает, и этот факт может оказаться наиболее вероятной причиной. Кроме того, веб-сайт Theano предоставляет пользователям Windows некоторую информацию, которая может вас поддерживать, когда все остальное не получает - ся: http://deeplearning.net/software/theano/install_windows.html. Для обеспечения горизонтального масштабирования при помощи Theano с ис - пользованием модулей GPU крайне необходима установка драйверов программно-аппаратной архитектуры параллельных вычислений CUDA от компании NVIDIA, а также SDK для генерирования исходного кода и его выполнения на CPU. Если вы не располагаете достаточными сведениями об инструментарии CUDA Toolkit, то, собственно, вы можете начать с приведенной ниже веб-страницы, чтобы глубже разобраться в используемой технологии: https://developer.nvidia.com/cuda-toolkit. Таким образом, если ваш компьютер обладает графическим процессо- ром NVIDIA, то вы сможете найти все необходимые инструкции по установке CUDA, используя приведенную ниже учебную страницу непосредственно от са-мой компании NVIDIA: http://docs.nvidia.com/cuda/cuda-quick-start-guide/index. html#axzz4A8augxYy. TensorFlow Еще одна программная библиотека с открытым исходным кодом TensorFlow так же, как и Theano, предназначена для численных вычислений. В ней вместо простых массивов используются графы потока данных. Узлы в таком графе пред-ставлены математическими операциями, тогда как дуги графа – многомерными массивами данных (так называемыми тензорами), перемещаемыми между узла-ми. Первоначально разработчики Google развивали проект TensorFlow в рамках команды Google Brain Team, но совсем недавно сделали эту библиотеку общедо-ступной. Веб-сайт: https: //github.com/tensorflow/tensorflow. Версия во время написания: 0.8.0 (1.2.0rc2). Чтобы установить TensorFlow на своем компьютере, следуйте инструкциям, на- ходящимся по следующей ссылке: https://www.tensorflow.org/install/. Имеется поддержка Windows; соответствующие инструкции по установке мож - но найти, перейдя по ссылке: https://www.tensorflow.org/install/install_windows. В отличие от других используемых в книге библиотек, TensorFlow работает только с Python 3.5+. Для пользователей Windows хорошим компромиссом может\n--- Страница 44 ---\nБиблиотеки Python  43 быть выполнение данной библиотеки в Linux в рамках виртуальной машины или машины Docker. (Приведенная выше веб-страница с информацией по установке библиотеки содержит указания относительно того, как это сделать.) sknn Библиотека sknn (расширение scikit-neuralnetwork библиотеки scikit для работы с нейронными сетями) представляет собой обертку вокруг библиотеки Pylearn2 и служит в качестве вспомогательного средства для реализации глубоких нейрон-ных сетей, не требуя от вас становиться экспертом по Theano. В качестве бону - са настоящая библиотека совместима с программным интерфейсом библиотеки Scikit-learn. Веб-сайт: https: //scikit-neuralnetwork.readthedocs.io/en/latest/. Версия во время написания: 0.7 (0.7). Пред лагаемая команда установки: $ pip install scikit-neuralnetwork В факультативном порядке, если есть потребность воспользоваться преиму - ществами наиболее продвинутого функционала, в частности сверткой (конволю-цией), объединением или вертикальным масштабированием, необходимо завер-шить установку следующим образом: $ pip install -r pip install -r https://raw.githubusercontent.com/aigamedev/scikit-neuralnetwork/master/requirements.txt После установки также необходимо выполнить следующее: $ git clone https://github.com/aigamedev/scikit-neuralnetwork.git$ cd scikit-neuralnetwork$ python setup.py develop Как и в случае с библиотекой XGBoost, это сделает библиотеку sknn доступной в вашей среде Python. Theanets Библиотека theanets представляет собой написанный на Python комплект ин- струментов по глубокому обучению и нейронным сетям, в котором для ускорения вычислений используется библиотека Theano. Как и в случае с библиотекой sknn, она разработана с целью упростить взаимодействие с функционалом библиотеки Theano для создания моделей глубокого обучения. Веб-сайт: https: //github.com/lmjohns3/theanets. Версия во время написания: 0.7.3 (0.7 .3). Пред лагаемая команда установки: $ pip install theanets Текущую версию данной библиотеки можно также загрузить с GitHub и устано- вить непосредственно в Python: $ git clone https://github.com/lmjohns3/theanets$ cd theanets$ python setup.py develop\n--- Страница 45 ---\n44  Первые шаги к масштабиру емости Keras Библиотека Python Keras – это минималистская, мультимодульная библиотека для программирования нейронных сетей, которая может выполняться поверх биб лиот ек TensorFlow и Theano. Веб-сайт: http: //keras.io/. Версия во время написания: 1.0.5 (2.0.5). Пред лагаемая команда установки: $ pip install keras Последнюю версию библиотеки (в виде версии, находящей в процессе доработ - ки) можно установить при помощи следующей команды: $ pip install git+git://github.com/fchollet/keras.git Другие вспомогательные библиотеки Завершая этот длинный обзор целого ряда библиотек, которые вы увидите в дей-ствии на последующих страницах настоящей книги, мы закончим тремя просты-ми и одновременно довольно полезными библиотеками, которые не нуждаются в подробных описаниях, но которые нужно обязательно установить в вашей опе-рационной системе: memory_profiler, climate и NeuroLab. Профилировщик оперативной памяти memory_profiler – это библиотека, которая управляет тем, как вычислительный процесс использует память. Она также по-могает анализировать потребление памяти конкретным сценарием Python в по-строчном режиме. Данную библиотеку можно установить следующим образом: $ pip install -U memory_profiler Библиотека climate состоит всего из нескольких основных утилит командной строки, предназначенных для Python. Ее можно быстро установить следующим образом: $ pip install climate Наконец, библиотека NeuroLab – это чрезвычайно простая нейросетевая биб - лиот ека, которая отдаленно основана на комплекте инструментов Neural Net- work Toolbo x (NNT) языка MATLAB. В ее основе лежат библиотеки NumPy и SciPy, но не Theano, и поэтому от нее не следует ожидать какой-то удивительной про-изводительности. Однако стоит отметить, что она предлагает хороший набор ин-струментов для обучения. Эту библиотеку можно легко установить следующим образом: $ pip install neurolab резюме В этой вводной главе мы проиллюстрировали различные способы, которыми можно сделать алгоритмы машинного обучения масштабируемыми, используя язык Python (методы вертикального и горизонтального масштабирования). Мы также предложили несколько мотивирующих примеров и заложили основу для\n--- Страница 46 ---\nРезюме  45 книги, проиллюстрировав, каким образом устанавливать на машину среду Python. В частности, мы познакомили вас со средой интерактивного программирования Jupyter и охватили все самые важные библиотеки, которые будут использоваться в настоящей книге. В следующей главе мы займемся обсуждением того, каким образом алгоритм стохастического градиентного спуска способен помочь справляться с крупными наборами данных с привлечением стандартного потока ввода-вывода на одиноч-ной машине. В целом мы охватим различные способы потоковой передачи дан-ных из больших файлов либо репозиториев данных и их подачи на вход базово-го обучаемого алгоритма. Вы будете поражены тем, как простые решения могут оказаться эффективными, и обнаружите, что даже настольный компьютер может легко перемалывать большие данные.",
      "debug": {
        "start_page": 20,
        "end_page": 46
      }
    },
    {
      "name": "Глава 2. Масштабируемое обучение в Scikit-learn 46",
      "content": "--- Страница 47 --- (продолжение)\nГлава 2 Масштабируемое обучение в Scikit-learn Зачастую при наличии довольно мощных и одновременно доступных компью- теров последних лет не представляет особого труда загрузить набор данных в оперативную память, подготовить матрицы данных, натренировать алгоритм машинного обучения и протестировать его обобщающие способности с использо-ванием вневыборочных наблюдений. Однако все чаще и чаще масштаб подлежа-щих переработке данных настолько огромен, что их загрузка в основную память компьютера становится невозможной, и даже если с этим удастся справиться, получить результат не получится как с точки зрения управления данными, так и с точки зрения машинного обучения. Приемлемые альтернативные стратегии, помимо обработки данных в основной памяти, вполне возможны: разбивка данных на выборки, использование парал-лелизма и, наконец, обучение малыми пакетами данных либо на одиночных пре-цедентах. В центре внимания настоящей главы будет готовое решение, которое предлагает библиотека Scikit-learn: потоковая передача мини-пакетов, сос то я щи х из прецедентов (т. е. наблюдений), из хранилища и при инкрементном обучении на их основе. Подобного рода решение называется внеядерным обучением (out-of-core learning), или обучением вне основной памяти компьютера. Обработка данных посильными порциями и инкрементное обучение – замеча- тельная идея. Однако когда вы попытаетесь ее реализовать, она может оказаться гораздо сложнее из-за ограничений в обучающихся алгоритмах. К тому же пере-дача данных в потоке требует, чтобы вы думали иначе с точки зрения управления данными и извлечения признаков. Помимо знакомства с функционалом библио - теки Scikit-learn для внеядерного обучения, мы также попытаемся представить решения на Python вне всякого сомнения чрезвычайно сложных задач, с кото-рыми можно столкнуться, когда придется в каждый момент времени наблюдать только малые порции своих данных. В этой главе мы затронем следующие темы: реализация внеядерног о обучения в библиотеке Scikit-learn; эффективное управление потоками данных при помощи хэширования признаков; сос тавные части механизма стохастического обучения; реализация науки о данных на основе онлайновог о обучения; преобразования пот оков данных в рамках обучения без учителя.\nГлава 2 Масштабируемое обучение в Scikit-learn Зачастую при наличии довольно мощных и одновременно доступных компью- теров последних лет не представляет особого труда загрузить набор данных в оперативную память, подготовить матрицы данных, натренировать алгоритм машинного обучения и протестировать его обобщающие способности с использо-ванием вневыборочных наблюдений. Однако все чаще и чаще масштаб подлежа-щих переработке данных настолько огромен, что их загрузка в основную память компьютера становится невозможной, и даже если с этим удастся справиться, получить результат не получится как с точки зрения управления данными, так и с точки зрения машинного обучения. Приемлемые альтернативные стратегии, помимо обработки данных в основной памяти, вполне возможны: разбивка данных на выборки, использование парал-лелизма и, наконец, обучение малыми пакетами данных либо на одиночных пре-цедентах. В центре внимания настоящей главы будет готовое решение, которое предлагает библиотека Scikit-learn: потоковая передача мини-пакетов, сос то я щи х из прецедентов (т. е. наблюдений), из хранилища и при инкрементном обучении на их основе. Подобного рода решение называется внеядерным обучением (out-of-core learning), или обучением вне основной памяти компьютера. Обработка данных посильными порциями и инкрементное обучение – замеча- тельная идея. Однако когда вы попытаетесь ее реализовать, она может оказаться гораздо сложнее из-за ограничений в обучающихся алгоритмах. К тому же пере-дача данных в потоке требует, чтобы вы думали иначе с точки зрения управления данными и извлечения признаков. Помимо знакомства с функционалом библио - теки Scikit-learn для внеядерного обучения, мы также попытаемся представить решения на Python вне всякого сомнения чрезвычайно сложных задач, с кото-рыми можно столкнуться, когда придется в каждый момент времени наблюдать только малые порции своих данных. В этой главе мы затронем следующие темы: реализация внеядерног о обучения в библиотеке Scikit-learn; эффективное управление потоками данных при помощи хэширования признаков; сос тавные части механизма стохастического обучения; реализация науки о данных на основе онлайновог о обучения; преобразования пот оков данных в рамках обучения без учителя.\n--- Страница 48 ---\nВнеядерное обучение  47 внеядернОе Обучение Как было упомянуто во вступлении к настоящей главе, внеядерное обучение от - носится к разряду алгоритмов, работающих с данными, которые не умещаются в оперативной памяти одиночного компьютера, но которые легко могут уместить-ся в хранилище данных, таком как локальный жесткий диск или веб-репозиторий. Объем имеющейся в распоряжении RAM, т. е. основной, памяти на одиночной ма-шине может в действительности колебаться от нескольких гигабайт (иногда 2 Гб, чаще 4 Гб, но мы возьмем за основу, что вы располагаете максимум 2 Гб) до 256 Гб на больших серверных машинах. Крупные серверы можно получить в облачных вычислительных веб-службах, таких как Эластичное вычислит ельное облак о компании Amazon (Elastic Compute Cloud, EC2), тогда как ваши возможности по хранению данных могут легко превышать терабайтные объемы, используя при этом всего лишь внешний привод (скорее всего, объемом порядка 1 Tб, но кото-рый может достигать 4 Tб). Поскольку машинное обучение основывается на глобальном уменьшении функции стоимости, многие алгоритмы первоначально задумывались для рабо-ты на каждой итерации процесса оптимизации с использованием абсолютно всех имеющихся данных. Это в особенности верно для всех алгоритмов, основанных на статистическом обучении, в которых используется матричное исчисление, к примеру инвертирование матриц, к тому же алгоритмы, основанные на жадном поиске, нуждаются в оценивании как можно большего объема данных, перед тем как сделать следующий шаг. Следовательно, наиболее распространенные гото-вые к работе алгоритмы наподобие регрессионных (взвешенные линейные ком-бинации признаков) обновляют свои коэффициенты, пытаясь минимизировать объединенную ошибку всего набора данных целиком. Деревья решений, будучи очень чувствительными к шуму в наборе данных, аналогичным образом вынуж - дены для отыскания наилучшего решения выбирать оптимальное расщепление, основываясь на всех имеющихся данных. Если в такой ситуации данные не умещаются в основной памяти компьютера, то остается не так уж и много возможных решений. Можно увеличить имеющую-ся оперативную память (в зависимости от ограничений системной платы, после чего придется обратиться к распределенным системам, таким как Hadoop и Spark, и это решение мы упомянем в последних главах настоящей книги), либо попросту уменьшить набор данных, чтобы он уместился в оперативной памяти. Если данные разреженные, т. е. в наборе данных есть много нулевых значений, то можно преобразовать плотную матрицу в разреженную. Это, как правило, про-исходит с текстовыми данными со многими столбцами, потому что каждый из них – это слово, но с малым числом значений, которые обозначают количества словоупотреблений, потому что одиночные документы обычно показывают огра-ниченный набор слов. Иногда решить проблему может использование разрежен-ных матриц, позволяя загружать и обрабатывать другие довольно крупные наборы данных, но это не панацея (извините, никаких бесплатных обедов, т. е. отсутствует решение, которое подходило бы для абсолютно всех задач), потому что некоторые матрицы данных, хотя и разреженные, могут иметь пугающие размеры. В такой ситуации можно всегда попытаться уменьшить набор данных путем со- кращения числа прецедентов (строк) либо ограничения числа признаков (столб-\n--- Страница 49 ---\n48  Масштабиру емое обучение в Scikit-learn цов), тем самым снижая размерности матрицы набора данных и занятой ею обла- сти в оперативной памяти. Сокращение набора данных посредством извлечения только части наблюдений называется подвыборкой (частичным отбором либо просто выборкой). Подвыборка, по сути, не является неправильным решением, но она имеет серьезные недостатки, и о них необходимо помнить прежде, чем принимать решение о дальнейшем ходе анализа данных. Подвыборка как приемлемый вариант При извлечении подвыборки в действительности отбрасывается часть имеюще- гося информационного многообразия, при этом невозможно быть абсолютно уве-ренным, что отбрасываются избыточные, не совсем полезные данные. В действи-тельности же некоторые неизвестные ранее «жемчужины» можно обнаружить только в результате рассмотрения всех данных. Несмотря на то что в вычисли-тельном плане это решение выглядит привлекательным – потому что извлечение подвыборки требует лишь того, чтобы генератор случайных чисел сообщал, следу - ет ли брать некий прецедент или нет, – извлекая отобранный набор данных, вы на самом деле рискуете ограничить возможности своего алгоритма тем, что он будет обучаться правилам и ассоциациям в данных не в полном объеме. В компромис - се между дисперсией и смещением извлечение подвыборки вызывает инфляцию дисперсии в предсказаниях, потому что оценки будут неопределеннее вследствие случайного шума в данных либо выбросов, т. е. периферийных наблюдений. В мире больших данных побеждает алгоритм с более качественными данными, потому что он может обучиться большему количеству способов соотнесения пред-сказания с предикторами, чем другие модели с меньшими по объему (либо более шумными) данными. Следовательно, подвыборка, хотя и является приемлемым ре-шением, может наложить ограничение на результаты вашей деятельности по ма-шинному обучению из-за менее точных предсказаний и большей дисперсии оценок. Недостатки подвыборки так или иначе могут быть преодолены, если обучать- ся многочисленным моделям на многочисленных подвыборках данных и за-тем собирать все решения в ансамбль либо накладывать результаты работы всех моделей поверх друг друга в каскад, тем самым создавая уменьшенную матри-цу данных для проведения дальнейшей тренировки. Эта процедура называется агрегацией бутстрапированных выборок, или бэггингом. (Благодаря этому вы на самом деле сжимаете признаки, тем самым сокращая пространство данных в опе-ративной памяти.) Мы займемся исследованием способов составления ансамблей и каскадов в одной из глав книги и узнаем, как они в действительности снижают дисперсию оценок, накачиваемую за счет подвыборки. Как вариант вместо сокращения прецедентов можно сократить признаки, но опять же мы получим проблему, которая состоит в том, что сперва нужно постро-ить модель из данных, чтобы проверить, какие признаки можно отобрать, так что нам по-прежнему нужно построить модель с данными, которые не умещаются в оперативной памяти. Оптимизация по одному прецеденту за раз Разобравшись, что подвыборка данных является хоть и всегда приемлемым, но не оптимальным решением, мы должны оценить другой подход, и внеядерный подход на самом деле не требует отказываться от наблюдений (строк) или призна-\n--- Страница 50 ---\nВнеядерное обучение  49 ков (столбцов). Просто на тренировку модели уходит чуть больше времени, требуя большего количества итераций и пересылки данных из хранилища в оператив-ную память компьютера. Прямо сейчас мы дадим первое интуитивно понятное объяснение того, как работает внеядерное обучение. Начнем с обучения, т. е. процесса, в котором мы пытаемся соотнести неизвест - ную функцию, демонстрирующую отклик (число либо исход, имея в виду зада-чу регрессии либо классификации) относительно имеющихся данных. Обучение осуществляется путем подгонки внутренних коэффициентов обучающегося ал-горитма в попытке достичь оптимального соответствия на имеющихся данных, а именно минимизируя функцию стоимости, т. е. меру, которая говорит, насколь-ко хорошей является наша аппроксимация. Если резюмировать, мы говорим о процессе оптимизации. Различные алгоритмы оптимизации, аналогично градиентному спуску, пред- ставляют собой процессы, которые в состоянии работать на любых объемах дан-ных. Они занимаются получением градиента оптимизации (направления в ходе оптимизации) и заставляют обучающийся алгоритм адаптировать свои парамет - ры, с ледуя вдоль градиента. Если говорить конкретно о градиентном спуске, то после определенного числа итераций при условии решаемости задачи и отсутствия других проб лем, таких как слишком высокий темп обучения, градиент должен стать настолько малым, что процесс оптимизации можно остановить. В конце процесса можно с уверен-ностью сказать, что было найдено решение, которое является оптимальным (по-тому что это глобальный оптимум, хотя иногда он может оказаться локальным, в случае если аппроксимируемая функция не выпуклая). Поскольку направленность, продиктованная градиентом, может быть взята, основываясь на любом количестве примеров, она может быть взята и на одиноч-ном прецеденте. Взятие градиента на одиночном прецеденте требует небольшо-го темпа обучения, но в конце процесс может достичь такой же оптимизации, что и градиентный спуск, взятый на полных данных. В конечном счете алгоритму требуется только направление, которое правильно сориентирует процесс обуче - ния по подгонке под имеющиеся данные. Следовательно, обучение такому на- правлению на случайном одиночном спуске из данных является полностью вы-полнимым: можно получить одинаковые результаты, как если бы мы обрабатывали все данные одним разом, хотя ход оптимизации может стать чуть более шероховатым; если большинство наблюдений указывает на оптимальное направление, то алгоритм пойдет по нему. Единственная трудность будет состоять в правильной доводке нужных параметров процесса обучения и многократном прохождении по данным, для того чтобы оптимизация гарантированно завершилась, так как такая процедура обучения намного медленнее, чем работа со всеми имеющимися данными; не б удет никаких особых трудностей с тем, как справиться с хранением оди- ночного прецедента в основной памяти машины, оставляя весь массив дан-ных вне ее. Другие трудности могут быть связаны с перемещением данных в размере одиночных примеров из репозитория в основную память маши-ны. Масштабируемость гарантируется, потому что требующееся для обра-ботки данных время линейное; времязатраты использования еще одного\n--- Страница 51 ---\n50  Масштабиру емое обучение в Scikit-learn прецедента всегда одинаковые, независимо от общего числа прецедентов, которые предстоит обработать. Метод подгонки обучающегося алгоритма каждый раз всего на одном преце- денте либо подмножестве данных, который умещается в оперативной памяти, называется онлайновым (или динамическим) обучением, а градиентный спуск, выполняемый на основе таких одиночных наблюдений, называется стохастиче-ским градиентным спуском (stochastic gradient descent, SGD). Как уже указывалось ранее, онлайновое обучение – это внеядерная методика, и она принята за основу в библиотеке Scikit-learn во многих обучающихся алгоритмах. Создание системы внеядерного обучения Мы проиллюстрируем внутреннее устройство алгоритма стохастического гради-ентного спуска чуть позже в следующих нескольких абзацах, предложив на этот счет больше подробностей и рассуждений. Сейчас же знание того, каким образом можно обучаться вне ядра (благодаря алгоритму стохастического градиентного спуска), позволяет нарисовать более четкую картину того, что мы должны сделать, чтобы заставить его работать на своем компьютере. Можно подразделить свои действия на несколько разных задач:1. Подг отовить доступ к своему репозиторию данных для потоковой переда- чи по одному прецеденту. Это действие может потребовать рандомизации порядка следования строк данных перед доставкой данных на компьютер, чтобы удалить любую информацию, которая может быть вызвана упорядо-ченностью данных. 2. Сначала выпо лнить небольшое обследование данных, возможно, на неболь- шой части всех данных (к примеру, взяв первые десять тысяч строк) в по-пытке выяснить, согласуются ли поступающие прецеденты между собой по числу признаков, типу данных, присутствию или отсутствию значений, ми-нимальным и максимальным значениям каждой переменной и их средним и медианным значениям. Уточнить размах или класс целевой переменной. 3. Преобразовать каждую полученную строку данных в фиксированный фор- мат, который может быть принят обучающимся алгоритмом (плотный или разреженный вектор). На данном этапе можно выполнить любое элементар-ное преобразование, к примеру превратить категориальные признаки в чис - ловые или дать числовым признакам взаимодействовать без посторонней помощи, взяв векторное произведение непосредственно самих признаков. 4. После рандомизации порядка следования примеров (как упомянуто в пер- вом пункте) определиться с процедурой перекрестной проверки, используя регулярно отложенные выборки (holdout) либо выборки, отложенные после определенного числа наблюдений. 5. Выполнить доводку гиперпараметров путем повторной потоковой передачи данных либо обрабатывая небольшие выборки из них. На этом этапе самое под-ходящее время, чтобы заняться конструированием признаков (с использовани-ем обучения без учителя и специальных функций преобразования, таких как ядерные аппроксимации) и задействовать регуляризацию и отбор признаков. 6. Построить свою итоговую модель с использованием данных, которые были зарезервированы для тренировки, и идеальным образом протестировать эффективность модели на новых данных.\n--- Страница 52 ---\nПотоковая передача данных из источников  51 В качестве первого шага мы обсудим способы подготовки данных и затем легко создадим поток, подходящий для онлайнового обучения, с привлечением вспо- могательных функций из библиотек Python, в частности из pandas и Scikit-learn. пОтОкОвая передача данных из истОчник Ов Некоторые данные в полном смысле этого слова текут потоком через ваш компью-тер, когда имеется порождающий процесс, который передает данные; эти данные можно обрабатывать на лету либо просто отбрасывать без возможности их после-дующего восстановления, если, конечно, их не сохранить где-нибудь в репозито-рии архивации данных. Это все равно, что таскать воду из речного потока, – река продолжает течь, однако вы можете фильтровать и обрабатывать всю воду, покуда она течет. Это совершенно другая стратегия, радикально отличающаяся от обра-ботки всех данных сразу, которая больше походит на ситуацию, когда вся вода помещена в отстойник, ограниченный плотиной (по аналогии с обработкой всех данных в оперативной памяти). В качестве примера потоковой передачи можно привести поток данных, кото- рый через определенные промежутки продуцируется датчиком, или, еще проще, поток сообщений социальной сети Twitter. Как правило, основными источниками потоков данных являются: датчики окружающей среды, измеряющие температуру, давление и влаж - ность; сле дящие датчики GPS, записывающие географическое положение (широ- ту-долготу); спутники, записывающие данные съемки поверхности Земли; данные видеонаблю дения и звукозаписи; интернет -трафик, или информационный обмен в Интернете. Однако вам нечасто придется обрабатывать реальные потоки данных, за ис - ключением разве что оставленных ими статических записей, которые хранятся в репозитории или файле. В таких случаях поток может быть воссоздан соглас - но определенным критериям, например путем извлечения каждый раз последо-вательно или случайным образом по одной записи. Если, например, данные со-держатся в текстовом файле либо файле данных с разделением полей запятыми (CSV), то нам остается лишь поставлять по одной строке файла и передавать стро-ку на вход обучающегося алгоритма. Что касается примеров в настоящей и следующей главах, то мы будем обраба- тывать файлы, хранящиеся на локальном жестком диске, и подготовим исходный код на Python, необходимый для их извлечения и потоковой передачи. Мы не бу - дем пользоваться миниатюрным набором данных, но и не перегрузим локальный жесткий диск слишком большим объемом данных для проведения проверок и де-монстраций. Наборы данных для реальных дел С 1987 г. в Калифорнийском университет е в Ирвайне (UCI), США, размещен ре- позиторий машинного обу чения UCI Machine Learning Repository. Это большое хранилище наборов данных для эмпирического тестирования алгоритмов ма-шинного обучения сообществом, занимающимся исследованием и разработкой\n--- Страница 53 ---\n52  Масштабиру емое обучение в Scikit-learn в области обучения машин. Во время написания настоящей книги этот репози- торий содержал порядка 350 наборов данных из совершенно разных предметных областей и для совершенно разных целей от задач регрессии и классификации, т. е. методов обучения с учителем, до задач без учителя. Все имеющиеся наборы данных можно просмотреть на https://archive.ics.uci.edu/ml/. С нашей стороны мы отобрали несколько наборов данных, которые на протяже- нии всей книги окажутся полезными в качестве исходного материала с большим числом строк или столбцов для решения сложных задач на основе необычного, по нашим временам, но все еще приемлемого компьютера с 2 Гб RAM: Название набора данныхURL-адрес набора данных Тип задачиСтроки и столбцы Набор данных Bike Sharinghttps://archive.ics.uci.edu/ml/datasets/Bike+Sharing+DatasetРегрессия 17389, 16 Набор данных BlogFeedbackhttps://archive.ics.uci.edu/ml/datasets/BlogFeedback Регрессия 60021, 281 Набор данных Buzz in social meadiahttps://archive.ics.uci.edu/ml/datasets/Buzz+in+social+media+Регрессия и классификация140000, 77 Набор данных Census Income (KDD)https://archive.ics.uci.edu/ml/datasets/Census-Income + % 28KDD%29Классификация с пропущенными данными299285, 40 Набор данных Covertypehttps://archive.ics.uci.edu/ml/datasets/Covertype Классификация 581012, 54 Данные Cup 1999 https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1999+DataКлассификация 4000000, 42 Чтобы скачать и использовать набор данных из репозитория UCI, необходимо попасть на страницу, посвященную набору данных, и перейти по ссылке под за- головком Download:Data Folder (Скачать:Папка данных). Мы приготовили не- сколько сценариев для автоматического скачивания данных, которые будут по-мещаться именно в тот каталог, в котором вы работаете на Python, тем самым упрощая доступ к данным. Ниже приведено несколько подготовленных нами функций, к которым мы бу - дем обращаться регулярно на протяжении всех глав, когда возникнет потребность скачать любой набор данных из UCI: In: import urllib2 import requests, io, os, StringIOimport numpy as npimport tarfile, zipfile, gzip def unzip_from_UCI(UCI_url, dest=''): \"\"\" Скачивает и распаковывает наборы данных из UCI в формате zip \"\"\" response = requests.get(UCI_url) compressed_file = io.BytesIO(response.content) z = zipfile.ZipFile(compressed_file) print(u'Извлечение в {0}'.format(os.getcwd().decode('cp1251') +'\\\\'+dest)) for name in z.namelist():\n--- Страница 54 ---\nПотоковая передача данных из источников  53 if '.csv' in name: print(u'\\tраспакован {0}'.format(name)) z.extract(name, path=os.getcwd().decode('cp1251')+'\\\\'+dest) def gzip_from_UCI(UCI_url, dest=''): \"\"\" Скачивает и распаковывает наборы данных из UCI в формате gzip \"\"\" response = urllib2.urlopen(UCI_url) compressed_file = io.BytesIO(response.read()) decompressed_file = gzip.GzipFile(fileobj=compressed_file) filename = UCI_url.split('/')[-1][:-3] with open(os.getcwd().decode('cp1251')+'\\\\'+filename, 'wb') as outfile: outfile.write(decompressed_file.read()) print(u'Файл {0} разархивирован'.format(filename)) def targzip_from_UCI(UCI_url, dest='.'): \"\"\" Скачивает и распаковывает наборы данных из UCI в формате tar.gz \"\"\" response = urllib2.urlopen(UCI_url) compressed_file = StringIO.StringIO(response.read()) tar = tarfile.open(mode=\"r:gz\", fileobj = compressed_file) tar.extractall(path=dest) datasets = tar.getnames() for dataset in datasets: size = os.path.getsize(dest+'\\\\'+dataset) print(u'Файл {0} составляет {1} байт'.format(dataset,size)) tar.close() def load_matrix(UCI_url): \"\"\" Скачивает наборы данных из UCI в матричной форме \"\"\" return np.loadtxt(urllib2.urlopen(UCI_url))  Скачивание исх одного кода примеров Подробные шаги по скачиванию комплекта с исходным кодом примеров упомянуты в пре-дисловии настоящей книги. Пожалуйста, ознакомьтесь с ними.Комплект исходного кода для настоящей книги также размещен на веб-сайте GitHub на https://github.com/PacktPublishing/Large-Scale-Machine-Learning-With-Python. Мы также располагаем другими комплектами исходного кода примеров, которые можно выбрать из нашего обширного каталога книг и видео, предлагаемого на странице https://github.com/ PacktPublishing/. Можете убедиться сами! Эти функции представляют собой вспомогательные обертки вокруг разных биб лиот ек, в частности tarfile , zipfile и gzip, предназначенных для работы со сжа- тыми данными. Файл открывается при помощью модуля urllib2 , который гене- рирует дескриптор для удаленной системы и предусматривает последовательную передачу данных с их сохранением в памяти в виде последовательности симво-лов либо в двоичном режиме, используя для этого соответственно классы StringIO и BytesIO из модуля io – этот модуль посвящен управлению потоками (https://docs. python.org/2/library/io.html). После помещения файла в оперативную память он\n--- Страница 55 ---\n54  Масштабиру емое обучение в Scikit-learn восстанавливается подобно обычному файлу, находящемуся на диске, благодаря функциям, специально предназначенным для распаковки сжатых файлов. Четыре предоставленные выше функции должны помочь быстро скачать набо- ры данных, независимо от того, в каком формате они упакованы: zip, tar, gzip или же просто как текст в матричной форме, устраняя хлопоты, связанные с ручным скачиванием и операциями по извлечению из архива. Первый пример – потоковая передача набора данных Bike-sharing В качестве первого примера мы обратимся к набору данных Bike-sharing об арен-де велосипедов. Этот набор данных содержит два CSV-файла с данными о поча-совом и ежедневном количестве велосипедов, арендованных между 2011 и 2012 г. в системе Capital Bikeshare, расположенной в г. Вашингтон, округ Колумбия, США (https://www.capitalbikeshare.com/). Данные характеризуются соответствующей по-годной и сезонной информацией относительно дня сдачи в аренду. Набор данных связан со следующей публикацией: Fanaee-T, Hadi, and Gama, Joao, Event labeling combining ensemble detectors and background knowledge, Progress in Artificial Intelligence (2013): pp. 1–15, Springer Berlin Heidelberg (« Маркировка событий с совмещением ан- самблевых детекторов и фонового знания»). Первая задача состоит в том, чтобы сохранить набор данных на локальном жестком диске при помощи вспомогательных оберточных функций, которые были определены всего несколько абзацев ранее: In: UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip'unzip_from_UCI(UCI_url, dest='data\\\\bikesharing') Out: Извлечение в C:\\scisoft\\WinPython-64bit-2.7.9.4\\notebooks\\data\\bikesharing распакован day.csv распакован hour.csv Если все выполнено успешно, то программный код укажет, в каком каталоге были сохранены CSV-файлы, и распечатает имена обоих разархивированных файлов. Сохранив информацию на физическом устройстве, в этом месте мы напишем сценарий, составляющий сердцевину нашей системы внеядерного обучения, ко-торое обеспечит потоковую передачу данных из файла. Сначала мы воспользуем-ся библиотекой csv, предлагающей двойной выбор: восстановить данные в виде списка либо словаря Python. Начнем со списка: In: import os, csv local_path = os.getcwd() source = 'data\\\\bikesharing\\\\hour.csv'SEP = str(',') # определяется с целью его замены в дальнейшем # для файла с другим разделителем with open(local_path.decode('cp1251')+'\\\\'+source, 'rb') as R:\n--- Страница 56 ---\nПотоковая передача данных из источников  55 iterator = csv.reader(R, delimiter=SEP) for n, row in enumerate(iterator): if n==0: header = row else: # заглушка для ОБРАБОТКИ ДАННЫХ # заглушка для МАШИННОГО ОБУЧЕНИЯ pass print('Всего строк: {0}'.format(n+1)) print('Заголовок: {0}'.format(', '.join(header))) print('Примеры значений: {0}'.format(', '.join(row))) Out: Всего строк: 17380Заголовок: instant, dteday, season, yr, mnth, hr, holiday, weekday, workingday, weathersit, temp, atemp, hum, windspeed, casual, registered, cntПримеры значений: 17379, 2012-12-31, 1, 1, 12, 23, 0, 1, 1, 1, 0.26, 0.2727, 0.65, 0.1343, 12, 37, 49 Полученный результат сообщит, сколько было прочитано строк, содержание за- головка – первая строка CSV-файла (сохраненного в списке) – и содержимое строк (для удобства мы распечатали последнюю строку). Читающий объект csv.reader создает итератор, который благодаря циклу for будет раз за разом выдавать по одной строке файла. Отметим, что внутрь фрагмента кода мы поместили два ком-ментария, тем самым указав, куда на протяжении этой главы мы будем помещать другой исходный код для предобработки данных и машинного обучения. В данном случае признаки должны обрабатываться с использованием пози- ционного подхода, который применяет индексную позицию метки в заголовке. Это может вызывать небольшое неудобство, в случае если вам придется актив-но управлять признаками. Это неудобство преодолевается использованием чи - тающего объекта csv.DictReader , который на выходе производит словарь Python (словарь не упорядочен, но признаки могут быть легко восстановлены по их меткам): In: with open(local_path.decode('cp1251')+'\\\\'+source, 'rb') as R: iterator = csv.DictReader(R, delimiter=SEP) for n, row in enumerate(iterator): # заглушка для ОБРАБОТКИ ДАННЫХ # заглушка для МАШИННОГО ОБУЧЕНИЯ pass print('Всего строк: {0}'.format(n+1)) print('Примеры значений: {0}'.format(row)) Out: Всего строк: 17379Примеры значений: {'mnth': '12', 'cnt': '49', 'holiday': '0', 'instant': '17379', 'temp': '0.26', 'dteday': '2012-12-31', 'hr': '23', 'season': '1', 'registered': '37', 'windspeed': '0.1343', 'atemp': '0.2727', 'workingday': '1', 'weathersit': '1', 'weekday': '1', 'hum': '0.65', 'yr': '1', 'casual': '12'}\n--- Страница 57 ---\n56  Масштабиру емое обучение в Scikit-learn Использование инструментов ввода-вывода библиотеки pandas В качестве альтернативы модулю csv можно воспользоваться функцией библио - теки pandas read_csv . Данная функция, как указано в документации к pandas на http://pandas.pydata.org/pandas-docs/stable/io.html, специально предназначена для закачивания CSV-файлов и является составной частью целого ряда функций, по-священных операциям ввода-вывода на различных файловых форматах. Большие преимущества использования функций ввода-вывода библиотеки pandas заключаются в следующем: можно обеспечить совместимость исходного кода при изменении типа ис - точника, т. е. требуется всего лишь переопределить потоковый итератор; можно получить доступ к большому числу разных форматов, таких как CSV, неформатированный текст, HDF, JSON и SQL-запрос к конкретной базе данных; данные пос тупают потоком в порции требуемого размера, реализованные как структуры данных DataFrame, в результате чего можно обращаться к приз на- кам позиционным способом либо путем восстановления их метки благо- даря методам loc, iloc, ix, которые, как правило, используются для взятия срезов и подмножеств в таблице данных DataFrame. Приведем пример с использованием того же самого подхода, что и ранее, но на этот раз построенный вокруг функции pandas read_csv : In: import pandas as pd CHUNK_SIZE = 1000with open(local_path.decode('cp1251')+'\\\\'+source, 'rb') as R: iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) for n, data_chunk in enumerate(iterator): print('Размер закачанной порции данных: %i прецедентов, %i признаков' % (data_chunk.shape)) # заглушка для ОБРАБОТКИ ДАННЫХ # заглушка для МАШИННОГО ОБУЧЕНИЯ pass print('Примеры значений: \\n{0}'.format(str(data_chunk.iloc[0])))Out: Размер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаков\n--- Страница 58 ---\nПотоковая передача данных из источников  57 Размер закачанной порции данных: 1000 прецедентов, 17 признаков Размер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 1000 прецедентов, 17 признаковРазмер закачанной порции данных: 379 прецедентов, 17 признаковПримеры значений: instant 17001dteday 2012-12-16season 4yr 1mnth 12hr 3holiday 0weekday 0workingday 0weathersit 2temp 0.34atemp 0.3333hum 0.87windspeed 0.194casual 1registered 37cnt 38Name: 17000, dtype: object Здесь очень важно отметить, что итератор инстанцирован с указанием размера порции данных, т. е. числа строк, которые итератор должен возвращать во время каждой итерации. Параметр размера порции chunksize может принимать значе- ния от 1 до любого значения, хотя ясно, что размер мини-пакета данных (извле-ченной порции данных) строго привязан к размеру доступной памяти, в которой он будет храниться и обрабатываться в следующей фазе предобработки. Передача в оперативную память более крупных порций дает преимущество только с точки зрения доступа к диску. Порции меньших размеров требуют мно-гократного доступа к диску и, в зависимости от характеристик физического хра-нилища данных, более продолжительного времени прохождения данных. Тем не менее, с точки зрения машинного обучения, порции меньшего или бóльшего раз-меров имеют для внеядерных функций библиотеки Scikit-learn незначительную разницу, так как они обучаются, учитывая всякий раз всего один прецедент, что делает их по-настоящему линейными в части вычислительных затрат. Работа с базами данных В качестве примера гибкости инструментов ввода-вывода библиотеки pandas мы покажем еще один пример, теперь с использованием СУБД SQLite3 и потоковой передачей данных из простого запроса поочередными порциями. Этот пример предлагается исключительно для дидактического применения. Работа с большим хранилищем в базах данных может действительно принести преимущества с точ-ки зрения времени обработки и дискового пространства. Данные, сгруппированные в таблицы базы данных SQL, могут быть нормали- зованы, тем самым удаляя избыточность и повторения и экономя дисковое про-странство. Нормализация базы данных позволяет расположить столбцы и табли-\n--- Страница 59 ---\n58  Масштабиру емое обучение в Scikit-learn цы в базе данных таким образом, чтобы уменьшить их размерности без потери информации. Нередко это осуществляется путем разбивки таблиц и перекодиро-вания повторяющихся данных в ключи. Кроме того, работу оптимизированной с точки зрения памяти, операций и многопроцессорной обработки реляционной базы данных можно ускорить и предвосхитить часть тех предварительных работ, которые выполняются в сценариях Python. В среде программирования на языке Python СУБД SQLite (http://www.sqlite.org) поставляется по умолчанию и является хорошим выбором благодаря следующим причинам: имеет о ткрытый исходный код; мож ет обрабатывать большие объемы данных (теоретически до 140 Tб в рас - чете на каждую базу данных, хотя вряд ли мы увидим приложение SQLite, которое будет иметь дело с такими объемами данных); работает в Mac OS и одновременно в 32- и 64-разрядных средах под управ- лением Linux и Windows; не треб ует какой-то серверной инфраструктуры или отдельной инсталля - ции (ну левой конфигурации), поскольку все данные хранятся в одном- един ственном дисковом файле; мож ет быть легко расширена при помощи исходного кода Python, который становится хранимой процедурой. Кроме того, стандартная библиотека Python содержит модуль sqlite3 , обеспе- чивающий все функции для создания базы данных с нуля и дальнейшей работы с ней. В нашем примере мы сначала закачаем в базу данных SQLite наш CSV-файл с набором данных Bike-sharing об аренде велосипедов на ежедневной и почасо-вой основе и затем организуем из нее поточную передачу, точно так же, как мы делали это из CSV-файла. Предоставленный исходный код закачки базы данных будет использоваться повторно на протяжении всей книги и допускает повтор-ное использование в ваших собственных приложениях, поскольку он не привязан к какому-то определенному примеру, который мы предлагаем (просто нужно по-менять параметры ввода и вывода, и это все): In: import os, sysimport sqlite3, csv,glob SEP = str(',')def define_field(s): try: int(s) return 'integer' except ValueError: try: float(s) return 'real' except: return 'text' def create_sqlite_db(db='data\\\\database.sqlite', file_pattern=''):\n--- Страница 60 ---\nПотоковая передача данных из источников  59 conn = sqlite3.connect(db) conn.text_factory = str # позволяет хранить данные в кодировке utf-8 c = conn.cursor() # обойти каталог и обработать все файлы .csv, # которые подходят для построения БД target_files = glob.glob(file_pattern) print('Создание {0} таблиц(ы) в {1} из файла(ов): {2}'.format(len(target_files), db, ', '.join(target_files))) for k,csvfile in enumerate(target_files): # удалить путь и расширение и использовать все остальное, # как имя таблицы tablename = os.path.splitext(os.path.basename(csvfile))[0] with open(csvfile, \"rb\") as f: reader = csv.reader(f, delimiter=SEP) f.seek(0) for n,row in enumerate(reader): if n==11: types = map(define_field,row) else: if n>11: break f.seek(0) for n,row in enumerate(reader): if n==0: sql = \"DROP TABLE IF EXISTS %s\" % tablename c.execute(sql) sql = \"CREATE TABLE %s (%s)\" % (tablename, \", \".join([\"%s %s\" % (col, ct) for col, ct in zip(row, types)])) print('{0}) {1}'.format(k+1,sql)) c.execute(sql) # создание индексов для ускоренных соединений # на длинных строках for column in row: if column.endswith(\"_ID_hash\"): index = \"%s__%s\" % ( tablename, column ) sql = \"CREATE INDEX %s on %s (%s)\" % ( index, tablename, column ) c.execute(sql) insertsql = \"INSERT INTO %s VALUES (%s)\" % (tablename, \", \".join([\"?\" for column in row])) rowlen = len(row) else: # поднять ошибку, если есть строки # с неправильным числом полей if len(row) == rowlen: c.execute(insertsql, row)\n--- Страница 61 ---\n60  Масштабиру емое обучение в Scikit-learn else: print('Ошибка в строке {0} в файле {1}' .format(n,csvfile)) raise ValueError( 'Хьюстон, у нас проблемы в строке {0}'.format(n)) conn.commit() print('* Вставлено {0} строк'.format(n)) c.close() conn.close() Следующий ниже сценарий передает допустимое имя базы данных и шаблон для определения файлов, которые вы хотите импортировать (подстановочные знаки, такие как *, допускаются), и с нуля создает необходимую новую базу дан- ных и таблицы, впоследствии заполняя их всеми имеющимися данными: In:create_sqlite_db(db='data\\\\bikesharing.sqlite', file_pattern='data\\\\bikesharing\\\\*.csv') Out: Создание 2 таблиц(ы) в data\\bikesharing.sqlite из файла(ов): data\\bikesharing\\day.csv, data\\ bikesharing\\hour.csv1) CREATE TABLE day (instant integer, dteday text, season integer, yr integer, mnth integer, holiday integer, weekday integer, workingday integer, weathersit integer, temp real, atemp real, hum real, windspeed real, casual integer, registered integer, cnt integer)* Вставлено 731 строк2) CREATE TABLE hour (instant integer, dteday text, season integer, yr integer, mnth integer, hr integer, holiday integer, weekday integer, workingday integer, weathersit integer, temp real, atemp real, hum real, windspeed real, casual integer, registered integer, cnt integer)* Вставлено 17379 строк Следующий ниже сценарий сообщает о типах данных для создаваемых полей и количестве строк, поэтому довольно просто удостовериться, что во время им-порта все прошло гладко. Теперь из базы данных можно легко организовать пото-ковую передачу. В нашем примере мы создадим внутреннее объединение между таблицами с почасовыми и ежедневными данными и извлечем данные на поча-совой основе с информацией об арендных платежах по дням: In:import os, sqlite3import pandas as pd DIR_PATH = os.getcwd() DB_NAME = 'data\\\\bikesharing.sqlite'CHUNK_SIZE = 2500 conn = sqlite3.connect(DB_NAME) conn.text_factory = str # позволяет хранить данные в кодировке utf-8 sql = \"SELECT H.*, D.cnt AS day_cnt FROM hour AS H INNER JOIN day as D ON (H.dteday = D.dteday)\"DB_stream = pd.io.sql.read_sql(sql, conn, chunksize=CHUNK_SIZE) for j,data_chunk in enumerate(DB_stream): x, y = data_chunk.shape\n--- Страница 62 ---\nПотоковая передача данных из источников  61 print('Порция {0} -'.format(j+1)), print('Размер закачанной порции данных: {0} прецедентов, {1} признаков' .format(x,y)) # заглушка для ОБРАБОТКИ ДАННЫХ # заглушка для МАШИННОГО ОБУЧЕНИЯ Out: Порция 1 - Размер закачанной порции данных: 2500 прецедентов, 18 признаков Порция 2 - Размер закачанной порции данных: 2500 прецедентов, 18 признаковПорция 3 - Размер закачанной порции данных: 2500 прецедентов, 18 признаковПорция 4 - Размер закачанной порции данных: 2500 прецедентов, 18 признаковПорция 5 - Размер закачанной порции данных: 2500 прецедентов, 18 признаковПорция 6 - Размер закачанной порции данных: 2500 прецедентов, 18 признаковПорция 7 - Размер закачанной порции данных: 2379 прецедентов, 18 признаков Если потоковую передачу нужно ускорить, то необходимо оптимизировать базу данных, прежде всего создав индексы для реляционного запроса, который вы на-мереваетесь использовать.  Строка conn.text_factory = str составляет очень важную часть сценария; позволяет хранить данные в кодировке UTF-8. Если эту команду проигнорировать, то можно столкнуться с не-привычными ошибками при вводе данных. Особое внимание упорядочению прецедентов В качестве заключительного комментария по теме потоковой передачи данных мы должны предупредить, что во время потоковой передачи в процесс обучения в действительности вносится скрытая информация в силу порядка следования примеров, на которых базируется процесс машинного обучения. По сути дела, онлайновые ученики оптимизируют свои параметры на основе каждого прецедента, который они оценивают. В процессе оптимизации каждый прецедент направляет ученика в определенном направлении. В глобальном пла-не ученик должен взять правильное направление оптимизации, если дано доста-точно большое число оцененных прецедентов. Однако если ученик вместо этого тренируется на смещенных наблюдениях (например, упорядоченных по времени либо сгруппированных осмысленным образом), то алгоритм тоже будет обучаться смещению. Во время обучения что-то еще можно сделать для того, чтобы не запо-минать ранее наблюдавшихся прецедентов, но так или иначе некоторое смеще-ние все равно будет допущено. Если в процессе обучения извлекается временной ряд – отклик на течение времени часто является частью модели, – такое смещение оказывается довольно полезным, но в большинстве других случаев оно действует как некая переподгонка, или излишняя аппроксимация, и приобретает форму де-фицита обобщающей способности в итоговой модели. Если в данных имеется некая упорядоченность (как, например, порядок следо- вания идентификаторов) и вы не хотите, чтобы алгоритм машинного обучения ей обучился, то, прежде чем начать потоковую передачу, в качестве предупредитель-ной меры можно перемешать строки данных и получить произвольный порядок следования, более подходящий для онлайнового стохастического обучения. Самый быстрый способ, который к тому же занимает меньше места на диске, состоит в том, чтобы передать набор данных потоком непосредственно в опе-ративную память и уменьшить его путем сжатия. В большинстве случаев, но не\n--- Страница 63 ---\n62  Масштабиру емое обучение в Scikit-learn во всех, это будет работать благодаря применяемому алгоритму сжатия и отно- сительной разреженности и избыточности данных, которые вы используете для тренировки. В случаях, когда это не работает, необходимо перемешать данные непосредственно на диске, что влечет за собой большее потребление дискового пространства. Ниже мы сначала представим быстрый способ перемешивания данных в опе- ративной памяти благодаря библиотеке zlib, которая может быстро сжимать стро- ки в оперативной памяти, и функции для перемешивания shuffle из стандартного модуля random : In: import zlibfrom random import shuffle def ram_shuffle(filename_in, filename_out, header=True): with open(filename_in, 'rb') as f: zlines = [zlib.compress(line, 9) for line in f] if header: first_row = zlines.pop(0) shuffle(zlines) with open(filename_out, 'wb') as f: if header: f.write(zlib.decompress(first_row)) for zline in zlines: f.write(zlib.decompress(zline)) import oslocal_path = os.getcwd().decode('cp1251') source = 'data\\\\bikesharing\\\\hour.csv' ram_shuffle(filename_in=local_path+'\\\\'+source, filename_out=local_path+'\\\\data\\\\bikesharing\\\\shuffled_hour.csv', header=True)  Для пользователей UNIX одиночный вызов команды sort (с параметром -R) очень легко перемешивает огромные количества текстовых файлов, делая это намного эффективнее, чем любая реализация на Python. Ее можно комбинировать с шагами распаковки и сжатия, используя для этого конвейерные операторы.Так, следующая ниже команда должна добиться поставленной цели: zcat sorted.gz | sort -R | gzip - > shuffled.gz В случае если объема RAM для хранения всех сжатых данных не хватает, то единственное эффективное решение состоит в том, чтобы выполнить операции непосредственно на самом файле в том виде, в котором он размещен на дис - ке. Следующий фрагмент исходного кода определяет функцию, которая будет систематически разбивать файл на меньшие по размеру файлы, перемешивать их содержимое и размещать их снова в случайном порядке в большем по раз-меру файле. В результате получится не совсем полная случайная перестановка, но строки файла разбросаны в достаточной мере, чтобы уничтожить любой пре-дыдущий порядок следования строк, который мог бы повлиять на онлайновое обучение:\n--- Страница 64 ---\nСтохастическое обучение  63 In: import osfrom random import shuffleimport numpy as npimport pandas as pd def disk_shuffle(filename_in, filename_out, header=True, iterations = 3, CHUNK_SIZE = 2500, SEP=str(',')): for i in range(iterations): with open(filename_in, 'rb') as R: iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) for n, df in enumerate(iterator): if n==0 and header: header_cols =SEP.join(df.columns)+'\\n' df.iloc[np.random.permutation(len(df)) ].to_csv(str(n)+'_chunk.csv', index=False, header=False, sep=SEP) ordering = list(range(0,n+1)) shuffle(ordering) with open(filename_out, 'wb') as W: if header: W.write(header_cols) for f in ordering: with open(str(f)+'_chunk.csv', 'r') as R: for line in R: W.write(line) os.remove(str(f)+'_chunk.csv') filename_in = filename_out CHUNK_SIZE = int(CHUNK_SIZE / 2) import oslocal_path = os.getcwd().decode('cp1251') source = 'data\\\\bikesharing\\\\hour.csv' disk_shuffle(filename_in=local_path+'\\\\'+source, filename_out=local_path+'\\\\data\\\\bikesharing\\\\shuffled_hour.csv', header=True) стОхастическ Ое Обучение Разработав процесс потоковой передачи данных, теперь пора взглянуть на про-цесс обучения, так как именно обучение и его специфические потребности опре-деляют выбор наилучшего способа управления данными и их преобразования на этапе предобработки. Онлайновое обучение, в противоположность пакетному, работает с бóльшим числом итераций и получает направление движения поочередно от каждого от - дельного прецедента, тем самым обеспечивая более блуждающую процедуру обуче ния, чем оптимизация, выполняемая на пакете данных, которая сразу же стремится указать правильное направление, исходя из всех данных целиком.\n--- Страница 65 ---\n64  Масштабируемое об учение в Scikit-learn Пакетный градиентный спуск В машинном обучении алгоритм градиентного спуска является стержневым, и по- этому он пересмотрен, чтобы его адаптировать под онлайновое обучение. Во время обработки пакетных данных градиентный спуск может минимизировать функцию стоимости линейного регрессионного анализа, используя намного меньше вычис - лений, чем статистические алгоритмы. Сложность вычисления градиентного спус - ка занимает порядка O(n*p), делая возможным обучение коэффициентам регрес - сии даже в случае больших значений n (которое обозначает число наблюдений) и p (число переменных). Он работает также идеально, когда в тренировочных данных присутствуют высококоррелированные или даже идентичные признаки. Все основывается на простом методе оптимизации: набор параметров изменя- ется в результате многочисленных итераций таким образом, что он постепенно сходится к оптимальному решению, начиная со случайного. Градиентный спуск – это теоретически хорошо проработанный метод оптимизации с известными га-рантиями сходимости для определенных задач, в частности регрессионных. Тем не менее давайте начнем со следующего ниже рисунка, на котором показано сложное соответствие (характерное для нейронных сетей) между значениями, которые параметры могут принять (представляя пространство гипотезы), и ре-зультатом с точки зрения минимизации функции стоимости: Отклик от функции стоимости1.0 0.80.60.40.20.0 –1.0 –0.5 0.0 0.5 1.0 Параметры Локальный минимум Глобальный минимумЛокальный минимум Используя образный пример, градиентный спуск напоминает прогулку по хол- мам с закрытыми глазами. Если вы хотите спуститься в самую низкую долину, не видя дороги, то можно продолжить движение, направляясь исключительно по пути, который, похоже, ведет вниз; надо чуть-чуть по нему пройти, затем остано-виться, снова нащупать местность и затем продолжить путь в том направлении, которое, похоже, ведет вниз, и т. д., снова и снова. Если продолжить двигаться туда, где поверхность уходит вниз, то вы, наконец, прибудете в точку, откуда боль-ше не сможете спуститься, потому что местность там будет плоской. Будем наде-яться, что в той точке вы достигнете места назначения. Используя этот подход, нам нужно выполнить следующие действия: принять решение об исходной точке. Обычно это достигается за счет перво- начального случайного угадывания параметров функции (многократные\n--- Страница 66 ---\nСтохастическое обучение  65 перезапуски обеспечат, что инициализация параметров не вынудит алго- ритм достичь локального оптимума из-за неудачных начальных условий); уметь нащупывать местность, т. е. быть в состоянии различать, когда она уходит вниз. С математической точки зрения это означает, что вы должны суметь взять производную фактической параметризованной функции от - носительно целевой переменной, т. е. частную производную оптимизируе-мой функции стоимости. Отметим, что обычный градиентный спуск рабо-тает на всех данных, пытаясь оптимизировать предсказания, исходя из всех прецедентов одновременно; решить, как долго следует двигаться по направлению, продиктованному про - изводной. С математической точки зрения это соответствует весу (обычно обозначаемому греческой буквой α), решающему, на какую величину надо изменять параметры на каждом шаге оптимизации. Этот аспект можно рас - сматривать как фактор обучения, потому что он показывает величину обуче-ния на данных на каждом шаге оптимизации. Как и в случае с любым другим гиперпараметром, оптимальное значение α можно определить оценкой ре- зультативности на перекрестно-проверочном наборе; определить, когда следует остановиться, если имеется слишком незначитель-ное улучшение функции стоимости относительно предыдущего шага. В этом смысле вы также должны суметь заметить, когда что-то пойдет не так, и вы движетесь в неправильном направлении, возможно, потому, что используете слишком большое значение α для обучения. Это в действительности связано с инерцией, т. е. скоростью, с которой алгоритм сходится к оптимуму. Это все равно, что бросить шар вниз по склону холма: он попросту будет перекаты-ваться через маленькие вмятины на поверхности, однако если его скорость слишком высока, то в нужной точке он не остановится. Таким образом, если значение α будет установлено неправильно, то шар просто перепрыгнет че- рез глобальный оптимум и продолжит сообщать об ошибках, которые подле-жат минимизации, как показано на следующем ниже изображении на левой панели, когда процесс оптимизации заставит параметры перепрыгивать че-рез разные значения, не достигая требуемой минимизации ошибки. Однако если значение α установлено правильно, то инерция естественным образом замедлится по мере приближения алгоритма к оптимуму, как показано на следующем ниже изображении в правой панели: 1.0 0.80.60.40.20.01.00.80.60.40.20.0Отклик от функции стоимости Отклик от функции стоимости –1.0 –1.0 –0.5 –0.5 0.0 0.0 0.5 0.5 1.0 1.0 Параметры Параметры\n--- Страница 67 ---\n66  Масштабиру емое обучение в Scikit-learn Чтобы точнее показать, что происходит с градиентным спуском, возьмем при- мер из линейной регрессии, чьи параметры оптимизируются благодаря такого рода процедуре. Мы начинаем с функции стоимости J на весовом векторе w: Матрично-векторное произведение Xw матрицы тренировочных данных X на вектор весовых коэффициентов w представляет собой предсказания из линейной модели, отклонения которых от отклика y возводятся в квадрат, затем суммиру - ются и, наконец, делятся на число прецедентов n, умноженное на 2. Первоначально вектор w может быть инстанцирован случайными числами, взятыми из стандартизированного нормального распределения с нулевым сред- ним значением и единичным стандартным отклонением. (На самом деле инициа - лизация мо жет быть выполнена большим количеством различных способов, при этом все работают одинаково хорошо, аппроксимируя линейную регрессию, чья функция стоимости в форме чаши имеет уникальный минимум.) Это позволяет запускать наш алгоритм где-нибудь вдоль траектории оптимизации и эффектив-ным образом ускоряет сходимость процесса. При оптимизации линейной регрес - сии инициализация не причиняет алгоритму каких-то неприятностей (в худшем случае неправильный старт попросту его замедлит). Напротив, когда мы исполь-зуем градиентный спуск для оптимизации других алгоритмов машинного обуче-ния, таких как нейронные сети, мы рискуем застрять из-за неправильной инициа - лизации. Это произойдет, если, например, начальный вектор w будет заполнен нулевыми значениями (риск застрять на совершенно симметричной вершине, где никакое направление не может сразу же повести оптимизацию по лучшему пути, чем какое-либо другое). Это также может произойти с процессами оптимизации, которые имеют многочисленные локальные минимумы. При наличии исходного случайного вектора коэффициентов w можно непо- средственно вычислить функцию стоимости J(w) и определить начальное направ- ление для каждого отдельного коэффициента, вычтя из каждого долю α (темпа обучения) частной производной функции стоимости, как эксплицируется в сле-дующей формуле: Ее смысл можно лучше понять, если решить частную производную следующим образом: Стоит отметить, что обновление выполняется на каждом отдельном коэффи- циенте (wj) при наличии его вектора признаков xj, но на основе всех предсказаний сразу (отсюда и суммирование). После итерации по всем коэффициентам в w обновление коэффициентов будет завершено, и процесс оптимизации может быть перезапущен снова, вычислени- ем частной производной и обновлением вектора w. Интересное свойство этого процесса заключается в том, что величина обнов- ления будет все меньше и меньше по мере приближения вектора w к оптималь-\n--- Страница 68 ---\nСтохастическое обучение  67 ной конфигурации. Поэтому процесс может остановиться, когда вызванное в w изменение относительно предыдущей операции будет небольшим. Так или ина- че, мы действительно имеем уменьшающуюся величину обновления, когда задан правильный размер темпа обучения α. По сути дела, если его значение слишком большое, то он может заставить процесс оптимизации отклониться и завершить-ся неудачей, приведя – в некоторых случаях – к полному расхождению процесса и невозможности сойтись к итоговому решению. В сущности, процесс оптимиза-ции продемонстрирует тенденцию промахиваться по цели и в действительности еще больше от него удалится. С другой стороны, слишком малое значение α не только будет двигать процесс оптимизации к своей цели слишком медленно, но, помимо этого, он легко может застрять где-нибудь в локальном минимуме. Это в особенности верно для более сложных алгоритмов, в частности которые используются в нейронных сетях. Что касается линейной регрессии и ее классификационного аналога, логистической регрессии, то поскольку в них кривая оптимизации имеет чашеобразную форму, подобную вогнутой кривой, она характеризуется единственным (глобальным) минимумом и полным отсутствием локальных минимумов. В проиллюстрированной нами реализации алгоритма темп обучения α – это константа (градиентный спуск с фиксированным темпом обучения). Поскольку темп обучения α играет в схождении к оптимальному решению особо важную роль, для него были разработаны различные стратегии, в которых он вначале по-лучает более крупное значение и сжимается по мере выполнения оптимизации. Мы обсудим такие подходы, когда займемся исследованием реализации алгорит - ма градиентного спуска в библиотеке Scikit-learn. Стохастический градиентный спуск Рассмотренная нами к настоящему моменту версия градиентного спуска называ-ется полным пакетным градиентным спуском. Она работает за счет оптимизации ошибки всего набора данных и тем самым нуждается в его наличии в оператив-ной памяти. Внеядерные версии называются стохастическим градиентным спуск ом и мини-пакетным градиентным спуском. Здесь формула остается точно такой же, за исключением обновления; за один раз обновляется всего один прецедент, тем самым позволяя оставлять базовые данные в своем хранилище и работать всего с одним наблюдением в оперативной памяти: w j = wj – α(xjw – y)xj. Центральная идея этого алгоритма состоит в том, что если прецеденты выби- раются случайным образом без особых смещений, то процесс оптимизации будет двигаться в среднем по направлению к целевой минимизации стоимости. Этим как раз и объясняется, почему мы обсудили вопрос, как из потока удалять любое упорядочение и генерировать его максимально случайным образом. В частности, если в примере с арендой велосипедов алгоритм стохастического градиентного спуска сначала обучается закономерностям начала сезона, потом сосредоточи-вается на лете, потом на осени и т. д., то в зависимости от сезона, когда процесс оптимизации будет остановлен, модель будет настроена на предсказании послед-него сезона лучше остальных, потому что большинство недавних примеров взя-\n--- Страница 69 ---\n68  Масштабиру емое обучение в Scikit-learn то из него. В оптимизации методом стохастического градиентного спуска, когда данные независимы и о динаково распреде лены (independent and identically distributed, i.i.d.), сходимость к глобальному минимуму гарантируется. В практи-ческом плане i.i.d. означает, что примеры не должны быть последовательно упо-рядочены или распределены, а предложены на вход алгоритма, будучи отобран-ными из данных случайным образом. Реализация алгоритма SGD в библиотеке Scikit-learn В библиотеке Scikit-learn можно найти большое количество онлайновых обучаю-щихся алгоритмов. Не все алгоритмы машинного обучения имеют свой онлайно-вый аналог, но этот список интересен и постоянно растет. Что касается обучения с учителем, то имеющихся учеников можно разделить на классификаторы и ре-грессоры. В качестве классификаторов можно упомянуть следующие 1: sklearn.naive_bayes.MultinomialNB ; sklearn.naive_bayes.BernoulliNB ; sklearn.linear_model.Perceptron ; sklearn.linear_model.PassiveAggressiveClassifier ; sklearn.linear_model.SGDClassifier . В качестве регрессоров имеются два варианта: sklearn.linear_model.PassiveAggressiveRegressor ; sklearn.linear_model.SGDRegressor . Они все могут обучаться инкрементно, обновляясь прецедент за прецедентом. Правда, из них только реализации SGDClassifier и SGDRegressor основаны на опи- санной ранее оптимизации методом стохастического градиентного спуска и яв-ляются основными темами этой главы. Ученики SGD оптимальны для всех круп-номасштабных задач, поскольку их вычислительная сложность связана с O(k*n*p), где k – это число прохождений по данным, n – число прецедентов и p – число признаков (естественно, ненулевых, если мы работаем с разреженными матрица-ми). Это полностью линейно-временные ученики, требующие большего времени в строгой пропорции с числом показанных примеров. Другие онлайновые алгоритмы будут использоваться для проведения срав- нительного тестирования. Более того, все алгоритмы пользуются одним общим программным интерфейсом, основанным на методе partial_fit для онлайнового и мини-пакетного обучения (в последнем случае при потоковой передаче боль-ших порций, а не одиночного прецедента). Предоставление общего API делает все эти методы обучения взаимозаменяемыми в рамках задачи машинного обучения. В противоположность методу подгонки fit, который использует все имеющие - ся данные для их непосредственной оптимизации, метод partial_fit управляет частичной оптимизацией, основываясь на каждом отдельно передаваемом ему прецеденте. Даже если на вход метода partial_fit будет передан набор данных, то алгоритм не будет обрабатывать весь пакет данных, а только по одному эле-менту, приводя к по-настоящему линейной вычислительной сложности операций 1 Полный перечень классификаторов и регрессоров можно найти на следующей страни- це библиотеки, посвященной методам обучения с учителем: http://scikit-learn.org/stable/ supervised_learning.html. – Прим. перев.\n--- Страница 70 ---\nСтохастическое обучение  69 обучения. Более того, после вызова метода partial_fit ученик может постоянно обновляться последующими вызовами partial_fit , что делает его идеальным для онлайнового обучения на непрерывных потоках данных. При выполнении задачи классификации единственная трудность состоит в том, что при первой инициализации необходимо знать, скольким классам мы собираемся обучиться и как они промаркированы. Это можно сделать при помо-щи параметра classes , указав список меток с числовыми значениями. При этом требуется предварительно их обследовать, просмотрев данные в потоке, чтобы записать метки задачи, и принять к сведению их распределения, в случае если они не сбалансированы – т. е. когда некий класс по численности слишком велик либо слишком мал относительно других (правда, реализация в Scikit-learn предлага-ет способ автоматической обработки такой ситуации). Если целевая переменная числовая, то по-прежнему полезно знать ее распределение, но это не является необходимым условием для успешной работы ученика. В библиотеке Scikit-learn есть две реализации алгоритма SGD – одна для задач классификации ( SGDClassifier ) и другая для регрессии ( SGDRegressor ). Классифика- ционная реализация может справляться с многоклассовыми задачами с исполь-зованием схемы «один против все х» (one-vs-all, OVA). Эта стратегия подразуме - вает, что если дано k классов, то строятся k моделей, одна для каждого класса против всех прецедентов из других классов, следовательно, создавая k бинарных классификаций. Это приводит к созданию k наборов коэффициентов и k векторов предсказаний и их вероятностей. В конце, основываясь на эмитированной веро-ятности каждого класса по сравнению с другим, распознанным считается класс с самой высокой вероятностью. Если нужно получить фактические вероятности для мультиномиального распределения, то можно нормализовать результаты, разделив на их сумму. (Это то, что происходит в нейронной сети в слое с функцией мягкого максимума, в чем мы убедимся в последующих главах.) Как классификационная, так и регрессионная реализации алгоритма SGD в биб - лиот еке Scikit-learn характерны разными функциями потерь (функция стоимо- сти – это ядро оптимизации на основе стохастического градиентного спуска). Для классификации данных, выраженной параметром loss, задающим функ - цию потерь, можно опираться на следующие функции: loss='log' : классическая логистическая регрессия; loss='hinge' : мягкий зазор, т. е. линейная машина опорных векторов; loss='modified_huber' : сглаженная кусочно-линейная функция потерь. Для задачи регрессии имеются три функции потерь: loss='squared_loss' : обычный мето д наименьших квадратов (обычный МНК, ordinary least squares, OLS) для линейной регрессии; loss='huber' : функция потерь Хьюбера для регрессии, устойчивой к вы - бросам; loss='epsilon_insensitive' : линейная машина опорных векторов для рег - рессии. Мы представим несколько примеров с использованием классических статисти- ческих функций потерь, т. е. логистической функции потерей и обычного МНК. Кусочно-линейная функция потерь и машины опорных вект оров (SVM) будут обсуждаться в следующей главе, так как необходима подробная вводная инфор-мация об их функционировании.\n--- Страница 71 ---\n70  Масштабиру емое обучение в Scikit-learn В качестве напоминания (чтобы не пришлось обращаться за консультацией к другой книге по машинному обучению), если задать функцию регрессии как h, а ее предсказания выразить как h(X), потому что X – это матрица признаков, тогда приемлемой формулой будет следующая: y ≈ h(X) = βX + β0. Следовательно, подлежащая минимизации функция стоимости по обычному методу МНК выглядит так: В логистической регрессии, в условиях преобразования бинарного исхода 0/1 в отношение шансов, где y – это вероятность положительного исхода, формула будет следующей: . Логистическая функция стоимости, следовательно, определяется следующим образом: . Определение параметров обучения алгоритма SGD Чтобы задать параметры алгоритма SGD в библиотеке Scikit-learn, как в задачах классификации, так и задачах регрессии (чтобы они были применимы одновре-менно для алгоритмов SGDClassifier и SGDRegressor ), необходимо выяснить, как вес - ти с ебя с несколькими важными параметрами, которые требуются для правиль- ного обучения, когда невозможно оценить все данные сразу. Первый параметр n_iter определяет число итераций по данным. Исходно его значение равно 5, однако опытным путем было показано, что он должен быть подстроен под ученика, с тем чтобы тот, при условии что другие парамет ры заданы по умолчанию, видел порядка 10^6 примеров; поэтому хорошим реше- нием будет его установка в n_iter = np.ceil (10 ** 6 / n), где n – это число преце- дентов. Стоит отметить, что n_iter работает только с наборами данных, распо- ложенными в оперативной памяти, и поэтому он действует только тогда, когда вы используете метод fit вместо partial_fit . В действительности метод partial_ fit выполнит повторный итеративный обход тех же данных, если их повторно «прокачать» в своей процедуре, при этом нужное число итераций повторных прокачек должно проверяться по ходу самой обучающей процедуры, так как оно зависит от типа данных. В следующей главе мы проиллюстрируем гипер-параметрическую оптимизацию и обсудим вопрос надлежащего числа прохож - дений по данным.  Во время выполнения мини-макетного обучения имеет смысл повторно перемешивать дан- ные пос ле каждого полного прохождения по всем данным. Параметр shuffle требуется на случай, если вы хотите перемешать свои дан- ные. Он касается мини-пакетов, находящихся в оперативной памяти, и не ак -\n--- Страница 72 ---\nСтохастическое обучение  71 туален для внеядерного упорядочения данных. Он также работает с методом partial_fit , но его эффект в таком случае сильно ограничен. Всегда устанавли- вайте его в True , за исключением тех случаев, когда данные передаются порция - ми; чт о касается перемешивания данных вне ядра, то поступайте так, как мы описали ранее. Параметр warm_start также работает с методом fit, потому что он помнит пре- дыдущие коэффициенты подгонки (но не темп обучения, в случае если он был ди- намически изменен). Если используется метод partial_fit , то алгоритм запомнит ранее усвоенные коэффициенты и состояние схемы темпа обучения. Параметр average инициирует вычислительный прием, который на определен- ном прецеденте запускает усреднение новых коэффициентов с предыдущими, позволяя быстрее достигать схождения. Его можно установить в True либо в цело- численное значение, тем самым отметив, с какого прецедента он начнет усред-нение. Наконец, но не в последнюю очередь, имеются параметр темпа обучения learning_rate и связанные с ним параметры eta0 и power_t . Параметр learning_rate означает, насколько каждый наблюдаемый прецедент влияет на процесс оптими-зации. При описании алгоритма SGD с теоретической точки зрения мы предста-вили темп обучения как константу, которая может реплицироваться путем уста-новки learning_rate='constant' . Однако есть и другие варианты, позволяющие η (греч. буква «эта», обозначаю- щая в Scikit-learn темп обучения, определяемый во время t) постепенно умень- шаться. В задаче классификации предлагается решение с темпом обучения learning_rate='optimal' , который выражается следующей ниже формулой: . Здесь t – это временной шаг, задаваемый числом прецедентов, умноженных на итерации, и t0 – выбираемое эвристическим способом значение на основании исследований Леона Боту (Léon Bottou), чья версия SVM на основе стохастических градиентных аппроксимаций в большой степени повлияла на реализацию алго- ритма SGD в библиотеке Scikit-learn (http://leon.bottou.org/projects/sgd). Явное пре-имущество такой стратегии обучения состоит в том, что обучение уменьшается вместе с ростом числа наблюдаемых примеров, не допуская внезапных возмуще-ний в процессе оптимизации, задаваемых необычными значениями. Разумеется, эта стратегия тоже предоставлена в готовом к использованию виде, имея в виду, что у вас не так уж много вариантов для работы с ней. В регрессии предлагаемое затухание обучения выражается следующей ниже формулой, что соответствует темпу обучения learning_rate='invscaling' : . Здесь eta0 и power_t – это гиперпараметры (они первоначально установлены в 0 и 0.5) и подлежат оптимизации оптимизационным поиском. Стоит отметить, что, используя темп обучения invscaling , алгоритм SGD начинает с более низкого, ме- нее оптимального темпа обучения, и он будет уменьшаться медленнее, при этом являясь чуть более адаптируемым во время обучения.\n--- Страница 73 ---\n72  Масштабиру емое обучение в Scikit-learn управление признаками на пОтОках данных Потоки данных ставят задачу, которую невозможно решить теми же средствами, когда работают с полным набором данных в оперативной памяти. Правильный и оптимальный подход к подаче данных во внеядерный обучающийся алгоритм SGD требует сначала обследовать данные (к примеру, взяв несколько пробных прецедентов из исходного файла) и узнать тип поступающих данных. Мы различаем следующие типы данных: ко личественные значения; катег ориальные значения, представленные целыми числами; нестр уктурированные категориальные значения, выраженные в текстовой форме. В случае количественных данных их можно сразу передать ученику SGD, за исключением того, что этот алгоритм довольно чувствителен к шкалированию признаков; иными словами, необходимо привести все количественные призна-ки к одному и тому же интервалу значений, иначе процесс обучения не сойдется легко и правильно. Возможные стратегии шкалирования конвертируют все зна-чения в интервалы [0,1], [–1,1] или стандартизируют переменную, центрируя ее среднее на нуле и конвертируя ее дисперсию в единицу. У нас нет каких-то особых предложений по поводу выбора стратегии шкалирования, но конвертирование в интервал [0,1] работает особенно хорошо, в случае если речь идет о разреженной матрице, и при этом большинство значений нулевые. Что касается обучения в оперативной памяти, то во время преобразования переменных на тренировочном наборе следует обращать внимание на исполь-зуемые значения (в основном нужно получить минимальные, максимальные, средние значения и стандартные отклонения каждого признака) и применить их повторно в тестовом наборе для достижения согласованных результатов. Учитывая, что данные передаются потоком и отсутствует возможность зака- чать все данные в оперативную память, необходимо их подсчитать, пройдя по всем данным либо как минимум их части (выборка всегда является эффективным решением). Ситуация, когда вы работаете со скоротечным эфемерным потоком (потоком, который невозможно реплицировать), ставит еще более сложные за-дачи; фактически необходимо постоянно вести учет значений, которые продол-жают поступать. Если выборка просто сводится к расчету необходимой статистики на порции из n прецедентов (при допущении, что поток не имеет какого-то определенного порядка следования), то вычисление статистик на лету требует регистрировать соответствующие количественные показатели. Для получения минимума и максимума по каждому количественному призна- ку следует хранить отдельную переменную. Начиная с самого первого значения, которое берется как исходный минимум и максимум, затем следует сопоставлять каждое новое значение, которое поступает из потока, с ранее зарегистрирован-ными минимальным и максимальным значениями. Если новый прецедент вы-ходит за пределы предыдущего интервала значений, то переменная соответст - вующим образом обновляется. Сре днее арифметическое значение тоже не вызывает каких-то особых проб - лем, потому что требуется хранить только сумму просмотренных значений и ко-\n--- Страница 74 ---\nУправление признаками на потоках данных  73 личество прецедентов. Что касается дисперсии, то напомним ее хрестоматийную формулу: Следует отметить, что здесь требуется знать среднее значение μ, которое также извлекается из потока инкрементно. Однако формула дисперсии для потоковой передачи может быть эксплицирована следующим образом: Поскольку число прецедентов n и сумма значений x уже регистрируются, требу - ется хранить всего одну дополнительную переменную – сумму значений x в квад - рате, и в результате будут получены все ингредиенты для рецепта. В качестве примера возьмем набор данных Bike-sharing об аренде велосипедов, вычислим текущее среднее значение, стандартное отклонение и размах значений и построим график, показывающий, как эти статистики изменялись по мере того, как данные потоком передавались из диска: In: import os, csv local_path = os.getcwd().decode('cp1251') source = 'data\\\\bikesharing\\\\hour.csv'SEP = str(',') running_mean = list() running_std = list() with open(local_path+'\\\\'+source, 'rb') as R: iterator = csv.DictReader(R, delimiter=SEP) x = 0.0 x_squared = 0.0 for n, row in enumerate(iterator): temp = float(row['temp']) if n == 0: max_x, min_x = temp, temp else: max_x, min_x = max(temp, max_x),min(temp, min_x) x += temp x_squared += temp**2 running_mean.append(x / (n+1)) running_std.append(((x_squared - (x**2)/(n+1))/(n+1))**0.5) # заглушка для ОБРАБОТКИ ДАННЫХ # заглушка для МАШИННОГО ОБУЧЕНИЯ pass print('Всего строк: {0}'.format(n+1)) print('Признак \\'temp\\': среднее=%0.3f, макс=%0.3f, мин=%0.3f, стд.откл.=%0.3f' % (running_mean[-1], max_x, min_x, running_std[-1])) Out: Всего строк: 17379Признак 'temp': среднее=0.497, макс=1.000, мин=0.020, стд.откл.=0.193\n--- Страница 75 ---\n74  Масштабиру емое обучение в Scikit-learn Через несколько мгновений данные пойдут потоком из источника, и ключевые цифры относительно признака temp начнут регистрироваться по мере того, как вычисляется текущая оценка среднего и стандартного отклонений и сохраняется в двух отдельных списках. Выведя на график списковые значения, можно узнать, насколько оценки ко- лебались относительно итоговых цифр, и получить представление о том, сколь-ко требуется прецедентов, прежде чем получить стабильную оценку среднего и стандартного отклонений 1: In: # См. инициализацию графических настроек в начале блокнота plt.plot(running_mean,'r-', label=u'среднее значение')plt.plot(running_std,'b-', label=u'стандартное отклонение')plt.xlim(0,18000)plt.ylim(0.0,0.6)plt.xlabel(u'Число тренировочных примеров')plt.ylabel(u'Значение') plt.legend(loc='lower right', numpoints= 1)plt.show() Если предварительно обработать исходный набор данных об аренде велосипе- дов, то получится график, на котором в данных отчетливо видна тенденция (бла-годаря упорядочению во времени, потому что температура естественно меняется в зависимости от времен года): 1 В начале каждого блокнота, где присутствуют графики, имеется следующая ниже ячейка с графическими нас тройками для графиков: import matplotlib.pyplot as plt %matplotlib inline from matplotlib import rcParams rcParams['font.family'] = 'sans-serif' rcParams['font.sans-serif'] = ['Ubuntu Condensed'] rcParams['figure.figsize'] = (5, 4) rcParams['legend.fontsize'] = 10 rcParams['xtick.labelsize'] = 9 rcParams['ytick.labelsize'] = 9 def saveplot(dest): plt.tight_layout() plt.savefig('images/' + dest, dpi=600)\n--- Страница 76 ---\nУправление признаками на потоках данных  75 И наоборот, если в качестве источника использовать перемешанную версию на- бора данных в файле shuffled_hour.csv , то мы, скорее всего, получим пару намного более стабильных и быстро сходящихся оценок. Следовательно, мы приобретем приблизительную, но более надежную оценку среднего значения и стандартного отклонения, регистрируя меньшее число прецедентов из потока:\n--- Страница 77 ---\n76  Масштабиру емое обучение в Scikit-learn Расхождение в двух диаграммах напоминает нам о важности рандомизации порядка следования наблюдений. Тенденции в данных могут оказывать сильное влияние даже на усвоение простых описательных статистик; следовательно, мы должны уделять этому особое внимание во время обучения сложных моделей ал-горитмом SGD. Описание целевой переменной В дополнение перед началом следует также проанализировать целевую перемен-ную. Мы, по сути дела, должны быть уверены в том, какие она принимает значе-ния, является ли категориальной, и выяснить ее несбалансированность, когда речь идет о классах, или скошенность распределения, когда она представлена числом. Если в процессе обучения извлекается числовой отклик, то можно принять ту же самую стратегию, показанную ранее для признаков, тогда как для классов бу - дет достаточно словаря Python, который будет регистрировать количество клас - сов (ключи) и их частоты (значения). В качестве примера скачаем набор данных Forest Co vertype с данными лесного покрова для задачи классификации. Чтобы быстро скачать и подготовить данные, мы воспользуемся функцией gzip_from_UCI в том виде, в каком она определена в разделе «Наборы данных для реальных дел» данной главы: In:UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'gzip_from_UCI(UCI_url, dest='data\\\\') В случае проб лем при выполнении программного кода либо если вы предпочи- таете подготовить файл самостоятельно, просто перейдите на веб-сайт UCI, ска-чайте набор данных и распакуйте его в каталог, где работает Python: https://archive. ics.uci.edu/ml/machine-learning-databases/covtype/. Когда данные будут на диске, можно просканировать все 581 012 прецедентов, конвертируя последнее значение каждой строки, которое представляет оценивае - мый клас с, в соответствующий ему тип лесного покрова: In:import os, csv local_path = os.getcwd().decode('cp1251') source = 'data\\\\covtype.data'SEP = str(',') forest_type = {1:\"ель/хвойное\", 2:\"сосна красная\", 3:\"сосна желтая\", 4:\"тополь/ива\", 5:\"осина\", 6:\"пихта\", 7:\"криволесье\"}forest_type_count = {value:0 for value in forest_type.values()}forest_type_count['другое'] = 0 lodgepole_pine = 0 spruce = 0proportions = list() with open(local_path+'\\\\'+source, 'rb') as R: iterator = csv.reader(R, delimiter=SEP)\n--- Страница 78 ---\nУправление признаками на потоках данных  77 for n, row in enumerate(iterator): response = int(row[-1]) # последнее значение - это отклик try: forest_type_count[forest_type[response]] +=1 if response == 1: spruce += 1 elif response == 2: lodgepole_pine +=1 if n % 10000 == 0: proportions.append([spruce/float(n+1), lodgepole_pine/float(n+1)]) except: forest_type_count['другое'] += 1 print('Всего строк: %i' % (n+1)) print('Частота классов:') for ftype, freq in sorted([(t,v) for t,v in forest_type_count.iteritems()], key = lambda x: x[1], reverse=True): print(\"%-18s: %6i %04.1f%%\" % (ftype, freq, freq*100/float(n+1))) Out: Всего строк: 581012Частота классов:сосна красная : 283301 48.8%ель/хвойное : 211840 36.5%сосна желтая : 35754 06.2%криволесье : 20510 03.5%пихта : 17367 03.0%осина : 9493 01.6%тополь/ива : 2747 00.5%другое : 0 00.0% Результат показывает, что два класса, «сосна красная» и «ель /хвойное», состав- ляют бóльшую часть наблюдений. Если примеры надлежаще перемешиваются в потоке, то алгоритм SGD соответствующим образом извлечет правильное апри-орное распределение и, следовательно, скорректирует свою эмиссию вероятности (апостериорную вероятность). Если вопреки нашему нынешнему случаю целью является не точность класси- фикации, а увеличение площади под ROC -кривой (AUC) либо отметка F1 (функ - ции ошибки, которые могут использоваться для оценивания модели), то для по-лучения обзорной информации можно непосредственно проконсультироваться с документацией по библиотеке Scikit-learn на http://scikit-learn.org/stable/modules/ model_evaluation.html относительно классификационной модели, натренирован-ной на несбалансированных данных, и затем предоставленная информация по-может вам сбалансировать веса с использованием параметра class_weight при за- дании классификатора SGDClassifier либо параметра sample_weight при частичной подгонке модели. Оба параметра изменяют влияние наблюдаемого прецедента путем присвоения положительного или отрицательного весового коэффициента. В обоих случаях оперирование этими двумя параметрами может изменить апри-орное распределение. Взвешивание классов и прецедентов будет обсуждаться в следующей главе.\n--- Страница 79 ---\n78  Масштабируемое об учение в Scikit-learn  Графики характеристической кривой, или графики ROC-кривой, иногда также кривой оши- бок, от англ. Receiver Operator Characteristic (ROC), т. е. график рабочей характеристики получателя, – это широко используемый инструмент отбора моделей для классификации данных. Диагональ ROC-графика можно интерпретировать как случайное гадание, и клас - сификационные модели, которые попадают ниже диагонали, считаются хуже случайного гадания. Основываясь на ROC-кривой, затем можно вычислить показатель так называемой площади под кривой AUC, от англ. area under the curve, чтобы охарактеризовать результа-тивность классификационной модели. Прежде чем перейти к тренировке модели и работе с классами, можно прове- рить, имеет ли соотношение классов всегда систематический характер, с тем что- бы подать на вход алгоритма SGD правильную априорную вероятность: In: import numpy as np proportions = np.array(proportions)plt.plot(proportions[:,0],'r-', label=u'ель/хвойное') plt.plot(proportions[:,1],'b-', label=u'сосна красная')plt.ylim(0.0,0.8)plt.xlabel(u'Тренировочные примеры (единица=10000)')plt.ylabel('%') plt.legend(loc='lower right', numpoints= 1)plt.show() На приведенном выше графике заметно, как процент примеров изменяется по мере того, как мы продвигаемся вперед, передавая данные потоком в сущест - вующем порядк е следования. В этом случае данные действительно необходимо перемешать, если мы на самом деле хотим, чтобы стохастический онлайновый алгоритм правильно обучился на данных.\n--- Страница 80 ---\nУправление признаками на потоках данных  79 Собственно говоря, соотношения классов можно изменить; этот набор данных имеет некую упорядоченность, возможно, географическую, которую следует ис - править путем перемешивания данных, иначе мы рискуем переоценить либо недо оценивать нек оторые классы относительно других. Хэширование признаков Если среди признаков имеются категории (представленные числовыми значения - ми или оставленные в текстовой форме), то задача может стать немного сложнее. В пакетном обучении категории обычно записываются с использованием прямо-го унитарного кодирования, в результате чего получается столько новых двоич-ных категорий, сколько имеется категорий. К сожалению, в потоке невозможно узнать заранее, с каким количеством категорий имеешь дело, и даже в результате выборки нельзя быть уверенным в их числе, потому что редко встречающиеся категории могут появиться в потоке существенно позже либо потребуют найти слишком большую выборку. Вам придется сначала прокачать все данные и заре-гистрировать каждую появившуюся категорию. Так или иначе, потоки могут быть эфемерными, и при этом число классов иногда может быть настолько большим, что их не получится сохранить в оперативной памяти. Онлайновые рекламные данные являются таким примером в силу своих высоких объемов, которые трудно сохранить в памяти, и потому, что по потоку нельзя пройти более одного раза. Бо-лее того, рекламные данные в значительной степени варьируются, а их признаки постоянно изменяются.  Прямое унитарное кодирование, от англ. one-hot encoding (OHE), – это представление ко-нечног о множества значений признака в виде двоичного кода фиксированной длины, со- держащего только одну 1, т. е. где i-е значение признака равно 1, а все остальные равны 0. При обратном кодировании имеется только один 0. Длина кода определяется количеством кодируемых значений признака, или объектов. Работа с текстами делает проблему значительно еще более отчетливой, потому что невозможно предвидеть, какое слово может быть частью анализируемого тек - ста. В модели «мешка слов», где в каждом тексте подсчитываются встретившиеся в нем слова и их частотное значение вставляется в соответствующий каждому слову элемент вектора признаков, необходимо суметь заранее сопоставить каж - дому слову номер индекса. И даже если с этим удастся справиться, всегда при-дется разбираться с ситуацией, когда неожиданно во время тестирования либо во время использования предиктора в эксплуатационной среде появится незнако-мое слово (которому поэтому индексный номер никогда заранее не сопоставлен). Косвенно следует также добавить, что, будучи отражением разговорного языка, совсем не являются необычными тематические словари, состоящие из сотен ты-сяч или даже миллионов разных терминов. Резюмируя сказанное, если вам удастся заранее узнать классы в признаках, то вы сможете справиться с ними при помощи прямого кодировщика из библиотеки Scikit-learn (http://Scikit-learn.org/stable/modules/generated/sklearn.preprocessing.One - Hot Encoder .html). Мы в действительности не будем здесь иллюстрировать его работу, но, в принципе, этот подход нисколько не отличается от того, который применялся бы во время работы с пакетным обучением. Однако мы хотим проиллюстрировать ситуацию, когда применить прямое кодирование действительно невозможно.\n--- Страница 81 ---\n80  Масштабиру емое обучение в Scikit-learn Существует опирающееся на хэш-функцию решение под названием «хэши- рование признаков », или хэш-трюк (hashing trick), которое может работать как с текстом, так и с категориальными переменными в целочисленной либо стро- ковой форме. Это решение также работает с категориальными переменными, смешанными с числовыми значениями из количественных признаков. Ключевая проблема с прямым унитарным кодированием состоит в том, что оно присваива-ет позицию значению в векторе признаков, только после того как его признаку сопоставлена эта позиция. Хэширование признаков, с другой стороны, способно недвусмысленно сопоставить значению его позицию без априорной необходимо-сти выполнить оценку признака, потому что эта операция задействует ключевое свойство хэш-функции – детерминированное преобразование величины либо строкового значения в целочисленное значение. Поэтому единственная необходимая подготовительная работа перед примене- нием хэширования признаков состоит в создании разреженного вектора, кото-рый будет достаточно крупным, чтобы представлять вычислительную сложность данных (потенциально содержащий от 2**19 до 2**30 элементов в зависимости от доступной оперативной памяти, шинной архитектуры компьютера и типа ис - пользуемой хэш-функции). Если будет обрабатываться текст, то, помимо этого, понадобится лексемизатор, т. е. функция, которая разобьет текст на отдельные слова и удалит пунктуацию. Простой миниатюрный пример внесет ясность. Мы воспользуемся двумя спе- циализированными функциями из библиотеки Scikit-learn: преобразователем HashingVectorizer , опирающимся на хэширование признаков, который обрабаты- вает текстовые данные, и еще одним преобразователем FeatureHasher , который за- точен на преобразовании строки данных в виде словаря Python в разреженный вектор признаков. В качестве первого примера превратим фразу в вектор: In:from sklearn.feature_extraction.text import HashingVectorizer h = HashingVectorizer(n_features=1000, binary=True, norm=None)# Перевод: простой миниатюрный пример внесет ясность в то, как он работает sentence = 'A simple toy example will make clear how it works.' sparse_vector = h.transform([sentence]) print(sparse_vector)Out: (0, 61) 1.0 (0, 271) 1.0 (0, 287) 1.0 (0, 452) 1.0 (0, 462) 1.0 (0, 539) 1.0 (0, 605) 1.0 (0, 726) 1.0 (0, 918) 1.0\n--- Страница 82 ---\nУправление признаками на потоках данных  81 Итоговый вектор имеет единичные значения только с определенным индексом, указывая на связь между маркером во фразе (словом) и определенной позицией в векторе. К сожалению, эту связь нельзя инвертировать, если, конечно, не сопо-ставить значение хэш-функции каждому маркеру во внешнем словаре Python. Хотя такое сопоставление возможно, оно в действительности будет потреблять много памяти, потому что словари бывают большими, в диапазоне миллионов элементов и даже больше, в зависимости от языка и тематической направленности. На самом деле нам не нужно вести такой учет, потому что хэш-функции всегда гарантиро-ванно из одинакового маркера продуцируют одинаковый индексный указатель. Реальную проблему для хэширования признаков в конечном счете представляет возможность конфликта, который случается, когда два разных маркера ассоции-руются с одной и той же позицией. Это редкое, но возможное явление возникает во время работы с большими словарями. С другой стороны, в скомпонованной из миллионов коэффициентов модели очень немногие из них имеют влияние. Сле-довательно, если конфликт произойдет, то весьма вероятно, что он будет связан с двумя неважными маркерами. При использовании хэширования признаков ве-роятность на вашей стороне, потому что весьма маловероятно, что в достаточно большом выходном векторе (например, с числом элементов свыше 2^24) конфлик - ты, хотя и являясь всегда возможными, будут содержать важные элементы модели. Хэширование признаков может применяться к нормальным векторам призна- ков, в особенности когда имеются категориальные переменные. Ниже приведен пример с преобразователем FeatureHasher : In: from sklearn.feature_extraction import FeatureHasher h = FeatureHasher(n_features=1000, non_negative=True) example_row = {'numeric feature':3, # числовой признак 'another numeric feature':2, # еще один числовой признак 'Categorical feature = 3':1, # категориальный признак 'f1*f2*f3':1*2*3} print(example_row))Out: {'another numeric feature': 2, 'f1*f2*f3': 6, 'numeric feature': 3, 'Categorical feature = 3': 1} Если словарь Python для числовых значений будет содержать имена признаков и для любой категориальной переменной – сочетание имени признака и значе- ния, то значения словаря будут отображаться посредством хэшированного ин-дексного номера ключей, создавая прямокодированный вектор признаков, гото-вый к тому, чтобы быть заученным алгоритмом SGD: In: print(h.transform([example_row])) Out: (0, 16) 2.0 (0, 373) 1.0 (0, 884) 6.0 (0, 945) 3.0\n--- Страница 83 ---\n82  Масштабиру емое обучение в Scikit-learn Другие элементарные преобразования После того как пример взят из хранилища данных, помимо превращения кате- гориальных переменных в числовые, к нему можно применить еще одно преоб-разование, с тем чтобы увеличить прогнозирующую способность обучающегося алгоритма. Преобразования могут применяться к признакам посредством функ - ции (путем применения квадратного корня, логарифма или другой функции пре-образования) либо посредством операций на группах признаков. В следующей главе мы предложим подробные примеры с полиномиальным раз- ложением и методами случайного неизбирательного отбора признаков (kitchen-sink). В настоящей главе мы, забегая вперед, создадим квадратичные признаки путем вложенных итераций. Квадратичные признаки обычно создаются при вы-числении полиномиальных разложений, и их цель состоит в том, чтобы уловить, каким образом прогнозные признаки взаимодействуют; они могут неожиданным образом повлиять на отклик в целевой переменной. В качестве примера, дающего интуитивно понятное разъяснение того, почему квадратичные признаки могут иметь важное значение в моделировании целе-вого отклика, объясним случай воздействия двух лекарств на пациента. По сути дела, может оказаться, что оба лекарства в большей или меньшей степени явля-ются эффективными в лечении болезни, против которой мы боремся. Как бы там ни было, но, когда пациент принимает оба этих препарата, состоящих из разных компонентов, вместе, они проявляют тенденцию аннулировать влияние друг дру - га. В данном случае, хотя оба лекарства эффективны, вместе они ввиду своего от - рицательного взаимодействия не работают вообще. В этом смысле взаимодействия между признаками можно найти среди широ- кого круга признаков, и не только в медицине. При этом критически важно выяс - нить наиболее значимый из них, чтобы модель работала лучше при выполнении предсказания своей целевой переменной. Если неизвестно, что определенные признаки взаимодействуют относительно задачи, то единственный выбор состо-ит в том, чтобы все их систематически протестировать и позволить модели оста-вить те признаки, которые работают лучше. В следующем ниже простом примере представим, что вектор v только что в ре- зультате потоковой передачи был помещен в оперативную память с целью его за-учивания, затем преобразован в другой вектор vv, в котором исходные признаки вектора v дополнены результатами их мультипликативных взаимодействий (каж - дый признак помножен один раз на все остальные). При наличии большего числа признаков в обучающийся алгоритм поступит вектор vv вместо исходного вектора v с целью достичь наилучшей аппроксимации данных: In:import numpy as np v = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) vv = np.hstack((v, [v[i]*v[j] for i in range(len(v)) for j in range(i+1, len(v))])) Out: [ 1 2 3 4 5 6 7 8 9 10 2 3 4 5 6 7 8 9 10 6 8 10 12 14 16 18 20 12 15 18 21 24 27 30 20 24 28 32 36 40 30 35 40 45 50 42 48 54 60 56 63 70 72 80 90]\n--- Страница 84 ---\nУправление признаками на потоках данных  83 Аналогичные или еще более сложные преобразования могут генерироваться на лету по ходу потоковой передачи примеров в обучающийся алгоритм, пользуясь тем, что пакет данных небольшой (иногда сводимый к одиночным примерам), и расширение числа признаков такого небольшого количества примеров может быть реально достигнуто в оперативной памяти. В следующей главе мы рассмот - рим еще больше примеров таких преобразований и их успешной интеграции в конвейер обучения. Т естирование и перекрестная проверка в потоке Мы воздержались от показа полных примеров тренировки после ознакомления с алгоритмом SGD, потому что необходимо сначала познакомиться с тем, каким образом в потоке выполняются тестирование и перекрестная проверка (контроль). При использовании пакетного обучения тестирование и перекрестная проверка являются вопросом рандомизации порядка следования наблюдений, нарезки на-бора данных на блоки и взятия конкретного блока в качестве тестового набора либо систематического взятия по очереди одного из блоков для тестирования способностей алгоритма к обучению. Потоки не могут оставаться в оперативной памяти, поэтому, исходя из того, что следующие прецеденты будут уже рандомизированы, лучшее решение состо-ит в том, чтобы брать проверочные прецеденты после некоторого времени с на-чала поступления данных потока либо систематически использовать конкретный воспроизводимый шаблон в потоке данных. Вневыборочный подход на части потока фактически сопоставим с тестовой вы- боркой и может быть успешно реализован, только если заранее известна длина потока. Для непрерывных потоков этот подход по-прежнему возможен, но озна-чает безусловную остановку процесса обучения, как только начинаются тестовые прецеденты. Этот метод называется стратегией с периодическим отложением данных (holdout) после n прецедентов.  Проверки эффективнос ти прогнозирования модели обычно проводятся путем разбиения заданного набора данных на внутривыборочный (in-sample) период, который используется для первоначальной оценки параметров и отбора модели, и вневыборочный (out-of-sample) период, используемый для оценки эффективности прогнозирования на ранее не встречав-шихся данных. Перекрестно-проверочный тип подхода возможен при использовании система- тического и воспроизводимого отбора проверочных прецедентов. После определе-ния стартового буфера прецедент отбирается для проверки с интервалом n. Такой прецедент используется не для тренировки, а исключительно для тестирования. Этот метод называется стратегией с отложением данных с периодичностью n . Поскольку проверка выполняется на основе одиночного прецедента, вычис - ляется глобальная мера результативности, усредняющая все меры ошибки, со-бранные на данный момент в пределах одного и того же прохождения по данным либо в оконном режиме с использованием самого последнего набора из k мер, где k – это число тестов, которое, как вы полагаете, является обоснованно репрезен- тативным. Собственно говоря, во время первого прохождения обучающийся алгоритм в действительности не знаком ни с одним прецедентом. Поэтому целесообразно\n--- Страница 85 ---\n84  Масштабиру емое обучение в Scikit-learn тестировать алгоритм по мере получения им случаев для обучения, проверяя его отклик на поступивший случай, перед тем как ему обучиться. Этот подход назы-вается прогрессивной проверкой. Применение алгоритма SGD в деле В заключение настоящей главы мы реализуем два примера: один – для задачи классификации на основе данных Covertype о лесном покрове, и другой – для ре-грессии на основе набора данных Bike-sharing об аренде велосипедов. Мы уви-дим, как реализовать на практике приведенные выше соображения об отклике и распределениях признаков и для каждой из этих задач применить оптимальную стратегию проверки. Если начать с задачи классификации, то следует отметить два аспекта. Гово- ря о мультиклассовой задаче, мы в первую очередь заметили, что в базе данных и распределении классов по потоку прецедентов существует некая упорядочен-ность. В качестве исходного шага мы перемешаем данные при помощи функции ram_shuffle , определенной в разделе «Особое внимание упорядочению прецедентов» настоящей главы: In: import os local_path = os.getcwd().decode('cp1251') source = 'data\\\\covtype.data' ram_shuffle(filename_in=local_path+'\\\\'+source, filename_out=local_path+'\\\\data\\\\shuffled_covtype.data', header=False) Поскольку мы распаковываем строки в оперативной памяти и перемешиваем их без особого использования дискового пространства, можно быстро получить новый рабочий файл. В следующем фрагменте кода мы тренируем классификатор SGDClassifier с логистической функцией потерь (эквивалентной логистической регрессии), с тем чтобы он задействовал наши предыдущие знания о присутст - вующих в наборе данных классах. Список forest_type содержит все коды классов, и он передается каждый раз (хотя всего одного, первого раза было бы достаточно) в метод partial_fit ученика SGD. Для целей проверки мы определяем «холодный» запуск из 200 000 наблюдав- шихся случаев. На каждом десятом прецеденте один будет изыматься из процес - са тренировки и использоваться для проверки. Эта схема допускает воспроизво-димость, даже если мы собираемся выполнять многократные прохождения по данным; при каждом прохождении те же самые прецеденты будут откладывать-ся в сторону в качестве вневыборочного теста, что позволяет проверить эффект многократных прохождений по тем же данным, создав кривую проверки. Схема с отложением данных также сопровождается прогрессивной проверкой. Поэтому каждый случай после «холодного» запуска оценивается до того, как по-даваться на вход этапа тренировки. Хотя прогрессивная проверка обеспечивает интересную обратную связь, такой способ будет работать только для первого про-хождения; на деле после первоначального прохождения все наблюдения (кроме тех, которые отложены согласно схеме отложенной выборки) станут внутривы-\n--- Страница 86 ---\nУправление признаками на потоках данных  85 борочными прецедентами. В нашем примере мы собираемся сделать всего одно прохождение. Напоминаем, что набор данных имеет 581 012 прецедентов, так что его потоко- вая передача и моделирование алгоритмом SGD могут занять достаточно продол-жительное время (это вполне крупномасштабная задача для одиночного компью-тера). Хотя мы установили ограничитель на количество наблюдений в размере всего 250 000 прецедентов, все же дайте компьютеру поработать в течение поряд-ка 15–20 мин., чтобы получить конечные результаты: In: import csv, timeimport numpy as npfrom sklearn.linear_model import SGDClassifier source = 'data\\\\shuffled_covtype.data' SEP = str(',')forest_type = [t+1 for t in range(7)] SGD = SGDClassifier(loss='log', penalty=None, random_state=1, average=True)accuracy = 0holdout_count = 0prog_accuracy = 0prog_count = 0cold_start = 200000k_holdout = 10 with open(local_path+'\\\\'+source, 'rb') as R: iterator = csv.reader(R, delimiter=SEP) for n, row in enumerate(iterator): if n > 250000: # сокращаем время выполнения эксперимента break # ОБРАБОТКА ДАННЫХ response = np.array([int(row[-1])]) # последнее значение – отклик features = np.array(map(float,row[:-1])).reshape(1,-1) # МАШИННОЕ ОБУЧЕНИЕ if (n+1) >= cold_start and (n+1-cold_start) % k_holdout==0: if int(SGD.predict(features))==response[0]: accuracy += 1 holdout_count += 1 if (n+1-cold_start) % 25000 == 0 and (n+1) > cold_start: print('%s Точность на отложенных данных: %0.3f' % (time.strftime('%X'), accuracy / float(holdout_count))) else: # ПРОГРЕССИВНАЯ ПРОВЕРКА if (n+1) >= cold_start: if int(SGD.predict(features)) == response[0]: prog_accuracy += 1 prog_count += 1 if n % 25000 == 0 and n > cold_start:\n--- Страница 87 ---\n86  Масштабиру емое обучение в Scikit-learn print('%s Прогрессивная точность: %0.3f' % (time.strftime('%X'), prog_accuracy / float(prog_count))) # ФАЗА ОБУЧЕНИЯ SGD.partial_fit(features, response, classes=forest_type)print('%s ИТОГОВАЯ точность на отложенных данных: %0.3f' % (time.strftime('%X'), accuracy / ((n+1-cold_start) / float(k_holdout))))print('%s ИТОГОВАЯ прогрессивная точность: %0.3f' % (time.strftime('%X'), prog_accuracy / float(prog_count))) Out: 13:39:59 Точность на отложенных данных: 0.63713:39:59 Прогрессивная точность: 0.61613:41:22 Точность на отложенных данных: 0.62813:41:22 Прогрессивная точность: 0.61413:41:22 ИТОГОВАЯ точность на отложенных данных: 0.62913:41:22 ИТОГОВАЯ прогрессивная точность: 0.614 В качестве второго примера мы попытаемся предсказать число сданных в арен- ду велосипедов в Вашингтоне на основе серии данных о погоде и времени. Учиты- вая исторический порядок следования данных в наборе, мы его не перемешиваем и рассматриваем задачу как временной ряд. Наша стратегия проверки состоит в тестировании результатов после просмотра определенного числа примеров, для того чтобы, начиная с этого момента, реплицировать неопределенности с целью выполнения прогноза. Кроме того, интересно отметить, что некоторые признаки являются катего - риальными, и поэтому мы применили класс FeatureHasher из библиотеки Scikit- learn, чтобы записать категории как ключи в словаре в виде комбинированной строки, состоящей из кода категории и имени переменной. Назначаемое каждому из этих ключей в словаре значение аналогично двоичной переменной в разре-женном векторе, который будет создаваться хэшированием признаков: In: import csv, time, osimport numpy as npfrom sklearn.linear_model import SGDRegressorfrom sklearn.feature_extraction import FeatureHasher local_path = os.getcwd().decode('cp1251') source = 'data\\\\bikesharing\\\\hour.csv'SEP = str(',') def apply_log(x): return np.log(float(x)+1) def apply_exp(x): return np.exp(float(x))-1 SGD = SGDRegressor(loss='squared_loss', penalty=None, random_state=1, average=True)h = FeatureHasher(non_negative=True)\n--- Страница 88 ---\nУправление признаками на потоках данных  87 val_rmse = 0 val_rmsle = 0predictions_start = 16000 with open(local_path+'\\\\'+source, 'rb') as R: iterator = csv.DictReader(R, delimiter=SEP) for n, row in enumerate(iterator): # ОБРАБОТКА ДАННЫХ target = np.array([apply_log(row['cnt'])]) features = {k+'_'+v:1 for k,v in row.iteritems() if k in [‘holiday’,’hr’,’mnth’,’season’, 'weathersit','weekday','workingday','yr']} numeric_features = {k:float(v) for k,v in row.iteritems() if k in ['hum', 'temp', 'atemp', 'windspeed']} features.update(numeric_features) hashed_features = h.transform([features]) # МАШИННОЕ ОБУЧЕНИЕ if (n+1) >= predictions_start: # ФАЗА С ОТКЛАДЫВАНИЕМ ПОСЛЕ N predicted = SGD.predict(hashed_features) val_rmse += (apply_exp(predicted) - apply_exp(target))**2 val_rmsle += (predicted - target)**2 if (n-predictions_start+1) % 250 == 0 and (n+1) > predictions_start: print('%s RMSE на отложенных данных: %0.3f,' % (time.strftime('%X'), (val_rmse / float(n-predictions_start+1))**0.5)), print('то же самое RMSLE: %0.3f' % ((val_rmsle / float(n-predictions_start+1))**0.5)) else: # ФАЗА ОБУЧЕНИЯ SGD.partial_fit(hashed_features, target) print('%s ИТОГОВАЯ RMSE на отложенных данных: %0.3f' % (time.strftime('%X'), (val_rmse / float(n-predictions_start+1))**0.5))print('%s ИТОГОВАЯ RMSLE на отложенных данных: %0.3f' % (time.strftime('%X'), (val_rmsle / float(n-predictions_start+1))**0.5)) Out: 15:42:20 RMSE на отложенных данных: 281.214, то же самое RMSLE: 1.90115:42:20 RMSE на отложенных данных: 255.092, то же самое RMSLE: 1.80315:42:20 RMSE на отложенных данных: 255.578, то же самое RMSLE: 1.79915:42:21 RMSE на отложенных данных: 254.598, то же самое RMSLE: 1.81615:42:21 RMSE на отложенных данных: 239.728, то же самое RMSLE: 1.73815:42:21 ИТОГОВАЯ RMSE на отложенных данных: 229.15415:42:21 ИТОГОВАЯ RMSLE на отложенных данных: 1.679  Аббревиатуры RMSE (root mean squared error) и RMSLE (root mean squared logarithmic error) обозначают соответственно корень из среднеквадратической ошибки и корень из средне-квадратической логарифмической ошибки.\n--- Страница 89 ---\n88  Масштабиру емое обучение в Scikit-learn резюме В этой главе мы выяснили, как организуется процесс обучения вне ядра путем потоковой передачи данных из текстового файла или базы данных на жестком диске, независимо от того, насколько большими эти данные являются. Эти мето-ды, конечно же, применяются к гораздо более крупным наборам данных, чем те примеры, которые мы использовали для их демонстрации (которые в действи-тельности могут быть решены в оперативной памяти при помощи нестандартных и мощных аппаратных средств). Мы также дали объяснение ключевого алгоритма, который делает возможным внеядерное обучение, – алгоритма стохастического градиентного спуска SGD – и исследовали его достоинства и недостатки, подчеркнув необходимость настоя - щей с тохастичности потоков (т. е. обеспечения произвольного порядка следова- ния прецедентов), чтобы стать по-настоящему эффективными, если, конечно, порядок следования не является частью задачи обучения. В частности, мы пред-ставили реализацию алгоритма SGD в библиотеке Scikit-learn, ограничившись ли-нейной регрессионной и логистической регрессионной функциями потерь. Наконец, мы обсудили подготовку данных, представили вводную информацию о хэшировании признаков и стратегии проверки потоков и воплотили получен-ные знания об алгоритме SGD, выполнив подгонку двух разных моделей – клас - сификации и регрессии. В следующей главе мы продолжим обогащать наши возможности по внеядер- ному обучению, выяснив, каким образом подключить в нашу схему обучения не-линейность и кусочно-линейную функцию потерь для метода опорных векторов. Мы также представим альтернативные библиотеке Scikit-learn программные ин-струменты, такие как Liblinear, Vowpal Wabbit и StreamSVM. Хотя все они вы- полняются на основе внешних команд оболочки, они легко обертываются и конт - ролир уются сценариями Python.",
      "debug": {
        "start_page": 47,
        "end_page": 89
      }
    },
    {
      "name": "Глава 3. Быстрообучающиеся реализации машин SVM 89",
      "content": "--- Страница 90 --- (продолжение)\nГлава 3 Быстрообучающиеся реализации машин SVM Поэкспериментировав с онлайновым стилем обучения в предыдущей главе, вы, возможно, были удивлены его простотой и вместе с тем эффективностью и мас - штабируемостью по сравнению с пакетным обучением. Несмотря на то что в каж - дый момент времени обучение протекает на основе всего одного примера, алго-ритм SGD может аппроксимировать результаты так же, как при обучении, когда все данные находятся в оперативной памяти и используется пакетный алгоритм. Тре-буется всего лишь обеспечить, чтобы поток был по-настоящему стохастическим (данные не содержат никаких тенденций) и чтобы ученик был хорошо настроен на задачу (темп обучения часто является ключевым и фиксированным параметром). Так или иначе, внимательно изучая подобного рода достижения, совершенно очевидно, что полученные результаты все же сопоставимы только с пакетными линейными моделями, но не с более сложными учениками, отличающимися бо-лее высокой дисперсией, чем смещение, такими как машины опорных векторов, нейронные сети или ансамбли деревьев решений с бэггингом и бустингом. Для определенных задач, в частности с высокими и широкими, но разрежен- ными данными, можно обойтись и линейными комбинациями, если исходить из наблюдения, что простой алгоритм с большим количеством данных зачастую по-беждает более сложные, натренированные на меньшем количестве данных. Но если же линейные модели применять, прибегая к явному отображению сущест - вующих признак ов на признаки с более высокой размерностью (используя другой порядок взаимодействий, полиномиальные разложения и ядерные аппроксима-ции), то они могут ускорить и улучшить процесс усвоения сложных нелинейных связей между откликом и признаками. В настоящей главе мы поэтому сначала представим линейные машины SVM как альтернативный линейным моделям алгоритм машинного обучения, который опи-рается на другой подход к задаче обучения на данных. Затем мы продемонстриру - ем, как из существующих признаков лучше всего создавать более полные признаки для решения задач машинного обучения, когда приходится иметь дело с крупно-масштабными данными, в особенности высокими данными (т. е. наборами данных, имеющими много прецедентов, на которых будет выполняться обучение). Таким образом, в этой главе мы затронем следующие темы: ознакомление с машинами SVM и предоставление лежащих в их основе понятий и математических формул, объясняющих принцип работы этого алгоритма;\nГлава 3 Быстрообучающиеся реализации машин SVM Поэкспериментировав с онлайновым стилем обучения в предыдущей главе, вы, возможно, были удивлены его простотой и вместе с тем эффективностью и мас - штабируемостью по сравнению с пакетным обучением. Несмотря на то что в каж - дый момент времени обучение протекает на основе всего одного примера, алго-ритм SGD может аппроксимировать результаты так же, как при обучении, когда все данные находятся в оперативной памяти и используется пакетный алгоритм. Тре-буется всего лишь обеспечить, чтобы поток был по-настоящему стохастическим (данные не содержат никаких тенденций) и чтобы ученик был хорошо настроен на задачу (темп обучения часто является ключевым и фиксированным параметром). Так или иначе, внимательно изучая подобного рода достижения, совершенно очевидно, что полученные результаты все же сопоставимы только с пакетными линейными моделями, но не с более сложными учениками, отличающимися бо-лее высокой дисперсией, чем смещение, такими как машины опорных векторов, нейронные сети или ансамбли деревьев решений с бэггингом и бустингом. Для определенных задач, в частности с высокими и широкими, но разрежен- ными данными, можно обойтись и линейными комбинациями, если исходить из наблюдения, что простой алгоритм с большим количеством данных зачастую по-беждает более сложные, натренированные на меньшем количестве данных. Но если же линейные модели применять, прибегая к явному отображению сущест - вующих признак ов на признаки с более высокой размерностью (используя другой порядок взаимодействий, полиномиальные разложения и ядерные аппроксима-ции), то они могут ускорить и улучшить процесс усвоения сложных нелинейных связей между откликом и признаками. В настоящей главе мы поэтому сначала представим линейные машины SVM как альтернативный линейным моделям алгоритм машинного обучения, который опи-рается на другой подход к задаче обучения на данных. Затем мы продемонстриру - ем, как из существующих признаков лучше всего создавать более полные признаки для решения задач машинного обучения, когда приходится иметь дело с крупно-масштабными данными, в особенности высокими данными (т. е. наборами данных, имеющими много прецедентов, на которых будет выполняться обучение). Таким образом, в этой главе мы затронем следующие темы: ознакомление с машинами SVM и предоставление лежащих в их основе понятий и математических формул, объясняющих принцип работы этого алгоритма;\n--- Страница 91 ---\n90  Быстрооб учающиеся реализации машин SVM представление алгоритма SGD с кусочно-линейной функцией потерь как эффек тивного решения крупномасштабных задач, в котором используется такой же подход к оптимизации, что и в пакетном алгоритме SVM; рассмо трение сопутствующих алгоритму SGD нелинейных приближений; обзор др угих крупномасштабных онлайновых решений, представленных в библиотеке Scikit-learn помимо алгоритма SGD. набОры данных для самОст ОятельнОг О экспериментир Ования Как и в предыдущей главе, мы будем использовать наборы данных из репозито-рия машинного обучения UCI, в частности наборы данных Bike-sharing об аренде велосипедов (задача регрессии) и Covertype о лесном покрове (задача многоклас - совой классификации). Если вы еще их не скачали либо если необходимо скачать оба набора данных снова, то вам потребуется пара функций, определенных в разделе «Наборы дан-ных для реальных дел» главы 2 « Масштабируемое обучение в Scikit-learn». Вам по- требуются функции unzip_from_UCI и gzip_from_UCI . Обе эти функции задействуют Python’овское подключение к репозиторию UCI; скачайте сжатый файл и разар-хивируйте его в рабочем каталоге Python. Если вызвать функции из ячейки Jupyter, то вы найдете соответствующие новые каталоги и файлы именно в том месте, где Jupyter будет их искать. В случае если эти функции не работают, не переживайте; мы предоставим ссыл- ку для прямого скачивания. После этого необходимо только распаковать данные в текущем рабочем каталоге Python, который можно узнать, выполнив следую-щую команду в интерфейсе Python (в Jupyter либо любом другом IDE): In: import osprint(u'Текущий каталог: \"{0}\"'.format(os.getcwd().decode('cp1251'))) Out: Текущий каталог: \"C:\\WinPython-64bit-2.7.9.4\\notebooks\\Packt-Large Scale\" Набор данных Bike-sharing Этот набор данных включает в себя два файла в формате CSV, содержащих по- часовое и ежедневное количество велосипедов, взятых в аренду за период с 2011 по 2012 г. в системе Capital Bike-share в г. Вашингтон, округ Колумбия, США. Как напоминание, в данных отражена соответствующая погодная и сезонная инфор-мация относительно дня сдачи в аренду. Следующий фрагмент кода сохранит набор данных на локальном жестком дис - ке при помощи вспомогательной оберточной функции unzip_from_UCI : In: UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip'unzip_from_UCI(UCI_url, dest='data\\\\bikesharing') Out: Извлечение в C:\\scisoft\\WinPython-64bit-2.7.9.4\\notebooks\\data\\bikesharing\n--- Страница 92 ---\nМашины опорных векторов  91 распакован day.csv распакован hour.csv В случае успешного выполнения программный код укажет, в каком каталоге CSV-файлы были сохранены, и распечатает имена обоих разархивированных файлов. При неуспешном выполнении просто скачайте файл, используя следую-щую прямую ссылку https://archive.ics.uci.edu/ml/machine-learning-databases/00275/ Bike-Sharing-Dataset.zip, и разархивируйте оба файла, day.csv и hour.csv , в каталоге bikesharing , который вы ранее создали в своем рабочем каталоге Python. Набор данных Covertype Набор данных Covertype, безвозмездно предоставленный Джоком А. Блэкардом, Дэнисом Дж. Дином, Чарлзом У. Андерсеном (Jock A. Blackard, Dr. Denis J. Dean, Dr. Charles W. Anderson) и Университетом штата Колорадо, содержит 581 012 при-меров и серию из 54 картографических переменных, включая рельеф местности и тип почвы, предназначенных для предсказания типа лесного покрова, который состоит из 7 видов (таким образом, эта задача является мультиклассовой). Для того чтобы обеспечить сравнимость с академическими исследованиями на тех же данных, согласно инструкции, рекомендуется использовать первые 11 340 запи-сей для обучения, следующие 3780 записей для проверки и, наконец, оставшиеся 565 892 записи как тестовые примеры: In:UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/ covtype/covtype.data.gz'gzip_from_UCI (UCI_url, dest='data\\\\') В случае проб лем при выполнении программного кода либо если вы предпо- читаете подготовить файл самостоятельно, просто перейдите на веб-сайт UCI, скачайте набор данных по прямой ссылке https://archive.ics.uci.edu/ml/machine- learning-databases/covtype/covtype.data.gz и распакуйте его в каталог, в котором в настоящее время работает Python. машины ОпОрных вект ОрОв Машины опорных вект оров (support v ector machines, SVMs) – это набор ме- тодов обучения с учителем для задач классификации и регрессии (а также для обнаружения выбросов), который довольно универсален, поскольку он может подбирать как линейные, так и нелинейные модели благодаря специальным ядерным функциям. Особенность этих ядерных функций состоит в том, что они способны отображать входные признаки на новый, более сложный вектор признаков, используя для этого ограниченный объем вычислений. Ядерные функции нелинейно рекомбинируют исходные признаки, делая возможным отобра жение отклика очень сложными функциями. В этом смысле машины SVM сравнимы с нейронными сетями как универсальными аппроксиматорами и тем самым во многих задачах могут демонстрировать аналогичную прогнозирую-щую способность. Вопреки рассмотренным в предыдущей главе линейным моделям, машины опорных векторов начинались как метод для решения не регрессионных, а клас - сификационных задач.\n--- Страница 93 ---\n92  Быстрооб учающиеся реализации машин SVM Алгоритм SVM был разработан в лабораториях AT&T в 90-х годах математи- ком Владимиром Вапником (Vladimir Vapnik) и программистом Коринна Кортес (Corinna Cortes) (но над алгоритмом вместе с Вапником также работали многие другие участники). В сущности, алгоритм SVM стремится решить задачу класси-фикации путем нахождения особой гиперплоскости, разделяющей классы в про-странстве признаков. Такого рода гиперплоскость должна характеризоваться на-личием самого большого зазора между границами классов (зазор призван служить в качестве промежутка или пространства без единого примера непосредственно между самими классами). Такое интуитивно понятное объяснение подразумевает два последствия: эмпирически машины SVM пытаются минимизировать ошибку на тесто- вом наборе путем нахождения решения в тренировочном наборе, которое находится точно посередине наблюдаемых классов, тем самым решение од-нозначно имеет вычислительный характер (оно представляет собой опти-мизацию на основе квадратичного программирования – https://en.wikipedia. org/wiki/Quadratic_programming; поско льку решение основывается только на границах классов, задаваемых смежными примерами (именуемыми опорными векторами), другие при-меры могут быть проигнорированы, что делает метод нечувствительным к выбросам и менее интенсивным относительно потребления оперативной памяти, чем методы, основанные на обращении матрицы, в частности ли-нейные модели. Предоставив общие сведения о машинах SVM, далее мы посвятим несколько страниц, делая акцент на ключевых формулах, которые характеризуют этот метод. Несмотря на то что полное и подробное объяснение этого метода выходит за рам-ки настоящей книги, схематичное описание того, как он работает, может действи-тельно помочь выяснить его внутреннюю механику и обеспечит основу для пони-мания, каким образом его можно сделать масштабируемым до больших данных. Исторически машины SVM рассматривались как классификаторы с жестким зазором, точно так же, как и персептрон. В сущности, машины SVM были перво-начально сформулированы в попытке найти две гиперплоскости, разделяющие классы, взаимное расстояние которых было максимально возможным. Такой под-ход отлично работал с линейно разделимыми синтетическими данными. Так или иначе, в версии с жестким зазором, когда машина SVM сталкивалась с нелиней-но разделимыми данными, она могла быть успешной только с использованием нелинейных преобразований признаков. Однако они всегда квалифицировались как неуспешные, когда ошибки неправильной классификации происходили не из-за нелинейности, а из-за шума в данных. По этой причине были введены мягкие зазоры с использованием функции стои мости, которая принимала в расчет серьезность ошибки (так как жесткие за- зоры срабатывали, только если ошибка происходила), тем самым позволяя иметь определенную терпимость к неправильно классифицированным случаям, ошиб-ка которых не была слишком большой из-за того, что они были размещены рядом с разделяющей гиперплоскостью. С введением мягких зазоров машины SVM стали способны противостоять неразделимости, вызванной шумом. Мягкие зазоры вводились путем создания функции стоимости вокруг слабой переменной, которая аппроксимирует число неправильно классифицированных примеров. Подобного рода слабая перемен-\n--- Страница 94 ---\nМашины опорных векторов  93 ная также называется эмпирическим риском (риском построения неправиль- ной классификации, если рассматривать его с точки зрения тренировочных данных). Математически, если дана матрица набора данных X из n примеров и m при- знаков и вектор отклика, выражающий принадлежность к классу, где +1 (принад- лежит) и -1 (не принадлежит), то алгоритм SVM для бинарной классификации стремится минимизировать следующую функцию стоимости: В приведенной выше функции w – это вектор коэффициентов, который выра- жает разделяющую гиперплоскость вместе со смещением b, которое обозначает смещение от источника; также имеется лямбда-параметр (λ ≥ 0), который являет - ся параметром регуляризации. Для лучшего понимания того, как работает функция стоимости, необходимо разделить ее на две части. Первая часть – это член регуляризации: Член регуляризации делает процесс минимизации более явным, когда вектор w принимает высокие значения. Второй член называется составляющей потерь, или слабой переменной, и в действительности является ядром процедуры мини-мизации в алгоритме SVM: Составляющая потерь выдает приближенное значение ошибок классификации. По сути дела, суммирование будет стремиться добавлять единичную величину каждой ошибки классификации, общее количество которых, деленное на число примеров n, обеспечит приблизительную долю ошибки классификации. Очень часто, подобно тому, как это реализовано в Scikit-learn, параметр λ уби- рается из члена регуляризации и заменяется параметром неправильной класси-фикации C, который умножается на составляющую потерь: Связь между приведенным выше параметром λ и новым параметром C в сле- дующем: Это просто вопрос принятых правил, так как переход от параметра λ к парамет - ру C в формуле оптимизации не означает разных результатов. Влияние составляющей потерь опосредуется гиперпараметром С. Высокие зна- чения С налагают высокий штраф на ошибки, тем самым вынуждая алгоритм SVM пытаться правильно классифицировать все тренировочные примеры. Следова- тельно, большие значения С вынуждают зазор становиться более жестким и рас -\n--- Страница 95 ---\n94  Быстрообучающие ся реализации машин SVM сматривать меньшее число опорных векторов. Такого рода сокращение зазора транслируется в увеличенное смещение и уменьшенную дисперсию. Это приводит к необходимости конкретизации роли определенных наблюде- ний относительно других; фактически в качестве опорных векторов мы задаем те примеры, которые неправильно классифицированы или не классифицированы с уверенностью, что они лежат в зазоре (шумные наблюдения, которые делают раз-деление классов невозможным). Оптимизация становится возможной, принимая в расчет только такого рода примеры, что делает алгоритм SVM по-настоящему эффективным методом использования оперативной памяти: На приведенной выше визуализации можно заметить проекцию двух групп точек (синих и белых) на две размерности признаков. Вычислительное решение на основе алгоритма SVM с гиперпараметром С, установленным в 1.0, способно легко обнаруживать линию разделения (на графике она представлена как сплош-ная линия), несмотря на то что с обеих сторон имеется несколько неправильно классифицированных случаев. Кроме того, зазор можно визуализировать (он за-дан двумя пунктирными линиями); он идентифицируется благодаря опорным векторам соответствующего класса, лежащим дальше от разделяющей линии. На графике опорные векторы отмечены внешним кругом, при этом можно заметить, что некоторые опорные векторы в действительности лежат вне зазора; это вы-звано тем, что они являются неправильно классифицированными случаями, и ал-горитм SVM должен отслеживать их в целях оптимизации, поскольку их ошибка учитывается в составляющей потерь:\n--- Страница 96 ---\nМашины опорных векторов  95 При увеличении значения С зазор стремится сузиться, поскольку в процессе оптимизации алгоритм SVM принимает в расчет меньше опорных векторов. Сле- довательно, наклон разделяющей линии также изменяется. И наоборот, меньшие значения С имеют тенденцию к ослаблению зазора и тем самым к увеличению дисперсии. Крайне малые значения С могут даже привести к тому, что алгоритм SVM будет рассматривать в зазоре все прецедентные точки. Меньшие значения С идеальны, когда имеется много шумных примеров. Такие условия вынуждают алгоритм SVM игнорировать большое число неправильно классифицированных примеров в определении зазора (ошибки получают мень-ший вес, и поэтому при поиске максимального зазора терпимость к ним больше): Продолжая с предыдущим визуальным примером, если уменьшить гиперпа- раметр С, то зазор в действительности расширяется, потому что число опорных векторов увеличивается. Следовательно, раз зазор другой, алгоритм SVM прини-мает решение в пользу другой разделяющей линии. Какое-либо значение С, кото - рое можно расценивать как правильное до его проверки на данных, отсутствует; правильное значение нужно всегда находить эмпирически путем перекрестной проверки. Вне всякого сомнения, в алгоритме SVM гиперпараметр С считается са- мым важным, и его настройка выполняется после решения о том, какую ядерную функцию использовать. Вместе с тем ядерные функции отображают исходные признаки в более высо- коразмерное пространство путем их нелинейного комбинирования, в результате чего явно неразделимые группы в исходном пространстве признаков могут стать разделимыми в более высокоразмерном представлении. Такая проекция не тре-бует слишком сложных вычислений, несмотря на то что процесс явного преобра-зования исходных значений признаков в новые может генерировать потенциаль-ный взрыв в числе признаков при проецировании в высокие размерности. Вместо того чтобы делать такого рода громоздкие вычисления, в решающую функцию просто подключаются ядерные функции, тем самым заменяя исходное скаляр-ное произведение между признаками и вектором коэффициентов и получая тот же самый результат оптимизации, который имело бы явное отображение. (Такого рода подключение называется ядерным трюком, потому что на самом деле он представляет собой математический прием.) Стандартные ядерные функции – это линейные функции (не предполагающие никаких преобразований), полиномиальные функции, радиально-базисные\n--- Страница 97 ---\n96  Быстрообучающие ся реализации машин SVM функции (radial basis func tion, RBF) и сигмоидальные функции. Чтобы получить представление, функцию RBF можно выразить следующим образом: K(xi, x) = exp(–|| xi – x||2/2σ). В сущности, ядро RBF и другие ядра подключаются непосредственно в вариант ранее встречавшейся минимизируемой функции. Ранее встречавшаяся функция оптимизации называется первичной формулировкой, тогда как аналогичное пре-образованное выражение называется двойственной формулировкой: Хотя без математической демонстрации переход от первичной к двойственной формулировке довольно сложен, важно понимать, что если дана ядерная функ - ция, которая сравнивает примеры парами, то ядерный трюк является просто во- просом ограниченного количества вычислений относительно бесконечномерно-го пространства признаков, которое она может развернуть. Такого рода ядерный трюк выражает алгоритм с наибольшей эффективностью (сравнимой с нейрон-ными сетями) относительно довольно сложных задач, таких как распознавание изображений или текстовая классификация: Например, приведенное выше решение алгоритма SVM возможно благодаря сигмоидальному ядру, тогда как следующее является результатом ядра RBF:\n--- Страница 98 ---\nМашины опорных векторов  97 Как видно из графика, ядро RBF делает возможным довольно сложные опреде- ления зазора, и в том числе с его расщеплением на многочисленные части (в при- веденном выше примере можно заметить анклав). Формула ядра RBF следующая: k(xi, xj) = exp(–γ|| xi – xj||2). Гиперпараметр гамма γ задается априорно. Ядерное преобразование создает своего рода классификационные пузыри вокруг опорных векторов, тем самым позволяя определять очень сложные граничные фигуры путем слияния самих пу - зырей. Формула сигмоидального ядра следующая: k(xi, xj) = tanh(γ〈xi, xj〉2 + r). Здесь, кроме параметра гамма γ, для получения оптимального результата нуж - но также подобрать r. Безусловно, решения на основе сигмоидального, RBF- и полиномиального ядер (последнее неявно выполняет полиномиальное разложение, которое мы обсудим в следующих параграфах), обеспечивают дисперсию оценок больше, чем их сме-щение, тем самым требуя строгой проверки при принятии решения об их исполь-зовании. Хотя алгоритм SVM устойчив к переподгонке, он, разумеется, от нее не застрахован. Опорно-векторная регрессия связана с опорно-векторной классификацией. Она отличается только математической записью (более похожа на линейную ре-грессию, используя коэффициенты β вместо вектора коэффициентов w) и функ - цией потерь: Стоит отметить, что единственное значительное отличие – это функция потерь Lε, которая нечувствительна к ошибкам (и тем самым их не вычисляет), если при- меры лежат на определенном расстоянии эпсилон ε от гиперплоскости регрессии. Минимизация такой функции стоимости оптимизирует результат задачи регрес - сии, выдавая на выходе значения, а не классы. Кусочно-линейная функция потерь и ее варианты В качестве заключительных комментариев по поводу составных частей алгорит - ма SVM стоит напомнить, что лежащая в основе алгоритма функция стоимости – это кусочно-линейная функция потерь (hinge loss): loss(y, yˆ) = max(0,1 – yy ˆ). Как уже отмечалось ранее, yˆ выражается как сумма скалярного произведения X и вектора коэффициентов w со смещением b: yˆ = wX + b. Такая функция потерь напоминает персептрон и штрафует ошибки линейно, показывая ошибку, когда пример классифицирован на неправильной стороне\n--- Страница 99 ---\n98  Быстрооб учающиеся реализации машин SVM зазора, пропорционально его расстоянию от самого зазора. Хотя и являясь вы- пуклой, она имеет недостаток, который состоит в том, что она не может быть по-всюду дифференцируемой, и поэтому она иногда заменяется на всегда диффе-ренцируемые варианты, в частности квадратичную кусочно-линейную функцию (также именуемую функцией потерь L2, при этом функция потерь L1 – это кусоч-но-линейная функция потерь): L2_loss(y, yˆ) = max(0,1 – yy ˆ) 2. Другой вариант – функция потерь Хьюбера, т. е. квадратичная функция, когда ошибка равна определенному пороговому значению h, или ниже его, и линейная функция в противном случае. Такой подход смешивает варианты L1 и L2 кусоч-но-линейной функции потерь, основываясь на ошибке, и представляет собой до-статочно стойкую к выбросам альтернативу, поскольку более крупные значения ошибок не возводятся в квадрат, и тем самым требует от обучающегося алгорит - ма SVM меньшего количества корректировок. Функция потерь Хьюбера является также альтернативой для логарифмической функции потерь (линейные модели), потому что ее вычисление быстрее, и она способна обеспечить оценки вероятно-стей классов (кусочно-линейная функция не обладает такой способностью). С практической точки зрения нет каких-либо конкретных сообщений о том, что функция потерь Хьюбера либо квадратичная кусочно-линейная функция L2 мо-жет систематически выполняться лучше, чем просто кусочно-линейная функция. В конечном итоге процесс выбора функции стоимости просто сводится к тестиро-ванию имеющихся функций относительно каждой конкретной задачи обучения. (Согласно принципу теоремы об отсутствии бесплатных обедов no-free-lunch, в машинном обучении отсутствуют какие-либо решения, которые были бы при-емлемы для всех задач.) Объяснение реализации алгоритма SVM в Scikit-learn Библиотека Scikit-learn предлагает реализацию алгоритма SVM с использованием двух библиотек C++, разработанных в Национальном Тайваньском университе-те (с программным интерфейсом на C для взаимодействия с другими языками): библиотеки для машин опорных вект оров LIBSVM (librar y for support vector machines) для классификации и регрессии на основе алгоритма SVM (http://www.csie.ntu.edu.tw/~cjlin/libsvm/) и LIBLINEAR для задач классификации с использова-нием линейных методов для больших и разреженных наборов данных (http://www.csie.ntu.edu.tw/~cjlin/liblinear/). Обе библиотеки общедоступны, довольно быстры в расчетах и уже протестированы в большом количестве других решений, и все реализации Scikit-learn в модуле sklearn.svm опираются на одно или другое (кста- ти, классы Perceptron и LogisticRegression тоже их используют), превращая Python просто в удобную обертку. С другой стороны, классы SGDClassifier и SGDRegressor используют иную реа- лизацию, поскольку ни LIBSVM, ни LIBLINEAR не обладают онлайновой верси-ей, являясь инструментами пакетного обучения. По сути дела, во время работы LIBSVM и LIBLINEAR показывают наилучшую производительность, когда по-средством параметра cache_size для ядерных операций выделяется надлежащая память.\n--- Страница 100 ---\nМашины опорных векторов  99 Реализации для задачи классификации следующие: Класс Цель Гиперпараметры sklearn.svm.SVC LIBSVM-реализация для бинарной и мультиклассо- вой линейной и ядерной классификацийC, kernel, degree, gamma sklearn.svm.NuSVC То же, что и выше nu, kernel, degree, gamma sklearn.svm. OneClassSVM Неконтролируемое (без учителя) обнаружение выбросовnu, kernel, degree, gamma sklearn.svm.LinearSVC Бинарный и мультиклассовый линейный классификатор на основе LIBLINEARPenalty, loss, C Что касается задачи регрессии, решения такие: Класс Цель Гиперпараметры sklearn.svm.SVR LIBSVM-реализация для регрессии C, kernel, degree, gamma, epsilon sklearn.svm.NuSVR То же, что и выше nu, C, kernel, degree, gamma Как видно, имеется немалое количество гиперпараметров, которые требу - ют доводки для каждой версии алгоритма, что делает машины SVM хорошими учениками, когда эти параметры используются со значениями, заданными по умолчанию, и отличными учениками, когда эти параметры соответствующим образом от строены благодаря перекрестной проверке с использованием класса GridSearchCV из модуля grid_search1. Золотое правило гласит, что некоторые параметры влияют на результат боль- ше, и поэтому их следует зафиксировать заранее, а другие зависят от их значений. Согласно такому эмпирическому правилу, следует правильно установить следую-щие ниже параметры (упорядочены по важности): C: значение штрафа, который мы обсуждали ранее. Его уменьшение делает зазор больше, тем самым позволяя игнорировать больше шума и одновре-менно способствуя увеличению объема вычислений. Наилучшее значение, как правило, можно отыскать в интервале np.logspace(-3, 3, 7); kernel : наиболее интенсивно используемый параметр, «рабочая лошадка» нелинейности, потому что алгоритм SVM может быть установлен в linear , poly, rbf, sigmoid , либо собственное ядро (только для экспертов!). Широко ис - пользуемым является параметр rbf; degree : работает с kernel='poly' , сигнализируя о размерности полиномиаль- ного разложения. Игнорируется другими ядрами. Значения 2–5 обычно ра-ботают лучше всего; gamma : этот коэффициент предназначен для 'rbf' , 'poly' и 'sigmoid' ; высокие значения стремятся аппроксимировать данные лучше всего. Сеточный по-иск 2 предлагается выполнять в интервале np.logspace(-3, 3, 7); 1 Начиная с версии 0.18.0 библиотеки Scikit-learn класс GridSearchCV из модуля grid_search перемещен в модуль model_selection . – Прим. перев. 2 Сеточный поиск (от англ. grid search) – это исчерпывающий поиск оптимальной пара-метрическ ой конфигурации в массиве, заданном списком значений параметра либо словарем, ключом которого является имя параметра, а значением – серия проверяемых значений. – Прим. перев.\n--- Страница 101 ---\n100  Быстрооб учающиеся реализации машин SVM nu: этот параметр для регрессии и классификации используется с классами nuSVR и nuSVC; он аппроксимирует тренировочные точки, которые не были классифицированы с уверенностью, неправильно классифицированные точки и правильные точки внутри или на зазоре. Параметр должен быть выражен числом в интервале [0,1], поскольку этот параметр представляет собой долю относительно тренировочного набора. В конце концов, он дей-ствует как параметр С, когда высокие доли увеличивают зазор; epsilon : этот параметр определяет, сколько ошибок класс SVR собирается принять, и делается это путем определения интервала в размере эпси-лон ε, где никакой штраф не ассоциируется относительно истинного зна- чения точки. Предлагается выполнять его поиск в интервале np.insert(np. logspace(-4, 2, 7), 0, [0]); penalty , loss и dual: для LinearSVC эти параметры принимают сочетания ('l1', 'squared_hinge', False) , ('l2', 'hinge', True) , ('l2', 'squared_hinge', True) и ('l2', 'squared_hinge', False) . Сочетание ('l2', 'hinge', True) эквивалентно ученику SVC(kernel='linear') . В качестве примера базовой классификации и регрессии с использованием классов SVC и SVR из модуля sklearn.svm библиотеки Scikit-learn мы возьмем пару популярных миниатюрных наборов данных – Iris и Boston (http://scikit-learn.org/stable/datasets/). Сначала загрузим набор данных Iris: In: from sklearn import datasets iris = datasets.load_iris() X_i, y_i = iris.data, iris.target Затем выполним подгонку SVC ядром RBF (параметры С и γ были выбраны на основе других известных примеров в Scikit-learn) и протестируем результаты при помощи функции cross_val_score для расчета отметки точности после перекрест - ной проверки: In: import numpy as npfrom sklearn.svm import SVCfrom sklearn.model_selection import cross_val_score h_class = SVC(kernel='rbf', C=1.0, gamma=0.7, random_state=101) scores = cross_val_score(h_class, X_i, y_i, cv=20, scoring='accuracy') print('Точность: %0.3f' % np.mean(scores))Out: Точность: 0.969 Подобранная модель предоставит массив индексов, указывающий на опорные векторы среди тренировочных примеров: In: h_class.fit(X_i, y_i)print(h_class.support_) Out: [ 13 14 15 22 24 41 44 50 52 56 60 62 63 66 68 70\n--- Страница 102 ---\nМашины опорных векторов  101 72 76 77 83 84 85 98 100 106 110 114 117 118 119 121 123 126 127 129 131 133 134 138 141 146 149] Ниже приведено графическое представление опорных векторов, которые модель SVC выбрала для набора данных Iris. Оно представлено цветными границами реше- ния (мы протестировали дискретный массив значений, чтобы для каждой области диаграммы иметь возможность спроецировать, какой класс модель предскажет): Длина чашелистика (см) Длина чашелистика (см) Длина чашелистика (см)Длина лепестка (см)  Если вы интересуетесь тем, как воспроизвести приведенные выше графики, то можете взглянуть на фрагмент программного кода на http://scikit-learn.org/stable/auto_examples/ svm/plot_iris.html и настроить его под свои потребности. Чтобы протестировать регрессор алгоритма SVM, мы решили попробовать мо- дель SVR с набором данных Boston. Сначала мы закачиваем набор данных в опе- ративную память и затем рандомизируем порядок следования примеров, так как можно заметить, что подобный набор данных, если присмотреться повниматель-нее, в действительности упорядочен, тем самым делая результаты перекрестной проверки с нерандомизированным порядком следования недейст вительными: In: import numpy as npfrom sklearn.datasets import load_bostonfrom sklearn.preprocessing import StandardScaler scaler = StandardScaler() boston = load_boston()shuffled = np.random.permutation(boston.target.size)X_b = scaler.fit_transform(boston.data[shuffled,:])y_b = boston.target[shuffled]  Ввиду того, что мы использовали функцию permutation из модуля random библиотеки NumPy, вы можете получить набор данных, который будет перемешан по-другому, и поэтому при выполнении следующего ниже теста может оказаться немного другая отметка после пере-крестной проверки. Кроме того, если признаки находятся в разных шкалах, то их рекомен-дуется стандартизировать, чтобы они имели центрованное на нуле среднее и единичную дисперсию. В особенности при использовании алгоритма SVM с ядрами. В этом случае стан-дартизация действительно крайне важна. Наконец, можно выполнить подбор модели SVR (мы выбрали несколько пара - метров – C, гамма и эпсилон, которые, по нашим сведениям, отлично работают),\n--- Страница 103 ---\n102  Быстрооб учающиеся реализации машин SVM и, воспользовавшись перекрестной проверкой, оценили ее при помощи квадрат - ного корня из среднеквадратической ошибки (RMSE): In: from sklearn.svm import SVRfrom sklearn.model_selection import cross_val_score h_regr = SVR(kernel='rbf', C=20.0, gamma=0.001, epsilon=1.0) scores = cross_val_score(h_regr, X_b, y_b, cv=20, scoring='neg_mean_squared_error') print('Среднеквадратическая ошибка: %0.3f' % abs(np.mean(scores)))Out: Среднеквадратическая ошибка: 28.087 Поиск нелинейных SVM с привлечением подвыборки Машины SVM имеют довольно много преимуществ перед другими алгоритмами машинного обучения: могу т обрабатывать большинство задач с учителем, таких как регрессия, классификация и обнаружение аномалий, хотя в действительности они луч-ше всего проявляют себя в бинарной классификации; обеспечиваю т хорошую обработку шумных данных и выбросов и менее склонны к переподгонке, так как работают только с опорными векторами; хорошо работают с широкими наборами данных (больше признаков, чем примеров); хотя, как и с другими алгоритмами машинного обучения, алго-ритм SVM извлекает выгоду как из снижения размерности, так и из отбора признаков. Среди недостатков необходимо упомянуть следующие: пред лагают лишь оценочные значения, а не вероятности, если только не выполнить некую времязатратную и в вычислительном отношении интен-сивную калибровку вероятности посредством шкалирования Платта; масштабирую тся суперлинейно вместе с числом примеров. В частности, последний недостаток накладывает сильное ограничение на ис - пользование машин SVM для больших наборов данных. Метод оптимизации в яд - ре э того обучающегося алгоритма – квадратичное программирование – масшта- бируется в реализации Scikit-learn между вычислительной сложностью О(чис ло признаков*чис ло прецедентов^2) и О(число признаков*число прецедентов^3), серьезно ограничивая применимость алгоритма к наборам данных, которые име-ют объем до 10^4 случаев. И опять-таки, как отмечено в предыдущей главе, когда имеется пакетный алгоритм и слишком много данных, существует всего несколько вариантов для выбора: подвыборка, параллелизация и внеядерное обучение путем потоковой передачи данных. Подвыборка и параллелизация редко упоминаются как самые лучшие решения, тогда как потоковой передаче отдается предпочтение при реа-лизации машин SVM для решения крупномасштабных задач. Вместе с тем подвыборка, хотя и реже используется, достаточно просто реа- лизуется с привлечением резервуарного отбора, посредством которого можно быстро продуцировать случайные выборки из потоков, получаемых из набо-ров данных, и бесконечных онлайновых потоков. Благодаря подвыборке можно производить многочисленные модели SVM, чьи результаты могут усредняться\n--- Страница 104 ---\nМашины опорных векторов  103 для получения более высоких общих результатов. Полученные из многочислен- ных моделей SVM прогнозы могут даже каскадироваться (или накладываться – stacked), тем самым создавая новый набор данных, и использоваться для созда-ния новой модели с объединением прогнозных способностей каждой отдельной модели. Этот механизм будет описан в главе 6 « Классификационные и регрессион- ные деревья в крупном масштабе». Резервуарный отбор (reservoir sampling) – это алгоритм для случайного выде- ления выборок из потока без предварительной информации о длине потока. По сути дела, каждое наблюдение в потоке имеет одинаковую вероятность быть вы-бранным. Он инициализируется выборкой, извлекаемой из первых наблюдений в потоке, и далее каждый элемент в выборке в любой момент может быть заменен примером из потока согласно вероятности, пропорциональной числу элементов, переданных к настоящему моменту. Так, например, когда поступает i-й элемент потока, он имеет вероятность быть вставленным вместо случайного элемента в вы-борке. Такая вероятность вставки эквивалентна размерности выборки, деленной на i; поэтому она прогрессивно уменьшается относительно длины потока. Если поток бесконечен, то остановка в любое время гарантирует, что выборка является пред-ставительной для элементов, которые были просмотрены к настоя щему моменту . В нашем примере мы берем две случайные взаимоисключающие выборки из потока – одна для тренировки, другая для тестирования. Мы извлечем такого рода выборки из базы данных Covertype, используя исходный упорядоченный файл. (Поскольку мы будем передавать все данные потоком до извлечения выборки, случайный отбор не будет затронут упорядочением.) Мы остановились на трени-ровочной выборке в размере 5000 примеров – это количество должно хорошо мас - штабироваться на большинстве настольных компьютеров. Что касается тестового набора, то мы будем использовать 20 000 примеров: In: from random import seed, randint SAMPLE_COUNT = 5000 TEST_COUNT = 20000 seed(0) # обеспечивает воспроизводимость результатовsample = list() test_sample = list() for index, line in enumerate(open('data\\\\covtype.data','rb')): if index < SAMPLE_COUNT: sample.append(line) else: r = randint(0, index) if r < SAMPLE_COUNT: sample[r] = line else: k = randint(0, index) if k < TEST_COUNT: if len(test_sample) < TEST_COUNT: test_sample.append(line) else: test_sample[k] = line\n--- Страница 105 ---\n104  Быстрооб учающиеся реализации машин SVM Алгоритм должен обеспечивать достаточно быструю потоковую передачу по- рядка на 500 000 строках матрицы данных. Для того чтобы обеспечить максималь- ное быстродействие потока, во время потоковой передачи мы, в сущности, не де- лали никакой предобработки. Следовательно, теперь мы должны преобразовать данные в массив NumPy и стандартизировать признаки: In: import numpy as npfrom sklearn.preprocessing import StandardScaler for n,line in enumerate(sample): sample[n] = map(float,line.strip().split(',')) y = np.array(sample)[:,-1] scaling = StandardScaler()X = scaling.fit_transform(np.array(sample)[:,:-1]) Закончив работу с тренировочными данными X, y, необходимо таким же обра- зом обработать тестовые данные; в частности, стандартизировать признаки, ис - пользуя такие же параметры стандартизации (средние значения и стандартные отклонения), что и в тренировочной выборке: In: for n,line in enumerate(test_sample): test_sample[n] = map(float,line.strip().split(','))yt = np.array(test_sample)[:,-1]Xt = scaling.transform(np.array(test_sample)[:,:-1]) Когда оба набора – тренировочный и тестовый – готовы, можно построить мо- дель SVC и предсказать результаты: In: from sklearn.svm import SVCfrom sklearn.metrics import accuracy_score h = SVC(kernel='rbf', C=250.0, gamma=0.0025, random_state=101) h.fit(X,y)prediction = h.predict(Xt) print(accuracy_score(yt, prediction))Out: 0.75205 Реализация SVM в крупном масштабе на основе SGD Учитывая недостатки подвыборки (в первую очередь недоподгонку относитель- но моделей, тренируемых на крупных наборах данных), единственным вари-антом при использовании Scikit-learn для применяемых к крупномасштабным потокам линейных машин SVM остается использование методов SGDClassifier и SGDRegressor , оба из которых доступны в модуле linear_model . Давайте посмотрим, как их можно применить оптимальным образом и как улучшить результаты на демонстрационных наборах данных. Для этой цели мы задействуем примеры, рассмотренные ранее в главе, посвя- щенной линейной и логистической регрессиям, и преобразуем их в эффективную\n--- Страница 106 ---\nМашины опорных векторов  105 модель SVM. Если говорить о классификации, то при помощи гиперпараметра loss требуется задать тип функции потерь. Значениями этого параметра могут быть 'hinge' , 'squared_hinge' и 'modified_huber' . Все эти функции потерь уже были представлены и обсуждались в настоящей главе при рассмотрении формулы ал- горитма SVM. Все они предполагают применение линейного алгоритма SVM с мягким за- зором (без ядер), тем самым приводя к модели SVM, которая невосприимчива к неправильным классификациям и шумным данным. Впрочем, можно также по-пробовать использовать функцию потерь 'perceptron' ; этот тип функции потерь ведет к кусочно-линейной функции без зазора. Такое решение приемлемо, когда необходимо обратиться к модели с бóльшим смещением, чем у других возможных вариантов функций потерь. Для получения оптимальных результатов при использовании такого набора ку - сочно-линейных функций потерь следует принять во внимание два аспекта: ког да используется любая функция потерь, стохастический градиентный спуск становится ленивым, причем вектор коэффициентов обновляется, только когда пример нарушает ранее определенные зазоры. Это совершенно противоречит логарифмической либо квадратической функции потерь, когда для обновления вектора коэффициентов в действительности рассмат ри вается каждый пример. В случае если в обучении участвует много признаков, такой ленивый подход в результате приводит к более разреженному вектору коэф-фициентов, тем самым уменьшая переподгонку (более плотный вектор вле-чет за собой бóльшую переподгонку, потому что некоторые коэффициенты, скорее всего, захватят больше шума, чем сигналы из данных); функция 'modified_huber' – это единственная функция потерь, которая допус кает вероятностное оценивание, что (как показывает стохастическая логистическая регрессия) делает ее приемлемой альтернативой для лога-рифмической функции потерь. Модифицированная функция Хьюбера так - же показывает более совершенные результаты при обработке мультиклассо-вых предсказаний по схеме «один против все х», поскольку вероятностные результаты множественных моделей лучше, чем свойство стандартных ре-шающих функций кусочно-линейных функций потерь (вероятности рабо-тают лучше, чем необработанный результат решающих функций, так как они находятся в одинаковой шкале в интервале от 0 до 1). Эта функция по-терь работает путем получения оценочного значения вероятности непо-средственно из решающей функции: (clip(decision_function(X), -1, 1) + 1) / 2. Что касается задач регрессии, то класс SGDRegressor предлагает два варианта функций потерь для SVM: 'epsilon_insensitive' 'squared_epsilon_insensitive' Обе активируют регрессию с линейными опорными векторами, где ошибки (остаток из прогноза) в значении эпсилон игнорируются. Помимо значения эпси-лон, функция потерь epsilon_insensitive рассматривает ошибку как есть. Функция потерь squared_epsilon_insensitive работает схожим образом, хотя здесь ошибка штрафуется больше, так как она возводится в квадрат, причем бóльшие ошибки влияют на построение модели больше.\n--- Страница 107 ---\n106  Быстрооб учающиеся реализации машин SVM В обоих случаях критически важно задание правильного гиперпараметра эп- силон ε. В качестве значения по умолчанию Scikit-learn предлагает epsilon=0.1 , но оптимальное для вашей задачи значение должно быть найдено в результате сеточного поиска, поддерживаемого перекрестной проверкой, как мы убедимся в следующих абзацах. Отметим, что среди регрессионных функций потерь также имеется функция 'huber' , которая не активирует свойственную для алгоритма SVM оптимизацию, а просто модифицирует обычную квадратическую функцию потерь 'squared_loss' , чтобы стать нечувствительной к выбросам, переключившись с квадратичной на линейную меру потерь за пределами значения расстояния параметра эпсилон. Если говорить о наших примерах, то мы повторим процесс потоковой переда- чи определенное число раз, чтобы продемонстрировать, как устанавливать разные гиперпараметры и преобразовывать признаки, а для сокращения числа повторяю - щихся строк программного кода воспользуемся вспомогательными функциями. Кроме того, чтобы ускорить выполнение примеров, мы установим ограничение на число случаев или значения терпимости (допуска), на которые алгоритм ссылается. Таким способом показатели времени тренировки и проверки будут оставаться на минимуме, и любой пример займет не больше, чем время на чашку чая или кофе. Что касается вспомогательных оберточных функций, то первая из них будет предназначаться для первоначальной потоковой передачи части либо всех дан-ных сразу (предел устанавливается при помощи параметра max_rows ). После за- вершения потоковой передачи функция сможет выяснить все уровни катего - риальных признак ов и записать различные интервалы числовых признаков. Как напоминание запись интервалов является важным аспектом, которому следует уделить внимание. Алгоритмы SGD и SVM чувствительны к шкалам разных ин-тервалов, и они работают хуже с числом, лежащим за пределами интервала [–1,1]. На выходе наша функция будет возвращать два натренированных объекта Scikit-learn: DictVectorizer (способный преобразовывать присутствующие в слова- ре интервалы признаков в вектор признаков) и MinMaxScaler , который перешкали- рует числовые переменные в интервал [0,1] (пригодный для хранения значений в наборе данных в разреженном виде, тем самым поддерживая использование памяти на низком уровне и достигая быстрых вычислений, когда большинство значений равно 0). В качестве уникального ограничения необходимо знать имена признаков числовых и категориальных переменных, которые вы хотите исполь-зовать для прогнозной модели. Признаки, не включенные в список для передачи параметрам binary_features или numeric_features , будут в действительности про- игнорированы. Когда поток не имеет имен признаков, вам необходимо дать им имена, используя для этого параметр fieldnames : In: import csv, time, osimport numpy as npfrom scipy.sparse import csr_matrixfrom sklearn.linear_model import SGDRegressorfrom sklearn.feature_extraction import DictVectorizerfrom sklearn.preprocessing import MinMaxScaler def explore(target_file, separator=',', fieldnames= None, binary_features=list(), numeric_features=list(),\n--- Страница 108 ---\nМашины опорных векторов  107 max_rows=20000): \"\"\" Генерировать из онлайнового потока DictVectorizer и MinMaxScaler. Параметры --------- target file = файл, из которого организовать потоковую передачу separator = символ разграничения полей fieldnames = метки полей (можно пропустить и читать из файла) binary_features = список качественных признаков для рассмотрения numeric_features = список числовых признаков для рассмотрения max_rows = число строк, читаемых из потока (может быть пустым) \"\"\" features = dict() min_max = dict() vectorizer = DictVectorizer(sparse=False) scaler = MinMaxScaler() with open(target_file, 'rb') as R: iterator = csv.DictReader(R, fieldnames, delimiter=separator) for n, row in enumerate(iterator): # ИССЛЕДОВАНИЕ ДАННЫХ for k,v in row.iteritems(): if k in binary_features: if k+'_'+v not in features: features[k+'_'+v]=0 elif k in numeric_features: v = float(v) if k not in features: features[k]=0 min_max[k] = [v,v] else: if v < min_max[k][0]: min_max[k][0]= v elif v > min_max[k][1]: min_max[k][1]= v else: pass # игнорировать признак if max_rows and n > max_rows: break vectorizer.fit([features]) A = vectorizer.transform([{f:0 if f not in min_max else min_max[f][0] for f in vectorizer.feature_names_}, {f:1 if f not in min_max else min_max[f][1] for f in vectorizer.feature_names_}]) scaler.fit(A) return vectorizer, scaler  Этот фрагмент исходного кода можно легко использовать повторно для своих собственных прило жений машинного обучения на крупномасштабных данных. В случае если поток он- лайновый (непрерывная потоковая передача) или слишком длинный, то путем установки параметра max_rows можно применить другой предел на число наблюдаемых примеров.\n--- Страница 109 ---\n108  Быстрооб учающиеся реализации машин SVM Вторая функция, напротив, будет просто извлекать данные из потока и преоб- разовывать их в вектор признаков, нормализуя числовые признаки, если вместо значения None будет предоставлен подходящий объект MinMaxScaler : In: def pull_examples(target_file, vectorizer, binary_features, numeric_features, target, min_max=None, separator=',', fieldnames=None, sparse=True): \"\"\" Читает онлайновый поток и возвращает генератор нормализованных векторов признаков Параметры ---------- target file = файл, из которого организовать потоковую передачу vectorizer = объект DictVectorizer binary_features = список качественных признаков для рассмотрения numeric_features = список числовых признаков для рассмотрения target = метка переменной отклика min_max = объект MinMaxScaler, можно пропустить, установив в None separator = символ разделения полей fieldnames = метки полей (можно пропустить и читать из файла) sparse = будет ли возвращен из генератора разреженный вектор \"\"\" with open(target_file, 'rb') as R: iterator = csv.DictReader(R, fieldnames, delimiter=separator) for n, row in enumerate(iterator): # ОБРАБОТКА ДАННЫХ stream_row = {} response = np.array([float(row[target])]) for k,v in row.iteritems(): if k in binary_features: stream_row[k+'_'+v]=1.0 else: if k in numeric_features: stream_row[k]=float(v) if min_max: features = min_max.transform( vectorizer.transform( [stream_row])) else: features = vectorizer.transform([stream_row]) if sparse: yield(csr_matrix(features), response, n) else: yield(features, response, n) Располагая этими двумя функциями, теперь попробуем еще раз смоделировать первую задачу регрессии из предыдущей главы с набором данных Bike-sharing об аренде велосипедов, но на этот раз с использованием кусочно-линейной функции вместо функции среднеквадратических ошибок модели, которую мы использова-ли ранее. В качестве первого шага мы передаем в поток имя файла и список качествен- ных и числовых переменных (взятых из заголовка файла и в результате его перво-\n--- Страница 110 ---\nМашины опорных векторов  109 начального исследования). Программный код нашей оберточной функции вернет информацию о прямокодированных переменных и интервалах значений. В этом случае большинство переменных будет двоичными единицами, что является идеаль ной сит уацией для разреженного представления, поскольку большинство значений в наборе данных нулевые: In: local_path = os.getcwd().decode('cp1251')source = '\\\\data\\\\bikesharing\\\\hour.csv' b_vars = ['holiday','hr','mnth', 'season', 'weathersit','weekday','workingday','yr']n_vars = ['hum', 'temp', 'atemp', 'windspeed'] std_row, min_max = explore(target_file=local_path+'\\\\'+source, binary_features=b_vars, numeric_features=n_vars) print('Признаки: ') for f,mv,mx in zip(std_row.feature_names_, min_max.data_min_, min_max.data_max_): print '%s:[%0.2f,%0.2f] ' % (f,mv,mx) Out: Признаки: atemp:[0.00,1.00] holiday_0:[0.00,1.00] holiday_1:[0.00,1.00] workingday_1:[0.00,1.00]yr_0:[0.00,1.00]yr_1:[0.00,1.00] Как можно заметить по итоговому результату, качественные переменные были закодированы с использованием их имен и добавления их значения после симво- ла подчеркивания и затем преобразованы в двоичные признаки (которые имеют значение 1, когда признак присутствует, и 0 в противном случае). Отметим, что мы всегда используем модели SGD с параметром average=True , чтобы обеспечить более быструю сходимость (это соответствует использованию модели усреднен- ного ст охастического градиентного спуска (averaged stochastic gradient descent, ASGD), как уже обсуждалось в предыдущей главе): In:from sklearn.linear_model import SGDRegressor SGD = SGDRegressor(loss='epsilon_insensitive', epsilon=0.001, penalty=None, random_state=1, average=True)val_rmse = 0val_rmsle = 0predictions_start = 16000 def apply_log(x): return np.log(x + 1.0)\n--- Страница 111 ---\n110  Быстрооб учающиеся реализации машин SVM def apply_exp(x): return np.exp(x) - 1.0 for x,y,n in pull_examples(target_file=local_path+'\\\\'+source, vectorizer=std_row, min_max=min_max, binary_features=b_vars, numeric_features=n_vars, target='cnt'): y_log = apply_log(y) # МАШИННОЕ ОБУЧЕНИЕ if (n+1) >= predictions_start: # ФАЗА С ОТКЛАДЫВАНИЕМ ПОСЛЕ N predicted = SGD.predict(x) val_rmse += (apply_exp(predicted) - y)**2 val_rmsle += (predicted - y_log)**2 if (n-predictions_start+1) % 250 == 0 and (n+1) > predictions_start: print(n), print('%s RMSE на отложенных данных: %0.3f,' % (time.strftime('%X'), (val_rmse / float(n-predictions_start+1))**0.5)), print('то же самое RMSLE: %0.3f' % ((val_rmsle / float(n-predictions_start+1))**0.5)) else: # ФАЗА ОБУЧЕНИЯ SGD.partial_fit(x, y_log)print('%s ИТОГОВАЯ RMSE на отложенных данных: %0.3f' % (time.strftime('%X'), (val_rmse / float(n-predictions_start+1))**0.5))print('%s ИТОГОВАЯ RMSLE на отложенных данных: %0.3f' % (time.strftime('%X'), (val_rmsle / float(n-predictions_start+1))**0.5)) Out: 16249 22:11:39 RMSE на отложенных данных: 276.604, то же самое RMSLE: 1.79616499 22:11:39 RMSE на отложенных данных: 250.419, то же самое RMSLE: 1.70616749 22:11:39 RMSE на отложенных данных: 250.639, то же самое RMSLE: 1.69416999 22:11:40 RMSE на отложенных данных: 249.561, то же самое RMSLE: 1.70217249 22:11:40 RMSE на отложенных данных: 234.840, то же самое RMSLE: 1.64022:11:40 ИТОГОВАЯ RMSE на отложенных данных: 224.40422:11:40 ИТОГОВАЯ RMSLE на отложенных данных: 1.594 А теперь попробуем решить задачу классификации с набором данных Covertype о лесном покрове: In: local_path = os.getcwd().decode('cp1251')source = 'data\\\\shuffled_covtype.data' n_vars = ['var_'+'0'*int(j<10)+str(j) for j in range(54)] std_row, min_max = explore(target_file=local_path+'\\\\'+source, binary_features=list(), fieldnames=n_vars+['covertype'], numeric_features=n_vars, max_rows=50000)\n--- Страница 112 ---\nМашины опорных векторов  111 print('Признаки: ') for f,mv,mx in zip(std_row.feature_names_, min_max.data_min_, min_max.data_max_): print('%s:[%0.2f,%0.2f] ' % (f,mv,mx)) Out: Признаки:var_00:[1871.00,3853.00]var_01:[0.00,360.00]var_02:[0.00,61.00]var_03:[0.00,1397.00]var_04:[-164.00,588.00]var_05:[0.00,7116.00]var_06:[58.00,254.00]var_07:[0.00,254.00]var_08:[0.00,254.00]var_09:[0.00,7168.00] После выборки из потока и настройки объектов DictVectorizer и MinMaxScaler можно запустить процесс обучения, используя на этот раз прогрессивную про-верку (меру ошибки получают путем тестирования модели на прецедентах, перед тем как они будут использованы для тренировки), при условии что имеется боль-шое количество примеров. Через каждое определенное число примеров, которое программно установлено в переменной sample , сценарий сообщает о ситуации по- казателем средней точности на самых последних примерах: In: from sklearn.linear_model import SGDClassifier SGD = SGDClassifier(loss='hinge', penalty=None, random_state=1, average=True) accuracy = 0 accuracy_record = list()predictions_start = 50sample = 5000early_stop = 50000 for x,y,n in pull_examples(target_file=local_path+'\\\\'+source, vectorizer=std_row, min_max=min_max, binary_features=list(), numeric_features=n_vars, fieldnames=n_vars+['covertype'], target='covertype'): # ФАЗА ОБУЧЕНИЯ if n > predictions_start: SGD.partial_fit(x, y, classes=range(1,8)) accuracy += int(int(SGD.predict(x))==y[0]) if n % sample == 0: accuracy_record.append(accuracy / float(sample))\n--- Страница 113 ---\n112  Быстрооб учающиеся реализации машин SVM print('%s Прогрессивная точность в примере %i: %0.3f' % (time.strftime('%X'), n, np.mean(accuracy_record[-sample:]))) accuracy = 0 if early_stop and n >= early_stop: break Out: 19:23:49 Прогрессивная точность в примере 50000: 0.699  Необходимо обработать свыше 575 000 примеров, однако после 50 000 примеров мы де- лаем в процессе обучения раннюю остановку. Вы свободны изменить эти параметры, исхо- дя из мощности вашего компьютера и наличия времени. Предупреждаем, что выполнение программного кода может занять достаточно продолжительное время. По нашему опыту на процессоре Intel Core i3 с тактовой частотой 2,20 ГГц вычисление заняло приблизительно 30 мин. ОтбОр признак Ов пОсредствОм регуляризации В пакетном контексте общепринято управлять отбором признаков следующим образом: предварит ельная фильтрация на основе полноты данных (распространен- ность пропущенных значений), дисперсия и высокая мультиколлинеар-ность между переменными, для того чтобы иметь более чистый набор дан-ных с соответствующими и операбельными признаками; еще о дна начальная фильтрация на основе одномерной связи (статистиче- ская проверка хи-квадрат, F-значение и простая линейная регрессия) меж - ду признаками и переменной отклика для немедленного удаления беспо-лезных для задачи прогнозирования признаков, так как они мало связаны с откликом или не связаны вообще; рекурсивный по дход во время моделирования со вставкой и/или исклю- чением признаков, исходя из их способности улучшить прогнозирующую способность тестируемого на отложенной выборке алгоритма. Использо-вание меньшего подмножества только релевантных признаков позволяет алгоритму машинного обучения быть менее подверженным переподгонке из-за шумных переменных и избыточных параметров вследствие высокой размерности признаков. Применение такого рода подходов в онлайновых условиях, разумеется, по-преж - нему возмо жно, но довольно дорогостояще с точки зрения требующегося времени из-за количества потоковых данных для завершения одиночной модели. Рекурсив-ные подходы, основанные на большом количестве итераций и проверок, требуют гибкого набора данных, который может уместиться в памяти. Как упоминалось ра-нее, в этом случае хорошим вариантом была бы подвыборка, что позволит выяснить признаки и модели для применения в более крупном масштабе в дальнейшем. Если придерживаться внеядерного подхода, то регуляризация является идеаль - ным решением, как способ отбора переменных во время потоковой передачи и фильтрации шумных или избыточных признаков. Регуляризация хорошо ра-ботает с онлайновыми алгоритмами, поскольку она действует так, как работает онлайновый алгоритм машинного обучения, и выполняет подгонку своих ко-\n--- Страница 114 ---\nОтбор признаков посредством регуляризации  113 эффициентов на примерах без какой-либо потребности запускать для целей от - бора другие потоки. Собственно, регуляризация – это просто значение штрафа, которое добавляется к оптимизации процесса обучения. Его значение зависит от коэффициента признака и параметра альфа α, задающего влияние регуляриза- ции. Регуляризационное выравнивание вступает в силу, когда модель обновляет веса коэффициентов. В это время регуляризация действует путем сокращения результирующих весов, если значение обновления недостаточно большое. Эф-фект исключения или ослабления избыточных переменных достигается за счет параметра регуляризации альфа, который должен быть установлен эмпирически в правильную величину для получения наилучшего результата относительно каж - дого конкретного элемента данных, который будет усвоен. Алгоритм SGD реализует те же самые стратегии регуляризации, которые можно найти в пакетных алгоритмах: подталкивание к нулю избыточных и не столь существенных переменных за счет штрафа L1-регуляризации; уменьшение веса менее важных признаков за с чет L2-регуляризации; смешивание эффекта о т регуляризации L1 и L2 методом эластичной сети. Регуляризация L1 является идеальной стратегией, когда имеются необычные и избыточные переменные, так как она подталкивает коэффициенты таких при-знаков к нулю, делая их нерелевантными при вычислении прогноза. Регуляризация L2 подходит, когда между переменными имеется много корреля- ций, так как ее стратегия состоит лишь в уменьшении весов тех признаков, вариа-ция которых менее важна для минимизации функции потерь. С L2 все переменные продолжают вносить свой вклад в прогноз, правда, некоторые немного меньше. Эластичная сеть смешивает L1 и L2, используя взвешенную сумму. Это реше- ние интересно тем, что иногда регуляризация L1 нестабильна во время работы с очень коррелированными переменными, выбирая одну или другую относитель-но наблю даемых примеров. Используя ElasticNet , многие необычные признаки будут по-прежнему подталкиваться к нулю, как при регуляризации L1, но корре-лируемые будут ослабляться, как в регуляризации L2. Классы SGDClassifier и SGDRegressor могут реализовывать регуляризацию L1, L2 и методом эластичной сети при помощи параметров penalty , alpha и l1_ratio .  Параметр alpha является самым критическим параметром после решения о виде штрафа или смеси из обоих. В идеальном случае подходящие значения можно протестировать в ин- тервале от 0.1 до 10^-7 , воспользовавшись списком значений, создаваемых при помощи 10.0**-np.arange(1,7) . Если параметр penalty определяет, какой вид регуляризации выбран, то пара- метр alpha , как уже упомянуто, будет определять ее силу. Поскольку alpha – это константа, которая умножается на штрафную составляющую, низкие значения этого параметра повлияют на итоговый коэффициент незначительно, тогда как его высокие значения окажут на него сильное влияние. Наконец, коэффициент l1_ratio показывает, при penalty='elasticnet' , какой процент приходится на L1- штраф относительно L2-штрафа. Настройка регуляризации с алгоритмом SGD очень простая. Например, можно попробовать изменить предыдущий пример исходного кода, вставив в SGDClas- sifier штраф L2:\n--- Страница 115 ---\n114  Быстрооб учающиеся реализации машин SVM SGD = SGDClassifier(loss='hinge', penalty='l2', alpha= 0.0001, random_state=1, average=True) Если вы желаете протестировать смешивание эффектов двух подходов регуля- ризации методом эластичной сети, то вам нужно явно указать только соотноше- ние между L1 и L2, задав коэффициент l1_ratio : SGD = SGDClassifier(loss=''hinge'', penalty=''elasticnet'', alpha= 0.001, l1_ratio=0.5, random_state=1, average=True) Поскольку успех регуляризации зависит от включения правильного вида штрафа и наилучшего значения параметра alpha , мы посмотрим на регуляризацию в дей- ствии на примерах во время решения задачи гиперпараметрической оптимизации. дОбавление нелинейнОсти в алгОритм SGD Самый быстрый (и, в сущности, не такой уж и сложный) способ добавить нелиней- ность к линейному SGD-ученику состоит в том, чтобы преобразовать полученный из потока вектор примера в новый вектор, включая степенные преобразования и сочетание признаков вплоть до определенной степени. Сочетания могут представлять взаимодействия между признаками (объясняя, когда два признака согласованно оказывают особое воздействие на отклик), тем самым помогая включать в линейную модель SVM определенную величину не-линейности. Например, двухстороннее взаимодействие создается в результате перемножения двух признаков, трехстороннее – в результате перемножения трех признаков и т. д., создавая еще более сложные взаимодействия для разложений более высокой степени. В библиотеке Scikit-learn модуль предобработки содержит класс PolynomialFea - tures , который может автоматически преобразовывать вектор признаков полино- миальным разложением в нужной степени: In: from sklearn.linear_model import SGDRegressorfrom sklearn.preprocessing import PolynomialFeatures local_path = os.getcwd() source = '\\\\data\\\\bikesharing\\\\hour.csv' b_vars = ['holiday','hr','mnth', 'season', 'weathersit','weekday','workingday','yr']n_vars = ['hum', 'temp', 'atemp', 'windspeed'] std_row, min_max = explore(target_file=local_path+'\\\\'+source, binary_features=b_vars, numeric_features=n_vars) poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)SGD = SGDRegressor(loss='epsilon_insensitive', epsilon=0.001, penalty=None, random_state=1, average=True) val_rmse = 0 val_rmsle = 0predictions_start = 16000\n--- Страница 116 ---\nДобавление нелинейности в алгоритм SGD  115 def apply_log(x): return np.log(x + 1.0) def apply_exp(x): return np.exp(x) - 1.0 for x,y,n in pull_examples(target_file=local_path+'\\\\' +source,vectorizer=std_row, min_max=min_max, sparse = False, binary_features=b_vars, numeric_features=n_vars, target='cnt'): y_log = apply_log(y) # Извлечь только качественные признаки и разложить их num_index = [j for j, i in enumerate(std_row.feature_names_) if i in n_vars] x_poly = poly.fit_transform(x[:,num_index])[:,len(num_index):] new_x = np.concatenate((x, x_poly), axis=1) # МАШИННОЕ ОБУЧЕНИЕ if (n+1) >= predictions_start: # ФАЗА С ОТКЛАДЫВАНИЕМ ПОСЛЕ N predicted = SGD.predict(new_x) val_rmse += (apply_exp(predicted) - y)**2 val_rmsle += (predicted - y_log)**2 if (n-predictions_start+1) % 250 == 0 and (n+1) > predictions_start: print(n), print('%s RMSE на отложенных данных: %0.3f,' % (time.strftime('%X'), (val_rmse / float(n-predictions_start+1))**0.5)), print('то же самое RMSLE: %0.3f' % ((val_rmsle / float(n-predictions_start+1))**0.5)) else: # ФАЗА ОБУЧЕНИЯ SGD.partial_fit(new_x, y_log)print('%s ИТОГОВАЯ RMSE на отложенных данных: %0.3f' % (time.strftime('%X'), (val_rmse / float(n-predictions_start+1))**0.5))print('%s ИТОГОВАЯ RMSLE на отложенных данных: %0.3f' % (time.strftime('%X'), (val_rmsle / float(n-predictions_start+1))**0.5)) Out: 21:49:24 ИТОГОВАЯ RMSE на отложенных данных: 219.19121:49:24 ИТОГОВАЯ RMSLE на отложенных данных: 1.480  PolynomialFeatures ожидает на входе плотную матрицу, неразреженную. Функция pull_ examples принимает параметр sparse , который обычно устанавливается в True, но который может быть установлен в False , тем самым возвращая плотную матрицу. Испытание явных высокоразмерных отображений Несмотря на то что полиномиальные разложения являются довольно мощным преобразованием, они могут оказаться дорогостоящими в вычислительном пла-не, когда мы пытаемся выполнить разложение с высокими степенями, и быстро нивелируют положительные эффекты захвата важной нелинейности из-за пере-подгонки, вызванный сверхпараметризацией (когда имеется слишком много из-\n--- Страница 117 ---\n116  Быстрооб учающиеся реализации машин SVM быточных и неполезных признаков). Как уже было отмечено при рассмотрении классов SVC и SVR библиотеки Scikit-learn, в этих случаях на помощь приходят ядер- ные преобразования. Ядерные преобразования алгоритма SVM, будучи неявны-ми, для своей работы требуют наличия в оперативной памяти матрицы данных. В библиотеке Scikit-learn имеется класс преобразований, основанный на случай-ных приближениях, которые в контексте линейной модели могут достигать очень похожих результатов, что и ядерный алгоритм SVM. Модуль sklearn.kernel_approximation содержит несколько таких алгоритмов: RBFSampler : аппроксимирует карту признаков ядра RBF; Nystroem : аппроксимирует карту ядра, используя подмножество тренировоч- ных данных; AdditiveChi2Sampler : аппроксимирует карту признаков для аддитивного ядра хи-квадрат; это ядро используется в компьютерном зрении; SkewedChi2Sampler : аппроксимирует карту признаков аналогично скошенно- му ядру хи-квадрат; также используется в компьютерном зрении. Кроме метода Nystroem, ни один из упомянутых выше классов не требует обуче ния из выборки данных, что делает их идеально подходящими для онлай- нового обучения. Единственное, что они должны знать, – это каким образом век - тор с примером сформирован (сколько в нем признаков), и затем они производят большое число случайных нелинейностей, которые, возможно, хорошо впишутся в вашу задачу обработки данных. В этих алгоритмах аппроксимации нет каких-то сложных оптимизационных алгоритмов, которые требуют объяснения; фактически сама оптимизация заме-нена рандомизацией, и результаты во многом зависят от числа выходных призна-ков, на которые указывают параметры n_components . Чем больше выходных при- знаков, тем выше вероятность, что случайно вы получите нужную нелинейность, идеально работающую с вашей задачей. Важно отметить, что если случай действительно играет очень существенную роль в создании нужных признаков, которые улучшают предсказание, то вос - производимость результатов приобретает крайне важное значение, и вы долж - ны стремиться ее получить; в противном случае вы не сможете последовательно перетренировывать и тонко настраивать алгоритм одинаковым образом. При-мечательно, что каждый класс обеспечен параметром random_state , тем самым позволяя управлять генерированием случайного признака и давая возможность воссоздавать его позже на том же компьютере, как и на других. Теоретические основы таких методов создания признаков объяснены в научных статьях Random Features for Large-Scale Kernel Machines («Случайные признаки круп- номасштабных ядерных машин»), A. Rahimi и Benjamin Recht (http://www.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf) и Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning («Взвешенные суммы случайно генери-руемых признаков: замена минимизации рандомизацией в обучении»), A. Rahimi и Benjamin Recht (http://www.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf). Для наших целей этого будет достаточно, чтобы знать, как реализовать и приме- нить этот метод для улучшения моделей SGD, как линейных, так и на основе SVM: In: from sklearn.linear_model import SGDClassifierfrom sklearn.kernel_approximation import RBFSampler\n--- Страница 118 ---\nДоводка гиперпараметров  117 local_path = os.getcwd() source = 'data\\\\shuffled_covtype.data' n_vars = ['var_'+str(j) for j in range(54)]std_row, min_max = explore(target_file=local_path+'\\\\'+source, binary_features=list(), fieldnames= n_vars+['covertype'], numeric_features=n_vars, max_rows=50000) SGD = SGDClassifier(loss='hinge', penalty=None, random_state=1, average=True)rbf_feature = RBFSampler(gamma=0.5, n_components=300, random_state=0) accuracy = 0 accuracy_record = list()predictions_start = 50sample = 5000early_stop = 50000 for x,y,n in pull_examples(target_file=local_path+'\\\\'+source, vectorizer=std_row, min_max=min_max, binary_features=list(), numeric_features=n_vars, fieldnames= n_vars+['covertype'], target='covertype', sparse=False): rbf_x = rbf_feature.fit_transform(x) # ФАЗА ОБУЧЕНИЯ if n > predictions_start: accuracy += int(int(SGD.predict(rbf_x))==y[0]) if n % sample == 0: accuracy_record.append(accuracy / float(sample)) print('%s Прогрессивная точность в примере %i: %0.3f' % (time.strftime('%X'), n, np.mean(accuracy_record[-sample:]))) accuracy = 0 if early_stop and n >= early_stop: break SGD.partial_fit(rbf_x, y, classes=range(1,8)) Out: 09:57:45 Прогрессивная точность в примере 50000: 0.707 дОвО дка гиперпараметр Ов Как и в пакетном обучении, короткие пути при тестировании оптимальных со-четаний гиперпараметров во внеядерных алгоритмах отсутствуют; придется опробовать определенное число их сочетаний, чтобы выяснить возможное опти-мальное решение и провести вневыборочный анализ ошибок для оценки резуль-тативности сочетаний.\n--- Страница 119 ---\n118  Быстрооб учающиеся реализации машин SVM Поскольку в действительности неизвестно, имеет ли задача прогнозирования простую гладкую выпуклую функцию потерь или же более сложную, и неизвест - но точно, как гиперпараметры взаимодействуют друг с другом, очень просто за- стрять в субоптимальном локальном минимуме, в случае если было опробовано недостаточное число сочетаний. К сожалению, на данный момент в Scikit-learn не предлагается никаких специализированных процедур оптимизации для внеядер-ных алгоритмов. Учитывая с неизбежностью продолжительное время обучения алгоритма SGD на длинном потоке, доводка гиперпараметров с использованием таких методов может фактически стать узким местом во время построения моде-ли на собственных данных. Здесь мы представим несколько эмпирических правил, которые могут помочь сэкономить время и усилия и достигнуть наилучших результатов. Прежде всего можно настроить параметры в окне или выборке из данных, ко- торые смогут уместиться в памяти. Как мы убедились на примере с ядерными машинами SVM, работа с резервуарной выборкой довольно быстрая, даже если поток огромен. Затем можно выполнить оптимизацию в оперативной памяти и использовать найденные в потоке оптимальные параметры. Леон Ботту (Léon Bottou) из Microsoft Research в своем техническом отчете Stochastic Gradient Descent Tricks («Приемы стохастического градиентного спуска») отметил: «Математика стохастического градиентного спуска удивительно независима от размера тренировочного набора». Это относится ко всем основным параметрам, и в особенности к темпу обуче - ния; т емп обучения, который хорошо работает с выборкой, будет лучше работать с полными данными. Кроме того, идеальное число прохождений по данным боль- шей частью можно угадать, попробовав выполнить схождение на небольшом вы-борочном наборе данных. В качестве практического ориентира мы предлагаем показательное число 10**6 исследуемых алгоритмом примеров – как указывается в документации Scikit-learn, это число мы часто находили точным, хотя идеальное число итераций может меняться в зависимости от параметров регуляризации. Хотя при использовании алгоритма SGD бóльшая часть работы может быть сде- лана в относительно малом масштабе, мы должны определиться с тем, как решать проблему фиксации многочисленных параметров. Традиционно чаще всего ис - пользовались подходы на основе ручного и сеточного поисков, при этом сеточ-ный поиск решал проблему, систематически тестируя все сочетания возможных параметров в значительных значениях (используя, к примеру, проверку в лога-рифмической шкале с возведением в разную степень: 10 или 2). Совсем недавно Джеймс Бергстра (James Bergstra) и Джошуа Бенджио (Yoshua Bengio) в своей статье Random Search for Hyper-Parameter Optimization («Случайный поиск для гиперпараметрической оптимизации») продемонстрировали другой подход, основанный на случайной выборке значений гиперпараметров. Такой подход, хотя и опирается на случайный выбор, по результатам часто эквивален-тен сеточному поиску (но при этом требует меньшего количества прогонов), когда число гиперпараметров низкое и может превысить производительность система-тического поиска, когда параметров много и не все они релевантны для эффек - тивности алгоритма.\n--- Страница 120 ---\nДоводка гиперпараметров  119 Мы предоставляем читателю право найти дополнительные причины, почему этот простой и привлекательный подход работает так хорошо в теории, отослав к ранее упомянутой статье Беркстры и Бенджио. Испытав на практике его превос - ходство относительно других подходов, в следующем фрагменте программного кода мы предлагаем подход, который в Scikit-learn работает хорошо для потоков на основе функции ParameterSampler . Функция ParameterSampler способна случайным образом отбирать (из функций распределения и списков дискретных значений) разные наборы гиперпараметров, которые позже применяются к обучающемуся алгоритму SGD посредством метода set_params : In: from sklearn.linear_model import SGDRegressorfrom sklearn.model_selection import ParameterSampler local_path = os.getcwd().decode('cp1251') source = '\\\\data\\\\bikesharing\\\\hour.csv' b_vars = ['holiday','hr','mnth', 'season', 'weathersit','weekday','workingday','yr']n_vars = ['hum', 'temp', 'atemp', 'windspeed'] std_row, min_max = explore(target_file=local_path+'\\\\'+source, binary_features=b_vars, numeric_features=n_vars) val_rmse = 0 val_rmsle = 0predictions_start = 16000tmp_rsmle = 10**6 def apply_log(x): return np.log(x + 1.0) def apply_exp(x): return np.exp(x) - 1.0 param_grid = {'penalty':['l1', 'l2'], 'alpha': 10.0**-np.arange(2,5)} random_tests = 3 search_schedule = list(ParameterSampler(param_grid, n_iter=random_tests, random_state=5))results = dict() for search in search_schedule: SGD = SGDRegressor(loss='epsilon_insensitive', epsilon=0.001, penalty=None, random_state=1, average=True) params = SGD.get_params() new_params = {p:params[p] if p not in search else search[p] for p in params} SGD.set_params(**new_params) print str(search)[1:-1] for iterations in range(200):\n--- Страница 121 ---\n120  Быстрооб учающиеся реализации машин SVM for x,y,n in pull_examples(target_file=local_path+'\\\\'+source, vectorizer=std_row, min_max=min_max, sparse = False, binary_features=b_vars, numeric_features=n_vars, target='cnt'): y_log = apply_log(y) # МАШИННОЕ ОБУЧЕНИЕ if (n+1) >= predictions_start: # ФАЗА С ОТКЛАДЫВАНИЕМ ПОСЛЕ N predicted = SGD.predict(x) val_rmse += (apply_exp(predicted) - y)**2 val_rmsle += (predicted - y_log)**2 else: # ФАЗА ОБУЧЕНИЯ SGD.partial_fit(x, y_log) examples = float(n-predictions_start+1) * (iterations+1) print_rmse = (val_rmse / examples)**0.5 print_rmsle = (val_rmsle / examples)**0.5 if iterations == 0: print('Итерация %i - RMSE: %0.3f - RMSE: %0.3f' % (iterations+1, print_rmse, print_rmsle)) if iterations > 0: if tmp_rmsle / print_rmsle <= 1.01: print('Итерация %i - RMSE: %0.3f - RMSE: %0.3f\\n' % (iterations+1, print_rmse, print_rmsle)) results[str(search)]= {'rmse':float(print_rmse), 'rmsle':float(print_rmsle)} break tmp_rmsle = print_rmsle Out: 'penalty': 'l2', 'alpha': 0.0001Итерация 1 - RMSE: 216.217 - RMSE: 1.440Итерация 20 - RMSE: 151.872 - RMSE: 0.856 'penalty': 'l1', 'alpha': 0.001 Итерация 1 - RMSE: 712.781 - RMSE: 4.090Итерация 31 - RMSE: 184.582 - RMSE: 1.053 'penalty': 'l1', 'alpha': 0.0001 Итерация 1 - RMSE: 1050.206 - RMSE: 6.039Итерация 36 - RMSE: 217.732 - RMSE: 1.252 В приведенном выше примере используется тот факт, что набор данных Bike- sharing об аренде велосипедов довольно небольшой и не требует выборки. В других контекстах целесообразно сначала ограничить число рассматриваемых строк либо создать меньшую выборку посредством резервуарного отбора или других потоко-вых методов отбора, встречавшихся до сих пор. Если требуется исследовать опти-мизацию более углубленно, то можно изменить переменную random_tests , зафикси- ровав число тестируемых сочетаний отобранных гиперпараметров. Затем следует модифицировать условие if tmp_rmsle / print_rmsle <= 1.01, используя число ближе\n--- Страница 122 ---\nДоводка гиперпараметров  121 к 1.0 – может, даже само число 1.0, – тем самым давая алгоритму полностью схо- диться, пока не будет получен возможный прирост в прогнозирующей способности.  Хотя рекомендуется вместо взятия значений из списков использовать функции распреде- ления, вы по-прежнему можете соответствующим образом использовать предложенные ранее интервалы гиперпараметров, просто увеличив число значений, которые, возмож - но, будут отобраны из списков. Например, для параметра alpha в регуляризации L1 и L2 можно использовать функцию библиотеки NumPy arrange с малым шагом, как, например, 10.0**-np.arange(1, 7, step=0.1) , либо logspace с высоким числом для параметра num: 1.0/ np.logspace(1,7, num=50) . Другие альтернативы быстро обучающихся реализаций SVM Хотя библиотека Scikit-learn предлагает достаточно много инструментов и алго-ритмов для внеядерного обучения, среди бесплатного программного обеспечения существуют другие интересные альтернативы. Некоторые опираются на те же са-мые библиотеки, которые используются в самом Scikit-learn, в частности Liblinear/SBM; другие же абсолютно новые, такие как Sofia-ml, LASVM и Vowpal Wabbit. На-пример, Liblinear/SBM основана на выборочно-блочной минимизации и реализо-вана как ветвление liblinear-cdblock исходной библиотеки (https://www.csie.ntu.edu. tw/~cjlin/libsvmtools/#large_linear_classification_when_data_cannot_fit_in_memory). Биб - лиот ека Liblinear/SBM способна подбирать нелинейные модели SVM на больших объемах данных, которые не умещаются в оперативной памяти, задействуя прием, когда для тренировки ученика используются новые выборки из данных и каждая из них затем смешивается с предыдущими выборками, уже используемыми для минимизации (отсюда и термин блочный, от англ. blocked, в названии алгоритма). Еще одна альтернативная библиотека SofiaML (https://code.google.com/archive/p/ sofia-ml/) основывается на онлайновом алгоритме оптимизации модели SVM под названием Pegasos SVM. Этот алгоритм является онлайновой аппроксимацией SVM, так же, как и другой программный продукт под названием LaSVM, создан-ный Леоном Ботту (http://leon.bottou.org/projects/lasvm). Все эти решения могут работать с разреженными данными, в особенности текстовыми, и решать зада-чи регрессии, классификации и ранжирования. До настоящего времени никакое протестированное нами альтернативное решение не оказалось столь же быстрым и универсальным, как Vowpal Wabbit. Этот программный продукт будет представ-лен в следующих разделах и будет использоваться для демонстрации того, каким образом можно интегрировать внешние программы в среде Python. Нелинейность и быстродействие с Vowpal Wabbit Проект с открытым исходным кодом Vowpal Wabbit (VW), предлагающий быст - родейс твующего онлайнового ученика, был выпущен в 2007 г. Джоном Лэнг - фордом (John Langford), Лихонг Ли (Lihong Li) и Алексом Штрелем (Alex Strehl) из Yahoo! Research (http://hunch.net/?p=309) и затем последовательно спонсиро-вался Microsoft Research, так как Джон Лэнгфорд стал главным исследователем в Microsoft. За эти годы проект получил дальнейшее развитие, достигнув сегодня версии 8.1.0 1 с почти сотней работающих над ним участников. (Имеется интерес - 1 Ко времени работы над переводом настоящей главы (июнь 2017 г.) последней была вер- сия 8.3.0.9. – Прим. перев.\n--- Страница 123 ---\n122  Быстрооб учающиеся реализации машин SVM ное видео, в котором процесс разработки и внесенные в проект усилия разработ - чиков визуализируются во времени, используя программу Gource, – https://www. youtube.com/watch?v=-aXelGLMMgk). По сей день проект VW по-прежнему находит - ся в стадии постоянной доработки, и его обучающие способности продолжают на- ращиваться после каждой итерации разработки. Поразительная характеристика ученика VW состоит в том, что он работает очень быстро, по сравнению с другими имеющимися решениями (LIBLINEAR, Sofia-ml, svmsgd и Scikit-learn). Его секрет прост и одновременно чрезвычайно эффективен: он может загружать данные и обучаться им одновременно. Асинхронный про-цесс (thread) выполняет разбор прибывающих примеров, в то время как несколько обучаю щихся процессов работает над непересекающимся набором признаков, тем самым гарантируя высокую вычислительную эффективность, несмотря на то что в результате разбора создаются высокоразмерные признаки (как, например, квадратическое или кубическое полиномиальное разложение). В большинстве случаев реальным узким местом процесса обучения является пропускная способ-ность передачи поступающих в ученика VW данных с диска или из Сети. Обучающийся алгоритм VW может решать задачу классификации (даже муль- тиклассовую и многозначную), задачу регрессии (регрессию на основе обычного МНК и квантильную регрессию) и активные задачи обучения, предлагая обшир-ный диапазон сопровождающих инструментов обучения (называемых редукция - ми), в частности матричную факторизацию, латентное размещение Дирихле (LDA), нейронные сети, n-граммы для языковых моделей и бутстрапирование. Инсталляция программы VW Программу VW можно получить из онлайнового версионного репозитория GitHub (https://github.com/JohnLangford/vowpal_wabbit), где ее можно клонировать при по-мощи Git или же просто скачать в форме упакованного zip-файла. Будучи разра-ботанной в системах Linux, ее легко скомпилировать в любой среде POSIX прос - той последовательностью команд make и make install. Подробные инструкции по инсталляции приводятся непосредственно на странице инсталляции программы, откуда можно скачать прекомпилированные бинарники Linux непосредственно от автора (https://github.com/JohnLangford/vowpal_wabbit/wiki/Download). Версию программы VW, работающую под операционными системами Windows, к сожалению, будет получить немного труднее. Чтобы ее создать, прежде всего нужно обратиться непосредственно к самой документации по библиотеке VW, где процедура компиляции подробно объяснена (https://github.com/JohnLangford/vowpal_wabbit/blob/master/README.windows.txt).  На сопутствующем книге веб-сайте и среди прилагаемых к книге исходных кодов примеров мы предлагаем 32- и 64-разрядные бинарники Windows библиотеки VW версии 8.1.0, кото- рые мы использовали во время работы над книгой. Анализ формата данных программы VW Программа VW может работать со специальным форматом данных и вызывать- ся из оболочки. Джон Лэнгфорд использует следующий ниже демонстрационный набор данных для своего онлайнового учебного руководства (https://github.com/JohnLangford/vowpal_wabbit/wiki/Tutorial), который описывает три дома, чьи кры-ши могут быть заменены. Мы считаем этот пример интересным и предлагаем его с комментариями:\n--- Страница 124 ---\nДоводка гиперпараметров  123 In: with open('data\\\\house_dataset','wb') as W: W.write(\"0 | price:.23 sqft:.25 age:.05 2006\\n\") W.write(\"1 2 'second_house | price:.18 sqft:.15 age:.35 1976\\n\") W.write(\"0 1 0.5 'third_house | price:.53 sqft:.32 age:.87 1924\\n\") with open('data\\\\house_dataset','rb') as R: for line in R: print(line.strip()) Out: 0 | price:.23 sqft:.25 age:.05 20061 2 'second_house | price:.18 sqft:.15 age:.35 19760 1 0.5 'third_house | price:.53 sqft:.32 age:.87 1924 Первый заметный аспект этого формата файла состоит в том, что он не имеет строки заголовка. Это объясняется тем, что в проекте VW для размещения призна- ков в разреженном векторе используется хэширование признаков, и, следователь-но, ненужные признаки известны заранее. Блоки данных разделены конвейерным символом (« |») на пространства имен, которые задуманы как различные призна- ковые кластеры, каждый из которых содержит один или несколько признаков. Первое пространство имен всегда содержит переменную отклика. Отклик мо- жет быть вещественным (или целым) числом, указывающим на числовое значе-ние, которое будет регрессироваться, бинарный класс либо один из множества классов. Отклик – это всегда первое число в строке. Бинарный класс кодируется при помощи 1 для положительного и -1 для отрицательного класса (использова- ние 0 в качестве отклика разрешено только для регрессии). Многочисленные клас - сы должны быть последовательно пронумерованы, начиная с 1, причем наличие разрывов в числах не желательно, потому что VW запрашивает последний класс и рассматривает все целые числа между 1 и последним. Число сразу после значения отклика является весом (которое говорит, следует ли рассматривать пример как множественный пример или как составную часть примера), затем идет базис, который играет роль первоначального прогноза (свое го рода смещение). Наконец, идет метка, предваряемая символом апострофа ('); она может быть числом или текстом, которые позже можно найти в результа- тах VW на выходе (в прогнозе имеется идентификатор для каждого оценочного значения). Вес, базис и метки не обязательны: если их нет, то в качестве веса будет подставлена 1, а базис и метка не будут иметь значения. После первого пространства имен можно добавить столько пространств имен, сколько нужно, маркируя каждое числом или строковым значением. Чтобы счи-таться меткой пространства имен, она должна идти сразу за конвейерным симво-лом, например |label . После метки пространства имен можно поименно добавлять любой признак. Имя признака может быть чем угодно, но должно содержать конвейерный символ или двоеточие. В пространстве имен можно разместить целые тексты, и каждое слово будет рассматриваться как признак. Каждый признак будут считаться рав-ным 1. Если нужно присвоить другое число, то следует добавить двоеточие в конец имени признака и разместить его значение после него. Например, вот допустимая в Vowpal Wabbit строка: 0 1 0.5 'third_house | price:.53 sqft:.32 age:.87 1924\n--- Страница 125 ---\n124  Быстрооб учающиеся реализации машин SVM В первом пространстве имен отклик равен 0, вес примера равен 1, базис – 0.5 и его метка – third_house . Пространство имен является безымянным и состоит из четырех признаков, а именно price (.53), sqft (.32), age (.87) и 1924 (значение равно 1). Если в примере имеется признак, который в другом примере отсутствует, алго- ритм решит, что значение признака во втором примере равно нулю. Поэтому та- кой признак, как 1924, в приведенном выше примере может служить как бинарная переменная, которой, когда она присутствует, автоматически присваивается 1, а когда отсутствует – 0. Это также говорит о том, как программа VW обрабатывает пропущенные значения, – она автоматически рассматривает их как 0.  Вы можете легко обрабатывать пропущенные значения, ставя новый признак, когда значе-ние отсутствует. К примеру, если признак является возрастом (age), то можно добавить но- вый признак age_missing, который будет бинарной переменной со значением 1. При оценке коэффициентов эта переменная будет действовать как оценщик отсутствующего значения.На веб-сайте автора можно также найти валидатор входных данных, проверяющий, что входные данные отформатированы правильно для передачи в VW, и отображающий про-цесс их интерпретации этим программным продуктом: http://hunch.net/~vw/validate.html. Интеграция с Python Существует несколько библиотек, которые интегрируют программу VW в Python (vowpal_porpoise, Wabbit Wappa либо pyvw ). Их инсталляция в системах Linux вы- полняется просто, но намного тяжелее в Windows. Не важно, работаете вы с Jupyter или IDE, самый простой способ интегрировать VW со сценариями Python состоит в том, чтобы задействовать функцию Popen из библиотеки subprocess . Она застав- ляет VW выполняться параллельно с Python. Python просто ожидает, когда про-грамма VW завершит свою работу, захватывает результат и печатает его на экране: In: import subprocess def execute_vw(parameters): execution = subprocess.Popen('data\\\\vw '+parameters, shell=True, stderr=subprocess.PIPE) line = \"\" history = \"\" while True: out = execution.stderr.read(1) history += out if out == '' and execution.poll() != None: print('------------ РАБОТА ЗАВЕРШЕНА ------------\\n') break if out != '': line += out if '\\n' in line[-2:]: print(line[:-2]) line = '' return history.split('\\r\\n') Эта функция возвращает список результатов процесса обучения, упрощая его обработку, при этом извлекается соответствующая информация, допускающая повторное использование (как, например, мера ошибки). В качестве предвари-\n--- Страница 126 ---\nДоводка гиперпараметров  125 тельного условия для его корректного функционирования поместите исполняе- мый файл VW (файл vw.exe ) в рабочий каталог Python или системный путь, где он может быть найден. Вызвав функцию на ранее записанном наборе данных о домах, можно взгля- нуть на то, как все работает и какие результаты получаются на выходе: In: params = \"data\\\\house_dataset\"results = execute_vw(params) Out: Num weight bits = 18learning rate = 0.5initial_t = 0power_t = 0.5using no cacheReading datafile = house_datasetnum sources = 1average since example example current current currentloss last counter weight label predict features0.000000 0.000000 1 1.0 0.0000 0.0000 50.666667 1.000000 2 3.0 1.0000 0.0000 5 finished run number of examples per pass = 3passes used = 1weighted example sum = 4.000000weighted label sum = 2.000000average loss = 0.750000best constant = 0.500000best constant's loss = 0.250000total feature number = 15------------ РАБОТА ЗАВЕРШЕНА ------------ Начальные строки результата просто напоминают об использованных парамет - рах и служат в качестве подтверждения о том, какой файл данных использовался. Самая интересная часть – это прогрессивный отчет с числом переданных пото- ком примеров (выраженным числом в степени 2, т. е. пример 1, 2, 4, 8, 16 и т. д.). Относительно функции потерь в отчете показана средняя мера потерь, прогрес - сивный отчет для первой итерации на основе отложенных данных, мера потерь которого обозначается отнесенной в конец буквой h (если откладывание данных исключено, то имеется возможность только сообщать о внутривыборочной мере). В столбце example weight сообщается о весе примера, и затем пример описывается как current label, current predict (т. е. текущая метка и текущее предсказание), и на экран выводится число найденных на этой строке признаков ( current features ). Вся эта информация должна помочь следить за потоком и процессом обучения. После того как обучение завершено, выводится несколько итоговых мер. Сред- няя потеря является самой важной, в частности когда используется отложенная выборка. Этот показатель полезнее всего использовать для сравнения, так как его можно сразу сравнить с показателем best constant's loss (мера потерь или базовая прогнозирующая способность простой константы) и в разных прогонах с разны-ми параметрическими конфигурациями.\n--- Страница 127 ---\n126  Быстрооб учающиеся реализации машин SVM Еще одна очень полезная приготовленная нами функция для интеграции про- граммы VW и Python связана с автоматическим преобразованием CSV-файлов в файлы данных VW. Ее можно увидеть в следующем ниже фрагменте исходного кода. Она поможет воспроизвести рассмотренные ранее задачи на основе наборов данных Bike-sharing и Covertype, на этот раз при помощи программы VW. Этой функ - цией можно легко воспользоваться повторно для своих собственных проектов: In: import csv def vw_convert(origin_file, target_file, binary_features, numeric_features, target, transform_target=lambda(x):x, separator=',', classification=True, multiclass=False, fieldnames= None, header=True, sparse=True): \"\"\" Читает онлайновый поток и возвращает генератор нормализованных векторов признаков Параметры ---------- target file = файл, из которого организовать потоковую передачу vectorizer = объект DictVectorizer binary_features = список качественных признаков для рассмотрения numeric_features = список числовых признаков для рассмотрения target = метка переменной отклика min_max = объект MinMaxScaler, можно пропустить, установив в None separator = символ разделения полей fieldnames = метки полей (можно пропустить и читать из файла) sparse = будет ли возвращен из генератора разреженный вектор \"\"\" with open(target_file, 'wb') as W: with open(origin_file, 'rb') as R: iterator = csv.DictReader(R, fieldnames, delimiter=separator) for n, row in enumerate(iterator): if not header or n>0: # ОБРАБОТКА ДАННЫХ response = transform_target(float(row[target])) if classification and not multiclass: if response == 0: stream_row = '-1 ' else: stream_row = '1 ' else: stream_row = str(response)+' ' quantitative = list() qualitative = list() for k,v in row.iteritems(): if k in binary_features: qualitative.append(str(k)+'_'+str(v)+':1') else: if k in numeric_features and (float(v)!=0 or not sparse): quantitative.append(str(k)+':'+str(v)) if quantitative: stream_row += '|n '+' '.join(quantitative)\n--- Страница 128 ---\nДоводка гиперпараметров  127 if qualitative: stream_row += '|q ' + ' '.join(qualitative) W.write(stream_row+'\\n') Несколько примеров с использованием редукций для алгоритма SVM и нейронных сетей Программа VW работает по принципу минимизации общей функции стоимости, которая выглядит следующим образом: Как и в других встречавшихся ранее формулах, w – это вектор коэффициен- тов, при этом оптимизация достигается отдельно для каждого xi и yi согласно вы- бранной функции потерь (обычный МНК, логистическая или кусочно-линейная). λ 1 и λ2 – это параметры регуляризации, которые по умолчанию равны нулю, но которые можно задать при помощи опций --l1 и --l2 в командной строке VW. С учетом такой базовой структуры VW со временем становился сложнее и со- вершеннее благодаря парадигме редукций. Редукция – это просто способ снова использовать существующий алгоритм для решения новых задач, не программи-руя новых алгоритмов решений с нуля. Другими словами, если есть сложная зада-ча машинного обучения A, то она просто сводится к задаче B. Решение B наводит на мысль о решении A. Внедрение этого способа также обусловлено растущим ин-тересом к машинному обучению и взрывному числу задач, которые невозможно решить созданием огромного числа новых алгоритмов. Этот интересный подход, усиливающий существующие возможности, предлагаемые основными алгорит - мами, стал одной из главных причин, почему за прошедшие годы приложимость VW получила свое развитие, хотя программа осталась довольно компактной. Если вы интересуетесь этим подходом, то можно взглянуть на следующие два учеб-ных руководства от Джона Лэнгфорда: http://hunch.net/~reductions_tutorial/ и http:// hunch.net/~jl/projects/reductions/reductions.html. В качестве другой иллюстрации мы кратко представим пару редукций для ре- ализации SVM с ядром RBF и мелкой нейронной сети при помощи программы VW в чисто внеядерном стиле, т. е. вне основной памяти компьютера. Для этого мы воспользуемся несколькими миниатюрными наборами данных. Ниже приведен набор данных Iris, модифицированный для задачи бинарной классификации с целью распознавания цветков ириса Iris Versicolor, Iris Setosa и Iris Virginica (ирис разноцветный, щетинистый и виргинский): In: from random import seedimport numpy as npfrom sklearn.datasets import load_iris, load_boston iris = load_iris() seed(2)re_order = np.random.permutation(len(iris.target)) with open('data\\\\iris_versicolor.vw','wb') as W1: for k in re_order: y = iris.target[k]\n--- Страница 129 ---\n128  Быстрооб учающиеся реализации машин SVM X = iris.values()[1][k,:] features = ' |f '+' '.join([a+':'+str(b) for a,b in zip(map(lambda(a): a[:-5].replace(' ','_'), iris.feature_names),X)]) target = '1' if y==1 else '-1' W1.write(target+features+'\\n') Затем для задачи регрессии мы воспользуемся набором данных Boston с цена- ми на жилую недвижимость: In: boston = load_boston()seed(2)re_order = np.random.permutation(len(boston.target)) with open('data\\\\boston.vw','wb') as W1: for k in re_order: y = boston.target[k] X = boston.data[k,:] features = ' |f '+' '.join([a+':'+str(b) for a,b in zip(map(lambda(a): a[:-5].replace(' ','_'), iris.feature_names),X)]) W1.write(str(y)+features+'\\n') Сначала мы опробуем алгоритм SVM. Редукция kvsm основана на алгоритме LaSVM (Быстрые ядерные классификаторы с онлайновым и активным обучением – http://www.jmlr.org/papers/volume6/bordes05a/bordes05a.pdf) без постоянного смеще-ния. Версия VW, как правило, выполняется всего за одно прохождение и с 1–2 пе-реработками (reprocessing) произвольно отбираемого опорного вектора (хотя некоторые задачи, возможно, потребуют многократных прохождений и перерабо-ток). В нашем случае мы используем всего одно прохождение и пару переработок, чтобы аппроксимировать ядро RBF на нашей бинарной задаче (опция kvsm работа- ет только для задачи классификации). Реализованы ядра линейное, радиально-ба-зисная функция и полиномиальное. Для того чтобы все работало, используйте оп-цию --ksvm , задайте при помощи --reprocess число переработок (по умолчанию оно равно 1), выберите ядро при помощи --kernel (варианты linear , poly и rbf). Затем, если ядро полиномиальное, задайте целое число для степени --degree или число c плавающей точкой (по умолчанию 1.0) для пропускной способности --bandwidth , если используется RBF. Кроме того, необходимо обязательно определить регуля-ризацию L2, в противном случае редукция не будет работать должным образом. В нашем примере мы задаем ядро RBF с пропускной способностью 0.1: In: params = '--ksvm --l2 0.000001 --reprocess 2 -b 18 --kernel rbf--bandwidth=0.1 -p data\\\\iris_bin.test -d data\\\\iris_versicolor.vw'results = execute_vw(params) import numpy as npdef sigmoid(x): return 1. / (1. + np.exp(-x))\n--- Страница 130 ---\nДоводка гиперпараметров  129 accuracy = 0 with open('data\\\\iris_bin.test', 'rb') as R: with open('data\\\\iris_versicolor.vw', 'rb') as TRAIN: holdouts = 0.0 for n,(line, example) in enumerate(zip(R,TRAIN)): if (n+1) % 10==0: predicted = float(line.strip()) y = float(example.split('|')[0]) accuracy += np.sign(predicted)==np.sign(y) holdouts += 1 print('Точность на отложенных данных: %0.3f' % ((accuracy / holdouts)**0.5)) Out: Точность на отложенных данных: 0.966 Нейронные сети – это еще одно замечательное дополнение к возможно- стям проекта VW; благодаря работе Пола Минейро (Paul Mineiro) (http://www. machinedlearnings.com/2012/11/unpimp-your-sigmoid.html) программа VW может реализовывать одноуровневую нейронную сеть с гиперболической тангенсной (tanh) функцией активации и, факультативно, с прореживанием весов (исполь-зуя опцию --dropout ). Несмотря на то что имеется возможность определять только число нейронов, нейронная редукция отлично справляется как с задачами ре-грессии, так и классификации и может гладко принимать другие преобразования VW в виде входных данных (такие как квадратичные переменные и n-граммы), делая этот программный продукт очень хорошо интегрированным, универсаль-ным (нейронные сети могут решать довольно много задач) и быстрым решением. В нашем примере мы применяем редукцию к набору данных Boston, используя пять нейронов и прореживание: In: params = 'data\\\\boston.vw -f data\\\\boston.model --loss_function squared -k--cache_file data\\\\cache_train.vw --passes=20 --nn 5 --dropout'results = execute_vw(params) params = '-t data\\\\boston.vw -i data\\\\boston.model -k --cache_file data\\\\cache_test.vw -p data\\\\boston.test'results = execute_vw(params) val_rmse = 0with open('data\\\\boston.test', 'rb') as R: with open('data\\\\boston.vw', 'rb') as TRAIN: holdouts = 0.0 for n,(line, example) in enumerate(zip(R,TRAIN)): if (n+1) % 10==0: predicted = float(line.strip()) y = float(example.split('|')[0]) val_rmse += (predicted - y)**2 holdouts += 1 print('RMSE на отложенных данных: %0.3f' % ((val_rmse / holdouts)**0.5)) Out: RMSE на отложенных данных: 7.010\n--- Страница 131 ---\n130  Быстрооб учающиеся реализации машин SVM Ускоренный набор данных Bike-sharing Давайте испытаем программу VW на ранее созданном демонстрационном файле Bike-sharing с данными об аренде велосипедов для объяснения выходных ком-понентов. В качестве первого шага необходимо преобразовать CSV-файл в VW-файл, и для этого ранее приведенная функция vw_convert окажется весьма кстати. Как и прежде, мы применим к числовому отклику логарифмическое преобразова-ние, воспользовавшись функцией apply_log , передаваемой параметром transform_ target функции vw_convert : In: import osimport numpy as np def apply_log(x): return np.log(x + 1.0) def apply_exp(x): return np.exp(x) - 1.0 local_path = os.getcwd().decode('cp1251') source = '\\\\data\\\\bikesharing\\\\hour.csv' origin = target_file=local_path+'\\\\'+source target = target_file=local_path+'\\\\'+'data\\\\bike.vw' b_vars = ['holiday','hr','mnth', 'season', 'weathersit','weekday','workingday','yr']n_vars = ['hum', 'temp', 'atemp', 'windspeed'] vw_convert(origin, target, binary_features=b_vars, numeric_features=n_vars, target = 'cnt', transform_target=apply_log, separator=',', classification=False, multiclass=False, fieldnames= None, header=True) Через несколько секунд должен быть готов новый файл. Мы можем сразу вы- полнить расчеты, которые представляют собой простую линейную регрессию (эта опция задана в VW по умолчанию). Обучение предположительно будет выполнять-ся в течение 100 прохождений и контролироваться вневыборочной проверкой, ко-торая в VW реализована автоматически (систематически и непрерывно выбирая в качестве проверочных данных одно наблюдение из каждых 10). В этом случае мы решаем задать отложенную выборку после 16 000 примеров (используя опцию --holdout_after ). Когда проверочная ошибка на перекрестной проверке увеличива - ется (а не уменьшается), программа VW останавливается после нескольких ите-раций (по умолчанию три, но это число может быть изменено при помощи опции --early_terminate ), избегая переподгонки (избыточной аппроксимации данных): In: params = 'data\\\\bike.vw -f data\\\\regression.model -k --cache_file data\\\\cache_train.vw --passes=1000 --hash strings --holdout_after 16000'\n--- Страница 132 ---\nДоводка гиперпараметров  131 results = execute_vw(params) Out: finished runnumber of examples per pass = 15999passes used = 6weighted example sum = 95994.000000weighted label sum = 439183.191893average loss = 0.427485 hbest constant = 4.575111total feature number = 1235898------------ РАБОТА ЗАВЕРШЕНА ------------ Итоговый отчет показывает, что были завершены шесть прохождений (из 100 возможных), и вневыборочная средняя потеря составила 0.428. Поскольку нас ин- тересует квадратный корень из среднеквадратической ошибки RMSE и квадратный корень из среднеквадратической логарифмической ошибки RMSLE , мы должны вы- числить их сами. Затем мы записываем результаты предсказания в файл ( pred.test ), чтобы по- том их прочесть и вычислить меру ошибки, используя ту же схему с откладыва-нием данных, что и в тренировочном наборе. Результаты действительно намного лучше (в долях времени), чем те, которые мы получили ранее с алгоритмом SGD библиотеки Scikit-learn: In: params = '-t data\\\\bike.vw -i data\\\\regression.model -k --cache_file data\\\\cache_test.vw -p data\\\\pred.test'results = execute_vw(params) val_rmse = 0 val_rmsle = 0 with open('data\\\\pred.test', 'rb') as R: with open('data\\\\bike.vw', 'rb') as TRAIN: holdouts = 0.0 for n,(line, example) in enumerate(zip(R,TRAIN)): if n > 16000: predicted = float(line.strip()) y_log = float(example.split('|')[0]) y = apply_exp(y_log) val_rmse += (apply_exp(predicted) - y)**2 val_rmsle += (predicted - y_log)**2 holdouts += 1print('RMSE на отложенных данных: %0.3f' % ((val_rmse / holdouts)**0.5))print('RMSLE на отложенных данных: %0.3f' % ((val_rmsle / holdouts)**0.5)) Out: RMSE на отложенных данных: 135.306RMSLE на отложенных данных: 0.845 Переработка набора данных Covertype в VW Задача с набором данных Covertype в программе VW тоже решается лучше и легче, чем получалось ранее. На этот раз следует настроить несколько параметров и при-нять решение на турнире коррек тировки ошибки (error c orrecting tournament,\n--- Страница 133 ---\n132  Быстрооб учающиеся реализации машин SVM ECT), запускаемом в программе VW параметром --ect , где каждый класс сорев- нуется на турнире с выбыванием, чтобы стать меткой для примера. Во многих примерах ECT может превзойти схему «один против все х», но это не общее пра- вило, и ECT является лишь одним из подходов, который требует проверки во вре-мя работы с мультиклассовыми задачами. (Другая возможная опция --log_multi использует онлайновые деревья решений для расщепления выборки на меньшие по объему множества, к которым затем применяются одиночные прогнозные мо-дели.) Мы также устанавливаем темп обучения в 1.0 и создаем полиномиальное разложение третьей степени, используя для этого параметр --cubic с указанием, какие пространства имен должны быть перемножены друг с другом (в этом случае пространство имен f для трех раз обозначается строкой nnn, которая идет после опции --cubic ): In: import os local_path = os.getcwd().decode('cp1251') source = 'data\\\\shuffled_covtype.data' origin = target_file=local_path+'\\\\'+source target = target_file=local_path+'\\\\'+'data\\\\covtype.vw' n_vars = ['var_'+'0'*int(j<10)+str(j) for j in range(54)]vw_convert(origin, target, binary_features=list(), fieldnames= n_vars+['covertype'], numeric_features=n_vars, target = 'covertype', separator=',', classification=True, multiclass=True, header=False, sparse=False) params = 'data\\\\covtype.vw --ect 7 -f data\\\\multiclass.model -k --cache_file data\\\\cache_ train.vw --passes=2 -l 1.0 --cubic nnn'results = execute_vw(params) Out: finished runnumber of examples per pass = 522911passes used = 2weighted example sum = 1045822.000000weighted label sum = 0.000000average loss = 0.235538 htotal feature number = 384838154------------ РАБОТА ЗАВЕРШЕНА ------------  Для того чтобы пример был быстрым, мы ограничиваем число прохождений по данным вс его двумя. Если у вас есть время, увеличьте число до 100, и вы увидите, что полученная точность может быть улучшена еще больше. Здесь нам не придется обследовать меру ошибки дальше, поскольку сообщае - мая сре дняя потеря является дополнением 1.0 меры точности; мы просто вычис - ляем ее для полноты, подтверждая, что наша точность на отложенных данных равна именно 0.769 : In: params = '-t data\\\\covtype.vw -i data\\\\multiclass.model -k --cache_file data\\\\cache_test.vw -p data\\\\covertype.test'\n--- Страница 134 ---\nРезюме  133 results = execute_vw(params) accuracy = 0with open('data\\\\covertype.test', 'rb') as R: with open('data\\\\covtype.vw', 'rb') as TRAIN: holdouts = 0.0 for n,(line, example) in enumerate(zip(R,TRAIN)): if (n+1) % 10==0: predicted = float(line.strip()) y = float(example.split('|')[0]) accuracy += predicted ==y holdouts += 1print('Точность на отложенных данных: %0.3f' % (accuracy / holdouts)) Out: Точность на отложенных данных: 0.769 резюме В этой главе мы подробно остановились на начальном обсуждении внеядерных алгоритмов путем добавления машин SVM к простым регрессионным линейным моделям. Большую часть времени мы уделили реализациям этих алгоритмов в биб лиот еке Scikit-learn – главным образом алгоритма SGD – и завершили об- зором внешних инструментов, которые могут быть интегрированы сценариями Python, такими как программа Vowpal Wabbit Джона Лэнгфорда. По пути мы за-вершили обзор методов совершенствования и проверки моделей во время работы вне ядра, обсудив алгоритм резервуарного отбора, регуляризацию, явные и неяв-ные нелинейные преобразования и гиперпараметрическую оптимизацию. В следующей главе мы займемся еще более сложными и мощными подхода- ми к обучению и представим глубокое обучение и нейронные сети в крупномас - штабных задачах. Если ваши проекты вращаются вокруг анализа изображений и звуков, то все увиденное пока может не оказаться тем волшебным решением, которое вы искали. Следующая глава предоставит вам все искомые решения.",
      "debug": {
        "start_page": 90,
        "end_page": 134
      }
    },
    {
      "name": "Глава 4. Искусственные нейронные сети и глубокое обучение 134",
      "content": "--- Страница 135 --- (продолжение)\nГлава 4 Искусственные нейронные сети и глубокое обучение В этой главе мы осветим одну из самых захватывающих областей в искусственном интеллекте и машинном обучении – глубокое обучение – и пройдемся по самым важным понятиям, необходимым для его эффективного применения. При этом мы затронем следующие темы: важнейшие элементы теории искусственных нейронных сетей; реализация нейронных сет ей на GPU или CPU; доводка параметров нейронных с етей; крупномасштабное г лубокое обучение в среде H2O; глубок ое обучение с автокодировщиками (предварительная тренировка). Глубокое обучение развилось из подобласти искусственного интеллекта, где как раз и были разработаны искусственные нейронные сети. Строго говоря, лю-бую крупную искусственную нейронную сеть можно рассматривать как глубоко обучаемую. Однако последние разработки в глубоких архитектурах (или тополо-гиях) требуют большего, чем просто создание крупных нейронных сетей. Разница между глубокими архитектурами и нормальными многослойными сетями заклю-чается в том, что глубокая архитектура состоит из множественной предобработ - ки и шагов, выполняющихся без учителя, которые обнаруживают латентную раз-мерность в данных, передаваемую позже на дальнейшие этапы обработки в сети. Самое важное, что нужно знать о глубоком обучении, состоит в том, что новые признаки усваиваются и трансформируются, проходя через эти глубокие архи-тектуры, с целью улучшения общей точности обучения. Поэтому важное различие между текущим поколением методов глубокого обучения и другими подходами машинного обучения – в том, что с глубоким обучением работа по конструирова-нию признаков частично автоматизирована. Не слишком переживайте, если эти понятия будут звучать абстрактно; позже в этой главе они получат свое разъяс - нение вместе с практическими примерами. Эти методы глубокого обучения при-вносят новые сложности, которые делают их эффективное применение довольно серьезным испытанием. Самое серьезное испытание заключается в том, что они с трудом поддаются тренировке, а также требуют продолжительных вычислений и доводки (тонкой\nГлава 4 Искусственные нейронные сети и глубокое обучение В этой главе мы осветим одну из самых захватывающих областей в искусственном интеллекте и машинном обучении – глубокое обучение – и пройдемся по самым важным понятиям, необходимым для его эффективного применения. При этом мы затронем следующие темы: важнейшие элементы теории искусственных нейронных сетей; реализация нейронных сет ей на GPU или CPU; доводка параметров нейронных с етей; крупномасштабное г лубокое обучение в среде H2O; глубок ое обучение с автокодировщиками (предварительная тренировка). Глубокое обучение развилось из подобласти искусственного интеллекта, где как раз и были разработаны искусственные нейронные сети. Строго говоря, лю-бую крупную искусственную нейронную сеть можно рассматривать как глубоко обучаемую. Однако последние разработки в глубоких архитектурах (или тополо-гиях) требуют большего, чем просто создание крупных нейронных сетей. Разница между глубокими архитектурами и нормальными многослойными сетями заклю-чается в том, что глубокая архитектура состоит из множественной предобработ - ки и шагов, выполняющихся без учителя, которые обнаруживают латентную раз-мерность в данных, передаваемую позже на дальнейшие этапы обработки в сети. Самое важное, что нужно знать о глубоком обучении, состоит в том, что новые признаки усваиваются и трансформируются, проходя через эти глубокие архи-тектуры, с целью улучшения общей точности обучения. Поэтому важное различие между текущим поколением методов глубокого обучения и другими подходами машинного обучения – в том, что с глубоким обучением работа по конструирова-нию признаков частично автоматизирована. Не слишком переживайте, если эти понятия будут звучать абстрактно; позже в этой главе они получат свое разъяс - нение вместе с практическими примерами. Эти методы глубокого обучения при-вносят новые сложности, которые делают их эффективное применение довольно серьезным испытанием. Самое серьезное испытание заключается в том, что они с трудом поддаются тренировке, а также требуют продолжительных вычислений и доводки (тонкой\n--- Страница 136 ---\nАрхитектура нейронной сети  135 настройки) параметров. В этой главе будут рассмотрены некоторые решения этих трудностей. За прошедшее десятилетие появились интересные применения глубокого обуче ния в машинном зрении, обработке естественного языка и аудиоданных, а также приложения, такие как проект Deep Face, созданный исследовательской группой компании Facebook, частично возглавляемый известным исследовате-лем в области глубокого обучения Яном Лекуном (Yann LeCun). Проект Deep Face ставит своей целью извлечение и идентификацию человеческих лиц из цифровых изображений. Компания Google имеет свой собственный проект DeepMind во гла-ве с Джеффри Хинтоном (Geoffrey Hinton). Недавно Google представил библиотеку с открытым исходным кодом TensorFlow, предлагающую применения глубокого обучения, которая будет подробно освещена в следующей главе. Прежде чем мы начнем запускать автономных интеллектуальных агентов, успешно проходящих тесты Тьюринга и выступающих на математических кон-курсах, давайте сделаем шаг назад и пробежимся по основам. архитектура нейр ОннОй сети Итак, сперва сосредоточимся на том, каким образом искусственные нейронные сети организованы, и начнем с их архитектуры и нескольких определений. Сеть, в которой поток обучения за одно прохождение передается вперед вплоть до выходов, называется нейронной сетью прямого распространения сигнала. Элементарную искусственную нейронную сеть прямого распространения мож - но легко изобразить на сетевом графике, как показано ниже: Вход Скрытый слой Выход На сетевом графике видно, что эта архитектура состоит из входного, скрыто- го и выходного слоев. Входной слой содержит векторы признаков (где каждое наблю дение имеет n признаков), а выходной слой состоит из отдельных узлов для каждого класса выходного вектора в случае классификации и единственного чис - лового вектора в случае регрессии.\n--- Страница 137 ---\n136  Искусс твенные нейронные сети и глубокое обучение Сила связей между узлами выражается весами, которые позже передаются функции активации. Цель функции активации состоит в том, чтобы преобразовы- вать свой вход в выход, который делает бинарные решения более разделимыми. Функции активации по приемуществу дифференцируемы, и поэтому они могут использоваться для обучения. Самыми популярными функциями активации являются сигмоидальная и ги- пербо лическая тангенсная (tanh), а совсем недавно получила распространение функция выпрямленный линейный узел (rectified linear unit, ReLU)1. Давайте сравним самые важные функции активации, чтобы разобраться в их преимущест - вах и не достатках. Отметим, что мы говорим о диапазонах выходных и актив- ных значений функции. Диапазон выходных значений – это просто фактические значения на выходе из самой функции. Диапазон активных значений, напротив, немного сложнее; это диапазон, где градиент имеет наибольшую дисперсию в за-ключительных обновлениях веса. Иными словами, за пределами этого диапазо-на градиент лежит близко к нулю и не вносит вклада в обновления параметров во время обучения. Проблему близкого к нулю градиента также называют проб - лемой исчезающего градиента, которая решается функцией активации ReLU. Данная функция активации в настоящее время является самой популярной и ис - пользуется в больших нейронных сетях: Важно отметить, что признаки должны быть прошкалированы в диапазон ак- тивных значений выбранной функции активации. В большинстве современных библиотек данная процедура предобработки является стандартной, и поэтому ее не придется делать самим: Сигмоид Диапазон активных значений: [sqrt(3), sqrt(3)]Диапазон выходных значений: (0, 1) 1 Эта функция аналогична однополупериодному выпрямителю в электротехнике. – Прим. перев.\n--- Страница 138 ---\nАрхитектура нейронной сети  137 Сигмоидальные функции часто используются для удобства математических расчетов, потому что их производные очень просто вычисляются, чем мы и вос - пользуемся для вычисления обновлений весовых коэффициентов во время тре- нировки алгоритмов: Функция tanh Диапазон активных значений: [–2, 2]Диапазон выходных значений: (–1, +1) Интересно отметить, что гиперболическая тангенсная и логистическая сиг - моидальная функции связаны линейно, и tanh может рассматриваться как пере-шкалированная версия сигмоиды, для того чтобы ее диапазон лежал в интервале между -1 и 1. Функция ReLU f(x) = max(0, x) Диапазон активных значений: [0, inf] Функция ReLU является наилучшим выбором для более глубоких архитектур. Ее можно рассматривать как линейно нарастающую функцию, диапазон которой находится выше 0 и до бесконечности. Хорошо видно, что ее намного проще вы- числить, чем сигмоидальную функцию. Самое большое преимущество этой функ - ции состоит в том, что она обходит проблему исчезающего градиента. Если ReLU выглядит как вариант для проекта с глубоким обучением, то непременно им вос - пользуйтесь. Функция мягкого максимума для классификации До сих пор мы видели, что функции активации, после того как они перемножены с весовыми векторами, преобразуют значения в определенном диапазоне. Мы также должны преобра-зовать выходы последнего скрытого слоя, прежде чем предоставить сбалансиро-ванные классы либо вероятностные выходы (значения логарифмического прав-доподобия). Данная функция будет преобразовывать выход предыдущего слоя в вероят - ностные значения, чтобы выполнить итоговое предсказание класса. Возведение в степень в этом случае будет возвращать почти нулевое значение каждый раз, когда выход значительно меньше, чем максимум всех значений; таким способом усиливаются различия: Прямое распространение сигнала Разобравшись в функциях активации и ко- нечных выходах из сети, теперь посмотрим, как входные признаки проходят по сети для предоставления итогового предсказания. Вычисления с участием огром-ных порций узлов и связей, возможно, выглядят сложной задачей, но, к счастью, процесс прямого распространения сигнала в нейронной сети сводится к последо-вательности векторных вычислений:\n--- Страница 139 ---\n138  Искусс твенные нейронные сети и глубокое обучение Cлой 1 (вход)Cлой 2 Cлой 3 Cлой 4 (выход) Вход Xi softmax θ1j θlj Мы приходим к итоговому прогнозу, выполнив следующие шаги: 1. Скалярное произведение на входах с весами между первым и вторым слоя- ми и преобразование результата функцией активации. 2. Скалярное произведение на выходах из первого скрытого слоя с весами меж - ду вторым и третьим слоями. Эти результаты затем преобразуются функци-ей активации в каждом узле второго скрытого слоя. 3. Наконец, мы приходим к прогнозу, умножив вектор на функцию активации (функцию мягкого максимума softmax для классификации). Каждый слой в сети можно рассматривать как вектор и применять простые век - торные умножения. Более формально это будет выглядеть следующим образом: z(2) = θ(1)x + b(1) a(2) = f(z(2)) трансформация после слоя 1 z(3) = θ(2)a(2) + b(2) h = a(3) = fsoftmax (z(3)) конечный выход с трансформацией функцией softmax где θ – это вектор весов слоя x, b(1) и b(2) – узлы смещения и f – функция активации.  Отметим, что этот пример основывается на сетевой архитектуре с единственным скрытым слоем. Давайте выполним простое прямое прохождение по нейронной сети с двумя скрытыми слоями, воспользовавшись элементарным функционалом библиотеки NumPy, и применим к конечному результату функцию мягкого максимума softmax : In: import numpy as npimport math b1 = 0 # узел смещения 1 b2 = 0 # узел смещения 2 def sigmoid(x): # сигмоидальная функция return 1 / (1+(math.e**-x)) def softmax(x): # функция мягкого максимума l_exp = np.exp(x) sm = l_exp/np.sum(l_exp, axis=0) return sm\n--- Страница 140 ---\nАрхитектура нейронной сети  139 # входной набор данных с 3 признаками X = np.array([ [.35,.21,.33], [.2,.4,.3], [.4,.34,.5], [.18,.21,16] ])len_X = len(X) # размер тренировочного набораinput_dim = 3 # размерность входного слояoutput_dim = 1 # размерность выходного слояhidden_units = 4np.random.seed(22) # создать векторы случайных весовых коэффициентов theta0 = 2*np.random.random((input_dim, hidden_units))theta1 = 2*np.random.random((hidden_units, output_dim)) # проход с прямым распространением сигнала d1 = X.dot(theta0)+b1 l1 = sigmoid(d1)l2 = l1.dot(theta1)+b2 # применить функцию softmax к выходу из конечного слоя output = softmax(l2)Out: array([[ 0.1635402 ], [ 0.15942338], [ 0.19455826], [ 0.48247816]])  Отметим, что узел смещения позволяет функции двигаться вверх-вниз и помогает плотнее по добрать целевые значения. Каждый скрытый слой содержит один узел смещения. Обратное распространение ошибки С прос тым примером использования пря- мого распространения сигнала мы сделали первые шаги в тренировке модели. Тренировка нейронных сетей выполняется вполне аналогично методам гради-ентного спуска, которые мы видели с другими алгоритмами машинного обуче-ния. А именно мы обновляем параметры модели для нахождения глобального ми-нимума функции ошибки. Важное отличие нейронных сетей заключается в том, что теперь мы должны иметь дело с многочисленными узлами по всей сети, ко-торые мы должны натренировать независимо. Мы делаем это, используя частную производную функции стоимости и вычисляя, насколько кривая ошибки падает, когда мы изменяем отдельно взятый признаковый вектор на определенную вели-чину (темп обучения). Мы начинаем с самого близкого к выходу слоя и вычисля-ем градиент относительно производной нашей функции потерь. Если существуют скрытые слои, то мы переходим во второй скрытый слой и обновляем веса, пока не будет достигнут первый слой сети прямого распространения сигнала. Центральная идея обратного распространения ошибки вполне аналогична дру - гим алгоритмам машинного обучения с одним важным усложнением – мы имеем дело с многочисленными слоями и узлами. Мы видели, что каждый слой в сети представлен весовым вектором θ ij. Тогда каким образом решить эту задачу? Не- обходимость независимо натренировать большое количество весов может выгля-деть устрашающе. Однако ради нашего удобства для этих целей мы можем задей-ствовать векторизованные операции. Аналогично тому, как мы делали во время\n--- Страница 141 ---\n140  Искус ственные нейронные сети и глубокое обучение прямого прохождения по сети, мы точно так же вычисляем градиенты и обновля- ем веса, принадлежащие весовым векторам (θij). В алгоритме обратного распространения ошибки можно резюмировать сле - дующие шаги: 1. Прямое прохождение: случайным образом инициализировать весовые век - торы и перемножить вход с последующими весовыми векторами вплоть до конечного результата. 2. Вычисление ошибки: вычислить меру ошибки/потери на выходе из этапа пря- мого прохождения. Случайным образом инициализировать весовые векторы. 3. Обратное распрос транение вплоть до последнего скрытого слоя (относи- тельно выхода). Вычислить градиент этой ошибки и изменить веса по на-правлению градиента. Это делается путем перемножения весового вектора θ j с полученными градиентами. 4. Обновление вес ов, пока не будет достигнут критерий останова (минималь- ная ошибка или число раундов тренировки): θij := θij – η*∆θJ(θij). Мы показали прохождение по произвольной двуслойной сети с прямым рас - пространением сигнала; теперь давайте применим к тому же самому входу, ко-торый мы использовали в приведенном ранее примере, метод обратного рас - пространения ошибки, используя для этого алгоритм SGD в библиотеке NumPy. Обратите особое внимание на то, как мы обновляем весовые параметры: In: import numpy as npimport math def sigmoid(x): # сигмоидальная функция return 1 / (1+(math.e**-x)) def deriv_sigmoid(y): # производная сигмоидальной функции return y * (1.0 - y) alpha = .1 # темп обученияX = np.array([ [.35,.21,.33], [.2,.4,.3], [.4,.34,.5], [.18,.21,16] ]) y = np.array([ [0], [1], [1], [0] ]) np.random.seed(1)# произвольно инициализировать слои theta0 = 2*np.random.random((3,4)) - 1theta1 = 2*np.random.random((4,1)) - 1 for iter in range(205000): # указываем количество тренировочных раундов # распространить вход вперед так же, # как было сделано в предыдущем упражнении input_layer = X l1 = sigmoid(np.dot(input_layer, theta0))\n--- Страница 142 ---\nАрхитектура нейронной сети  141 l2 = sigmoid(np.dot(l1, theta1)) # рассчитать ошибку l2_error = y - l2 if (iter% 1000) == 0: print(\"Точность нейросети:\" + str(np.mean(1-(np.abs(l2_error))))) # рассчитать градиенты в векторизованной форме; # функция softmax и узлы смещения пропущены в учебных целях l2_delta = alpha*(l2_error * deriv_sigmoid(l2)) l1_error = l2_delta.dot(theta1.T) l1_delta = alpha*(l1_error * deriv_sigmoid(l1)) theta1 += l1.T.dot(l2_delta) theta0 += input_layer.T.dot(l1_delta) Теперь убедимся, как с каждым прохождением по сети точность увеличивается: Out: Точность нейросети:0.983345051044Точность нейросети:0.983404936523Точность нейросети:0.983464255273Точность нейросети:0.983523015841Точность нейросети:0.983581226603Точность нейросети:0.983638895759Точность нейросети:0.983696031345Точность нейросети:0.983752641234Точность нейросети:0.983808733139Точность нейросети:0.98386431462Точность нейросети:0.983919393086 Точность нейросети:0.983973975799 Точность нейросети:0.984028069878 Точность нейросети:0.984081682304 Точность нейросети:0.984134819919 Типичные проблемы обратного распространения ошибки Известная проблема с нейронными сетями состоит в том, что во время оптимизации с обратным рас - пространением ошибки градиент может застревать в локальных минимумах. Это происходит, когда процесс минимизации ошибки попал в ловушку и видит мини-мум (точка S на рисунке) там, где это в действительности просто локальная неров-ность на пути прохождения пика S: Глобальный минимумЛокальный минимум\n--- Страница 143 ---\n142  Искус ственные нейронные сети и глубокое обучение Еще одна типичная проблема появляется, когда градиентный спуск не попада- ет по глобальному минимуму, что иногда может приводить к удивительно плохо работающим моделям. Данная проблема еще называется промахом. Обе эти проблемы можно решить путем выбора более низкого темпа обучения, когда модель промахивается, либо выбора более высокого темпа обучения при застревании в локальных минимумах. Иногда эта корректировка по-прежнему не приводит к удовлетворительному и быстрому схождению. В последнее время был найден целый ряд решений, которые смягчают эти проблемы. Были разработаны обучающиеся алгоритмы с доработками классических алгоритмов SGD, которые мы недавно рассматривали. Важно их понимать, чтобы можно было выбрать пра-вильный алгоритм, подходящий для любой конкретной задачи. Давайте рассмот - рим эти алг оритмы обучения более подробно. Обратное распространение в мини-пакетном режиме Пакетный градиентный спуск вычисляет градиент, используя весь набор данных, но алгоритм SGD об-ратного распространения ошибки может также работать с так называемыми ми- ни-пакетами, где выборка из набора данных размером k (пакеты) используется для обновления параметра обучения. Величина нерегулярности ошибки между каждым обновлением может быть сглажена мини-пакетом, который способен из-бегать застревания и промаха по локальным минимумам. В большинстве нейро-сетевых пакетов можно поменять размер пакета алгоритма (мы рассмотрим это позже). В зависимости от количества тренировочных примеров используется раз-мер пакета данных где угодно между 10 и 300. Тренировка с использованием инерции Инерция (momentum) – это метод, кото- рый добавляет долю предыдущего обновления веса к текущему1: vt+1 = μvt – η∇ℒ(θt); θt+1 = θt + vt+1. Здесь доля предыдущего обновления веса добавляется к текущему. Высокая величина параметра инерции помогает увеличивать скорость сходимости и в ре-зультате быстрее достигать глобального минимума. Глядя на формулу, вы видите параметр v . Это эквивалент скорости обновлений градиента с темпом обучения η. Чтобы разобраться в том, как он работает, надо просто понять, что, когда гради-ент продолжает указывать в одинаковом направлении на протяжении многочис - ленных прецедентов, скорость схождения увеличивается с каждым шагом к гло-бальному минимуму. Он также до определенного предела удаляет нерегулярности между градиентами. Параметр инерции имеется в большинстве библиотек (как мы убедимся в более позднем примере). Когда мы задаем слишком высокое зна-чение этого параметра, мы должны иметь в виду, что существует риск промаха по глобальному минимуму. С другой стороны, когда мы устанавливаем параметр инерции слишком низким, коэффициент может застрять в локальных миниму - мах, а также замедлить обучение. Идеальные значения коэффициента инерции обычно находятся в интервале между .5 и .99. 1 Этот метод помогает процессу схождения принять инерционную скорость, которая сде- лает его устойчивым к шуму, появляющемуся в первоначальной переходной фазе тре- нировки. – Прим. перев.\n--- Страница 144 ---\nАрхитектура нейронной сети  143 Инерция Нестерова Инерция Нес терова – это более новая и улучшенная вер- сия классической инерции. В дополнение к классической инерции он заглядыва- ет вперед в направлении градиента. Другими словами, инерция Нестерова делает простой шаг, идущий из x в y, и перемещается немного далее в том же направле- нии таким образом, что x в y принимает вид x в { y(v1 + 1)} в направлении, задан- ном предыдущей точкой. Мы обойдем технические подробности этого метода, но стоит напомнить, что он систематически превосходит нормальную инерцию с точки зрения сходимости. Если существует возможность применения инерции Нестерова, то ею следует воспользоваться. Метод ADAGRAD Мето д адаптивного градиента ADAGRAD обеспечивает харак - терный для признака темп обучения, привлекая информацию из предыдущих обновлений: g t+1 = gt – ∇ℒ(θt)2; Метод ADAGRAD обновляет темп обучения для каждого параметра согласно информации из ранее выполненных итерированных градиентов для этого пара-метра. Это делается путем деления каждой составляющей на квадратный корень суммы квадратов его предыдущего градиента. Благодаря этому темп обучения уменьшается в течение продолжительного времени, потому что сумма квадра-тов будет продолжать увеличиваться с каждой итерацией. Уменьшающийся темп обуче ния имеет вполне существенное преимущество в том, что риск промаха по глобальному минимуму сокращается. Метод RPROP Мето д устойчивого обратного распространения RPROP (resi - lient backpropagation) – это адаптивный метод, в котором историческая инфор- мация не используется, а вместо этого просто учитывается знак частной про-изводной тренировочного прецедента, и веса обновляются соответствующим образом. где 0 < η– < 1< η+. Прямой адаптивный метод для более быстрого обучения методом обратного распространения: Алго-ритм RPROP . Мартин Ридмиллер (Martin Riedmiller) 1993 Детально обследуя приведенные выше формулы, мы видим, что, как только частная производная ошибки изменяет свой знак (> 0 или < 0), градиент начинает двигаться в противоположном направлении, которое ведет к глобальному мини-муму, тем самым исправляя промах. Однако если знак не меняется вообще, то де-\n--- Страница 145 ---\n144  Искус ственные нейронные сети и глубокое обучение лаются более крупные шаги к глобальному минимуму. Во множестве статей было доказано превосходство метода RPROP над методом ADAGRAD, но на практике это не подтверждается систематически. Следует иметь в виду еще одну важную деталь – метод RPROP не работает надлежащим образом с мини-пакетами. Метод RMSProp Мето д адаптивного скользящего среднего градиентов RMSProp – это метод адаптивного обучения без сжатия темпа обучения: В этом методе также задействуется идея инерции и метода ADAGRAD, но с од- ним важным дополнением – он позволяет избегать уменьшения темпа обучения на протяжении всего времени. С этим методом сжатие контролируется функцией экспоненциального затухания на среднем числе значений градиентов. Ниже приведен список алгоритмов оптимизации по методу градиентного спуска: Приложения Типичные проблемы Практические подсказки Регулярный SGDШироко применим Промах, попадание в локальные минимумыИспользовать с инерцией и мини-пакетом ADAGRAD Наборы данных меньшего объема < 10kМедленная сходимость Использовать темп обучения между .01 и.1. Широко применим. Работает с разреженными данными RPROP Более крупные наборы данных > 10kНе эффективен с мини-пакетамиИспользовать RMSProp, если это возможно RMSProp Более крупные наборы данных > 10kНе эффективен с широкими и мелкими сетямиОсобенно полезен для широких разреженных данных Чему и как нейронные сети обучаются Теперь, когда у нас есть основное понимание метода обратного распространения ошибки во всех его формах, пора обратиться к наиболее трудной задаче в ней-росетевых проектах: как выбрать правильную архитектуру? Ключевая способ-ность нейронных сетей состоит в том, что веса внутри архитектуры могут преоб-разовывать вход в нелинейное пространство признаков и таким образом решать нелинейную классификацию (границы решения) и задачи регрессии. Давайте выполним простое и одновременно показательное упражнение с целью проде-монстрировать эту идею с использованием библиотеки neurolab . Мы воспользуем- ся библиотекой neurolab только для короткого упражнения; для масштабируемых задач обучения мы предложим другие методы. Прежде всего установите библиотеку neurolab при помощи менеджера библио- тек pip, используя для этого терминал: $ pip install neurolab В этом примере мы сгенерируем простую нелинейную косинусную функцию при помощи NumPy и натренируем нейронную сеть, чтобы предсказывать коси-\n--- Страница 146 ---\nАрхитектура нейронной сети  145 нусную функцию из переменной. Мы настроим несколько нейросетевых архитек - тур, чтобы увидеть, насколько хорошо каждая архитектура способна предсказы- вать косинусную целевую переменную: In:import neurolab as nlimport numpy as npfrom sklearn import preprocessing # Создать тренировочные выборки x = np.linspace(-10, 10, 60)y = np.cos(x) * 0.9size = len(x)x_train = x.reshape(size, 1)y_train = y.reshape(size, 1) # Создать сеть с 4 слоями и инициализировать случайным образом # Поэкспериментировать с количеством слоев d = [[1,1], [45,1], [45,45,1], [45,45,45,1]]for i in range(4): net = nl.net.newff([[-10, 10]], d[i]) train_net = nl.train.train_gd(net, x_train, y_train, epochs=1000, show=100) outp = net.sim(x_train) # Построить график результатов # (двойной график с кривой ошибки и предсказанными значениями) plt.subplot(2, 1, 1) plt.grid(True) plt.plot(train_net) plt.title(u'Скрытые слои: ' + str(i)) plt.xlabel(u'Эпохи') plt.ylabel(u'квадратическая ошибка') x2 = np.linspace(-10.0,10.0,150) y2 = net.sim(x2.reshape(x2.size, 1)).reshape(x2.size) y3 = outp.reshape(size) plt.subplot(2, 1, 2) plt.grid(True) plt.plot(x2, y2, '-',x , y, '.', x, y3, 'p') plt.legend([u'y предсказанное', u'y целевое']) saveplot('ex_4_' + str(i) + '.png') plt.show() Теперь приглядимся, как ведет себя кривая ошибки и как ожидаемые значения начинают аппроксимировать целевые значения по мере добавления к нейросети новых слоев. С нулевыми скрытыми слоями нейросеть проецирует прямую линию через це- левые значения. Кривая ошибки быстро падает до минимума с плохой подгонкой:\n--- Страница 147 ---\n146  Искусс твенные нейронные сети и глубокое обучение С одним скрытым слоем сеть начинает аппроксимировать целевой выход. Об- ратите внимание, насколько нерегулярна кривая ошибки: С двумя скрытыми слоями нейронная сеть аппроксимирует целевое значение еще ближе. Кривая ошибки падает быстрее и ведет себя менее нерегулярно:\n--- Страница 148 ---\nАрхитектура нейронной сети  147 Почти совершенная подгонка с тремя скрытыми слоями. Кривая ошибки пада- ет намного быстрее (приблизительно 220 итераций). Синяя линия в верхнем графике представляет собой визуализацию того, как ошибка падает с каждой эпохой (полное прохождение через тренировочный на- бор). Она показывает, что для достижения глобального минимума нужно опре-деленное число прохождений через тренировочный набор. Если обследовать эту кривую ошибки внимательнее, то вы увидите, что кривая ошибки с каждой архи-тектурой ведет себя по-разному. Нижний график (пунктирная линия) показыва-ет, как ожидаемые значения начинают аппроксимировать целевые значения. Без скрытого слоя нейронная сеть неспособна обнаруживать нелинейные функции,\n--- Страница 149 ---\n148  Искус ственные нейронные сети и глубокое обучение но как только мы добавляем скрытые слои, сеть начинает усваивать нелинейные функции и все более и более сложные функции. По сути дела, нейронные сети могут усваивать любую возможную функцию. Такая способность обучаться любой возможной функции называется универсальной теоремой аппроксимации (теорема Цыбенко). Мы можем модифицировать эту аппроксимацию, добавляя в нейронную сеть скрытые нейроны (узлы и слои). При этом, однако, нужно быть осторожными, чтобы не вызвать переподгонку; добавление большого количест - ва с лоев и узлов приведет к запоминанию тренировочных данных, вместо того чтобы подобрать оптимальную обобщаемую функцию. Довольно часто слишком много слоев в сети может оказаться вредным для точности прогнозирования. Выбор правильной архитектуры Как мы убедились, комбинаторное пространство возможных нейросетевых архи-тектур почти бесконечно. Тогда как узнать заранее, какая архитектура подойдет для нашего проекта? Хотелось бы иметь своего рода эвристическое или эмпи-рическое правило, для того чтобы сконструировать архитектуру для конкретной задачи. В последнем разделе мы использовали простой пример всего с одним выходом и одной функцией. Однако недавняя волна нейросетевых архитектур, которую мы называем глубоким обучением, весьма многосложна, и крайне важно уметь создавать правильную архитектуру нейронной сети для любой задачи. Как мы упомянули ранее, типичная нейронная сеть состоит из входного слоя, одного или более скрытых слоев и выходного слоя. Давайте подробно рассмотрим каж - дый слой архитектуры, с тем чтобы можно было ориентироваться при формиро-вании нужной архитектуры для той или иной задачи. Входной слой Когда мы упоминаем входной слой, мы в основном говорим о признаках, которые будут использоваться как вход в нейронную сеть. Требуемые шаги предобработки очень зависят от формы и содержания данных. Если имеются признаки, которые измеряются в разных шкалах, то нам нужно перешкалировать и нормализовать данные. В случаях, когда имеется большое количество признаков, рекомендуется использовать один из методов снижения размерности, в частности методы PCA или SVD. Следующие ниже методы предобработки могут быть применены ко входам перед процессом обучения: нормализация, шкалирование и обнар ужение выбросов; снижение размернос ти (метод SVD и факторный анализ); предварит ельная тренировка (автокодировщики и машины Больцмана). Каждый из этих методов будет рассмотрен в нижеследующих примерах. Скрытый слой Как выбрать количество узлов в скрытых слоях? Сколько скрытых слоев добавить к сети? В предыдущем примере мы увидели, что нейронная сеть без скрытого слоя неспособна обучиться нелинейной функции (это касается и подбора кривых для задачи регрессии, и границ решения для задачи классификации). Таким об-разом, если будет иметься нелинейная закономерность или граница решения для проецирования, то нам понадобятся скрытые слои. Когда дело касается выбора количества узлов в скрытом слое, обычно стремятся к тому, чтобы в скрытом слое\n--- Страница 150 ---\nАрхитектура нейронной сети  149 количество узлов было меньше, чем количество узлов во входном слое, и больше, чем количество выходных узлов. Желательно иметь: меньше скрытых узлов, чем количество входных признаков; больше узлов, чем количество выходных узлов (классы для классификации). Иногда, когда целевая функция очень сложна по форме, существует исключе- ние. В случае когда мы добавляем больше узлов, чем входных размерностей, мы добавляем расширение пространс тва признаков. Сети с такими слоями обычно называются широкими. Многосложные сети могут обучаться большему количеству сложных функций, но это не означает, что можно просто продолжать накладывать слои один на дру - гой. Рекомендуется держать количество слоев под контролем, потому что слиш-ком много слоев вызовет проблемы с переподгонкой, более высокой загрузкой CPU и даже недоподгонкой. Обычно достаточно иметь от одного до четырех скры-тых слоев.  В качестве отправной точки рекомендуется использовать от одного до четырех слоев. Выходной слой Каждая нейронная сеть имеет один выходной слой, который, аналогично вход-ному слою, очень зависит от структуры используемых данных. Для задачи клас - сификации мы обычно будем использовать функцию мягкого максимума softmax . В этом случае следует использовать такое же количество узлов, что и количество предсказываемых классов. Нейронные сети в действии Давайте получим небольшой практический опыт работы с тренировкой нейрон-ных сетей для классификации. Мы воспользуемся библиотекой sknn, которая представляет собой обертку Scikit-learn для библиотек lasagne и Pylearn2. Допол-нительную информацию об этой библиотеке можно узнать на https://github.com/ aigamedev/scikit-neuralnetwork/. Мы воспользуемся этим инструментом из-за его практичного и Python’овского интерфейса, а также потому, что он является великолепным введением в более сложные платформы, такие как библиотека Keras. Библиотека sknn может по вашему выбору выполняться как на CPU, так и на GPU. Отметим, что если вы решите использовать GPU, то sknn будет работать на Theano. Для CPU (наиболее стабильная версия): # Использовать CPU в 32-разрядном режиме, from sknn.platform import gpu32 from sknn.platform import cpu32, threading # Использовать CPU в 64-разрядном режиме, from sknn.platform import cpu64 from sknn.platform import cpu64, threading Для GPU: # Использовать GPU в 32-разрядном режиме from sknn.platform import gpu32# Использовать GPU в 64-разрядном режиме from sknn.platform import gpu64\n--- Страница 151 ---\n150  Искус ственные нейронные сети и глубокое обучение Параллелизация для библиотеки sknn Параллельная обработка может быть задействована следующим образом, но сразу нужно предупредить – это не самый стабильный метод: from sknn.platform import cpu64, threading Можно указать sknn использовать определенное количество процессов: from sknn.platform import cpu64, threads2 # любое нужное число потоков После того как задано соответствующее число процессов, программный код можно параллелизировать, добавив в перекрестную проверку n_jobs=nthreads , т. е. число заданий равно числу процессов. После того как были рассмотрены самые важные понятия и подготовлена сре- да, теперь приступим к реализации нейронной сети. Для этого примера мы при-меним удобный, но довольно скучный набор данных Iris. Загрузив данные, мы выполним их предобработку в форме нормализации и шкалирования и начнем строить модель: In: import numpy as npfrom sknn.mlp import Classifier, Layerfrom sklearn import model_selection, datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_irisfrom sklearn import preprocessingfrom sklearn.metrics import accuracy_score # импортировать уже знакомый набор данных Iris iris = datasets.load_iris()X_train, X_test, y_train, y_test = \\ train_test_split(iris.data, iris.target, test_size=0.2, random_state=0) Ниже применим ко входам предобработку, нормализацию и шкалирование: X_trainn = preprocessing.normalize(X_train, norm='l2')X_testn = preprocessing.normalize(X_test, norm='l2') X_trainn = preprocessing.scale(X_trainn) X_testn = preprocessing.scale(X_testn) Теперь сформируем нейросетевую архитектуру и параметры. Начнем с двух - слойной нейронной сети. В компоненте Layer мы независимо определяем на- стройки каждого слоя. (Мы снова встретим этот метод, когда будем работать с Tensorflow и Keras.) Набор данных цветков ириса Iris состоит из четырех призна-ков, но поскольку в данном конкретном случае широкая нейронная сеть работает вполне хорошо, мы будем использовать 13 узлов в каждом скрытом слое. Отме-тим, что sknn применяет алгоритм SGD по умолчанию: clf = Classifier( layers=[Layer(\"Rectifier\", units=13), Layer(\"Rectifier\", units=13), Layer(\"Softmax\")], learning_rate=0.001, learning_rule='sgd',\n--- Страница 152 ---\nНейронные сети и регуляризация  151 random_state=201, n_iter=200) model1 = clf.fit(X_trainn, y_train) y_hat = clf.predict(X_testn)scores = model_selection.cross_val_score(clf, X_trainn, y_train, cv=5) print('Средняя точность на тренировочных данных %s' % np.mean(scores)) print('Точность на тестовых данных с классическим sgd %s' % accuracy_score(y_hat,y_test)) Out: Средняя точность на тренировочных данных 0.949909090909Точность на тестовых данных с классическим sgd 0.933333333333 На тренировочном наборе получен порядочный результат, но можно было бы добиться большего успеха. Как уже обсуждалось, инерция Нестерова может сократить длину пути к гло- бальному минимуму; давайте выполним этот алгоритм, задав rule='nesterov' , что - бы посмотреть, удастся ли увеличить точность и улучшить сходимость: In:clf = Classifier( layers=[Layer(\"Rectifier\", units=13), Layer(\"Rectifier\", units=13), Layer(\"Softmax\")], learning_rate=0.001, learning_rule='nesterov', random_state=101, n_iter=1000) model1 = clf.fit(X_trainn, y_train) y_hat = clf.predict(X_testn)scores = model_selection.cross_val_score(clf, X_trainn, y_train, cv=5) print('Средняя точность на тренировочных данных с инерцией Нестерова %s' % np.mean(scores)) print('Точность на тестовых данных с инерцией Нестерова %s' % accuracy_score(y_hat,y_test)) Out: Средняя точность на тренировочных данных с инерцией Нестерова, 0.966575757576 Точность на тестовых данных с инерцией Нестерова 0.966666666667 В данном случае модель с инерцией Нестерова стала лучше. нейрОнные сети и регуляризация Несмотря на то что предыдущий пример не привел к переподгонке модели, сле- дует предусмотреть стратегии регуляризации нейронных сетей. Приведем три наиболее широко используемых способа, при помощи которых к нейронной сети можно применить регуляризацию:\n--- Страница 153 ---\n152  Искусс твенные нейронные сети и глубокое обучение регуляризация L1 и L2 с затуханием весов в качестве параметра для силы регуляризации; прореживание (dropout) – означает, что деактивирование узлов нейрон- ной сети методом случайного отбора может заставить другие узлы взять работу на себя; Слева находится обычная нейронная сеть. Справа – архитектура с применением прореживания, где узлы сети случайным образом деактивированы (помечены как X) усреднение, или ансамблирование, многочисленных нейронных сетей (каждый с разными настройками). Теперь попробуем применить к этой модели прореживание и посмотрим, будет ли оно работать: In: clf = Classifier( layers = [Layer(\"Rectifier\", units=13), Layer(\"Rectifier\", units=13), Layer(\"Softmax\")], learning_rate=0.01, n_iter=2000, learning_rule='nesterov', regularize='dropout', # задаем процедуру прореживания dropout_rate=.1, # доля прореживания нейронных узлов во всей сети random_state=0) model1 = clf.fit(X_trainn, y_train) scores = model_selection.cross_val_score(clf, X_trainn, y_train, cv=5)y_hat = clf.predict(X_testn) print('Средняя точность на тренировочных данных с прореживанием %s' % np.mean(scores)) print('Точность на тестовых данных с прореживанием %s' % accuracy_score(y_hat,y_test)) Out: Средняя точность на тренировочных данных с прореживанием 0.958242424242 Точность на тестовых данных с прореживанием 0.833333333333 В данном случае прореживание не привело к удовлетворительным результа- там, и поэтому его следует полностью исключить. Не стесняйтесь эксперименти-\n--- Страница 154 ---\nНейронные сети и гиперпараметрическая оптимизация  153 ровать и с другими методами. Просто измените параметр learning_rule и посмот - рите, что он делает с общей точностью. Можно попробовать модели sgd, momentum , nesterov , adagrad и rmsprop . Из этого примера вы узнали, что инерция Нестерова может увеличить общую точность, а прореживание dropout не стало подходящим методом регуляризации и оказалось вредным для результативности модели. Учи- тывая, что все эти многочисленные параметры взаимодействуют и приводят к не-предсказуемым результатам, нам действительно нужен метод тонкой настройки. Это именно то, чем мы займемся в следующем разделе. нейрОнные сети и гиперпараметрическая Оптимизация Поскольку параметрическое пространство нейронных сетей и моделей глубокого обучения очень широкое, оптимизация является задачей трудной и в вычисли-тельном отношении очень дорогостоящей. Неправильная архитектура нейронной сети может стать самым лучшим рецептом для неудачи. Эти модели могут быть точными, только если применены правильные параметры и выбрана правильная архитектура для решаемой задачи. К сожалению, существует всего несколько при-ложений, которые предоставляют методы тонкой настройки. Мы обнаружили, что наилучший метод тонкой настройки параметров на данный момент – это алгоритм рандомизированного поиска, который выполняет итеративный обход параме- трического пространства в случайном порядке, экономя вычислительные ресурсы. Библиотека sknn в действительности единственная, которая имеет эту возмож - ность. Давайте пройдемся по методам тонкой настройки (или доводки) параметров вместе со следующим ниже примером с набором данных Wine о качестве вин. В этом примере мы сначала загружаем набор данных Wine, их преобразуем и затем строим модель, основываясь на выбранных параметрах. Отметим, что этот набор данных имеет 13 признаков, и мы задаем узлы в каждом слое в коли-честве между 4 и 20. В данном случае мини-пакет не используется; набор данных просто слишком мал: In: import numpy as npimport scipy as sp import pandas as pdfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCVfrom scipy import statsfrom sklearn.model_selection import train_test_splitfrom sknn.mlp import Layer, Regressor, Classifier as skClassifier # загрузить данные df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep = ';')X = df.drop('quality', 1).values # отбросить целевую переменнуюy1 = df['quality'].values # исходная целевая переменнаяy = y1 <= 5 # новая целевая переменная: рейтинг <= 5? # разбить данные на тренировочный и тестовый наборы X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.2, random_state=42) print(X_train.shape)\n--- Страница 155 ---\n154  Искус ственные нейронные сети и глубокое обучение max_net = skClassifier( layers = [ Layer(\"Rectifier\",units=1), Layer(\"Rectifier\",units=1), Layer(\"Rectifier\",units=1), Layer(\"Softmax\") ])params = {'learning_rate' : [.002], 'hidden0__units': sp.stats.randint(8, 20), 'hidden0__type' : [\"Rectifier\"], 'hidden1__units': sp.stats.randint(8, 20), 'hidden1__type' : [\"Rectifier\"], 'learning_rule' :[\"adam\",\"rmsprop\",\"sgd\"]}max_net2 = RandomizedSearchCV(max_net, param_distributions=params, n_iter=10,cv=3,random_state=101, scoring='accuracy',verbose=10, pre_dispatch=None)model_tuning=max_net2.fit(X_train,y_train) print(\"Лучшая отметка %s\" % model_tuning.best_score_) print(\"Лучшие параметры %s\" % model_tuning.best_params_) Out: [CV] hidden0__units=11, learning_rate=0.100932183167, hidden2__units=4, hidden2__type=Rectifier, batch_size=30, hidden1__units=11, learning_rule=adagrad, hidden1__type=Rectifier, hidden0__type=Rectifier, score=0.655914 - 3.0s[Parallel(n_jobs=1)]: Done 74 tasks | elapsed: 3.0min[CV] hidden0__units=11, learning_rate=0.100932183167, hidden2__units=4, hidden2__type=Rectifier, batch_size=30, hidden1__units=11, learning_rule=adagrad, hidden1__type=Rectifier, hidden0__type=Rectifier[CV] hidden0__units=11, learning_rate=0.100932183167, hidden2__units=4, hidden2__type=Rectifier, batch_size=30, hidden1__units=11, learning_rule=adagrad, hidden1__type=Rectifier, hidden0__type=Rectifier, score=0.750000 - 3.3s[Parallel(n_jobs=1)]: Done 75 tasks | elapsed: 3.0min[Parallel(n_jobs=1)]: Done 75 out of 75 | elapsed: 3.0min finishedЛучшая отметка 0.721366278222 Лучшие параметры {'hidden0__units': 14, 'learning_rate': 0.03202394348494512, 'hidden2__units': 19, 'hidden2__type':'Rectifier', 'batch_size': 30, 'hidden1__units': 17, 'learning_rule':'adagrad', 'hidden1__type': 'Rectifier', 'hidden0__type': 'Rectifier'}  Предупреждение: ввиду того что поиск в параметрическом пространстве осуществляется в произво льном порядке, результаты могут быть непоследовательными. Мы видим, что лучшими параметрами для нашей модели являются прежде все- го первый слой с 14 узлами, второй слой с 17 узлами и третий слой с 19 узлами. Это достаточно сложная архитектура, которую мы никогда, возможно, не смогли бы вывести сами, что демонстрирует важность гиперпараметрической оптимизации. нейрОнные сети и границы решения В предыдущем разделе мы обсудили, что, добавляя скрытые узлы в нейронную сеть, можно ближе аппроксимировать функцию. Однако мы не применили ее\n--- Страница 156 ---\nНейронные сети и границы решения  155 к задаче классификации. Чтобы сделать это, мы сгенерируем данные с нелиней- ным целевым значением и посмотрим, как при добавлении в архитектуру скры-тых узлов изменяется поверхность решения. Пора взглянуть на универсальную теорему аппроксимации в деле! Сначала сгенерируем немного нелинейно раз-делимых данных с двумя признаками, сформируем нейросетевую архитектуру и посмотрим, как границы решения изменяются вместе с каждой архитектурой: In: import numpy as npfrom itertools import productfrom sklearn import preprocessingfrom sklearn import datasetsfrom sklearn.datasets import make_blobsfrom sknn.mlp import Classifier, Layer X, y = datasets.make_moons(n_samples=500, noise=.2, random_state=222)net1 = Classifier( layers=[Layer(\"Softmax\")], random_state=222, learning_rate=0.01, n_iter=100) net2 = Classifier( layers=[Layer(\"Rectifier\", units=4), Layer(\"Softmax\")], random_state=12, learning_rate=0.01, n_iter=100) net3 = Classifier( layers=[Layer(\"Rectifier\", units=4), Layer(\"Rectifier\", units=4), Layer(\"Softmax\")], random_state=22, learning_rate=0.01, n_iter=100) net4 = Classifier( layers=[Layer(\"Rectifier\", units=4), Layer(\"Rectifier\", units=4), Layer(\"Rectifier\", units=4), Layer(\"Rectifier\", units=4), Layer(\"Rectifier\", units=4), Layer(\"Rectifier\", units=4), Layer(\"Softmax\")], random_state=62, learning_rate=0.01, n_iter=100) net1.fit(X, y) net2.fit(X, y)net3.fit(X, y)net4.fit(X, y) # построение графика областей решения x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n--- Страница 157 ---\n156  Искус ственные нейронные сети и глубокое обучение y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) f, arxxx = plt.subplots(2, 2, sharey='row', sharex='col', figsize=(8, 8))f.suptitle(u'Нейронная сеть - граница решения', fontsize=14) for idx, clf, ti in zip(product([0, 1], [0, 1]), [net1, net2, net3,net4], [u'0 скрытых слоев', u'1 скрытый слой', u'2 скрытых слоя', u'6 скрытых слоев']): Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) arxxx[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.5) arxxx[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, alpha=0.5) arxxx[idx[0], idx[1]].set_title(ti) plt.show() Нейронная сеть – граница решения 0 скрытых слоев 2 скрытых слоя1 скрытый слой 6 скрытых слоев\n--- Страница 158 ---\nГлубокое обучение в крупном масштабе с H2O  157 На этом рисунке видно, что по мере добавления скрытых слоев в нейронную сеть можно извлекать все более и более сложные границы решения. Интересно отметить, что двуслойная сеть произвела самые точные прогнозы.  Отметим, что результаты от прогона к прогону могут различаться. глубОк Ое Обучение в крупнОм масштабе с h2o В предыдущих разделах мы рассмотрели нейронные сети и глубокую архитектуру, работающую на локальном компьютере, и узнали, что нейронные сети уже вы-соковекторизованы, но по-прежнему остаются в вычислительном плане дорого - ст оящими. Если возникнет потребность сделать алгоритм более масштабируе- мым на настольном компьютере, то не так уж и многого можно добиться, кроме как использовать Theano и вычисления на GPU. Поэтому если мы действительно хотим масштабировать алгоритмы глубокого обучения, то должны найти инстру - мент, который может выполнять алгоритмы вне ядра, а не на локальном CPU/GPU. Платформа H2O – это на сегодняшний день единственный внеядерный инстру - мент с открытым исходным кодом, который может быстро выполнять алгоритмы глубокого обучения. Этот инструмент является также кросс-платформенным; по-мимо Python, существуют API для R, Scala и Java. Платформа H2O скомпилирована на основе Java и разработана для широкого спектра связанных с наукой о данных задач, таких как обработка данных и ма-шинное обучение. Данная платформа выполняется на распределенных и парал-лельных CPU в оперативной памяти, а ее данные хранятся в кластере H2O. На данный момент она имеет приложения для общих линейных моде лей (General Linear Model, GLM), случайных лесов, машин градиентного бу стинга (Gradient Boosting Machines, GBM), k средних, наивного Байеса, анализа главных компонент, регрессии главных компонент и, конечно, для основной темы этой главы, глубо-кого обучения. Отлично, теперь мы готовы выполнить первый внеядерный анализ на плат - форме H2O. Давайте запустим экземпляр H2O и загрузим файл в ее систему с распределен- ной памятью: In:import syssys.prefix = \"/usr/local\"import h2o h2o.init(start_h2o=True) Наберите нижеследующую команду, чтобы получить интересную информацию о спецификации вашего кластера. Обратите внимание на допустимую память и число ядер. h2o.cluster().show_status()h2o.cluster().shutdown(prompt=False) Эта информация будет более или менее выглядеть следующим образом (могут быть незначительные различия между пробными версиями и системами):\n--- Страница 159 ---\n158  Искус ственные нейронные сети и глубокое обучение Out: Checking whether there is an H2O instance running at http://localhost:54321 not found.Attempting to start a local H2O server ; Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode) Starting server from c:\\python27\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar Ice root: c:\\users\\labor\\appdata\\local\\temp\\tmpqpvewu JVM stdout: c:\\users\\labor\\appdata\\local\\temp\\tmpqpvewu\\h2o_labor_started_from_python.out JVM stderr: c:\\users\\labor\\appdata\\local\\temp\\tmpqpvewu\\h2o_labor_started_from_python.err Server is running at http://127.0.0.1:54321Checking whether there is an H2O instance running at http://localhost:54321. connected.H2O cluster uptime: 5 mins 38 secsH2O cluster version: 3.10.4.8H2O cluster version age: 23 daysH2O cluster name: H2O_from_python_labor_3ljeweH2O cluster total nodes: 1H2O cluster free memory: 1.258 GbH2O cluster total cores: 4H2O cluster allowed cores: 4H2O cluster status: locked, healthyH2O connection url: http://localhost:54321H2O connection proxy: NoneH2O internal security: FalsePython version: 2.7.13 finalH2O cluster uptime: 5 mins 38 secsH2O cluster version: 3.10.4.8H2O cluster version age: 23 daysH2O cluster name: H2O_from_python_labor_3ljewe H2O cluster total nodes: 1H2O cluster free memory: 1.258 GbH2O cluster total cores: 4H2O cluster allowed cores: 4H2O cluster status: locked, healthyH2O connection url: http://localhost:54321H2O connection proxy: NoneH2O internal security: FalsePython version: 2.7.13 final H2O session _sid_8f32 closed. Крупномасштабное глубокое обучение с H2O При выполнении задачи глубокого обучения на основе H2O для тренировки моде- ли мы воспользуемся известным набором данных MNIST. Он состоит из изобра - жений рукописных цифр в формате 28×28 разной интенсивности пикселов. Тре- нировочный набор имеет 70 000 тренировочных элементов с 784 признаками вместе с меткой для каждой записи, содержащей целевую метку digits. Теперь, когда мы чувствуем себя увереннее в управлении данными в H2O, да- вайте выполним пример с глубоким обучением. В платформе H2O преобразование либо нормализация входных данных не тре- буется; эта стандартная процедура встроена и выполняется изнутри автоматиче-ски. Каждый признак преобразуется в пространство N(0, 1). Давайте импортируем набор данных MNIST с изображением рукописных цифр из сервера Amazon в кластер H2O:\n--- Страница 160 ---\nГлубокое обучение в крупном масштабе с H2O  159 In: import h2o h2o.init(start_h2o=True) train_url = \"https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz\" test_url = \"https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz\" train = h2o.import_file(train_url) test = h2o.import_file(test_url) train.describe() test.describe() In: y = 'C785'x = train.names[0:784]train[y] = train[y].asfactor()test[y] = test[y].asfactor() from h2o.estimators.deeplearning import H2ODeepLearningEstimatormodel_cv = H2ODeepLearningEstimator(distribution='multinomial', activation='RectifierWithDropout', hidden=[32,32,32], input_dropout_ratio=.2, sparse=True, l1=.0005, epochs=5, nfolds=3) Итоговый результат предоставит много подробной информации1. Ниже пока- зана первая таблица, которую вы увидите. Она предоставляет все специфические подробности об архитектуре нейронной сети. Хорошо видно, что мы использо-вали нейронную сеть, вход в которую имеет размерность 717, с тремя скрытыми слоями (состоящими из 32 узлов каждый), с функцией активации softmax, приме-няемой к выходному слою, и функцией ReLU между скрытыми слоями: model_cv.train(x=x, y=y, training_frame=train)model_cv.show()Scoring the model.Status of Neuron Layers (predicting C785, 10-class classification, multinomial distribution, CrossEntropy loss, 25 418 weights/biases, 404,0 KB, 8 807 training samples, mini-batch size 1): 1 Вся информация также хранится в журнале регистрации событий, который можно ска- чать командой h2o.download_all_logs(dirname='./data/', filename = 'autoh2o_log.zip' ). В этом архиве нас интересует файл h2o_127.0.0.1_00000-0-info.log . – Прим. перев.\n--- Страница 161 ---\n160  Искус ственные нейронные сети и глубокое обучение Model Metrics Type: Multinomial Description: Metrics reported on temporary training frame with 10130 samples model id: DeepLearning_model_python_1497852092123_1_cv_1 frame id: DeepLearning_model_python_1497852092123_1_cv_1_train.temporary.sample.16,67% MSE: 0.80062 RMSE: 0.8947737 logloss: 2.257932 mean_per_class_error: 0.77239877 Если нужно получить краткую сводку о результативности модели, то это очень практичный способ. В следующей таблице показаны самые интересные метрики – это ошибка клас - сификации фазы тренировки и ошибка классификации фазы проверки на каждом блоке. В случае если вы хотите оценить правильность своей модели, эти метрики можно легко сравнить: print(model_cv.scoring_history()) Ошибка классификации фазы тренировки .091017 и т очность в районе .908 на наборе данных MNIST являются довольно хорошими показателями; они почти столь же хороши, что и сверточное представление нейронной сети Яна Лекуна. Платформа H2O также предоставляет вспомогательный метод для получения метрик проверки. Это можно сделать, передав проверочную таблицу данных в функцию перекрестной проверки: model_cv.train(x=x, y=y, training_frame=train, validation_frame=test) model_cv.show() Статистика результативности: training_rmse training_logloss training_ classification_errorvalidation_rmse validation_logloss validation_ classification_error NaN NaN NaN NaN NaN NaN 0.783207 1.640004 0.490323 0.782864 1.640282 0.4876 0.384048 0.478873 0.113892 0.378055 0.471375 0.10690.348065 0.416549 0.104367 0.344056 0.413099 0.1009 В этом случае можно легко сравнить ошибку классификации фазы трениров- ки training_classification_error (.1044) с ошибкой классификации фазы проверки validation_classification_error (.1009). Возможно, удастся улучшить эту отметку; давайте воспользуемся моделью ги- перпараметрической оптимизации.\n--- Страница 162 ---\nГлубокое обучение в крупном масштабе с H2O  161 Сеточный поиск в H2O Учитывая, что предыдущая модель показала достаточно хорошую результатив- ность, мы сосредоточим свои усилия на доводке параметров сетевой архитекту - ры. Функция H2O сеточного поиска gridsearch весьма похожа на рандомизирован- ный поиск в Scikit-learn; а именно, вместо того чтобы выполнять исчерпывающий поиск по всему параметрическому пространству, она выполняет итеративный об-ход случайного списка параметров. Сначала мы сформируем список параметров, который мы передадим в функцию gridsearch . Платформа H2O предоставит нам результат для каждой модели и соответствующую отметку, полученную в резуль-тате параметрического поиска: In:import h2o from h2o.estimators.deeplearning import H2ODeepLearningEstimatorfrom h2o.grid.grid_search import H2OGridSearch h2o.init(start_h2o=True) train_url = \"https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz\" test_url = \"https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz\" train = h2o.import_file(train_url) test = h2o.import_file(test_url) hidden_opt = [[18,18],[32,32],[32,32,32],[100,100,100]] hyper_parameters = {\"hidden\":hidden_opt} # важно: здесь мы задаем параметры поиска # Будьте осторожны - время тренировки может иметь взрывной рост# (см. max_models)search_c = {\"strategy\":\"RandomDiscrete\", \"max_models\":10, \"max_runtime_secs\":100, \"seed\":222} model_grid = H2OGridSearch(H2ODeepLearningEstimator, hyper_params=hyper_parameters) model_grid.train(x=x, y=y, distribution=\"multinomial\", epochs=1000, training_frame=train, validation_frame=test, score_interval=2, stopping_rounds=3, stopping_tolerance=0.05, search_criteria=search_c) print(model_grid)# Результаты сеточного поиска для оценщика H2ODeepLearningEstimator:Out: hidden \\0 [100, 100, 100]1 [32, 32, 32]2 [32, 32]3 [18, 18] model_ids logloss 0 Grid_DeepLearning_py_1_model_python_1464790287811_3_model_3 0.1481621 Grid_DeepLearning_py_1_model_python_1464790287811_3_model_2 0.173675\n--- Страница 163 ---\n162  Искус ственные нейронные сети и глубокое обучение 2 Grid_DeepLearning_py_1_model_python_1464790287811_3_model_1 0.212246 3 Grid_DeepLearning_py_1_model_python_1464790287811_3_model_0 0.227706 Мы видим, что наша лучшая архитектура будет иметь три слоя со 100 узлами каждый. Мы также ясно видим, что функция gridsearch существенно увеличивает время тренировки даже на таком мощном вычислительном кластере, на котором выполняется H2O. Поэтому даже на H2O сеточный поиск следует использовать с осторожностью и быть консервативными с параметрами, которые анализиру - ются в модели. Теперь, прежде чем продолжить изложение, давайте завершим работу экземп - ляра H2O: h2o.cluster().shutdown(prompt=False) глубОк Ое Обучение и предтренир Овка без учителя В данном разделе мы представим самую важную методологию глубокого обуче - ния, а именно способы улучшения обучения при помощи предварительной тренировки (или инициализации) без учителя. Нейронные сети используются с предтренировкой без учителя для отыскания в данных латентных признаков и факторов, с тем чтобы передать их дальше в нейронную сеть. Этот метод обла-дает мощной способностью тренировать сети для усвоения задач, которые другие методы машинного обучения не способны решать без ручного конструирования признаков. Мы познакомим со специфическими подробностями этого метода и представим новую мощную библиотеку. глубОк Ое Обучение с theanetS Нейросетевое приложение Scikit-learn особенно интересно для целей тонкой на-стройки параметров. Но, к сожалению, его возможности для нейросетевых при-ложений без учителя ограничены. Для следующей темы, где мы затронем более сложные методы глубокого обучения, нам потребуется еще одна библиотека. В настоящей главе мы сосредоточимся на библиотеке theanets. Разработанная Лифом Джонсоном (Lief Johnson) в Техасском университете библиотека theanets привлекательна простотой использования и своей стабильной работой. Она хо-рошо отшлифована и поддерживается в хорошем состоянии. Формирование ней-росетевой архитектуры происходит вполне аналогично модулю sklearn; а именно инстанцируется цель обучения (классификация или регрессия), определяются слои и тренируется нейросеть. Для получения дополнительной информации от - носительно библиотеки можно посетить веб-страницу http://theanets.readthedocs. org/en/stable/. Чтобы начать работать с библиотекой theanets , нужно ее установить менедже- ром библиотек pip: $ pip install theanets Поскольку библиотека theanets сконструирована поверх библиотеки Theano, также необходимо, чтобы должным образом была установлена библиотека Theano. Давайте реализуем элементарную нейросетевую модель, чтобы увидеть,\n--- Страница 164 ---\nГлубокое обучение с theanets  163 как работает библиотека theanets. Сходство с Scikit-learn будет очевидным. От - метим, что в приводимом ниже примере мы используем инерцию и что функция мягкого максимума softmax применяется в theanets по умолчанию, так что ее ука-зывать не придется: In:import climate # предоставляет отчетность в отношении итераций import numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_errorfrom sklearn import preprocessing, datasetsimport theanetsimport theano climate.enable_default_logging() digits = datasets.load_digits()digits = datasets.load_digits() X = np.asarray(digits.data, 'float32') Y = digits.target Y = np.array(Y, dtype=np.int32) #X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001) # шкалирование [0-1] X_train, X_test, y_train, y_test = \\ train_test_split(X, Y, test_size=0.2, random_state=0) # Построить модель классификатора с 64 входами, # 1 скрытым слоем из 100 узлов и с 10 выходами. net = theanets.Classifier([64,100,10]) N = 10 # Натренировать модель с использованием устойчивого метода # обратного распространения и инерции. net.train([X_train,y_train], algo='sgd', learning_rate=.001, momentum=0.9, patience=0, validate_every=N, min_improvement=0.8) # Показать матрицы ошибок на тренировочном/контрольном наборе. print(confusion_matrix(y_test, net.predict(X_test)))print(accuracy_score(y_test, net.predict(X_test))) Out: [[27 0 0 0 0 0 0 0 0 0] [ 0 32 0 0 0 1 0 0 0 2] [ 0 1 34 0 0 0 0 1 0 0] [ 0 0 0 29 0 0 0 0 0 0] [ 0 0 0 0 29 0 0 1 0 0] [ 0 0 0 0 0 38 0 0 0 2] [ 0 1 0 0 0 0 43 0 0 0] [ 0 0 0 0 1 0 0 38 0 0] [ 0 2 1 0 0 0 0 0 36 0] [ 0 0 0 0 0 1 0 0 0 40]]0.961111111111\n--- Страница 165 ---\n164  Искусс твенные нейронные сети и глубокое обучение автОкОдир Овщики и Обучение без учителя До настоящего момента в центре обсуждения были нейронные сети с многочис - ленными слоями и большим разнообразием параметров для оптимизации. Те- кущее поколение нейронных сетей, которое часто называют глубоким обучени-ем, способно на большее; они могут автоматически обучаться новым признакам, в результате чего почти не требуется конструировать признаки и обладать пред-метными знаниями. Такие признаки создаются методами машинного обучения без учителя на немаркированных данных, которые дальше подаются в последую - щий слой нейронной сети. Метод их создания называется предварительной тре- нировкой (без учителя), и он зарекомендовал себя как чрезвычайно успешный подход в распознавании образов, усвоении языка и даже в классических проек - тах машинного обучения. Самая важная и доминирующая методика последних лет – это помехоустойчивые автоко дировщики и алгоритмы на основе мето- дов Больцмана. Машины Больцмана, которые были структурными элементами для глубокой сети доверия (deep belief network, DBN), за последнее время стали выходить из употребления в профессиональной среде глубокого обучения по при-чине того, что их оказалось трудно тренировать и оптимизировать. Поэтому мы сосредоточимся только на автокодировщиках. Давайте обсудим эту важную тему небольшими и понятными шагами. Автокодировщик Мы пытаемся найти функцию (F ), которая имеет такой же выход, что и ее вход, с наи- менее возможной ошибкой F(x) ≈ 'x . Такая функция обычно называется тождествен- ным отображ ением, которое мы стремимся оптимизировать так, чтобы x был мак - симально близок к 'x . Разница между x и 'x называется ошибкой реконс трукции. Рассмотрим простую однослойную архитектуру, чтобы получить интуитивно понятное представление о том, что происходит. Мы увидим, что данная архитек - тура очень гибка и нуждается в тщательной тонкой настройке: Выходной вектор X'Входной вектор XВесовой вектор W Одноуровневая архитектура автокодировщика\n--- Страница 166 ---\nАвтокодировщики и обучение без учителя  165 Важно понимать, что когда в скрытом слое узлов меньше, чем входное про- странство, мы побуждаем веса сжимать входные данные. В данном случае мы имеем набор данных с пятью признаками. В середине на- ходится скрытый слой, содержащий три узла (Wij). Эти узлы имеют то же самое свойство, что и весовой вектор, который мы видели в нейронных сетях; а именно они состоят из весов, которые могут быть натренированы методом обратного рас - пространения ошибки. Вместе с выходом из скрытого слоя мы получаем пред-ставления признаков как результат тех же самых векторных операций по прямо-му распространению сигнала, что мы видели у нейронных сетей. Процесс вычисления вектора 'x очень похож на тот, который мы видели у мето- да прямого распространения сигнала, выполняемого путем вычисления скаляр-ных произведений весовых векторов каждого слоя: h = sigmoid((W 1, x * x) + b1(i, 1)); 'x = sigmoid((W2, x * hi) + b2(i, 1)). Здесь W – это веса. Ошибку реконструкции можно измерить при помощи квад - ратической ошибки либо в перекрестно-энтропийной форме, которые мы видели в большом количестве других методов. В данном случае yˆ обозначает реконструи- рованный выход, а y – истинный вход: Перекрестная энтропия Отметим важный принцип – при наличии всего одного скрытого слоя захва- ченные моделью автокодировщика размерности в данных аппроксимируют ре-зультаты анализа главных компонент (PCA). Однако автокодировщик ведет себя во многом по-другому, если присутствует нелинейность. Автокодировщик обнаружит различные скрытые факторы, которые метод PCA никогда не будет в состоянии обнаружить. Зная больше об архитектуре автокодировщика и то, как можно вычислить отклонение (ошибку) от ее аппроксимации тождества, теперь посмотрим на параметры разреж енности, при помощи которых мы сжимаем входные данные. Вы можете задасться вопросом: зачем вообще нужен этот параметр разрежен- ности? Разве нельзя просто выполнить алгоритм, чтобы найти тождественное отображение, и идти дальше? К сожалению, это не так уж и просто. Существуют случаи, когда тождественное отображение проецирует вход почти идеально, и при этом по-прежнему не уда-ется извлечь латентные размерности входных признаков. В этом случае функция просто запоминает входные данные, вместо того чтобы извлечь значимые при-знаки. В связи с этим можно сделать две вещи. Во-первых, сознательно добавить шум к сигналу (помехоустойчивые авток одировщики) и, во-вторых, ввести параметр разреженности, вызывая деактивацию слабо активированных узлов. Давайте сначала посмотрим на то, как работает разреженность. Представим порог активации биологического нейрона; нейрон может быть в активном состоянии, если его потенциал близок к 1, либо неактивном, если его выходное значение близко к 0. Можно наложить на нейроны ограничение оста-ваться неактивными большую часть времени, увеличив порог активации. Это делается путем уменьшения средней вероятности активации каждого нейрона/\n--- Страница 167 ---\n166  Искус ственные нейронные сети и глубокое обучение узла. Глядя на следующую ниже формулу, мы видим, как можно минимизировать порог активации: Здесь – это средний порог активации каждого нейрона в скрытом слое, ρ – тре- буемый порог активации сети, который мы определяем заранее (в большинстве случаев это значение устанавливается в .05), α – весовой вектор скрытых слоев. Возможность для оптимизации здесь появляется, если штрафовать трениро- вочный раунд на коэффициент ошибки между порогами и ρ. В этой главе мы не будем слишком переживать по поводу технических подроб - ност ей этой цели оптимизации. В большинстве библиотек для этого можно вос - пользоваться очень простой строкой программного кода (как мы убедимся в сле-дующем примере). Самое главное – понять, что с участием автокодировщиков имеются две основные цели обучения: уменьшение ошибки между входным век - тором x и выходным вектором 'x путем оптимизации тождественного отображе- ния и уменьшение разницы между требуемым порогом активации и средней ак - тивацией каждого нейрона в сети. Второй способ, которым можно вынудить автокодировщик обнаруживать ла- тентные признаки, состоит во введении в модель шума; именно отсюда про-исходит название помехоустойчивые авток одировщики. Идея заключается в том, чтобы путем повреждения входных данных вынуждать автокодировщик обучать ся бо лее устойчивому представлению данных. В нижеследующем при- мере мы введем в модель автокодировщика гауссов шум, т. е. нормально рас - пределенный. По-настоящему глубок ое обу чение с каскадными помех оустойчивыми авток одировщиками – пре дтренировка для к лассификации. Изучив это упражнение, вы станете непохожим на тех многих людей, которые говорят о глубоком обучении, в отличие от тех немногих, кто в действительности им занимается! Теперь мы применим автокодировщик к мини-версии известного набора данных MNIST, который можно легко загрузить из Scikit-learn. Напомним, что этот набор данных состоит из изображений рукописных цифр в формате 28×28 разной пиксельной интенсивности. Тренировочный набор мини-версии имеет 1797 тренировочных элементов с 64 признаками с метками для каждой записи, содержащей целевые метки с цифрами от 0 до 9. Таким образом, имеются 64 при- знака с целевой переменной, состоящей из 10 классов (цифры от 0–9) для пред-сказания. Для начала натренируем модель каскадного помехоустойчивого автокодиров- щика с разреженностью .9 и обследуем ошибку реконструкции. В качестве ру - ководства для настроек мы воспользуемся результатами научно-исследователь-ских работ по глубокому обучению. Для получения дополнительной информации можно прочесть вот эту статью: http://arxiv.org/pdf/1312.5663.pdf. Однако у нас есть некоторые ограничения вследствие огромной вычислительной нагрузки для таких типов моделей. Поэтому для создаваемого автокодировщика мы вос - пользуемся пятью слоями с функцией активации ReLU и сожмем данные с 64 до 45 признаков:\n--- Страница 168 ---\nАвтокодировщики и обучение без учителя  167 In: model = theanets.Autoencoder([64,(45,'relu'),(45,'relu'), (45,'relu'),(45,'relu'), (45,'relu'),64])dAE_model = model.train([X_train], algo='rmsprop', input_noise=0.1, hidden_l1=.001, sparsity=0.9, num_updates=1000)X_dAE = model.encode(X_train)X_dAE = np.asarray(X_dAE, 'float32')Out:I 2017-06-14 12:25:27 theanets.layers.base:371 layer Input \"in\": 64 inputsI 2017-06-14 12:25:27 theanets.layers.base:207 layer Feedforward \"hid1\": (in:out)64 -> 45, relu, 2925 parametersI 2017-06-14 12:25:27 theanets.layers.base:207 layer Feedforward \"hid2\": (hid1:out)45 -> 45, relu, 2070 parametersI 2017-06-14 12:25:27 theanets.layers.base:207 layer Feedforward \"hid3\": (hid2:out)45 -> 45, relu, 2070 parametersI 2017-06-14 12:25:27 theanets.layers.base:207 layer Feedforward \"hid4\": (hid3:out)45 -> 45, relu, 2070 parametersI 2017-06-14 12:25:27 theanets.layers.base:207 layer Feedforward \"hid5\": (hid4:out)45 -> 45, relu, 2070 parametersI 2017-06-14 12:25:27 theanets.layers.base:207 layer Feedforward \"out\": (hid5:out)45 -> 64, linear, 2944 parametersI 2017-06-14 12:25:27 theanets.graph:94 network has 14149 total parametersvalid: 45 of 45 mini-batches from (1437L, 64L)train: 45 of 45 mini-batches from (1437L, 64L)I 2017-06-14 12:25:27 theanets.graph:447 building computation graphI 2017-06-14 12:25:27 theanets.losses:67 using loss: 1.0 * MeanSquaredError (output out:out)I 2017-06-14 12:25:27 theanets.regularizers:711 regularizer: 0.1 * GaussianNoise(('in:out',))I 2017-06-14 12:25:27 theanets.regularizers:160 regularizer: 0.001 * HiddenL1(None)downhill: compiling evaluation function Теперь, когда имеется результат работы автокодировщика, который мы созда- ли из нового набора со сжатыми признаками, давайте рассмотрим этот новый набор данных поближе: In:X_dAE.shape Out: (1437, 45) Здесь в действительности видно, что мы сжали данные с 64 до 45 признаков. Новый набор данных менее разрежен (т. е. с меньшим числом нулей) и численно более непрерывен. Имея в распоряжении предварительно натренированные дан-ные из автокодировщика, теперь можно применить к ним глубокую нейронную сеть, чтобы выполнить обучение с учителем: # Скрытые слои по умолчанию используют передаточную функцию relu, # поэтому нам не нужно указывать их. # Relu – это наилучший вариант для автокодировщиков. # Классификатор Theanets также по умолчанию использует функцию softmax,# поэтому нам тоже не нужно их указывать.\n--- Страница 169 ---\n168  Искус ственные нейронные сети и глубокое обучение net = theanets.Classifier(layers=(45,45,45,10)) autoe = net.train([X_dAE, y_train], algo='rmsprop', learning_rate=.0001, batch_size=110, min_improvement=.0001, momentum=.9, nesterov=True, num_updates=1000) ## Получите удовольствие от редкой радости лицезреть ## 100% точность на тренировочном наборе. Out: I 2016-04-19 10:33:07 downhill.base:232 RMSProp 14074 loss=0.000000err=0.000000 acc=1.000000I 2016-04-19 10:33:07 downhill.base:232 RMSProp 14075 loss=0.000000err=0.000000 acc=1.000000I 2016-04-19 10:33:07 downhill.base:232 RMSProp 14076 loss=0.000000err=0.000000 acc=1.000000 Прежде чем применить эту нейронную сеть для предсказания на тестовом на- боре, сперва необходимо применить натренированную модель автокодировщика к тестовому набору: dAE_model = model.train([X_test], algo='rmsprop', input_noise=0.1, hidden_l1=.001, sparsity=0.9, num_updates=100)X_dAE2=model.encode(X_test)X_dAE2=np.asarray(X_dAE2, 'float32') Теперь проверим результативность модели на тестовом наборе: final=net.predict(X_dAE2)from sklearn.metrics import accuracy_scoreprint accuracy_score(final,y_test) Out: 0.972222222222 Мы видим, что итоговая точность модели с автокодированными признаками (.9722) превосходит точность модели без них (.9611). резюме В этой главе наряду с масштабируемыми решениями мы рассмотрели самые важ - ные понятия, лежащие в основе глубокого обучения. Мы убрали с этой темы часть покрова секретности, изучив способы создания соответствующей архитектуры для той или иной задачи и проанализировав ме-ханизмы прямого и обратного распространений. Обновление весов нейронной сети представляет собой трудную задачу, причем обычный стохастический гради-ентный спуск может приводить к застреванию в глобальных минимумах или про-маху. Более сложные алгоритмы, такие как инерция, ADAGRAD, RPROP и RMSProp, могут предоставить приемлемые решения. Несмотря на то что нейронные сети тренировать труднее, чем другие методы машинного обучения, они способны преобразовывать представления признаков и могут обучаться любой отдельно взятой функции (согласно универсальной теореме аппроксимации). Мы также\n--- Страница 170 ---\nРезюме  169 коснулись крупномасштабного глубокого обучения с использованием платформы H2O и даже задействовали очень горячую тему параметрической оптимизации для глубокого обучения. Предтренировка без учителя при помощи автокодировщиков может увели- чивать точность любой отдельно взятой глубокой сети, и мы проанализировали практический пример в рамках библиотеки theanets, чтобы разобраться в этой теме. В этой главе мы прежде всего работали с библиотеками, созданными поверх платформы Theano. В следующей главе мы раскроем методы глубокого обучения на основе библиотек, созданных поверх новой платформы с открытым исходным кодом TensorFlow.",
      "debug": {
        "start_page": 135,
        "end_page": 170
      }
    },
    {
      "name": "Глава 5. Глубокое обучение с библиотекой T ensorFlow 170",
      "content": "--- Страница 171 --- (продолжение)\nГлава 5 Глубокое обучение с библиотекой T ensorFlow В этой главе, где в центре нашего внимания будет TensorFlow, мы затронем сле- дующие темы: элементарные операции T ensorFlow; машинное об учение с нуля с TensorFlow – регрессия, классификатор на ос - нове SGD и нейронная сеть; глубок ое обучение с SkFlow; инкрементное глубок ое обучение с большими файлами; сверт очные нейронные сети с Keras. Платформа TensorFlow была выпущена во время написания данной книги и уже зарекомендовала себя как великолепное дополнение к ландшафту машинного обуче ния. Данная плат форма была запущена командой Google Brain, состоящей из большин- ства исследователей, которые в последнее десятилетие работали над важными раз-работками в области глубокого обучения (Джеффри Хинтон, Сэми Бенджио и дру - гие). Это, в сущности, развитие платформы более раннего поколения под названием DistBelief для распределенных глубоких нейронных сетей. В отличие от TensorFlow, DistBelief не являет ся платформой с открытым исходным кодом. Интересными при- мерами успешных проектов DistBelief является система обратного поиска изображе-ний, Google’овская программа компьютерного зрения DeepDream и распознавание речи в приложениях Google. DistBelief позволил разработчикам Google использовать тысячи ядер (CPU и GPU) для распределенной тренировки моделей. Платформа TensorFlow является шагом вперед, по сравнению с DistBelief, в том, что она теперь имеет абсолютно открытый исходный код, а ее язык программи-рования менее абстрактен. TensorFlow по праву считается более гибкой платфор-мой, которая к тому же имеет более широкий диапазон приложений. Во время написания книги (в конце 2015 г.) платформа TensorFlow находилась на этапе ста-новления, и, как мы увидим далее в этой главе, уже появились интересные легко-весные библиотеки, созданные поверх этой платформы. Аналогично Theano, платформа TensorFlow работает с символьными вычисле- ниями на тензорах, т. е. большинство вычислений в платформе основывается на векторно-матричных умножениях. В регулярных языках программирования определяются переменные со зна- чениями или символами, к которым могут применяться операции. В языках сим-вольного программирования, таких как Theano или TensorFlow, операции струк -\nГлава 5 Глубокое обучение с библиотекой T ensorFlow В этой главе, где в центре нашего внимания будет TensorFlow, мы затронем сле- дующие темы: элементарные операции T ensorFlow; машинное об учение с нуля с TensorFlow – регрессия, классификатор на ос - нове SGD и нейронная сеть; глубок ое обучение с SkFlow; инкрементное глубок ое обучение с большими файлами; сверт очные нейронные сети с Keras. Платформа TensorFlow была выпущена во время написания данной книги и уже зарекомендовала себя как великолепное дополнение к ландшафту машинного обуче ния. Данная плат форма была запущена командой Google Brain, состоящей из большин- ства исследователей, которые в последнее десятилетие работали над важными раз-работками в области глубокого обучения (Джеффри Хинтон, Сэми Бенджио и дру - гие). Это, в сущности, развитие платформы более раннего поколения под названием DistBelief для распределенных глубоких нейронных сетей. В отличие от TensorFlow, DistBelief не являет ся платформой с открытым исходным кодом. Интересными при- мерами успешных проектов DistBelief является система обратного поиска изображе-ний, Google’овская программа компьютерного зрения DeepDream и распознавание речи в приложениях Google. DistBelief позволил разработчикам Google использовать тысячи ядер (CPU и GPU) для распределенной тренировки моделей. Платформа TensorFlow является шагом вперед, по сравнению с DistBelief, в том, что она теперь имеет абсолютно открытый исходный код, а ее язык программи-рования менее абстрактен. TensorFlow по праву считается более гибкой платфор-мой, которая к тому же имеет более широкий диапазон приложений. Во время написания книги (в конце 2015 г.) платформа TensorFlow находилась на этапе ста-новления, и, как мы увидим далее в этой главе, уже появились интересные легко-весные библиотеки, созданные поверх этой платформы. Аналогично Theano, платформа TensorFlow работает с символьными вычисле- ниями на тензорах, т. е. большинство вычислений в платформе основывается на векторно-матричных умножениях. В регулярных языках программирования определяются переменные со зна- чениями или символами, к которым могут применяться операции. В языках сим-вольного программирования, таких как Theano или TensorFlow, операции струк -\n--- Страница 172 ---\nГлубокое обучение с библиотекой TensorFlow  171 турированы вокруг графов, а не переменных. Это имеет свои вычислительные преимущества, поскольку они могут быть распределены и параллелизированы по всем вычислительным узлам (GPU и CPU): Фронтэнд для PythonФронтэнд для C++… Механизм исполнит е льного ядра T ensorFlow CPU GPU Android iOS … Архитектура TensorFlow, как было представлено в ноябре 2015 г. Платформа Tensorflow имеет следующие особенности и приложения: платформа T ensorFlow может быть (горизонтально) параллелизирована на многочисленных модулях GPU; наличие платформы разработки для мобильного развертывания; наличие инструментальной панели TensorBoard для визуализации (на ран- ней стадии); является фронт ендом, т. е. внешним представлением, для нескольких язы- ков программирования (Python, Go, Java, Lua, JavaScript, R, C++ и в недале-ком будущем Julia); обеспечивает интеграцию крупномасштабных решений, таких как плат - форма Spark и Google-облако (https://cloud.google.com/ml/). Идея, что тензорные операции с графоподобной структурой (согласно заявлени- ям компании Google) предлагают новые подходы к параллелизированным вычис - лениям, может стать вполне понятной, если взглянуть на следующее изобра жение: CPU среднее обновление переменныеGPU1 GPU2 градиенты градиенты потери потери модель модель Архитектура распределенной обработки с TensorFlow\n--- Страница 173 ---\n172  Глубокое обучение с библиотекой TensorFlow Из этого рисунка становится понятно, что каждая модель может быть назначе- на отдельным модулям GPU, после чего вычисляется среднее значение предсказа- ний, получаемых из каждой модели. Среди других методов этот подход остается центральным для тренировки очень больших распределенных нейронных сетей на GPU-кластерах. инсталляция tenSor Flow В этой главе мы будем использовать версию TensorFlow 0.8, поэтому убедитесь, что установлена именно данная версия. Поскольку платформа TensorFlow нахо-дится в стадии активной разработки, то небольшие изменения неизбежны. Плат - форму TensorFlow можно довольно легко установить при помощи команды pip install , независимо от используемой операционной системы: pip install tensorflow Если уже установлены предыдущие версии, то можно обновиться в соответ - ствии с вашей операционной системой: # Ubuntu/Linux 64-разрядная, только CPU: $ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.1-cp27-none-linux_x86_64.whl # Ubuntu/Linux 64-разрядная, GPU установлен: $ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.1-cp27-none-linux_x86_64.whl # Mac OS X, только CPU: $ sudo easy_install --upgrade six$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.1-cp27-none-any.whl Установив TensorFlow, теперь можно протестировать его в терминале: $ python>>> import tensorflow as tf>>> hello = tf.constant('Hello, TensorFlow!')>>> sess = tf.Session()>>> print(sess.run(hello)) Out: >>> Hello, TensorFlow! Операции T ensorFlow Давайте рассмотрим несколько простых примеров, чтобы получить представле- ние о том, как эта платформа работает. Важное отличие платформы TensorFlow состоит в том, что сначала нам нужно инициализировать переменные и только потом можно применять к ним опера-ции. Для выполнения вычислений TensorFlow работает с бэкендом, т. е. внутрен-ней реализацией, на C++, поэтому, чтобы подключиться к этому бэкенду, сначала необходимо инстанцировать сеанс: In: x = tf.constant([22,21,32], name='x')\n--- Страница 174 ---\nИнсталляция TensorFlow  173 d = tf.constant([12,23,43], name='x') y = tf.Variable(x * d, name='y')print(y) Вместо того чтобы получить на экране выходной вектор x*d, вы увидите что-то вроде этого: Out:<tf.Variable 'y:0' shape=(3,) dtype=int32_ref> Чтобы получить фактические результаты вычисления из бэкенда на C++, необ- ходимо инстанцировать сеанс, как показано ниже: In:x = tf.constant([22,21,32], name='x')d = tf.constant([12,23,43], name='d')y = tf.Variable(x * d, name='y') model = tf.global_variables_initializer()with tf.Session() as session: session.run(model) print(session.run(y)) Out: [ 264 483 1376] До сих пор мы использовали переменные непосредственно, однако, чтобы во время работы с тензорными операциями получить бóльшую гибкость, удобнее присваивать данные предварительно указанному контейнеру. Таким образом мы получаем возможность выполнять операции на вычислительном графе, не за-гружая данных в память заранее. Затем в терминах платформы TensorFlow дан-ные передаются в граф через так называемые заполнители (placeholder). Именно тут проявляется сходство с языком платформы Theano (см. приложение, введение в GPU и Theano). Заполнители в TensorFlow – это просто объектные контейнеры с определенны- ми предварительными настройками и классами. Так, для выполнения операций на объекте сначала для этого объекта создается заполнитель вместе с соответ - ствующим ему классом (в данном случае целым числом): In: a = tf.placeholder(tf.int32)b = tf.placeholder(tf.int32)sess = tf.Session()sess.run(a+b, feed_dict={a: 111, b: 222}) Out: 333 Умножение матриц будет работать следующим образом: In:matrix1 = tf.constant([[1, 2,32], [3, 4,2], [3,2,11]])matrix2 = tf.constant([[21,3,12], [3, 56,2], [35,21,61]])product = tf.matmul(matrix1, matrix2)\n--- Страница 175 ---\n174  Глубокое обучение с библиотекой TensorFlow with tf.Session() as sess: result = sess.run(product)print(result) Out: [[1147 787 1968] [ 145 275 166] [ 454 352 711]] Интересно отметить, что на выходе объект result представляет собой объект ndarray библиотеки NumPy, к которому можно применять операции, находясь за пределами TensorFlow. Вычисления на GPU Если нужно выполнять операции TensorFlow на модулях GPU, то требуется указать только устройство. Предупреждаем: следующее ниже работает только с соответ - ствующим образом установленным CUDA-совместимым модулем GPU компании NVIDIA: with tf.device('/gpu:0'): product = tf.matmul(matrix1, matrix2)with tf.Session() as sess: result = sess.run(product) print(result) Если вы хотите задействовать многочисленные модули GPU, то нужно присво- ить каждый модуль GPU определенной задаче: matrix3 = tf.constant([[13, 21,53], [4, 3,6],[3,1,61]])matrix4 = tf.constant([[13,23,32], [23, 16,2],[35,51,31]]) with tf.device('/gpu:0'): product = tf.matmul(matrix1, matrix2)with tf.Session() as sess: result = sess.run(product) print(result) with tf.device('/gpu:1'): product = tf.matmul(matrix3, matrix4)with tf.Session() as sess: result = sess.run(product) print(result) Линейная регрессия с SGD После того как мы рассмотрели основы, теперь можно приступить к написанию с нуля первого алгоритма машинного обучения на платформе TensorFlow. Позже мы воспользуемся более практическими легковесными приложениями в более высоких абстракциях поверх TensorFlow. Мы выполним очень простую линейную регрессию со стохастическим гради- ентным спуском, чтобы получить представление о том, как работают процессы тренировки и оценивания модели в TensorFlow. Прежде всего мы создадим не-сколько рабочих переменных для их размещения в заполнителях, которые будут их содержать. Затем передадим x и y в функцию cost и натренируем модель гра- диентным спуском:\n--- Страница 176 ---\nИнсталляция TensorFlow  175 In: import numpy as npimport tensorflow as tf X = tf.placeholder(\"float\") # создать символические переменные Y = tf.placeholder(\"float\")X_train = np.asarray([1,2.2,3.3,4.1,5.2])Y_train = np.asarray([2,3,3.3,4.1,3.9,1.6]) def model(X, w): return tf.multiply(X, w) w = tf.Variable(0.0, name=\"weights\") y_model = model(X, w) # предсказанные значения cost = (tf.pow(Y-y_model, 2)) # функция стоимости как квадратическая ошибка# оптимизация по методу sgd train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost) sess = tf.Session() init = tf.global_variables_initializer()sess.run(init) for trials in range(50): for (x, y) in zip(X_train, Y_train): sess.run(train_op, feed_dict={X: x, Y: y}) print(sess.run(w))Out: 0.844732 Резюмируя, линейная регрессия на основе алгоритма SGD выполняется сле- дующим образом: сначала инициализируются регрессионные веса (коэффици- енты), затем на втором шаге задается функция стоимости, чтобы позже обучить и оптимизировать эту функцию при помощи градиентного спуска. В конце нужно написать цикл for и указать число тренировочных раундов, которые требуются для вычисления итоговых прогнозов. Аналогичная базовая структура станет оче-видной во время работы с нейронными сетями. Нейронная сеть с нуля в TensorFlow Теперь давайте создадим и реализуем нейронную сеть на языке TensorFlow и про-анализируем этот процесс. Мы также воспользуемся набором данных Iris и в данном случае несколькими приложениями Scikit-learn для предобработки: In: import osimport loggingfrom datetime import datetime as dtimport numpy as npimport pandas as pdfrom sklearn import datasetsfrom sklearn import preprocessingfrom sklearn.preprocessing import OneHotEncoder\n--- Страница 177 ---\n176  Глубокое обучение с библиотекой TensorFlow from sklearn import model_selection from sklearn.model_selection import train_test_splitfrom sklearn.utils import shuffleimport tensorflow as tf iris = datasets.load_iris() X = np.asarray(iris.data, 'float32')X = preprocessing.scale(X)min_max_scaler = preprocessing.MinMaxScaler()X = min_max_scaler.fit_transform(X) lb = preprocessing.LabelBinarizer() Y = lb.fit_transform(iris.target) Это важный шаг. Нейронные сети в TensorFlow не могут работать с целевыми метками внутри одиночного вектора. Чтобы нейронная сеть могла работать с вы- ходом по схеме «один против всех», целевые метки должны быть преобразованы в бинаризованные признаки (так называемые фиктивные переменные): X_train, x_test, y_train, y_test = \\ train_test_split(X, Y, test_size=0.3, random_state=22) def init_weights(shape): return tf.Variable(tf.random_normal(shape, stddev=0.01)) Ниже мы видим прямое прохождение по данным: # прохождение с прямым распространением сигнала def model(X, w_h, w_o): h = tf.nn.sigmoid(tf.matmul(X, w_h)) return tf.matmul(h, w_o) X = tf.placeholder(\"float32\", [None, 4]) Y = tf.placeholder(\"float32\", [None, 3]) Далее мы развертываем слоеную архитектуру с одним скрытым слоем: w_h = init_weights([4, 4])w_o = init_weights([4, 3])py_x = model(X, w_h, w_o) cost = tf.reduce_mean( # вычислить стоимости tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y)) train_op = tf.train.GradientDescentOptimizer( # сконструировать оптимизатор learning_rate=0.01).minimize(cost) predict_op = tf.argmax(py_x, 1)sess = tf.Session() init = tf.global_variables_initializer()sess.run(init) for i in range(500): for start, end in zip(range(0, len(X_train),1 ), range(1, len(X_train),1)): sess.run(train_op, feed_dict={X: X_train[start:end], Y: y_train[start:end]}) if i % 100 == 0: print(i, np.mean(np.argmax(y_test, axis=1) ==\n--- Страница 178 ---\nМашинное обучение в TensorFlow посредством SkFlow  177 sess.run(predict_op, feed_dict={X: x_test, Y: y_test}))) Out: 0 0.288888888889100 0.711111111111200 0.933333333333300 0.977777777778400 0.977777777778 Точность этой нейронной сети составляет порядка .977%, но может от прогона к прогону давать немного различающиеся результаты. Данная отметка является более или менее целевым ориентиром для нейронной сети с единственным скры-тым слоем и классическим алгоритмом SGD. Как мы увидели в предыдущих примерах, реализация метода оптимизации и формирование тензоров вполне интуитивно понятны. И понятны больше, чем выполнение того же самого в NumPy (см. главу 4 « Нейронные сети и глубо- кое обуче ние»). Единственный недостаток на данный момент состоит в том, что оценивание и предсказание требуют иногда утомительного цикла for, тогда как такие библиотеки, как Scikit-learn, могут предоставлять эти методы в одной стро-ке сценария. К счастью, имеются более высокоуровневые библиотеки, которые разработаны поверх TensorFlow, и они намного упрощают процессы тренировки и оценивания. Одной из подобного рода библиотек является SkFlow; как видно из названия библиотеки, она представляет собой обертку, основанную на стиле сценариев, который работает аналогично библиотеке Scikit-learn. машиннОе Обучение в tenSor Flow пОсредствОм SkFlow Теперь, когда мы познакомились с основными операциями платформы Tensor - Flow , давайте рассмотрим более высокоуровневые приложения, созданные поверх TensorFlow, которые сделают машинное обучение немного практичнее. И SkFlow будет первым приложением, которое мы рассмотрим. В SkFlow от нас не требуется определять типы и заполнители. Можно загружать данные и управлять ими таким же образом, как это делается в Scikit-learn и NumPy. Давайте установим библиоте-ку при помощи менеджера библиотек pip: Простейший способ установить данную библиотеку – взять ее прямиком с ре- позитория Github: $ pip install git+git://github.com/tensorflow/skflow.git  Недавно библиотека SkFlow стала составной частью платформы TensorFlow и была переме-щена в папку contrib в репозитории Github http://github.com/tensorflow/tensorflow , поэтому ее установка менеджером библиотек pip не требуется. Чтобы воспользоваться функциона- лом библиотеки SkFlow, необходимо ее импортировать из TensorFlow следующим образом: from tensorflow.contrib import learn as skflow Этот вариант относится к версиям TensorFlow, начиная с 1.0. Описываемый в книге про-граммный код относится к версии TensorFlow 0.8.1 и более ранним. В SkFlow имеются три главных класса обучающихся алгоритмов: линейные классификаторы, линейная регрессия и нейронные сети. Линейный классифи-\n--- Страница 179 ---\n178  Глубокое обучение с библиотекой TensorFlow катор – это в основном простой (мульти-) классификатор на основе алгоритма SGD, а вот реализация нейронных сетей в SkFlow превосходна. Данная библиотека предлагает относительно простые в использовании обертки для очень глубоких нейронных, рекуррентных нейронных и сверточных нейронных сетей. К сожале-нию, другие алгоритмы, в частности случайный лес, градиентный бустинг, SVM и наив ный Байес, еще не реализованы. Впрочем, на GitHub были дискуссии по поводу реализации в SkFlow алгоритма случайного леса, который является пер-спективной разработкой и, вероятно, получит название tf_forest. Давайте применим первый в SkFlow алгоритм многоклассовой классификации. Для этого примера мы будем использовать набор данных Wine, который в своем первоначальном виде взят из репозитория машинного обучения UCI. Это лег - кий набор данных, который состоит из 13 признаков непрерывных химических мет рик, таких как магний, алкоголь, яблочная кислота и т. д., и содержит всего 178 прецедентов и один целевой признак с 3 классами. Целевая переменная со-стоит из трех разных культурных сортов растения. Вина классифицируются со-гласно соответствующему им культурному сорту растения (типу используемого для вина винограда), применяя для этого химический анализ тринадцати хими-ческих метрик. Вы видите, что мы загружаем данные из URL точно так же, как это делается во время работы в среде Scikit-learn: In: import urllib.request as urllib2import numpy as npfrom sklearn.metrics import accuracy_scorefrom sklearn.datasets import load_svmlight_filefrom sklearn.model_selection import train_test_splitimport skflow url = \\ 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/wine.scale'set1 = urllib2.Request(url)wine = urllib2.urlopen(set1) X_train, y_train = load_svmlight_file(wine) X_train = X_train.toarray() X_train, X_test, y_train, y_test = \\ train_test_split(X_train, y_train, test_size=0.30, random_state=4) kwargs = { \"n_classes\":4, \"learning_rate\":0.01, \"optimizer\": \"SGD\", \"continue_training\": True, \"steps\": 1000}classifier = skflow.TensorFlowLinearClassifier(**kwargs)classifier.fit(X_train, y_train)score = accuracy_score(y_train, classifier.predict(X_train)) d = classifier.predict(X_test) c = accuracy_score(d,y_test) print(\"Точность: %f\" % score)\n--- Страница 180 ---\nМашинное обучение в TensorFlow посредством SkFlow  179 print('Точность на контрольном/тестовом наборе: %f' % c) Out: Step #1, avg. loss: 1.58672Step #101, epoch #25, avg. loss: 1.45840Step #201, epoch #50, avg. loss: 1.09080Step #301, epoch #75, avg. loss: 0.84564Step #401, epoch #100, avg. loss: 0.68503Step #501, epoch #125, avg. loss: 0.57680Step #601, epoch #150, avg. loss: 0.50120Step #701, epoch #175, avg. loss: 0.44486Step #801, epoch #200, avg. loss: 0.40151Step #901, epoch #225, avg. loss: 0.36760Точность на тренировочном наборе: 0.967742Точность на тестовом наборе: 0.981481 К настоящему моменту этот метод уже довольно знаком; он, в сущности, пред- ставляет собой тот же способ, каким работал бы классификатор в Scikit-learn. Од- нако тут стоит обратить внимание на две важные вещи. С SkFlow можно исполь-зовать объекты NumPy и TensorFlow взаимозаменяемо, в результате чего нам не нужно объединять и преобразовывать объекты в тензорные таблицы и обратно. Благодаря этому работа с TensorFlow посредством такого высокоуровневого при-ложения, как SkFlow, становится намного гибче. Второй нюанс состоит в том, что к главному объекту данных был применен метод toarray . Это вызвано тем, что набор данных довольно разреженный (много нулевых записей), а TensorFlow не в состоянии хорошо обрабатывать разреженные данные. TensorFlow показывает себя превосходно во время работы с нейронными се- тями, и в SkFlow довольно просто натренировать нейронную сеть с многочис - ленными слоями. Давайте реализуем нейронную сеть на наборе данных Diabetes о больных диабетом. Этот набор данных содержит метрики диабета и (бинарные целевые) диагностические признаки беременных женщин в возрасте от 21 года и старше из племени индейцев Пима. У индейцев племени Пима, проживающих в шт. Аризона, США, имеется самая высокая зарегистрированная степень распро-страненности диабета среди населения во всем мире, и поэтому эта этническая группа стала добровольным предметом исследования диабета. Указанный набор данных состоит из следующих признаков: число беременнос тей; плазменная к онцентрация глюкозы в два часа в пероральной пробе на то- лерантность к глюкозе; диаст олическое кровяное давление (мм рт. ст.); то лщина кожных складок трицепса (мм); 2-часовой инс улин в сыворотке (мкМЕ/мл); индекс мас сы тела (вес в кг/(высота в м)^2); функция диабетической наследственности; возраст (лет); переменная клас са (0 либо 1). В приведенном ниже примере мы сначала загружаем и шкалируем данные: In: import urllib.request as urllib import numpy as np\n--- Страница 181 ---\n180  Глубокое обучение с библиотекой TensorFlow from sklearn import preprocessing from sklearn.preprocessing import Normalizerfrom sklearn import datasets, metrics, model_selectionfrom sklearn.model_selection import train_test_split # набор данных Diabetes (Репозиторий машинного обучения UCI) url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"# скачать файлraw_data = urllib.urlopen(url)dataset = np.loadtxt(raw_data, delimiter=\",\") print(dataset.shape)X = dataset[:,0:7] y = dataset[:,8]X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.2, random_state=0) X = preprocessing.scale(X) min_max_scaler = preprocessing.MinMaxScaler()X = min_max_scaler.fit_transform(X) Этот шаг особенно интересен; чтобы нейронные сети лучше сходились, можно использовать более гибкие скорости затухания. Во время тренировки многослой-ных нейронных сетей обычно полезно со временем уменьшать темп обучения. Вообще говоря, когда темп обучения слишком высокий, то можно промахнуться по оптимуму. С другой стороны, когда он слишком низкий, мы впустую тратим вычислительные ресурсы и застреваем в локальном минимуме. Экспоненциаль-ное затухание – это метод, который со временем ослабляет темп обучения, в ре-зультате чего он становится более чувствительным, когда начинает приближаться к минимуму. Существуют три распространенных способа реализовать затухание темпа обучения, а именно пошаговое затухание, затухание 1/t и экспоненциаль-ное затухание: Экспоненциальное затухание: α = α 0e–kt. В данном случае α – это темп обучения, k – гиперпараметр и t – итерация.В этом примере мы будем использовать экспоненциальное затухание, пото- му что оно очень хорошо зарекомендовало себя на этом наборе данных. Вот как мы реализуем функцию экспоненциального затухания (при помощи встроенной в TensorFlow функции tf.train.exponential_decay ): In:import tensorflowimport tensorflow as tf def exp_decay(global_step): return tf.train.exponential_decay(learning_rate=0.01, global_step=global_step, decay_steps=steps, decay_rate=0.01) Теперь можно передать функцию затухания в модель нейронной сети Ten sor- Flow . В этой нейронной сети мы будем использовать двуслойную сеть, в которой первый слой будет состоять из пяти узлов и второй – из четырех узлов. По умол-\n--- Страница 182 ---\nМашинное обучение в TensorFlow посредством SkFlow  181 чанию в SkFlow реализована функция активации ReLU, и поскольку мы отдаем ей предпочтение по сравнению с другими (tanh, сигмоидальная и т. д.), то будем придерживаться этой функции. Согласно этому примеру, кроме алгоритма стохастического градиентного спус - ка (SGD), можно реализовать и другие алгоритмы оптимизации. Давайте реализу - ем адаптивный алгоритм под названием Adam, основываясь на статье Дидерика Кингма (Diederik Kingma) и Джимми Ба (Jimmy Ba) (http://arxiv.org/abs/1412.6980). Алгоритм Adam разработан в Амстердамском университете и расшифровыва- ется как adaptive moment estimation (метод адаптивной оценки инерции). В пре-дыдущей главе мы узнали, как работает алгоритм ADAGRAD, – путем сокращения градиентов во времени, по мере того как они движутся (в надежде) к глобальному минимуму. В алгоритме Adam тоже используются адаптивные методы, но в соче-тании с идеей тренировки с инерцией, где учитываются предыдущие обновления градиента: In:from tensorflow.contrib import learn as skflow steps = 5000 classifier = skflow.DNNClassifier(hidden_units=[5,4], n_classes=2, batch_size=300, steps=steps, optimizer='Adam', # SGD/RMSProp # назначаем функцию затухания learning_rate=exp_decay) classifier.fit(X_train, y_train) score1a = metrics.accuracy_score(y_train, classifier.predict(X_train))score1b = metrics.accuracy_score(y_test, classifier.predict(X_test)) print(\"Точность: %f\" % score1a) print(\"Точность на контрольном наборе: %f\" % score1b) Out: (768, 9)Step #1, avg. loss: 12.83679Step #501, epoch #167, avg. loss: 0.69306Step #1001, epoch #333, avg. loss: 0.56356Step #1501, epoch #500, avg. loss: 0.54453Step #2001, epoch #667, avg. loss: 0.54554Step #2501, epoch #833, avg. loss: 0.53300Step #3001, epoch #1000, avg. loss: 0.53266Step #3501, epoch #1167, avg. loss: 0.52815Step #4001, epoch #1333, avg. loss: 0.52639Step #4501, epoch #1500, avg. loss: 0.52721Точность на тренировочном наборе: 0.754072Точность на тестовом наборе: 0.740260 Полученный показатель точности не особо убедителен; можно было бы его улучшить, применив ко входу алгоритм анализа главных компонент (PCA). В этой статье за 1999 г. Ставроса Дж. Перантониса (Stavros J Perantonis) и Васси-лиса Вирвилиса (Vassilis Virvilis) (http://rexa.info/paper/dc4f2babc5ca4534b435280ae\n--- Страница 183 ---\n182  Глубокое обучение с библиотекой TensorFlow c32f5816ddb53b0) было показано, что набор данных Diabetes извлекает пользу из снижения размерности методом PCA перед его передачей в нейронную сеть. Для этого набора данных мы воспользуемся конвейерным объектом Pipeline библио- теки Scikit-learn: In: from sklearn.decomposition import PCAfrom sklearn import linear_model, decomposition, datasetsfrom sklearn.pipeline import Pipelinefrom sklearn.metrics import accuracy_score pca = PCA(n_components=4,whiten=True)lr = pca.fit(X) classifier = skflow.TensorFlowDNNClassifier(hidden_units=[5,4], n_classes=2, batch_size=300, steps=steps, optimizer='Adam', # SGD/RMSProp learning_rate=exp_decay) pipe = Pipeline(steps=[('pca', pca), ('NNET', classifier)])X_train, X_test, Y_train, Y_test = \\ train_test_split(X, y, test_size=0.2, random_state=0)pipe.fit(X_train, Y_train) score2 = metrics.accuracy_score(Y_test, pipe.predict(X_test))print(\"Точность на контрольном наборе с pca: %f\" % score2) Out: Step #1, avg. loss: 1.07512Step #501, epoch #167, avg. loss: 0.54236Step #1001, epoch #333, avg. loss: 0.50186Step #1501, epoch #500, avg. loss: 0.49243Step #2001, epoch #667, avg. loss: 0.48541Step #2501, epoch #833, avg. loss: 0.46982Step #3001, epoch #1000, avg. loss: 0.47928Step #3501, epoch #1167, avg. loss: 0.47598Step #4001, epoch #1333, avg. loss: 0.47464Step #4501, epoch #1500, avg. loss: 0.47712Точность на тестовом наборе с PCA: 0.805195 У нас получилось чуть-чуть улучшить результативность нейронной сети за счет простого шага с предобработкой на основе алгоритма PCA. Мы сократили число признаков с семи до четырех, т. е. четырех размерностей. Алгоритм PCA обычно сглаживает сигнал путем центрирования признаков на нуле, тем самым умень-шая признаковое пространство путем использования векторов, которые содержат только самое высокое собственное значение. Выбеливание (whitening) гарантиру - ет, что признаки преобразуются в нуль-коррелированные. Оно приводит к более гладкому сигналу и меньшему набору признаков, что позволяет нейронной сети сходиться быстрее. См. главу 7 «Обучение без учителя в крупном масштабе» для получения более подробного объяснения метода PCA.\n--- Страница 184 ---\nМашинное обучение в TensorFlow посредством SkFlow  183 Глубокое обучение с большими файлами – инкрементное обучение До сих пор мы имели дело с несколькими операциями TensorFlow и методами ма- шинного обучения в SkFlow на относительно малых наборах данных. Однако на-стоящая книга посвящена крупномасштабному и масштабируемому машинному обуче нию. Чт о же платформа TensorFlow предлагает в этом отношении? До недавнего времени параллельные вычисления находились на этапе станов- ления и были недостаточно стабильными, чтобы освещать их в этой книге. Вы-числения на многочисленных модулях GPU недоступны для читателей без CUDA-совместимых карт NVIDIA. Крупномасштабные облачные сервисы (https://cloud.google.com/products/machine-learning/) или Amazon EC2 сопровождаются значи-тельными финансовыми затратами. Таким образом, остается всего один способ, которым можно масштабировать наш проект, – путем пошагового обучения. Вообще говоря, любой размер файла, который превышает порядка 25% имею - щейся оперативной памяти компьютера, вызовет проблемы, связанные с ее перегрузкой. Поэтому, если вы располагаете компьютером с 2 Гб RAM и хотите применить технологию машинного обучения к файлу объемом 500 МБ, то пора задуматься о способах, которые позволяют избежать излишнего потребления па-мяти. Для того чтобы предотвратить перегрузку оперативной памяти, мы советуем использовать метод внеядерного обучения, который разбивает данные на мень-шие по объему порции с целью инкрементной тренировки и обновления моде-лей. Методы частичной подгонки в библиотеке Scikit-learn, которые мы затронули в главе 2 «Масштабируемое обучение в Scikit-learn», тому являются примерами. Библиотека SkFlow тоже предлагает великолепный метод пошагового обуче- ния, аналогичный методу частичной подгонки в Scikit-learn, для всех своих моде-лей машинного обучения. В этом разделе мы будем использовать классификатор глубокого обучения в инкрементном режиме, потому что считаем его самым пер-спективным. В данном разделе для нашего масштабируемого проекта глубокого внеядерно- го обучения мы будем использовать две стратегии, а именно пошаговое обучение и случайную подвыборку. Сначала сгенерируем немного данных, а затем создадим функцию отбора наблю дений, которая будет извлекать случайные подвыборки из синтетического набора данных и инкрементно тренировать модель глубокого обучения на этих подмножествах: In: import gcimport randomimport numpy as npimport pandas as pdfrom sklearn.datasets import make_classificationfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scoreimport tensorflow as tfimport skflow\n--- Страница 185 ---\n184  Глубокое обучение с библиотекой TensorFlow Прежде всего сгенерируем немного демонстрационных данных и запишем их на диск: X, y = make_classification(n_samples=5000000, n_features=10, n_classes=4, n_informative=6, random_state=222, n_clusters_per_class=1)X_train, X_test, y_train, y_test = \\ train_test_split(X,y, test_size=0.2, random_state=22) Big_trainm = pd.DataFrame(X_train, y_train) Big_testm = pd.DataFrame(X_test, y_test) Big_trainm.to_csv('lsml-Bigtrainm', sep=',') Big_testm.to_csv('lsml-Bigtestm', sep=',') Теперь освободим память, удалив из нее все созданные нами объекты. Методом gc.collect мы заставляем сборщик «мусора» языка Python очистить память: del(X, y, X_train, y_train, X_test)gc.collect Ниже мы создаем функцию, которая извлекает случайные подвыборки из дис - ка. Отметим, что используется доля выборки 1/2. Можно было бы использовать доли поменьше, но в этом случае нам также пришлось бы скорректировать две важные вещи. Во-первых, необходимо привести в соответствие размер пакета модели глубокого обучения, так чтобы он никогда не превышал объема выборки. Во-вторых, необходимо скорректировать количество эпох в цикле for таким обра- зом, чтобы гарантировать использование для тренировки модели самой большой части тренировочных данных: In: import randomimport pandas as pd def sample_file(): global skip_idx global train_data global X_train global y_train big_train='lsml-Bigtrainm' Подсчитаем число строк во всем наборе: num_lines = sum(1 for i in open(big_train)) Мы используем одну третью долю тренировочного набора: size = int(num_lines / 3) Берем индексы пропуска и сохраняем: skip_idx = random.sample(range(1, num_lines), num_lines - size) train_data = pd.read_csv(big_train, skiprows=skip_idx) X_train = train_data.drop(train_data.columns[[0]], axis=1) y_train = train_data.ix[:,0] В предыдущем разделе мы уже видели, как работает затухание весов. Ниже мы воспользуемся им снова:\n--- Страница 186 ---\nМашинное обучение в TensorFlow посредством SkFlow  185 def exp_decay(global_step): return tf.train.exponential_decay(learning_rate=0.01, global_step=global_step, decay_steps=steps, decay_rate=0.01) Далее мы развернем глубокий нейросетевой (DNN) классификатор с тремя скрытыми слоями, соответственно с 5, 4 и 4 узлами. Отметим, что мы задаем раз- мер пакета равным 300, т. е. в каждой эпохе используются 300 тренировочных слу - чаев. Этот размер также помогает предотвратить перегрузку памяти: steps = 5000clf = skflow.TensorFlowDNNClassifier(hidden_units=[5,4,4], n_classes=4, batch_size=300, steps=steps, optimizer='Adam', learning_rate=exp_decay) Ниже мы задаем количество подвыборок, равное трем ( epochs=3 ), т. е. мы инкре- ментно тренируем модель глубокого обучения на трех последовательных подвы- борках: epochs = 3 for i in range(epochs): sample_file() clf.partial_fit(X_train,y_train) test_data = pd.read_csv('lsml-Bigtestm', sep=',') X_test = test_data.drop(test_data.columns[[0]], axis=1)y_test = test_data.ix[:,0] score = accuracy_score(y_test, clf.predict(X_test)) print(score)Out: Step #501, avg. loss: 0.55220Step #1001, avg. loss: 0.31165Step #1501, avg. loss: 0.27033Step #2001, avg. loss: 0.25250Step #2501, avg. loss: 0.24156Step #3001, avg. loss: 0.23438Step #3501, avg. loss: 0.23113Step #4001, avg. loss: 0.23335Step #4501, epoch #1, avg. loss: 0.23303Step #1, avg. loss: 2.57968Step #501, avg. loss: 0.57755Step #1001, avg. loss: 0.33215Step #1501, avg. loss: 0.27509Step #2001, avg. loss: 0.26172Step #2501, avg. loss: 0.24883Step #3001, avg. loss: 0.24343Step #3501, avg. loss: 0.24265Step #4001, avg. loss: 0.23686Step #4501, epoch #1, avg. loss: 0.236810.929022\n--- Страница 187 ---\n186  Глубокое обучение с библиотекой TensorFlow Нам удалось получить точность на тестовом наборе в размере .929 в течение очень управляемого времени тренировки модели и без перегрузки оперативной памяти, что значительно быстрее, чем если бы мы тренировали эту же модель на всем наборе данных сразу. инсталляция библиО теки keraS и платфОрма tenSor Flow Ранее мы видели практические примеры оберточного функционала библиотеки SkFlow для приложений платформы TensorFlow. Для реализации более продви-нутого подхода к нейронным сетям и глубокому обучению, когда имеется боль-ший контроль над параметрами, мы предлагаем библиотеку Keras (http://keras.io/). Данная библиотека была первоначально разработана в рамках платформы Theano, но совсем недавно была также адаптирована под TensorFlow. Таким образом, мы можем использовать библиотеку Keras в качестве более высоко-го абстрактного уровня поверх платформы TensorFlow. Впрочем, следует иметь в виду, что библиотека Keras в своих методах является немного менее прямо-линейной, чем SkFlow. Библиотека Keras может работать как на GPU, так и на CPU, что делает эту библиотеку действительно гибкой при ее переносе в другие среды. Давайте сначала установим библиотеку Keras и удостоверимся, что она исполь- зует бэкенд платформы TensorFlow. Ее инсталляция выполняется в командной строке при помощи pip: $ pip install Keras Библиотека Keras исходно создавалась поверх платформы Theano, поэтому мы должны сообщить библиотеке Keras вместо этой платформы задействовать плат - форму TensorFlow. Для этого нам сначала нужно запустить Keras один раз на соб-ственной стандартной платформе, т. е. платформе Theano. Сначала мы выполним небольшой программный код в Keras, чтобы убедиться, что все элементы библиотеки должным образом установлены. Давайте натрени-руем элементарную нейронную сеть и по ходу познакомимся с некоторыми клю-чевыми понятиями. Ради удобства мы используем данные, сгенерированные в Scikit-learn, которые состоят из четырех признаков и целевой переменной из трех классов. Эти раз-мерности очень важны, потому что они нам нужны для определения архитектуры нейронной сети: In: import timeimport numpy as npfrom sklearn.datasets import make_classificationfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import OneHotEncoderimport kerasfrom keras.utils import np_utils, generic_utilsfrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Activationfrom keras.optimizers import SGD\n--- Страница 188 ---\nИнсталляция библиотеки Keras и платформа TensorFlow  187 nb_classes = 3 X, y = make_classification(n_samples=1000, n_features=4, n_classes=nb_classes, n_informative=3, n_redundant=0, random_state=101) Теперь, когда мы определили переменные, необходимо преобразовать целевую переменную в прямокодированный массив (точно так же, как это делалось в плат - форме TensorFlow). В противном случае библиотека Keras не сможет вычислить целевые выходы по схеме «один против всех». Для Keras, вместо прямого унитар-ного кодировщика модуля sklearn, мы будем использовать функцию np_utils . Это делается следующим образом: y = np_utils.to_categorical(y, nb_classes)print(y) Наш массив y будет выглядеть так: Out:[[ 0. 0. 1.] [ 1. 0. 0.] [ 0. 0. 1.] , [ 0. 0. 1.] [ 0. 0. 1.] [ 0. 1. 0.]] Теперь разделим данные на тестовые и тренировочные: x_train, x_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.30, random_state=101) С этого места мы начинаем придавать форму задуманной нами архитектуре нейронной сети. Нейронная сеть будет с двумя скрытыми слоями, функцией ак - тивации relu и тремя узлами в каждом скрытом слое. Первый слой имеет четыре входа, потому что в данном случае у нас четыре признака. После этого мы добав-ляем скрытые слои с тремя узлами, отсюда model.add(dense(3) . Как и ранее, мы воспользуемся функцией мягкого максимума softmax , чтобы передать сеть в выходной слой: model = Sequential()model.add(Dense(4, input_shape=(4,)))model.add(Activation('relu'))model.add(Dense(3))model.add(Activation('relu'))model.add(Dense(3))model.add(Activation('softmax')) Сначала мы определяем функцию SGD, где реализуем самые важные парамет - ры, к оторые к настоящему моменту нам знакомы, а именно: lr: темп обучения; decay: функция затухания для замедления темпа обучения. Не путайте этот параметр с затуханием веса, которое является параметром регуляризации; momentum: параметр инерции используется, чтобы предотвратить застре- вание в локальном минимуме;\n--- Страница 189 ---\n188  Глубокое обучение с библиотекой TensorFlow nesterov: булева переменная, которая определяет, будем ли мы использо- вать инерцию Нестерова; эта переменная применима, только если мы опре- делили целое число для параметра инерции (обратитесь к главе 4 « Нейрон- ные сети и глубокое обучение» за более подробным объяснением); optimiz er: здесь мы определяем предпочтительный алгоритм оптимиза- ции (из следующего списка: SGD, RMSProp, ADAGRAD, Adadelta и Adam). Обратимся к следующему фрагменту исходного кода: seed = 22 # использовать для воспроизводимости результатов np.random.seed(seed) model = Sequential() model.add(Dense(4, input_shape=(4,)))model.add(Activation('relu'))model.add(Dense(3))model.add(Activation('relu'))model.add(Dense(3))model.add(Activation('softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd)model.fit(x_train, y_train, verbose=1, batch_size=100, epochs=50, validation_data=(x_test, y_test))time.sleep(0.1) В данном случае мы использовали размер пакета batch_size , равный 100, т. е. ис - пользовался мини-пакетный градиентный спуск со 100 тренировочными приме- рами в каждой эпохе. В этой модели были задействованы 50 тренировочных эпох. В итоге мы получим следующий результат: Out: acc: 0.8129 - val_loss: 0.5391 - val_acc: 0.8000Train on 700 samples, validate on 300 samples В последней модели, в которой мы использовали алгоритм SGD с инерцией Нес терова, мы не смогли улучшить отметку точности независимо от того, сколько эпох мы использовали для ее тренировки. Для увеличения точности желательно испытать другие алгоритмы оптими- зации. В более раннем примере мы уже успешно применили метод оптимиза- ции Adam, поэтому давайте здесь применим его снова и посмотрим, сможем ли мы увеличить точность. Поскольку алгоритмы с адаптивным темпом обучения, в частности Adam, со временем снижают темп обучения, для достижения опти-мального решения требуется больше эпох. Поэтому в приведенном ниже примере мы зададим количество эпох, равное 200: adam = keras.optimizers.Adam(lr=0.01) model.compile(loss='categorical_crossentropy', optimizer=adam)model.fit(x_train, y_train, verbose=1, batch_size=100, epochs=200, validation_data=(x_test, y_test))time.sleep(0.1) Out: Epoch 200/200700/700 [==============================] - 0s - loss: 0.3755 - acc:0.8657 - val_loss: 0.4725 - val_acc: 0.8200\n--- Страница 190 ---\nИнсталляция библиотеки Keras и платформа TensorFlow  189 В этот раз, используя алгоритм оптимизации Adam, нам удалось достичь убеди- тельного улучшения с 0.8 до 0.82. К настоящему моменту мы охватили самые важные элементы нейронных се- тей в библиотеке Keras. Теперь приступим к настройке библиотеки Keras, чтобы она могла задействовать платформу TensorFlow. По умолчанию библиотека Keras будет использовать бэкенд Theano. Чтобы заставить Keras работать на TensorFlow, сначала нужно найти местоположение папки библиотеки Keras в папке с библио - теками: In: import osprint(keras.__file__) Ваш путь может выглядеть по-другому: Out: /Library/Python/2.7/site-packages/keras/__init__.pyc Определилив местоположение папки библиотеки Keras, теперь необходимо найти файл ~/.keras/keras.json .  В Windows папка .keras с файлом keras.json находится в пользовательском домашнем ка- талоге (C:\\Users\\[ПОЛЬЗОВАТЕЛЬ]\\.keras ). В этом файле есть фрагмент сценария, который выглядит так: {\"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"theano\"} Вам просто нужно поменять \"backend\": \"theano\" на \"backend\": \"tensorflow\" , и в ре - зультате получится следующая строка: {\"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\"} Если по каким-либо причинам файл JSON в папке Keras отсутствует, т. е. в папке /Library/Python/2.7/site-packages/keras/ , то можно просто скопировать и вставить показанную ниже строку в текстовый редактор: {\"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\"} Затем сохранить файл с расширением .json и разместить его в папке keras . Чтобы проверить, что среда TensorFlow должным образом используется из Keras, можно набрать следующее: In:from keras import backend as Kinput = K.placeholder(shape=(4, 4, 5))# работает так же:input = K.placeholder(shape=(None, 2, 5))# работает так же:input = K.placeholder(ndim=2) Out: Using Theano backend. Некоторые пользователи, возможно, не увидят результата вообще, что тоже до- пустимо. Ваш бэкенд TensorFlow должен быть готов к использованию.\n--- Страница 191 ---\n190  Глубокое обучение с библиотекой TensorFlow сверт Очные нейр Онные сети в tenSor Flow пОсредствОм keraS Между настоящей и предыдущей главами мы прошли достаточно длинный путь, охватив самые важные темы глубокого обучения. Теперь мы понимаем, как кон-струировать архитектуры, размещая каскадом многочисленные слои нейронной сети, и различать и применять методы обратного распространения ошибки. Мы также затронули понятие предварительной тренировки без учителя с каскад-ными помехоустойчивыми автокодировщиками. Следующий и действительно захватывающий шаг в глубоком обучении представлен быстро развивающейся областью сверточных нейронных сет ей (Conv olutional Neural Networks, CNN), т. е. методом построения многослойных локальносвязных сетей. CNN-сети, также именуемые сетями ConvNet, так быстро развиваются, что во время написания этой книги нам пришлось переписать и обновить эту главу буквально в течение одного месяца. В этой главе мы осветим большинство фундаментальных и важ - ных понятий, которые лежат в основе CNN-сетей, с тем чтобы можно было выпол-нить несколько элементарных примеров, не перегружая себя иногда их огромной сложностью. Однако мы не сможем в полной мере отдать должное обширным тео - ретическим и вычислительным предпосылкам CNN-сетей, вследствие чего этот абзац обеспечит лишь практическую отправную точку1. Лучший способ концептуально разобраться в CNN-сетях состоит в том, чтобы вернуться в прошлое, начать с небольшого обзора когнитивной нейробиологии и рассмотреть исследование Хьюбера (Huber) и Визеля (Wiesel) в области зритель-ной зоны коры головного мозга. Хьюбер и Визель регистрировали нейроактивации зрительной зоны у кошек при измерении нейронной активности путем введения микроэлектродов в зрительную зону коры головного мозга. (Бедные кошки!) Они делали это в то время, когда кошки наблюдали проецируемые на экран прими-тивные изображения фигур. Самое интересное, что в результате наблюдений они обнаружили, что определенные нейроны откликались только на фигуры опреде-ленной ориентации или формы. Этот факт позволил выдвинуть теоретическое предположение, что зрительная зона коры состоит из локальных и специфичных для ориентации нейронов. Другими словами, определенные нейроны отклика-ются только на изображения характерной ориентации и формы (треугольники, круги или квадраты). Учитывая, что кошки и другие млекопитающие могут вос - принимать как единое целое сложные и изменяющиеся фигуры, можно предпо- 1 Свое название сверточная сеть получила по названию операции – свертка (convolution), она часто используется для обработки изображений и может быть описана следующей формулой: (f * g)[m, n] = ∑k,l f[m – k, n – l] * g[k, l](f * g)[m, n] = ∑k,l f[m – k, n – l] * g[k, l]. Здесь f – исходная матрица изображения, g – ядро (матрица) свертки. Неформально эту операцию можно описать следующим образом: окном размера ядра g проходим с заданным шагом (обычно 1) все изображение f, на каждом шаге поэле- ментно умножаем содержимое окна на ядро g, результат суммируется и записывается в матрицу результата.При этом в зависимости от метода обработки краев исходной матрицы результат может быть меньше исходного изображения (valid), такого же размера (same) или большего размера (full). – Прим. перев.\n--- Страница 192 ---\nСверточные нейронные сети в TensorFlow посредством Keras  191 ложить, что восприятие является агрегатом всех этих локально и иерархически организованных нейронов. К тому времени уже в полной мере были разработаны первые многослойные персептроны, и поэтому прошло совсем немного времени, прежде чем эта идея локальности и характерной чувствительности в нейронах была смоделирована в персептронной архитектуре. С точки зрения вычислитель-ной нейробиологии эта идея развилась в карты локальных рецептивных областей в мозгу с добавлением селективно-связных слоев. Эта идея была воспринята уже существующей областью нейронных сетей и искусственного интеллекта. Первым, кто заявил, что применил это понятие локальной специфичности в вычислитель-ном плане к многослойному персептрону, был Фукусима (Fukushima) с его так называемым неокогнитроном (1982). Ян Лекун разработал идею неокогнитрона в своей версии под названием LeNet с добавлением алгоритма обратного распространения методом градиентного спус ка. Архитектура LeNet по-прежнему остается основой для многих более эво- люционно развитых архитектур CNN, которые были внедрены в последнее время. Элементарная CNN-сеть, как и LeNet, учится обнаруживать края у необработанных пикселов в первом слое, затем использует эти края для обнаружения простых форм во втором слое и позже в процессе использует эти формы для обнаружения высо-коуровневых признаков, таких как объекты в окружающей среде в более высоких слоях. Слой, который находится в нейронной последовательности глубже, являет - ся заключительным классификатором, который использует эти высокоуровневые признаки. Мы видим прохождение по CNN-сети с прямым распространением сиг - нала следующим образом: мы переходим от матричного входа к пикселам, обна-руживаем края, исходя из пикселей, затем фигуры из краев и обнаруживаем все более и более отличительные и более абстрактные и сложные признаки из фигур.  Каждая свертка или слой в сети восприимчивы к определенному признаку (такому как фи- гура, угол или цвет). Более глубокие слои будут сочетать эти признаки в более многосложный агрегат. Таким образом, сеть может обрабатывать цельные изображения, не обременяя себя полным вход-ным пространством изображения на этом шаге. До сих пор мы работали только с полносвязными нейронными сетями, каждый слой которых подсоединен к каждому соседнему слою. Такие сети доказали свою достаточную эффективность, но они имеют недостаток существенно увеличивать число параметров, которые мы должны натренировать. Следует отметить, что в случае когда мы тренируем небольшое по размеру изображение (28×28), можно обойтись и полносвязной сетью. Однако тренировка полносвязной сети на более крупных изображениях, которые занимают всю площадь изображения, была бы в вычислительном плане чрезвычайно затратной.  Резюмируя, можем констатировать, что CNN-сети обладают следующими преимуществами пере д полносвязными нейронными сетями: • они уменьшаю т пространство параметров и тем самым предотвращают переподгонку и вычислительную перегрузку; • CNN-сети инвариантны для ориентации объектов (представьте распознавание объектов в различных местах с целью их классификации); • CNN-сети спос обны обучаться и обобщать сложные многомерные признаки; • CNN-сети могут быть полезными в распознавании речи, классификации изображений и в последнее время в сложных рекомендательных механизмах.\n--- Страница 193 ---\n192  Глубокое обучение с библиотекой TensorFlow В CNN-сетях задействованы так называемые рецептивные поля для соедине- ния входа с картой признаков. Лучший способ разобраться в работе CNN-сетей состоит в том, чтобы глубже проанализировать их архитектуру и, конечно же, по-лучить практический опыт. Поэтому давайте рассмотрим типы слоев, которые со-ставляют CNN-сети. Архитектура CNN-сети состоит из трех типов слоев, а именно сверточного слоя, объединяющего слоя и полносвязного слоя, где каждый слой принимает входной 3D-объем (h, w, d) и преобразует его в 3D-выход посредством дифференцируемой функции. Сверточный слой Значение свертки можно понять, если представить прожектор определенного размера, скользящий по входным данным (пиксельные значения и размерности RGB-цвета), после чего вычисляется скалярное произведение между отфильт - рованными значениями (также именуемыми заплатками – patch) и истинным входом. Благодаря этому выполняются две важные вещи: во-первых, сжимаются данные на входе, и, во-вторых, что еще более важно, сеть обучается фильтрам, которые активируются, только когда на входе они видят некий определенный тип положения признака в пространстве. Посмотрим на следующее ниже изображение, чтобы понять, как это работает: Смещение b0 (1×1×1) b0[:,:,0]Смещение b1 (1×1×1)b1[:,:,0]Входной объем (+отступ 1) (7×7×3)x[:,:,0] x[:,:,1] x[:,:,2]Фильтр W0 (3×3×3) w0[:,:,0] w0[:,:,1] w0[:,:,2]Фильтр W1 (3×3×3) w1[:,:,0] w1[:,:,1] w1[:,:,2]Выходной объем (3×3×2) o[:,:,0] o[:,:,0] Введены два сверточных уровня, обрабатывающих изображение [7×7×3] входной объем: изображение ширины 7, высоты 7 и с тремя цветовыми каналами R, G, B\n--- Страница 194 ---\nСверточные нейронные сети в TensorFlow посредством Keras  193 По этому рисунку видно, что имеются два уровня фильтров (W0 и W1) и три раз- мерности входов (в форме массивов), и результатом всего этого является скаляр- ное произведение прожектора/окна на входной матрице. Размер этого прожек - тора называется шагом; это означает, что чем больше шаг, тем меньше конечный результат. Как видно по рисунку, когда мы применяем фильтр 3×3, весь объем фильтра об- рабатывается в центре матрицы, но когда мы смещаемся близко к краям либо за их пределы, мы начинаем его терять на краях входа. В этом случае применяется так называемое дополнение нулями. Все элементы, которые попадают за преде- лы входных размерностей, в этом случае обнуляются. Дополнение нулями в по-следнее время стало более или менее использоваться в настройках по умолчанию в большинстве приложений на основе CNN-сетей. Объединяющий слой Следующий тип слоя, который часто помещается в промежутке между фильтрующи-ми слоями, называется объединяющим (pooling) слоем, или так называемым субдиск - ретизирующим слоем, т. е. слоем с уменьшением размерности. Его работа заключает - ся в отборе данных с пониженной частотой вдоль пространственных размерностей (ширина, высота), что, в свою очередь, помогает решать проблемы с переподгонкой и сокращением вычислительной нагрузки. Существует несколько способов выполне-ния субдискретизации, но в последнее время наиболее эффективным оказался метод объединения по максимальному значению элемента max pooling. Метод выбора максимального элемента – это простой метод сжатия признаков путем взятия максимального значения заплатки соседнего признака. Следующий ниже рисунок разъяснит эту идею; каждая цветная ячейка в матрице представля-ет собой подвыборку с шагом 2: c Oдноуровневый срез Объединяющий слой c выбором максимального элемента c шагом 2 Объединяющие слои используются в основном по следующим причинам: уменьшение ко личества параметров и тем самым вычислительной на- грузки; регуляризация. Инт ересно отметить, что последние результаты исследований предлагают не учитывать объединяющего слоя совсем, что приводит к улучшению точности (правда, за счет большей нагрузки на CPU или GPU).\n--- Страница 195 ---\n194  Глубокое обучение с библиотекой TensorFlow Полносвязный слой По поводу этого типа слоя особо нечего комментировать. Конечный выход, где вычисляются классификации (главным образом функцией мягкого максимума softmax), является полносвязным слоем. Однако в промежуточных сверточных слоях (хотя и редко) тоже имеются полносвязные слои. Прежде чем мы самостоятельно применим CNN-сеть, давайте возьмем то, чему вы научились к этому времени, и обследуем архитектуру CNN, чтобы про-верить наше понимание. Когда мы смотрим на архитектуру ConvNet на следу - ющем ниже рисунке, мы уже имеем представление о том, что именно ConvNet делает со входом. В качестве примера представлена эффективная сверточная нейронная сеть под названием AlexNet, предназначенная для классификации 1,2 млн изобра жений на 1000 классов. Она использовалась для конкурса ImageNet в 2012 г. ImageNet – это самое важное соревнование в области классификации и локализации изображений в мире, которое проходит каждый год. В самом на-звании AlexNet имеются в виду ее создатели Алекс Крыжевски (Alex Krizhevsky) в сотрудничестве с Вайнодом Наиром (Vinod Nair) и Джеффри Хинтоном (Geoffrey Hinton). Входное изображение (RGB) Шаг размером 4Слой max -poolingСлой max -poolingСлой max-poolingПлотный ПлотныйПлотный Архитектура AlexNet Разглядывая архитектуру сети, сразу же видна входная размерность 224×224 с трехмерной глубиной. Шаг в размере 4 во входе, где объединяющие слои max pooling уложены каскадно, уменьшает размерность входа. В свою очередь, за ними следует сверточный слой. Два плотных слоя размером 4096 – это полно-связные слои, которые ведут к конечному выходу, о чем говорилось ранее. В предыдущем абзаце мы уже упомянули, что в платформе TensorFlow вычис - ления на графах позволяют выполнять параллелизацию по модулям GPU. К све-дению: авторы нейронной сети AlexNet сделали то же самое. Взгляните на сле - дующий ниж е рисунок. На нем показано, каким образом они параллелизировали архитектуру по всем модулям GPU:\n--- Страница 196 ---\nCNN-сети с подходом на основе инкрементной тренировки  195 Шаг размером 4Слой max -poolingСлой max -poolingСлой max -poolingПлотный ПлотныйПлотный Классификация в ImageNet на основе глубоких сверточных нейронных сетей, NIPS 2012 Приведенный выше рисунок взят из работы ImageNet Classification with Deep Convolutional Neural Networks (ImageNet – классификация на основе глубокой сверточной нейронной сети) http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf. Нейронная сеть AlexNet позволяет различным моделям использовать модули GPU путем вертикального разбиения архитектуры, чтобы в дальнейшем ее объ-единить в конечный результат классификации. Подобного рода CNN-сети боль-ше всего подходят для распределенной обработки, что является одним из самых больших преимуществ локальносвязных сетей над полносвязными. Данная мо-дель была натренирована на наборе из 1,2 млн изображений, и на это потребо-валось 5 дней, чтобы завершить вычисления на двух 3-гигабайтных модулях GPU NVIDIA GTX 580. Для этого проекта использовались два блока с несколькими GPU (в общей сложности шесть модулей GPU). Cnn- сети с пОдхОдОм на ОснОве инкрементнОй тренир Овки Теперь, когда у нас есть порядочное понимание архитектуры CNN-сетей, пора за-катать рукава и применить CNN-сеть в библиотеке Keras. Для этого примера мы воспользуемся известным набором данных с изображе- ниями объектов CIFAR-10, который легкодоступен в домене Keras. Набор данных состоит из 60 000 цветных изображений в формате 32×32 с 10 целевыми класса-ми, включая самолет, автомобиль, птицу, кошку, оленя, собаку, лягушку, лошадь, корабль и грузовик. Это меньший по объему набор данных, чем тот, который ис - пользовался для примера с сетью AlexNet. Для получения дополнительной ин-формации в отношении этого набора данных можно обратиться на веб-страницу https://www.cs.toronto.edu/~kriz/cifar.html. В данной CNN-сети мы будем использовать следующую ниже архитектуру для классификации изображения в соответствии с упомянутыми 10 классами: вход -> свертка 1 (32,3,3) -> свертка 2 (32,3,3) -> объединяющий слой -> прореживание -> выход (полносвязный слой и функция мягкого максимума)\n--- Страница 197 ---\n196  Глубокое обучение с библиотекой TensorFlow вычисления на GPU Если у вас установлена CUDA-совместимая видеокарта, то для этого примера CNN вы можете использовать графический процессор, разместив в своем IDE следую-щий фрагмент кода в верхней части программы: import osos.environ['THEANO_FLAGS'] = \\ 'device=gpu0, assert_no_cpu_op=raise, on_unused_input=ignore, floatX=float32' Мы, правда, рекомендуем сначала опробовать этот пример на обычном CPU. Прежде всего выполним импорт необходимых библиотек и подготовим дан- ные. Мы используем входной размер 32×32, учитывая, что это фактический размер изображения: In:from keras.datasets import cifar10from keras.preprocessing.image import ImageDataGeneratorfrom keras.models import Sequentialfrom keras.layers.core import Dense, Dropout, Activation, Flattenfrom keras.layers.convolutional import Convolution2D, MaxPooling2Dfrom keras.optimizers import SGDfrom keras.utils import np_utils batch_size = 32 nb_classes = 10nb_epoch = 5 # число эпох, пользоваться осторожно, чтобы не испортить cpu/gpu img_rows, img_cols = 32, 32 # размерности входного изображения img_channels = 3 # изображения CIFAR10 имеют цветность RGB # данные перемешиваются и разбиваются на тренировочный и тестовый наборы (X_train, y_train), (X_test, y_test) = cifar10.load_data()print('Форма массива X_train:', X_train.shape) print(X_train.shape[0], 'тренировочных прецедентов')print(X_test.shape[0], 'тестовых прецедентов') # напоминаем, что нужно закодировать целевую переменную Y_train = np_utils.to_categorical(y_train, nb_classes)Y_test = np_utils.to_categorical(y_test, nb_classes) Теперь давайте развернем архитектуру CNN-сети и построим модель в соот - ветствии с задуманной архитектурой. Для этого примера мы натренируем модель CNN с использованием классиче- ского алгоритма SGD и инерции Нестерова: model = Sequential() # первый сверточный слой, устанавливаем размер фильтра model.add(Convolution2D(32, 3, 3, border_mode='same', input_shape=(img_channels, img_rows, img_cols)))model.add(Activation('relu'))\n--- Страница 198 ---\nВычисления на GPU  197 # второй сверточный слой model.add(Convolution2D(32, 3, 3))model.add(Activation('relu')) # задаем объединяющий слой model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.25)) # сначала сглаживаем вход в полносвязный слой для передачи в функцию softmax model.add(Flatten()) model.add(Dense(512))model.add(Activation('relu'))model.add(Dropout(0.2))model.add(Dense(nb_classes))model.add(Activation('softmax')) # тренируем модель при помощи алгоритма SGD + инерции, как и ранее sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd) X_train = X_train.astype('float32') X_test = X_test.astype('float32') # применяем к признакам шкалирование X_train /= 255 X_test /= 255 Этот шаг очень важен, потому что здесь мы определяем CNN-сеть для ее инкре- ментной тренировки. В предыдущих главах (см. главу 2 « Масштабируемое обуче - ние в Scikit-learn») и в предыдущем абзаце мы убедились в вычислительной эф- фективности онлайнового и инкрементного обучения. Мы можем сымитировать некоторые его свойства и применить его к CNN-сетям, используя очень неболь-шой размер эпохи с меньшим размером пакета batch_size (долей тренировочного набора в каждой эпохе), и натренировать их инкрементно в цикле for. Таким об- разом можно получить одинаковое количество эпох и натренировать CNN-сеть за гораздо более короткое время и с более низкой нагрузкой на оперативную па-мять. Эта очень мощная идея может быть реализована на основе простого цикла for следующим образом: for epoch in xrange(nb_epoch): model.fit(X_train, Y_train, batch_size=batch_size, epochs=1, validation_data=(X_test, Y_test), shuffle=True) Out: Форма массива X_train: (50000, 32, 32, 3)50000 тренировочных прецедентов10000 тестовых прецедентовTrain on 50000 samples, validate on 10000 samplesEpoch 1/150000/50000 [==============================] - 1480s - loss: 1.4464 -acc: 0.4803 - val_loss: 1.1774 - val_acc: 0.5785Train on 50000 samples, validate on 10000 samplesEpoch 1/150000/50000 [==============================] - 1475s - loss: 1.0701 -acc: 0.6212 - val_loss: 0.9959 - val_acc: 0.6525\n--- Страница 199 ---\n198  Глубокое обучение с библиотекой TensorFlow Train on 50000 samples, validate on 10000 samples Epoch 1/150000/50000 [==============================] - 1502s - loss: 0.8841 -acc: 0.6883 - val_loss: 0.9395 - val_acc: 0.6750Train on 50000 samples, validate on 10000 samplesEpoch 1/150000/50000 [==============================] - 1555s - loss: 0.7308 -acc: 0.7447 - val_loss: 0.9138 - val_acc: 0.6920Train on 50000 samples, validate on 10000 samplesEpoch 1/150000/50000 [==============================] - 1587s - loss: 0.5972 -acc: 0.7925 - val_loss: 0.9351 - val_acc: 0.6820 Выше показан процесс тренировки CNN-сети, в результате которого мы в ито- ге приходим к точности на проверочном наборе порядка 0.7. Учитывая, что мы натренировали многосложную модель на высокоразмерном наборе данных из 50 000 тренировочных примеров с 10 целевыми классами, этот результат уже до-статочно убедительный. Максимально возможная отметка, которая может быть достигнута с CNN-сетью на таком наборе данных, требует, по крайней мере, 200 эпох. Предложенный в этом примере метод ни в коем случае не является окончательным. Это довольно-таки элементарная реализация, чтобы приступить к работе с CNN-сетями. Не стесняйтесь экспериментировать, добавляя или удаляя слои, корректируя размер пакета и т. д. Поиграйте с параметрами, чтобы почув-ствовать, как они работают. Если вы хотите больше узнать о последних наработках относительно сверточ- ных слоев, обратитесь к остаточной сети (ResNet), которая представляет собой одно из последних усовершенствований в архитектуре CNN-сетей. Кайминг Хе (Kaiming He) и другие разработчики сети этого типа стали победи- телями в состязании ImageNet за 2015 г. (ILSVRC). Она располагает интересной архитектурой, в которой используется метод пакетной нормализации, или батч-нормализации, чья задача состоит в нормализации преобразования признаков между слоями. В библиотеке Keras имеется функция пакетной нормализации, с ко-торой можно было бы поэкспериментировать (http://keras.io/layers/normalization/). Чтобы получить общее представление о последнем поколении сетей ConvNet, следует ознакомиться со следующими настройками параметров для CNN-сетей, которые были признаны более эффективными: малый шаг; затухание веса (рег уляризация вместо прореживания); от сутствие прореживания; пакетная нормализация ме жду слоями среднего уровня; меньший об ъем предварительной тренировки или полное ее отсутствие (автокодировщики и машины Больцмана постепенно выходят из употреб - ления в клас сификации образов). Еще одним интересным моментом является то, что в последнее время сверточ- ные сети стали использоваться в других приложениях, которые выходят за преде-лы идентификации образов. Они используются для классификации языков и тек - стов, заполнения пропусков в предложении и даже в рекомендательных системах. Любопытным примером является музыкальный рекомендательный механизм\n--- Страница 200 ---\nРезюме  199 Spotify, который основан на сверточных нейронных сетях. Чтобы получить о нем дополнительную информацию, можно пройти по указанным ниже ссылкам: http://benanne.github .io/2014/08/05/spotify-cnns.html; http://machinelearning. wustl.edu/mlpapers/paper_files/NIPS2013_5004.pdf. В настоящее время сверточные сети используются в следующих областях: распознавание лиц (Fac ebook); клас сификация кинолент (YouTube); обработка ре чи и текста; генеративное иску сство (к примеру, Google DeepDream); рекомендат ельные механизмы (рекомендация музыкальных композиций – Spotify). резюме В этой главе мы проделали по-настоящему длинный путь, охватив среду TensorFlow и соответствующие методы. Мы познакомились с тем, как настраивать элемен-тарные регрессоры, классификаторы и нейронные сети с единственным скрытым слоем. Несмотря на то что программирование операций платформы TensorFlow для стандартных задач машинного обучения выполняется относительно пря-молинейно, платформа TensorFlow может быть немного утомительной. Именно здесь на первый план выходит более высокоуровневая библиотека SkFlow с ин-терфейсом, весьма похожим на Scikit-learn. Для инкрементных решений или даже внеядерных решений SkFlow предоставляет метод частичной подгонки, который можно легко настроить. Другие крупномасштабные решения либо ограничены приложениями на основе GPU, либо находятся на преждевременной стадии. По-этому в том, что касается масштабируемых решений, на данный момент мы долж - ны остановиться на стратегии инкрементного обучения. Мы также предоставили введение в сверточные нейронные сети и увидели, как они могут реализовываться в библиотеке Keras.",
      "debug": {
        "start_page": 171,
        "end_page": 200
      }
    },
    {
      "name": "Глава 6. Классификационные и регрессионные деревья в крупном масштабе 200",
      "content": "--- Страница 201 --- (продолжение)\nГлава 6 Классификационные и регрессионные деревья в крупном масштабе В этой главе мы сосредоточимся на масштабируемых методах для классифика- ционных и регрессионных деревьев. При этом будут затронуты следующие темы: советы и рек омендации для приложений на основе быстрых случайных ле- сов с использованием Scikit-learn; аддитивные мо дели со случайными лесами и подвыборкой; алгоритм градиентног о бустинга GBM; алгоритм X GBoost вместе с потоковыми методами; очень быстрый алгоритм GBM и случайный лес в среде H2O. Цель дерева решений состоит в том, чтобы, основываясь на тренировочных данных, обучиться серии решающих правил, которые позволяют прийти к за-ключению о целевых метках. Процесс обучения на основе рекурсивного алгорит - ма начинается в корне дерева и далее расщепляет данные по признаку, который приводит к самой низкой неоднородности. В настоящее время самое широкое распространение получили масштабируемые приложения с построением де-ревьев решений на основе алгоритма CART. Данный алгоритм был разработан в 1984 г. профессорами статистики Брейманом (Breiman), Фридманом (Friedman), Стоуном (Stone) и Олшеном (Ohlson), а его название является аббревиатурой от Classification and Regression Trees, т. е. классификационные и регрес сионные деревья . Алгоритм CART отличается от других моделей на основе дерева реше- ний (таких как ID3, C4.5/C5.0, CHAID и MARS) двумя аспектами. Во-первых, CART применим к задачам классификации и регрессии. Во-вторых, он создает бинар-ные деревья (каждое расщепление в результате является бинарным). Это позво-ляет деревьям CART рекурсивно оперировать заданными признаками и в жадном режиме выполнять оптимизацию на метрике ошибки в форме неоднородности. Подобного рода бинарные деревья вместе с масштабируемыми решениями на-ходятся в центре внимания настоящей главы. Давайте приглядимся к тому, каким образом эти деревья сконструированы. Мы видим дерево решений в виде графа с узлами, в котором информация передается сверху вниз, начиная с самой вершины. Каждое решение внутри дерева принима-ется путем бинарных расщеплений как для классов (булева переменная), так и для\nГлава 6 Классификационные и регрессионные деревья в крупном масштабе В этой главе мы сосредоточимся на масштабируемых методах для классифика- ционных и регрессионных деревьев. При этом будут затронуты следующие темы: советы и рек омендации для приложений на основе быстрых случайных ле- сов с использованием Scikit-learn; аддитивные мо дели со случайными лесами и подвыборкой; алгоритм градиентног о бустинга GBM; алгоритм X GBoost вместе с потоковыми методами; очень быстрый алгоритм GBM и случайный лес в среде H2O. Цель дерева решений состоит в том, чтобы, основываясь на тренировочных данных, обучиться серии решающих правил, которые позволяют прийти к за-ключению о целевых метках. Процесс обучения на основе рекурсивного алгорит - ма начинается в корне дерева и далее расщепляет данные по признаку, который приводит к самой низкой неоднородности. В настоящее время самое широкое распространение получили масштабируемые приложения с построением де-ревьев решений на основе алгоритма CART. Данный алгоритм был разработан в 1984 г. профессорами статистики Брейманом (Breiman), Фридманом (Friedman), Стоуном (Stone) и Олшеном (Ohlson), а его название является аббревиатурой от Classification and Regression Trees, т. е. классификационные и регрес сионные деревья . Алгоритм CART отличается от других моделей на основе дерева реше- ний (таких как ID3, C4.5/C5.0, CHAID и MARS) двумя аспектами. Во-первых, CART применим к задачам классификации и регрессии. Во-вторых, он создает бинар-ные деревья (каждое расщепление в результате является бинарным). Это позво-ляет деревьям CART рекурсивно оперировать заданными признаками и в жадном режиме выполнять оптимизацию на метрике ошибки в форме неоднородности. Подобного рода бинарные деревья вместе с масштабируемыми решениями на-ходятся в центре внимания настоящей главы. Давайте приглядимся к тому, каким образом эти деревья сконструированы. Мы видим дерево решений в виде графа с узлами, в котором информация передается сверху вниз, начиная с самой вершины. Каждое решение внутри дерева принима-ется путем бинарных расщеплений как для классов (булева переменная), так и для\n--- Страница 202 ---\nКлассификационные и регрессионные деревья в крупном масштабе  201 непрерывных переменных (пороговое значение), которые приводят к итоговому предсказанию. Деревья конструируются и обучаются при помощи следующей процедуры.Выполняется рекурсивный поиск переменной, которая наилучшим образом расщепляет целевую метку от корня до терминального узла. Данный процесс измеряется коэффициентом неоднородности (impurity) для каждого признака, который мы минимизируем, основываясь на целевом исходе. В настоящей гла-ве используются соответствующие меры неоднородности: индекс Джини и пере-крестная энтропия. Индекс Джини: Индекс Джини (Gini impurity) – это метрика, которая измеряет степень расхож - дения между вероятностями pi целевых классов (k) таким образом, что равный разброс значений вероятностей по целевым классам приводит к высокому ин-дексу Джини 1. Перекрестная энтропия: В перекрестной энтропии мы смотрим на логарифмическую вероятность не- правильной классификации. Обе метрики, как было доказано, приводят к очень похожим результатам. Однако индекс Джини в вычислительном плане более эф-фективен, потому что не требует вычисления логарифмов. Мы продолжаем, пока не будет удовлетворен критерий останова. Этот критерий может примерно означать две вещи: во-первых, добавление новых переменных больше не улучшает целевого результата, и, во-вторых, достигнута максимальная глубина дерева, или порог сложности дерева. Отметим, что очень глубокие и слож - ные деревья со многими узлами могут легко привести к переподгонке. Для ее пре-дотвращения мы обычно подрезаем дерево путем ограничения его глубины. Для получения интуитивно понятного представления о том, как этот процесс работает, создадим дерево решений при помощи Scikit-learn и визуализируем его в программе graphviz. Прежде всего создадим миниатюрный набор данных, кото-рый позволит предсказывать курильщика, основываясь на следующих признаках: IQ (число), возраст (число), годовой доход (число), предприниматель (булева пере-менная) и университетский диплом (булева переменная). Программу graphviz не-обходимо скачать с http://www.graphviz.org; она потребуется для загрузки и визуа- лизации файла tree.dot , который мы собираемся создать при помощи Scikit-learn: 1 Этот показатель был разработан итальянским экономистом, статистиком и демогра- фом Коррадо Джини (1884–1965 гг.). В статистике это количественный показатель, ко- торый показывает степень неравенства различных вариантов распределения доходов и называется коэффициентом Джини. Процентное представление этого коэффициен-та называется индексом Джини, и, в частности, в машинном обучении он применяется для предсказания непрерывных величин, где его смысл – погрешность должна быть на-столько равномерной, т. е. однородной, насколько можно. – Прим. перев.\n--- Страница 203 ---\n202  Классификационные и регрес сионные деревья в крупном масштабе In: import numpy as npfrom sklearn import tree iq = [90,110,100,140,110,100] age = [42,20,50,40,70,50]anincome = [40,20,46,28,100,20]businessowner = [0,1,0,1,0,0]univdegree = [0,1,0,1,0,0]smoking = [1,0,0,1,1,0] ids = np.column_stack((iq, age, anincome, businessowner, univdegree)) names = ['iq', 'age', 'income', 'univdegree'] dt = tree.DecisionTreeClassifier(random_state=99)dt.fit(ids,smoking) dt.predict(ids)tree.export_graphviz(dt, out_file='data\\\\tree2.dot', feature_names=names, label=all, max_depth=5, class_names=True) Теперь в рабочем каталоге можно найти файл tree.dot . Найдя его там, откройте этот файл в программе graphviz: Корневой узел (доход): стартовый узел, который представляет признак с са- мым выс оким приростом информации и самой низкой неоднородностью (Gini = .5). Внутренние узлы (возраст и IQ): это каждый узел между корневым узлом и терминальным. Родительские узлы передают решающие правила вниз по направлению к принимающей стороне – дочерним узлам (левому и правому).\n--- Страница 204 ---\nАгрегация бутстрапированных выборок  203 Терминальные узлы (листовые узлы): целевые метки, разделенные иерар- хической структурой. Глубина дерева – эт о число дуг из корневого узла до терминальных узлов. В данном случае глубина дерева равна 3. Теперь мы видим все бинарные расщепления, которые получились в результате генерирования дерева. Вверху в корневом узле мы видим, что человек с доходом ниже 24k не является курильщиком (доход < 24). В каждом узле мы также видим со-ответствующий уровень неоднородности, т. е. индекс Gini (.5) для данного расщеп - ления. Левосторонние дочерние узлы отсутствуют, потому что решение является окончательным. В этом месте путь просто заканчивается, потому что он полностью разделяет целевой класс. Однако в правостороннем дочернем узле дохода (возраст) дерево ответвляется. Здесь, если возраст меньше или равен 46, этот человек не ку - рильщик, но с возрастом более 46 и IQ ниже 105 этот человек – курильщик. В такой же степени важно несколько созданных нами признаков – диплом и предприни-мательство, но они не являются частью дерева. Это вызвано тем, что переменные в дереве способны расклассифицировать целевые метки без них. Эти пропущенные признаки просто не участвуют в уменьшении уровня неоднородности в дереве. Одиночные деревья имеют недостатки – они легко поддаются переподгонке и потому плохо обобщаются на ранее не встречавшихся данных. Тренировка те-кущего поколения этих методов выполняется при помощи ансамблевых методов, где одиночные деревья агрегируются в намного более мощные модели. Такие ансамблевые CART-методы получили в машинном обучении самое широкое рас - пространение вследствие их точности, простоты использования и возможности обрабатывать гетерогенные данные. Эти методы успешно применялись на недав-них конкурсах в области науки о данных, в частности Kaggle и KDD-cup. Посколь-ку ансамблевые методы для классификационных и регрессионных деревьев в на-стоящее время в мире ИИ и науки о данных являются нормой, масштабируемые решения для ансамблевых CART-методов будут основной темой настоящей главы. Мы обычно различаем два класса ансамблевых методов, которые используются с CART-моделями, а именно бэггинг и бустинг. Давайте разберемся в этих поняти-ях, чтобы сформировать достаточное понимание в отношении того, как работает процесс создания ансамбля. агрегация бутстрапир Ованных выбОр Ок Бэггинг – это аббревиатура от англ. термина bootstrap aggregation, т. е. агрега- ция бу тстрап-выборок. Метод бутстрапирования первоначально появился в контексте, где аналитикам приходилось иметь дело с дефицитом данных. На основе этого статистического подхода подвыборки использовались для оценки параметров генеральной совокупности, когда статистическое распределение не-возможно было вычислить априорно. Цель бустрапирования состоит в том, чтобы обеспечить более устойчивую оценку параметров генеральной совокупности, где бóльшая вариабельность вносится в меньший по объему набор данных случайной подвыборкой с возвратом. Обычно бутстрапирование проходит по следующему базовому сценарию.1. Взять из отдельно взятого набора данных произвольную выборку размером x с возвратом.\n--- Страница 205 ---\n204  Классификационные и регре ссионные деревья в крупном масштабе 2. Вычислить метрику или параметр из каждой выборки с целью оценки пара- метров г енеральной совокупности. 3. Агрегировать резу льтаты. В последние годы бутстрап-методы стали использоваться и для параметров мо- делей машинного обучения. Ансамбль показывает свою наибольшую эффектив-ность, когда его классификаторы обеспечивают весьма разнообразные границы решения. Это разнообразие в ансамблях может быть достигнуто за счет разно - образия ег о базовых моделей и данных, на которых выполняется тренировка этих моделей. Деревья очень хорошо подходят для такого разнообразия среди класси-фикаторов, потому что структура деревьев может быть крайне изменчивой. Одна-ко самый популярный метод ансамблирования состоит в использовании разных тренировочных наборов данных для тренировки отдельных классификаторов. Нередко подобного рода наборы данных получают посредством методов подвы-борки-выборки, в частности методами бутстрапирования и бэггинга. Все началось с идеи, что путем привлечения большего количества данных мы можем умень-шить дисперсию в оценочных значениях. Если нет возможности иметь под рукой больший объем данных, то повторная выборка может обеспечить существенное улучшение ситуации, потому что она позволяет выполнять повторную тренировку алгоритма на многих версиях тренировочной выборки. Как раз отсюда появилась идея беггинга; используя бэггинг, мы продвигаем идею бутстрапирования еще не-много дальше за счет агрегации (к примеру, усреднения) результатов многих по-вторных выборок, с тем чтобы в итоге прийти к заключительному предсказанию, где ошибки из-за внутривыборочной переподгонки взаимно сглаживаются. Когда мы применяем ансамблевый метод, в частности бэггинг, к древовидным моделям, мы создаем многочисленные деревья на каждой отдельной бустрапиро-ванной выборке (т. е. подвыборке с использованием отбора прецедентов без воз-врата) из исходного набора данных и затем агрегируем результаты (обычно путем арифметического, геометрического усреднения или голосования). В таком варианте стандартный алгоритм бэггинга выглядит следующим об - разом. 1. Из по лного набора данных взять n случайных выборок размером K с возвра- том (S1, S2, …, Sn). 2. Натренировать о тдельные деревья на выборках (S1, S2, …, Sn). 3. Вычислить предсказания на новых данных, исходя из выборок (S1, S2, …, Sn), и агрегировать их результаты. CART-модели извлекают огромную выгоду из методов бэггинга вследствие вносимой ими стохастичности и разнообразия. случайный лес и экстремальнО ранд Омизир Ованный лес Помимо бэггинга, который основан на тренировочных примерах, случайные под-выборки могут также извлекаться на основе признаков. Такой метод называется методом случайных подпрос транств. Метод случайных подпространств осо- бенно полезен для высокоразмерных данных (данных с большим числом призна-ков) и является основой метода, который называется «случайным лесом». Во вре-мя написания этой главы алгоритм случайного леса являлся самым популярным алгоритмом машинного обучения вследствие своей простоты использования,\n--- Страница 206 ---\nСлучайный лес и экстремально рандомизированный лес  205 устойчивости к грязным данным и параллелизируемости. Он нашел свое приме- нение в самых разнообразных приложениях, в том числе в приложениях позицио - нирования, играх и методах скрининга в медицинских программах. Например, в игровой приставке Xbox Kinect используется модель обнаружения движения на основе случайного леса. Учитывая, что алгоритм случайного леса основывается на методах бэггинга, он является относительно прямолинейным. 1. Взять из имеющейся выборки m бустрап-выборок размера N. 2. Независимо с троить деревья на каждом подмножестве (S1, S2, …, Sn) с исполь- зованием в каждом расщеплении узла разной доли набора признаков G (без возврата). 3. Минимизировать ошибку расщеплений узлов (основываясь на индексе Джи- ни или энтропийной мере). 4. Дать каждому дереву сделать предсказание и агрегировать результаты с ис - пользованием голосования для классификации и усреднения для регрессии. Поскольку бэггинг опирается на многочисленные подвыборки, он является превосходным кандидатом для параллелизации, где каждый модуль CPU выделя-ется для вычисления отдельных моделей. В силу этого обучение можно ускорить благодаря широкой доступности многочисленных ядер. Правда, в такой стратегии масштабирования имеется предел, поскольку мы должны осознавать, что Python – это однопоточный язык, и нам придется реплицировать множество экземпляров Python, причем каждый будет тиражировать область памяти с задействованными в ней внутривыборочными примерами. Следовательно, мы должны иметь в рас - поряжении достаточно много оперативной памяти, чтобы в ней уместились тре-нировочная матрица и ряд процессов. Если объема доступной RAM не будет хва-тать, то задание числа параллельных древовидных вычислений, работающих на компьютере одновременно, масштабировать алгоритм не поможет. В этом случае память RAM и использование CPU являются серьезными узкими местами. Модели на основе случайных лесов довольно просты для применения в ма- шинном обучении, потому что для своей качественной работы они не требуют какой-то особой доводки гиперпараметров. Самые важные параметры, пред-ставляющие интерес, – это количество деревьев и глубина деревьев, имеющие наибольшее влияние на результативность модели. Оперирование этими двумя гиперпараметрами приводит к компромиссу между точностью и результативно-стью, когда большее число деревьев и бóльшая глубина приводят к более высокой вычислительной нагрузке. Как показывает наш практический опыт, рекоменду - ется не устанавливать значение количества деревьев слишком высоким, потому что в конечном счете модель достигнет плато результативности и не будет больше улучшаться при добавлении большего количества деревьев, а попросту приведет к чрезмерной нагрузке на ядра CPU. Исходя из таких соображений, несмотря на то что модель на основе случайного леса со стандартными параметрами выпол-няется хорошо «из коробки», мы по-прежнему можем увеличить ее результатив-ность путем настройки числа деревьев. Посмотрим на приведенную ниже таблицу с обзором гиперпараметров для случайных лесов. Самые важные параметры для бэггинга с алгоритмом случайного леса сле - дующие: n_estimators : число деревьев в модели; max_features : число признаков, используемых для конструирования дерева;\n--- Страница 207 ---\n206  Классификационные и регре ссионные деревья в крупном масштабе min_sample_leaf : расщепление узла удаляется, если терминальный узел со- держит меньше выборок, чем минимальное значение; max_depth : число узлов, которые мы передаем сверху вниз от корня до тер- минального узла; criterion : метод, используемый для вычисления оптимального расщепле- ния (Джини или энтропия); min_samples_split : минимальное число выборок, требуемых для расщепле- ния внутреннего узла. Библиотека Scikit-learn предоставляет широкий спектр мощных ансамблевых CART-приложений, некоторые из которых в вычислительном плане вполне эф- фективны. В том, что касается случайных лесов, существует нередко упускаемый из виду алгоритм, который называется extra-trees, более известный как экстре- мально рандомизированный лес (extremely randomized forest). И когда речь заходит об эффективности CPU, алгоритм extra-trees может обеспечить значи-тельное ускорение по сравнению с обычными случайными лесами, иногда даже в десятки раз. В следующей ниже таблице вы видите скорость вычисления для каждого мето- да. Алгоритм extra-trees значительно быстрее с более явным отрывом, когда мы увеличиваем объем выборки: Объем выборки Экстральные деревья, сек. Случайный лес, сек. 100 000 25.9 164 50 000 9.95 35.1 10 000 2.11 6.3 Модели тренировались 50 признаками и 100 оценщиками, как для экстремаль- но рандомизированного леса extra-trees, так и для случайного леса. Для этого при- мера мы использовали четырехъядерный MacBook Pro с 16 ГБ RAM и измеряли затраченное на тренировку время в секундах. В этом абзаце мы будем использовать метод экстремальных лесов вместо клас - сического метода случайного леса из Scikit-learn. Поэтому вы справедливо може-те задать вопрос: чем они отличаются? Различие не поражает своей сложностью. В случайных лесах правила решения в расщеплении узла опираются на наилуч-шую отметку, получаемую в результате случайного отбора признаков во время каждой итерации. В чрезвычайно рандомизированных лесах случайное расщеп - ление г енерируется на каждом признаке в случайном подмножестве (и потому от - сутствуют вычисления, расходуемые на поиск лучшего расщепления для каждого признака), и затем отбирается порог с наилучшей отметкой. Такого рода подход приносит с собой некоторые выгодные свойства, потому что этот метод приво-дит к получению моделей с более низкой дисперсией, несмотря на то что каждое отдельное дерево строится до тех пор, пока не будет иметь самую большую точ-ность в терминальных узлах. Поскольку в расщепление ветвей добавляется боль-ше случайности, древовидный ученик делает ошибки, которые систематически менее коррелированы среди деревьев в ансамбле. Это приводит к намного более некоррелированным оценочным значениям в ансамбле и, в зависимости от за-дачи обучения (в конце концов, бесплатные обеды отсутствуют), к более низкой\n--- Страница 208 ---\nСлучайный лес и экстремально рандомизированный лес  207 ошибке обобщения, чем стандартный ансамбль из случайных лесов. На практике, однако, обычная модель случайного леса может обеспечить слегка более высокую точность. С учетом таких интересных свойств обучения с более эффективным вычисле- нием расщепления узла и тем же параллелизмом, который задействован в слу - чайных лесах, мы рассматриваем экстремально рандомизированные деревья в качестве превосходного кандидата в разряд ансамблевых алгоритмов на основе дерева решений, если хотим ускорить внутриядерное обучение. Для получения подробного описания алгоритма экстремально рандомизиро- ванного леса можно почитать следующую ниже статью, с которой все началось: Geurts D. Ernst и L. Wehenkel, Extremely randomized trees, Machine Learning, 63 (1), 3–42, 2006 («Экстремально рандомизированные деревья»). Данная статья нахо-дится в свободном доступе, и ее можно скачать по прямой ссылке https://www. semanticscholar.org/paper/Extremely-randomized-trees-Geurts-Ernst/336a165c17c9c56160d332b9f4a2b403fccbdbfb/pdf. В качестве примера того, как масштабировать внутриядерные древовидные ансамбли, мы выполним пример, где применим эффективный метод случайного леса для кредитных данных Credit. Этот набор данных используется для предска-зания для клиента соответствующего уровня его кредитной карты. Данные состо-ят из 18 признаков и 30 000 тренировочных примеров. Поскольку нужно импор-тировать файл в формате XLS, необходимо установить библиотеку xlrd; это можно сделать, набрав в терминальном окне командной строки следующее: $ pip install xlrd import os import xlrdimport urllibimport pandas as pdimport numpy as npfrom sklearn.ensemble import ExtraTreesClassifierfrom sklearn.model_selection import cross_val_score, train_test_splitfrom sklearn.datasets import make_classification #os.chdir(‘/ваш-путь’) # задать свой путь path = ‘data’ # в данном случае подкаталог в главе 6url = ‘http://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls’filename = ‘creditdefault.xls’fullfilename = os.path.join(path, filename)urllib.urlretrieve(url, fullfilename) data = pd.read_excel(‘data/creditdefault.xls’, skiprows=1)target = ‘default payment next month’ y = np.asarray(data[target])features = data.columns.drop([‘ID’, target])X = np.asarray(data[features]) X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.30, random_state=101) clf = ExtraTreesClassifier(n_estimators=500, random_state=101)\n--- Страница 209 ---\n208  Классификационные и регре ссионные деревья в крупном масштабе clf.fit(X_train,y_train) scores = cross_val_score(clf, X_train, y_train, cv=3, scoring=’accuracy’, n_jobs=-1) print(‘Перекрестно-проверочная точность: среднее = %0.3f стд.откл = %0.3f’ % (np.mean(scores), np.std(scores))) Out: Перекрестно-проверочная точность: среднее = 0.812 стд.откл = 0.003 Теперь, когда есть некая базовая оценка точности на тренировочном наборе, посмотрим на результативность модели на тестовом наборе. В этом случае мы хотим проследить ложноположительные и ложноотрицательные исходы, а также выполнить проверку на несбалансированность классов на целевой переменной: In: from sklearn.metrics import confusion_matrix, accuracy_score y_pred = clf.predict(X_test) confusionMatrix = confusion_matrix(y_test, y_pred) print(confusionMatrix) print('Общая точность на тестовом наборе %f' % accuracy_score(y_test, y_pred)) Out: [[6610 448] [1238 704]]Общая точность на тестовом наборе 0.812667 Интересно отметить, что точность тестового набора равна результатам на тре- нировочном наборе. Поскольку наша базовая модель функционирует в условиях по умолчанию, можно попытаться улучшить ее результативность путем доводки гиперпараметров, т. е. задачи, которая может оказаться в вычислительном плане дорогостоящей. Недавно были разработаны вычислительно более эффективные методы гиперпараметрической оптимизации, которых мы коснемся в следую-щем разделе. быстрая параметрическая Оптимизация пОсредствОм ранд Омизир ОваннОг О пОиска Возможно, вы уже знакомы с функционалом сеточного поиска в библиотеке Scikit-learn. В целом это отличный инструмент, но когда дело доходит до больших файлов, то в зависимости от пространства параметров он может значительно уве-личивать продолжительность тренировки. Для экстремальных случайных лесов мы можем ускорить время вычисления доводки параметров при помощи альтер-нативного метода параметрического поиска, который называется рандомизиро- ванным поиском . Там, где обычный исчерпывающий сеточный поиск приводит к чрезмерной нагрузке на CPU и оперативную память вследствие систематиче-ского тестирования всех возможных комбинаций значений гиперпарамет ров, рандомизированный поиск выбирает сочетания гиперпараметров наугад. Этот метод может привести к значительному увеличению скорости вычислений, когда сеточный поиск тестирует более 30 сочетаний (для меньших пространств\n--- Страница 210 ---\nБыстрая параметрическая оптимизация посредством рандомизированного  209 поиска сеточный поиск по-прежнему конкурентоспособен). Достижимый при- рост находится на том же уровне, который мы видели, когда переключились со случайных лесов на экстремально рандомизированные леса (от двух- до десяти-кратного увеличения в зависимости от спецификации оборудования, гиперпара-метрического пространства и размера набора данных). В параметре n_iter можно конкретизировать число значений гиперпараметров, которые оцениваются в случайном порядке: In: from sklearn.model_selection import GridSearchCV, RandomizedSearchCV param_dist = {\"max_depth\": [1,3,7,8,12, None], \"max_features\": [8,9,10,11,16,22], \"min_samples_split\": [8,10,11,14,16,19], \"min_samples_leaf\": [1,2,3,4,5,6,7], \"bootstrap\": [True, False]} # задать настройки поиска; используются только 25 произвольных # оцениваний параметров, при этом время тренировки удается # держать под контролемrsearch = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=25) rsearch.fit(X_train, y_train) bestclf = rsearch.best_estimator_#print(rsearch.cv_results_)print(bestclf) Out: ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=7, max_features=22, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1, oob_score=False, random_state=101, verbose=0, warm_start=False) Выше мы видим список оптимальных для нашей модели значений параметров. Теперь можно использовать эту модель для выполнения предсказаний на тесто- вом наборе: In: y_pred = bestclf.predict(X_test)confusionMatrix = confusion_matrix(y_test, y_pred)accuracy = accuracy_score(y_test, y_pred) print(confusionMatrix) print(accuracy) Out: [[6719 339] [1242 700]]0.824333333333 Нам удалось увеличить результативность модели в пределах допустимого диа- пазона продолжительностей тренировки и одновременно увеличить ее точность.\n--- Страница 211 ---\n210  Классификационные и регре ссионные деревья в крупном масштабе Экстремально рандомизированные деревья и большие наборы данных До сих пор мы рассматривали решения с горизонтальным масштабированием, за- действуя многоядерные CPU и рандомизацию благодаря специфическим характе-ристикам случайного леса и его более эффективной альтернативе – экстремально рандомизированному лесу. Однако если приходится иметь дело с большим набо-ром данных, который не помещается в оперативной памяти либо требует слишком больших затрат CPU, то можно попробовать внеядерное решение. Лучшим реше-нием для внеядерного подхода с ансамблями является решение, предо ставляемое сре дой H2O, которое далее в настоящей главе будет рассмотрено подробно. Однако мы можем применить еще один практический прием, чтобы заставить алгоритм случайного леса или экстремальных деревьев выполняться гладко на крупномас - штабном наборе данных. Второе наилучшее решение состоит в тренировке моде-лей на подвыборках данных с последующей сборкой в ансамбль результатов всех моделей, построенных на разных подвыборках данных (в конце концов, требуется только усреднить или сгруппировать результаты). В главе 3 « Быстрообучающиеся реализации машин SVM» мы уже ввели понятие резервуарного отбора, в результа-те которого продуцируются выборки на потоках данных. В настоящей главе мы снова воспользуемся выборкой данных, обратившись к более широкому набору алгоритмов генерирования выборок. Сначала давайте установим по-настоящему удобный инструмент командой строки под названием subsample , разработанной Полом Батлером (Paul Butler) (https://github.com/paulgb/subsample) для извлечения выборок из большого набора данных в формате с разделением полей символом новой строки (как правило, CSV-подобного файла). Этот инструмент предостав-ляет быстродействующие и простые методы отбора, в частности резервуарного отбора. Как явствует из главы 3 « Быстрообучающиеся реализации машин SVM», резерву - арный отбор – это алгоритм отбора наблюдений, который помогает извлекать вы-борки фиксированного размера из потока. Это концептуально простой алгоритм (в главе 3 была представлена его формула), требующий простого прохождения по данным для продуцирования выборки, которая будет сохранена в новом файле на диске. (Наш сценарий в главе 3 вместо этого сохранял ее в оперативной памяти.) В следующем ниже примере мы воспользуемся инструментом для извлечения подвыборок subsample вместе с методом по ансамблированию натренированных на этих подвыборках моделей. Собрав все вместе, в этом разделе мы должны выполнить следующие действия:1) создать набор данных и разделить его на тестовые и тренировочные данные; 2) взять подвыборки тренировочных данных и сохранить их как отдельные файлы на жестком диске; 3) загрузить эти подвыборки и натренировать на них модели на основе экст - ремально рандомизированного леса; 4) агрегировать мо дели; 5) проверить резу льтаты. Сначала при помощи pip установим инструмент для извлечения подвыборок: $ pip install subsample\n--- Страница 212 ---\nБыстрая параметрическая оптимизация посредством рандомизированного  211 В командной строке укажите рабочий каталог, содержащий файл, из которого вы хотите брать выборки: $ cd /ваш-путь В этом месте при помощи команды cd можно указать рабочий каталог, где не- обходимо хранить файл, который мы создадим на следующем шаге. Мы выполним это следующим образом: In: import numpy as npfrom sklearn.datasets import fetch_covtypefrom sklearn.model_selection import train_test_split #dataset = fetch_covtype(random_state=111, shuffle=True) dataset = fetch_covtype() X, y = dataset.data, dataset.target X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.3, random_state=0)del(X,y) covtrain = np.c_[X_train,y_train] covtest = np.c_[X_test,y_test]np.savetxt('covtrain.csv', covtrain, delimiter=\",\")np.savetxt('covtest.csv', covtest, delimiter=\",\") Теперь, когда мы разбили набор данных на тестовые и тренировочные наборы, давайте извлечем подвыборки из тренировочного набора, чтобы получить пор- ции данных, которые мы сможем закачать в оперативную память. Учитывая, что размер полного тренировочного набора данных составляет 30 000 примеров, мы извлечем три подвыборки меньшего размера, каждая по 10 000 элементов. Если ваш компьютер оборудован менее 2 Гб RAM, то вы, возможно, посчитаете более приемлемым разбить исходный тренировочный набор на меньшие по размеру файлы, правда, в результате этого полученные модельные результаты, скорее все-го, будут отличаться от примера, основанного на трех подвыборках. Как прави-ло, чем меньше примеров в подвыборках, тем больше смещение модели. Работая с подвыборками, мы в действительности пользуемся преимуществом работы на более управляемом объеме данных за счет увеличенного смещения оценок: $ subsample --reservoir -n 10000 covtrain.csv > cov1.csv $ subsample --reservoir -n 10000 covtrain.csv > cov2.csv$ subsample --reservoir -n 10000 covtrain.csv > cov3.csv  Напомним, что команды операционной системы могут быть исполнены внутри блокнота Jupyter, при этом они предваряются восклицательным знаком: !subsample --reservoir -n 10000 covtrain.csv > cov1.csv Теперь можно найти эти наборы в папке, которую вы указали в командной строке. Сейчас следует удостовериться, что в своем IDE или в блокноте Jupyter вы на- значили тот же самый путь.\n--- Страница 213 ---\n212  Классификационные и регре ссионные деревья в крупном масштабе Загрузим выборки поочередно и натренируем на них модель, основанную на случайном лесе. Прежде чем объединить их впоследствии для итогового предсказания, обра- щаем внимание, что мы придерживаемся пошагового подхода, чтобы вы могли проследить последовательность выполняемых шагов. Для того чтобы эти примеры выполнились успешно, удостоверьтесь, что вы установили тот же самый путь в своем IDE или блокноте Jupyter: import osos.chdir('/ваш-путь') В этом месте мы готовы начать обучаться на данных и можем поочередно загру - зить выборки в оперативную память и натренировать на них ансамбль деревьев: In:import osimport numpy as npimport pandas as pdfrom sklearn.ensemble import ExtraTreesClassifierfrom sklearn.model_selection import cross_val_score, train_test_split После вывода сообщения об отметке перекрестной проверки программа пе- рейдет к тренировке модели поочердно на всех порциях данных. Поскольку мы обучаемся раздельно на разных частях данных поочередными порциями, мы инициализируем ансамблевого ученика (в данном случае классификатор ExtraTreeClassifier ), используя параметр warm_start=True и метод set_params , кото - рый инкрементно добавляет деревья из предыдущих сеансов тренировки, так как метод подгонки fit вызывается многократно: # загружается выборка 1df = pd.read_csv('cov1.csv')y = df[df.columns[54]]X = df[df.columns[0:54]] clf1 = ExtraTreesClassifier(n_estimators=100, random_state=101, warm_start=True)clf1.fit(X,y) scores = cross_val_score(clf1, X, y, cv=3, scoring='accuracy', n_jobs=-1) print('Перекрестно-проверочая точность: среднее = %0.3f стд.откл = %0.3f' % (np.mean(scores), np.std(scores)))print(scores)print('Количество деревьев в модели: %s' % len(clf1.estimators_)) # выборка 2 df = pd.read_csv('cov2.csv')y = df[df.columns[54]]X = df[df.columns[0:54]] clf1.set_params(n_estimators=150, random_state=101, warm_start=True)\n--- Страница 214 ---\nБыстрая параметрическая оптимизация посредством рандомизированного  213 clf1.fit(X,y) scores = cross_val_score(clf1, X, y, cv=3, scoring='accuracy', n_jobs=-1) print('После params ->') print('Перекрестно-проверочая точность: среднее = %0.3f стд.откл = %0.3f' % (np.mean(scores), np.std(scores)))print(scores)print('Количество деревьев в модели: %s' % len(clf1.estimators_)) # выборка 3 df = pd.read_csv('cov3.csv')y = df[df.columns[54]]X = df[df.columns[0:54]]clf1.set_params(n_estimators=200, random_state=101, warm_start=True)clf1.fit(X,y)scores = cross_val_score(clf1, X, y, cv=3, scoring='accuracy', n_jobs=-1) print('После params ->') print('Перекрестно-проверочая точность: среднее = %0.3f стд.откл = %0.3f' % (np.mean(scores), np.std(scores)))print(scores)print('Количество деревьев в модели: %s' % len(clf1.estimators_)) # теперь выполним предсказание комбинированной моделью # на тестовом наборе и проверим отметку точности df = pd.read_csv('covtest.csv')X = df[df.columns[0:54]]y = df[df.columns[54]]pred2 = clf1.predict(X)scores = cross_val_score(clf1, X, y, cv=3, scoring='accuracy', n_jobs=-1)print(\"Итоговая отметка на тестовом наборе %r\" % np.mean(scores)) Out: Перекрестно-проверочая точность: среднее = 0.798 стд.откл = 0.008[ 0.7862069 0.80408041 0.80276193]Количество деревьев в модели: 100 После params ->Перекрестно-проверочая точность: среднее = 0.806 стд.откл = 0.007[ 0.80635492 0.79687969 0.81441441]Количество деревьев в модели: 150 После params ->Перекрестно-проверочая точность: среднее = 0.803 стд.откл = 0.003[ 0.80569715 0.79867987 0.80396277]Количество деревьев в модели: 200 Итоговая отметка на тестовом наборе 0.92185447181058278  Предупреждение: такая реализация выглядит не совсем по Python’овски, и тем не менее она дово льно эффективная.\n--- Страница 215 ---\n214  Классификационные и регре ссионные деревья в крупном масштабе Мы улучшили отметку относительно итогового предсказания, перейдя от по- казателя точности на тестовом наборе порядка .8 к точности .922. Это вызвано тем, что мы имеем итоговую объединенную модель со всей древовидной инфор-мацией, состоящей из предыдущих трех моделей, основанных на случайном лесе. В полученных результатах работы сценария можно также найти число деревьев, которые были добавлены к исходной модели. В дальнейшем вы можете попробовать такой подход на наборе данных еще большего объема с привлечением большего числа подвыборок, либо применить рандомизированный поиск к одной из подвыборок для улучшения настроек па-раметров. алгОритм Cart и бустинг Мы начали эту главу с бэггинга, т. е. агрегации бустрапированных выборок; теперь мы завершим наш обзор другим ансамблевым методом – бустингом, т. е. разго-ном базового алгоритма. Точно так же, как бэггинг, бустинг может использоваться и для регрессии, и для классификации, и в последнее время затмевает алгоритм случайного леса более высокой точностью. Как процесс оптимизации, бустинг основывается на принципе стохастического градиентного спуска, который мы видели в других методах, а именно оптими-зации модели путем минимизации ошибки в соответствии с градиентами. Са-мые известные на сегодня методы бустинга представлены алгоритмом AdaBoost и градиентным бустингом (GBM и недавно XGBoost). Алгоритм AdaBoost сводится к минимизации ошибки тех случаев, где предсказание немного неправильное, и в силу этого случаи, которые труднее классифицировать, привлекают больше внимания. В последнее время алгоритм AdaBoost не в почете, поскольку другие методы бустинга оказывались, как правило, точнее. В этой главе мы охватим два самых эффективных алгоритма бустинга, доступ- ных к сегодняшнему дню пользователям Python: машину градиентного бу стин- га (gradient boosting machine, GBM), чью реализацию можно найти в библиотеке Scikit-learn, и экстремальный градиентный бу стинг (extreme gradient boosting, XGBoost). Поскольку алгоритм GBM по своей природе последовательный, его трудно параллелизировать и тем самым тяжелее масштабировать, чем алгоритм случайного леса, но некоторые приемы с этой задачей справляются. Далее будут рассмотрены некоторые рекомендации и приемы ускорения алгоритма вместе с прекрасным решением вне оперативной памяти для платформы H2O. Машины градиентного бустинга Как мы видели в предыдущих разделах, случайные леса и экстремальные деревья являются эффективными алгоритмами – оба этих алгоритма работают вполне хо-рошо с минимальным усилием. Хотя алгоритм машины градиентного бустинга GBM признается как более точный метод, он не так прост в использовании, и для достижения наилучших результатов всегда существует необходимость в довод-ке большого количества свойственных ему гиперпараметров. Случайный лес, с другой стороны, показывает вполне хорошее качество работы с учетом всего нескольких параметров (главным образом глубины дерева и числа деревьев). Еще стоит обратить внимание на переподгонку. Случайные леса менее чувствительны\n--- Страница 216 ---\nАлгоритм CART и бустинг  215 к переподгонке, чем алгоритм GBM. Поэтому, используя его, мы также должны подумать о стратегиях регуляризации. Но самое главное, случайные леса намно-го проще настроить на выполнение параллельных операций, тогда как алгоритм GBM является последовательным и тем самым вычисляется медленнее. В настоящей главе мы применим алгоритм GBM из библиотеки Scikit-learn, за- тем обратимся к следующему поколению алгоритмов бустинга деревьев под на-званием XGBoost и реализуем бустинг в более крупном масштабе в среде H2O. Алгоритм GBM, который мы будем использовать в Scikit-learn и H2O, основан на двух важных понятиях: аддитивном расширении и градиентной оптимиза- ции алгоритмом наискорейшего спуска . Общая идея первого состоит в генери- ровании последовательности относительно простых деревьев (слабых учеников), где каждое следующее дерево добавляется вдоль градиента. Пусть имеются де - ревья М , которые агрегируют итоговые предсказания в ансамбле. Дерево в каж - дой итерации fk теперь является частью намного более широкого пространства всех возможных деревьев в модели (ø) (в библиотеке Scikit-learn этот параметр больше известен как n_estimators ): Новые деревья будут добавляться к предыдущим деревьям по принципу адди- тивного расширения поэтапно: yˆi(0) = 0; yˆi(1) = f1(xi) = yˆi(0) + f1(xi); – это первое дерево yˆi(2) = f1(xi) + f2(xi) = yˆi(1) + f2(xi); – второе дерево добавляется к предыдущему и т. д., пока не будет достигнут критерий останова. Предсказание ансамбля градиентного бустинга состоит из суммы предсказа- ний всех предыдущих деревьев и недавно добавленного дерева (y ˆi(t–1)) + f1(xi), что в более формальном плане приводит к следующему: Вторая важная и при этом довольно хитроумная часть алгоритма GBM – это градиентная оптимизация наискорейшим спуск ом. Иными словами, в адди- тивную модель мы добавляем все более мощные деревья. Это достигается путем применения к новым деревьям градиентной оптимизации. Каким образом вы-полняются обновления градиента с деревьями, раз нет никаких параметров, ана-логичных тем, которые мы видели во время работы с традиционными обучаю-щимися алгоритмами? Прежде всего мы параметризуем деревья; мы делаем это путем рекурсивного обновления значений расщепления узлов вдоль градиента, где узлы представлены вектором. Вследствие этого направление наискорейшего спуска является отрицательным градиентом функции потерь, и расщепления уз-лов будут обновляться и обучаться, приводя к: λ: параметру сжатия (в данном контексте также именуемом темпом обуче- ния), который заставит ансамбль обучаться медленно с добавлением боль-шего количества деревьев;\n--- Страница 217 ---\n216  Классификационные и регре ссионные деревья в крупном масштабе γmi: параметру обновления градиента, также именуемому длиной шага. Прогнозная отметка для каждого листа тем самым является итоговой отметкой для нового дерева, которая, в свою очередь, является результатом простого сум- мирования по каждому листу: Таким образом, если резюмировать, алгоритм GBM работает путем инкремент - ного добавления более точных деревьев, усвоенных вдоль градиента. Теперь, когда мы разбираемся в базовых понятиях, выполним пример с алго- ритмом GBM и посмотрим на самые важные параметры. Для алгоритма GBM эти параметры имеют дополнительную важность, потому что, когда мы устанавли-ваем число деревьев слишком высоко, мы обречены на экспоненциальный рост нагрузки на вычислительные ресурсы, поэтому с этими параметрами следует об-ращаться осторожно. Большинство параметров в приложении GBM библиотеки Scikit-learn совпадает с алгоритмом случайного леса, который мы рассмотрели в предыдущем абзаце. Мы должны учитывать три параметра, которые требуют особого внимания. Максимальная глубина (max_depth) В отличие от алгоритма случайного леса, который работает лучше, когда деревья создают свои древовидные структуры в их максимальной протяженности (тем са-мым конструируя и собирая в ансамбль предикторы с высокой дисперсией), алго-ритм GBM демонстрирует тенденцию работать лучше с более мелкими деревьями (тем самым привлекая предикторы с более высоким смещением, т. е. слабых уче-ников). Работа с более мелкими деревьями решений или только с пнями (деревья-ми решений только с одним-единственным ответвлением) может сократить про-должительность тренировки, получая скорость выполнения взамен на большее смещение (потому что более мелкое дерево едва способно перехватывать более сложные связи в данных). Темп обучения (learning_rate) Этот параметр, также именуемый сжатием λ (shrinkage), связан с оптимизацией методом градиентного спуска и с тем, как каждое дерево будет участвовать в ан-самбле. Меньшие значения этого параметра могут улучшить оптимизацию в про-цессе тренировки, хотя для этого потребуется схождение большего количества оценщиков и тем самым больше времени на вычисления. Поскольку он влияет на вес каждого дерева в ансамбле, его меньшие значения подразумевают, что каж - дое дерево вносит в процесс оптимизации небольшую часть, и потребуется боль-ше деревьев, прежде чем будет достигнуто хорошее решение. Следовательно, во время оптимизации этого параметра в целях результативности необходимо из-бегать слишком больших значений, которые могут привести к субоптимальным моделям; также необходимо избегать использования слишком низких значений, потому что это будет серьезно затрагивать продолжительность вычисления (для схождения ансамбля к решению потребуется больше деревьев). На нашем опыте хорошей начальной точкой является использование темпа обучения в интервале < 0.1 и > .001.\n--- Страница 218 ---\nАлгоритм CART и бустинг  217 Подвыборка Вспомним принципы бэггинга и вставки, где мы извлекаем случайные выборки и создаем деревья на их основе. Если в алгоритме GBM применить подвыборку, то мы рандомизируем создание деревьев и избегаем переподгонки, уменьшаем нагрузку на память и даже иногда увеличиваем точность. Эту процедуру можно также применить к алгоритму GBM, делая его более стохастическим и тем самым задействуя преимущества бэггинга. Можно рандомизировать создание деревьев в алгоритме GBM путем установки параметра подвыборки в .5. Ускоренный GBM с теплым стартом (warm_start) Этот параметр позволяет сохранять новую древовидную информацию, после того как каждая итерация добавляется к предыдущей без генерирования новых де - ревь ев. Таким образом можно сэкономить память и значительно ускорить время вычисления. Используя имеющийся в библиотеке Scikit-learn метод GBM, можно предпри- нять два действия, чтобы увеличить память и эффективность CPU: теплый с тарт для (полу)инкрементного обучения; использование паралле льной обработки во время перекрестной проверки. Проанализируем пример классификации на основе алгоритма GBM с исполь- зованием набора данных Spam из библиотеки машинного обучения UCI. Сначала загрузим данные, предварительно их обработаем и посмотрим на важность каж - дого признака: In: import pandasimport urllib2from sklearn import ensemble columnNames1_url = 'https://archive.ics.uci.edu/ml/machine-learningdatabases/spambase/ spambase.names'columnNames1 = [ line.strip().split(':')[0] for line in urllib2.urlopen(columnNames1_url).readlines()[33:]] columnNames1 n = 0for i in columnNames1: columnNames1[n] = i.replace('word_freq_','') n += 1print columnNames1 spamdata = pandas.read_csv( 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data', header=None, names=(columnNames1 + ['spam'])) X = spamdata.values[:,:57] y = spamdata['spam'] spamdata.head()\n--- Страница 219 ---\n218  Классификационные и регре ссионные деревья в крупном масштабе Выше будут выведены список с именами столбцов и первые 5 записей в табли- це. Далее рассчитаем отметки точности: import numpy as np from sklearn import cross_validationfrom sklearn.model_selection import cross_val_scorefrom sklearn.model_selection import cross_val_predict, train_test_splitfrom sklearn.metrics import classification_report, recall_scorefrom sklearn.metrics import f1_score, confusion_matrix, accuracy_scorefrom sklearn.ensemble import GradientBoostingClassifier X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.3, random_state=22) clf = ensemble.GradientBoostingClassifier(n_estimators=300, random_state=222, max_depth=16, learning_rate=.1, subsample=.5)scores = clf.fit(X_train,y_train)scores2 = cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1)print('Средняя точность %s' % scores2.mean()) y_pred = cross_val_predict(clf, X_test, y_test, cv=10)print('Точность на контрольных данных %s' % accuracy_score(y_test, y_pred)) Out: Средняя точность 0.943789291121Точность на контрольных данных 0.931209268646 В заключение покажем матрицу ошибок и признаки, упорядоченные по вели- чине важности. def featureImp_order(clf, X, k=5): return X[:,clf.feature_importances_.argsort()[::-1][:k]]#print(featureImp_order(clf, X, 2))#print(clf.feature_importances_) print(classification_report(y_test, y_pred)) confusionMatrix = confusion_matrix(y_test, y_pred)print(confusionMatrix) # упорядочим признаки по величине важности print('\\nВажности признаков:')print(sorted(zip(map(lambda x: round(x, 4), clf.feature_importances_), columnNames1), reverse=True)) Out: precision recall f1-score support 0 0.93 0.95 0.94 835 1 0.93 0.90 0.91 546\n--- Страница 220 ---\nАлгоритм CART и бустинг  219 avg / total 0.93 0.93 0.93 1381 [[797 38] [ 57 489]] Важности признаков: [(0.2193, 'char_freq_;'), (0.1079, 'report'), (0.0755, 'capital_run_length_average'), (0.0498, 'capital_run_length_total'), (0.0433, 'you'), (0.0354, 'char_freq_!'), (0.0325, '000'), (0.0312, 'capital_run_length_longest'), (0.0254, 'will'), (0.0243, 'business'), (0.0236, 'your'), (0.0226, 'char_freq_$'), (0.02, 'char_freq_('), (0.0161, 'mail'), (0.0159, 'internet'), (0.0, '415')] Мы видим, что при классификации спама символ « ;» является самым дискри- минирующим.  Важность признака показывает, насколько расщепление каждого признака уменьшает от - носит ельную неоднородность по всем расщеплениям в дереве. Ускорение алгоритма GBM при помощи параметра warm_start К сожалению, в библиотеке Scikit-learn параллельная обработка алгоритма GBM отсутствует. Параллелизировать можно только перекрестную проверку и сеточ-ный поиск. Тогда что можно сделать для его ускорения? Мы видели, что машина градиентного бустинга GBM работает по принципу аддитивного расширения, где деревья добавляются инкрементно. Мы можем использовать эту идею в Scikit-learn посредством параметра warm_start . Это можно легко смоделировать благо- даря функционалу GBM библиотеки Scikit-learn, если создавать древовидные мо-дели инкрементно при помощи цикла for. Поэтому давайте выполним это с тем же набором данных и обследуем вычислительное преимущество, которое он обес - печивает: gbc = GradientBoostingClassifier(warm_start=True, learning_rate=.05, max_depth=20, random_state=0) for n_estimators in range(1, 1500, 100): gbc.set_params(n_estimators=n_estimators) gbc.fit(X_train, y_train) y_pred = gbc.predict(X_test)print(classification_report(y_test, y_pred))print(gbc.set_params)\n--- Страница 221 ---\n220  Классификационные и регре ссионные деревья в крупном масштабе Out: precision recall f1-score support 0 0.93 0.95 0.94 835 1 0.92 0.89 0.91 546 avg / total 0.93 0.93 0.93 1381<bound method GradientBoostingClassifier.set_params of GradientBoostingClassifier(criterion='f riedman_mse', init=None, learning_rate=0.05, loss='deviance', max_depth=20, max_features=None, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=1401, presort='auto', random_state=0, subsample=1.0, verbose=0, warm_start=True)> Рекомендуем обратить особое внимание на результаты доводки дерева ( n_ es ti- mators=1401 ). Вы видите, что мы использовали модель размером 1401 дерево. Этот небольшой прием помог намного уменьшить продолжительность тренировки (думаем, наполовину или меньше), если сравнивать его с подобной моделью на основе алгоритма GBM, которую мы бы натренировали на 1401 дереве сразу. От - метим, что этот метод можно также использовать для алгоритма случайного леса и экстремального случайного леса. Тем не менее мы считаем его в особенности полезным для алгоритма GBM. Взглянем на рисунок, который показывает время тренировки обычного алго- ритма GBM и нашего на основе метода warm_start . Мы видим значительное уско- рение вычислений с отметкой точности относительно на том же уровне: 120 90 6030 0CекундыGBM на основе теплого старта против обычного GBM Обычный GBM GBM c теплым стартом Количество деревьев1500 1000 600 300120 105 85 74 14 15 16 17 Тренировка и хранение моделей GBM Вы когда-нибудь задумывались о тренировке модели на трех компьютерах одно- временно? Или о тренировке модели GBM на экземпляре Amazon EC2? Вполне может случиться так, что вам понадобится натренировать модель и сохранить ее для использования в дальнейшем. Когда приходится ждать в течение двух дней\n--- Страница 222 ---\nАлгоритм XGBoost  221 до полного завершения раунда тренировки, мы вовсе не хотим пройти через этот процесс снова. В случае если вы натренировали модель в облаке на экземпляре Amazon EC2, вы можете сохранить эту модель и использовать ее повторно позже на другом компьютере при помощи функционала joblib библиотеки Scikit-learn. Поэтому давайте проанализируем этот процесс, раз уж библиотека Scikit-learn предоставила нам удобный инструмент по работе с ним. Выполним импорт нужных библиотек и укажем каталоги, где расположены наши файлы: import errno import os path = '/data/clfs' # укажите здесь свой путьclfm = os.makedirs(path) os.chdir(path) Теперь экспортируем модель в указанное расположение на жестком диске: from sklearn.externals import joblibjoblib.dump(gbc, 'clf_gbc.pkl') Сейчас можно загрузить модель и снова ее использовать для других целей: model_clone = joblib.load('clf_gbc.pkl')zpred = model_clone.predict(X_test)print(zpred) алгОритм XGB ooSt Мы только что обсудили, что параллельная обработка при использовании алгорит - ма машины градиентного бустинга GBM в Scikit-learn отсутствует без вариантов, и именно здесь на первый план выходит алгоритм XGBoost. Расширяя алгоритм GBM, алгоритм экстремального градиентного бустинга XGBoost добавляет боль-ше масштабируемых методов, которые задействуют многопоточность на одиноч-ной машине и параллельную обработку на кластерах из многочисленных серве-ров (используя сегментирование). Самое важное усовершенствование, вносимое алгоритмом XGBoost, по сравнению с алгоритмом GBM, состоит в возможности первого управлять разреженными данными. Алгоритм XGBoost принимает раз-реженные данные автоматически, не храня нулевых значений в памяти. Второе преимущество XGBoost заключается в том, каким образом вычисляются значения наилучшего расщепления узлов при ветвлении дерева, при этом используется ме-тод, который называется квантильной схемой (quantile sketch). Этот метод преоб-разует данные алгоритмом взвешивания, в результате которого потенциальные расщепления сортируются на основе определенного уровня точности. Для полу - чения дополнительной информации читайте статью, которую можете скачать по следующей прямой ссылке: http://arxiv.org/pdf/1603.02754v3.pdf. Алгоритм XGBoost расшифровывается как «экстремальный градиентный раз- гон» (extreme gradient boosting). Данный алгоритм градиентного бустинга с откры-тым исходным кодом получил большую популярность на конкурсах в области нау - ки о данных, в частности Kaggle (https://www.kaggle.com/) и KDD-cup в 2015 г. (Его\n--- Страница 223 ---\n222  Классификационные и регре ссионные деревья в крупном масштабе исходный код доступен на GitHub по адресу https://github.com/dmlc/XGBoost, как уже упоминалось в главе 1 « Первые шаги к масштабируемости».) По сообщениям авторов работы (Tianqui Chen, Tong He и Carlos Guestrin), посвященной этому ме- тоду бустинга, среди 29 соревнований, проводившихся в Kaggle в течение 2015 г., в 17 победивших решениях алгоритм XGBoost использовался автономно либо в ка-честве составной части какого-нибудь ансамбля из нескольких моделей. В своей статье XGBoost: A Scalable Tree Boosting System (« XGBoost: Масштабируемая система бустинга деревьев» (которую можно скачать по адресу http://learningsys.org/papers/ LearningSys_2015_paper_32.pdf)) авторы сообщают, что в недавнем соревновании KDD-cup 2015 г. алгоритм XGBoost использовался каждой командой, которая по итогам соревнований оказалась в десятке лучших. Помимо успешных показателей результативности (как точности, так и вычислительной эффективности), нашей основной задачей в этой книге является масштабируемость, и экстремальный гра-диентный бустинг XGBoost является по-настоящему масштабируемым решением с различных точек зрения. XGBoost – это новое поколение алгоритмов градиент - ного бустинга GBM с серьезной доводкой исходного алгоритма бустинга деревьев GBM. Алгоритм XGBoost обеспечивает параллельную обработку; предлагаемая ал-горитмом масштабируемость реализуется благодаря доработанным авторами не-скольким параметрическим настройкам и добавлениям: алгоритм принимает разреженные данные, в которых могут задействовать- ся разреженные матрицы, экономя оперативную память (отсутствует по-требность в плотных матрицах) и продолжительность вычисления (нулевые значения обрабатываются особым образом); обу чение приближенному дереву (взвешенный метод квантильной схемы), которое показывает аналогичные результаты, но за гораздо меньшее время, чем классический исчерпывающий просмотр возможных точек ветвления; параллельные вычисления на одиночной машине (используя многопоточ- ность в фазе поиска лучшего расщепления) и аналогичным образом рас - пределенные вычисления на нескольких машинах; внеядерные вычисления на одиночной машине с привлечением решения для хранения данных под названием «постолбцовый блок» (column block), которое располагает данные на диске столбцами, тем самым экономя вре-мя – данные с диска поступают в том виде, в котором их ожидает алгоритм оптимизации (который оперирует векторами-столбцами). С практической точки зрения экстремальный градиентный бустинг XGBoost демонстрирует главным образом те же параметры, что и алгоритм GBM. Алгоритм XGBoost также довольно хорошо обрабатывает пропущенные данные. Другие древовидные ансамбли, основанные на стандартных деревьях решений, требуют сначала импутировать 1 пропущенные данные, используя внешкальное значе- ние (в частности, большое отрицательное число), чтобы выработать надлежащее ветвление дерева в случае пропущенных значений. В отличие от них, алгоритм XGBoost сначала выполняет подгонку всех непропущенных значений и после соз-дания ветвления для переменной затем решает, какая ветвь лучше всего подхо-дит для пропущенных значений с целью уменьшения ошибки прогнозирования. 1 Импутация (imputation) – процесс замещения пропущенных, некорректных или несо- с тоятельных значений другими значениями. – Прим. перев.\n--- Страница 224 ---\nАлгоритм XGBoost  223 Такой подход приводит к более компактным деревьям, а эффективная стратегия импутации – к большей прогнозирующей способности. Самые важные параметры алгоритма XGBoost следующие: eta (по умолчанию = 0.3): эквивалент темпа обучения в алгоритме GBM биб - лиот еки Scikit-learn; min_child_weight (по умолчанию = 1): более высокие значения предотвраща- ют переподгонку и вычислительную сложность деревьев; max_depth (по умолчанию = 6): число взаимодействий в деревьях; subsample (по умолчанию = 1): доля выборок из тренировочных данных, ко- торые берутся в каждой итерации; colsample_bytree (по умолчанию = 1): доля признаков в каждой итерации; lambda (по умолчанию = 1): регуляризация L2 (булева переменная); seed (по умолчанию = 0): эквивалент параметра random_state в библиотеке Scikit-learn, обеспечивающий воспроизводимость процессов обучения по всем тестам и на разных машинах. Теперь, когда мы знаем самые важные параметры алгоритма XGBoost, выпол- ним пример его применения на том же самом наборе данных, который мы ис - пользовали для алгоритма GBM, с теми же настройками параметров (насколько это возможно). Применение алгоритма экстремального градиентного бустинга XGBoost немного менее прямолинейно, чем алгоритма GBM в библиотеке Scikit-learn. И поэтому мы предоставим несколько элементарных примеров, которые можно использовать в качестве отправной точки для более сложных моделей. Прежде чем углубиться в приложения на основе экстремального градиентного бустинга XGBoost, сравним этот метод с методом GBM в модуле sklearn на наборе данных о спаме; мы уже загрузили эти данные в память: import numpy as np import xgboost as xgbfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.metrics import classification_reportfrom sklearn import model_selectionfrom sklearn.model_selection import train_test_split clf = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=.1, subsample=.5) clf1 = GradientBoostingClassifier(n_estimators=100, max_depth=8, learning_rate=.1, subsample=.5) X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.3, random_state=22)xgm = clf.fit(X_train, y_train)gbmf = clf1.fit(X_train, y_train) y_pred = xgm.predict(X_test) y_pred2 = gbmf.predict(X_test) print('Результаты алгоритма XGBoost:') print(classification_report(y_test, y_pred))print('Результаты алгоритма GBM:')print(classification_report(y_test, y_pred2)) Out: 1 loop, best of 3: 1.71 s per loop 1 loop, best of 3: 2.91 s per loop\n--- Страница 225 ---\n224  Классификационные и регре ссионные деревья в крупном масштабе Результаты алгоритма XGBoost: precision recall f1-score support 0 0.95 0.97 0.96 835 1 0.95 0.93 0.94 546 avg / total 0.95 0.95 0.95 1381 Результаты алгоритма GBM: precision recall f1-score support 0 0.95 0.97 0.96 835 1 0.95 0.92 0.93 546 avg / total 0.95 0.95 0.95 1381 Ясно видно, что алгоритм экстремального градиентного бустинга XGBoost вы- полняется заметно быстрее, чем алгоритм GBM (1.71 против 2.91 сек.), несмотря на то что параллелизация для XGBoost даже не использовалась. Позднее мы даже сможем добиться большего ускорения, когда воспользуемся параллельной обра-боткой и внеядерными методами для XGBoost во время использования потоковой передачи вне основной памяти. В некоторых случаях модель на основе алгоритма экстремального градиентного бустинга XGBoost приводит к более высокой точ-ности, чем модель на основе алгоритма GBM, и (почти) никогда наоборот. Регрессия на основе XGBoost Методы бустинга часто используются для классификации, но также могут быть очень мощными для задач регрессии. Поскольку регрессия нередко упускается из виду, давайте выполним пример регрессии и проанализируем ключевые вопросы. Мы выполним подгонку модели бустинга на наборе данных о жилой недвижимо-сти в шт. Калифорния с использованием сеточного поиска. Набор данных о жи-лой недвижимости шт. Калифорния был недавно добавлен в Scikit-learn и потому сэко номит нам неск олько шагов предобработки: import os import numpy as npimport pandas as pdimport scipy.sparsefrom sklearn.model_selection import train_test_split, GridSearchCVfrom sklearn.datasets import fetch_california_housingfrom sklearn.metrics import mean_squared_errorimport xgboost as xgbfrom xgboost.sklearn import XGBClassifier pd = fetch_california_housing()# поскольку переменная y сильно скошена, # мы применяем логарифмическое преобразование y = np.log(pd.target)X_train, X_test, y_train, y_test = \\ train_test_split(pd.data, y, test_size=0.15, random_state=111)names = pd.feature_names print(names)\n--- Страница 226 ---\nАлгоритм XGBoost  225 clf = xgb.XGBRegressor(gamma=0, objective=\"reg:linear\", n_jobs=-1) clf.fit(X_train,y_train) y_pred = clf.predict(X_test) print('Отметка до сеточного поиска %r' % mean_squared_error(y_test, y_pred)) params = { 'max_depth' :[4,6,8], 'n_estimators' :[1000], 'min_child_weight' :range(1,3), 'learning_rate' :[.1,.01,.001], 'colsample_bytree' :[.8,.9,1], 'gamma' :[0,1]} # параметром nthread мы задаем параллелизацию алгоритма XGBoost cvx = xgb.XGBRegressor(objective=\"reg:linear\", n_jobs=-1)clf = GridSearchCV(estimator=cvx, param_grid=params, n_jobs=-1, scoring='mean_absolute_error', verbose=True) clf.fit(X_train,y_train) y_pred = clf.predict(X_test) print(clf.best_params_) print('Отметка после сеточного поиска %r' % mean_squared_error(y_test, y_pred)) # В зависимости от используемого оборудования # ваш результат может выглядеть немного иначе Out: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']Отметка до сеточного поиска 0.072204301145003813Fitting 3 folds for each of 108 candidates, totalling 324 fits[Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 6.1min (1.9min) [Parallel(n_jobs=-1)]: Done 192 tasks | elapsed: 29.4min (11.3min)[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed: 53.9min (22.3min) finished{'colsample_bytree': 0.9, 'learning_rate': 0.1, 'min_child_weight': 1, 'n_estimators': 1000, 'max_depth': 8, 'gamma': 0}Отметка после сеточного поиска 0.050217877735748678 При помощи сеточного поиска нам удалось порядком улучшить нашу отметку; мы видим оптимальные параметры сеточного поиска. Они аналогичны обычным методам бустинга в модуле sklearn. Однако метод экстремального градиентного бустинга XGBoost по умолчанию параллелизирует алгоритм по всем доступным ядрам. Можно улучшить производительность модели путем увеличения пара-метра n_estimators порядка до 2500 или 3000. Однако мы посчитали, что продол- жительность обучения будет излишне долгой для читателей с менее мощными компью терами. Алгоритм XGBoost и показатель важности признаков Алгоритм XGBoost имеет очень практичный встроенный функционал для построе - ния графика важности признаков. Прежде всего имеется удобный инструмент для\n--- Страница 227 ---\n226  Классификационные и регрес сионные деревья в крупном масштабе отбора признаков относительно текущей модели. Как вы, вероятно, знаете, пока- затель важности основывается на относительном влиянии каждого признака при создании дерева. Он предоставляет практические методы для отбора признаков и помогает глубже понять природу прогнозной модели. Поэтому давайте посмо-трим, каким образом можно вывести на график важность с алгоритмом XGBoost: import numpy as np import osfrom matplotlib import pylab as plt# %matplotlib inline <- это работает только в блокноте Jupyter # наш набор наилучших параметров# {'colsample_bytree': 1, 'learning_rate': 0.1, 'min_child_weight': 1,# 'n_estimators': 500, #'max_depth': 8, 'gamma': 0}params = {'objective': \"reg:linear\", 'eval_metric': 'rmse', 'eta': 0.1, 'max_depth':8, 'min_samples_leaf':4, 'subsample':.5, 'gamma':0} dm = xgb.DMatrix(X_train, label=y_train, feature_names=names) regbgb = xgb.train(params, dm, num_boost_round=100)np.random.seed(1)regbgb.get_fscore() regbgb.feature_names regbgb.get_fscore()xgb.plot_importance(regbgb,color='magenta',title='Калифорнийская недвижимость|важность признаков') Калифорнийская недвижимость|важность переменных Признаки F-отметка График построен средствами библиотеки xgboost\n--- Страница 228 ---\nАлгоритм XGBoost  227 Метрикой важности признаков следует пользоваться с некоторой осторож - ностью (то же касается алгоритмов GBM и случайного леса). Показатель важности признаков основан чисто на структуре деревьев, которая опирается на конкрет - ную модель, натренированную параметрами этой модели. Это означает, что если изменить параметры модели, то изменится и метрика важности, и часть ранжиро-вания. Поэтому следует отметить, что любая метрика важности не должна браться в качестве универсального заключения о признаке, обобщаемом на все модели. Потоковая передача больших наборов данных посредством XGBoost С точки зрения компромисса между точностью и производительностью экстре- мальный градиентный бустинг XGBoost является наилучшим настольным ре-шением. На примере со случайным лесом мы увидели, что для предотвращения перегрузки оперативной памяти приходилось прибегать к извлечению подвы-борок. Часто упускается из виду способность алгоритма XGBoost выступать как ме- тод потоковой передачи данных через память. Этот метод выполняет поэтапное преобразование проходящих через оперативную память данных, впоследствии передавая их в соответствующем виде в алгоритм XGBoost для тренировки мо-дели. Данный метод является необходимым предварительным этапом для тре-нировки моделей на больших наборах данных, которые невозможно разместить в оперативной памяти. Потоковая передача методом XGBoost работает только с файлами LIBSVM, т. е. сначала нужно преобразовать набор данных в формат LIBSVM и затем импортировать его в кэш-память, выделенную для алгоритма XGBoost. Еще стоит отметить, что мы используем разные методы инстанцирова-ния моделей XGBoost. Scikit-learn-подобный интерфейс для алгоритма XGBoost работает только на обычных объектах NumPy. Поэтому давайте посмотрим, как это работает. Прежде чем перейти к предобработке и тренировке, сначала нужно загрузить набор данных в формате LIBSVM и разбить его на тренировочный и тестовый на-боры. Тонкая настройка параметров при помощи сеточного поиска, к сожалению, с этим методом XGBoost не возможна. Если требуется выполнить доводку пара-метров, то необходимо преобразовать файл LIBSVM в объект Numpy, который вы-грузит данные из кэш-памяти в оперативную память. Такая операция, к сожале-нию, не масштабируема, поэтому, если вы хотите выполнять доводку параметров на больших наборах данных, рекомендуется использовать представленные ранее инструменты резервуарного отбора и выполнить настройку на подвыборках: import urllib import numpy as npfrom sklearn.datasets import dump_svmlight_file, load_svmlight_filefrom sklearn.metrics import classification_reportimport xgboost as xgb path = \"http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/\" trainfile = urllib.URLopener()trainfile.retrieve(path + \"poker.bz2\", \"pokertrain.bz2\")\n--- Страница 229 ---\n228  Классификационные и регре ссионные деревья в крупном масштабе X, y = load_svmlight_file('pokertrain.bz2') dump_svmlight_file(X, y, 'pokertrain', zero_based=True, query_id=None, multilabel=False)testfile = urllib.URLopener()testfile.retrieve(path + \"poker.t.bz2\", \"pokertest.bz2\")X, y = load_svmlight_file('pokertest.bz2')dump_svmlight_file(X, y, 'pokertest', zero_based=True, query_id=None, multilabel=False)del(X, y) dtrain = xgb.DMatrix('pokertrain#dtrain.cache') dtest = xgb.DMatrix('pokertest#dtestin.cache') param = {'max_depth':8, 'objective':'multi:softmax', 'nthread':2, 'num_class':10, 'verbose':True} num_round = 100 watchlist = [(dtest,'eval'), (dtrain,'train')]bst = xgb.train(param, dtrain, num_round, watchlist) print(bst)Out: [89] eval-merror:0.228659 train-merror:0.016913[90] eval-merror:0.228599 train-merror:0.015954[91] eval-merror:0.227671 train-merror:0.015354[92] eval-merror:0.227777 train-merror:0.014914[93] eval-merror:0.226247 train-merror:0.013355[94] eval-merror:0.225397 train-merror:0.012155[95] eval-merror:0.224070 train-merror:0.011875[96] eval-merror:0.222421 train-merror:0.010676[97] eval-merror:0.221881 train-merror:0.010116[98] eval-merror:0.221922 train-merror:0.009676[99] eval-merror:0.221733 train-merror:0.009316 За счет алгоритма XGBoost с обработкой в оперативной памяти можно полу - чить по-настоящему огромное ускорение. Потребовалось бы намного больше времени на тренировку, если бы использовалась версия на основе внутренней памяти. В этом примере мы уже включили тестовый набор в качестве провероч-ного раунда в список наблюдения watchlist. Однако если мы хотим предсказывать значения на ранее не встречавшихся данных, то можно просто использовать ту же процедуру прогнозирования, что и с любой другой моделью в библиотеке Scikit-learn и алгоритме XGBoost: bst.predict(dtest) Out: array([0., 0., 1., , 0., 0., 1.], dtype=float32) Персистентность модели XGBoost В предыдущей главе мы коснулись того, как сохранять модель на основе алгорит - ма GBM на диске, чтобы позже импортировать и использовать ее для прогнозиро- вания. Алгоритм XGBoost обеспечивает ту же самую функциональность. Посмот - рим, как мо жно сохранить и импортировать модель:\n--- Страница 230 ---\nВнеядерный алгоритм CART в среде H2O  229 import pickle bst.save_model('xgb.model') Теперь можно импортировать сохраненную модель из каталога, который вы указали ранее: imported_model = xgb.Booster(model_file='xgb.model') Отлично, теперь эту модель можно использовать для прогнозирования: imported_model.predict(dtest) Out: array([ 9., 9., 9., , 1., 1., 1.], dtype=float32) внеядерный алгОритм Cart в среде h2o До настоящего момента мы имели дело только с настольными решениями для моделей CART. В главе 4 « Нейронные сети и глубокое обучение» мы представили платформу H2O для глубокого обучения вне оперативной памяти, которая пре-доставила мощный масштабируемый метод. К счастью, платформа H2O также предо ставляет древовидные ансамблевые методы, используя для этого свою мощ- ную параллельную экосистему Hadoop. Поскольку в предыдущих разделах мы уже всесторонне рассматривали алгоритмы градиентного бустинга и случайного леса, то давайте сразу же приступим к делу. Для этого упражнения мы воспользуемся набором данных Spam, который уже применялся ранее. Случайный лес и сеточный поиск в H2O Давайте реализуем случайный лес с гиперпараметрической оптимизацией на основе сеточного поиска. В этом разделе мы сначала загрузим набор с данными о спаме из URL-ресурса: In: import osimport xlrdimport urllibimport tempfileimport numpy as npimport pandas as pdimport h2ofrom h2o.estimators.gbm import H2OGradientBoostingEstimatorfrom h2o.estimators.random_forest import H2ORandomForestEstimatorfrom h2o.grid.grid_search import H2OGridSearch url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data' temp_file = tempfile.NamedTemporaryFile().nameurllib.urlretrieve(url, temp_file) Out: ('c:\\\\users\\\\labor\\\\appdata\\\\local\\\\temp\\\\tmpcrk5gi', <httplib.HTTPMessage instance at 0x000000000CD88AC8>) Загрузив данные, теперь можно инициализировать сеанс H2O:\n--- Страница 231 ---\n230  Классификационные и регре ссионные деревья в крупном масштабе h2o.init(max_mem_size_GB = 2) Out: Checking whether there is an H2O instance running at http://localhost:54321. connected.H2O cluster uptime: 13 mins 05 secsH2O cluster version: 3.10.4.8H2O cluster version age: 28 days, 2 hours and 30 minutesH2O cluster name: H2O_from_python_labor_b52cfaH2O cluster total nodes: 1H2O cluster free memory: 1.292 GbH2O cluster total cores: 4H2O cluster allowed cores: 4H2O cluster status: locked, healthyH2O connection url: http://localhost:54321H2O connection proxy: NoneH2O internal security: FalsePython version: 2.7.13 final Ниже данные подвергаются предобработке, в результате которой они разбива- ются на тренировочный, перекрестно-проверочный и тестовый наборы. Это вы- полняется при помощи функции H2O split_frame . Также отметим важный шаг, где происходит преобразование целевого вектора C58 в факторную переменную: In: spamdata = h2o.import_file(path=temp_file)spamdata['C58'] = spamdata['C58'].asfactor() train, valid, test = spamdata.split_frame([0.6,.2], seed=1234)spam_X = spamdata.col_names[:-1] spam_Y = spamdata.col_names[-1] В следующей части мы зададим параметры, которые будем оптимизировать при помощи сеточного поиска. В первую очередь мы устанавливаем число де- ревьев в модели в единственное значение 300. Параметры, итеративный обход которых выполняется сеточным поиском, приведены ниже: max_depth : максимальная глубина дерева; balance_classes : каждая итерация использует сбалансированные классы для целевого исхода; sample_rate : доля строк, которые отбираются для каждой итерации. Теперь передадим эти параметры в список Python, который будет использо- ваться в нашей модели H2O с сеточным поиском: hyper_parameters={'ntrees':[300], 'max_depth':[3,6,10,12,50], 'balance_classes':['True','False'], 'sample_rate':[.5,.6,.8,.9]} grid_search = H2OGridSearch(H2ORandomForestEstimator, hyper_params=hyper_parameters)grid_search.train(x=spam_X, y=spam_Y, training_frame=train) print('Оптимальное решение для гиперпараметрического поиска') grid_search.show()\n--- Страница 232 ---\nВнеядерный алгоритм CART в среде H2O  231 Out: Оптимальное решение для гиперпараметрического поиска model_ids balance_classes max_depth ntrees sample_ratelogloss 0 Grid_DRF_py_10_sid_bfff_model_python_1497763697022_5193_model_38 true, 50, 300, 0.9 0.11861763801092395 1 Grid_DRF_py_10_sid_bfff_model_python_1497763697022_5193_model_28 true, 50, 300, 0.8 0.13727604944883123 2 Grid_DRF_py_10_sid_bfff_model_python_1497763697022_5193_model_8 true, 50, 300, 0.5 0.1453175326621316 3 Grid_DRF_py_10_sid_bfff_model_python_1497763697022_5193_model_18 true, 50, 300, 0.6 0.14584867383308084 4 Grid_DRF_py_10_sid_bfff_model_python_1497763697022_5193_model_19 false, 50, 300, 0.6 0.17548394969865572 … … … … 35 Grid_DRF_py_10_sid_bfff_model_python_1497763697022_5193_model_1 false, 3, 300, 0.5 0.33499715745920294 36 Grid_DRF_py_10_sid_bfff_model_python_1497763697022_5193_model_0 true, 3, 300, 0.5 0.35441515942802954 37 Grid_DRF_py_10_sid_bfff_model_python_1497763697022_5193_model_30 true, 3, 300, 0.9 0.3553639619687519 38 Grid_DRF_py_10_sid_bfff_model_python_1497763697022_5193_model_10 true, 3, 300, 0.6 0.3592075912922027 39 Grid_DRF_py_10_sid_bfff_model_python_1497763697022_5193_model_20 true, 3, 300, 0.8 0.3614194946048834 Из всех возможных сочетаний модель с долей отобранных строк .9, глубиной дерева 50 и сбалансированными классами приводит к самой высокой точности. Теперь натренируем новую модель случайного леса, используя оптимальные па-раметры, полученные в результате сеточного поиска, и предскажем результат на тестовом наборе: final = H2ORandomForestEstimator(ntrees=300, max_depth=50, balance_classes=True, sample_rate=.9)final.train(x=spam_X, y=spam_Y, training_frame=train) print(final.predict(test)) В результате прогнозирования в H2O будет сгенерирован массив, первый стол- бец которого содержит фактические предсказанные классы, а остальные столб- цы – вероятности классов каждой целевой метки: Out: predict p0 p1 1 0.00393672 0.996063 1 0 1 1 0.0804741 0.919526 1 0.0476355 0.952365 1 0.00146838 0.998532 1 0.0455365 0.954463 0 0.793245 0.206755 1 0.00726585 0.992734 1 0.071177 0.928823 1 0.0330345 0.966966 Стохастический градиентный бустинг и сеточный поиск в H2O В предыдущих примерах мы видели, что большую часть времени хорошо отстро-енная модель на основе алгоритма GBM превосходит по результативности слу - чайный лес. Поэтому теперь выполним алгоритм GBM с сеточным поиском в H2O и посмотрим, сможем ли мы улучшить нашу отметку. Для этого сеанса мы введем\n--- Страница 233 ---\n232  Классификационные и регре ссионные деревья в крупном масштабе аналогичный метод генерирования случайных подвыборок, который мы исполь- зовали для модели на основе случайного леса в H2O ( sample_rate ). Метод стохас- тическог о градиентного бу стинга был представлен на основе статьи 1999 г. Джерома Фридмана (Jerome Friedman) ( https://statweb.stanford.edu/~jhf/ftp/stobst. pdf). Добавленная в модель стохастичность во время каждой итерации по дереву использует случайный отбор наблюдений без возврата, что позволяет предотвра-тить переподгонку и увеличить общий показатель точности. В данном примере мы развиваем эту идею стохастичности дальше, вводя случайный отбор призна-ков во время каждой итерации. Подобного рода метод случайного отбора признаков также называется мето- дом слу чайных подпрос транств, который мы уже видели в разделе «Случайный лес и чрезвычайно рандомизированный лес» настоящей главы. Он реализуется пара-метром col_sample_rate . Таким образом, резюмируя все вышесказанное, в модели на основе алгоритма GBM мы выполним оптимизацию методом сеточного поиска со следующими параметрами: max_depth : максимальная глубина дерева; sample_rate : часть строк, используемая в каждой итерации; col_sample_rate : часть признаков, используемая в каждой итерации. Мы воспользуемся тем же набором с данными о спаме, что и в предыдущем разделе, поэтому сразу же приступаем к его обработке: # Отметим, что из-за случайности результаты сеточного поиска # могут отличаться от результатов, приводимых в книге. hyper_parameters = {'ntrees':[300], 'max_depth':[12,30,50], 'sample_rate':[.5,.7,1], 'col_sample_rate':[.9,1], 'learn_rate':[.01,.1,.3]} grid_search = H2OGridSearch(H2OGradientBoostingEstimator, hyper_params=hyper_parameters)grid_search.train(x=spam_X, y=spam_Y, training_frame=train) print('Отимальное решение для гиперпараметрического поиска %s' % grid_search.show()) model_ids col_sample_rate learn_rate max_depth ntrees sample_ratelogloss 0 Grid_GBM_py_10_sid_bfff_model_python_1497763697022_10537_model_34 0.9, 0.3, 50, 300, 0.7 0.00157330308618194 1 Grid_GBM_py_10_sid_bfff_model_python_1497763697022_10537_model_22 0.9, 0.3, 12, 300, 0.7 0.00157927687447262 2 Grid_GBM_py_10_sid_bfff_model_python_1497763697022_10537_model_29 1.0, 0.3, 30, 300, 0.7 0.0015832978610301 3 Grid_GBM_py_10_sid_bfff_model_python_1497763697022_10537_model_35 1.0, 0.3, 50, 300, 0.7 0.00159738892462822 4 Grid_GBM_py_10_sid_bfff_model_python_1497763697022_10537_model_28 0.9, 0.3, 30, 300, 0.7 0.00159886840370323 … … … … 49 Grid_GBM_py_10_sid_bfff_model_python_1497763697022_10537_model_13 1.0, 0.01, 50, 300, 0.5 0.104796736092932 50 Grid_GBM_py_10_sid_bfff_model_python_1497763697022_10537_model_12 0.9, 0.01, 50, 300, 0.5 0.10489031471896 51 Grid_GBM_py_10_sid_bfff_model_python_1497763697022_10537_model_18 0.9, 0.01, 12, 300, 0.7 0.105071198911258 52 Grid_GBM_py_10_sid_bfff_model_python_1497763697022_10537_model_1 1.0, 0.01, 12, 300, 0.5 0.115521269067435 53 Grid_GBM_py_10_sid_bfff_model_python_1497763697022_10537_model_0 0.9, 0.01, 12, 300, 0.5 0.117272332422832\n--- Страница 234 ---\nРезюме  233 Верхняя часть результата сеточного поиска показывает, что мы должны исполь- зовать исключительно высокий темп обучения .3, долю отбора столбцов .9 и мак - симальную глубину дерева 30. Случайный отбор наблюдений на основе строк не увеличил результативности, однако отбор на основе признаков с долей .9 в дан-ном случае был довольно эффективным. Теперь натренируем новую модель на основе алгоритма GBM с использованием оптимальных параметров, полученных в результате оптимизации методом сеточного поиска, и предскажем результат на тестовом наборе: spam_gbm2 = H2OGradientBoostingEstimator(ntrees=300, learn_rate=0.3, max_depth=30, sample_rate=1, col_sample_rate=0.9, score_each_iteration=True, seed=2000000) spam_gbm2.train(spam_X, spam_Y, training_frame=train, validation_frame=valid)confusion_matrix = spam_gbm2.confusion_matrix(metrics=\"accuracy\") print(confusion_matrix) Confusion Matrix (Act/Pred) for max accuracy @ threshold = 0.967610499121: 0 1 Error Rate 0 1683.0 0.0 0.0 (0.0/1683.0) 1 3.0 1077.0 0.0028 (3.0/1080.0) Total 1686.0 1077.0 0.0011 (3.0/2763.0) Полученный результат предоставляет интересные диагностические данные о результативности модели, в частности accuracy , rmse, logloss и AUC. Однако ко- нечный результат ее работы слишком объемный, чтобы его можно было здесь по- казать целиком. Все результаты в полном объеме можно посмотреть в блокноте Jupyter. Его можно использовать следующим образом: print(spam_gbm2.score_history()) Разумеется, итоговые предсказания можно получить следующим образом: print(spam_gbm2.predict(test)) Отлично, мы смогли улучшить точность нашей модели почти до 100%. Как вид- но, в платформе H2O предлагается меньшая гибкость с точки зрения моделиро-вания и подготовки данных, но достигаемые скорость обработки и точность не имеют равных. В заключение, чтобы завершить сеанс, можно сделать следующее: h2o.cluster().shutdown(prompt=False) резюме Мы увидели, что методы CART, натренированные ансамблевыми подпрограмма-ми, проявляют свою мощь, когда дело касается точности прогнозирования. Одна-ко они могут быть в вычислительном плане дорогостоящими, и мы раскрыли не-\n--- Страница 235 ---\n234  Классификационные и регре ссионные деревья в крупном масштабе которые методы их ускорения в приложениях из модуля sklearn. Мы обнаружили, что использование экстремально рандомизированных лесов, настроенных при помощи рандомизированного поиска, смогло ускорить их работу десятикратно при их соответствующем использовании. Тем не менее процедура параллелиза-ции метода GBM в модуле sklearn не реализована, и именно по этой причине на первый план выходит алгоритм экстремального градиентного бустинга XGBoost. Алгоритм экстремального градиентного бустинга XGBoost имеет встроенную возможность параллелизации, которая значительно его ускоряет. Когда мы ис - пользуем большие файлы (тренировочные примеры > 100k), на помощь приходит внеядерный метод, который гарантирует, что во время тренировки моделей мы не будем перегружать оперативную память. Самый большой прирост в скорости и преимущество от использования памяти можно получить благодаря платформе H2O; мы познакомились с ее мощными способностями по доводке параметров вместе с впечатляющей скоростью трени-ровки моделей.",
      "debug": {
        "start_page": 201,
        "end_page": 235
      }
    },
    {
      "name": "Глава 7. Обучение без учителя в крупном масштабе 235",
      "content": "--- Страница 236 --- (продолжение)\nГлава 7 Обучение без учителя в крупном масштабе В предыдущих главах в центре задачи было предсказание переменной, которая, возможно, является числом, классом или категорией. В этой главе мы изменим подход и попробуем создать новые признаки и переменные в крупном масштабе, которые для целей нашего предсказания, надо надеяться, окажутся лучше, чем те, которые уже включены в матрицу наблюдений. Сначала мы представим методы машинного обучения без учителя и проиллюстрируем три из них, которые спо-собны масштабироваться до больших данных: анализ главных компонент , эффективный способ снизить количество признаков; k-сре дних, масштабируемый алгоритм кластеризации данных; латентное размещение Дирихле , очень эффективный алгоритм, который способен извлекать темы из серии текстовых документов. метОды машиннОг О Обучения без учителя Обучение без учителя – это раздел машинного обучения, алгоритмы которого де-лают выводы из данных без явно заданной метки (немаркированных данных). Цель таких методов состоит в том, чтобы извлекать скрытые образы, или паттер-ны, и группировать похожие данные. В этих алгоритмах вызывающие интерес неизвестные параметры каждого наблю дения (к примеру, принадлежность к группе и тематическая структура) часто моделируются как латентные переменные (или как серия латентных пере-менных), скрытые в системе наблюдаемых переменных, которые не могут наблю-даться непосредственно, а лишь выводятся из прошлых и настоящих конечных результатов работы системы. Как правило, конечные результаты работы системы содержат шум, который затрудняет эту операцию. В типичных задачах методы машинного обучения без учителя используются в двух основных ситуациях: с маркированными наборами данных для извлечения дополнительных признаков, которые затем обрабатываются вниз по цепочке обработки классификатором/регрессором. Усиленные дополнительными признаками, эти методы могут оказаться более результативными; с маркированными или немаркированными наборами данных для извлече- ния некой информации о структуре данных. Этот класс алгоритмов обыч-\nГлава 7 Обучение без учителя в крупном масштабе В предыдущих главах в центре задачи было предсказание переменной, которая, возможно, является числом, классом или категорией. В этой главе мы изменим подход и попробуем создать новые признаки и переменные в крупном масштабе, которые для целей нашего предсказания, надо надеяться, окажутся лучше, чем те, которые уже включены в матрицу наблюдений. Сначала мы представим методы машинного обучения без учителя и проиллюстрируем три из них, которые спо-собны масштабироваться до больших данных: анализ главных компонент , эффективный способ снизить количество признаков; k-сре дних, масштабируемый алгоритм кластеризации данных; латентное размещение Дирихле , очень эффективный алгоритм, который способен извлекать темы из серии текстовых документов. метОды машиннОг О Обучения без учителя Обучение без учителя – это раздел машинного обучения, алгоритмы которого де-лают выводы из данных без явно заданной метки (немаркированных данных). Цель таких методов состоит в том, чтобы извлекать скрытые образы, или паттер-ны, и группировать похожие данные. В этих алгоритмах вызывающие интерес неизвестные параметры каждого наблю дения (к примеру, принадлежность к группе и тематическая структура) часто моделируются как латентные переменные (или как серия латентных пере-менных), скрытые в системе наблюдаемых переменных, которые не могут наблю-даться непосредственно, а лишь выводятся из прошлых и настоящих конечных результатов работы системы. Как правило, конечные результаты работы системы содержат шум, который затрудняет эту операцию. В типичных задачах методы машинного обучения без учителя используются в двух основных ситуациях: с маркированными наборами данных для извлечения дополнительных признаков, которые затем обрабатываются вниз по цепочке обработки классификатором/регрессором. Усиленные дополнительными признаками, эти методы могут оказаться более результативными; с маркированными или немаркированными наборами данных для извлече- ния некой информации о структуре данных. Этот класс алгоритмов обыч-\n--- Страница 237 ---\n236  Обучение без учит еля в крупном масштабе но используется во время фазы моделирования, именуемой разведочным анализом данных (e xploratory data analysis, EDA). В первую очередь, прежде чем приступить к иллюстрации, давайте импортиру - ем в наш блокнот Jupyter модули, которые будут необходимы по ходу настоящей главы: In: import osimport copyimport tempfileimport numpy as npimport pandas as pdimport matplotlib.cm as cm разлО жение признак Ов – PC a Анализ главных компонент (principal component analysis, PCA) – это алгоритм, который широко применяется для разложения размерностей входного сигнала и сохранения только главных из них. С математической точки зрения метод PCA выполняет ортогональное преобразование матрицы наблюдений, продуцируя набор линейных некоррелированных переменных, именуемых главными ком-понентами. Выходные переменные формируют базисный набор, каждая ком-понента которого ортонормирована относительно других. Кроме того, есть воз-можность ранжировать выходные компоненты (с тем чтобы использовать только главные), так как первая компонента содержит самую большую возможную дис - персию входного набора данных, вторая ортогональна первой (по определению) и содержит самую большую дисперсию остаточного сигнала, и третья ортогональ-на первым двум, и она опирается на остаточную дисперсию, и т. д. Универсальное преобразование при помощи алгоритма PCA может быть вы- ражено как проекция в пространство. Если из трансформационного базиса берут - ся только главные компоненты, то выходное пространство будет иметь меньшую размерность, чем входное. Математически это можно выразить следующим об-разом: Xˆ = X · T. Здесь X – это обобщенная точка тренировочного набора размерностью N, T – трансформационная матрица, поступающая из алгоритма PCA, и Xˆ – выходной вектор. Отметим, что в данном матричном уравнении символ «·» обозначает ска-лярное произведение. С практической точки зрения также стоит отметить, что, прежде чем выполнять эту операцию, все признаки X должны быть центрированы на нуле. Теперь давайте начнем с практического примера; позже мы подробно объяс - ним математический аппарат алгоритма PCA. В этом примере мы создадим фик - тивный набор данных, состоящий из двух скоплений точек – одно центрировано в (–5, 0), а другое – в (5, 5). Мы воспользуемся алгоритмом PCA, чтобы преобразо-вать набор данных и построить график конечных результатов в сравнении с вход-ными данными. В этом простом примере будут использованы все признаки, т. е. операция сокращения признаков выполняться не будет:\n--- Страница 238 ---\nРазложение признаков – PCA  237 In: from sklearn.datasets.samples_generator import make_blobsfrom sklearn.decomposition import PCA X, y = make_blobs(n_samples=1000, # скопления точек random_state=101, centers=[[-5, 0], [5, 5]])pca = PCA(n_components=2)X_pca = pca.fit_transform(X)pca_comp = pca.components_.T test_point = np.matrix([5, -2]) test_point_pca = pca.transform(test_point) plt.subplot(1, 2, 1) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='none')plt.quiver(0, 0, -pca_comp[:,0], -pca_comp[:,1], # -1 width=0.02, scale=5, color='orange')plt.plot(test_point[0, 0], test_point[0, 1], 'o')plt.title(u'Входной набор данных') plt.subplot(1, 2, 2) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolors='none')plt.plot(-test_point_pca[0, 0], -test_point_pca[0, 1], 'o') # -1plt.title(u'После обработки PCA \"без потерь\"')plt.show()  При выполнении приведенных в книге проекций PCA при помощи библиотеки Scikit-learn неко торые графики представляли собой зеркальное отражение графиков, приводимых в книге. Отметим, что это не следствие ошибки в одной из этих двух реализаций; причина такой разницы состоит в том, что в зависимости от алгоритма вычисления собственных век - торов и значений они могут иметь отрицательный либо положительный знак. Не то чтобы это имеет значение, но в случае необходимости можно просто обратить зеркальное отра-жение, умножив данные на –1, что и было сделано при переводе главы ради сохранения логики изложения.\n--- Страница 239 ---\n238  Обучение без учит еля в крупном масштабе Как видно, конечный результат более организован, чем пространство исходных признаков, и если бы следующей задачей была классификация, то потребовался бы всего один признак набора данных, экономя почти 50% необходимого простран-ства и вычисления. На рисунке отчетливо видно ядро PCA: это проекция входного набора данных на трансформационный базис, который на рисунке изобра жен сле- ва оранжевым цветом. Если вы в этом не уверены, давайте проверим: In: print('Синяя точка лежит в {0}'.format(test_point[0, :])) print('После трансформации лежит в {0}' .format(-test_point_pca[0, :])) # -1print('Поскольку (X-MEAN) * PCA_MATRIX = {0}' .format(np.dot(-test_point - -pca.mean_, pca_comp))) # -1 дважды Out: Синяя точка лежит в [[ 5 -2]] После трансформации лежит в [-2.34969911 -6.2575445 ] Поскольку (X-MEAN) * PCA_MATRIX = [[-2.34969911 -6.2575445 ]] Теперь проанализируем базовую проблему: каким образом можно сгенери- ровать T из тренировочного набора? Он должен содержать ортонормированные векторы, и векторы должны быть ранжированы согласно количеству дисперсии (т. е. энергии или информации в матрице наблюдений), которую они могут объ-яснить. На сегодняшний день было реализовано большое количество решений, но наиболее распространенная реализация основана на сингулярном разлож ении. Сингулярное разложение (singular value decomposition, SVD) – это метод, кото- рый разлагает любую матрицу М на три матрицы (U, Σ, W) с особыми свойствами, умножение которых снова дает M: M = U · Σ · WT. А именно если дана матрица M размера m строк и n столбцов, результирующие элементы эквивалентности следующие: U – это матрица размера m×n (квадратная матрица), она унитарная, и ее столбцы формируют ортонормированный базис. Они также называются ле-выми, или входными, сингулярными векторами и представляют собой соб-ственные векторы матричного произведения M · M T; Σ – это матрица размера m×n, которая имеет на своей диагонали только не- нулевые элементы. Эти значения называются сингулярными значениями, все они неотрицательные и являются собственными значениями как M · M T, так и MT · M; W – это унитарная матрица размера n×n (квадратная матрица), ее столбцы формируют ортонормированный базис, и они называются правыми (или выходными) сингулярными векторами. Кроме того, они являются собствен-ными векторами матричного произведения M T · M. Зачем это необходимо? Решение довольно простое: цель алгоритма PCA состо- ит в том, чтобы попытаться оценить направления, где дисперсия входного набора данных больше. Для этого сначала из каждого признака нужно удалить среднее значение и затем оперировать с ковариационной матрицей X T · X. С учетом этого, разложив матрицу X методом SVD, мы имеем столбцы матрицы W, которые являются главными компонентами ковариации (т. е. матрицы T, кото -\n--- Страница 240 ---\nРазложение признаков – PCA  239 рую мы ищем), диагональ матрицы Σ, которая содержит дисперсию, объясненную главными компонентами, и столбцы матрицы U – главные компоненты. Именно поэтому анализ главных компонент всегда выполняется при помощи сингуляр- ного разложения. Давайте теперь рассмотрим это на реальном примере. Протестируем метод на наборе данных Iris с извлечением первых двух главных компонент (т. е. перейдем от набора данных, состоящего из четырех признаков, к набору данных из двух признаков): In:from sklearn import datasets iris = datasets.load_iris() X = iris.datay = iris.target print('Набор данных Iris содержит {0} признаков' .format(X.shape[1])) pca = PCA(n_components=2) X_pca = pca.fit_transform(X) print('После обработки алгоритмом PCA он содержит {0} признаков' .format(X_pca.shape[1]))print('Дисперсия составляет [% от оригинала]: {0}' .format(sum(pca.explained_variance_ratio_))) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolors='none') plt.title(u'Первые 2 главные компоненты набора данных Iris')plt.show() Out: Набор данных Iris содержит 4 признакаПосле обработки алгоритмом PCA он содержит 2 признака(ов)Дисперсия составляет [% от оригинала]: 0.977631775025 Первые две главные компоненты набора данных Iris\n--- Страница 241 ---\n240  Обучение без учит еля в крупном масштабе Проведем анализ итоговых результатов процесса: объ ясненная дисперсия составляет почти 98% от исходной дисперсии из входных данных. Число признаков сократилось вдвое, но только 2% инфор- мации не находится в конечных результатах на выходе, и, надо надеяться, это просто шум; исх одя из визуального обследования, по всей видимости, разные классы, которые составляют набор данных Iris, друг от друга разделены. Это озна-чает, что классификатор, который оперирует таким сокращенным набо-ром, будет иметь сопоставимую результативность с точки зрения точности, и при этом тренировка и предсказание будут проходить быстрее. Как доказательство второго вывода давайте теперь попробуем натренировать и протестировать два классификатора, один из которых использует исходный на-бор данных, а другой – уменьшенный, и распечатаем их точность: In:from sklearn.linear_model import SGDClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_score def test_classification_accuracy(X_in, y_in): X_train, X_test, y_train, y_test = \\ train_test_split(X_in, y_in, random_state=101, train_size=0.50) clf = SGDClassifier('log', random_state=101) clf.fit(X_train, y_train) return accuracy_score(y_test, clf.predict(X_test)) print('Точность SGDClassifier на данных Iris: {0}' .format(test_classification_accuracy(X, y)))print('Точность после обработки алгоритмом PCA (2 компоненты): {0}' .format(test_classification_accuracy(X_pca, y))) Out: Точность SGDClassifier на данных Iris: 0.586666666667Точность после обработки алгоритмом PCA (2 компоненты): 0.72 Как видно, этот метод не только уменьшает сложность и пространство ученика по цепочке вниз, но и помогает достигнуть обобщения (в точности как гребневая либо лассо-регуляризация). Если вы не уверены в том, сколько компонент должно быть на выходе, обычно в качестве практического ориентира следует выбрать минимальное число, кото-рое в состоянии объяснить, по крайней мере, 90% (или 95%) входной дисперсии. Эмпирически такой выбор обычно гарантирует, что будет отсечен только шум. Пока все выглядит идеально: мы нашли отличное решение, которое сокраща- ет число признаков и создает некоторые из них с очень высокой прогнозирую-щей способностью, при этом мы также располагаем эмпирическим правилом для угадывания нужного их числа. Давайте теперь проверим, насколько это решение масштабируемо: мы обследуем масштабируемость решения при увеличении чис - ла наблюдений и признаков. Первое, что следует отметить, – алгоритм SVD, т. е. ядро алгоритма PCA, не является стохастическим, и поэтому ему нужна вся мат - рица, чтобы он смог извлечь ее главные компоненты. Теперь давайте посмотрим на масштабируемый алгоритм PCA в деле на нескольких синтетических наборах\n--- Страница 242 ---\nРазложение признаков – PCA  241 данных с возрастающим числом признаков и наблюдений. Мы выполним пол- ное разложение (без потерь) (аргумент при инстанцировании объекта PCA равен None), поскольку более низкое число признаков не влияет на результативность (это прос то вопрос нарезки вых одных матриц метода SVD). В приведенном ниже примере мы сначала создаем матрицы из 10 000 точек и с 20, 50, 100, 250, 1000 и 2500 признаками. Затем создаем матрицы со 100 при-знаками и 1, 5, 10, 25, 50 и 100 000 наблюдениями. Все они будут обработаны ал-горитмом PCA: In: import time def check_scalability(test_pca, plotnum=1): rcParams['figure.figsize'] = (8, 4) # ПРИЗНАКИ n_points = 10000 n_features = [20, 50, 100, 250, 500, 1000, 2500] time_results = [] for n_feature in n_features: X, _ = make_blobs(n_points, n_features=n_feature, random_state=101) pca = copy.deepcopy(test_pca) tik = time.time() pca.fit(X) time_results.append(time.time()-tik) plt.subplot(1, 2, 1) plt.plot(n_features, time_results, 'o--') plt.title(u'Масштабируемость признаков') plt.xlabel(u'Число признаков') plt.ylabel(u'Время обучения [сек]') # НАБЛЮДЕНИЯ n_features = 100 n_observations = [1000, 5000, 10000, 25000, 50000, 100000] time_results = [] for n_points in n_observations: X, _ = make_blobs(n_points, n_features=n_features, random_state=101) pca = copy.deepcopy(test_pca) tik = time.time() pca.fit(X) time_results.append(time.time()-tik) plt.subplot(1, 2, 2) plt.plot(n_observations, time_results, 'o--') plt.title(u'Масштабируемость наблюдений') plt.xlabel(u'Число тренировочных наблюдений') plt.ylabel(u'Тренировочное время [сек]') plt.show() rcParams['figure.figsize'] = (5, 4) check_scalability(PCA(None), 1)\n--- Страница 243 ---\n242  Обучение без учите ля в крупном масштабе Четко видно, что алгоритм PCA на основе метода SVD не масштабируется: если число признаков увеличивается линейно, то необходимое на тренировку алго- ритма время возрастает экспоненциально. Кроме того, время, требующееся на обработку матрицы с несколькими сотнями наблюдений, становится слишком продолжительным, и (на рисунке не показано) объем потребления оперативной памяти делает проблему невыполнимой для домашнего компьютера (с 16 Гб или меньшим количеством оперативной памяти). Становится очевидным, что алго-ритм PCA на основе метода SVD не подходит для больших данных; к счастью, в по-следние годы было предложено много обходных решений. В последующих разде-лах будет предложено краткое введение в каждое из них. Рандомизированный алгоритм PCA Правильное название этой методики – алгоритм PCA на основе рандомизированно- го метода SVD, но она стала популярна именно под названием рандомизирован- ного алгоритма PCA . В основе рандомизации лежит центральная идея избыточ- ности всех главных компонент; фактически, если целью метода является снижение размерности, можно ожидать, что на выходе будет нужно всего несколько векто-ров (K главных). Если сосредоточиться на проблеме нахождения наилучших K глав- ных векторов, то алгоритм будет масштабироваться лучше. Отметим, что в этом алгоритме K – число главных компонент на выходе – является ключевым парамет - ром: если задать его слишком большим, то результативность не будет лучше, чем у классического алгоритма PCA; а если задать слишком низким, то объясненная итоговыми векторами дисперсия тоже будет слишком низкой. Как и в классическом алгоритме PCA, мы хотим найти приближение матрицы, содержащее наблюдения X, такие что X ≈ Q · Q T · X; мы также хотим получить\n--- Страница 244 ---\nРазложение признаков – PCA  243 мат рицу Q с орт онормированными столбцами K (они будут называться главными компонентами). При помощи метода SVD мы теперь можем вычислить разложе- ние малой матрицы QT · X = U · Σ · WT. Как мы уже доказали, это не займет продол- жительное время. Поскольку X ≈ Q · QT · X = Q · U · Σ · WT, беря Q · U = S, мы теперь имеем усеченное приближение X, основанное на низкоранговом SVD, X ≈ S · Σ · WT. Математически это выглядит идеально, но по-прежнему не хватает ответа на два вопроса: какую роль здесь играет рандомизация и как получить матрицу Q? Ответы на оба вопроса приводятся далее: извлекается гауссова случайная матри- ца Ω, и вычисляется Y как Y = X · Ω. Затем Y подвергается QR-разложению, приводя к Y = Q · R, где получаем искомую матрицу Q из K ортонормированных столбцов. Математический аппарат в основе этого разложения довольно тяжел, но, к сча- стью, в библиотеке Scikit-learn весь уже реализован, и поэтому не нужно выяснять, как обращаться с гауссовыми случайными переменными и т. д. Давайте сперва посмотрим, насколько плохо рандомизированный алгоритм PCA работает, когда он вычисляет полное разложение (без потерь): In:check_scalability(PCA(svd_solver='randomized'), 2) Результативность хуже, чем у классического алгоритма PCA; на самом деле это преобразование работает очень хорошо, когда запрашивается уменьшенный на-бор компонент. Теперь посмотрим на результативность, когда K = 20: In:check_scalability(PCA(n_components=20, svd_solver='randomized'), 3)\n--- Страница 245 ---\n244  Обучение без учите ля в крупном масштабе Как и ожидалось, вычисления очень быстрые; алгоритм способен выполнить самые сложные разложения на факторы меньше чем за секунду. Проверяя результаты и алгоритм, мы по-прежнему замечаем нечто странное: для того чтобы тренировочный набор данных X можно было подвергнуть разло- жению, он целиком должен находиться в оперативной памяти, даже в случае ран- домизированного алгоритма PCA. Существует ли онлайновая версия алгоритма PCA, которая в состоянии инкрементно выполнять подгонку главных векторов без размещения всего набора данных в памяти? Да, такая версия существует. Это инкрементный алгоритм PCA. Инкрементный алгоритм PCA Инкрементный, или мини-пакетный, алгоритм PCA является онлайновой вер-сией анализа главных компонент. Ядро алгоритма очень простое: пакет данных первоначально разделяется на мини-пакеты с одинаковым числом наблюдений. (Единственное ограничение – число наблюдений в расчете на мини-пакет долж - но быть больше числа признаков.) Затем первый мини-пакет центрируется (уда-ляется среднее значение), и выполняется его SVD-разложение, сохраняя главные компоненты. Потом, когда в процесс вступает следующий мини-пакет, он сначала центрируется и затем накладывается вместе с главными компонентами, извле-ченными из предыдущего мини-пакета (они вставляются как дополнительные наблюдения). Теперь выполняется другое SVD-разложение, и главные компонен-ты перезаписываются новыми. Процесс идет до последнего мини-пакета: для каждого из них сначала выполняется центрирование, затем наложение, и нако-нец SVD-разложение. Поступая таким образом, вместо большого SVD-разложения мы выполняем столько малых SVD-разложений, сколько имеется мини-пакетов. Как можно понять, этот метод не превосходит по результативности рандомизи- рованного алгоритма PCA, но его цель состоит в том, чтобы предложить решение (или единственное решение), когда алгоритм PCA требуется на наборе данных, ко-торый не умещается в оперативной памяти. Инкрементный алгоритм PCA пред-назначен не для того, чтобы выиграть состязание на скорость, а чтобы ограничить потребление памяти; в течение тренировки использование памяти остается по-\n--- Страница 246 ---\nРазложение признаков – PCA  245 стоянным и может быть оптимизировано настройкой размера мини-пакета. Как показывает опыт, объем потребляемой памяти имеет приблизительно такой же порядок величины, что и квадрат размера мини-пакета. В качестве примера давайте теперь проверим, как инкрементный алгоритм PCA справляется с большим набором данных, который в целях иллюстрации состоит из 10 млн наблюдений и 100 признаков. Ни один из предыдущих алгоритмов не в состоянии с ним справиться, если вы не хотите повредить свой компьютер (либо стать свидетелем огромного файла подкачки между оперативной памятью и дис - ковым хранилищем). При помощи инкрементного алгоритма PCA выполнение такой задачи становится проще простого, и, учитывая все обстоятельства, про-цесс не такой уж и медленный (отметим, что мы выполняем полное разложение без потерь со стабильным объемом потребляемой памяти): In: from sklearn.decomposition import IncrementalPCA X, _ = make_blobs(100000, n_features=100, random_state=101) pca = IncrementalPCA(None, batch_size=1000) tik = time.time() for i in range(100): pca.partial_fit(X)print('PCA на 10M точках выполнялся с постоянным потреблением памяти {0} сек.' .format(time.time() - tik)) Out: PCA на 10M точках выполнялся с постоянным потреблением памяти 223.279000044 сек. Разреженный алгоритм PCA Разреженный алгоритм PCA действует по-другому, чем предыдущие алгоритмы; вместо того чтобы управлять сокращением признаков при помощи SVD-разло - жения, применяемого к ковариационной матрице (после центрирования), он вы- полняет операцию, похожую на отбор признаков этой матрицы, отыскивая на-бор разреженных компонент, которые лучше всего реконструируют данные. Как и в лассо-регуляризации, количество разреженности контролируется штрафом (либо ограничением), налагаемым на коэффициенты. По сравнению с классическим алгоритмом PCA, разреженный алгоритм PCA не гарантирует, что получающиеся компоненты будут ортогональными, но ре-зультат больше поддается толкованию, поскольку главные векторы в действи-тельности являются составной частью входного набора данных. Кроме того, он масштабируется с точки зрения числа признаков: если классический алгоритм PCA и его масштабируемые версии зависают, когда число признаков становится больше (скажем, более 1000), разреженный алгоритм PCA по-прежнему остается оптимальным решением с точки зрения скорости благодаря внутреннему мето-ду, решающему Lasso-задачу, который, как правило, основывается на алгоритме LARS 1 или методе координатного спуска. (Напомним, что Lasso2 пытается мини- 1 Метод наименьших углов (least angle regression, LARS) – алгоритм отбора признаков в задачах линейной регрессии. – Прим. перев. 2 Lasso (Least absolute shrinkage and selection operator) – метод оценивания коэффициен-т ов линейной регрессионной модели, который одновременно выполняет отбор призна- ков и регуляризацию. – Прим. перев.\n--- Страница 247 ---\n246  Обучение без учит еля в крупном масштабе мизировать L1-норму коэффициентов.) Кроме того, он великолепен, когда чис - ло признаков больше числа наблюдений, как, например, в некоторых наборах изобра жений. Давайт е теперь посмотрим, как он работает на наборе данных из 25 000 наблю- дений и 10 000 признаков. Для этого примера мы воспользуемся мини-пакетной версией алгоритма SparsePCA , гарантирующей постоянное использование памяти и могущей справиться с крупными наборами данных, которые в конечном счете больше имеющейся в распоряжении оперативной памяти (отметим, что пакетная версия называется SparsePCA , но она не поддерживает тренировку в онлайновом режиме): In:from sklearn.decomposition import MiniBatchSparsePCA X, _ = make_blobs(25000, n_features=10000, random_state=101)tik = time.time() pca = MiniBatchSparsePCA(20, method='cd', random_state=101, n_iter=1000)pca.fit(X)print('Разреженный алгоритм PCA на матрице {0} выполнялся {1} сек.' .format(X.shape, time.time() - tik)) Out: Разреженный алгоритм PCA на матрице (25000L, 10000L) выполнялся 134.220999956 сек. Алгоритм SparsePCA способен произвести решение примерно за 40 секунд при постоянном потреблении памяти. Алгоритм PCA в среде H2O Можно также воспользоваться реализацией алгоритма PCA, предлагаемой средой H2O. (Мы уже встречались с платформой H2O в предыдущей главе и не раз ее упо-минали на протяжении всей книги.) Работая в среде H2O, сначала при помощи метода init необходимо включить сервер. Затем выгрузить набор данных в файл (в CSV-файл, если быть точным) и наконец выполнить PCA-анализ. В качестве последнего шага мы закрываем сервер. Испытаем эту реализацию на самых больших наборах данных, которые мы встречали до сих пор, – с 100K наблюдениями и 100 признаками и с 10K наблюде-ниями и 2500 признаками: In: import h2ofrom h2o.transforms.decomposition import H2OPCA h2o.init(max_mem_size_GB=4)def testH2O_pca(nrows, ncols, k=20): temp_file = tempfile.NamedTemporaryFile().name X, _ = make_blobs(nrows, n_features=ncols, random_state=101) np.savetxt(temp_file, np.c_[X], delimiter=\",\") del X\n--- Страница 248 ---\nКластеризация – алгоритм K-средних  247 pca = H2OPCA(k=k, transform='NONE', pca_method='Power') tik = time.time() pca.train(x=range(100), training_frame=h2o.import_file(temp_file)) print('H2O PCA на матрице {0} выполнялся {1} сек.' .format((nrows, ncols), time.time() - tik)) os.remove(temp_file) testH2O_pca(100000, 100) testH2O_pca(10000, 2500)h2o.cluster().shutdown(prompt=False) Out: H2O PCA на матрице (100000, 100) выполнялся 40.0310001373 сек H2O PCA на матрице (10000, 2500) выполнялся 19.6699998379 сек. Как видно, в обоих случаях платформа H2O действительно работает очень быст - ро, вполне сопоставимо с библиотекой Scikit-learn (если вообще не превосходит его по производительности). кластеризация – алгОритм k-средних Алгоритм K-средних (K-means) – это алгоритм без учителя, который создает K не - пересекающихся кластеров точек с равной дисперсией, минимизируя искажение (так называемую инерцию). При наличии всего одного параметра K, представляющего число кластеров, ко- торые будут создаваться, алгоритм K-средних создает K множеств точек S 1, S2, …, Sk, каждое из которых представлено его центроидом: C1, C2, …, Ck. Обобщенный центроид Ci – это просто среднее значение из выборок точек, связанных с класте- ром Si, с целью минимизации внутрикластерного расстояния. Конечные результа- ты работы системы следующие. 1. Композиция кластеров S1, S2, …, Sk, т. е. множество точек, составляющее тре- нировочный набор, которые связаны с номером кластера 1, 2, …, K. 2. Центроиды каж дого кластера C1, C2, …, Ck. Центроиды могут использоваться для будущих ассоциаций. 3. Искажение, вносимое кластеризацией, которое вычисляется следующим об- разом: Данное уравнение обозначает оптимизацию, которая органически выполняет - ся в алгоритме K-средних: центроиды выбираются с целью минимизации внутри-кластерного искажения, т. е. суммы евклидовых норм расстояний между каждой входной точкой и центроидом кластера, с которым точка ассоциирована. Другими словами, алгоритм пытается подобрать наилучшее векторное квантование. Тренировочная фаза алгоритма K-средних также именуется алгоритмом Ллойда, названным в честь Стюарта Ллойда (Stuart Lloyd), который первым предложил этот алгоритм. Это итеративный алгоритм, состоящий из двух фаз,\n--- Страница 249 ---\n248  Обучение без учит еля в крупном масштабе многократно итерируемых до сходимости (искажение достигает минимума). Этот алгоритм является вариантом обобщенного алгоритма максимизации ожидания (expec tation-maximization, EM), так как первый шаг создает функцию для ожидания (E) о тметки, а шаг максимизации (M) вычис ляет параметры, которые максимизируют эту отметку. (Отметим, что в этой формулировке мы пытаемся достигнуть противоположного, т. е. минимизации искажения.) Вот его формула: шаг ожидания: на этом шаге точки в тренировочном наборе назначаются ближайшему центроиду: Этот шаг также называется присвоением, или векторным квантованием; шаг максимизации: центроид каждого кластера перемещается в середину кластера путем усреднения составляющих его точек: Этот шаг также называется шагом обновления. Эти два шага выполняются до сходимости (точки стабильны в своем кластере), либо пока алгоритм не достигнет предварительно установленного числа итера-ций. Отметим, что в соответствии с композицией искажение не может увеличить-ся на протяжении тренировочной фазы (в отличие от методов на основе стохас - тическог о градиентного спуска); следовательно, в этом алгоритме чем больше итераций, тем лучше результат. Теперь давайте посмотрим, как он выглядит на фиктивном двумерном набо- ре данных. Сначала мы создаем набор из 1000 точек, сконцентрированных в че-тырех расположениях, симметричных относительно источника. Каждый кластер в соответствии с конструкцией имеет одинаковую дисперсию: In:import numpy as npimport pandas as pdfrom sklearn.datasets.samples_generator import make_blobs centers = [[1, 1], [1, -1], [-1, -1], [-1, 1]] X, y = make_blobs(n_samples=1000, centers=centers, cluster_std=0.5, random_state=101) Теперь построим график набора данных. Чтобы упростить, мы окрасим класте- ры в разные цвета: In:plt.scatter(X[:,0], X[:,1], c=y, edgecolors='none', alpha=0.9)plt.show()\n--- Страница 250 ---\nКластеризация – алгоритм K-средних  249 Теперь выполним алгоритм K-средних и проинспектируем, что происходит на каждой итерации. Для этого мы будем останавливать итеративный процесс на 1, 2, 3 и 4-й итерациях и строить график с изображением точек со связанным с ними кластером (с цветной маркировкой), а также изображением центроида, искажения (в заголовке) и границы решения (так называемые ячейки Вороного). Первоначальный отбор центроидов выполняется случайным образом, т. е. четыре тренировочные точки – это отобранные центроиды в первой итерации во время тренировки в фазе ожидания: In:rcParams['figure.figsize'] = (8, 7)from sklearn.cluster import KMeans for n_iter in range(1, 5): cls = KMeans(n_clusters=4, max_iter=n_iter, n_init=1, init='random', random_state=101) cls.fit(X) # Вывести на графике ячейки Вороного plt.subplot(2, 2, n_iter) h=0.02 xx, yy = np.meshgrid(np.arange(-3, 3, h), np.arange(-3, 3, h)) Z = cls.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) plt.imshow(Z, interpolation='nearest', cmap=plt.cm.Accent, extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto', origin='lower') plt.scatter(X[:,0], X[:,1], c=cls.labels_, edgecolors='none', alpha=0.7) plt.scatter(cls.cluster_centers_[:,0], cls.cluster_centers_[:,1], marker='x', color='r', s=100, linewidths=4) plt.title(u'итер=%s, искажение=%s' % (n_iter, int(cls.inertia_))) plt.show() rcParams['figure.figsize'] = (5, 4)\n--- Страница 251 ---\n250  Обучение без учите ля в крупном масштабе Как видно из графиков, искажение становится все ниже и ниже по мере того, как число итераций увеличивается. Для этого фиктивного набора данных, по всей видимости, достаточно нескольких итераций (пяти итераций), чтобы достичь сходимости. Методы инициализации Нахождение глобального минимума искажения в алгоритме K-средних являет - ся задачей недетерминированной полиномиальной сложности (NP-hard); кроме того, точно так же, как и со стохастическим градиентным спуском, этот метод демонстрирует тенденцию сходиться к локальным минимумам, в особенности если число размерностей высокое. Во избежание такого поведения и ограниче-ния максимального количества итераций можно воспользоваться следующими контрмерами: многократно выпо лнить алгоритм, используя разные исходные условия. В библиотеке Scikit-learn класс KMeans имеет параметр n_init, который управляет тем, сколько раз алгоритм K-средних будет выполнен с разными первоначальными числами центроидов. В конце отбирается модель, кото-рая обеспечивает более низкое искажение. Если CPU имеет несколько ядер,\n--- Страница 252 ---\nКластеризация – алгоритм K-средних  251 то этот процесс может выполняться параллельно путем установки парамет - ра n_jobs в число требуемых заданий для запуска. Отметим, что потребле- ние памяти линейно зависит от числа параллельных заданий; от дать предпочтение инициализации методом K-средних++ (класс KMeans используется по умолчанию) случайному выбору тренировочных точек. В результате инициализации методом K-средних++ отбираются далеко от- стоящие друг от друга точки; это должно гарантировать, что центроиды смогут сформировать кластеры в универсальных подпространствах про-странства. Как было доказано, этот факт также гарантирует, что этот метод с большей вероятностью найдет наилучшее решение. Допущения алгоритма K-средних Алгоритм K-средних опирается на допущения, что каждый кластер имеет (гипер)сферическую форму, т. е. он не имеет удлиненной формы (подобно стреле), внут - ри вс е кластеры имеют одинаковую дисперсию, а их размер сопоставим (либо они отстоят очень далеко). Все эти допущения могут быть обеспечены при помощи мощного шага пред - обработки признаков; классический алгоритм PCA, ядерный алгоритм PCA, нор- мализация признаков и выборка могут быть неплохим началом. Давайте теперь посмотрим, что происходит, когда допущения, лежащие в осно- ве алгоритма K-средних, не удовлетворяются: In:pylab.rcParams['figure.figsize'] = (5.0, 10.0)from sklearn.datasets import make_moons # Продолговатые/удлиненные наборы X, _ = make_moons(n_samples=1000, noise=0.1, random_state=101)cls = KMeans(n_clusters=2, random_state=101)y_pred = cls.fit_predict(X) plt.subplot(3, 1, 1) plt.scatter(X[:, 0], X[:, 1], c=y_pred, edgecolors='none')plt.scatter(cls.cluster_centers_[:,0], cls.cluster_centers_[:,1], marker='x', color='r', s=100, linewidths=4)plt.title(\"Удлиненные кластеры\") # Разная дисперсия среди кластеров centers = [[-1, -1], [0, 0], [1, 1]]X, _ = make_blobs(n_samples=1000, cluster_std=[0.1, 0.4, 0.1], centers=centers, random_state=101)cls = KMeans(n_clusters=3, random_state=101)y_pred = cls.fit_predict(X) plt.subplot(3, 1, 2) plt.scatter(X[:, 0], X[:, 1], c=y_pred, edgecolors='none')plt.scatter(cls.cluster_centers_[:,0], cls.cluster_centers_[:,1], marker='x', color='r', s=100, linewidths=4)plt.title(\"Неодинаковая дисперсия между кластерами\") # Неравномерно-размерные скопления centers = [[-1, -1], [1, 1]]\n--- Страница 253 ---\n252  Обучение без учит еля в крупном масштабе centers.extend([[0,0]]*20) X, _ = make_blobs(n_samples=1000, centers=centers, cluster_std=0.28, random_state=101)cls = KMeans(n_clusters=3, random_state=101)y_pred = cls.fit_predict(X) plt.subplot(3, 1, 3) plt.scatter(X[:, 0], X[:, 1], c=y_pred, edgecolors='none')plt.scatter(cls.cluster_centers_[:,0], cls.cluster_centers_[:,1], marker='x', color='r', s=100, linewidths=4)plt.title(\"Неравномерно-размерные скопления\") plt.show() Во всех предыдущих примерах операция кластеризации не идеальна, произво- дя на выходе неверный и нестабильный результат. До настоящего момента мы приняли, что знаем наверняка, каково точное зна- чение K, т. е. число кластеров, которые мы ожидаем использовать в операции клас - теризации. На деле в реальных задачах это не всегда соответствует истине. Мы часто используем метод обучения без учителя для обнаружения глубинной струк - туры данных, включая число кластеров, составляющих набор данных. Давайте посмотрим, что происходит, когда мы пытаемся выполнить алгоритм K-средних с неправильным числом K, на простом фиктивном наборе данных; мы испытаем более низкое и более высокое значение K:\n--- Страница 254 ---\nКластеризация – алгоритм K-средних  253 In: rcParams['figure.figsize'] = (10, 4)X, _ = make_blobs(n_samples=1000, centers=3, random_state=101) for K in [2, 3, 4]: cls = KMeans(n_clusters=K, random_state=101) y_pred = cls.fit_predict(X) plt.subplot(1, 3, K-1) plt.title(u'K средних, K=%s' % K) plt.scatter(X[:, 0], X[:, 1], c=y_pred, edgecolors='none') plt.scatter(cls.cluster_centers_[:,0], cls.cluster_centers_[:,1], marker='x', color='r', s=100, linewidths=4) plt.show() rcParams['figure.figsize'] = (5, 4) Как видно, результаты являются в массовом порядке неправильными в случае, если правильное число K не угадано, даже для такого простого фиктивного набора данных. В следующем разделе мы объясним некоторые приемы того, как наилуч-шим образом подобрать значение числа K. Подбор оптимальной величины K Если допущения, лежащие в основе алгоритма K-средних, удовлетворены, то для обнаружения наилучшей величины числа K разработано несколько методов. Не- которые из них основываются на перекрестной проверке и метриках конечных результатов; они могут использоваться на всех методах кластеризации, но только когда имеются сведения, полученные в результате непосредственных наблюде-ний (ground truth) (это так называемые контролируемые метрики). Некоторые другие основываются на внутренне присущих параметрах алгоритма кластери-зации и могут использоваться независимо от присутствия или отсутствия непо-средственных наблюдений (это так называемые неконтролируемые метрики). К сожалению, ни один из них не гарантирует 100%-ную точность нахождения правильного результата.\n--- Страница 255 ---\n254  Обучение без учит еля в крупном масштабе Контролируемые метрики требуют непосредственных наблюдений (содержа- щих истинные ассоциации в наборах), и они обычно объединяются с анализом на основе сеточного поиска для понимания того, какая величина K будет опти- мальной. Некоторые из этих метрик вытекают из эквивалентных классификаци-онных метрик, но они позволяют иметь разное число неупорядоченных наборов в ка честве предсказанных меток. Первая метрика, которую мы собираемся рас - смотреть, называется однородностью. Как можно ожидать, она дает меру того, сколько из предсказанных кластеров содержат точки одного класса. Данная мера основана на энтропии и является кластерным эквивалентом прецизионности 1 в классификации. Значения этой метрики лежат между 0 (худший) и 1 (лучший), а ее математическая формула следующая: Здесь H(C|K) – это условная энтропия распределения классов при наличии пред- ложенного назначения кластеров, и H(C) – энтропия классов. H(C|K) максимальна и равна H(C), когда кластеризация не предоставляет новой информации, и равна нулю, когда каждый кластер содержит член только одного класса. С ней связана, как в случае прецизионности и полноты для классификации, от - метка полноты (completeness): она дает меру того, сколько всех членов класса назначено одному и тому же кластеру. И данная метрика тоже ограничена зна-чениями между 0 (худший) и 1 (лучший), а ее математическая формула прочно основана на энтропии: Здесь H(K|C) – это условная энтропия предложенного распределения кластеров при наличии класса, и H(K) – энтропия кластеров. Наконец, эквивалентная отметке F1 для задачи классификации V-мера – это среднее гармоническое однородности и полноты: Давайте вернемся к первому набору данных (четыре симметричных шумных кластера) и попытаемся увидеть, как эти отметки действуют и способны ли они выделить оптимальную величину K для использования: In:from sklearn.metrics import homogeneity_completeness_v_measure centers = [[1, 1], [1, -1], [-1, -1], [-1, 1]] X, y = make_blobs(n_samples=1000, centers=centers, cluster_std=0.5, random_state=101) Ks = range(2, 10) 1 Прецизионность (англ. precision – точность измерений) – степень близости друг к другу независимых результатов измерений, полученных в конкретных установленных усло- виях. – Прим. перев.\n--- Страница 256 ---\nКластеризация – алгоритм K-средних  255 HCVs = [] for K in Ks: y_pred = KMeans(n_clusters=K, random_state=101).fit_predict(X) HCVs.append(homogeneity_completeness_v_measure(y, y_pred)) plt.plot(Ks, [el[0] for el in HCVs], 'r', label=u'Однородность') plt.plot(Ks, [el[1] for el in HCVs], 'g', label=u'Полнота')plt.plot(Ks, [el[2] for el in HCVs], 'b--', label=u'V-мера')plt.ylim([0, 1])plt.xlabel(u'Величина K')plt.legend(loc=4)plt.show() На графике первоначально (K < 4) полнота высокая, но однородность низкая; для K > 4 все наоборот: однородность высокая, но полнота низкая. В обоих случаях V-мера является низкой. Для K = 4, напротив, вся мера достигает их максимума, указывая, что это наилучшая величина K, т. е. число кластеров. За пределами этих контролируемых метрик существуют другие так называе- мые неконтролируемые, которые не требуют непосредственных наблюдений, а просто основаны на самом ученике. Первая метрика, которую мы рассмотрим в этом разделе, называется методом локтя и применяется к искажению. Она очень простая и не требует никакой ма- тематики: просто необходимо построить график искажения нескольких моделей K-средних с разными значениями K, затем выбрать ту, в которой увеличение K не вносит в решение намного более низкого искажения. На Python это реализуется очень просто: In:Ks = range(2, 10)Ds = []for K in Ks: cls = KMeans(n_clusters=K, random_state=101)\n--- Страница 257 ---\n256  Обучение без учите ля в крупном масштабе cls.fit(X) Ds.append(cls.inertia_) plt.plot(Ks, Ds, 'o-') plt.xlabel(u'Величина K')plt.ylabel(u'Искажение')plt.show() Как можно ожидать, искажение резко падает вплоть до K = 4 и затем уменьша- ется медленно. Здесь оптимальная величина K равна 4. Еще одна неконтролируемая метрика, которую мы собираемся рассмотреть, – это силуэтная отметка. Данная метрика более сложная, но и более мощная, чем предыдущая эвристика. На очень высоком уровне она измеряет то, насколько близко (похоже) наблюдение назначенному кластеру и насколько неточно (непо-хоже) оно совпадает с данными соседних кластеров. Силуэтная отметка 1 указыва-ет, что все данные находятся в наилучшем кластере, и –1 указывает на абсолютно неправильный результат кластеризации. Python’овский программный код для по-лучения этой меры очень прост благодаря ее реализации в библиотеке Scikit-learn: In:from sklearn.metrics import silhouette_score Ks = range(2, 10) Ds = []for K in Ks: cls = KMeans(n_clusters=K, random_state=101) Ds.append(silhouette_score(X, cls.fit_predict(X))) plt.plot(Ks, Ds, 'o-') plt.xlabel(u'Величина K')plt.ylabel(u'Силуэтная отметка')plt.show()\n--- Страница 258 ---\nКластеризация – алгоритм K-средних  257 И в этом случае мы тоже пришли к тому же самому выводу: оптимальное значе- ние для K равняется 4, так как силуэтная отметка намного ниже при более низком и более высоком значениях K. Масштабирование алгоритма K-средних – мини-пакет Теперь давайте протестируем масштабируемость алгоритма K-средних. На веб- сайте UCI мы отобрали подходящий для этой задачи набор данных: данные пе - реписи насе ления США 1990 года. Этот набор данных содержит почти 2.5 млн наблюдений и 68 категориальных (кодированных числами) атрибутов. Пропу - щенные данные отсутствуют, и файл находится в формате CSV. Каждое наблю-дение содержит идентификатор человека (который должен быть удален перед кластеризацией) и другую информацию о половой принадлежности, доходе, се-мейном положении, работе и т. д.  Дополнительную информацию о наборе данных можно найти на http://archiv e.ics.uci.edu/ ml/datasets/US+Census+Data+%281990%29 либо в работе, опубликованной Миком, Тис - соном и Хекерманом (Meek, Thiesson, Heckerman) в сборнике Journal of Machine Learning Research (2001), под названием The Learning Curve Method Applied to Clustering («Метод кри- вой обучения применительно к кластеризации»). Первое, что нужно сделать, – скачать файл, содержащий набор данных, и сохра- нить его во временном каталоге. Отметим, что он имеет объем 345 Мб, и поэтому на низкоскоростных соединениях его скачивание может занять продолжительное время: In: import urllibimport os.path url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/census1990-mld/USCensus1990. data.txt'\n--- Страница 259 ---\n258  Обучение без учит еля в крупном масштабе census_csv_file = './USCensus1990.data.txt' if not os.path.exists(census_csv_file): testfile = urllib.URLopener() testfile.retrieve(url, census_csv_file)if not os.path.exists(census_csv_file): testfile = urllib.URLopener() testfile.retrieve(url, census_csv_file) Теперь давайте выполним несколько тестов, замеряя время, требующееся на тренировку ученика K-средних с величиной K, равной 4, 8 и 12, и с набором дан- ных, содержащим 20K, 200K и 0.5M наблюдений. Поскольку мы не хотим пере-гружать оперативную память машины, поэтому мы прочтем первые 500K строк и отбросим столбец, содержащий идентификатор пользователя. В заключение построим график времен тренировки, чтобы выполнить полную оценку резуль-тативности 1: In:piece_of_dataset = \\ pd.read_csv(census_csv_file, iterator=True) .get_chunk(500000).drop('caseid', axis=1).as_matrix() time_results = {4: [], 8:[], 12:[]} dataset_sizes = [20000, 200000, 500000] for dataset_size in dataset_sizes: print('Размер набора данных: {0}'.format(dataset_size)) X = piece_of_dataset[:dataset_size,:] for K in [4, 8, 12]: print('K: {0}'.format(K)) cls = KMeans(K, random_state=101) timeit = %timeit -o -n1 -r1 cls.fit(X) time_results[K].append(timeit.best) plt.plot(dataset_sizes, time_results[4], 'r', label='K=4') plt.plot(dataset_sizes, time_results[8], 'g', label='K=8')plt.plot(dataset_sizes, time_results[12], 'b', label='K=12') plt.xlabel(u'Размер тренировочного набора') plt.ylabel(u'Время тренировки')plt.legend(loc=0)plt.show() Out: Размер набора данных: 20000K: 41 loop, best of 1: 1.16 s per loop (478 ms)K: 81 loop, best of 1: 2.38 s per loop (1.22 s)K: 12 1 В связи с тем, что прилагаемый к книге программный код тестировался на ноутбуке дос таточно скромной производительности Asus Intel Core i3 CPU @ 1.8 GHz 6 Гб, в не- скольких местах в переводе среди результатов работы фрагментов программного кода в скобках показаны временные отметки, приводимые в оригинале книги. – Прим. перев.\n--- Страница 260 ---\nКластеризация – алгоритм K-средних  259 1 loop, best of 1: 3.72 s per loop (1.76 s) Размер набора данных: 200000K: 41 loop, best of 1: 11.7 s per loop (6.35 s)K: 81 loop, best of 1: 19.7 s per loop (10.5 s)K: 121 loop, best of 1: 30.7 s per loop (17.7 s)Размер набора данных: 500000K: 41 loop, best of 1: 27 s per loop (13.4 s)K: 81 loop, best of 1: 1min 8s per loop (48.6 s)K: 121 loop, best of 1: 1min 39s per loop (1min 5s) Представляется очевидным, что с учетом графика и фактического хронометра- жа время тренировки увеличивается линейно вместе с K и размером тренировоч- ного набора, но для крупных значений K и размеров тренировочного набора такая связь становится нелинейной. Исчерпывающий поиск с полным тренировочным набором для многих значений K не выглядит как масштабируемая задача. К счастью, онлайновая версия алгоритма K-средних на основе мини-пакетов в Scikit-learn уже реализована и называется MiniBatchKMeans . Давайте испытаем ее на самом медленном случае из предыдущего графика, т. е. с K = 12. Тренировка классического алгоритма K-средних на 500 000 выборках (примерно 20% от пол-ного набора данных) заняла больше минуты; теперь посмотрим на производи-тельность онлайновой мини-пакетной версии, установив размер пакета в 1000 и импортировав из набора данных порции в объеме 50 000 наблюдений. В ка - честве к онечного результата мы построим график времени тренировки против числа порций, уже пропущенных через тренировочную фазу:\n--- Страница 261 ---\n260  Обучение без учите ля в крупном масштабе In: import timefrom sklearn.cluster import MiniBatchKMeans cls = MiniBatchKMeans(12, batch_size=1000, random_state=101) ts = [] tik = time.time() for chunk in pd.read_csv(census_csv_file, chunksize=50000): cls.partial_fit(chunk.drop('caseid', axis=1)) ts.append(time.time()-tik) plt.plot(range(len(ts)), ts) plt.xlabel(u'Тренировочные пакеты')plt.ylabel(u'Время [сек]')plt.show() Время тренировки линейно для каждой порции, причем кластеризация выпол- няется на полном наборе данных из 2.5 млн наблюдений почти за 20 секунд. При такой реализации мы сможем выполнить полный поиск, чтобы выбрать опти-мальную величину K, воспользовавшись для этого методом локтя на искажении. Давайте выполним исчерпывающий поиск по списку значений K от 4 до 12 и по- строим график искажения: In:Ks = list(range(4, 13))ds = [] for K in Ks: print('K={0}'.format(K)) cls = MiniBatchKMeans(K, batch_size=1000, random_state=101) for chunk in pd.read_csv(census_csv_file, chunksize=50000):\n--- Страница 262 ---\nАлгоритм K-средних в среде H2O  261 cls.partial_fit(chunk.drop('caseid', axis=1)) ds.append(cls.inertia_) plt.plot(Ks, ds) plt.xlabel(u'Величина K')plt.ylabel(u'Искажение')plt.show() Из графика видно, что локоть, похоже, соответствует K = 8. Помимо этого значе- ния, хотелось бы отметить, что благодаря пакетной реализации нам удалось вы-полнить массивную операцию на большом наборе данных менее чем пару минут; поэтому, если набор данных становится большим, то стоит отказаться от исполь-зования простого классического алгоритма K-средних. алгОритм k-средних в среде h2o В этом разделе мы сравним реализацию алгоритма K-средних в среде H2O с его реализацией в библиотеке Scikit-learn. Если быть более точным, мы выполним мини-пакетный эксперимент с использованием имеющегося в среде H2O объекта H2OKMeansEstimator для алгоритма K-средних. Его настройка аналогична той, кото- рая была показана в разделе «Алгоритм PCA в среде H2O», и данный эксперимент полностью совпадает с тем, который рассматривался в предыдущем разделе: In:import h2ofrom h2o.estimators.kmeans import H2OKMeansEstimatorh2o.init(max_mem_size_GB=4) def testH2O_kmeans(X, k): temp_file = tempfile.NamedTemporaryFile().name np.savetxt(temp_file, np.c_[X], delimiter=\",\")\n--- Страница 263 ---\n262  Обучение без учите ля в крупном масштабе cls = H2OKMeansEstimator(k=k, standardize=True) blobdata = h2o.import_file(temp_file) tik = time.time() cls.train(x=range(blobdata.ncol), training_frame=blobdata) fit_time = time.time() - tik os.remove(temp_file) return fit_timepiece_of_dataset = \\ pd.read_csv(census_csv_file, iterator=True).get_chunk(500000).drop('caseid', axis=1).as_matrix()time_results = {4: [], 8:[], 12:[]}dataset_sizes = [20000, 200000, 500000] for dataset_size in dataset_sizes: print('Размер набора данных: {0}'.format(dataset_size)) X = piece_of_dataset[:dataset_size,:] for K in [4, 8, 12]: print('K: {0}'.format(K)) fit_time = testH2O_kmeans(X, K) time_results[K].append(fit_time) plt.plot(dataset_sizes, time_results[4], 'r', label='K=4') plt.plot(dataset_sizes, time_results[8], 'g', label='K=8')plt.plot(dataset_sizes, time_results[12], 'b', label='K=12') plt.xlabel(u'Размер тренировочного набора') plt.ylabel(u'Время тренировки')plt.legend(loc=0)plt.show() h2o.cluster().shutdown(prompt=False)\n--- Страница 264 ---\nАлгоритм LDA  263 Благодаря архитектуре платформы H2O ее реализация алгоритма K-средних очень быстра, масштабируема и способна выполнять кластеризацию наборов то- чечных данных объемом 500K менее чем за 30 секунд для всех отобранных зна-чений K. алгОритм lDa Алгоритм LDA – это одна из наиболее широко распространенных методик, кото-рая применяется для анализа корпусов текстовых документов. Аббревиатура LDA означает латентное размещение Дирихле (latent Dirichlet allocation).  Аббревиатура LD A также используется для еще одного метода – линейного дискриминант - ного анализа (linear discriminant analysis), являющегося методом машинного обучения с учителем, предназначенным для задач классификации. Следует быть внимательным с тем, как данная аббревиатура используется, поскольку между этими двумя алгоритмами нет ни-какой связи. Полное математическое объяснение алгоритма LDA потребовало бы знания вероятностного моделирования, которое выходит за рамки этой практической книги. Здесь же вместо этого мы предоставим наиболее важные интуитивные со-ображения, которые лежат в основе модели, и советы по ее практическому при-менению на крупном наборе данных. Прежде всего латентное размещение Дирихле используется в разделе науки о данных под названием «глубинный анализ текстов» (text mining), где основное внимание уделяется созданию учеников для понимания естественного языка, на-пример на основе текстовых примеров. Если быть точным, алгоритм LDA при-надлежит к категории алгоритмов тематического моделирования, поскольку он пытается смоделировать темы, входящие в состав документа. В идеальном случае алгоритм LDA в состоянии понять, посвящен ли документ, к примеру, финансам, политике или религии. Однако, в отличие от классификатора, он также в состоя-нии оценивать присутствие тем в документе количественно. Например, возьмем роман Роулинг о Гарри Поттере. Классификатор смог бы идентифицировать его категорию (роман в стиле фэнтези); алгоритм LDA со своей стороны в состоянии понять, в каком количестве там присутствуют комедия, драма, мистерия, роман и приключение. Кроме того, алгоритм LDA не требует никаких меток; это метод машинного обучения без учителя, который изнутри строит выходные категории или тему и ее структуру (т. е. заданную набором слов, составляющих тему). Во время обработки алгоритм LDA создает модель с темами в каждом документе и словами в каждой теме, моделируемыми как распределения Дирихле. Несмотря на то что его вычислительная сложность высокая, время обработки, необходимое для выработки стабильных результатов, не слишком продолжительное благодаря итеративной ядерной функции, похожей на метод Монте-Карло. Модель LDA нетрудно понять: каждый документ моделируется как распределе- ние тем, и каждая тема моделируется как распределение слов. В качестве распре-делений вероятностей приняты априорные распределения Дирихле (с разными параметрами, поскольку число слов в расчете на тему обычно разное, в отличие от числа тем на документ). Благодаря методу сэмплирования по Гиббсу каждое рас - пределение не приходится отбирать непосредственно, а вместо этого итеративно\n--- Страница 265 ---\n264  Обучение без учит еля в крупном масштабе рассчитывается его точная аппроксимация. Аналогичные результаты могут быть получены при помощи вариационного байесовского метода, где аппроксимация генерируется вследствие подхода на основе максимизации ожидания. Результирующая модель LDA генеративна (как в случае со скрытыми марков- скими моделями, наивным Байесом и ограниченными машинами Больцмана), поэтому каждая переменная может моделироваться и наблюдаться. Теперь давайте посмотрим, как этот алгоритм работает на реальном наборе данных – наборе данных о 20 группах новостей (20 Newsgroups). Он состоит из набора электронных сообщений, передававшихся в 20 группах новостей. Прежде всего мы его загрузим, удалив из отправленных электронных писем заголовки с адресом электронной почты, нижние колонтитулы и кавычки: In: from sklearn.datasets import fetch_20newsgroups documents = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'), random_state=101).data Проверим размер набора данных (т. е. количество документов) и распечатаем один из них, чтобы увидеть, из чего документ состоит в действительности: In:len(documents) Out: 11314 In: document_num = 9960print documents[document_num] Out: Help!!! I have an ADB graphicsd tablet which I want to connect to my Quadra 950. Unfortunately, the 950 has only one ADB port andit seems I would have to give up my mouse. Please, can someone help me? I want to use the tablet as well as the mouse (and the keyboard of course!!!). Thanks in advance. Согласно примеру, некто ищет помощь по поводу видеосокета на своем план- шете. Теперь мы выполним импорт библиотек Python, необходимых для выполнения модели LDA. Библиотека Gensim – одна из лучших и, как вы убедитесь в конце на- стоящего раздела, тоже отлично масштабируется: In: import gensimimport nltkfrom gensim.utils import simple_preprocessfrom gensim.parsing.preprocessing import STOPWORDSfrom nltk.stem import WordNetLemmatizer, SnowballStemmer np.random.seed(101)\n--- Страница 266 ---\nАлгоритм LDA  265 # появится окно загрузчика, где выбрать Corpora -> Wordnet -> Download nltk.download()  В приведенном выше примере использована естественно-языковая библиотека NLTK, по-с тавляемая с большим корпусом текстовых данных, миниатюрных грамматик, натрениро- ванных моделей и т. д., которая устанавливается стандартным образом менеджером биб - лиот ек pip. Полный их перечень размещен на http://nltk.org/nltk_data/. Чтобы установить эти данные, можно воспользоваться загрузчиком NLTK, как описано ниже.Помимо отдельных пакетов данных, можно скачать всю коллекцию (используя параметр «all») либо только данные, которые требуются для примеров и упражнений из книги по NLTK, либо только корпус документов без грамматик и натренированных моделей (исполь-зуя параметр «all-corpora»). В данном случае нужно в окне загрузчика NL TK Downloader в разделе Corpora выбрать из списка Wordnet и нажать Download. В качестве первого этапа мы должны очистить текст. Для этого необходимо вы- полнить несколько шагов, которые типичны для любой естественно-языковой об-работки текста. 1. Лекс емизация, т. е. когда текст разбивается на предложения, а предложения разбиваются на слова. В заключение слова переводятся в строчные буквы. В этом месте удаляется пунктуация (и диакритические знаки). 2. Удаление слов, состоящих менее чем из трех символов. (На этом шаге удаля- ется большинство акронимов, эмограмм и союзов.) 3. Удаление слов из списка английских стоп-слов. Слова в этом списке очень распространены и не имеют никакой прогнозирующей способности (такие как the, an, so, then, have и т. д.). 4. Лекс емы затем лемматизируются; слова в третьем лице переводятся в пер- вое лицо, глаголы в прошедшем и будущем временах переводятся в настоя-щее (например, goes, went и gone становятся go).\n--- Страница 267 ---\n266  Обучение без учит еля в крупном масштабе 5. Наконец, стемминг, или процедура выделения основы слова, удаляет слово- изменение, сводя слово к его корню (например, shoes становится shoe). В следующем фрагменте программного кода мы как раз это и сделаем: попыта- емся очистить текст настолько, насколько это возможно, и выведем список слов, которые его составляют. В конце мы увидим, как эта работа изменит приведен-ный выше документ: In: lm = WordNetLemmatizer()stemmer = SnowballStemmer('english') def lem_stem(text): return stemmer.stem(lm.lemmatize(text, pos='v')) def tokenize_lemmatize(text): return [lem_stem(token) for token in gensim.utils.simple_preprocess(text) if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3] print(tokenize_lemmatize(documents[document_num]))Out: [u'help', u'graphicsd', u'tablet', u'want', u'connect', u'quadra', u'unfortun', u'port', u'mous', u'help', u'want', u'tablet', u'mous', u'keyboard', u'cours', u'thank', u'advanc'] Сейчас в качестве следующего этапа давайте выполним шаги по очистке всех документов. После этого нам нужно создать словарь, содержащий количества вхождений слов в тренировочный набор. Благодаря библиотеке Gensim эта работа достаточно прямолинейна: In: processed_docs = [tokenize(doc) for doc in documents]word_count_dict = gensim.corpora.Dictionary(processed_docs) Теперь, поскольку мы хотим создать универсальное и быстрое решение, да- вайте удалим все очень редкие и очень общие слова. Например, мы можем от - фильтровать все слова, которые встречаются меньше 20 раз (всего) и не более, чем в 20% документов: In: word_count_dict.filter_extremes(no_below=20, no_above=0.2) На следующем этапе, располагая таким сокращенным набором слов, мы созда- дим модель мешка слов для каждого документа, т. е. по каждому документу мы создаем словарь, который предоставляет сведения о том, сколько слов использу - ется и сколько раз они встречаются в документе: In: bag_of_words_corpus = [word_count_dict.doc2bow(pdoc) for pdoc in processed_docs] В качестве примера давайте взглянем на модель мешка слов для предыдущего документа: In:bow_doc1 = bag_of_words_corpus[document_num]\n--- Страница 268 ---\nАлгоритм LDA  267 for i in range(len(bow_doc1)): print('Слово {0} (\"{1}\") появляется {2} раз[а]' .format(bow_doc1[i][0], word_count_dict[bow_doc1[i][0]], bow_doc1[i][1])) Out: Слово 179 (\"want\") появляется 2 раз[а]Слово 250 (\"keyboard\") появляется 1 раз[а]Слово 832 (\"unfortun\") появляется 1 раз[а]Слово 1036 (\"port\") появляется 1 раз[а]Слово 1142 (\"help\") появляется 2 раз[а]Слово 1542 (\"quadra\") появляется 1 раз[а]Слово 2001 (\"advanc\") появляется 1 раз[а]Слово 2121 (\"cours\") появляется 1 раз[а]Слово 2391 (\"thank\") появляется 1 раз[а]Слово 2898 (\"mous\") появляется 2 раз[а]Слово 3313 (\"connect\") появляется 1 раз[а] Сейчас мы подошли к базовой части алгоритма: выполнению модели LDA. Что касается нашего решения, давайте запросим 12 тем (имеются 20 разных новост - ных рассылок, но некоторые из них похожие): In:# Алгоритм LDA в моноядерном режиме lda_model = gensim.models.LdaModel(bag_of_words_corpus, num_topics=12, id2word=word_count_dict, iterations=200, passes=50) # Алгоритм LDA в мультиядерном режиме # (в этой конфигурации по умолчанию n_cores-1) '''lda_model = gensim.models.LdaMulticore(bag_of_words_corpus, num_topics=12, id2word=word_count_dict, passes=50, workers=4)'''  Если вы получите сообщение с кодом ошибки, попробуйте выполнить обработку версии в монорежиме при помощи класса gensim.models.LdaModel вместо класса gensim.models. LdaMulticore . Теперь распечатаем тематическую структуру, т. е. слова, появляющиеся в каж - дой теме, и их относительный вес: In:for idx, topic in lda_model.print_topics(-1): print('Тема:{0} Словарный состав:{1}\\n'.format(idx, topic)) Out: Тема:0 Словарный состав:0.031*\"window\" + 0.018*\"file\" + 0.016*\"program\" + 0.014*\"imag\" + 0.012*\"version\" + 0.011*\"display\" + 0.011*\"graphic\" + 0.009*\"color\" + 0.009*\"applic\" + 0.009*\"softwar\" Тема:1 Словарный состав:0.010*\"time\" + 0.009*\"bike\" + 0.008*\"good\" + 0.008*\"look\" + 0.008*\"go\" + 0.007*\"drive\" + 0.006*\"right\" + 0.006*\"car\" + 0.006*\"engin\" + 0.006*\"turn\"\n--- Страница 269 ---\n268  Обучение без учит еля в крупном масштабе Тема:2 Словарный состав:0.041*\"file\" + 0.022*\"entri\" + 0.017*\"section\" + 0.015*\"line\" + 0.014*\"program\" + 0.014*\"output\" + 0.012*\"return\" + 0.010*\"error\" + 0.009*\"read\" + 0.009*\"rule\" Тема:3 Словарный состав:0.024*\"space\" + 0.011*\"launch\" + 0.011*\"year\" + 0.009*\"nasa\" + 0.008*\"orbit\" + 0.008*\"satellit\" + 0.007*\"earth\" + 0.006*\"mission\" + 0.006*\"develop\" + 0.006*\"cost\" Тема:4 Словарный состав:0.031*\"game\" + 0.022*\"team\" + 0.019*\"play\" + 0.018*\"year\" + 0.014*\"player\" + 0.012*\"season\" + 0.010*\"hockey\" + 0.010*\"leagu\" + 0.008*\"score\" + 0.007*\"good\" Тема:5 Словарный состав:0.027*\"christian\" + 0.021*\"jesus\" + 0.014*\"believ\" + 0.012*\"bibl\" + 0.012*\"church\" + 0.012*\"religion\" + 0.011*\"book\" + 0.010*\"faith\" + 0.009*\"word\" + 0.009*\"christ\" Тема:6 Словарный состав:0.028*\"armenian\" + 0.018*\"say\" + 0.017*\"peopl\" + 0.015*\"kill\" + 0.012*\"go\" + 0.011*\"come\" + 0.011*\"turkish\" + 0.008*\"jew\" + 0.007*\"leav\" + 0.007*\"live\" Тема:7 Словарный состав:0.015*\"state\" + 0.013*\"govern\" + 0.013*\"right\" + 0.012*\"peopl\" + 0.009*\"presid\" + 0.006*\"work\" + 0.006*\"american\" + 0.006*\"say\" + 0.006*\"israel\" + 0.006*\"countri\" Тема:8 Словарный состав:0.026*\"drive\" + 0.019*\"card\" + 0.014*\"disk\" + 0.014*\"thank\" + 0.013*\"work\" + 0.012*\"problem\" + 0.011*\"price\" + 0.011*\"scsi\" + 0.011*\"control\" + 0.010*\"driver\" Тема:9 Словарный состав:0.017*\"inform\" + 0.014*\"mail\" + 0.013*\"list\" + 0.012*\"post\" + 0.011*\"send\" + 0.010*\"univers\" + 0.009*\"address\" + 0.009*\"anonym\" + 0.008*\"public\" + 0.008*\"internet\" Тема:10 Словарный состав:0.016*\"peopl\" + 0.012*\"thing\" + 0.009*\"time\" + 0.009*\"good\" + 0.008*\"say\" + 0.008*\"question\" + 0.007*\"mean\" + 0.007*\"want\" + 0.007*\"reason\" + 0.007*\"post\" Тема:11 Словарный состав:0.019*\"chip\" + 0.019*\"encrypt\" + 0.010*\"key\" + 0.010*\"clipper\" + 0.010*\"secur\" + 0.009*\"number\" + 0.009*\"wire\" + 0.009*\"data\" + 0.008*\"devic\" + 0.008*\"algorithm\" К сожалению, алгоритм LDA не генерирует названия каждой темы; мы должны сделать это вручную самостоятельно, основываясь на нашей интерпретации ре- зультатов алгоритма. Тщательно исследовав тематическую структуру, мы можем назначить имена обнаруженным темам следующим образом: Т ема Имя 0 Программное обеспечение 1 Приложения2 Аргументация3 Виды транспорта4 Правительство5 Религия6 Поступки людей7 Ближний Восток8 Устройства ПК9 Пространство 10 Игры11 Накопители\n--- Страница 270 ---\nАлгоритм LDA  269 Теперь давайте попытаемся понять, какие темы представлены в предыдущем документе и каковы их веса: In: for index, score in sorted(lda_model[bag_of_words_corpus[document_num]], key=lambda tup: -1*tup[1]): print('Отметка: {}\\t Тема: {}'.format(score, lda_model.print_topic(index, 10))) Out: Отметка: 0.488365059302 Тема: 0.026*\"drive\" + 0.019*\"card\" + 0.014*\"disk\" + 0.014*\"thank\" + 0.013*\"work\" + 0.012*\"problem\" + 0.011*\"price\" + 0.011*\"scsi\" + 0.011*\"control\" + 0.010*\"driver\"Отметка: 0.45607837152 Тема: 0.031*\"window\" + 0.018*\"file\" + 0.016*\"program\" + 0.014*\"imag\" + 0.012*\"version\" + 0.011*\"display\" + 0.011*\"graphic\" + 0.009*\"color\" + 0.009*\"applic\" + 0.009*\"softwar\" Самая высокая отметка связана с темой «Устройства ПК». Основываясь на на- ших предыдущих сведениях о наборах документов, по всей видимости, процесс извлечения тем прошел вполне хорошо. Теперь оценим модель в целом. Перплексивность, т. е. степень неопределен- ности вероятностной модели (или ее логарифм), обеспечивает метрикой, помо-гающей понять, насколько хорошо модель LDA показала себя на тренировочном наборе данных: In: print('Логарифмическая перплексивность модели составляет {0}' .format(lda_model.log_perplexity(bag_of_words_corpus))) Out: Логарифмическая перплексивность модели составляет -7.2481550955 В данном случае перплексивность равна 2-7.298 и связана с (логарифмическим) правдоподобием, что модель LDA в состоянии генерировать документы в тесто- вом наборе при наличии распределения тем для этих документов. Чем ниже пер-плексивность, тем лучше модель, потому что это в основном означает, что модель может регенерировать текст достаточно хорошо. Теперь попробуем использовать модель на ранее не встречавшемся документе. Для простоты документ содержит только следующие предложения: Golf or tennis? Which is the best sport to play? («Гольф или теннис? В какой из них лучше всего по-играть?»): In: unseen_document = 'Golf or tennis? Which is the best sport to play?' bow_vector = \\ word_count_dict.doc2bow(tokenize_lemmatize(unseen_document))for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]): print('Отметка: {0}\\t Тема: {1}' .format(score, lda_model.print_topic(index, 5))) Out: Отметка: 0.816665932757 Тема: 0.031*\"game\" + 0.022*\"team\" + 0.019*\"play\" + 0.018*\"year\" + 0.014*\"player\"\n--- Страница 271 ---\n270  Обучение без учит еля в крупном масштабе Отметка: 0.0166668646637 Тема: 0.026*\"drive\" + 0.019*\"card\" + 0.014*\"disk\" + 0.014*\"thank\" + 0.013*\"work\" Как и ожидалось, тема с самой высокой отметкой относится к «Играм», за кото- рой следуют другие с отметкой относительно меньше. Насколько хорошо алгоритм LDA масштабируется вместе с размером корпуса текстовых документов? К счастью, очень хорошо. Алгоритм является итератив- ным и позволяет выполнять онлайновое обучение, аналогично мини-пакетному. Ключом для онлайнового процесса является метод update , предлагаемый классом LdaModel (или LdaMulticore ). Давайте проведем его тестирование на подмножестве исходного корпуса тек - стовых документов, состоящем из первых 1000 документов, при этом мы будем обновлять модель LDA пакетами по 50, 100, 200 и 500 документов. Для каждого ми-ни-пакетного обновления модели мы будем записывать время и строить график: In: small_corpus = bag_of_words_corpus[:1000]batch_times = {} for batch_size in [50, 100, 200, 500]: print('batch_size = {0}'.format(batch_size)) tik0 = time.time() lda_model = gensim.models.LdaModel(num_topics=12, id2word=word_count_dict) batch_times[batch_size] = [] for i in range(0, len(small_corpus), batch_size): lda_model.update(small_corpus[i:i+batch_size], update_every=25, passes=1+500/batch_size) batch_times[batch_size].append(time.time() - tik0) Out: batch_size = 50batch_size = 100batch_size = 200batch_size = 500 Отметим, что мы установили параметры update_every и passes в функции обнов- ления модели. Это необходимо, чтобы заставить модель сходиться при каждой итерации и не возвращать несходящуюся модель. Обратите внимание, что зна-чение 500 для размера пакета было выбрано эвристически; если задать это зна-чение ниже, то вы получите большое количество предупреждений из библиотеки Gensim о несходимости модели. Теперь отобразим результаты на графике: In: plt.plot(range(50, 1001, 50), batch_times[50], 'g', label=u'размер 50')plt.plot(range(100, 1001, 100), batch_times[100], 'b', label=u'размер 100')plt.plot(range(200, 1001, 200), batch_times[200], 'k', label=u'размер 200')\n--- Страница 272 ---\nАлгоритм LDA  271 plt.plot(range(500, 1001, 500), batch_times[500], 'r', label=u'размер 500') plt.xlabel(u'Размер тренировочного набора') plt.ylabel(u'Время тренировки')plt.xlim([0, 1000])plt.legend(loc=0)plt.show() Чем пакет больше, тем быстрее проходит тренировка. (Напомним, что боль- шие пакеты требуют при обновлении модели меньшего числа прохождений по данным.) С другой стороны, чем больше пакет, тем больше объем памяти, требу - емый для хранения и обработки корпуса текстовых документов. Благодаря ми-ни-пакетному методу обновления алгоритм LDA способен масштабироваться для обработки корпусов, состоящих из миллионов документов. По сути дела, предо-ставленная библиотекой Gensim реализация способна на домашнем компьютере отмасштабировать и обработать всю Википедию за всего пару часов. Если вы до-статочно храбры, чтобы попытаться самому, то по ссылке можно получить полные инструкции по выполнению этой задачи, которые были предоставлены автором библиотеки: https://radimrehurek.com/gensim/wiki.html. Масштабирование алгоритма LDA – оперативная память, CPU и машины Библиотека Gensim очень гибка и создана для уплотнения больших текстовых корпусов; фактически данная библиотека способна масштабировать без какой-либо модификации или дополнительно скачиваемых материалов, используя для этого: 1) число мо дулей CPU, позволяя выполнять параллельную обработку на одиноч- ном узле (при помощи классов, как уже было показано в первом примере);\n--- Страница 273 ---\n272  Обучение без учит еля в крупном масштабе 2) число наблюдений, позволяя выполнять онлайновое обучение на основе мини-пак етов. Это можно реализовать при помощи метода update, который имеется в классах LdaModel и LdaMulticore (как показано в предыдущем при- мере); 3) выполнение на кластере, распределяя рабочую нагрузку по узлам в кластере благодаря Python’овской библиотеке Pyro4 и объектам models.lda_dispatcher (в качестве планировщика) и models.lda_worker (в качестве рабочего процес - са), оба из которых предоставляются библиотекой Gensim. Помимо классического алгоритма LDA, библиотека Gensim также предоставля- ет его иерархическую версию, именуемую иерархическим процес сом Дирихле (hierarchical Dirichlet processing, HDP). Используя этот алгоритм, темы подчиня- ются многоуровневой структуре, позволяя пользователю лучше понять много-сложные корпуса текстовых документов (т. е. где одни документы универсальны, а другие более конкретны по теме). Этот модуль является довольно новым и на конец 2015 г. не был масштабируем в той же степени, что и классический алго-ритм LDA. резюме В этой главе мы представили трех популярных учеников без учителя, которые способны масштабироваться для работы с большими данными. Первый из них, алгоритм PCA, способен сократить количество признаков, создавая такие, кото-рые содержат бóльшую часть дисперсии (т. е. главные). Алгоритм кластеризации K-средних способен собирать в группы подобные точки и связывать их с центрои-дом. Мощный алгоритм LDA предназначен для тематического моделирования на текстовых данных, т. е. совместного моделирования распределения тем в каждом документе и слов, встречающихся в теме. В следующей главе мы представим некоторые усовершенствованные и новей- шие методы машинного обучения, которые пока не являются частью господст - вующей т енденции, однако они естественным образом великолепно подходят для малых наборов данных, а также для обработки крупномасштабного машинного обучения.",
      "debug": {
        "start_page": 236,
        "end_page": 273
      }
    },
    {
      "name": "Глава 8. Распределенные среды – Hadoop и Spark 273",
      "content": "--- Страница 274 --- (продолжение)\nГлава 8 Распределенные среды – Hadoop и Spark В этой главе мы представим новый способ обработки данных, осуществляя мас - штабирование горизонтально. До сих пор в центре нашего внимания прежде все- го находилась обработка больших данных на автономной машине; здесь мы пред-ставим некоторые методы, которые работают на кластере из машин. Если быть точным, мы сначала проиллюстрируем побудительные причины и обстоятельства, когда нам необходим кластер для обработки больших данных. Затем на нескольких примерах мы представим платформу Hadoop и все ее ком-поненты (HDFS, MapReduce и YARN), и, наконец, мы познакомимся с платформой Spark и ее Python’овским интерфейсом – pySpark. От автОнОмнОй машины к набОру узлОв Объем хранения данных во всем мире увеличивается экспоненциально. В наше вре-мя уже стало обычным делом, когда исследователю-аналитику поступает запрос на обработку нескольких терабайт данных в день. Дело осложняется тем, что обычно данные поступают из большого числа различных неоднородных систем, при этом бизнес рассчитывает, что модель будет произведена в течение короткого времени. Обработка больших данных поэтому не является просто вопросом размера, это на самом деле трехмерное явление. В сущности, согласно 3V-модели (от англ. volume, velocity и variety), оперирующие на больших данных системы могут быть классифицированы при помощи трех (ортогональных) критериев. 1. Первый крит ерий – это скорость, которую система достигает во время обра- ботки данных. Хотя еще несколько лет назад скорость означала, как быстро система может обрабатывать пакет данных, в наше время она свидетельству - ет о том, может ли система обеспечивать непосредственные результаты на потоковых данных в реальном времени. 2. Второй критерий – объем, т. е. сколько имеется информации для обработки. Это может быть выражено числом строк, признаков либо чисто простым ко-личеством байт. В потоковой передаче данных объем означает пропускную способность прибывающих в систему данных. 3. После дний критерий – разнообразие, т. е. тип источников данных. Несколь- ко лет назад разнообразие было ограничено структурированными набора-ми данных; сегодня же данные могут быть структурированными (таблицы,\nГлава 8 Распределенные среды – Hadoop и Spark В этой главе мы представим новый способ обработки данных, осуществляя мас - штабирование горизонтально. До сих пор в центре нашего внимания прежде все- го находилась обработка больших данных на автономной машине; здесь мы пред-ставим некоторые методы, которые работают на кластере из машин. Если быть точным, мы сначала проиллюстрируем побудительные причины и обстоятельства, когда нам необходим кластер для обработки больших данных. Затем на нескольких примерах мы представим платформу Hadoop и все ее ком-поненты (HDFS, MapReduce и YARN), и, наконец, мы познакомимся с платформой Spark и ее Python’овским интерфейсом – pySpark. От автОнОмнОй машины к набОру узлОв Объем хранения данных во всем мире увеличивается экспоненциально. В наше вре-мя уже стало обычным делом, когда исследователю-аналитику поступает запрос на обработку нескольких терабайт данных в день. Дело осложняется тем, что обычно данные поступают из большого числа различных неоднородных систем, при этом бизнес рассчитывает, что модель будет произведена в течение короткого времени. Обработка больших данных поэтому не является просто вопросом размера, это на самом деле трехмерное явление. В сущности, согласно 3V-модели (от англ. volume, velocity и variety), оперирующие на больших данных системы могут быть классифицированы при помощи трех (ортогональных) критериев. 1. Первый крит ерий – это скорость, которую система достигает во время обра- ботки данных. Хотя еще несколько лет назад скорость означала, как быстро система может обрабатывать пакет данных, в наше время она свидетельству - ет о том, может ли система обеспечивать непосредственные результаты на потоковых данных в реальном времени. 2. Второй критерий – объем, т. е. сколько имеется информации для обработки. Это может быть выражено числом строк, признаков либо чисто простым ко-личеством байт. В потоковой передаче данных объем означает пропускную способность прибывающих в систему данных. 3. После дний критерий – разнообразие, т. е. тип источников данных. Несколь- ко лет назад разнообразие было ограничено структурированными набора-ми данных; сегодня же данные могут быть структурированными (таблицы,\n--- Страница 275 ---\n274  Распре деленные среды – Hadoop и Spark изображения и т. д.), полуструктурированными (JSON, XML и т. д.) и неструк - турированными (веб-страницы, социальные данные и т. д.). Системы обра- ботки больших данных обычно пытаются перерабатывать как можно больше соответствующих источников, перемешивая все виды источников. Помимо этих критериев, в последние годы появились многие другие V, стре- мящиеся объяснить другие особенности больших данных. Некоторые из них сле - дующие: правдивость (veracity, указывает на ненормальность, смещение и шум, ко- торые содержатся в данных, и в конечном счете на их достоверность); волатильнос ть (volatility, указывает на то, как долго данные могут использо- ваться для извлечения осмысленной информации); валидность (validity, корректность данных); ст оимость (value, указывает на отдачу от вложений в данные). В последние годы роль всех V существенно увеличилась; теперь многие компа- нии осознали, что хранящиеся у них данные имеют огромную стоимость, которая может быть монетизирована, и они испытывают потребность в извлечении из нее информации. Техническая проблема переместилась в сторону обладания до-статочным количеством накопителей и вычислительных мощностей, чтобы быть в состоянии извлекать значимые величины быстро, в крупном масштабе и с ис - пользованием различных входных потоков данных. Нынешние компьютеры, даже новейшие и самые дорогие, имеют ограниченный объем дисковой и оперативной памяти и CPU. И задача обработки ими терабайт (или петабайт) информации в день с генерированием быстрой модели выглядит неподъемной. Кроме того, автономный сервер, который содержит и данные, и об-рабатывающее программное обеспечение, нуждается в своей репликации; иначе он может стать единственной точкой отказа в системе. Мир больших данных поэтому переместился в кластеры: они состоят из пере- менного числа не очень-то дорогостоящих узлов и находятся на высокоскорост - ном интернет-соединении. Обычно кластеры выделяют для хранения данных (большой жесткий диск, небольшой CPU и низкий объем памяти), другие же пред-назначаются для их обработки (мощный CPU, объем памяти от среднего до боль-шого и маленький жесткий диск). Кроме того, если кластер настроен грамотно, то он способен гарантировать надежность (нет точек отказа) и высокую доступность. Стоит отметить, что когда мы храним данные в распределенной среде (в виде кластера), мы также должны рассмотреть ограничение вследствие теоремы Брюе - ра (C AP-теоремы – Consistency, Availability, Partition tolerance); система может га- рантировать только два из следующих трех свойств: сог ласованность: все узлы в состоянии поставлять клиенту те же самые дан- ные и в то же самое время; дост упность: клиент, который запрашивает данные, всегда гарантированно получит отклик на успешные и неуспешные запросы; ус тойчивость к разделению: если сеть сбоит и не все узлы на связи, то систе- ма остается в рабочем состоянии. Говоря конкретно, последствия теоремы CAP следующие: если отказаться от согласованности, то вы создадите среду, где данные рас - пределены по всем узлам, и даже если сеть испытывает некоторые пробле-мы, система по-прежнему в состоянии обеспечить отклик на любой запрос,\n--- Страница 276 ---\nОт автономной машины к набору узлов  275 хотя при этом не гарантируется, что отклик на тот же самый запрос будет тем же самым (он может оказаться противоречивым). Типичными при-мерами такой конфигурации являются СУБД DynamoDB, CouchDB и Cas - sandra; если отказаться от доступности, то вы создадите распределенную систему, которая может не дать отклика на запрос. Примерами такого класса являются СУБД с распределенным кэшем, в частности Redis, MongoDb и MemcacheDb; наконец, если отказаться от устойчивости к разделению, то вы окажетесь в жесткой схеме реляционных СУБД, которые не позволяют сети быть раз-деленной. Данная категория включает в себя MySQL, Oracle и SQLServer. Зачем нужна распределенная платформа? Самый простой способ создать кластер состоит в том, чтобы несколько узлов ис - пользовать в качестве узлов хранения данных, а остальные – в качестве узлов об-работки. Такая конфигурация выглядит очень простой, так как, для того чтобы с ней справиться, вовсе не нужна сложная платформа. По сути дела, многие не-большие кластеры построены именно таким образом: пара серверов занята хра-нением данных (плюс их копий), другая же группа их обрабатывает. Несмотря на то что это решение может показаться отличным, по многим при- чинам оно используется нечасто: оно рабо тает только для чрезвычайно параллельных алгоритмов. Если алго- ритм требует общей зоны памяти, разделяемой обрабатывающими серве-рами, то этот подход использоваться не может; если один или несколько узлов хранения выходят из строя, то не гаранти- руется, что данные останутся согласованными (представьте ситуацию, где узел и его копия выходят из строя одновременно, или где узел выходит из строя сразу после выполнения операции записи, которая еще не была реп - лицирована); если обрабатывающий узел выходит из строя, то мы не в состоянии отсле- дить процесс, который он выполнял, что затрудняет возобновление обра-ботки на другом узле; если сеть регулярно испытывает отказы, то очень трудно предсказать эту ситуацию, после того как она вернется в нормальное состояние. Давайте вычислим, какова вероятность отказа узла. Является ли она действи- тельно такой редкой, что мы можем ее отбросить? Или же эта вероятность являет - ся чем-то более конкретным, и мы будем всегда ее учитывать? Решение простое: возьмем кластер со 100 узлами, где каждый узел имеет вероятность отказа 1% (суммарно определяемую выходом из строя аппаратного и программного обеспе-чений) в первый год. Какова вероятность, что все эти 100 узлов переживут первый год? Исходя из гипотезы, что каждый сервер независим (т. е. каждый узел может выйти из строя независимо от всех остальных), эту вероятность получают прос - тым произведением: P (кластер = ok) = P(узел1 = ok, узел2 = ok, …, узел100 = ok) = (1 – P(fail))100 = 37%.\n--- Страница 277 ---\n276  Распре деленные среды – Hadoop и Spark Полученный результат поначалу очень удивляет, но он объясняет, почему в про- шедшее десятилетие сообщество специалистов, работающих с большими данными, сделало большой акцент на этой проблеме и разработало много решений для управ-ления кластерами. Исходя из результатов формулы, представляется, что событие выхода из строя (или даже несколько таких событий) имеет большую вероятность, и этот факт требует, чтобы такое происшествие было предусмотрено заранее и об-работано должным образом для обеспечения непрерывности операций на данных. Более того, при использовании дешевых аппаратных средств или более крупного кластера сбой, по меньшей мере, одного узла выглядит почти бесспорным.  Важный урок здесь состоит в том, что, раз вы внедряете обработку больших данных на предприятии, вы обязаны принять достаточно контрмер против отказов узлов; это норма, а не исключение, и к ним следует подходить грамотно, чтобы обеспечить непрерывность операций. Сегодня в подавляющем большинстве кластерных платформ используется под- ход divide et impera (разделяй и властвуй): сущес твуют модули, специально предназначенные для узлов данных, и не- сколько других, специально предназначенных для узлов обработки данных (так называемых рабочих узлов); данные тиражир уются по всем узлам данных, при этом один узел является ведущим, гарантируя успешность операций записи и чтения; шаги обрабо тки разделены между рабочими узлами. Они не имеют разде - ляемых ме жду собой состояний (если только они не хранятся в узлах дан- ных), и их ведущий узел гарантирует, что все задачи выполняются положи-тельно и в нужном порядке.  Чуть позже в этой главе мы представим платформу Apache Hadoop; хотя сегодня это зре- лая система управления кластерами, она по-прежнему опирается на прочный фундамент. Прежде чем перейти к ознакомлению с платформой Apache Hadoop, давайте настроим на наших машинах правильную рабочую среду. настр Ойка виртуальнОй машины Поднятие кластера – это долгая и трудная работа; старшие инженеры в области больших данных зарабатывают свои (высокие) зарплаты, не просто скачивая и выполняя двоичное приложение, а профессионально и тщательно настраивая менеджер кластера к требующейся рабочей среде. Это сложная и трудоемкая опе-рация, которая может потребовать много времени, и если результаты будут ниже ожиданий, то весь бизнес (включая исследователей-аналитиков и разработчиков программного обеспечения) не сможет быть продуктивным. Инженеры данных должны знать все мельчайшие подробности относительно узлов, данных, вы-полняемых операций и сети, прежде чем начинать строить кластер. В результа-те обычно получается сбалансированный, настраиваемый, быстрый и надежный кластер, который может использоваться в течение многих лет всем техническим персоналом компании.  Что лучше: кластер с малым числом очень мощных узлов или кластер с большим числом мене е мощных серверов? Ответ на этот вопрос следует давать только после индивидуаль- ной оценки в каждом конкретном случае, и он сильно зависит от данных, обрабатывающих\n--- Страница 278 ---\nНастройка виртуальной машины  277 алгоритмов, числа людей, имеющих нему доступ, скорости, на которой мы хотим получать результаты, общей себестоимости, устойчивой масштабируемости, скорости сети и многих других факторов. Проще говоря, совсем не легко решить, что лучше! Поскольку настройка среды является трудоемкой задачей, мы, авторы, предпо- читаем предоставить своим читателям образ виртуальной машины, содержащий все, в чем вы нуждаетесь, для того чтобы испытать некоторые операции на клас - тере. В следующих разделах вы научитесь настраивать на своей машине гостевую операционную систему, содержащую один узел кластера со всем программным обеспечением, которое вы найдете на реальном кластере. Почему всего один узел? Поскольку платформа, которую мы использовали, не легковесная, мы решили остановиться на атомарной части кластера, гарантирую - щей, что среда, которую вы найдете в узле, точно такая же, что и та, которую вы найдете в реальной ситуации. Для того чтобы запустить виртуальную машину на компьютере, понадобятся два программных продукта: Virtualbox и Vagrant. Обе эти программы бесплатные и с открытым исходным кодом. Виртуализатор VirtualBox Программный продукт VirtualBox с открытым исходным кодом используется для виртуализации гостевых операционных систем по схеме «один ко многим» в Windows, macOS и хост-машинах Linux. С точки зрения пользователя виртуа-лизированная машина выглядит как еще один компьютер, работающий в окне со всем своим функционалом. Программа VirtualBox стала очень популярной из-за своей высокой производи- тельности, простоты и чистого графического интерфейса пользоват еля (GUI). Запуск, останов, импорт и завершение работы виртуальной машины в VirtualBox – это лишь дело нескольких нажатий. С технической стороны VirtualBox является гипервизором, который поддержи- вает создание двух и более виртуальных машин (VM) и управление ими, в том числе многие версии Windows, Linux и BSD-подобные дистрибутивы. Машина, на которой выполняется VirtualBox, называется хостом, в то время как виртуализи- рованные машины называются гостями. Отметим, что между хостом и гостями нет никаких ограничений; например, хост Windows может выполнять Windows (той же версии, предыдущей или самой последней), а также любую Linux и BSD-дистрибутив, которые совместимы с VirtualBox. Виртуализатор Virtualbox часто используется для выполнения системнозависи- мого программного обеспечения; некоторое программное обеспечение выполня-ется только под Windows или только в определенной версии Windows, некоторые доступны только в Linux и т. д. Еще одно применение состоит в моделировании нового функционала в клонированной эксплуатационной среде; прежде чем опробовать модификации в живой (эксплуатационной) среде, разработчики про-граммного обеспечения обычно тестируют его на клоне, подобном тому, который выполняется в VirtualBox. Благодаря гостевой изоляции от хоста, если в госте что-то идет не так, как надо (даже форматирование жесткого диска), это не повлияет на хост. Чтобы вернуть его назад, просто клонируйте свою машину, прежде чем сделать что-либо опасное, и вы всегда успеете его восстановить. Для тех, кто хочет начать с нуля, программа VirtualBox поддерживает виртуаль - ные ж есткие диски (включая жесткие диски, CD-, DVD- и гибкие диски); это во\n--- Страница 279 ---\n278  Распре деленные среды – Hadoop и Spark многом упрощает инсталляцию новой ОС. Например, если вы хотите установить классическую версию Linux Ubuntu 14.04, то сначала скачайте файл .iso. Вместо того чтобы записывать его на CD/DVD, можно просто добавить его в VirtualBox как виртуальный диск. Затем благодаря простому пошаговому интерфейсу мож - но выбрать размер жесткого диска и компоненты гостевой машины (RAM, число модулей CPU, видеопамять и сетевое соединение). Работая с реальной BIOS, мож - но выбрать порядок начальной загрузки: назначая CD/DVD-диску более высокий приоритет, можно запустить процесс инсталляции Ubuntu, как только вы вклю - чаете г остя. Теперь давайте скачаем программу VirtualBox, при этом следует помнить о не- обходимости выбрать правильную версию, соответствующую вашей операцион-ной системе.  Для того чтобы установить этот программный продукт на компьютер, следуйте инструкциям на https://www.virtualbox.org/wiki/Downloads. Во время написания настоящей главы последней была версия 5.1 (5.1.22 на мо- мент публикации перевода). После инсталляции графический интерфейс будет выглядеть, как показано на приведенном ниже снимке экрана: Мы настоятельно рекомендуем разобраться в том, как поднимать гостевую ма- шину на своем компьютере. Каждая гостевая машина будет появляться в левой стороне окна. (На снимке вы видите, что на нашем компьютере имеются три оста-новленных гостя.) Если нажать на каждом из них, то на правой стороне появится подробное описание виртуализированных аппаратных средств. На демонстраци-\n--- Страница 280 ---\nНастройка виртуальной машины  279 онном снимке, если включить виртуальную машину sparkbox_test (выделена сле- ва), она будет запущена на виртуальном компьютере, аппаратные средства кото- рого состоят из 4 Гб RAM, двух процессоров, жесткого диска на 40 Гб и видеокарты с 12 Мб RAM, подключенных к сети через NAT. Конфигуратор Vagrant Программный продукт Vagrant (версии 1.9.5 на момент публикации) конфигури-рует виртуальные среды на высоком уровне. Ключевым элементом программы Vagrant является возможность по созданию сценариев, которые часто использу - ются для программного и автоматического создания конкретных виртуальных сред. Для построения и конфигурирования виртуальной машины программа Vagrant использует виртуализатор VirtualBox (а также другие виртуализаторы).  Чтобы проинсталлировать этот программный продукт, следуйте инструкциям, которые при-ве дены на https://www.vagrantup.com/downloads.html. Использование виртуальной машины После установки программного обеспечения Vagrant и VirtualBox все готово к за-пуску узла кластерной среды. Создайте пустой каталог и вставьте в новый файл с именем Vagrantfile следующие ниже команды Vagrant: Vagrant.configure(\"2\") do |config| config.vm.box = \"sparkpy/sparkbox_test_1\" config.vm.hostname = \"sparkbox\" config.ssh.insert_key = false # Менеджер ресурсов Hadoop config.vm.network :forwarded_port, guest: 8088, host: 8088, auto_correct: true # Узел имен Hadoop config.vm.network :forwarded_port, guest: 50070, host: 50070, auto_correct: true # Узел данных Hadoop config.vm.network :forwarded_port, guest: 50075, host: 50075, auto_correct: true # Блокноты Ipython (yarn и автономные) config.vm.network :forwarded_port, guest: 8888, host: 8888, auto_correct: true # Spark UI (автономный) config.vm.network :forwarded_port, guest: 4040, host: 4040, auto_correct: true config.vm.provider \"virtualbox\" do |v| v.customize [\"modifyvm\", :id, \"--natdnshostresolver1\", \"on\"] v.customize [\"modifyvm\", :id, \"--natdnsproxy1\", \"on\"] v.customize [\"modifyvm\", :id, \"--nictype1\", \"virtio\"] v.name = \"sparkbox_test\" v.memory = \"4096\" v.cpus = \"2\" end end\n--- Страница 281 ---\n280  Распре деленные среды – Hadoop и Spark Если смотреть сверху вниз, первые строки скачивают нужную виртуальную машину (которую мы, авторы, создали и закачали в репозиторий). Затем мы на- страиваем несколько портов, которые будут направлены в гостевую машину; тем самым вы сможете иметь доступ к нескольким веб-службам виртуализированной машины. И в конце мы настраиваем аппаратные средства узла.  Конфигурация настроена для виртуальной машины с исключительным использованием 4 Гб R AM и двух ядер. Если ваша система не может удовлетворить этим требованиям, то поме- няйте значения v.memory и v.cpus на те, которые подходят для вашей машины. Отметим, что некоторые из приведенных ниже примеров программного кода могут не работать, если настроенная вами конфигурация не отвечает требованиям. Теперь откройте терминал и перейдите в каталог с файлом Vagrantfile , находясь в нем, запустите виртуальную машину при помощи следующей команды: $ vagrant up В первый раз эта команда займет достаточно продолжительное время, т. к. она будет скачивать (скачиваемые данные занимают почти 2 Гб) и строить соответ - ствующую структуру виртуальной машины. В следующий раз она займет мини- мальное количество времени, поскольку больше ничего скачивать не потребуется После включения в вашей локальной системе виртуальной машины к ней мож - но получить доступ следующим образом: $ vagrant ssh Эта команда имитирует SSH-доступ. В результате вы наконец окажетесь внутри виртуализированной машины.  На машинах Windows эта команда может не сработать и выдать ошибку из-за отсутствия испо лнимого SSH. В такой ситуации скачайте и установите SSH-клиент для Windows, в част - ности Putty (http://www.putty.org/), Cygwin openssh (http://www.cygwin.com/) либо Openssh для Windows (http://sshwindows.sourceforge.net/). Системы Unix эта проблема не должна затрагивать.  В Windows запуск Vagrant может также быть осуществлен посредством Git. Для этого надо у становить Git (http://git-scm.com/download/win). При этом в системную переменную Path должен быть добавлен путь к папке bin: C:\\Program Files\\Git\\usr\\bin . Затем открыть окно терминала (Стартовое меню -> cmd), нажав комбинацию клавиш Shift+Enter, чтобы войти в качестве Администратора, и набрать либо скопировать/вставить: set PATH=%PATH%;C:\\Program Files\\Git\\usr\\bin vagrant ssh В качестве бонуса можно вместо командной строки Windows воспользоваться консолью Console (http://sourceforge.net/projects/console/), в которой есть возможность указывать не-обходимый путь PATH, чтобы впоследствии всегда открывать новый терминал с нужными настройками (конфигурируется в разделе Options). Для того чтобы выключить виртуальную машину, сначала нужно из нее выйти. Находясь внутри виртуальной машины, просто примените команду exit, чтобы выйти из SSH-соединения, и затем закройте виртуальную машину: $ vagrant halt\n--- Страница 282 ---\nЭкосистема Hadoop  281  Виртуальная машина потребляет ресурсы. Не забудьте по окончании работы ее выключить при помощи к оманды vagrant halt из каталога, где она расположена. Приведенная выше команда закрывает виртуальную машину точно так же, как это делается при закрытии сервера. Чтобы ее удалить вместе со всем ее содержи-мым, нужно применить команду vagrant destroy . Следует использовать ее осторож - но: после уничтожения машины вы не сможете восстановить в ней файлы. Ниже приведены инструкции по использованию блокнота Jupyter в виртуаль- ной машине. Запус тить vagrant up и vagrant ssh из папки, содержащей файл Vagrantfile . Те- перь вы должны быть внутри виртуальной машины. Запус тить сценарий: vagrant@sparkbox:~$ ./start_hadoop.sh В этом месте запустите следующий ниже сценарий оболочки: vagrant@sparkbox:~$ ./start_jupyter_yarn.sh Откройте браузер на своей локальной машине и направьте его в http:// localhost:8888 . Такой блокнот будет поддерживаться кластерным узлом. Чтобы выключить блокнот и виртуальную машину, выполните следующие шаги: 1) для закрытия к онсоли Jupyter нажмите Ctrl+C (и затем наберите Y для Yes ); 2) закройте плат форму Hadoop следующим образом: vagrant@sparkbox:~$ ./stop_hadoop.sh 3) выйдите из виртуальной машины при помощи следующей команды: vagrant@sparkbox:~$ exit 4) закройте вир туальную машину VirtualBox командой vagrant halt. экОсистема haDooP Платформа Apache Hadoop – это очень популярная программная платформа для распределенного хранения данных и распределенной обработки на кластере. Ее сильная сторона заключается в ее цене (она бесплатная), гибкости (имеет откры-тый исходный код и, хотя она написана на Java, может использоваться другими языками программирования), масштабируемости (может обрабатывать кластеры, состоящие из тысяч узлов) и устойчивости (толчком для ее создания послужила публикация компанией Google работы о вычислительной концепции MapReduce, и она в ходу начиная с 2011 г.). Все это делает ее де-факто стандартом поддержа-ния и обработки больших данных. Более того, ее функционал расширен большим числом других проектов от компании Apache Foundation. Архитектура Логически Hadoop состоит из двух частей: распределенного хранения данных (HDFS) и распределенной обработки (YARN и MapReduce). Несмотря на то что ее программная реализация очень сложная, общая архитектура довольно понятная. Клиент может получать доступ к хранению данных и их обработке посредством\n--- Страница 283 ---\n282  Распре деленные среды – Hadoop и Spark двух специализированных модулей; затем они отвечают за распределение зада- ния по всем рабочим узлам: КлиентУзел имен Распределенное хранение Распределенная обработка Менеджер ресурсовУзел данных Узел данных Менеджер узла Менеджер узла Все модули Hadoop работают как службы (или экземпляры), т. е. физический или виртуальный узел может выполнять многие из них. Как правило, в небольших кластерах все узлы выполняют распределенные вычисления и обрабатывающие службы; в больших же кластерах бывает лучше разделить эти две функциональ-ности и детализировать узлы. Рассмотрим подробно функциональности, предлагаемые этими двумя уровнями. Распределенная файловая система HDFS Распределенная файловая сист ема Hadoop (Hadoop distributed file system, HDFS) – это отказоустойчивая распределенная файловая система, предназначен- ная для работы на товарном недорогом оборудовании и способная обрабатывать очень большие наборы данных (в порядке сотен петабайт до эксабайт). Несмотря на это, распределенная файловая система HDFS требует наличия высокоскорост - ного сетевого соединения для передачи данных через узлы, задержка не настоль-ко же низкая, что и в классических файловых системах (она может быть в порядке нескольких секунд); поэтому HDFS разработана, имея в виду пакетную обработку данных и высокую пропускную способность. Каждый узел HDFS содержит часть данных этой файловой системы, и те же самые данные также реплицируются в других экземплярах, что гарантирует отказоустойчивость и доступ с высокой пропускной способностью. Архитектура HDFS имеет тип «ведущий–ведомый». Если ведущее устройство (узел имен) дает сбой, то имеется вторичный/резервный, готовый взять на себя управление. Все другие экземпляры являются ведомыми (узлы данных); если один из них дает сбой, то это не представляет проб лем, так как HDFS был разрабо- тан с учетом такой ситуации.\n--- Страница 284 ---\nЭкосистема Hadoop  283 Узлы данных содержат блоки данных: каждый сохраненный в HDFS файл раз- бит на порции (или блоки), обычно по 64 Мб каждый блок, и затем распределяется и реплицируется в наборах узлов данных. Узел имен просто хранит метаданные файлов из распределенной файловой си- стемы; в нем хранятся не фактические данные, а соответствующие указания от - носительно того, как получить доступ к файлам в многочисленных узлах данных, которыми он управляет. Клиент, делающий запрос на чтение файла, должен сначала связаться с узлом имен, который вернет таблицу, содержащую упорядоченный список блоков и их расположений (в узлах данных). В этом месте клиент должен отдельно связать-ся с узлами данных, скачав все блоки и восстановив файл (путем конкатенации блоков). Для записи файла клиент вместо этого должен прежде всего связаться с узлом имен, который сначала решит, как поступить с запросом, обновит свои учетные данные и затем ответит клиенту упорядоченным списком узлов данных, куда можно записать каждый блок файла. Клиент затем установит контакт и закачает блоки в узлы данных, как указано в ответе узла имен. Запросы к пространству имен (например, вывод содержимого каталога, созда- ние папки и т. д.), напротив, полностью обрабатываются узлом имен путем реали-зации доступа к своим метаданным. Узел имен, помимо этого, также отвечает за соответствующую обработку си - туации отказа узла данных (он помечается как мертвый, если тактовые пакеты не доходят), и заново тиражирует его данные по другим узлам. Несмотря на то что эти операции долго и сложно реализуемы с обеспечением устойчивой работы, они абсолютно прозрачны для пользователя благодаря мно-гим библиотекам и оболочке HDFS. Характер работы в распределенной файловой системе HDFS сильно напоминает то, что вы в настоящее время делаете в своей файловой системе, что является большим преимуществом Hadoop: сокрытие вы-числительной сложности и предоставление пользователю возможности с легко-стью ее использовать. Теперь взглянем на оболочку HDFS и чуть позднее на библиотеку Python.  Используйте предыдущие инструкции относительно того, как на своем компьютере вклю-чить виртуальную машину и запустить блокнот Jupyter. Теперь откройте блокнот; эта работа займет больше времени, чем обычно, по- скольку каждый блокнот подключается к кластерной платформе Hadoop. Когда блокнот будет готов к использованию, вы увидите, что флажок с сообщением Kernel starting, please wait… (Запуск ядра, ожидайте…) в правом верхнем углу исчезнет. Первая часть исходного кода касается оболочки распределенной файловой си- стемы HDFS; поэтому все следующие команды могут выполняться в командой строке либо в оболочке виртуальной машины. Чтобы их выполнить в блокноте Jupyter, все они предваряются вопросительным знаком « !», который представляет собой быстрый способ выполнения команд bash в блокноте. Общий знаменатель следующих ниже командных строк – это исполнение; мы будем всегда выполнять команду hdfs. Это основной интерфейс для получения до- ступа к системе HDFS и управления ею и основная команда оболочки HDFS.\n--- Страница 285 ---\n284  Распре деленные среды – Hadoop и Spark Мы начинаем с отчета о состоянии HDFS. Чтобы получить подробности о рас- пределенной файловой сист еме (dfs) и ее узлах данных, следует использовать подкоманду dfsadmin : In: !hdfs dfsadmin -report Out: Configured Capacity: 42241163264 (39.34 GB)Present Capacity: 37569168058 (34.99 GB)DFS Remaining: 37378433024 (34.81 GB)DFS Used: 190735034 (181.90 MB)DFS Used%: 0.51%Under replicated blocks: 0Blocks with corrupt replicas: 0Missing blocks: 0 ------------------------------------------------- Live datanodes (1): Name: 127.0.0.1:50010 (localhost) Hostname: sparkboxDecommission Status : NormalConfigured Capacity: 42241163264 (39.34 GB)DFS Used: 190735034 (181.90 MB)Non DFS Used: 4668290330 (4.35 GB)DFS Remaining: 37380775936 (34.81 GB)DFS Used%: 0.45%DFS Remaining%: 88.49%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Tue Feb 09 19:41:17 UTC 2016 Подкоманда dfs позволяет использовать некоторые известные команды Unix для доступа и взаимодействия с распределенной файловой системой. Например, перечислить содержимое корневого каталога можно следующим образом: In: !hdfs dfs -ls / Out: Found 2 itemsdrwxr-xr-x - vagrant supergroup 0 2016-01-30 16:33 /sparkdrwxr-xr-x - vagrant supergroup 0 2016-01-30 18:12 /user Результат аналогичен использованию имеющейся в Linux команды ls, кото - рая перечисляет полномочия, число ссылок, владеющих файлом пользователя, и группу, размер, метку времени последнего изменения и имя каждого файла или каталога. Аналогично команде df можно вызвать аргумент -df для отображения объема доступного дискового пространства в HDFS. Опция -h сделает вывод более читае- мым (вместо байт используются гигабайты и мегабайты):\n--- Страница 286 ---\nЭкосистема Hadoop  285 In: !hdfs dfs -df -h / Out: Filesystem Size Used Available Use%hdfs://localhost:9000 39.3 G 181.9 M 34.8 G 0% Аналогично команде du для отображения размера каждой папки, содержавшей- ся в корне, можно использовать аргумент -du. И опять-таки, опция -h произведет более читаемый результат: In:!hdfs dfs -du -h / Out: 178.9 M /spark1.4 M /user До сих пор мы извлекали некоторую информацию из HDFS. Теперь давайте выполним несколько операций в распределенной файловой системе, которые ее изменят. Мы можем начать с создания папки, используя для этого опцию -mkdir , после которой следует имя. Отметим, что данная операция может не дать резуль-тата, если каталог уже существует (точно так же, как в Linux с командой mkdir ): In: !hdfs dfs -mkdir /datasets Теперь передадим несколько файлов из жесткого диска узла в распределенную файловую систему. В созданной нами виртуальной машине в каталоге /datasets уже существует текстовый файл; давайте скачаем текстовый файл из Интерне- та и переместим оба файла в каталог HDFS, который мы создали предыдущей коман дой: In: !wget -q http://www.gutenberg.org/cache/epub/100/pg100.txt \\ -O /datasets/shakespeare_all.txt !hdfs dfs -put /datasets/shakespeare_all.txt \\ /datasets/shakespeare_all.txt !hdfs dfs -put /datasets/hadoop_git_readme.txt \\ /datasets/hadoop_git_readme.txt Импорт прошел успешно? Да, ошибок не было. Однако, чтобы избавиться от каких-либо сомнений, давайте выведем содержимое каталога/наборов данных распределенной файловой системы HDFS на экран, и мы увидим эти два файла: In: !hdfs dfs -ls /datasets Out: Found 2 items-rw-r--r-- 1 vagrant supergroup 1365 2016-01-31 12:41 /datasets/hadoop_git_readme.txt-rw-r--r-- 1 vagrant supergroup 5589889 2016-01-31 12:41 /datasets/shakespeare_all.txt\n--- Страница 287 ---\n286  Распре деленные среды – Hadoop и Spark Для конкатенации нескольких файлов в стандартном потоке вывода можно применить аргумент -cat. В следующем ниже фрагменте кода мы подсчитаем количество символов новой строки в текстовом файле. Отметим, что первая ко- манда передается по конвейеру (« |») в следующую команду, которая действует на локальной машине: In:!hdfs dfs -cat /datasets/hadoop_git_readme.txt | wc –l Out: 30 В сущности, аргументом -cat можно связать несколько файлов из локальной машины и из распределенной файловой системы HDFS. Чтобы в этом убедиться, теперь давайте подсчитаем количество символов новых строк, когда файл, кото-рый хранится в HDFS, конкатенируется к тому же самому файлу, который хра-нится на локальной машине. Чтобы избежать недоразумений, можно применить универсальный идентификатор ресурса (URI) по схеме: hdfs: при обращении к файлам в HDFS и file: при обращении к локальным файлам. In:!hdfs dfs -cat \\ hdfs:///datasets/hadoop_git_readme.txt \\ file:///home/vagrant/datasets/hadoop_git_readme.txt | wc –l Out: 60 Для копирования в HDFS применяется аргумент -cp: In: !hdfs dfs -cp /datasets/hadoop_git_readme.txt \\/datasets/copy_hadoop_git_readme.txt Для удаления файла (или каталогов с правильной опцией) используется аргу - мент –rm. В этом фрагменте кода мы удаляем файл, который мы только что созда- ли предыдущей командой. Отметим, что распределенная файловая система HDFS имеет thrash-механизм; следовательно, удаленный файл в действительности не удаляется из HDFS, а просто перемещается в специальный каталог: In:!hdfs dfs -rm /datasets/copy_hadoop_git_readme.txt Out: 16/02/09 21:41:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes. Deleted /datasets/copy_hadoop_git_readme.txt Для очистки удаленных данных предназначена следующая команда: In: !hdfs dfs –expunge Out: 16/02/09 21:41:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n--- Страница 288 ---\nЭкосистема Hadoop  287 Чтобы передать файл из распределенной файловой системы HDFS на локаль- ную машину, используется аргумент -get: In: !hdfs dfs -get /datasets/hadoop_git_readme.txt \\/tmp/hadoop_git_readme.txt Чтобы взглянуть на хранящийся в HDFS файл, используется аргумент -tail . От- метим, что функция head в HDFS отсутствует, поскольку данная функция может быть выполнена при помощи команды cat, и ее результат затем передается по кон вейеру в локальную команду head. Что касается функции tail, то оболочка HDFS просто отображает последний килобайт данных: In: !hdfs dfs -tail /datasets/hadoop_git_readme.txt Out: ntry, of encryption software. BEFORE using any encryption software, please check your country's laws, regulations and policies concerning the import, possession, or use, and re-export of encryption software, to see if this is permitted. See <http://www.wassenaar.org/> for more information Команда hdfs является основной точкой входа для распределенной файловой системы HDFS, но она – медленная, а вызов системных команд из Python и чтение назад выходных данных весьма утомительны. Для этой цели существует библио - тека Python под названием Snakebite, которая служит клиентской оберткой для многих операций распределенной файловой системы Hadoop. К сожалению, данная библиотека не настолько полная, как оболочка HDFS, и привязана к узлу имен. Чтобы ее установить на локальную машину, просто используйте команду pip install snakebite . Для инстанцирования клиентского объекта мы должны предоставить IP (или его псевдоним) и порт узла имен. Наша виртуальная машина работает на порту 9000: In: from snakebite.client import Clientclient = Client(\"localhost\", 9000) Чтобы распечатать информацию о HDFS, клиентский объект имеет метод serverdefaults : In: client.serverdefaults() Out: {'blockSize': 134217728L, 'bytesPerChecksum': 512, 'checksumType': 2, 'encryptDataTransfer': False, 'fileBufferSize': 4096, 'replication': 1, 'trashInterval': 0L, 'writePacketSize': 65536}\n--- Страница 289 ---\n288  Распре деленные среды – Hadoop и Spark Чтобы перечислить файлы и каталоги, находящиеся в корне, можно применить ls. Ее результатом является список словарей, один для каждого файла, содержа- щий, в частности, информацию о полномочиях, метке времени последнего из- менения и т. д. В этом примере мы просто интересуемся путями (т. е. именами): In:for x in client.ls(['/']): print(x['path']) Out: /datasets/spark/user В точности, как и в приведенном выше примере, клиент Snakebite имеет в рас - поряжении методы du (от disk usage для получения информации об использовании диска) и df (от disk free для получения информации о свободном пространстве). Отметим, что многие методы (подобно du) возвращают генераторы, т. е. чтобы их исполнить, необходимо их потребить (аналогично итератору или списку): In:client.df() Out: {'capacity': 42241163264L, 'corrupt_blocks': 0L, 'filesystem': 'hdfs://localhost:9000', 'missing_blocks': 0L, 'remaining': 37373218816L, 'under_replicated': 0L, 'used': 196237268L} In: list(client.du([\"/\"])) Out: [{'length': 5591254L, 'path': '/datasets'}, {'length': 187548272L, 'path': '/spark'}, {'length': 1449302L, 'path': '/user'}] Что касается примера оболочки HDFS, попробуем теперь подсчитать символы новой строки в том же файле при помощи библиотеки Snakebite. Отметим, что ее метод cat возвращает генератор: In:for el in client.cat(['/datasets/hadoop_git_readme.txt']): print(el.next().count(\"\\n\")) Out: 30 Давайте теперь удалим файл из распределенной файловой системы HDFS. И опять-таки, обращаем внимание, что метод delete возвращает генератор, и его выполнение никогда не бывает неуспешным, даже если попытаться удалить не-существующий каталог. По сути дела, библиотека Snakebite не поднимает исклю-\n--- Страница 290 ---\nЭкосистема Hadoop  289 чения, а просто отправляет пользователю сообщение в результирующем словаре, что операция не удалась: In:client.delete(['/datasets/shakespeare_all.txt']).next() Out: {'path': '/datasets/shakespeare_all.txt', 'result': True} Теперь скопируем файл из распределенной файловой системы HDFS в локаль- ную файловую систему. Отметим, что результат является генератором, и, чтобы увидеть, была ли операция успешной, необходимо проверить выходной словарь: In:(client.copyToLocal(['/datasets/hadoop_git_readme.txt'], '/tmp/hadoop_git_readme_2.txt').next()) Out: {'error': '', 'path': '/tmp/hadoop_git_readme_2.txt', 'result': True, 'source_path': '/datasets/hadoop_git_readme.txt'} Наконец, создадим каталог и удалим все файлы, совпадающие со строкой: In:list(client.mkdir(['/datasets_2'])) Out: [{'path': '/datasets_2', 'result': True}] In: client.delete(['/datasets*'], recurse=True).next() Out: [{'path': '/datasets', 'result': True}, {'path': '/datasets_2', 'result': True}] Почему нет примера программного кода для размещения файла в распреде- ленной файловой системе HDFS и примера копирования файла из HDFS в другой файл? Весь этот функционал в библиотеке Snakebite пока еще не реализован. Для этих операций мы будем использовать оболочку HDFS посредством системных вызовов 1. Вычислительная парадигма MapReduce Модель программирования в вычислительной парадигме MapReduce была реа-лизована еще в самых ранних версиях Hadoop. Это очень простая модель, пред-назначенная для обработки параллельными пакетами больших наборов данных 1 Операция put(источник, место_назначения) для выгрузки локального файла в HDFS реали- зована в последних версиях библиотеки. См. документацию по библиотеке на https:// snakebite.readthedocs.io/en/latest/genindex.html. – Прим. перев.\n--- Страница 291 ---\n290  Распре деленные среды – Hadoop и Spark на распределенном кластере. Ядро MapReduce состоит из двух программируе - мых ф ункций – трансформатора (mapper), который выполняет фильтрацию, и редуктора (reducer), который выполняет агрегацию, – а также диспетчера (shuf fl er), который перемещает объекты от трансформаторов в соответствующие редукторы.  Компания Google опубликовала работу по MapReduce в 2004 г., спустя несколько месяцев пос ле получения патента на эту технологию. Говоря конкретно, ниже приведены реализованные в Hadoop шаги вычисли- тельной парадигмы MapReduce. Фрагментатор данных (data chunker). Данные считываются из файловой си- стемы и разбиваются на порции. Порция является частью входного набора данных; обычно это блок фиксированного размера (например, блок HDFS, считанный из узла данных) либо другое более подходящее разбиение.Например, если мы хотим подсчитать число символов, слов и строк в тек - стовом файле, хорошим разбиением может быть строка текста. Транс форматор (mapper): из каждой порции генерируется серия пар «ключ- значение». Каждый экземпляр трансформатора применяет к разным пор-циям данных одинаковую функцию преобразования.Если продолжить с предыдущим примером, то на этом шаге для каждой строки генерируются три пары «ключ-значение» – одна содержит число символов в строке (ключом может быть строка chars, «символы»), другая со- держит число слов (в данном случае ключ должен отличаться, скажем, это words, «слова»), и еще одна содержит число строк, которое всегда равно еди-нице (в данном случае ключом может быть lines, «строки»). Диспетчер (или расфасовщик, shuffler): при наличии ключа и числа имею- щихся редукторов диспетчер направляет все пары «ключ-значение» с оди-наковым ключом в одинаковые редукторы. Как правило, данная операция сводится к хэшированию ключа, остаток от деления на число редукторов, что гарантирует достаточное количество ключей для каждого редуктора. Этот функционал не программируется пользователем и предоставляется платформой MapReduce. Ре дуктор (reducer): каждый редуктор получает все пары «ключ-значение» для конкретного набора ключей и производит ноль или более конечных ре-зультатов. В приведенном выше примере в редуктор прибывают все значения, связан-ные с ключом words; его работа – просто суммировать все значения. То же самое происходит и для остальных ключей, приводя к трем итоговым зна-чениям: количеству символов, слов и строк. Отметим, что эти результаты могут находиться на разных редукторах. Выхо дной регистратор (output writer): результаты работы редукторов запи- сываются в файловую систему (или в распределенную файловую систему HDFS). В конфигурации платформы Hadoop по умолчанию каждый редуктор пишет свой файл ( part-r-00000 – это выход первого редуктора, part-r-00001 – выход второго и т. д.). Чтобы получить полный список выходных результа-тов в одном файле, необходимо их конкатенировать.\n--- Страница 292 ---\nЭкосистема Hadoop  291 Эту работу можно визуально представить и понять следующим образом: Редукторы (HD)FS Трансформаторы (HD)FSДиспетчер После шага преобразования имеется еще один дополнительный шаг, кото- рый может выполняться каждым экземпляром трансформатора, – объединитель (combiner). Он в основном предугадывает, если это возможно, шаг редукции на трансформаторе и часто используется для уменьшения объема информации для диспетчера, тем самым ускоряя процесс. В предыдущем примере, если транс - форматор обрабатывает более одной строки входного файла, то во время (до-полнительного) шага объединителя он может предварительно выполнить агре-гирование результатов, выведя меньшее число пар «ключ-значение». Например, если трансформатор обрабатывает 100 строк текста в каждой порции, то зачем выводить 300 пар «ключ-значение» (100 для числа символов, 100 для слов и 100 для строк), когда всю информацию можно агрегировать в три значения? В этом в действительности состоит цель объединителя. В реализации MapReduce, предоставляемой платформой Hadoop, операция дис - петчеризации распределена, тем самым оптимизируя стоимость обмена данными, и можно выполнять более одного трансформатора и редуктора в расчете на узел, в полной мере задействуя имеющиеся на узлах аппаратные ресурсы. Кроме того, инфраструктура Hadoop обеспечивает избыточность и отказоустойчивость, т. е. одна и та же задача может быть назначена многочисленным рабочим процессам. Теперь давайте посмотрим, как это работает. Несмотря на то что платформа Hadoop написана на Java, благодаря утилите Hadoop Streaming трансформаторы и редукторы могут исполняться в любой программной среде, включая Python. Для потоковой передачи содержимого утилита Hadoop Streaming использует конвей-ер и стандартные потоки вводы-вывода; поэтому трансформаторы и редукторы должны реализовывать функцию чтения из stdin и функцию записи пар «ключ-значение» в stdout. Теперь включим виртуальную машину и откроем новый блокнот Jupyter. И в этом случае тоже сначала познакомимся с предоставляемым платформой\n--- Страница 293 ---\n292  Распре деленные среды – Hadoop и Spark Hadoop способом выполнения задания MapReduce из командной строки, и только потом представим чистую библиотеку Python. Первый пример будет в точности, как мы описали ранее: счетчик числа символов, слов и строк в текстовом файле. Сначала вставим наборы данных в распределенную файловую систему HDFS; мы будем использовать файл Hadoop Git (короткий текстовый файл readme, рас - пространяемый вместе с Apache Hadoop) и полный текст всех сочинений Уильяма Шекспира, предоставленных проектом Gutenberg (несмотря на то что этот файл имеет объем всего 5 Мб, он содержит почти 125K строк). В первой ячейке мы очис - тим папку от предыдущего эксперимента, затем скачаем файл с библиографи- ей Уильяма Шекспира в папку dataset и, наконец, поместим оба набора данных в HDFS: In:!hdfs dfs -mkdir -p /datasets!wget -q http://www.gutenberg.org/cache/epub/100/pg100.txt \\ -O /datasets/shakespeare_all.txt!hdfs dfs -put -f \\ /datasets/shakespeare_all.txt/datasets/shakespeare_all.txt!hdfs dfs -put -f \\ /datasets/hadoop_git_readme.txt/datasets/hadoop_git_readme.txt!hdfs dfs -ls /datasets Теперь давайте создадим исполняемые файлы Python, содержащие трансфор- матор и редуктор. Здесь мы воспользуемся очень грязным приемом: мы напи-шем файлы Python (и сделаем их исполняемыми), используя операцию записи из блокнота. Как трансформатор, так и редуктор читают из стандартного потока stdin и пи- шут в стандартный поток stdout (при помощи простых команд печати). Если быть точным, трансформатор читает строки из stdin и распечатывает пары «ключ-значение» с количеством символов (кроме символа новой строки), количеством слов (разбивая строки по пробелу) и количеством строк, всегда равным единице. Редуктор, со своей стороны, суммирует значения для каждого ключа и распеча-тывает общую сумму: In:with open('mapper_hadoop.py', 'w') as fh: fh.write(\"\"\"#!/usr/bin/env python import sysfor line in sys.stdin: print \"chars\", len(line.rstrip('\\\\n')) print \"words\", len(line.split()) print \"lines\", 1 \"\"\") with open('reducer_hadoop.py', 'w') as fh: fh.write(\"\"\"#!/usr/bin/env python import syscounts = {\"chars\": 0, \"words\":0, \"lines\":0}for line in sys.stdin:\n--- Страница 294 ---\nЭкосистема Hadoop  293 kv = line.rstrip().split() counts[kv[0]] += int(kv[1]) for k,v in counts.items(): print k, v \"\"\") In: !chmod a+x *_hadoop.py Чтобы увидеть, как этот сценарий работает, давайте испытаем его локально, не используя Hadoop. По сути дела, поскольку трансформаторы и редукторы чита- ют и пишут в стандартный поток ввода-вывода, мы можем связать все операции в один конвейер. Отметим, что диспетчер можно заменить командой sort -k1,1 , которая сортирует входные строки по первому полю (т. е. ключу): In: !cat /datasets/hadoop_git_readme.txt | ./mapper_hadoop.py | \\ sort -k1,1 | ./reducer_hadoop.py Out: chars 1335lines 31words 179 Теперь давайте воспользуемся реализацией вычислительной парадигмы MapReduce в среде Hadoop, чтобы получить тот же самый результат. В первую очередь мы должны создать пустой каталог в распределенной файловой системе HDFS, который будет хранить результаты. В данном случае мы создадим каталог /tmp и удалим все, что находится внутри одноименного каталога, что и каталог результата работы задания (Hadoop не будет работать, если выходной файл уже существует). Затем мы применим команду, необходимую для выполнения зада-ния MapReduce. Данная команда включает следующее: использование возможности утилиты Hadoop Streaming (с указанием на jar- файл данной утилиты); трансформат оры и редукторы, которые мы хотим применить (опции –mapper и -reducer ); файлы, которые мы хотим направить в каждый трансформатор, так как это локальные файлы (при помощи опции -files ); вхо дной файл (опция -input ) и выходной каталог (опция -output ). In: !hdfs dfs -mkdir -p /tmp!hdfs dfs -rm -f -r /tmp/mr.out !hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar \\ -files mapper_hadoop.py,reducer_hadoop.py \\ -mapper mapper_hadoop.py -reducer reducer_hadoop.py \\ -input /datasets/hadoop_git_readme.txt \\ -output /tmp/mr.out Out: 17/06/17 06:13:28 INFO mapreduce.Job: Running job: job_1497616070323_0009\n--- Страница 295 ---\n294  Распре деленные среды – Hadoop и Spark 17/06/17 06:13:38 INFO mapreduce.Job: Job job_1497616070323_0009 running in uber mode : false 17/06/17 06:13:38 INFO mapreduce.Job: map 0% reduce 0%17/06/17 06:13:48 INFO mapreduce.Job: map 50% reduce 0%17/06/17 06:13:55 INFO mapreduce.Job: map 100% reduce 0%17/06/17 06:14:06 INFO mapreduce.Job: map 100% reduce 100%17/06/17 06:14:07 INFO mapreduce.Job: Job job_1497616070323_0009 completed successfully Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 17/06/17 06:14:07 INFO streaming.StreamJob: Output directory: /tmp/mr.out Полученный результат весьма многословный; мы извлекли из него только три важных раздела. Первый показывает продвижение работы задания MapReduce; эта информация очень полезна для отслеживания и оценки времени, необхо-димого для завершения работы. Второй раздел показывает ошибки, которые, возможно, произошли во время работы задания, и последний раздел сообщает о выходном каталоге и метке времени завершения задания. Весь процесс на не-большом файле (из 30 строк) занял почти полминуты! Причины очень простые: во-первых, вычислительная парадигма MapReduce в среде Hadoop была реали-зована для устойчивой обработки больших данных и содержит много накладных расходов, и, во-вторых, идеальной для нее средой является кластер из мощных машин, а не виртуальная машина с 4 Гб RAM. С другой стороны, этот программ-ный код без каких-либо изменений можно выполнять на наборах данных гораздо большего объема и в кластере из очень мощной машины. Не будем судить по первым результатам. Сначала посмотрим на выходной ка- талог в распределенной файловой системе HDFS: In:!hdfs dfs -ls /tmp/mr.out Out: Found 2 items-rw-r--r-- 1 vagrant supergroup 0 2017-06-17 06:14 /tmp/mr.out/_SUCCESS-rw-r--r-- 1 vagrant supergroup 33 2017-06-17 06:14 /tmp/mr.out/part-00000 Там имеются два файла: первый, пустой, под названием _SUCCESS . Он указывает, что задание MapReduce закончило этап записи в каталог; второй называется part- 00000 и содержит фактические результаты (поскольку мы работаем на узле всего с одним редуктором). Прочитав этот файл, мы получим конечные результаты: In:!hdfs dfs -cat /tmp/mr.out/part-00000 Out: chars 1335lines 31words 179\n--- Страница 296 ---\nЭкосистема Hadoop  295 Как и ожидалось, они совпадают с конвейерной командной строкой, показан- ной ранее. Несмотря на то что утилита Hadoop Streaming концептуально простая, она не самый лучший способ выполнения заданий Hadoop с использованием программ- ного кода Python. Для этого на PyPy существует много других библиотек, и здесь мы представим одну из самых гибких и поддерживаемых общедоступных биб - лиот ек с открытым исходным кодом – MrJob. Она позволяет беспрепятственно выполнять задания на локальной машине, кластере Hadoop или тех же облачных кластерных средах, таких как Эластичный MapReduce компании Amazon; она объ-единяет весь исходный код в один автономный файл, даже если необходимы мно-гочисленные шаги MapReduce (к примеру, в итеративных алгоритмах), и интер-претирует ошибки Hadoop в ходе выполнения исходного кода. Кроме того, данная библиотека очень легко устанавливается; чтобы библиотека MrJob имелась на ло-кальной машине, просто примените команду pip install mrjob . Хотя библиотека MrJob является замечательным образцом программного обес - печения, она не очень хорошо работает с блокнотами Jupyter, поскольку требует основной функции в качестве точки входа. Нам придется записать Python’овский программный код MapReduce в отдельный файл и затем выполнить его в команд-ной строке. Начнем с примера, который мы уже встречали много раз: подсчет символов, слов и строк в файле. Сначала напишем файл Python с использованием функцио-нала библиотеки MrJob; трансформаторы и редукторы обернуты в подкласс MRJob . Входные данные не считываются из стандартного потока stdin, а передаются как аргумент функции, а конечные результаты не распечатываются, а выдаются в ле-нивом режиме (или возвращаются). Благодаря библиотеке MrJob целая программа MapReduce становится всего не- сколькими строками программного кода: In: with open(\"MrJob_job1.py\", \"w\") as fh: fh.write(\"\"\"from mrjob.job import MRJob class MRWordFrequencyCount(MRJob): def mapper(self, _, line): yield \"chars\", len(line) yield \"words\", len(line.split()) yield \"lines\", 1 def reducer(self, key, values): yield key, sum(values) if __name__ == '__main__': MRWordFrequencyCount.run() \"\"\") Теперь давайте выполним этот программный код локально (с локальной вер- сией набора данных). Библиотека MrJob, помимо выполнения шагов трансформа- тора и редуктора (в данном случае локально), распечатывает результат и очищает временный каталог:\n--- Страница 297 ---\n296  Распре деленные среды – Hadoop и Spark In: !python MrJob_job1.py /datasets/hadoop_git_readme.txt Out: Streaming final output from /tmp/MrJob_job1.vagrant.20170617.061809.672548/output \"chars\" 1335 \"lines\" 31 \"words\" 179 Removing temp directory /tmp/MrJob_job1.vagrant.20170617.061809.672548 Для выполнения того же самого процесса в Hadoop просто запустите этот же файл Python, но на этот раз вставив в командной строке опцию –r hadoop , и MrJob автоматически выполнит его при помощи реализованной в среде Hadoop вычис - лительной парадигмы MapReduce и с использованием распределенной файловой системы HDFS. В данном случае следует помнить, что необходимо указать путь hdfs входного файла: In:!python MrJob_job1.py -r hadoop hdfs:///datasets/hadoop_git_readme.txt Out: Job job_1497616070323_0010 running in uber mode : false map 0% reduce 0% map 50% reduce 0% map 100% reduce 0% map 100% reduce 100%Job job_1497616070323_0010 completed successfully Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 Streaming final output from hdfs:///user/vagrant/tmp/mrjob/MrJob_job1.vagrant.20170617.061919.506254/output \"chars\" 1335 \"lines\" 31 \"words\" 179 Removing HDFS temp directory hdfs:///user/vagrant/tmp/mrjob/MrJob_job1.vagrant.20170617.061919.506254 Removing temp directory /tmp/MrJob_job1.vagrant.20170617.061919.506254 Используя утилиту Hadoop Streaming из командной строки, вы увидите те же самые выходные данные, что и ранее, плюс результаты. В данном случае времен-ный каталог распределенной файловой системы HDFS, используемый для хране-ния результатов, после завершения задания удаляется. Теперь, чтобы убедиться в гибкости библиотеки MrJob, попробуем выполнить процесс, который требует больше одного шага MapReduce. Его выполнение из ко-мандной строки было бы очень трудно осуществить; фактически пришлось бы\n--- Страница 298 ---\nЭкосистема Hadoop  297 выполнить первую итерацию MapReduce, проверить ошибки, прочитать резуль- таты и затем выполнить вторую итерацию, снова проверить ошибки и наконец прочитать результаты. Такой процесс требует больших временных затрат и под-вержен ошибкам. Благодаря библиотеке MrJob эта работа становится очень прос - той: в программном коде можно создать каскад операций MapReduce, где каждый выход является входом на следующий этап. В качестве примера теперь отыщем наиболее распространенное слово в сочи- нениях Уильяма Шекспира (используя в качестве входных данных файл, состоя-щий из 125K строк). Эту операцию невозможно выполнить за один шаг MapReduce; требуются, по крайней мере, два таких шага. Мы реализуем очень простой алго-ритм на основе двух итераций MapReduce: фрагментатор данных: входной файл разбивается по каждой строке, что в библиотеке MrJob тоже является операцией по умолчанию; этап 1 – трансформация: по каждому слову выдается кортеж «ключ-зна че- ние»; клю ч – это слово строчными буквами, значение – это всегда 1; этап 1 – редукция: по каждому ключу (слово строчными буквами) суммиру - ются все значения. Выход сообщит, сколько раз слово появляется в тексте; этап 2 – трансформация: во время этого шага мы переворачиваем кортежи «ключ-значение» и помещаем их как значения новой пары ключей. Чтобы назначить одному редуктору все кортежи, мы присваиваем каждому выход-ному кортежу одинаковый ключ None; этап 2 – редукция: просто отбрасываем единственный имеющийся ключ и извлекаем максимум из значений, в частности извлекаем максимум из всех кортежей (количество, слово). In:with open(\"MrJob_job2.py\", \"w\") as fh: fh.write(\"\"\" from mrjob.job import MRJob from mrjob.step import MRStepimport re WORD_RE = re.compile(r\"[\\w']+\")class MRMostUsedWord(MRJob): def steps(self): return [ MRStep(mapper=self.mapper_get_words, reducer=self.reducer_count_words), MRStep(mapper=self.mapper_word_count_one_key, reducer=self.reducer_find_max_word) ] def mapper_get_words(self, _, line): # выдавать по одному слову в строке for word in WORD_RE.findall(line): yield (word.lower(), 1) def reducer_count_words(self, word, counts): # отправить все пары (слово, число появлений) в тот же редуктор. yield (word, sum(counts))\n--- Страница 299 ---\n298  Распре деленные среды – Hadoop и Spark def mapper_word_count_one_key(self, word, counts): # отправить все кортежи в тот же редуктор yield None, (counts, word) def reducer_find_max_word(self, _, count_word_pairs): # каждый элемент пары word_count_pairs, т. е. # слово-частота – это кортеж (частота, слово) yield max(count_word_pairs) if __name__ == '__main__': MRMostUsedWord.run()\"\"\") Далее можно выбрать, исполнить этот сценарий локально или на кластере Hadoop, получив тот же самый результат: наиболее распространенным словом в сочинениях Уильяма Шекспира является слово the, которое использовалось бо- лее 27K раз. В этом фрагменте программного кода требуется просто вывести ре-зультат, и поэтому мы запускаем задание с опцией -quiet : In:!python MrJob_job2.py --quiet /datasets/shakespeare_all.txt Out: 27801 \"the\" In: !python MrJob_job2.py -r hadoop --quiet hdfs:///datasets/shakespeare_all.txt Out: 27801 \"the\" Менеджер ресурсов YARN С выходом Hadoop 2 (текущее ответвление по состоянию на 2016 г.) был введен новый слой поверх распределенной файловой системы HDFS, который обеспечи-вает выполнение многочисленных приложений. Например, MapReduce является одним из них (который предназначается для пакетной обработки данных). Этот слой называется Yet Another Resourc e Negotiator (Y ARN, или «еще один посред- ник между ресурсами и кластером»), и его задача заключается в управлении рас - пределением ресурсов в кластере. Менеджер YARN придерживается парадигмы «ведущий-ведомый» и состоит из двух служб: менеджера ресурсов и менеджера узла. Менеджер ресурсов – это ведущий процесс, который отвечает за две задачи: планирование (выделение ресурсов) и управление приложениями (передача за-даний и отслеживание их состояния). Каждый менеджер узла, согласно архитекту - ре, является ведомым и представляет собой предваряющую рабочий процесс (pre-worker) платформу, которая выполняет задачи и сообщает результаты менеджеру ресурсов. Введенный вместе с Hadoop 2 слой YARN гарантирует следующее: муль тиарендность, т. е. предоставление многочисленных механизмов, ко- торые используют Hadoop; ус овершенствованное использование кластера, так как выделение задач яв- ляется динамическим и планируемым;\n--- Страница 300 ---\nПлатформа Spark  299 продвину тая масштабируемость; YARN не предоставляет обрабатывающего алгоритма, он является просто менеджером ресурсов кластера; совмес тимость с MapReduce (более высокий уровень, чем Hadoop 1). платфОрма SPark Ставшая за последние несколько лет очень популярной платформа Apache Spark является результатом эволюции платформы Hadoop. В противоположность Ha - doop и ее Java-ориентированной и пакетно-центрированной конструкции, плат - форма Spark способна быстро и легко продуцировать итеративные алгоритмы. Кроме того, она имеет очень богатый комплект API для языков параллельного программирования и исходно поддерживает много разных типов обработки дан-ных (машинное обучение, потоковая передача, анализ графов, SQL и т. д.). Apache Spark – это кластерная платформа, разработанная для быстрой и обще- целевой обработки больших данных. Одно из усовершенствований в скорости вызвано тем фактом, что после выполнения каждого задания данные остаются в оперативной памяти и не сохраняются в файловой системе (если вы, разумеет - ся, не хотите обратного), подобно тому как это происходит с Hadoop, MapReduce и HDFS. Эта особенность делает итеративные задания (такие как алгоритм клас - теризации K-средних) гораздо быстрее, поскольку задержка и пропускная спо- собность, обеспечиваемые оперативной памятью, более производительны, чем у физического диска. Отсюда кластеры, на которых работает платформа Spark, нуждаются в большом объеме оперативной памяти для каждого узла. Несмотря на то что платформа Spark была разработана на языке Scala (который работает в JVM, как Java), она имеет API для языков параллельного программиро-вания, включая Java, Scala, Python и R. В этой книге мы сосредоточимся на Python. Платформа Spark может работать двумя разными способами: в авт ономном режиме, при этом она работает на вашей локальной машине. В этом случае максимальная параллелизация определяется числом ядер ло-кальной машины, и доступный объем оперативной памяти точно такой же, что и у локальной машины; в к ластерном режиме, при этом она работает на кластере из многочислен- ных узлов, используя менеджер кластера, в частности YARN. В этом случае максимальная параллелизация определяется числом ядер во всех состав-ляющих кластер узлах, и объем оперативной памяти является суммой объ-емов оперативной памяти каждого узла. Библиотека pySpark Чтобы воспользоваться функционалом платформы Spark (либо библиотеки pySpark, содержащей программные интерфейсы Python для платформы Spark), необходи-мо инстанцировать специальный объект под названием SparkContext. Он сообщает платформе Spark способ получения доступа к кластеру и содержит некоторые спе - цифические д ля приложения параметры. В содержащемся в виртуальной машине блокноте Jupyter данная переменная уже имеется и называется sc (эта опция задана по умолчанию, когда блокнот Jupyter запущен); давайте посмотрим, что он содержит. Сначала откройте новый блокнот Jupyter. Когда он будет готов к использова- нию, в первой ячейке наберите следующее:\n--- Страница 301 ---\n300  Распреде ленные среды – Hadoop и Spark In: sc._conf.getAll() Out: [(u'spark.rdd.compress', u'True'), (u'spark.master', u'yarn-client'), (u'spark.serializer.objectStreamReset', u'100'), (u'spark.yarn.isPython', u'true'), (u'spark.submit.deployMode', u'client'), (u'spark.executor.cores', u'2'), (u'spark.app.name', u'PySparkShell')] Эта переменная содержит многочисленную информацию, и самыми важ - ными ее элементами являются параметр spark.master , в этом случае установлен в ка честве к лиента в YARN, spark.executor.cores установлен в 2 по числу модулей CPU виртуальной машины, и spark.app.name – имя приложения. Имя приложения пригодится, в частности, когда (YARN-)кластер будет использоваться совместно; перей дя по адресу http:/127.0.0.1:8088 , можно проверить состояние приложения: В платформе Spark используется модель данных под названием отказоустой- чивый распределенный набор данных (resilient distributed dataset, RDD). Это распределенный набор элементов, который может обрабатываться параллель- но. Набор элементов RDD может быть создан из существующего набора (список Python, например) или из внешнего набора данных, хранящегося в виде файла на локальной машине, в распределенной файловой системе HDFS или же в других источниках. Теперь давайте создадим RDD-набор, содержащий целые числа от 0 до 9. Для этого можно воспользоваться методом parallelize , предоставляемым объектом SparkContext : In: numbers = range(10)numbers_rdd = sc.parallelize(numbers)numbers_rdd Out: ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423 Как видно, нельзя просто взять и распечатать содержимое RDD-набора, по- скольку он разделен на многочисленные разделы (и распределен в кластере). Чис - ло разделов по умолчанию в два раза больше числа модулей CPU (т. е. 4 в предо- ставленной виртуальной машине), но его можно установить вручную при помощи второго аргумента метода parallelize .\n--- Страница 302 ---\nПлатформа Spark  301 Чтобы распечатать данные, содержащиеся в RDD-наборе, необходимо вызвать метод collect . Отметим, что этот метод, выполненный на кластере, собирает все данные на узле, поэтому узел должен располагать достаточным объемом памяти, чтобы они все уместились: In:numbers_rdd.collect() Out: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Для оперативного просмотра по частям используйте метод take, указав, сколько элементов вы хотели бы увидеть. Отметим, что, поскольку мы имеем дело с рас - пределенным набором данных, не гарантируется, что элементы в нем находятся в том же порядке, в каком их вставляли: In:numbers_rdd.take() Out: [0, 1, 2, 3] Чтобы прочесть текстовый файл, можно воспользоваться методом textFile , предоставляемым контекстом платформы Spark. Этот метод позволяет читать файлы HDFS и локальные файлы, при этом он разделяет текст по символу новой строки, поэтому первый элемент RDD-набора (применяя метод first ) является первой строкой текстового файла. Отметим, что при использовании локального пути все узлы, которые составляют кластер, должны обращаться к тому же самому файлу через тот же самый путь: In:sc.textFile(\"hdfs:///datasets/hadoop_git_readme.txt\").first() Out: u'For the latest information about Hadoop, please visit ourwebsite at:' In: sc.textFile(\"file:///home/vagrant/datasets/hadoop_git_readme.txt\").first() Out: u'For the latest information about Hadoop, please visit ourwebsite at:' ' Чтобы сохранить содержимое RDD-набора на диске, можно воспользоваться методом saveAsTextFile , предоставляемым моделью данных RDD. Здесь можно ис - пользовать несколько мест назначения. В приведенном ниже примере мы сохра-ним его в распределенной файловой системе HDFS и затем выведем содержимое на экран: In:numbers_rdd.saveAsTextFile(\"hdfs:///tmp/numbers_1_10.txt\") In: !hdfs dfs -ls /tmp/numbers_1_10.txt\n--- Страница 303 ---\n302  Распре деленные среды – Hadoop и Spark Out: Found 5 items-rw-r--r-- 1 vagrant supergroup 0 2017-06-17 16:49 /tmp/numbers_1_10.txt/_SUCCESS-rw-r--r-- 1 vagrant supergroup 4 2017-06-17 16:49 /tmp/numbers_1_10.txt/part-00000-rw-r--r-- 1 vagrant supergroup 4 2017-06-17 16:48 /tmp/numbers_1_10.txt/part-00001-rw-r--r-- 1 vagrant supergroup 4 2017-06-17 16:49 /tmp/numbers_1_10.txt/part-00002-rw-r--r-- 1 vagrant supergroup 8 2017-06-17 16:48 /tmp/numbers_1_10.txt/part-00003 Платформа Spark пишет один файл для каждого раздела, в точности как Map - Reduc e, который пишет один файл для каждого редуктора. Такой способ записи ускоряет время сохранения, поскольку каждый раздел сохраняется независимо, но на кластере с 1 узлом этот способ утяжеляет операцию чтения. Можно ли собрать все разделы в один перед записью файла или, в общем слу - чае, можно ли снизить число разделов в RDD-наборе? Ответ да, это можно сделать посредством метода coalesce , предоставляемого моделью данных RDD, передав в качестве аргумента число разделов, которые мы хотели бы иметь. Если пере-дать 1, то это вынудит RDD-набор находиться в автономном разделе и во время сохранения произведет всего один выходной файл. Отметим, что это происходит даже во время сохранения в локальной файловой системе: для каждого раздела создается файл. Следует учитывать, что выполнение этого метода в кластерной среде, состоящей из многочисленных узлов, не гарантирует, что все узлы будут видеть те же самые выходные файлы: In:numbers_rdd.coalesce(1) .saveAsTextFile(\"hdfs:///tmp/numbers_1_10_one_file.txt\") In: !hdfs dfs -ls /tmp/numbers_1_10_one_file.txt Out: Found 2 items-rw-r--r-- 1 vagrant supergroup 0 2017-06-17 16:49 /tmp/numbers_1_10_one_file.txt/_SUCCESS-rw-r--r-- 1 vagrant supergroup 20 2017-06-17 16:49 /tmp/numbers_1_10_one_file.txt/part-00000 In: !hdfs dfs -cat /tmp/numbers_1_10_one_file.txt/part-00000 Out: 01 23 4 5 6 78 9 In: numbers_rdd.saveAsTextFile(\"file:///tmp/numbers_1_10.txt\")\n--- Страница 304 ---\nПлатформа Spark  303 In: !ls /tmp/numbers_1_10.txt Out: part-00000 part-00001 part-00002 part-00003 _SUCCESS Модель данных RDD поддерживает всего два типа операций: преобразования трансформируют набор данных в другой вид. Входами и вы- ходами преобразований являются RDD-наборы, поэтому многочисленные преобразования можно объединять в цепь, приближаясь к функционально-му стилю программирования. Более того, все преобразования ленивые, т. е. они не вычисляют своих результатов сразу же; действия возвращают значения из RDD-наборов, в частности сумму эле- ментов и количества, или просто собирают все элементы. Действия являют - ся триггером для выполнения цепочки (ленивых) преобразований, так как наличие выходных данных обязательно. Типичные программы платформы Spark представляют собой цепочку преобра- зований с действием в конце. По умолчанию все преобразования на RDD-наборе выполняются всегда, когда вы выполняете действие (т. е. промежуточное состоя - ние пос ле каждого трансформатора не сохраняется). Однако такое поведение можно переопределять при помощи метода persist (на RDD-наборе) всегда, когда вы хотите кэшировать значение преобразованных элементов. Метод persist обес - печивает постоянное хранение данных в памяти и на диске. В следующем примере мы возведем в квадрат все значения в RDD-наборе и затем их просуммируем; этот алгоритм может быть выполнен посредством трансформа- тора (возведение элементов в квадрат), за которым следует редуктор (суммирова-ние массива). В соответствии с платформой Spark метод map является трансфор- матором, поскольку он просто выполняет поэлементное преобразование, а метод reduce – действием, поскольку он генерирует значение из всех элементов. Давайте подойдем к решению этой задачи в пошаговом режиме, чтобы уви- деть многочисленные способы, которыми мы можем действовать. Прежде всего начнем с преобразования: сначала мы определим функцию, которая возвращает квадрат входного аргумента, затем передадим эту функцию методу map в модели данных RDD и наконец соберем элементы в RDD-набор: In: def sq(x): return x**2 numbers_rdd.map(sq).collect()Out: [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] Несмотря на правильность результата, функция sq занимает много места. Бла- годаря лямбда-выражению языка Python можно переписать преобразование бо- лее сжато; оно будет выглядеть следующим образом: In: numbers_rdd.map(lambda x: x**2).collect() Out: [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n--- Страница 305 ---\n304  Распре деленные среды – Hadoop и Spark Напомним, почему для распечатывания значения в преобразованном RDD- наборе нужно вызывать метод collect . Это вызвано тем, что метод map не пере- ходит к действию, а просто лениво оценивается. С другой стороны, метод reduce является действием, и поэтому добавление шага редукции к предыдущему RDD- набору обязательно сгенерирует значение. Как и в отношении метода map, метод reduce берет в качестве аргумента функцию, которая должна иметь два аргумента (левое и правое значения) и должна вернуть значение. И даже в этом случае дан-ная функция может быть многословной, если определять ее при помощи опера-тора def или функции lambda : In:numbers_rdd.map(lambda x: x**2).reduce(lambda a,b: a+b) Out: 285 Чтобы сделать ее еще проще, вместо редуктора можно воспользоваться дей- ствием суммирования: In:numbers_rdd.map(lambda x: x**2).sum() Out: 285 Мы пока показали очень простой пример работы с библиотекой pySpark. Давай- те разберемся, что происходит в этот момент под капотом: сначала загружается набор данных и распределяется по всему кластеру, затем в распределенной сре-де выполняется операция преобразования, и потом все разделы сворачиваются, чтобы сгенерировать конечный результат (операция sum либо метод reduce ), кото - рый в итоге распечатывается в блокноте Jupyter. Таким образом, задача огромной сложности сверхупрощается библиотекой pySpark. Теперь продвинемся еще на один шаг и задействуем пары «ключ-значение»; хотя RDD-наборы могут содержать любой вид объектов (до сих пор мы видели целые числа и текстовые строки), кое-какие операции можно выполнить, когда элементами являются кортежи, состоящие из двух элементов: ключа и значения. Чтобы проиллюстрировать на примере, давайте сначала сгруппируем числа в RDD-наборе на четные и нечетные и затем вычислим сумму этих двух групп по отдельности. Как и в отношении модели MapReduce, было бы неплохо промарки-ровать каждое число ключом ( even и odd) и затем выполнить редукцию по каждому ключу с использованием операции суммирования. Можно начать с операции преобразования: сначала создадим функцию, ко- торая тегирует числа, выводя even , если число в аргументе четное, и odd в про- тивном случае. Затем создадим преобразование «ключ-значение», которое создает пару «ключ-значение» для каждого числа, где ключ – это тег, а значе-ние – само число: In:def tag(x): return 'even' if x%2==0 else 'odd' numbers_rdd.map(lambda x: (tag(x), x) ).collect()\n--- Страница 306 ---\nПлатформа Spark  305 Out: [('even', 0), ('odd', 1), ('even', 2), ('odd', 3), ('even', 4), ('odd', 5), ('even', 6), ('odd', 7), ('even', 8), ('odd', 9)] Чтобы выполнить редукцию (свертку) каждого ключа по отдельности, теперь можно воспользоваться методом reduceByKey (который в платформе Spark не яв- ляется действием). В качестве аргумента мы должны передать функцию, которую следует применить ко всем значениям, поставленным в соответствие каждому ключу; в данном случае мы их все просуммируем. Наконец, необходимо вызвать метод collect , чтобы распечатать результаты: In: numbers_rdd.map(lambda x: (tag(x), x) ).reduceByKey( lambda a,b: a+b).collect() Out: [('even', 20), ('odd', 25)] Теперь давайте перечислим некоторые самые важные методы, которые имеют - ся в платформе Spark; это не исчерпывающее руководство, но оно содержит самые используемые из них. Начнем с преобразований; они могут применяться к RDD-набору, и они гене- рируют RDD-набор: map(функция) : возвращает RDD-набор, формируемый в результате прохожде- ния каждого элемента через функцию; flatMap(функция) : возвращает RDD-набор, формируемый в результате упло- щения (линеаризации) результата функции для каждого элемента входно-го RDD-набора. Используется, когда каждое значение на входе может быть преобразовано в 0 или более элементов на выходе. Например, для подсчета количества вхождений каждого слова в текст мы долж - ны преобразовать каждое слово в пару «ключ-значение» (слово будет ключом, а единица – значением), благодаря этому сгенерировав более одного элемента «ключ-значение» для каждой входной строки текста; filter(функция) : возвращает набор данных, состоящий из всех значений, где функция возвращает истину; sample(с_заменой, доля, начальное число) : извлекает бутстрап-выборку из RDD- набора, позволяя создавать выборочный RDD-набор (с заменой или без), чья длина составляет долю входного набора данных; начальное число соот - ветствует начальному числу (seed) для генератора случайных чисел; distinct() : возвращает RDD-набор, содержащий уникальные элементы входного RDD-набора; coalesce(число_разделов) : сокращает число разделов в RDD;\n--- Страница 307 ---\n306  Распре деленные среды – Hadoop и Spark repartition(число_разделов) : изменяет число разделов в RDD. Этот метод всег - да перетасовывает все данные по сети; groupByKey() : создает RDD-набор, где каждому ключу соответствует значе- ние, являющееся последовательностью значений, которые имеют этот ключ во входном наборе данных; reduceByKey(функция) : агрегирует входной RDD-набор по ключу и затем при- меняет функцию reduce к значениям каждой группы; sortByKey(по_возрастанию) : сортирует элементы в RDD-наборе по ключу в вос - ходящем или нисходящем порядке; union(другой_RDD) : объединяет два RDD-набора; intersection(другой_RDD) : возвращает RDD-набор, состоящий только из зна- чений, встречающихся и во входном RDD-наборе, и в RDD-наборе, передан-ном в качестве аргумента; join(другой_RDD) : возвращает набор данных, где входные пары «ключ- значение» присоединены (по ключу) к RDD-набору, переданному в качестве аргумента. Так же как в SQL, помимо функции join, также имеются следующие методы: cartesian , leftOuterJoin , rightOuterJoin и fullOuterJoin . Теперь давайте получим общее представление о наиболее популярных дей- ствиях, которые имеются в библиотеке pySpark. Отметим, что действия иниции-руют обработку RDD-набора всеми находящимися в цепочке преобразователями: reduce(функция) : агрегирует элементы в RDD-наборе, генерируя выходное значение; count() : возвращает количество элементов в RDD-наборе; countByKey() : возвращает словарь Python, где каждому ключу поставлено в соответствие число элементов в RDD-наборе с этим ключом; collect() : возвращает все элементы в трансформированном RDD-наборе локально; first() : возвращает первое значение в RDD-наборе; take(N) : возвращает первые N значений в RDD-наборе; takeSample(с_заменой, N, начальное_число) : возвращает бутстрап-выборку из N элементов в RDD-наборе с заменой или без, возможно, при помощи на- чального числа для генератора случайных чисел, передаваемого в качестве аргумента; takeOrdered(N, упорядочивание) : возвращает верхние N элементов в RDD-на - бо ре пос ле его сортировки по значению (в восходящем или нисходящем порядке); saveAsTextFile(путь) : сохраняет RDD-набор как набор текстовых файлов в ука занном каталоге. Кроме того, имеется несколько методов, которые не являются ни преобразова- телями, ни действиями: cache() : кэширует элементы RDD-набора; поэтому в будущих расчетах на основе того же самого RDD-набора он может использоваться повторно в ка-честве отправной точки; persist(хранилище) : то же самое, что и метод cache , но имеется возможность указать устройство, куда сохранять элементы RDD-набора (оперативная па-мять, диск либо оба этих устройства); unpersist() : отменяет операцию сохранения либо кэширования.\n--- Страница 308 ---\nПлатформа Spark  307 Давайте теперь попробуем реплицировать примеры, которые мы видели в раз- деле о реализации вычислительной парадигмы MapReduce в среде Hadoop. В плат - форме Spark алгоритм должен быть следующим: 1. Вхо дной файл считывается и параллелизируется на RDD-наборе. Эта опера- ция может быть выполнена при помощи метода textFile , предоставляемого контекстом Spark. 2. Для каждой строки входного файла возвращаются три пары «ключ-значение»: одна содержит число символов, другая – число слов, и последняя – число строк. В платформе Spark эта операция выполняется методом flatMap , так как три результата генерируются для каждой входной строки. 3. Для каждого ключа суммируются все значения. Это может быть сделано при помощи метода reduceByKey . 4. В зак лючение собираются результаты. В данном случае можно применить метод collectAsMap , который собирает пары «ключ-значение» в RDD-набор и возвращает словарь Python. Отметим, что эта операция является действи-ем, поэтому на RDD-наборе исполняется цепочка преобразований и возвра-щается результат. In: def emit_feats(line): return [('chars', len(line)), ('words', len(line.split())), ('lines', 1)] print(sc.textFile('/datasets/hadoop_git_readme.txt') .flatMap(emit_feats) .reduceByKey(lambda a,b: a+b) .collectAsMap()) Out: {'chars': 1335, 'lines': 31, 'words': 179} Сразу же можно отметить огромную скорость этого метода по сравнению с реа - лизацией MapR educe. Это вызвано тем, что весь набор данных хранится в опера- тивной памяти, а не в распределенной файловой системе HDFS. Во-вторых, это чистая реализация на Python, и нам не нужно вызывать внешние команды или библиотеки – библиотека pySpark автономна. Теперь давайте для примера поработаем на более крупном файле с сочинения- ми Уильяма Шекспира и извлечем самое популярное слово. В реализации Hadoop MapReduce эта задача требует двух шагов трансформации-редукции и, следова-тельно, четырех операций записи-чтения в HDFS. В библиотеке pySpark все это можно сделать в RDD-наборе. 1. Вхо дной файл считывается и параллелизируется на RDD-наборе при помо- щи метода textFile . 2. Из каж дой строки извлекаются все слова. Для этой операции можно восполь- зоваться методом flatMap и регулярным выражением. 3. Каждое слово в тексте (т. е. каждый элемент RDD-набора) теперь преобразу - ется в пару «ключ-значение»: ключ – это слово строчными буквами, а значе-ние всегда равняется единице. Это операция преобразования. 4. При помощи вызова метода reduceByKey подсчитывается, сколько раз каждое слово (ключ) появляется в тексте (RDD-наборе). На выходе получаем пары\n--- Страница 309 ---\n308  Распре деленные среды – Hadoop и Spark «ключ-значение», где ключ – это слово, а значение – количество вхождений слова в текст. 5. Переворачиваем к лючи и значения, создавая новый RDD-набор. Это опера- ция преобразования. 6. Сор тируем RDD-набор в порядке убывания и извлекаем (take) первый эле- мент. Это действие, и оно может быть выполнено одной операцией при по-мощи метода takeOrdered . In:import reWORD_RE = re.compile(r\"[\\w']+\") print(sc.textFile('/datasets/shakespeare_all.txt') .flatMap(lambda line: WORD_RE.findall(line)) .map(lambda word: (word.lower(), 1)) .reduceByKey(lambda a,b: a+b) .map(lambda (k,v): (v,k)) .takeOrdered(1, key = lambda x: -x[0])) Out: [(27801, u'the')] Конечные результаты такие же, что и при использовании платформы Hadoop и вычислительной парадигмы MapReduce, но в данном случае все вычисление занимает намного меньше времени. В действительности можно еще больше улучшить это решение, свернув второй и третий шаги (обработав пару «ключ-значение» для каждого слова методом flatMap , где ключ – это слово строчными буквами, а значение – количество появлений слова), а также пятый и шестой шаги (взяв первый элемент и упорядочив элементы в RDD-наборе по их значению, т. е. второму элементу пары): In:print(sc.textFile('/datasets/shakespeare_all.txt') .flatMap(lambda line: [(word.lower(), 1) for word in WORD_RE.findall(line)]) .reduceByKey(lambda a,b: a+b) .takeOrdered(1, key = lambda x: -x[1])) Out: [(u'the', 27801)] Для проверки состояния обработки можно воспользоваться веб-интерфейсом платформы Spark: это графический интерфейс, который пошагово показывает за-дания, выполняемые платформой Spark. Для доступа к веб-интерфейсу необхо-димо сначала выяснить имя приложения pySpark Jupyter, отыскав его имя в окне оболочки bash в том месте, где вы запустили блокнот (обычно имя приложения имеет форму application_<число>_<число> ), и затем направить свой браузер на стра- ницу http://localhost:8088/proxy/application_<число>_<число>1. 1 Имя приложения можно выяснить на странице приложений http://localhost: 8088/cluster . – Прим. перев.\n--- Страница 310 ---\nРезюме  309 Будет получен результат, который аналогичен показанному на следующем ниже изображении. Он содержит все задания, выполненные в платформе Spark (в качестве ячеек блокнота Jupyter), при этом можно также визуализировать план исполнения в виде направленного ацик лического графа (DAG): резюме В этой главе мы представили некоторые примитивы, которые позволяют выпол-нять распределенные задания на кластере, состоящем из многочисленных уз-лов. Мы продемонстрировали платформу Hadoop и все ее компоненты, функции и ограничения, а затем проиллюстрировали работу платформы Spark. В следующей главе мы более детально остановимся на платформе Spark и по- кажем, каким образом можно реализовывать проекты в области науки о данных в распределенной среде.",
      "debug": {
        "start_page": 274,
        "end_page": 310
      }
    },
    {
      "name": "Глава 9. Практическое машинное обучение в среде Spark 310",
      "content": "--- Страница 311 --- (продолжение)\nГлава 9 Практическое машинное обучение в среде Spark В предыдущей главе мы познакомились с основным функционалом обработки данных при помощи платформы Spark. В этой главе в центре внимания будет ре-шение реальных задач в области науки о данных при помощи платформы Spark. По ходу настоящей главы вы изучите следующие темы: как распространять переменные по всем узлам кластера; как с оздавать объекты DataFrame из структурированных (CSV) и полуструк - турированных (JSON) файлов, сохранять их на диске и загружать c диска; как испо льзовать SQL-подобный синтаксис, чтобы выполнять выборку, фильт - рацию, соединение, группировку и агрегацию данных, тем самым чрезвы- чайно упрощая предобработку; как обращаться с пропущенными данными в наборе данных; какие в платформе Spark имеются готовые к использованию алгоритмы для конструирования признаков и как их применять в реальном сценарии; какие ес ть ученики и как измерять их результативность в распределенной среде; как выпо лнять перекрестную проверку для гиперпараметрической опти- мизации в кластере. настр Ойка виртуальнОй машины для даннОй главы Поскольку машинное обучение нуждается в больших вычислительных мощно-стях, в настоящей главе, чтобы сэкономить немного ресурсов (в особенности опе-ративную память), мы будем использовать среду Spark без поддержки менеджера ресурсов YARN. Этот операционный режим называется автономным и создает узел Spark без кластерного функционала; вся обработка будет на драйверной (ве-дущей) машине и не будет разделенной. Не переживайте: программный код, ко-торый мы увидим в этой главе, будет работать и в кластерной среде тоже. Для того чтобы начать работать в такого рода режиме, необходимо выполнить следующие шаги: 1) вклю чить виртуальную машину командой vagrant up; 2) полу чить доступ к виртуальной машине, когда она будет готова, при помощи команды vagrant ssh;\nГлава 9 Практическое машинное обучение в среде Spark В предыдущей главе мы познакомились с основным функционалом обработки данных при помощи платформы Spark. В этой главе в центре внимания будет ре-шение реальных задач в области науки о данных при помощи платформы Spark. По ходу настоящей главы вы изучите следующие темы: как распространять переменные по всем узлам кластера; как с оздавать объекты DataFrame из структурированных (CSV) и полуструк - турированных (JSON) файлов, сохранять их на диске и загружать c диска; как испо льзовать SQL-подобный синтаксис, чтобы выполнять выборку, фильт - рацию, соединение, группировку и агрегацию данных, тем самым чрезвы- чайно упрощая предобработку; как обращаться с пропущенными данными в наборе данных; какие в платформе Spark имеются готовые к использованию алгоритмы для конструирования признаков и как их применять в реальном сценарии; какие ес ть ученики и как измерять их результативность в распределенной среде; как выпо лнять перекрестную проверку для гиперпараметрической опти- мизации в кластере. настр Ойка виртуальнОй машины для даннОй главы Поскольку машинное обучение нуждается в больших вычислительных мощно-стях, в настоящей главе, чтобы сэкономить немного ресурсов (в особенности опе-ративную память), мы будем использовать среду Spark без поддержки менеджера ресурсов YARN. Этот операционный режим называется автономным и создает узел Spark без кластерного функционала; вся обработка будет на драйверной (ве-дущей) машине и не будет разделенной. Не переживайте: программный код, ко-торый мы увидим в этой главе, будет работать и в кластерной среде тоже. Для того чтобы начать работать в такого рода режиме, необходимо выполнить следующие шаги: 1) вклю чить виртуальную машину командой vagrant up; 2) полу чить доступ к виртуальной машине, когда она будет готова, при помощи команды vagrant ssh;\n--- Страница 312 ---\nРаспространение переменных по всем узлам кластера  311 3) запус тить автономный режим платформы Spark вместе с блокнотом Jupyter изнутри виртуальной машины при помощи ./start_jupyter.sh ; 4) открыть браузер и направить его по URL-адресу http://localhost:8888 . Чтобы его выключить, используйте клавиши Ctrl+C для выхода из блокнота Jupyter и команду vagrant halt для выключения виртуальной машины.  Отметим, что даже в такой конфигурации можно получить доступ к веб-интерфейсу Spark (когда, как минимум, работает блокнот Jupyter) по следующему URL-адресу: http:// localhost:4040. распр Остранение переменных пО всем узлам кластера Когда мы работаем в распределенной среде, иногда требуется делиться инфор- мацией между узлами, чтобы все узлы могли работать, используя согласованные переменные. Платформа Spark регулирует этот случай, обеспечивая два вида пе-ременных: переменные только для записи и переменные только для чтения. Не гарантируя больше, что разделяемая переменная доступна одновременно и для чтения, и для записи, платформа Spark также отказывается от необходимости в обеспечении согласованности, перекладывая всю тяжелую работу по управле-нию такой ситуацией на плечи разработчика. Как правило, решение достигается быстро, так как платформа Spark на самом деле является гибкой и настраиваемой средой. Широковещательные переменные только для чтения В нашей конфигурации широковещательные переменные – это переменные, ко-торые распространяются драйверным узлом, т. е. узлом, выполняющим блокнот Jupyter, по всем узлам в кластере. Такая переменная доступна только для чтения, поскольку она распространяется одним узлом и никогда не считывается назад, если другой узел ее изменяет. Теперь на простом примере посмотрим, как это работает: мы применим пря- мое унитарное кодирование к набору данных с информацией о половой принад-лежности в виде сроки. Если быть точным, фиктивный набор данных содержит всего один признак, который может иметь три значения: мужчина M, женщина F и неизвестно U (если информация отсутствует). Говоря конкретно, нужно, чтобы все узлы использовали прямое унитарное кодирование, как показано в приведен-ном ниже словаре: In: one_hot_encoding = {\"M\": (1, 0, 0), \"F\": (0, 1, 0), \"U\": (0, 0, 1) } Теперь выполним этот пример в пошаговом режиме. Самое простое решение (которое, впрочем, не работает) – параллелизировать фиктивный набор данных (или прочесть его с диска) и затем применить метод map с лямбда-функцией к RDD-набору, чтобы линейно преобразовать информацию о половой принадлежности в соответствующий кодированный кортеж:\n--- Страница 313 ---\n312  Практическ ое машинное обучение в среде Spark In: (sc.parallelize([\"M\", \"F\", \"U\", \"F\", \"M\", \"U\"]) .map(lambda x: one_hot_encoding[x]) .collect()) Out: [(1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0), (0, 0, 1)] Это решение работает локально, но оно не будет работать в реальной распре- деленной среде, поскольку не все узлы имеют переменную one_hot_encoding в сво- ей рабочей области. Быстрое обходное решение состоит во включении словаря Python в функцию, (распределенно) выполняющую линейное преобразование, как нам удалось сделать ниже: In: def map_ohe(x): ohe = {\"M\": (1, 0, 0), \"F\": (0, 1, 0), \"U\": (0, 0, 1) } return ohe[x] sc.parallelize([\"M\", \"F\", \"U\", \"F\", \"M\", \"U\"]).map(map_ohe).collect()Out: [(1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0), (0, 0, 1)] Такое решение работает и локально, и на сервере, но оно не очень хорошее: мы смешали данные и процесс, сделав преобразующую функцию немногоразовой. Гораздо лучше, если преобразующая функция будет обращаться к широковеща-тельной переменной, которая может использоваться с любым возможным преоб-разованием, необходимым для прямого кодирования набора данных. Для этого сначала следует распространить словарь Python (вызвав метод broad- cast , предоставленный контекстом Spark sc) внутри функции, выполняющей ли- нейное преобразование. Теперь к словарю можно получить доступ, используя его свойство value . В результате этой операции мы получим обобщенную функ - цию преобразования, которая может работать с любым прямокодированным словарем: In: bcast_map = sc.broadcast(one_hot_encoding) def bcast_map_ohe(x, shared_ohe): return shared_ohe[x] (sc.parallelize([\"M\", \"F\", \"U\", \"F\", \"M\", \"U\"]) .map(lambda x: bcast_map_ohe(x, bcast_map.value)) .collect()) Out: [(1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0), (0, 0, 1)] Думайте о широковещательной переменной как о файле, записанном в рас - пределенной файловой системе HDFS. Впоследствии, когда обобщенный узел хо-чет получить к этой переменной доступ, ему нужен только путь в HDFS (который\n--- Страница 314 ---\nРаспространение переменных по всем узлам кластера  313 передается как аргумент метода map). Таким образом, если будет использоваться одинаковый путь, то все узлы гарантированно прочитают то же самое. Разумеет - ся, платформа Spark использует не саму распределенную файловую систему HDFS, а ее вариант в оперативной памяти.  Широковещательные переменные хранятся в оперативной памяти во всех узлах, состав - ляющих кластер, поэтому на них никогда не приходится большой объем данных, который мог бы их заполнить и сделать невозможной последующую обработку. Для удаления широковещательной переменной примените к ней метод un per- sist. Данная операция высвободит память от этой переменной на всех узлах: In: bcast_map.unpersist() Аккумуляторные переменные только для записи Аккумуляторы – это еще один вид переменных, которые могут быть распростра- нены в кластере Spark. Эти переменные доступны только для записи; они могут складываться с другими и обычно используются для реализации сумм или счет - чиков. Значение аккумуляторной переменной может быть прочитано только драйверным узлом, т. е. тем, который выполняет блокнот Jupyter; все другие узлы этого делать не могут. Давайте посмотрим на примере, как работает аккумулятор: требуется обрабо- тать текстовый файл и понять, сколько строк в нем являются пустыми. Разумеет - ся, это можно сделать, дважды просканировав набор данных (с использованием двух заданий Spark): в первый раз, прочитав пустые строки, и второй раз, вы-полнив реальную обработку, однако такое решение будет не очень эффективным. В первом неэффективном решении с извлечением количества пустых строк, используя два автономных задания Spark, как показано ниже, мы читаем тексто-вый файл, фильтруем пустые строки и подсчитываем их количество: In: print('Количество пустых строк: {0}' .format(sc.textFile( 'file:///home/vagrant/datasets/hadoop_git_readme.txt') .filter(lambda line: len(line) == 0).count())) Out: Количество пустых строк: 6 Второе решение, напротив, будет эффективнее (и сложнее). Мы инстанцируем аккумулятор (начальным значением 0) и добавляем 1 для каждой пустой строки, которую мы находим при обработке каждой строки входного файла (при помощи функции map). Одновременно с этим можно выполнять некую обработку каждой строки. В следующем ниже фрагменте кода, например, для каждой строки просто возвращается 1, благодаря чему подсчитываются все строки в файле. В конце обработки будут два элемента данных: первый – это число строк, по- лученное в результате действия count() на преобразованном RDD-наборе, и вто- рой – число пустых строк в свойстве value аккумулятора. Напомним – оба этих значения доступны после однократного сканирования набора данных:\n--- Страница 315 ---\n314  Практическ ое машинное обучение в среде Spark In: accum = sc.accumulator(0) def split_line(line): if len(line) == 0: accum.add(1) return 1 tot_lines = ( sc.textFile('file:///home/vagrant/datasets/hadoop_git_readme.txt') .map(split_line) .count()) empty_lines = accum.valueprint('Файл содержит %d строк(у),' % tot_lines) print('из которых %d пустые' % empty_lines) Out: Файл содержит 31 строк(у), из которых 6 пустые Платформа Spark нативно поддерживает аккумуляторы числовых типов, при этом операция суммирования в них задана по умолчанию. Приложив еще немно- го усилий, эту переменную можно превратить в нечто более сложное. Широковещательные и аккумуляторные переменные – пример Несмотря на то что широковещательные и аккумуляторные переменные прос - ты и функционально очень ограничены (одна доступна только для чтения, дру - гая – только для записи), они могут активно использоваться для создания очень сложных операций. Например, теперь попробуем применить разные алгоритмы машинного обучения в распределенной среде на наборе данных Iris. Мы скон-струируем задание Spark следующим образом: 1. Набор данных считывается и распространяется по всем узлам (поскольку он достаточно небольшой, чтобы уместиться в оперативной памяти). 2. Каждый узел применит к набору данных отличающийся классификатор и вернет имя классификатора и его отметку точности на полном наборе данных. Здесь необходимо отметить, что в этом простом примере предобра - ботка, разделение набора данных на тренировочный и тестовый наборы и гиперпараметрическая оптимизация выполняться не будут. Это делается с целью оставить все как можно проще. 3. Если классификаторы поднимут исключение, то в аккумуляторе должны быть сохранены строковое представление ошибки и имя классификатора. 4. Коне чный результат должен содержать список классификаторов, которые выполнили задачу классификации без ошибок, и их отметку точности. В качестве первого шага загрузим набор данных Iris и распространим его по всем узлам в кластере: In: from sklearn.datasets import load_irisbcast_dataset = sc.broadcast(load_iris()) Теперь создадим пользовательский аккумулятор. Он будет содержать список кортежей, в которых будут храниться имя классификатора и произошедшее ис -\n--- Страница 316 ---\nРаспространение переменных по всем узлам кластера  315 ключение в виде строки символов. Пользовательский аккумулятор является про- изводным от класса AccumulatorParam и содержит как минимум два метода: zero (который вызывается во время его инициализации) и addInPlace (который вызы- вается, когда на аккумуляторе вызывается метод add). Самый простой способ реализации пользовательского аккумулятора показан в следующем ниже примере, в конце которого выполняется его инициализация в виде пустого списка. Следует учесть, что аддитивная операция немного слож - нее: необходимо объединить два элемента, кортеж и список, но мы не знаем, какой элемент является списком, а какой кортежем, поэтому сначала обеспечи-вается, чтобы оба элемента были списками, и только потом выполняется их кон-катенация простым способом (при помощи оператора +): In: from pyspark import AccumulatorParam class ErrorAccumulator(AccumulatorParam): def zero(self, initialList): return initialList def addInPlace(self, v1, v2): if not isinstance(v1, list): v1 = [v1] if not isinstance(v2, list): v2 = [v2] return v1 + v2 errAccum = sc.accumulator([], ErrorAccumulator()) Теперь определим преобразующую функцию: каждый узел должен натрениро- вать классификатор, протестировать и оценить его на распространенном наборе данных Iris. В качестве своего аргумента функция получит объект классификатора и должна вернуть список, состоящий из кортежа с именем классификатора и его отметкой точности. Если при выполнении будет поднято исключение, то в аккумулятор будут до- бавлены имя классификатора и исключение в виде строки символов и возвращен пустой список: In: def apply_classifier(clf, dataset): clf_name = clf.__class__.__name__ X = dataset.value.data y = dataset.value.target try: from sklearn.metrics import accuracy_score clf.fit(X, y) y_pred = clf.predict(X) acc = accuracy_score(y, y_pred) return [(clf_name, acc)] except Exception as e: errAccum.add((clf_name, str(e))) return []\n--- Страница 317 ---\n316  Практическ ое машинное обучение в среде Spark Наконец, мы дошли до сердцевины задания. Теперь мы проинстанцируем не- сколько объектов из библиотеки Scikit-learn (некоторые из них не являются клас - сификаторами с целью протестировать аккумулятор). Мы преобразуем их в RDD- набор и применим функцию преобразования, которая была создана в предыдущей ячейке. Поскольку возвращенное значение является списком, мы можем восполь-зоваться методом flatMap для сбора результатов только из тех преобразователей, в которых не было перехвачено исключение: In: from sklearn.linear_model import SGDClassifierfrom sklearn.dummy import DummyClassifierfrom sklearn.decomposition import PCAfrom sklearn.manifold import MDS classifiers = [DummyClassifier('most_frequent'), SGDClassifier(), PCA(), MDS()] (sc.parallelize(classifiers) .flatMap(lambda x: apply_classifier(x, bcast_dataset)) .collect()) Out: [('DummyClassifier', 0.33333333333333331), ('SGDClassifier', 0.66666666666666663)] Как и ожидалось, результат содержит только реальные классификаторы. Теперь посмотрим, какие классификаторы сгенерировали ошибку. Неудивительно, здесь мы обнаруживаем два классификатора, пропущенных в предыдущем результате: In: print('Список ошибок: {0}'.format(errAccum.value)) Out: Список ошибок: [('PCA', \"'PCA' object has no attribute 'predict'\"), ('MDS', \"Proximity must be 'precomputed' or 'euclidean'. Got euclidean instead\")] В качестве последнего шага очистим распространенный по всем узлам набор данных: In:bcast_dataset.unpersist() Напомним, что в указанном примере был использован небольшой набор дан- ных, который можно было распространить по всем узлам. В реальных задачах с большими данными придется загрузить набор данных из распределенной фай-ловой системы HDFS и распространить по всем узлам путь в HDFS. предОбрабО тка данных в среде SPark До сих пор мы видели способы загрузки текстовых данных из локальной файло-вой системы и распределенной файловой системы HDFS. Текстовые файлы могут содержать любые неструктурированные данные (к примеру, текстовый документ) или структурированные данные (как, например, CSV-файл). Что касается полу -\n--- Страница 318 ---\nПредобработка данных в среде Spark  317 структурированных данных, то так же, как и файлы с объектами JSON, платформа Spark имеет специальные подпрограммы, которые в состоянии преобразовывать файл в объект DataFrame, который аналогичен таблицам данных DataFrame в сре-де R и в библиотеке pandas языка Python. Объекты DataFrame очень похожи на таблицы реляционных СУБД, в которых задана схема данных. Файлы JSON и объекты DataFrame платформы Spark Чтобы импортировать JSON-совместимые файлы, прежде всего необходимо соз-дать контекст SQL, в свою очередь, создав объект sqlContext из локального контекс - та Spark: In:from pyspark.sql import SQLContextsqlContext = SQLContext(sc) Теперь посмотрим на содержимое небольшого файла JSON (он находится в вир- туальной машине Vagrant). Он представляет собой таблицу с шестью строками и тремя столбцами, представленную в формате JSON, в которой некоторые атри-буты пропущены (в частности, атрибут gender у пользователя с идентификатором user_id=0 ): In:!cat /home/vagrant/datasets/users.json Out: {\"user_id\":0, \"balance\": 10.0}{\"user_id\":1, \"gender\":\"M\", \"balance\": 1.0}{\"user_id\":2, \"gender\":\"F\", \"balance\": -0.5}{\"user_id\":3, \"gender\":\"F\", \"balance\": 0.0}{\"user_id\":4, \"balance\": 5.0}{\"user_id\":5, \"gender\":\"M\", \"balance\": 3.0} Благодаря методу read.json , предоставленному объектом sqlContext , в перемен- ной уже имеется хорошо отформатированная таблица со всеми нужными имена-ми столбцов. Выходная переменная имеет тип DataFrame платформы Spark. Что-бы показать переменную в хорошо отформатированной таблице, используется ее метод show: In:df = sqlContext.read.json('file:///home/vagrant/datasets/users.json')df.show() Out: +-------+------+-------+|balance|gender|user_id|+-------+------+-------+| 10.0| null| 0|| 1.0| M| 1|| -0.5| F| 2|| 0.0| F| 3|| 5.0| null| 4|| 3.0| M| 5|+-------+------+-------+\n--- Страница 319 ---\n318  Практическ ое машинное обучение в среде Spark Помимо этого, при помощи метода printSchema можно просмотреть схему объ- екта DataFrame. Очевидно, что при чтении файла JSON каждый тип столбца опре- деляется по типу находящихся в нем данных (в примере столбец user_id содержит длинное целое, столбец gender – последовательности символов и balance – ве - щественное число двойной точности с плавающей точкой): In:df.printSchema() Out: root |-- balance: double (nullable = true) |-- gender: string (nullable = true) |-- user_id: long (nullable = true) Данные в объекте DataFrame можно нарезать горизонтально и вертикально в точности, как в таблице реляционной СУБД, отбирая столбцы и фильтруя дан-ные по атрибутам. В этом примере нужно распечатать баланс, пол и идентифика-тор пользователей, чей пол не пропущен и которые имеют баланс строго больше нуля. Для этого можно воспользоваться методами filter и select : In:(df.filter(df['gender'] != 'null') .filter(df['balance'] > 0) .select(['balance', 'gender', 'user_id']) .show()) Out: +-------+------+-------+|balance|gender|user_id|+-------+------+-------+| 1.0| M| 1|| 3.0| M| 5|+-------+------+-------+ Кроме того, можно переписать все предыдущее задание на SQL-подобном язы- ке. По сути дела, методы filter и select могут принимать строки, отформатиро- ванные по правилам SQL: In:(df.filter('gender is not null') .filter('balance > 0').select(\"*\").show()) Out: +-------+------+-------+|balance|gender|user_id|+-------+------+-------+| 1.0| M| 1|| 3.0| M| 5|+-------+------+-------+ Можно также применить всего один вызов метода filter : In:df.filter('gender is not null and balance > 0').show()\n--- Страница 320 ---\nПредобработка данных в среде Spark  319 Out: +-------+------+-------+|balance|gender|user_id|+-------+------+-------+| 1.0| M| 1|| 3.0| M| 5|+-------+------+-------+ Работа с пропущенными данными Типичную проблему во время предобработки представляет управление пропу - щенными данными. Объекты DataFrame платформы Spark, аналогично таблицам данных DataFrame библиотеки pandas, предлагают широкий диапазон операций, предназначенных для их обработки. Например, самый простой вариант собрать набор данных только из полных строк состоит в том, чтобы отбросить строки, со-держащие пропущенную информацию. Для этого в объектах DataFrame платфор-мы Spark сначала нужно обратиться к атрибуту na объекта DataFrame и затем вы- звать метод drop. Результирующая таблица будет содержать только полные строки: In:df.na.drop().show() Out: +-------+------+-------+|balance|gender|user_id|+-------+------+-------+| 1.0| M| 1|| -0.5| F| 2|| 0.0| F| 3|| 3.0| M| 5|+-------+------+-------+ Если такая операция удаляет слишком много строк, то всегда можно опреде- лить, какие столбцы служат причиной для удаления строки (применив расширен-ное подмножество метода drop): In:df.na.drop(subset=[\"gender\"]).show() Out: +-------+------+-------+|balance|gender|user_id|+-------+------+-------+| 1.0| M| 1|| -0.5| F| 2|| 0.0| F| 3|| 3.0| M| 5|+-------+------+-------+ Кроме того, если вместо удаления этих строк нужно для каждого столбца уста- новить значения по умолчанию, то можно применить метод fill, передав словарь, состоящий из имени столбца (в котором ключом является имя столбца) и значе-ния по умолчанию для замены пропущенных данных в этом столбце (т. е. в виде значения ключа в словаре).\n--- Страница 321 ---\n320  Практическ ое машинное обучение в среде Spark В качестве одного из примеров, если нужно обеспечить, чтобы переменные balance и gender там, где их значения пропущены, были установлены соответ - ственно в 0 и U, можно просто сделать следующее: In: df.na.fill({'gender': \"U\", 'balance': 0.0}).show() Out: +-------+------+-------+|balance|gender|user_id|+-------+------+-------+| 10.0| U| 0|| 1.0| M| 1|| -0.5| F| 2|| 0.0| F| 3|| 5.0| U| 4|| 3.0| M| 5|+-------+------+-------+ Группирование и создание таблиц в оперативной памяти Чтобы применить функцию к группе строк (как в случае с командой SQL GROUP BY), можно воспользоваться двумя аналогичными методами. В следующем ниже при-мере вычисляется средний баланс в расчете на пол: In:(df.na.fill({'gender': \"U\", 'balance': 0.0}) .groupBy(\"gender\").avg('balance').show()) Out: +------+------------+|gender|avg(balance)|+------+------------+| F| -0.25|| M| 2.0|| U| 7.5|+------+------------+ До сих пор мы работали только с объектами DataFrame, но, как вы могли уже за- метить, расстояние между методами объекта DataFrame и командами SQL мини-мальное. На самом деле при помощи платформы Spark можно зарегистрировать объект DataFrame как таблицу SQL и в полной мере воспользоваться возможно-стями SQL. Такая таблица хранится в памяти и распределяется аналогично RDD-набору. Чтобы зарегистрировать таблицу, нужно предоставить имя, которое будет ис - пользоваться в будущих командах SQL. В данном случае мы назначаем имя users: In:df.registerTempTable(\"users\") Посредством вызова метода sql, предоставленного объектом sqlContext плат - формы Spark, можно сгенерировать любую SQL-совместимую таблицу: In:sqlContext.sql(\"\"\" SELECT gender, AVG(balance)\n--- Страница 322 ---\nПредобработка данных в среде Spark  321 FROM users WHERE gender IS NOT NULL GROUP BY gender\"\"\").show() Out: +------+-----+|gender| _c1|+------+-----+| F|-0.25|| M| 2.0|+------+-----+ Неудивительно, что выведенная этой командой таблица (а также сама таблица users ) имеет тип DataFrame платформы Spark: In:type(sqlContext.table(\"users\")) Out: pyspark.sql.dataframe.DataFrame Объекты DataFrame, таблицы и RDD-наборы тесно между собой связаны, и ме- тоды RDD могут использоваться на объектах DataFrame. Напомним, что каждая строка в объекте DataFrame является элементом RDD-набора. Давайте рассмот - рим эт от функционал детально и сначала соберем полную таблицу: In:sqlContext.table(\"users\").collect() Out: [Row(balance=10.0, gender=None, user_id=0), Row(balance=1.0, gender=u'M', user_id=1), Row(balance=-0.5, gender=u'F', user_id=2), Row(balance=0.0, gender=u'F', user_id=3), Row(balance=5.0, gender=None, user_id=4), Row(balance=3.0, gender=u'M', user_id=5)] In: a_row = sqlContext.sql(\"SELECT * FROM users\").first()a_row Out: Row(balance=10.0, gender=None, user_id=0) Результатом является список объектов Row (они аналогичны именованному кортежу namedtuple в языке Python). Давайте здесь остановимся подробнее: объект Row содержит многочисленные атрибуты, к которым можно получить доступ через свойство либо по ключу словаря. Иными словами, чтобы извлечь из первой строки баланс, можно выбрать между двумя следующими вариантами: In:print(a_row['balance'])print(a_row.balance) Out: 10.010.0\n--- Страница 323 ---\n322  Практическ ое машинное обучение в среде Spark Кроме того, объект Row можно собрать как словарь Python при помощи мето- да asDict объекта Row. Результат будет содержать имена свойств в качестве ключа и значения свойств в качестве значений словаря: In: a_row.asDict() Out: {'balance': 10.0, 'gender': None, 'user_id': 0} Запись предобработанного объекта DataFrame или RDD-набора на диск Для записи объекта DataFrame или RDD-набора на диск можно воспользоваться методом write . Имеется ряд форматов на выбор; в данном случае мы сохраним его на локальной машине, как файл JSON: In: (df.na.drop().write .save(\"file:///tmp/complete_users.json\", format='json')) Проверяя результаты в локальной файловой системе, сразу видно, что что-то отличается от того, что ожидалось получить: эта операция создает многочислен- ные файлы ( part-r- ). Каждый из них содержит несколько строк, сериализованных как объекты JSON, и их слияние на выходе создаст исчерпывающий результат. Поскольку платформа Spark создана для обработки больших и распределенных файлов, операция запи-си соответствующим образом адаптирована для таких задач, и каждый узел пи-шет часть полного RDD-набора: In: !ls -als /tmp/complete_users.json Out: total 284 drwxrwxr-x 2 vagrant vagrant 4096 Jun 17 14:40 .4 drwxrwxrwt 9 root root 4096 Jun 17 14:40 4 -rw-r--r-- 1 vagrant vagrant 83 Jun 17 14:40 part-r-00000-78a81d94-92fd-4853-acda-1f199c40dbab4 -rw-rw-r-- 1 vagrant vagrant 12 Jun 17 14:40 .part-r-00000-78a81d94-92fd-4853-acda-1f199c40dbab.crc4 -rw-r--r-- 1 vagrant vagrant 82 Jun 17 14:40 part-r-00001-78a81d94-92fd-4853-acda-1f199c40dbab4 -rw-rw-r-- 1 vagrant vagrant 12 Jun 17 14:40 .part-r-00001-78a81d94-92fd-4853-acda-1f199c40dbab.crc0 -rw-r--r-- 1 vagrant vagrant 0 Jun 17 14:40 _SUCCESS4 -rw-rw-r-- 1 vagrant vagrant 8 Jun 17 14:40 ._SUCCESS.crc Для того чтобы его прочитать обратно, создавать автономный файл не требует - ся – операция чтения прекрасно обрабатывает даже многочисленные части. Файл JSON можно также прочитать в операторе FROM SQL-запроса. Теперь попробуем распечатать файл JSON, который мы только что записали на диск, не создавая промежуточного объекта DataFrame:\n--- Страница 324 ---\nПредобработка данных в среде Spark  323 In: sqlContext.sql( \"SELECT * FROM json.`file:///tmp/complete_users.json`\").show() Out: +-------+------+-------+|balance|gender|user_id|+-------+------+-------+| 1.0| M| 1|| -0.5| F| 2|| 0.0| F| 3|| 3.0| M| 5|+-------+------+-------+ Помимо JSON, существует еще один формат, очень популярный при работе со структурированными большими наборами данных: формат Parquet. Формат Parquet – это колоночный формат хранения данных, который имеется в экоси-стеме Hadoop; он сжимает и кодирует данные и может работать с вложенными структурами: все эти особенности делают его очень эффективным. Запись и загрузка файла в формате Parquet очень похожи на работу с форматом JSON, при этом операция записи в этом случае тоже производит многочисленные файлы, которые записываются на диск: In:df.na.drop().write.save('file:///tmp/complete_users.parquet', format='parquet') In: !ls -als /tmp/complete_users.parquet/ Out: total 444 drwxrwxr-x 2 vagrant vagrant 4096 Jun 17 14:40 .4 drwxrwxrwt 10 root root 4096 Jun 17 14:40 4 -rw-r--r-- 1 vagrant vagrant 376 Jun 17 14:40 _common_metadata4 -rw-rw-r-- 1 vagrant vagrant 12 Jun 17 14:40 ._common_metadata.crc4 -rw-r--r-- 1 vagrant vagrant 1082 Jun 17 14:40 _metadata4 -rw-rw-r-- 1 vagrant vagrant 20 Jun 17 14:40 ._metadata.crc4 -rw-r--r-- 1 vagrant vagrant 750 Jun 17 14:40 part-r-00000-e637364c-fa40-466b-8c9e-90d9d3ed047e.gz.parquet4 -rw-rw-r-- 1 vagrant vagrant 16 Jun 17 14:40 .part-r-00000-e637364c-fa40-466b-8c9e-90d9d3ed047e.gz.parquet.crc4 -rw-r--r-- 1 vagrant vagrant 746 Jun 17 14:40 part-r-00001-e637364c-fa40-466b-8c9e-90d9d3ed047e.gz.parquet4 -rw-rw-r-- 1 vagrant vagrant 16 Jun 17 14:40 .part-r-00001-e637364c-fa40-466b-8c9e-90d9d3ed047e.gz.parquet.crc0 -rw-r--r-- 1 vagrant vagrant 0 Jun 17 14:40 _SUCCESS4 -rw-rw-r-- 1 vagrant vagrant 8 Jun 17 14:40 ._SUCCESS.crc Работа с объектами DataFrame До сих пор давалось описание того, как загружать объекты DataFrame из файлов JSON и Parquet, но не как их создавать из существующего RDD-набора. Для этого необходимо создать один объект Row для каждой записи в RDD-наборе и вызвать\n--- Страница 325 ---\n324  Практическ ое машинное обучение в среде Spark метод createDataFrame контекста SQL. В самом конце полученный объект можно зарегистрировать как временную таблицу, что позволит в полной мере восполь- зоваться возможностями синтаксиса SQL: In:from pyspark.sql import Row rdd_gender = \\ sc.parallelize([Row(short_gender=\"M\", long_gender='Male'), Row(short_gender=\"F\", long_gender='Female')]) (sqlContext.createDataFrame(rdd_gender) .registerTempTable('gender_maps')) In: sqlContext.table(\"gender_maps\").show() Out: +-----------+------------+|long_gender|short_gender|+-----------+------------+| Male| M|| Female| F|+-----------+------------+  Этот способ, кроме того, является предпочтительным для работы с CSV-файлами. Сначала файл считывается методом sc.textFile ; затем при помощи метода split , конструктора объ- екта Row и метода createDataFrame создается итоговый объект DataFrame. Когда в оперативной памяти много объектов DataFrame или когда они могут быть загружены из диска, можно воспользоваться операциями, которые имеются в классической реляционной СУБД. В приведенном ниже примере созданный из RDD-набора объект DataFrame объединяется операцией join с набором данных users , содержащимся в сохраненном ранее Parquet-файле. Результат поразителен: In:sqlContext.sql(\"\"\" SELECT balance, long_gender, user_id FROM parquet.`file:///tmp/complete_users.parquet` JOIN gender_maps ON gender=short_gender\"\"\").show() Out: +-------+-----------+-------+|balance|long_gender|user_id|+-------+-----------+-------+| 3.0| Male| 5|| 1.0| Male| 1|| 0.0| Female| 3|| -0.5| Female| 2|+-------+-----------+-------+ В веб-интерфейсе под вкладкой SQL каждый SQL-запрос отображается как вир- туальный направленный ациклический граф (DA G). Он замечательно помога- ет отслеживать процесс исполнения вашего задания и понять сложность запроса. При выполнении предыдущего SQL-запроса с оператором JOIN четко видно, что\n--- Страница 326 ---\nПредобработка данных в среде Spark  325 два ответвления входят в одинаковый блок BroadcastHashJoin : первый – из RDD- набора, и второй – из Parquet-файла. Затем следующий блок попросту является проекцией на отобранные столбцы: Поскольку таблицы находятся в оперативной памяти, последней является опе- рация очистки с целью высвобождения памяти, используемой для их хранения. В результате вызова метода tableNames , предоставленного объектом sqlContext , мы получим список всех таблиц, которые в настоящее время находятся в памяти. Затем, чтобы высвободить место, можно воспользоваться методом dropTempTable с именем таблицы в качестве аргумента. После этого вызова дальнейшее обраще-ние к этим таблицам вернет ошибку: In:sqlContext.tableNames() Out: [u'gender_maps', u'users'] In: for table in sqlContext.tableNames(): sqlContext.dropTempTable(table))\n--- Страница 327 ---\n326  Практическ ое машинное обучение в среде Spark Начиная с версии Spark 1.3 объект DataFrame является предпочтительным спо- собом работы с набором данных при выполнении задач в области науки о данных. машиннОе Обучение с платфОрмОй SPark Здесь мы подходим к главной цели вашей работы: созданию модели для предска- зания одного или многочисленных атрибутов, отсутствующих в наборе данных. Для этого мы построим несколько моделей машинного обучения, и платформа Spark может оказать нам в этом контексте неоценимую помощь. MLlib – это библиотека машинного обучения в рамках платформы Spark; не- смотря на то что она написана на Scala и Java, ее функционал также доступен в среде Python. Она содержит учеников для выполнения задач классификации, регрессии и генерирования рекомендаций, несколько подпрограмм для сниже-ния размерности и отбора признаков, имеет обширный функционал по обработке текста. Все они в состоянии справиться с огромными наборами данных и исполь-зуют мощь всех узлов кластера для достижения поставленной цели. На данный момент (2016 г.) эта библиотека состоит из двух основных библио- тек: библиотеки mllib , которая оперирует на RDD-наборах, и библиотеки ml, кото - рая работает на объектах DataFrame. Поскольку последняя из двух имеет хорошую производительность и является наиболее популярным способом представления данных в науке о данных, разработчики сделали выбор в пользу участия и усо-вершенствования ветки ml, позволив первой библиотеке остаться, как есть, но без дальнейшего развития. На первый взгляд, библиотека MLlib выглядит закончен-ной, но, начав использовать платформу Spark, вы заметите, что в ее стандартном комплекте нет ни статистической, ни числовой библиотек. И здесь на выручку приходят библиотеки SciPy и NumPy, которые, повторимся, очень важны для нау - ки о данных! В этом разделе мы попытаемся разведать функционал новой библиотеки pyspark.ml ; на данный момент она по-прежнему находится на ранних стадиях по сравнению с современным состоянием библиотеки Scikit-learn, но она определен-но имеет большой потенциал для будущего.  Платформа Spark – это высокоуровневое, распределенное и многосложное программное обе спечение, которое должно использоваться только на больших данных и с кластером из многочисленных узлов; на самом деле если набор данных может уместиться в оператив-ной памяти, то удобнее пользоваться другими библиотеками, такими как Scikit-learn или ей подобными, в центре внимания которых находится только та сторона задачи, которая имеет отношение к науке о данных. Выполнение платформы Spark на единственном узле на небольшом наборе данных может быть в пять раз медленнее эквивалентного алгоритма в Scikit-learn. Платформа Spark на наборе данных KDD99 Давайте выполним эту разведку с использованием реального набора данных. На-бор данных KDD99 использовался для третьего международного соревнования по обнаружению знаний и инструментам глубинного анализа данных. Цель сорев-нования состояла в том, чтобы создать систему обнаружения вторжений в сеть (intrusion detection system, IDS), способную распознавать, какой сетевой поток является злонамеренным и какой нет. Более того, много разных атак находится\n--- Страница 328 ---\nМашинное обучение с платформой Spark  327 в самом наборе данных; цель состоит в том, чтобы точно их предсказать при по- мощи содержащихся в наборе данных признаков пакетного потока. В качестве дополнения отметим, что указанный набор данных был чрезвы- чайно полезен для разработки отличных решений в области систем обнаружения вторжений в первые несколько лет после его выпуска. Сегодня, как результат, все включенные в набор данных атаки очень легко обнаруживаются, и поэтому для разработки IDS он больше не используется. В частности, в указанном наборе данных представлены следующие признаки: протокол (tcp, icmp и udp), служба (http, smtp и т. д.), размер пакетов, активные в протоколе флаги, количество попыток стать корнем и т. д.  Дополнительную информацию о соревновании KDD99 и наборах данных можно найти на http: //kdd.ics.uci.edu/databases/kddcup99/kddcup99.html. Описанная выше задача является классической задачей многоклассовой клас - сификации. Несмотря на это, мы остановимся на ней подробнее, чтобы показать, как выполнять эту задачу в платформе Spark. Чтобы все было чисто, мы восполь-зуемся новым блокнотом Jupyter. Чтение набора данных Прежде всего скачаем и распакуем этот набор данных. Мы будем очень консерва-тивными и воспользуемся всего 10% исходного тренировочного набора данных (75 Мб в несжатом виде), так как весь анализ выполняется на небольшой виртуаль - ной машине. Если есть желание попробовать, то в следующем ниже фрагменте кода можно раскомментировать строки и загрузить полный тренировочный на-бор данных (750 Мб в несжатом виде). Мы скачаем тренировочный набор данных, тестовый набор (47 Мб) и имена признаков при помощи команд bash: In:!rm -rf kdd* # !wget -q -O /datasets/kddtrain.gz \\ # http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz !wget -q -O /datasets/kddtrain.gz \\ http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz !wget -q -O /datasets/kddtest.gz \\ http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz !wget -q -O /datasets/kddnames \\ http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names !gunzip /datasets/kdd*gz Теперь распечатаем первые несколько строк, что получить представление о его формате. Совершенно очевидно, что это классический CSV без заголовка и с точ- кой в конце каждой строки. Кроме того, мы видим, что большинство полей явля-ется числовыми, но несколько из них – текстовые, и целевая переменная содер-жится в последнем поле: In:!head -3 /datasets/kddtrain\n--- Страница 329 ---\n328  Практическ ое машинное обучение в среде Spark Out: 0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal. Для создания объекта DataFrame с именованными полями сначала необходимо прочитать заголовок, включенный в файл kddnames . Целевое поле будет называться просто target . Прочитав файл и выполнив его разбор, распечатаем количество признаков решаемой задачи (напомнив, что целевая переменная не является признаком) и первые 10 имен признаков: In:with open(' /datasets/kddnames', 'r') as fh: header = [line.split(':')[0] for line in fh.read().splitlines()][1:] header.append('target')print('Число признаков: {0}'.format(len(header)-1)) print('Первые 10: {0}'.format(header[:10])) Out: Число признаков: 41Первые 10: ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot'] Теперь создадим два отдельных RDD-набора – один для тренировочных данных и другой для тестовых: In:train_rdd = sc.textFile('file:///home/vagrant/datasets/kddtrain')test_rdd = sc.textFile('file:///home/vagrant/datasets/kddtest') Далее необходимо выполнить разбор каждой строки каждого файла для созда- ния объекта DataFrame. Сначала разобьем каждую строку CSV-файла на отдель-ные поля и затем приведем каждое численное значение к типу с плавающей точ-кой и каждое текстовое значение к строковому типу. В заключение удалим точку в конце каждой строки. В качестве последнего шага при помощи метода createDataFrame , предостав- ляемого объектом sqlContext , создадим два объекта DataFrame с именованными столбцами для тренировочного и тестового наборов данных: In:def line_parser(line): def piece_parser(piece): if \".\" in piece or piece.isdigit(): return float(piece) else: return piece return [piece_parser(piece) for piece in line[:-1].split(',')]\n--- Страница 330 ---\nМашинное обучение с платформой Spark  329 train_df = sqlContext.createDataFrame( train_rdd.map(line_parser), header) test_df = sqlContext.createDataFrame( test_rdd.map(line_parser), header) До настоящего момента для RDD-наборов были написаны только преобразова- тели; теперь введем действие, чтобы увидеть, сколько наблюдений имеется в на-борах данных, и одновременно проверим правильность предыдущего программ-ного кода. In:print('Тренировочные наблюдения: {0}'.format(train_df.count()))print('Тестовые наблюдения: {0}'.format(test_df.count())) Out: Тренировочные наблюдения: 494021Тестовые наблюдения: 311029 Несмотря на то что мы используем всего одну десятую полного набора данных KDD99, все равно получается полмиллиона наблюдений. Умножив их на число признаков (41), ясно понимаем, что будем тренировать классификатор на матри-це наблюдений, содержащей более 20 млн значений. Для платформы Spark это не такой уж и большой набор данных (кстати, и полный набор данных KDD99 тоже); разработчики по всему миру уже используют ее на петабайтах и миллиардах запи - сей. Не пугайтесь, если числа выглядят большими: платформа Spark разработана для того, чтобы с ними справляться! Теперь посмотрим, как он выглядит на схеме объекта DataFrame. Если быть точ- ным, мы хотим установить, какие поля являются числовыми и какие содержат строковые значения (заметим, что результат для краткости сокращен): In:train_df.printSchema() Out: root|-- duration: double (nullable = true)|-- protocol_type: string (nullable = true)|-- service: string (nullable = true)|-- flag: string (nullable = true)|-- src_bytes: double (nullable = true)|-- dst_bytes: double (nullable = true)|-- land: double (nullable = true)|-- wrong_fragment: double (nullable = true)|-- urgent: double (nullable = true)|-- hot: double (nullable = true) |-- target: string (nullable = true) Конструирование признаков Исходя из визуального анализа, только четыре поля являются строковыми: pro- tocol_type , service , flag и цель target (которая, как ожидалось, является целевой мультиклассовой меткой).\n--- Страница 331 ---\n330  Практическ ое машинное обучение в среде Spark Поскольку мы будем использовать древовидный классификатор, необходимо по каждой переменной закодировать текст каждого уровня в число. В Scikit-learn эта операция может быть выполнена при помощи объекта sklearn.preprocessing. LabelEncoder . Его эквивалентом в Spark является StringIndexer из модуля pyspark. ml.feature . В среде Spark нужно закодировать четыре переменные; затем объединить в кас - кадную цепь четыре объекта StringIndexer : каждый из них будет оперировать определенным столбцом объекта DataFrame, выводя объект DataFrame с дополни-тельным столбцом (аналогично операции преобразования map). Преобразование выполняется автоматически и упорядочивает элементы по частоте: платформа Spark ранжирует счетчик каждого уровня в выбранном столбце, ставя в соответ - ствие самому популярному уровню 0, следующему – 1 и т. д. Отметим, что, вы-полняя эту операцию, набор данных будет пройден один раз для подсчета случа-ев каждого уровня. Если бы соответствие было уже известно, то было бы гораздо эффективнее его распространить по всем узлам и применить операцию map, как было показано в начале настоящей главы. Аналогичным образом можно воспользоваться прямым унитарным кодиров- щиком для генерирования числовой матрицы наблюдений. В случае прямого ко-дировщика в объекте DataFrame были бы многочисленные выходные столбцы, один для каждого уровня каждого категориального признака. Для этой цели плат - форма Spark предлагает класс pyspark.ml.feature.OneHotEncoder .  В более общем плане все содержащиеся в библиотеке pyspark.ml.feature классы использу- ются для извлечения, преобразования и отбора признаков из объекта DataFrame. Все они читают столбцы и создают другие столбцы в объекте DataFrame. Начиная с версии Spark 1.6 имеющиеся в Python операции на признаках содер- жатся в следующем ниже исчерпывающем списке (их все можно найти в библио - тек е pyspark.ml.feature ). Имена операций должны быть интуитивно понятными, за исключением пары из них, которые будут объяснены во фрагментах кода или позднее в тексте. Для текстовых входных данных (идеально): – HashingTF и IDF д ля векторизации признаков с хэшированием; – Tok enizer и его реализация RegexTokenizer на основе регулярных выраже- ний для лексемизации текста; – Word2v ec для векторизации слов; – StopWordsR emover для удаления стоп-слов; – N-gramm д ля выделения n-грамм. Для кат егориальных признаков: – строк овый индексатор StringIndexer и обратный ему кодировщик In dex- ToString; – прямой унитарный к одировщик OneHotEncoder; – Vec torIndexer (нестандартный индексатор с переводом из категорий в числа). Для других входных данных: – Binarizer для двоичного преобразования; – PC A для анализа главных компонент; – PolynomialExpansion для полиномиального разложения;\n--- Страница 332 ---\nМашинное обучение с платформой Spark  331 – нормализатор Normalizer, стандартный шкалировщик StandardScaler и ми - нимаксный шкалировщик MinMaxScaler; – Bucketiz er (упаковщик значений признака); – ElementwiseProduc t (перемножает столбцы) для поэлементного умноже- ния. Обобщенные: – SQLT ransformer (реализует преобразования, определенные SQL-опера то- ром, обращаясь к объекту DataFrame как таблице под названием __THIS ); – RFormula (выбирает столбцы, используя синтаксические конструкции в стиле языка R); – Vec torAssembler (создает вектор признаков из многочисленных столбцов). Возвращаясь к примеру, теперь закодируем уровни в каждой категориальной переменной дискретными числами. Как уже объяснялось выше, для этого по каж - дой переменной применяется объект StringIndexer . Более того, воспользуемся конвейером машинного обучения и установим их в качестве его этапов. Затем, чтобы выполнить подгонку все индексаторов, нужно просто вызвать метод fit конвейера. Внутренне он последовательно выполнит подгонку всех по- этапных объектов. Когда он завершит операцию подгонки, будет создан новый объект, к которому можно обращаться как подогнанному конвейеру. Вызов ме-тода transform этого нового объекта последовательно вызовет все поэтапные эле- менты (которые уже подогнаны), при этом каждый будет вызываться после того, как предыдущий завершен. В приведенном ниже фрагменте кода можно увидеть конвейер в действии. Отметим, что конвейер состоит из преобразователей. И по-этому на самом деле ничего не выполняется, так как действия отсутствуют. В вы-ходном объекте DataFrame будут четыре дополнительных столбца под теми же самыми именами, что и исходные категориальные, но с суффиксом _cat: In: from pyspark.ml import Pipelinefrom pyspark.ml.feature import StringIndexer cols_categorical = ['protocol_type', 'service', 'flag', 'target'] preproc_stages = [] for col in cols_categorical: out_col = col + '_cat' preproc_stages.append( StringIndexer( inputCol=col, outputCol=out_col, handleInvalid='skip')) pipeline = Pipeline(stages=preproc_stages) indexer = pipeline.fit(train_df) train_num_df = indexer.transform(train_df) test_num_df = indexer.transform(test_df) Давайте исследуем конвейер чуть подробнее. В конвейере можно увидеть эта- пы: неподогнанный конвейер и подогнанный конвейер. Отметим, что существу - ет большая разница между платформой Spark и библиотекой Scikit-learn: в Scikit- learn методы fit и transform вызываются на одном и том же объекте, в платформе Spark метод fit генерирует новый объект (обычно его имя дополняется суффик -\n--- Страница 333 ---\n332  Практическ ое машинное обучение в среде Spark сом Model , так же, как для Pipeline и PipelineModel ), в котором можно вызвать метод transform . Эта разница вытекает из замыканий – подогнанный объект проще рас - пределять по всем процессам и всему кластеру: In: print(pipeline.getStages())print()print(pipeline)print(indexer) Out: [StringIndexer_4103bd5fa259e598be03, StringIndexer_436da471ddb860cb168f, StringIndexer_485b8e58313de366d730, StringIndexer_4b6d961bf2d526a76f75]()Pipeline_4e94a8cb25b9b64db564PipelineModel_4ae799b82c588eb50907 Давайте посмотрим, как изменяется первое наблюдение, т. е. первая строка в CSV-файле, после прохождения через конвейер. Отметим, что здесь использу - ется действие, поэтому все этапы в конвейере и в конвейерной модели исполня- ются: In: print('Первое наблюдение после 4 индексаторов StringIndexer:\\n')print(train_num_df.first()) Out: Первое наблюдение после 4 индексаторов StringIndexer: Row(duration=0.0, protocol_type=u'tcp', service=u'http', flag=u'SF', src_bytes=181.0, dst_ bytes=5450.0, land=0.0, wrong_fragment=0.0, urgent=0.0, hot=0.0, num_failed_logins=0.0, logged_in=1.0, num_compromised=0.0, root_shell=0.0, su_attempted=0.0, num_root=0.0, num_file_creations=0.0, num_shells=0.0, num_access_files=0.0, num_outbound_cmds=0.0, is_host_login=0.0, is_guest_login=0.0, count=8.0, srv_count=8.0, serror_rate=0.0, srv_serror_rate=0.0, rerror_rate=0.0, srv_rerror_rate=0.0, same_srv_rate=1.0, diff_srv_rate=0.0, srv_diff_host_rate=0.0, dst_host_count=9.0, dst_host_srv_count=9.0, dst_host_same_srv_rate=1.0, dst_host_diff_srv_rate=0.0, dst_host_same_src_port_rate=0.11, dst_host_srv_diff_host_rate=0.0, dst_host_serror_rate=0.0, dst_host_srv_serror_rate=0.0, dst_host_rerror_rate=0.0, dst_host_srv_rerror_rate=0.0, target=u'normal', protocol_type_cat=1.0, service_cat=2.0, flag_cat=0.0, target_cat=2.0) Получившийся объект DataFrame выглядит вполне завершенным и понятным: все переменные имеют имена и значения. Мы сразу отмечаем, что категориаль- ные признаки по-прежнему присутствуют, например protocol_type (категориаль- ная переменная) и protocol_type_cat (числовая версия переменной, полученная из категориальной). Несколько столбцов из объекта DataFrame можно извлечь так же просто, как и при использовании оператора SELECT в SQL-запросе. Теперь сконструируем спи- сок названий всех числовых признаков: начиная с найденных в заголовке имен, удаляем категориальные и заменяем их производными числовыми версиями. В заключение удалим целевую переменную и производный от нее числовой эк - вивалент, так как требуются только признаки:\n--- Страница 334 ---\nМашинное обучение с платформой Spark  333 In: features_header = set(header) \\ - set(cols_categorical) \\ | set([c + \"_cat\" for c in cols_categorical]) \\ - set([\"target\", \"target_cat\"])features_header = list(features_header) print(features_header) print('Всего числовых признаков: {0}'.format(len(features_header))) Out: ['num_access_files', 'src_bytes', 'srv_count', 'num_outbound_cmds', 'rerror_rate', 'urgent', 'protocol_type_cat', 'dst_host_same_srv_rate', 'duration', 'dst_host_diff_srv_rate', 'srv_serror_rate', 'is_host_login', 'wrong_fragment', 'serror_rate', 'num_compromised', 'is_guest_login', 'dst_host_rerror_rate', 'dst_host_srv_serror_rate', 'hot', 'dst_host_srv_count', 'logged_in', 'srv_rerror_rate', 'dst_host_srv_diff_host_rate', 'srv_diff_host_rate', 'dst_host_same_src_port_rate', 'root_shell', 'service_cat', 'su_attempted', 'dst_host_count', 'num_file_creations', 'flag_cat', 'count', 'land', 'same_srv_rate', 'dst_bytes', 'num_shells', 'dst_host_srv_rerror_rate', 'num_root', 'diff_srv_rate', 'num_failed_logins', 'dst_host_serror_rate']Всего числовых признаков: 41 Ниже на помощь приходит класс VectorAssembler , который создает признаковую матрицу. В качестве аргумента в этот класс нужно передать отбираемые столбцы и создаваемый в объекте DataFrame новый столбец. Мы решаем назвать выходной столбец просто features и применяем это преобразование к обоим наборам: тре- нировочному и тестовому, а затем отбираем только два интересующих нас столб-ца – features и target_cat : In:from pyspark.mllib.linalg import Vectorsfrom pyspark.ml.feature import VectorAssembler assembler = VectorAssembler(inputCols=features_header, outputCol='features') Xy_train = (assembler.transform(train_num_df) .select('features', 'target_cat'))Xy_test = (assembler.transform(test_num_df) .select('features', 'target_cat')) Кроме того, поведение векторного сборщика VectorAssembler по умолчанию сво- дится к созданию плотных или разреженных векторов, соответственно DenseVector или SparseVector . В данном случае сборщик возвращает разреженный вектор, по- скольку вектор features содержит много нулей. Чтобы посмотреть, что находится внутри конечного результата, можно распечатать первую строку. Отметим, что это будет действием. Следовательно, задание исполнится прежде, чем будет полу - чен распечатанный результат: In:Xy_train.first() Out: Row(features=SparseVector(41, {1: 181.0, 2: 8.0, 6: 1.0, 7: 1.0, 19: 9.0, 20: 1.0, 24: 0.11, 26: 2.0, 28: 9.0, 31: 8.0, 33: 1.0, 34: 5450.0}), target_cat=2.0)\n--- Страница 335 ---\n334  Практическ ое машинное обучение в среде Spark Тренировка ученика Наконец, мы достигли самой «заводной» части задачи: тренировки классифика- тора. Классификаторы содержатся в библиотеке pyspark.ml.classification , и для данного примера мы используем случайный лес. Начиная с версии Spark 1.6 расширен список классификаторов, в которых ис - пользуется интерфейс Python. Ниже приводим этот список. Клас сификация (библиотека pyspark.ml.classification ): – LogisticRegression для классификации с использованием логистической регрессии; – DecisionTreeClassifier для классификации с использованием деревьев ре- шений; – GBTClassifier для классификации с использованием деревьев решений (реа лизация градиентного б устинга); – RandomForestClassifier для классификации с использованием случайного леса; – NaiveBayes для классификации с использованием наивного Байеса; – MultilayerPerceptronClassifier для классификации с использованием мно- гослойного персептрона. Отметим, что не все из них способны обрабатывать мультиклассовые задачи, и они могут иметь отличающиеся параметры; следует всегда проверять докумен-тацию, связанную с используемой версией. Помимо классификаторов, в Spark 1.6 также реализованы и другие ученики с интерфейсом Python. Ниже приводим их список. Клас теризация (библиотека pyspark.ml.clustering ): – KMeans д ля кластеризации методом K-средних. Регрес сия (библиотека pyspark.ml.regression ): – AFTSurvivalR egression для регрессии с использованием модели ускорен- ного отказа (на основе анализа выживаемости); – DecisionT reeRegressor для регрессии с использованием деревьев реше- ний; – GBTRegressor для регрессии с использованием регрессионных деревьев (реализация градиентного бустинга); – IsotonicR egression для изотонической регрессии; – LinearRegression для линейной регрессии; – RandomForestRegressor для регрессии с использованием случайного леса. Рек омендатель (библиотека pyspark.ml.recommendation ): – ALS (рек омендательная система с использованием коллаборативной фильтрации на основе чередующихся наименьших квадратов). Давайте вернемся к цели задачи KDD99. Теперь пора инстанцировать класси- фикатор на основе случайного леса и задать его параметры. Такими параметрами являются featuresCol (столбец, содержащий признаковую матрицу), labelCol (стол - бец объекта DataFrame, содержащий целевую метку), seed (начальное число для ге- нератора случайных чисел, чтобы обеспечить воспроизводимость эксперимента) и maxBins (максимальное число дискретных интервалов, используемых для точки расщепления в каждом узле дерева). Значение числа деревьев в лесе по умолча-нию равно 20, и каждое дерево имеет максимум пять уровней в глубину. Кроме\n--- Страница 336 ---\nМашинное обучение с платформой Spark  335 того, этот классификатор по умолчанию создает три выходных столбца в объек - те DataFrame: rawPrediction (для хранения показателя точности предсказания для каждой возможной метки), probability (для хранения вероятности каждой метки) и prediction (наиболее вероятная метка): In: from pyspark.ml.classification import RandomForestClassifier clf = RandomForestClassifier(labelCol='target_cat', featuresCol='features', maxBins=100, seed=101)fit_clf = clf.fit(Xy_train) Однако даже и в этом случае натренированный классификатор будет представ- лен другим объектом. Как и ранее, натренированный классификатор имеет то же самое имя, что и классификатор плюс суффикс Model : In: print(clf)print(fit_clf) Out: RandomForestClassifier_40cca1c5d6cb00c6099bRandomForestClassificationModel (uid=rfc_11eebe9e9fa6) with 20 trees На объекте обученного классификатора RandomForestClassificationModel можно вызвать метод transform . Теперь предскажем метку на тренировочном и тестовом наборах данных и распечатаем первую строку тестового набора данных; как опре- делено в классификаторе, предсказания будут находиться в столбце prediction : In: Xy_pred_train = fit_clf.transform(Xy_train)Xy_pred_test = fit_clf.transform(Xy_test) In: print('Первое наблюдение после этапа классификации:')print(Xy_pred_test.first()) Out: Первое наблюдение после этапа классификации:Row(features=SparseVector(41, {1: 105.0, 2: 1.0, 6: 2.0, 7: 1.0, 9: 0.01, 19: 254.0, 26: 1.0, 28: 255.0, 31: 1.0, 33: 1.0, 34: 146.0}), target_cat=2.0, rawPrediction=DenseVector([0.0283, 0.0112, 19.3474, 0.0677, 0.0251, 0.1414, 0.0357, 0.1194, 0.1309, 0.041, 0.0257, 0.0079, 0.0046, 0.0004, 0.0029, 0.0016, 0.002, 0.0023, 0.0013, 0.0008, 0.0012, 0.0006, 0.0006]), probability=DenseVector([0.0014, 0.0006, 0.9674, 0.0034, 0.0013, 0.0071, 0.0018, 0.006, 0.0065, 0.002, 0.0013, 0.0004, 0.0002, 0.0, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0, 0.0001, 0.0, 0.0]), prediction=2.0) Оценка результативности ученика Следующий шаг в любой задаче науки о данных состоит в проверке результатив- ности ученика на тренировочном и тестовом наборах. Для этой задачи мы вос - пользуемся метрикой F1, поскольку она удачно объединяет показатели результа-тивности – прецизионность и полноту.\n--- Страница 337 ---\n336  Практическ ое машинное обучение в среде Spark Метрики оценивания включены в библиотеку pyspark.ml.evaluation ; среди не- скольких вариантов для выбора мы используем вариант с оценкой мультиклассо- вых классификаторов: MulticlassClassificationEvaluator . В качестве параметров мы предоставляем метрику (прецизионность, полноту, точность, метрику F1 и т. д.) и имена столбцов, содержащих истинную и предсказанную метки: In:from pyspark.ml.evaluation import MulticlassClassificationEvaluator evaluator = MulticlassClassificationEvaluator(labelCol='target_cat', predictionCol='prediction', metricName='f1') print('Отметка F1 на тренировочном наборе: {0}' .format(evaluator.evaluate(Xy_pred_train)))print('Отметка F1 на тестовом наборе: {0}' .format(evaluator.evaluate(Xy_pred_test))) Out: Отметка F1 на тренировочном наборе: 0.991904372002Отметка F1 на тестовом наборе: 0.966840043466 Полученные значения довольно высокие, и между результативностью на тре- нировочном и тестовом наборах существует большая разница. Помимо оценщика мультиклассовых классификаторов, в той же самой биб - лиот еке имеется объект с оценщиком регрессора (где метриками могут быть сред- неквадратическая ошибка MSE, корень из среднеквадратической ошибки RMSE, R-квадрат либо средняя абсолютная ошибка MAE) и бинарных классификаторов. Возможности конвейера машинного обучения До сих пор результаты создавались и выводились поэтапно. Помимо этого, можно также помещать все операции в каскад и задавать их в качестве этапов конвейера. По сути дела, в автономный конвейер можно связать цепочкой все, что мы видели до сих пор (четыре кодировщика меток, векторный построитель и классифика-тор), выполнить его подгонку на тренировочном наборе данных и, наконец, при-менить его на тестовом наборе данных для получения прогнозов. Этот способ работы более эффективен, но при этом теряются разведочные воз- можности постепенного анализа. Читателям, занимающимся аналитикой дан-ных, рекомендуется использовать сквозные конвейеры, только когда они абсо-лютно уверены в том, что происходит внутри, и только для построения серийных (эксплуатационных) моделей. Чтобы показать, что конвейер эквивалентен тому, что мы видели до сих пор, вычислим отметку F1 на тестовом наборе и ее распечатаем. Неудивительно, что значение будет в точности таким же: In:full_stages = preproc_stages + [assembler, clf]full_pipeline = Pipeline(stages=full_stages)full_model = full_pipeline.fit(train_df)predictions = full_model.transform(test_df)print('Отметка F1 на тестовом наборе: {0}' .format(evaluator.evaluate(predictions)))\n--- Страница 338 ---\nМашинное обучение с платформой Spark  337 Out: Отметка F1 на тестовом наборе: 0.966840043466 На драйверном узле, т. е. на том, который выполняет блокнот Jupyter, можно также использовать библиотеку matplotlib для визуализации результатов анали- за. Например, чтобы показать нормализованную матрицу ошибок результатов классификации (нормализованную поддержкой каждого класса), можно создать следующую функцию: In:import numpy as np def plot_confusion_matrix(cm): cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] plt.imshow( cm_normalized, interpolation='nearest', cmap=plt.cm.Blues) plt.title(u'Нормализованная матрица ошибок') plt.colorbar() plt.tight_layout() plt.ylabel(u'Истинная метка') plt.xlabel(u'Предсказанная метка') Платформа Spark способна создавать матрицу ошибок, но соответствующий метод находится в библиотеке pyspark.mllib . Чтобы иметь возможность использо- вать методы из этой библиотеки, необходимо при помощи метода rdd преобразо- вать объект DataFrame в RDD-набор: In:from pyspark.mllib.evaluation import MulticlassMetrics metrics = MulticlassMetrics(predictions.select('prediction', 'target_cat').rdd)conf_matrix = metrics.confusionMatrix().toArray()plot_confusion_matrix(conf_matrix) Нормализованная матрица ошибок Предсказанная меткаИстинная метка0 5 1015201,0 0,90,80,70,60,50,40,30,20,10,0 20 15 10 5 0\n--- Страница 339 ---\n338  Практическ ое машинное обучение в среде Spark Ручная доводка Несмотря на то что отметка F1 приблизилась к 0.97, нормализованная матри- ца ошибок показывает, что классы сильно разбалансированы, и классификатор только что обучился правильно классифицировать самые популярные из них. Для улучшения результатов можно сделать перевыборку каждого класса в попытке получше сбалансировать тренировочный набор данных. Сначала подсчитаем количество случаев в тренировочном наборе данных для каждого класса: In:train_composition = train_df.groupBy(\"target\").count().rdd.collectAsMap()train_composition Out: {u'back': 2203, u'buffer_overflow': 30, u'ftp_write': 8, u'guess_passwd': 53, u'neptune': 107201, u'nmap': 231, u'normal': 97278, u'perl': 3, u'warezmaster': 20} Мы получили явное доказательство сильного дисбаланса. Можно попробовать улучшить результативность путем отбора с повышенной частотой только редких классов и отбора с пониженной частотой слишком популярных классов. В приведенном ниже примере будет создан тренировочный набор данных, где каждый класс представлен, по крайней мере, 1000 раз, но не более 25 000 раз. Для этого сначала рассчитаем уровень пониженной/повышенной частоты отбора наблю дений и распространим этот уровень по всему кластеру, а затем обрабо- таем каждую строку тренировочного набора данных методом flatMap , чтобы вы- полнить его тщательную перевыборку: In:def set_sample_rate_between_vals(cnt, the_min, the_max): if the_min <= cnt <= the_max: # без выборки return 1 elif cnt < the_min: # Выборка с повышенной частотой (oversampling) # вернуть много раз одно и то же наблюдение return the_min/float(cnt) else: # Выборка с пониженной частотой (subsampling): # иногда наблюдение не будет возвращено return the_max/float(cnt) sample_rates = {k:set_sample_rate_between_vals(v, 1000, 25000) for k,v in train_composition.iteritems()}\n--- Страница 340 ---\nМашинное обучение с платформой Spark  339 sample_rates Out: {u'back': 1, u'buffer_overflow': 33.333333333333336, u'ftp_write': 125.0, u'guess_passwd': 18.867924528301888, u'neptune': 0.23320677978750198, u'nmap': 4.329004329004329, u'normal': 0.2569954152017928, u'perl': 333.3333333333333, u'warezmaster': 50.0} In: bc_sample_rates = sc.broadcast(sample_rates) def map_and_sample(el, rates): rate = rates.value[el['target']] if rate > 1: return [el]*int(rate) else: import random return [el] if random.random() < rate else [] sampled_train_df = (train_df .flatMap( lambda x: map_and_sample(x, bc_sample_rates)) .toDF() .cache()) Перевыбранный набор данных в переменной объекта DataFrame sampled_train_ df также кэшируется; мы будем использовать ее много раз во время шага гипер- параметрической оптимизации. Он должен легко уместиться в оперативной па-мяти, поскольку число строк в этом наборе данных ниже исходного: In:sampled_train_df.count() Out: 96015 Чтобы понять, что находится внутри, можно распечатать первую строку. Распе- чатка значения выполняется заметно быстро. Разумеется, ведь набор кэширован! In:sampled_train_df.first() Out: Row(duration=0.0, protocol_type=u'tcp', service=u'http', flag=u'SF', src_bytes=217.0, dst_bytes=2032.0, land=0.0, wrong_fragment=0.0, urgent=0.0, hot=0.0, num_failed_logins=0.0, logged_in=1.0, num_compromised=0.0, root_shell=0.0, su_attempted=0.0, num_root=0.0, num_file_creations=0.0, num_shells=0.0, num_access_files=0.0, num_outbound_cmds=0.0, is_host_login=0.0, is_guest_login=0.0, count=6.0, srv_count=6.0, serror_rate=0.0, srv_serror_rate=0.0, rerror_rate=0.0, srv_rerror_rate=0.0, same_srv_rate=1.0, diff_srv_rate=0.0, srv_diff_host_rate=0.0, dst_host_count=49.0, dst_host_srv_count=49.0, dst_host_same_srv_rate=1.0, dst_host_diff_\n--- Страница 341 ---\n340  Практическ ое машинное обучение в среде Spark srv_rate=0.0, dst_host_same_src_port_rate=0.02, dst_host_srv_diff_host_rate=0.0, dst_host_ serror_rate=0.0, dst_host_srv_serror_rate=0.0, dst_host_rerror_rate=0.0, dst_host_srv_rerror_rate=0.0, target=u'normal') Теперь воспользуемся созданным конвейером, чтобы выполнить несколько предсказаний и распечатать отметку F1 этого нового решения: In:full_model = full_pipeline.fit(sampled_train_df)predictions = full_model.transform(test_df) print('Отметка F1 на тестовом наборе: {0}' .format(evaluator.evaluate(predictions))) Out: Отметка F1 на тестовом наборе: 0.967037720279 Протестируем его на классификаторе из 50 деревьев. Для этого построим еще один конвейер (под именем refined_pipeline ) и заменим заключительный этап новым классификатором. Показатели результативности выглядят одинаковыми, даже несмотря на то что тренировочный набор был резко сокращен: In:clf = RandomForestClassifier( numTrees=50, maxBins=100, seed=101, labelCol='target_cat', featuresCol='features') stages = full_pipeline.getStages()[:-1] stages.append(clf) refined_pipeline = Pipeline(stages=stages)refined_model = refined_pipeline.fit(sampled_train_df) predictions = refined_model.transform(test_df) print('Отметка F1 на тестовом наборе: {0}' .format(evaluator.evaluate(predictions))) Out: Отметка F1 на тестовом наборе: 0.96774867423 Перекрестная проверка Мы можем обойтись ручной оптимизацией и найти правильную модель после исчерпывающего испытания разных конфигураций. Однако такой режим рабо-ты приведет к огромной потере времени (и возможности многократного исполь-зования программного кода) и вызовет переподгонку тестового набора данных. С другой стороны, для гиперпараметрической оптимизации предназначена пе-рекрестная проверка. Давайте посмотрим, как платформа Spark выполняет эту принципиально важную задачу. Прежде всего, поскольку тренировка будет использоваться многократно, ее результаты можно кэшировать. Поэтому занесем их в кэш после всех преобра-зований: In:pipeline_to_clf = Pipeline( stages=preproc_stages + [assembler]).fit(sampled_train_df)\n--- Страница 342 ---\nМашинное обучение с платформой Spark  341 train = pipeline_to_clf.transform(sampled_train_df).cache() test = pipeline_to_clf.transform(test_df) Классы, используемые для гиперпараметрической оптимизации посредством перекрестной проверки, содержатся в библиотеке pyspark.ml.tuning . При этом крайне важны два элемента: карта-сетка параметров (которая может быть скон- струирована при помощи ParamGridBuilder ), и фактическая процедура перекрест - ной проверки (выполняемая классом CrossValidator ). В данном примере нужно установить несколько параметров классификатора, которые не изменятся во время перекрестной проверки. Они устанавливаются в точности как в Scikit-learn, т. е. когда создается объект классификации (в дан-ном случае имена столбцов, начальное значение для генератора случайных чисел и максимальное число дискретных интервалов). Затем благодаря сеточному построителю мы выбираем, какие параметры должны быть изменены в каждой итерации алгоритма перекрестной проверки. В данном примере нужно проверить результативность классификации, изменяя максимальную глубину каждого дерева в лесу с 3 до 12 (с приращением 3) и число деревьев в лесу до 20 или 50. В конце, после того как будут заданы карта-сетка, тестируемый классификатор и число блоков, мы запускаем перекрестную проверку (методом fit). Оценщик параметров estimatorParamMaps играет крайне важную роль: он сообщает, какую модель лучше всего сохранить после перекрестной проверки. Отметим, что вы-полнение этой работы может занять 15–20 минут (за кадром выполняются трени-ровка и тестирование 4*2*3 = 24 моделей): In: from pyspark.ml.tuning import ParamGridBuilder, CrossValidator # Может занять порядка 10 минут rf = RandomForestClassifier(cacheNodeIds=True, seed=101, labelCol='target_cat', featuresCol='features', maxBins=100) grid = (ParamGridBuilder() .addGrid(rf.maxDepth, [3, 6, 9, 12]) .addGrid(rf.numTrees, [20, 50]) .build()) cv = CrossValidator(estimator=rf, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3)cvModel = cv.fit(train) Наконец, при помощи перекрестно проверенной модели можно предсказать метку, используя конвейер либо непосредственно сам классификатор. В этом слу - чае результативность выбранного перекрестной проверкой классификатора не- много лучше, чем в предыдущем случае, позволяя преодолеть отметку 0.97: In:predictions = cvModel.transform(test) print('Отметка F1 на тестовом наборе: {0}' .format(evaluator.evaluate(predictions)))\n--- Страница 343 ---\n342  Практическое машинное об учение в среде Spark Out: Отметка F1 на тестовом наборе: 0.970034862617 Более того, построив график нормализованной матрицы ошибок, также сразу становится понятным, что это решение способно обнаруживать более широкое разнообразие атак и даже менее распространенные из них: In:metrics = MulticlassMetrics(predictions.select('prediction', 'target_cat').rdd)conf_matrix = metrics.confusionMatrix().toArray()plot_confusion_matrix(conf_matrix) Нормализованная матрица ошибок Предсказанная меткаИстинная метка0 5 1015201,0 0,90,80,70,60,50,40,30,20,10,0 20 15 10 5 0 Заключительная очистка Здесь мы подходим к концу задачи классификации. Напомним о необходимости удалить все использованные переменные и созданную из кэша временную таб - лицу: In:bc_sample_rates.unpersist()sampled_train_df.unpersist()train.unpersist() После того как память платформы Spark очищена, можно выключить блокнот. резюме Это была последняя глава книги. Мы увидели способы решения задач в области науки о данных в крупном масштабе на кластере машин. Платформа Spark способ-на тренировать и тестировать алгоритмы машинного обучения с использованием\n--- Страница 344 ---\nРезюме  343 всех узлов в кластере при помощи простого интерфейса, который очень похож на Scikit-learn. Это программное решение подтвердило свою состоятельность во время работы с петабайтами информации, создавая действенную альтернативу подвыборке из набора наблюдений и онлайновому обучению. Чтобы стать экспертом по платформе Spark и потоковой обработке данных, мы настоятельно рекомендуем прочесть книгу Mastering Apache Spark, Mike Frampton, Packt Publishing. Если вы достаточно храбры, чтобы перейти на язык Scala, который является основным языком программирования для среды Spark, то следующая книга будет самой подходящей для такого перехода: Scala for Data Science, Pascal Bugnion, Packt Publishing.\n--- Страница 345 ---\nПриложение Введение в графические процессоры и платформа Theano До сих пор мы генерировали нейронные сети и выполняли задачи глубокого обуче ния, используя обычные центральные процессоры (CPU). Однако в послед- нее время широкое распространение получили вычислительные преимущества графических процессоров (GPU). Данное приложение посвящено основам GPU и их применению на платформе Theano, разработанной для решения задач в об-ласти глубокого обучения. вычисления на GPU Когда мы используем библиотеки для машинного обучения с вычислениями на обычных CPU, в частности библиотеку Scikit-learn, объем параллелизации необы - чайно ограничен, потому что алгоритм по умолчанию использует всего одно ядро, даже когда в наличии имеются многочисленные ядра. В главе о классификаци- онных и регрес сионных деревьях (CART) показаны некоторые продвинутые примеры ускорения алгоритмов Scikit-learn. В отличие от CPU, модули GPU с самого начала были разработаны, чтобы ра- ботать в параллельном режиме. Представим проецирование изображения на экран через графическую карту. Ни для кого не является секретом, что модуль GPU должен уметь одновременно обрабатывать и проецировать большой объем информации (движение, цвет и пространственность). Модули CPU, с другой сто-роны, разработаны для последовательной обработки, подходящей для задач, где необходимо больше контроля, в частности ветвление и проверка. В отличие от CPU, модули GPU состоят из большого количества ядер, которые могут обрабаты-вать тысячи задач одновременно. GPU может превзойти CPU в 100 раз и по более низкой цене. Еще одно преимущество современных модулей GPU состоит в том, что они являются относительно дешевыми, по сравнению с ультрасовременными модулями CPU. Таким образом, все это выглядит великолепно, но помните, что модуль GPU хорош только при выполнении определенного типа задач. Модуль CPU состоит из нескольких ядер, оптимизированных для последовательной серийной обработки, в то время как GPU состоит из тысяч более мелких и более эффективных ядер, предназначенных для обработки задач параллельно.\n--- Страница 346 ---\nВычисления на GPU  345 Модули CPU и GPU имеют разную архитектуру, которая делает их более под- ходящими для разных задач. Однако существует много задач, таких как проверка, отладка и переключение, где модули GPU по-прежнему неспособны работать эф-фективно вследствие своей архитектуры. Простой способ понять различие между CPU и GPU состоит в том, чтобы срав- нить то, как они обрабатывают задачи. Часто приводится аналогия с аналити-ческим и последовательным левым полушарием мозга (CPU) и всеобъемлющим правым полушарием (GPU). Правда, эту простую аналогию не стоит принимать слишком серьезно. GPU CPU Большое число ядер, но медленнее ядер CPU Малое число ядер, но намного быстрее ядер GPU Высокая пропускная способность памяти для управления ядрамиМеньшая пропускная способность памяти Целевого назначения Общего назначения Высокопараллельная обработка Последовательная обработка О GPU можно узнать больше, перейдя по следующим ссылкам: http://www .nvidia.com/object/what-is-gpu-computing.html; http://www .nvidia.com/object/gpu-applications-domain.html. Для того чтобы задействовать модули GPU для машинного обучения, требуется особая платформа. К сожалению, на данный момент, кроме платформы CUDA1, других стабильных платформ для вычислений на GPU не существует; это озна- чает, что на вашем компьютере должна быть установлена графическая карта NVIDIA. Вычисления на GPU работать без карты NVIDIA НЕ будут. Да, мы знаем, что это плохие новости для большинства пользователей компьютеров Mac, и нам действительно жаль, что не выходит по-другому, но с этим ограничением необ-ходимо смириться. Существуют другие проекты, в частности OpenCL, которые предоставляют вычисления на GPU для других брендов GPU посредством таких инициатив, как BLAS (https: //github.com/clMathLibraries/clBLAS), но они находятся в стадии интенсивной разработки и не полностью оптимизированы для прило-жений глубокого обучения в среде Python. Еще одно ограничение проекта OpenCL состоит в том, что в нем активно участвует только компания AMD, в результате чего он будет выгоден исключительно для модулей GPU производства AMD. В бли-жайшие годы (и даже десятилетия!) нет никакой надежды на появление аппа-ратно-независимого приложения на основе GPU для машинного обучения. Тем не менее стоит последить за новостями и разработками проекта OpenCL (https://www.khronos.org/opencl/). Судя по широким откликам средств массовой информа-ции, это ограничение в отношении доступности GPU может быть весьма разоча-ровывающим. По всей видимости, только компания NVIDIA ведет научно-иссле-довательские работы с целью разработки платформ GPU, и крайне маловероятно, что мы увидим какие-либо новые серьезные наработки в этой области в ближай-шие годы. 1 CUDA (англ. Compute Unified Device Architecture) – программно-аппаратная архитектура паралле льных вычислений, которая позволяет существенно увеличить вычислитель- ную производительность благодаря использованию графических процессоров фирмы Nvidia. – Прим. перев.\n--- Страница 347 ---\n346  Введение в графиче ские процессоры и платформа Theano Для использования платформы CUDA понадобится следующее. Необходимо проверить, подходит ли графическая карта на компьютере для платформы CUDA. Как минимум, это должна быть карта NVIDIA. Можно протес - тировать пригодность модуля GPU для CUDA при помощи следующих команд в окне терминала: $ su Находясь в корне, теперь наберите свой пароль: $ lspci | grep-i Nvidia Если модуль GPU действительно основан на NVIDIA, то можно скачать инстру - ментарий NVIDIA CUDA Toolkit (http://developer.nvidia.com/cuda-downloads). Во время работы над этой книгой компания NVIDIA вот-вот должна была вы - пус тить платформу CUDA версии 8, в которой предусмотрен другой порядок ин- сталляции, поэтому мы рекомендуем следовать указаниям на веб-сайте CUDA. Для дальнейших процедур инсталляции проконсультируйтесь с веб-сайтом NVIDIA: http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/#axzz3xBimv9ou. платфОрма theano – параллельные вычисления на GPU Библиотека Python Theano была первоначально разработана Джеймсом Бергстра (James Bergstra) в Монреальском университете. Она призвана обеспечить бо-лее выразительные способы написания математических функций при помощи средств символического представления (F. Bastien, P . Lamblin, R. Pascanu, J. Bergst - ra, I. Goodfellow, A. Bergeron, N. Bouchard, D. Warde-Farley and Y. Bengio. Theano: new features and speed improvements (« Theano: новые возможности и повышение быстро- действия»). NIPS 2012 Deep Learning Workshop). Интересно, что данная библиотека названа в честь ученицы Пифагора Феано, которая, возможно, была его женой. Самыми сильными сторонами библиотеки являются быстрые откомпилирован-ные на С вычисления, символические выражения и вычисления на GPU, при этом библиотека Theano находится в процессе активного развития, в результате ко-торого регулярно вносятся улучшения в виде нового функционала. Реализации с использованием библиотеки Theano гораздо шире масштабируемого машинно-го обучения, и поэтому мы сузим круг и будем использовать Theano для глубокого обучения. Посетите веб-сайт Theano для получения дополнительной информа-ции – http://deeplearning.net/software/theano/. Когда возникает потребность в выполнении более сложных вычислений на многомерных матрицах, базовая библиотека NumPy будет обращаться к дорого-стоящим циклам и итерациям, увеличивающим нагрузку на CPU, что мы и видели ранее. Библиотека Theano стремится эти вычисления оптимизировать, компили-руя их в высокооптимизированный C-код и, если это возможно, задействуя GPU. Что касается нейронных сетей и глубокого обучения, то библиотека Theano имеет полезную способность автоматически дифференцировать математические функ - ции, что очень удобно для вычисления частных производных при использовании таких алгоритмов, как обратное распространение ошибки. В настоящее время библиотека Theano используется во всех видах проектов глубокого обучения и стала наиболее применяемой платформой в этой области.\n--- Страница 348 ---\nУстановка платформы Theano  347 В последнее время были созданы новые библиотеки поверх библиотеки Theano, чтобы упростить использование функционала глубокого обучения. Учитывая крутую кривую обучения работе с библиотекой Theano, мы будем использовать библиотеки, построенные поверх Theano, в частности theanets, pylearn2 и La - sagne. устанОвка платфОрмы theano Сначала удостоверьтесь, что вы устанавливаете дорабатываемую (development) версию со страницы Theano. Отметим, что если сделать $ pip install theano , то в итоге можно столкнуться с проблемами. Установка дорабатываемой версии не-посредственно с GitHub является беспроигрышным вариантом: $ git clone git://github.com/Theano/Theano.git$ pip install Theano Если требуется обновить библиотеку Theano, то можно воспользоваться сле - дующей командой: $ sudo pip install --upgrade theano Если у вас есть вопросы и вы желаете связаться с сообществом Theano, то можно обратиться на https://groups.google.com/forum/#!forum/theano-users. Вот и все, а теперь приступим к работе!Чтобы удостовериться в том, что мы задали путь к папке с библиотекой Theano, необходимо сделать следующее: #!/usr/bin/pythonimport osimport cPickle as picklefrom six.moves import cPickle as pickle # здесь задать путь к папке theano path = '/Users/Quandbee1/Desktop/pthw/Theano/' Теперь давайте установим все необходимые библиотеки: import numpy import numpy as npfrom theano import tensorimport theano.tensor as Timport theano.tensor.nnet as nnet Для того чтобы библиотека Theano заработала на GPU (если есть карта NVIDIA и установлена платформа CUDA), сначала необходимо сконфигурировать плат - форму Theano. В обычной ситуации библиотеки NumPy и Theano используют формат числа двойной точности с плавающей точкой ( float64 ). Однако если нужно использо- вать GPU для Theano, то применяется 32-разрядный формат числа с плавающей точкой. Это означает, что необходимо изменить настройки между 32-разрядным и 64-разрядным форматами с плавающей точкой в зависимости от потребностей. Если вы желаете посмотреть, какая конфигурация используется вашей системой по умолчанию, то наберите следующее:\n--- Страница 349 ---\n348  Введение в графиче ские процессоры и платформа Theano print(theano.config.floatX) output: float64 Поменять конфигурацию на 32-разрядную для вычислений на GPU можно сле- дующим образом: theano.config.floatX = 'float32' Иногда практичнее поменять настройки через терминал. Для 32-разрядного числа с плавающей точкой наберите следующее: $ export THEANO_FLAGS=floatX=float32 Для 64-разрядного числа с плавающей точкой следует набрать следующее: $ export THEANO_FLAGS=floatX=float64 Если нужно закрепить заданную настройку за конкретным сценарием Python, то можно сделать так: $ THEANO_FLAGS=floatX=float32 python you_name_here.py Если нужно посмотреть, какой вычислительный метод используется в вашей системе Theano, то наберите следующее: print(theano.config.device) Если нужно изменить все настройки, разрядность с плавающей точкой и вы- числительный метод (GPU или CPU) конкретного фрагмента сценария, то набе-рите следующее: $ THEANO_FLAGS=device=gpu,floatX=float32 python your_script.py Это может оказаться очень удобным для тестирования и программирования. Вполне возможно, что использовать GPU все время не потребуется; иногда лучше использовать CPU для прототипирования и эскизирования и выполнить свой сце-нарий на GPU, только когда он будет готов. Сначала проверим, работает ли GPU в вашей конфигурации. Если карта NVIDIA GPU на компьютере отсутствует, то следующее ниже можно пропустить: from theano import function, config, shared, sandbox import theano.tensor as Timport numpyimport time vlen = 10 * 30 * 768 # 10 x #cores x # threads per core iters = 1000 rng = numpy.random.RandomState(22) x = shared(numpy.asarray(rng.rand(vlen), config.floatX))f = function([], T.exp(x))print(f.maker.fgraph.toposort())t0 = time.time()for i in xrange(iters): r = f()t1 = time.time()print(\"Выполнение цикла %d раз заняло %f секунд\" % (iters, t1 - t0))print(\"Result is %s\" % (r,))if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.\n--- Страница 350 ---\nУстановка платформы Theano  349 toposort()]): print('Использован cpu')else: print('Использован gpu') Теперь, когда мы знаем, как сконфигурировать платформу Theano, давайте пробежимся по нескольким простым примерам, чтобы увидеть, как она работает. В основном любая часть кода Theano имеет одинаковую структуру: 1) инициализация, г де декларируются переменные в классе; 2) компиляция, где формируются функции; 3) исполнение, где применяются функции к типам данных. Воспользуемся этими принципами в нескольких элементарных примерах век - торных вычислений и математических выражений: # инициализировать простой скаляр x = T.dscalar() fx = T.exp(T.tan(x**2)) # инициализировать требующуюся функциюtype(fx) # просто чтобы показать, что fx - это переменная theano type # скомпилировать, чтобы создать функцию tanh f = theano.function(inputs=[x], outputs=[fx]) # исполнить функцию в данном случае на числе f(10) Как было упомянуто ранее, библиотека Theano может использоваться для мате- матических выражений. Посмотрите на следующий ниже пример, где мы использу - ем мощный функционал Theano под названием автодифференцирование, который становится очень полезным для алгоритма обратного распространения ошибки: fp = T.grad(fx, wrt=x) fs= theano.function([x], fp) fs(3)output: 4.59 Теперь, когда понятно, как используются переменные и функции, выполним простую логистическую функцию: # теперь можно применить эту функцию к матрицам тоже x = T.dmatrix('x') s = 1 / (1 + T.exp(-x))logistic = theano.function([x], s)logistic([[2, 3], [.7, -2],[1.5,2.3]]) output: array([[ 0.88079708, 0.95257413],[ 0.66818777, 0.11920292],[ 0.81757448, 0.90887704]]) Здесь четко видно, что Theano предоставляет более скоростные методы при- менения функций к объектам данных, чем это было бы возможным в библиотеке NumPy.\n--- Страница 351 ---\nПредметный указатель А Автокодировщики глубокое обучение с каскадными помехоустойчивыми автокодировщиками, 166, 167 обучение без учителя, 164описание, 164 Автономная машина необходимость в наличии распределенной платформы, 275обработка больших данных, 273 Агрегация бутстрапированных выборок (бэггинг), 203Аддитивное расширение, 215Аккумуляторные переменные только для записи, распространение по всем узлам кластера, 313Аккумуляторы, переменные только для записи, распространение по всем узлам кластера, 313Алгоритм максимизации ожидания (EM), 248Анализ главных компонент (PCA) инкрементный алгоритм PCA, 244описание, 165, 181, 235 разреженный алгоритм PCA, 245рандомизированный алгоритм PCA, 242с платформой H2O, 246справочная информация, 181 Архитектура нейронной сети входной слой, 148выходной слой, 149скрытый слой, 148 Б Библиотека Python NumPy, 37pandas, 37Scikit-learn, 38SciPy, 37описание, 37 Бустинг, 214Быстрая параметрическая оптимизация при помощи рандомизированного поиска, 208В Валидатор входных данных, URL-адрес, 124 Виртуальные машины (VM) Vagrant, 279 VirtualBox, 277использование, 279настройка, 276, 310 описание, 277 Внеядерное обучение описание, 47оптимизация прецедентов, 48подвыборка как приемлемый вариант, 48создание, 50 Выбеливание, метод, 182Выпрямленный линейный узел (ReLU), функция активации, 136 Г Гиперпараметры оптимизация, 153тонкая настройка (доводка), 117 Глубина дерева, 203Глубокие сети доверия (DBN), 164Глубокое обучение масштабирование с H2O, 157предтренировка без учителя, 162 Гости, виртуализированные машины, 277Границы решения, 154, 157 График рабочей характеристики получателя (ROC), 77Графический интерфейс пользователя (GUI), 277 Д Данные, потоковая передача данных из источников, 51Дополнение нулями, 193 И Иерархический процесс Дирихле (HDP), 272Индекс Джини (индекс неоднородности), 201Инерция Нестерова, 143\n--- Страница 352 ---\nПредметный указатель  351 Инкрементное обучение, 183 Инкрементный алгоритм PCA, 244 К Каскадные помехоустойчивые автокодировщики, глубокое обучение, 166Каталог библиотек Python (PyPI) URL-адрес, 29описание, 29 Квадратичное программирование, URL-адрес, 92Классификаторы с жестким зазором, 92Классификационные и регрессионные деревья (CART) описание, 200, 214 с платформой H2O, 229 Кластеризация, алгоритм K-средних, 247Кликабельность (CTR), 23Крупномасштабное глубокое обучение с платформой H2O, 158Крупномасштабное машинное обучение на Python, 27Крупномасштабные облачные службы, URL-адрес, 183Кусочно-линейная функция потерь варианты, 97описание, 97 Л Латентное размещение Дирихле (LDA) масштабирование, 271описание, 39, 122, 263 Латентный семантический анализ (LSA), 39Линейная регрессия на основе алгоритма SGD, 174 М Масштабируемость Python, 24вертикальное масштабирование на Python, 25горизонтальное масштабирование на Python, 26научные дистрибутивы, 32описание, 21создание крупномасштабных примеров, 23 Машина градиентного бустинга (GBM) learning_rate, 216max_depth, 216subsample, 217warm_start, 217описание, 214тренировка моделей GBM, 220 хранение моделей GBM, 220 Машинное обучение в TensorFlow посредством SkFlow, 177инкрементное обучение, 183 Машинное обучение в среде Spark возможности конвейера машинного обучения (ML), 336конструирование признаков, 329оценка результативности ученика, 335перекрестная проверка, 340ручная доводка параметров, 338тренировка ученика, 334чтение набора данных, 327 Машинное обучение с Spark, описание, 326Машины Больцмана, 148, 164 Машины опорных векторов (SVM) Scikit-learn, 98кусочно-линейная функция потерь, 97описание, 91поиск нелинейных машин SVM, 101реализация на основе SGD, 104 Метод локтя, 255Метод случайных подпространств, 232Минималистский GNU для Windows (MinGW), компилятор URL-адрес, 40описание, 40 Мини-пакеты, 142Музыкальный рекомендательный механизм Spotify, ссылки, 198 Н Набор данных Bike-sharing URL-адрес, 91описание, 90 Набор данных Boston, URL-адрес, 99Набор данных Covertype URL-адрес, 91описание, 91 Набор данных Iris, URL-адрес, 99Набор данных US Census, URL-адрес, 257Наборы данных Bike-sharing об аренде велосипедов, 90BlogFeedback о переписке в блогах, ссылка, 52Buzz о переписке в социальных сетях, ссылка, 52Census-Income (KDD) с данными переписи населения, ссылка, 52Covertype о лесном покрове, 91KDD Cup 1999, ссылка, 52\n--- Страница 353 ---\n352  Предметный указат ель использование, 90 справочная информация, 51 Наискорейший спуск, алгоритм, 215Направленный ациклический граф (DAG), 309, 324 Независимые и одинаково распределенные данные (i.i.d.), 68Нейронная сеть sknn и параллелизация, 150архитектура, 135выбор архитектуры, 148гиперпараметрическая оптимизация, 153границы решения, 154инерция Нестерова, 143метод адаптивного градиента (ADAGRAD), 143метод адаптивного скользящего среднего градиентов RMSProp, 144метод устойчивого обратного распространения (resilient propagation, RPROP), 143на GPU на основе theanets, 162обратное распространение ошибки, 139прямое распространение сигнала, 137реализация, 149реализация в TensorFlow, 175регуляризация, 151типичные проблемы алгоритма обратного распространения ошибки, 141тренировка с использованием инерции, 142функция мягкого максимума softmax для классификации, 137 Нейронная сеть прямого распространения, 135Нелинейные машины SVM, поиск при помощи подвыборки, 101 О Обработка данных в среде Spark группирование таблиц в оперативной памяти, 320запись на диск RDD-наборов, 322запись на диск предобработанных объектов DataFrame, 322импортирование JSON-файлов, 317объекты DataFrames, 317описание, 316работа с пропущенными данными, 319создание таблиц в оперативной памяти, 320Обработка потоков, справочная информация, 53Обратное распространение ошибки в мини-пакетном режиме, 142описание, 139типичные проблемы, 141 Обучение без учителя автокодировщики, 164методы без учителя, 235предтренировка без учителя, 162 Объединяющий слой, 193Обычный метод наименьших квадратов (обычный МНК, OLS), 69Один против всех, стратегия, 69, 105, 132 Однородность, 254Отбор признаков, путем регуляризации, 112Отказоустойчивый распределенный набор данных (RDD), 300Ошибка импорта ImportError, 31Ошибка реконструкции, 164 П Пакетный градиентный спуск, 64Параметры разреженности, 165Перекрестная энтропия, 201Переменные описание, 170распространение по всем узлам кластера, 311 Площадь под ROC-кривой (AUC), 77Подвыборка, поиск нелинейных машин SVM, 101Полносвязный слой, 194Помехоустойчивые автокодировщики, 166Потоки данных применительно к управлению признаками, 72Потоковая передача данных из источников использование инструментов ввода-вывода pandas, 56набор данных Bike-sharing, потоковая передача, 54описание, 51особое внимание упорядочению прецедентов, 61работа с базами данных, 57экспериментирование с наборами данных, 51 Предтренировка, 164Прекомпилированные бинарники Linux, URL-адрес, 122Проблема исчезающего градиента, 136\n--- Страница 354 ---\nПредметный указатель  353 Промах, 142 Профилировщик оперативной памяти, 44Прямое распространение сигнала, 137Прямой унитарный кодировщик, справочная информация, 79 Р Радиально-базисная функция (RBF), 95Разведочный анализ данных (EDA), 236Разложение признаков, анализ главных компонент (PCA), 236Разряженный автокодировщик, URL-адрес, 166Разряженный алгоритм PCA, 245Рандомизированный алгоритм PCA, 242Рандомизированный поиск быстрая параметрическая оптимизация, 208крупные наборы данных, 210описание, 153, 208 экстремально рандомизированные деревья, 210 Распределенная платформа, необходимость в ее наличии, 275Распределенная файловая система Hadoop (HDFS), 21, 282 Расширение пространства признаков, 149Регуляризация описание, 151отбор признаков, 112 С Сверточные нейронные сети (CNN) в среде TensorFlow посредством Keras, 190вычисления на GPU, 196объединяющий слой, 193описание, 190полносвязный слой, 194применение, 195сверточный слой, 192 Сверточный слой, 192Сеточный поиск в платформе H2O, 161, 231 Сигмоидальная функция активации, 136Силуэтная отметка, 256Сингулярное разложение (SVD), 238Случайный лес алгоритм, 204параметры для бэггинга criterion, 206max_depth, 206max_features, 205min_sample_leaf, 206min_samples_split, 206n_estimators, 205 Cтохастический градиентный бустинг, 231 URL-адрес, 232 Cтохастический градиентный спуск (SGD), 67 добавление нелинейности, 114линейная регрессия, 174описание, 67применение для реализации машин SVM, 104явные высокоразмерные отображения, 115 Cтохастическое обучение определение параметров обучения алгоритма SGD, 70пакетный градиентный спуск, 64реализация алгоритма SGD в Scikit-learn, 68стохастический градиентный спуск (SGD), 67 Cубдискретизирующий слой, 193 Т Тождественное отображение, функция, 164Тренировка с использованием инерции, 142Турнир корректировки ошибки (ECT), 131 У Узлы кластера аккумуляторные переменные только для записи их распространения, 313переменные, их распространение, 311широковещательные и аккумуляторные переменные, их распространение, 311широковещательные переменные только для чтения, их распространение, 311 Универсальная теорема аппроксимации, 148Универсальный идентификатор ресурса (URI), 286Управление признаками на потоках данных, описание, 72Управление признаками на потоках потоков данных описание целевой переменной, 76применение алгоритма SGD, 84тестирование, 83хэширование признаков, 79элементарные преобразования, 82\n--- Страница 355 ---\n354  Предметный указат ель Управление признаками при помощи потоков данных, контроль, 83Усредненный стохастический градиентный спуск (ASGD), алгоритм, 109Устойчивое обратное распространение, метод (RPROP), 143 Ф Функция пакетной нормализации, URL-адрес, 198 Х Хэширование признаков, 79Хост, виртуализированная машина, 277 Ц Цикл чтения-вычисления-печати результата (REPL), 29 Ш Шаг, 193Широкие сети, 149Широковещательные и аккумуляторные переменные, распространение по всем узлам кластера, 311Широковещательные переменные только для чтения, распространение по всем узлам кластера, 311 Э Экстремально рандомизированные деревья для рандомизированного поиска, 210Экстремально рандомизированный лес URL-адрес, 207описание, 204 Экстремальный градиентный бустинг (XGBoost) обеспечение персистентности моделей, 228описание, 221параметры colsample_bytree [], 223eta [], 223lambda [], 223max_depth [], 223min_child_weight [], 223seed [], 223subsample [], 223 построение графика важности переменных, 225потоковая передача крупных наборов данных, 227регрессия, 224 справочная информация, 221 Я Явные высокоразмерные отображения, 116Ячейки, 34 A AdaBoost, алгоритм, 214ADAGRAD, алгоритм, адаптивный градиент, 143Adam, алгоритм URL-адрес, 181описание, 181 AlexNet, пример, URL-адрес, 195Anaconda, научный дистрибутив URL-адрес, 32описание, 32 B BLAS, инициатива, URL-адрес, 345 C climate, библиотека, 44conda, двоичный диспетчер библиотек, 32ConvNet-сети, сверточные нейронные сети, 190CUDA описание, 42справочная информация, 42 CUDA Toolkit инструментарий, URL-адрес, 26, 42 CUDA, описание, 26 D DataFrame, объекты таблиц данных в Spark, работа с ними, 323DeepMind, проект, 135DistBelief, нейронная сеть, 170 E Elastic Compute Cloud (EC2), эластичное вычислительное облако, 47 G Gensim, библиотека URL-адрес, 39описание, 39, 271 get-pip.py, сценарий, URL-адрес, 30Git для Windows, URL-адрес, 40Google-облако, URL-адрес, 171Gource, программа, URL-адрес, 121GPU вычисления, 196, 344, 346\n--- Страница 356 ---\nПредметный указатель  355 использование для сверточных нейронных сетей (CNN), 196нейронная сеть с theanets, 162параллельные вычисления с Theano, 346справочная информация для вычислений, 345 graphviz, программа, URL-адрес, 201 H H2O, платформа URL-адрес, 39алгоритм CART, 229алгоритм K-средних, 261, 263 анализ главных компонент (алгоритм PCA), 246крупномасштабное глубокое обучение, 158масштабирование глубокого обучения, 157описание, 39сеточный поиск, 161, 162, 229, 230 случайный лес, 229стохастический градиентный бустинг, 231 Hadoop, распределенная среда MapReduce, 289архитектура, 281менеджер ресурсов YARN, 298распределенная файловая система Hadoop (HDFS), 282экосистема, 281 I IPython, описание, 33 J Java Development Kit (JDK), комплект разработчика приложений на Java, 39Jupyter, среда интерактивного программирования URL-адрес, 33описание, 33, 34, 35 Jupyter Notebook Viewer, средство просмотра блокнотов, URL-адрес, 34 K Kaggle, конкурс в области науки о данных, URL-адрес, 221Keras, библиотека URL-адрес, 44, 186 инсталляция, 186описание, 44KSVM, редукция описание, 128 справочная информация, 128 K-средних, алгоритм допущения, 251масштабирование, 257методы инициализации, 250описание, 247подбор оптимальной величины k, 253с платформой H2O, 261, 263 L LaSVM, библиотека URL-адрес, 121описание, 121 liblinear-cdblock, библиотека, URL-адрес, 121LIBSVM, библиотека для машин опорных векторов URL-адрес, 98описание, 25, 98 M MapReduce, вычислительная парадигма выходной регистратор, 290диспетчер, 290описание, 21, 289 редуктор, 290трансформатор, 290фрагментатор данных, 290 matplotlib, библиотека URL-адрес, 38описание, 38 MLlib, библиотека, 326MrJob, библиотека, 295 N Neural Network Toolbox (NNT), инструментарий, 44NumPy, библиотека URL-адрес, 37описание, 37 NVIDIA, компания, URL-адрес, 346NVIDIA CUDA Toolkit, инструментарий, URL-адрес, 346 O OpenCL, проект, URL-адрес, 345Openssh, URL-адрес, для Windows, 280 P pandas, библиотека URL-адрес, 38\n--- Страница 357 ---\n356  Предметный указат ель описание, 26, 37 pip, менеджер библиотек URL-адрес, 30 описание, 29 Putty, клиент SSH и telnet для Windows, URL-адрес, 280PyPy, компилятор URL-адрес, 25описание, 25 pySpark библиотека, 299действия collect(), 306count(), 306countByKey(), 306first(), 306reduce(функция), 306saveAsTextFile(путь), 306take(N), 306takeOrdered(N_упорядочивание), 306takeSample(с_заменой, N, начальное число), 306 методы cache(), 306persist(хранилище), 306unpersist(), 306 Python, язык IPython, 33Jupyter, 33URL-адрес, 25, 28 вертикальное масштабирование, 25выбор между Python 2 и Python 3, 27горизонтальное масштабирование, 26инсталляция, 28интеграция с Vowpal Wabbit (VW), 124, 125 крупномасштабное машинное обучение, 27научные дистрибутивы, 32, 33 обновление библиотек, 31описание, 24преимущества, 24установка библиотек, 29, 30 Python 3 и Python 2, выбор между ними, 27 Python-Future, веб-сайт, URL-адрес, 28pyvw, библиотека, 124 S Scikit-learn библиотека Gensim, 39H2O, 39Keras, 44TensorFlow, 42 theanets, 43Theano, 41URL-адрес, 38XGBoost, 40библиотека matplotlib, 38библиотека sknn, 43другие альтернативы, 121описание, 31, 38, 98 установка вспомогательных библиотек, 44 программа Vowpal Wabbit (VW), 121 scikit-neuralnetwork, библиотека, 43SciPy, библиотека URL-адрес, 37описание, 37 SGD параметры функции decay, 187lr, 187 momentum, 187nesterov, 188optimizer, 188 реализация алгоритма в Scikit-learn справочная информация, 70 SkFlow, библиотека, машинное обучение в TensorFlow, 177sklearn, модуль, 31sklearn.svm, параметры модуля C, 99 degree, 99dual, 99epsilon, 99gamma, 99kernel, 99loss, 99nu, 99penalty, 99 sknn, библиотека URL-адрес, 43описание, 43параллелизация, 150 SofiaML, библиотека URL-адрес, 121описание, 121 softmax, функция активации, для задачи классификации, 137Spark методы coalesce(число_разделов), 305distinct(), 305\n--- Страница 358 ---\nПредметный указатель  357 filter(функция), 305 flatMap(функция), 305groupByKey(), 306intersection(другой_RDD), 306join(другой_RDD), 306map(функция), 305reduceByKey(функция), 306repartition(число_разделов), 306sample(с_заменой, доля, начальное число), 305sortByKey(по_возрастанию), 306union(другой_RDD), 306 платформа pySpark, 299на наборе данных KDD99, 326описание, 299, 326 Spyder, инструментальная среда разработки, 33SQLite, простая СУБД, справочная информация, 57 T tanh, функция активации, 137TDM-GCC x64, комплект компиляторов, URL-адрес, 42TensorBoard, инструментальная панель, 171 TensorFlow операции вычисления на GPU, 174линейная регрессия с SGD, 174реализация нейронной сети, 175 платформа cверточные нейронные сети (CNN), 190URL-адрес, 42инсталляция, 172машинное обучение с SkFlow, 177операции, 172описание, 42, 135 посредством Keras, 177ссылки, 42 theanets, библиотека URL-адрес, 43, 162 нейронная сеть на GPU, 162описание, 43, 162Theano, платформа URL-адрес, 41, 347 инсталляция, 347 описание, 41применение для параллельных вычислений на GPU, 346 U UCI, репозиторий машинного обучения, 51 V Vagrant, конфигуратор URL-адрес, 279описание, 279 VirtualBox, виртуализатор URL-адрес, 278описание, 277 vowpal_porpoise, библиотека, 124Vowpal Wabbit, программный продукт (VW) URL-адрес с информацией по компиляции, 122инсталляция, 122интеграция с Python, 124набор данных Covertype, 131описание, 25, 121 примеры, 127ускоренный набор данных Bike-sharing, 130формат данных, 122 W Wabbit Wappa, библиотека, 124WinPython, научный дистрибутив URL-адрес, 33описание, 33 word2vec, алгоритм, 39 X XGBoost, алгоритм URL-адрес для инсталляции, 40описание, 26, 40 Y Yet Another Resource Negotiator (YARN), менеджер ресурсов, 27\n--- Страница 359 ---\nКниги издательства «ДМК Пресс» можно заказать в торгово-издательском холдинге «Планета Альянс» наложенным платежом, выслав открытку или письмо по почтовому адресу: 115487, г. Москва, 2-й Нагатинский пр-д, д. 6А. При оформлении заказа следует указать адрес (полностью), по которому должны быть высланы книги; фамилию, имя и отчество получателя. Желательно также указать свой телефон и электронный адрес. Эти книги вы можете заказать и в интернет-магазине: www.alians-kniga.ru. Оптовые закупки: тел. (499) 782-38-89. Электронный адрес: books@alians-kniga.ru. Бастиан Шарден, Лука Массарон, Альберто Боскетти Крупномасштабное машинное обучение вместе с Python Главный редактор Мовчан Д. А. dmkpress@gmail.com Перевод Логунов А. В. Корректор Синяева Г. И. Верстка Чаннова А. А. Дизайн обложки Мовчан А. Г. Формат 70×100 1/16. Гарнитура «Петербург». Печать офсетная. Усл. печ. л. 33,5625. Тираж 200 экз. Веб-сайт издательства: www.дмк.рф",
      "debug": {
        "start_page": 311,
        "end_page": 360
      }
    }
  ]
}
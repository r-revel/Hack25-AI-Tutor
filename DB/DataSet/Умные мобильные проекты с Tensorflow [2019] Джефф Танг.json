{
  "title": "Умные мобильные проекты с Tensorflow [2019] Джефф Танг",
  "chapters": [
    {
      "name": "Глава 1. Начало работы с платформой T ensorFlow Mobile 24",
      "content": "--- Страница 26 --- (продолжение)\nГлава 1 Начало работы с платформой T ensorFlow Mobile Данная глава посвящена настройке среды разработки для создания всех тех приложений для iOS или Android с поддержкой TensorFlow, которые рассмат - риваю тся в остальной части книги. Мы не будем подробно обсуждать все под- держиваемые версии платформы TensorFlow, версии ОС, версии сред Xcode и Android Studio, которые могут использоваться для разработки, поскольку такую информацию можно легко найти на веб-сайте TensorFlow (http: //www. tensorfiow.org) или в Google. Вместо этого в данной главе мы кратко расскажем о примерах рабочих сред, которые позволят нам быстро углубиться в изучение и разработку всех этих удивительных приложений. Если у вас уже установлена платформа TensorFlow, а также среды разработки Xcode и Android Studio, и вы можете запускать и тестировать примеры прило-жений TensorFlow для iOS и Android, и если у вас уже установлен графический процессор NVIDIA GPU, предназначенный для более быстрой тренировки глу - боко обучающихся моделей, то вы можете пропустить эту главу либо перейти непосредственно к незнакомому разделу. В данной главе мы рассмотрим следующие темы (настройка среды разработ - ки в компьютере Raspberry Pi будет обсуждаться в главе 12 «Разработка при- ложений TensorFlow на компьютере Raspberry Pi»): настройка платформы TensorFlow; настройка среды разработки Xcode; настройка среды разработки Android Studio; TensorFlow Mobile против TensorFlow Lite; запу ск примеров приложений TensorFlow для iOS; запу ск примеров приложений TensorFlow для Android. настройка платформы Tensor Flow Платформа TensorFlow является ведущей платформой с открытым исход-ным кодом для машинного интеллекта. Когда компания Google выпустила\nГлава 1 Начало работы с платформой T ensorFlow Mobile Данная глава посвящена настройке среды разработки для создания всех тех приложений для iOS или Android с поддержкой TensorFlow, которые рассмат - риваю тся в остальной части книги. Мы не будем подробно обсуждать все под- держиваемые версии платформы TensorFlow, версии ОС, версии сред Xcode и Android Studio, которые могут использоваться для разработки, поскольку такую информацию можно легко найти на веб-сайте TensorFlow (http: //www. tensorfiow.org) или в Google. Вместо этого в данной главе мы кратко расскажем о примерах рабочих сред, которые позволят нам быстро углубиться в изучение и разработку всех этих удивительных приложений. Если у вас уже установлена платформа TensorFlow, а также среды разработки Xcode и Android Studio, и вы можете запускать и тестировать примеры прило-жений TensorFlow для iOS и Android, и если у вас уже установлен графический процессор NVIDIA GPU, предназначенный для более быстрой тренировки глу - боко обучающихся моделей, то вы можете пропустить эту главу либо перейти непосредственно к незнакомому разделу. В данной главе мы рассмотрим следующие темы (настройка среды разработ - ки в компьютере Raspberry Pi будет обсуждаться в главе 12 «Разработка при- ложений TensorFlow на компьютере Raspberry Pi»): настройка платформы TensorFlow; настройка среды разработки Xcode; настройка среды разработки Android Studio; TensorFlow Mobile против TensorFlow Lite; запу ск примеров приложений TensorFlow для iOS; запу ск примеров приложений TensorFlow для Android. настройка платформы Tensor Flow Платформа TensorFlow является ведущей платформой с открытым исход-ным кодом для машинного интеллекта. Когда компания Google выпустила\n--- Страница 27 ---\nНачало работы с платформой TensorFlow Mobile  25 TensorFlow в качестве проекта с открытым исходным кодом в ноябре 2015 года, уже существовало несколько других подобных платформ с открытым исход-ным кодом для глубокого обучения: Caffe, Torch и Theano. Согласно Google I/O от 10 мая 2018 года, платформа TensorFlow на GitHub достигла 99k звезд, по-казав рост 14k звезд за 4 месяца, в то время как платформа Caffe увеличила свой показатель только на 2K до 24K звезд. По прошествии двух лет с момен-та первого релиза это уже самая популярная платформа с открытым исход-ным кодом для тренировки и развертывания глубоко обучающихся моделей (к тому же она имеет хорошую поддержку традиционного машинного обу - чения). По состоянию на январь 2018 года платформа TensorFlow имеет по-рядка 85k звезд (https: //github.com/tensorfiow/tensorflow) на GitHub, в то время как три остальные ведущие платформы с открытым исходным кодом для глу - бокого обуче ния, Caffe (https: //github.com/BVLC/caffe), CNTK (https: //github.com/ Microsoft/CNTK) и Mxnet (http://github.com/apache/incubator-mxnet), имеют соот - ветственно более 22k, 13k и 12k звезд.  Ес ли вы немного запутались в технических жаргонизмах «машинное обучение», «глу- бокое обучение», «машинный интеллект» и «искусственный интеллект» (ИИ), то дадим краткое резюме: термины «машинный интеллект» и «ИИ» на самом деле означают одно и то же; термином «машинное обучение» обозначается область ИИ, к тому же самая по-пулярная; термином «глубокое обучение» обозначается особый тип машинного обуче-ния, а также современный и наиболее эффективный подход к решению сложных задач, таких как компьютерное зрение, распознавание и синтез речи и обработка естественно-го языка. Поэтому когда в данной книге мы говорим «ИИ», то мы в первую очередь под-разумеваем глубокое обучение, настоящего спасителя, который перенес ИИ из долгой зимы в лето. Для получения подробной информации о зиме ИИ и о глубоком обуче - нии обратитесь к веб-страницам https: //en.wikipedia.org/wiki/AI_winter и http: //www. deeplearningbook.org. Мы исходим из того, что у вас уже имеется базовое понимание принци- па работы платформы TensorFlow, но если это не так, то вы должны обра-титься к разделу краткого руководства (https://www.tensorflow.org/get_started) и учебных пособий (https://www.tensorflow.org/tutorials) веб-сайта TensorFlow или учебным пособиям Awesome TensorFlow (https: //github.com/jtoy/awesome- tensorflow). На эту тему имеется две хорошие книги: «Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn» («Python и машинное обучение: машинное обучение и глубокое обучение с помощью Python, scikit-learn и TensorFlow»), 2-е изд., Себастьян Рашка (Sebastian Raschka) и Вахид Мирджали-ли (Vahid Mirjalili), и «Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems» («Практическое машин- ное обучение с помощью пакета Scikit-Learn и платформы TensorFlow: понятия, инструменты и методы построения интеллектуальных систем»), автор Орели-ен Жерон (Aurélien Géron). Платформа TensorFlow может быть установлена в MacOS, Ubuntu или Windows. Мы рассмотрим этапы установки платформы TensorFlow версии 1.4\n--- Страница 28 ---\n26  Начало рабо ты с платформой TensorFlow Mobile из ее исходного кода в MacOS X El Capitan (10.11.6), в macOS Sierra (10.12.6) и в Ubuntu 16.04. Если у вас другая ОС или версия платформы, то для полу - чения дополнительной информации вы можете обратиться к документации по установке платформы TensorFlow (https://www.tensorflow.org/install). К тому времени, когда вы будете читать эту книгу, вероятно, выйдет уже новая вер-сия платформы TensorFlow. Хотя вы все равно сможете запускать приводимый в книге программный код с более новой версией, это утверждение не является гарантией того, что это будет именно так, поэтому для настройки платформы TensorFlow в Mac и Ubuntu мы используем исходный код TensorFlow версии 1.4; благодаря этому вы сможете легко проверить ее работоспособность и поиграть с приложениями данной книги.  С того времени (декабрь 2017 года), когда был написан приведенный выше абзац, было выпущено уже четыре новых официальных релиза TensorFlow (1.5, 1.6, 1.7 и 1.8), кото-рые можно загрузить с веб-страницы релизов по адресу https: //github.com/tensorflow/ tensorflow/releases или из репозитория исходного кода TensorFlow (https: //github.com/ tensorflow/tensorflow), а также новая версия среды разработки Xcode (9.3 по состоя-нию на май 2018 года). Новые версии TensorFlow, такие как 1.8, по умолчанию под-держивают новые версии платформы NVIDIA CUDA и библиотеки cuDNN (см. раздел «Настройка платформы TensorFlow в Ubuntu с поддержкой GPU» относительно подроб- ностей), однако мы рекомендуем обязательно просмотреть официальную документа-цию TensorFlow по установке последней версии TensorFlow с поддержкой GPU. В этой и следующих главах в качестве примера мы можем ссылаться на определенную версию TensorFlow, однако при этом будем поддерживать весь исходный код для iOS и Android и на Python в репозитории исходного кода книги по адресу https: //github.com/jeffxtang/ mobiletfbook в протестированном и, при необходимости, в обновленном виде до са- мой последней версии платформы TensorFlow, а также сред разработки Xcode и Android Studio. В целом мы будем использовать платформу TensorFlow в Mac для разра- ботки приложений для iOS и для Android и платформу TensorFlow в Ubuntu для тренировки глубоко обучающихся моделей, которые используются в при-ложениях. настройка Tensor Flow в Macos Как правило, для установки платформы TensorFlow в изолированной сре-де следует использовать виртуальную среду virtualenv, контейнерную сре-ду Docker или дистрибутив Anaconda. Но поскольку мы будем разрабатывать приложения TensorFlow для iOS и Android, используя исходный код платформы TensorFlow, то мы с тем же успехом можем выполнить сборку непосредственно самой платформы TensorFlow из ее исходного кода, и в этом случае использо-вание нативной установки с помощью менеджера пакетов pip может быть про-ще, чем другие варианты. Если вы хотите поэкспериментировать с различны-ми версиями TensorFlow, то рекомендуем установить другие версии TensorFlow с помощью одного из вариантов: virtualenv, Docker и Anaconda. Здесь мы уста-\n--- Страница 29 ---\nНачало работы с платформой TensorFlow Mobile  27 новим TensorFlow 1.4 непосредственно в вашу систему MacOS с использовани- ем менеджера пакетов pip и среды языка Python 2.7.10, устанавливаемых в ОС по умолчанию. Выполните следующие ниже действия, чтобы скачать и установить TensorFlow 1.4 в MacOS. 1. Скачать исходный код TensorFlow 1.4.0 ( zip или tar.gz ) со страницы ре- лизов платформы TensorFlow в GitHub: https: //github.com/tensorflow/ tensorflow/releases. 2. Распаковать скачанный файл и перетащить папку tensorflow‑1.4.0 в свой домашний каталог. 3. Убедиться, что у вас установлена среда разработки Xcode 8.2.1 или выше (если нет, то сначала следует прочитать раздел «Настройка среды раз- работки Xcode»). 4. Открыть новое окно терминала, затем выполнить команду cd tensor‑ flow‑1.4.0 , чтобы перейти в указанный в команде каталог. 5. Выпо лнить команду xcode‑select ‑‑install , чтобы установить инструмен- ты командной строки. 6. Выпо лнить следующие ниже команды, чтобы установить другие инстру - менты и пакеты, необходимые для сборки платформы TensorFlow1: sudo pip install six numpy wheelbrew install automakebrew install libtool./configurebrew upgrade bazel 7. Выпо лнить сборку из исходного кода платформы TensorFlow с поддерж - кой только CPU (мы рассмотрим поддержку GPU в следующем разделе) и сгенерировать пакетный файл менеджера пакетов pip с расширением .whl: bazel build --config=opt //tensorflow/tools/pip_package: build_pip_packagebazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg 8. У становить пакет TensorFlow 1.4.0 CPU: sudo pip install --upgrade /tmp/tensorflow_pkg/tensorflow -1.4.0-cp27-cp27m-macosx_10_12_ intel.whl Если во время данного процесса вы столкнулись с какой-либо ошибкой, то, честно говоря, поиск сообщения об ошибке в Google должен быть самым лучшим способом для ее исправления, поскольку в данной книге мы намере-ны сосредоточиться на советах и знаниях, труднодоступных в другом месте и полученных после долгих часов сборки и отладки практических мобильных 1 Baz el – это версия с открытым исходным кодом внутреннего для компании Google инструмента сборки. См. http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html. – Прим. перев.\n--- Страница 30 ---\n28  Начало рабо ты с платформой TensorFlow Mobile приложений TensorFlow. Одной из конкретных ошибок, которую вы можете увидеть, является Operation not permitted (Операция не разрешена) при вы- полнении команд sudo pip install . Для того чтобы ее исправить, вы можете отключить защиту целостности системы (System Integrity Protection, SIP) ва-шего Mac, перезапустив Mac и нажав клавиши Cmd+R, чтобы войти в режим восстановления, а затем перед перезапуском Mac в разделе Утилиты – Терми- нал, выполнить csrutil disable . Если вам не совсем удобно отключать SIP , то вы можете просмотреть документацию платформы TensorFlow, с тем чтобы по-пробовать один из более сложных методов установки, таких как виртуальная среда virtualenv. Если все идет хорошо, то у вас должно получиться запустить Python или предпочтительно IPython в окне терминала, а затем выполнить инструкции import tensorflow as tf и tf.__version__ и увидеть 1.4.0 на выходе. настройка Tensor Flow в UbUnTU с поДДержкой GPU Одним из преимуществ использования хорошей платформы глубокого обуче - ния, такой как TensorFlow, является ее полная поддержка использования гра- фического процессора (GPU) во время тренировки модели. Тренировка не-тривиальной модели TensorFlow на базе GPU проходит намного быстрее, чем на CPU, и в настоящее время NVIDIA предлагает самые лучшие и самые за-тратно-эффективные GPU, поддерживаемые платформой TensorFlow. При этом Ubuntu является самой лучшей ОС для совместной работы графических процессоров NVIDIA и платформы TensorFlow. Вы можете легко купить один GPU за несколько сотен долларов и установить его на недорогой настольный компьютер с операционной системой Ubuntu. Вы также можете установить NVIDIA GPU в Windows, но поддержка платформы TensorFlow в Windows не так хороша, как в Ubuntu. Для тренировки моделей, развернутых в приложениях этой книги, мы используем видеокарту NVIDIA GTX 1070, которую вы можете приобрести в Amazon или eBay примерно за $400. Есть хороший блог-пост Тима Дэттмер-са (Tim Dettmers) относительно того, какой GPU использовать для глубоко-го обучения (http://timdettmers.com/2017/04/09/which-gpu-for-deep-learning/). Пос ле того как вы получите такой GPU и установите его в операционную си- стему Ubuntu, и перед тем как вы установите платформу TensorFlow с под-держкой GPU, вам нужно установить параллельно-вычислительную плат - форму NVIDIA CUDA и 8.0 (или 9.0) и библиотеку cuDNN (библиотеку CUDA для глубоких нейронных сетей) 6.0 (или 7.0), обе из которых поддерживаются в TensorFlow 1.4.  А льтернативой настройке собственной операционной системы Ubuntu с поддержкой GPU для работы с TensorFlow является использование TensorFlow в специальной об-лачной службе с поддержкой GPU, такой как Cloud ML Engine облачной платформы Google Cloud Platform (https: //cloud.google.com/ml-engine/docs/using-gpus). У каждо-\n--- Страница 31 ---\nНачало работы с платформой TensorFlow Mobile  29 го варианта есть свои плюсы и минусы. Облачные службы обычно применяют по- временное выставление счетов. Если ваша цель состоит в тренировке или вторичной тренировке моделей для развертывания на мобильных устройствах, имея в виду, что ваши модели будут не какими-то сверхсложными, и если вы планируете выполнять тренировку машинно-обучающихся моделей в течение длительного времени, то за-тратно-эффективнее и удовлетворительнее иметь свой собственный графический процессор. Выполните следующие ниже действия, чтобы установить платформу CUDA 8.0 и библиотеку cuDNN6.0 в Ubuntu 16.04 (у вас должно получиться скачать и установить CUDA 9.0 и cuDNN7.0 аналогичным образом). 1. Найти релиз NVIDIA CUDA 8.0 GA2 на https://developer.nvidia.com/cuda -80- ga2-download-archive и сделать соответствующий выбор, как показано на приведенном ниже скриншоте: Рисунок 1.1. Подготовка к скачиванию платформы CUDA 8.0 в Ubuntu 16.04 2. Скачать базовый установщик (Base Installer), как показано на следую- щем ниже скриншоте:\n--- Страница 32 ---\n30  Начало рабо ты с платформой TensorFlow Mobile Рисунок 1.2. Выбор установочного файла CUDA 8.0 для Ubuntu 16.04 3. Открыть новый терминал и выполнить следующие ниже команды (вам также потребуется добавить две последние команды в файл .bashrc , бла- годаря чему две переменные окружения вступят в силу при следующем запуске нового терминала): sudo dpkg -i /home/jeff/Downloads/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.debsudo apt-get updatesudo apt-get install cuda -8-0 export CUDA_HOME=/usr/local/cudaexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH 4. Скачать библиотеку NVIDIA cuDNN6.0 для платформы CUDA 8.0 на https://developer.nvidia.com/rdp/cudnn-download – прежде чем вы сможете ее скачать, вам будет предложено (бесплатно) зарегистрироваться с использо-ванием учетной записи разработчика NVIDIA, как показано на следующем ниже скриншоте (выберите выделенную библиотеку cuDNN v6.0 для Linux): Рисунок 1.3. Выбор библиотеки cuDNN6.0 для платформы CUDA 8.0 в Linux\n--- Страница 33 ---\nНачало работы с платформой TensorFlow Mobile  31 5. Распаковать скачанный файл, при этом он должен находиться в установ- ленном по умолчанию каталоге ‑/Downloads , и в нем вы увидите папку с именем cuda с двумя подпапками include и lib64 . 6. Скопировать файлы cuDNN в папках include и lib64 в подпапки include и lib64 папки CUDA_HOME : sudo cp ~/Downloads/cuda/lib64/* /usr/local/cuda/lib64 sudo cp ~/Downloads/cuda/include/cudnn.h /usr/local/cuda/include Теперь мы готовы установить TensorFlow 1.4 с поддержкой GPU в Ubuntu (первые приведенные здесь два шага являются такими же, как и те, которые описаны в разделе «Настройка платформы TensorFlow в MacOS»): 1. Скачать исходный код TensorFlow 1.4.0 ( zip или tar.gz ) со страницы ре- лизов платформы TensorFlow на GitHub: https: //github.com/tensorflow/ tensorflow/releases. 2. Распаковать скачанный файл и перетащить папку в свой домашний ка- талог. 3. Скачать установщик bazel, bazel‑0.5.4 ‑installer ‑linux‑x86_64.sh с https: //github.com/bazelbuild/bazel/releases. 4. Открыть новое окно терминала, затем выполнить следующие ниже ко- манды для установки инструментов и пакетов, необходимых для сборки платформы TensorFlow: sudo pip install six numpy wheelcd ~/Downloadschmod +x bazel -0.5.4-installer-linux-x86_64.sh ./bazel-0.5.4-installer-linux-x86_64.sh --user 5. Выпо лнить сборку из исходного кода платформы TensorFlow с поддерж - кой GPU и сгенерировать пакетный файл менеджера пакетов pip с рас - ширением .whl: cd ~/tensorflow -1.4.0 ./configurebazel build --config=opt --config=cuda //tensorflow/tools/pip_package: build_pip_packagebazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg 6. У становить пакет TensorFlow 1.4.0 GPU: sudo pip install --upgrade /tmp/tensorflow_pkg/tensorflow -1.4.0-cp27-cp27mu-linux_x86_64.whl Теперь, если все идет хорошо, вы можете запустить IPython и ввести следу - ющие ниже инструкции, чтобы увидеть информацию о GPU, который исполь-зуется платформой TensorFlow: In [1]: import tensorflow as tfIn [2]: tf.__version__Out[2]: '1.4.0' In [3]: sess=tf.Session()\n--- Страница 34 ---\n32  Начало рабо ты с платформой TensorFlow Mobile 2017-12-28 23:45:37.599904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (--1), but there must be at least one NUMA node, so returning NUMA node zero2017 -12-28 23:45:37.600173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7845pciBusID: 0000:01:00.0totalMemory: 7.92GiB freeMemory: 7.60GiB2017 -12-28 23:45:37.600186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device: GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1) Поздравляю! Теперь вы готовы к тренировке глубоко обучающихся моделей, используемых в приложениях этой книги. Прежде чем мы начнем играть с на-шей новой игрушкой и использовать ее для тренировки наших моделей, а за-тем развертывать и запускать их на мобильных устройствах, давайте сначала посмотрим, что нужно для того, чтобы быть готовым к разработке мобильных приложений. настройка среДы разработки Xcode Среда разработки Xcode используется для разработки приложений iOS. Вы можете ее скачать и установить, имея в распоряжении компьютер Mac и бес - платный идентификатор Apple ID. Если ваш Mac является относительно старым и имеет операционную систему OS X El-Capitan версии 10.11.6, то вы можете скачать Xcode 8.2.1 с https://deveioper.appie.com/downioad/more. Если же у вас операционная система macOS Sierra версии 10.12.6 или более поздней версии, то вы можете скачать Xcode 9.2 или 9.3, то есть последнюю версию по состоянию на май 2018 года, по ранее указанной ссылке. Все приложения для iOS в данной книге были протестированы в среде разработки Xcode версий 8.2.1, 9.2 и 9.3. Для того чтобы установить среду Xcode, просто дважды щелкните на скачан- ном файле и следуйте инструкциям на экране. Все довольно просто. Теперь вы можете запускать приложения в симуляторе iOS, который поставляется вместе со средой Xcode, или на своем собственном устройстве iOS. Начиная с Xcode 7 вы можете запускать и отлаживать свои приложения iOS на устройстве iOS бес - платно, но если же вы желаете свои приложения распространять или публико-вать, то вам придется зарегистрироваться в качестве физического лица в про-грамме Apple Developer Program за 99 долларов США в год: https://developer. apple.com/programs/enroll. Хотя вы можете выполнять тестовый прогон многих приложений книги с помощью симулятора Xcode, некоторые приложения данной книги задей-ствуют фотокамеру вашего устройства iOS, делая снимок перед его обработ - кой глубоко обучающейся моделью, натренированной с помощью платформы TensorFlow. Кроме того, обычно предпочтительнее тестировать модель на ре-альном устройстве для определения точной производительности и потребле-\n--- Страница 35 ---\nНачало работы с платформой TensorFlow Mobile  33 ния оперативной памяти: модель, которая отлично работает в симуляторе, может завершить работу аварийно или же работать слишком медленно на ре-альном устройстве. Поэтому настоятельно рекомендуется или требуется тести-ровать и запускать приводимые в книге приложения для iOS на своем факти-ческом устройстве iOS, хотя бы раз, если не всегда. В этой книге мы исходим из того, что вы знакомы с программированием для iOS, но если вы в разработке приложений для iOS новичок, то можете по-знакомиться с ней из многочисленных и отличных онлайновых руководств, таких как руководства по iOS Рэя Вендерлика (Ray Wenderlich) (https://www.raywenderlich.com). Мы не будем касаться сложных вопросов программирова-ния для iOS; мы в основном покажем вам, как использовать API C++ платформы TensorFlow в наших приложениях для iOS с целью запуска натренированных моделей TensorFlow и выполнения всевозможных интеллектуальных задач. При этом для взаимодействия с программным кодом C++ в наших мобильных приложениях искусственного интеллекта будет использоваться программный код на двух официальных языках программирования iOS, предлагаемых ком-панией Apple: Objective-C и Swift. настройка среДы разработки android sTUdio Среда разработки Android Studio является самым лучшим инструментом раз-работки приложений для Android, и платформа TensorFlow имеет весомую поддержку ее использования. В отличие от среды разработки Xcode, вы можете установить и запустить Android Studio в Mac, Windows или Linux. Подробные требования к операционной системе вы можете найти на веб-сайте Android Studio (https://developer.android.com/studio/index.html). Здесь мы расскажем, как настроить среду разработки Android Studio 3.0 или 3.0.1 в Mac – все приложе- ния в книге были протестированы в обеих этих версиях. Сначала следует скачать среду разработки Android Studio 3.0.1 или ее по- следнюю версию, если она новее, чем 3.0.1, и, если вы не возражаете, испра-вить возможные незначительные проблемы, упомянутые в предыдущей ссыл-ке. Вы также можете скачать версии 3.0.1 или 3.0 из ее архивов на веб-странице по адресу https://developer.android.com/studio/archive.html. Затем нужно дважды щелкнуть на скачанном файле и перетащить значок Android studio.app в Applications (Приложения). Если у вас среда разработки Android Studio уже установлена, то вам будет предложено заменить ее на более новую версию. Вы можете просто выбрать Replace (Заменить). Далее нужно открыть среду разработки Android Studio и указать путь к ком- плекту разработчика Android SDK, который по умолчанию расположен в папке ~/Library/Android/sdk , в случае если у вас установлена предыдущая версия Android Studio, либо можно выбрать в меню Open an existing Android Studio project (Открыть существующий проект Android Studio), а затем перейти в каталог с ис - ходным кодом платформы TensorFlow 1.4, созданный в разделе «Настройка плат -\n--- Страница 36 ---\n34  Начало рабо ты с платформой TensorFlow Mobile формы TensorFlow в macOS», и открыть папку tensorflow/examples/android . После этого вы можете скачать Android SDK, щелкнув ссылку на сообщении Install Build Tools (Установить инструменты сборки) либо перейдя в Android Studio Tools (Инс трументы Android Studio) | Android | SDK Manager (Мене джер SDK), как показано на следующем ниже скриншоте. На вкладке SDK Tools (Инс тру- менты SDK) можно установить флажок рядом с определенной версией инстру - ментов Android SDK Tools и нажать кнопку OK, чтобы установить эту версию: Рисунок 1.4. Менеджер Android SDK для установки инструментов SDK и NDK Наконец, поскольку для загрузки и выполнения моделей TensorFlow в прило- жениях TensorFlow для Android используется нативная библиотека TensorFlow C++, вам нужно установить нативный комплект разработчика Android Native Development Kit (NDK). Это можно сделать из менеджера Android SDK, по- казанного на предыдущем скриншоте, либо скачать NDK непосредственно с https://developer.android.com/ndk/downloads/index.html. Обе версии NDK, r16b и r15c, были протестированы на предмет запуска приводимых в данной книге приложений для Android. Если вы скачаете NDK непосредственно, то вам также может потребоваться задать месторасположение Android NDK после открытия проекта Android Studio и выбора File (Файл) | Project Structure (Структура про- екта), как показано на следующем ниже скриншоте:\n--- Страница 37 ---\nНачало работы с платформой TensorFlow Mobile  35 Рисунок 1.5. Установка расположения Android NDK уровня проекта После установки и настройки Android SDK и NDK вы готовы к тестовому про- гону примеров приложений TensorFlow для Android. Tensor Flow Mobile против Tensor Flow liTe Прежде чем начать работать с примерами приложений TensorFlow для iOS и Android, давайте проясним одну общую картину. Платформа TensorFlow в на-стоящее время имеет два подхода к разработке и развертыванию приложений с применением глубокого обучения на мобильных устройствах: TensorFlow Mobile и TensorFlow Lite. Решение TensorFlow Mobile было частью платформы TensorFlow с самого начала, тогда как TensorFlow Lite представляет собой но-вый способ разработки и развертывания приложений TensorFlow, поскольку эта платформа обеспечивает лучшую производительность и меньший размер приложения. Однако существует один ключевой фактор, который позволит нам в этой книге сосредоточиться исключительно на платформе TensorFlow Mobile и посвятить рассмотрению платформы TensorFlow Lite всего одну гла-ву: на момент выхода TensorFlow 1.8 и по данным конференции разработчи-ков Google I/O, проходившей в мае 2018 года, платформа TensorFlow Lite все\n--- Страница 38 ---\n36  Начало рабо ты с платформой TensorFlow Mobile еще находится в предварительной версии. Поэтому, как рекомендует сама компания Google, в настоящее время для разработки готовых к производству мобильных приложений с поддержкой платформы TensorFlow вам нужно ис - пользовать платформу TensorFlow Mobile. Еще одна причина, по которой мы сейчас решили сосредоточиться на плат - форме TensorFlow Mobile, состоит в том, что, в отличие от платформы TensorFlow Lite, которая предлагает лишь ограниченную поддержку модельных опера-торов, платформа TensorFlow Mobile поддерживает кастомизацию, обеспе-чивающую добавление новых операторов, не поддерживаемых платформой TensorFlow Mobile по умолчанию, что, как вы увидите, происходит довольно часто в наших самых разных моделях из разных приложений ИИ. Однако в будущем, когда платформа TensorFlow Lite выйдет из предвари- тельной версии, она, вероятно, заменит собой платформу TensorFlow Mobile или, по крайней мере, преодолеет свои текущие ограничения. Для того что-бы подготовиться к этому моменту, мы подробно рассмотрим платформу TensorFlow Lite в одной из последних глав. выполнение примеров приложений Tensor Flow Для ios В последних двух разделах этой главы мы протестируем три примера приложе-ний для iOS и четыре примера приложений для Android, которые поставляются вместе с TensorFlow 1.4. Это позволит вам убедиться, что ваши мобильные сре-ды разработки TensorFlow настроены правильно, а также получить предвари-тельное впечатление о том, что способны выполнять мобильные приложения с поддержкой TensorFlow. Исходный код трех примеров приложений TensorFlow для iOS находится в папке tensorflow/examples/ios . Эти приложения называются simple (простое), camera (фотокамера) и benchmark (оценка производительности). Для того чтобы успешно выполнить эти примеры, сначала необходимо скачать одну глубоко обучающуюся модель, предварительно натренированную компанией Google, под названием Inception (Исходная) (https: //github.com/tensorflow/models/tree/ master/research/inception), предназначенную для распознавания изображений. Существует несколько версий модели Inception: от v1 до v4, с более высокой точностью в каждой новой версии. Здесь мы будем использовать Inception v1, так как примеры приложений были разработаны для нее. Скачав файл модели, скопируйте связанные с моделью файлы в папку data каждого примера: curl -o ~/graphs/inception5h.zip https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zipunzip ~/graphs/inception5h.zip -d ~/graphs/inception5hcd tensorflow/examples/ioscp ~/graphs/inception5h/* simple/data/cp ~/graphs/inception5h/* camera/data/cp ~/graphs/inception5h/* benchmark/data/\n--- Страница 39 ---\nНачало работы с платформой TensorFlow Mobile  37 Теперь перейдите в каждую папку app и выполните следующие ниже коман- ды, чтобы перед открытием и запуском приложений скачать необходимый для каждого приложения модуль: cd simplepod installopen tf_simple_example.xcworkspacecd /camerapod installopen tf_camera_example.xcworkspacecd /benchmarkpod installopen tf_benchmark_example.xcworkspace Затем вы можете запустить все три приложения на устройстве iOS или же приложения simple и benchmark в симуляторе iOS. Если после запуска прило-жения simple коснуться кнопки Run Model (Запу стить модель), то вы увидите текстовое сообщение о том, что модель Inception платформы TensorFlow за-гружена, а затем будет выведено несколько самых лучших результатов распо - знавания вмес те с показателями достоверности. Если после запуска приложения benchmark коснуться кнопки Benchmark Model (Оценка производительности), то можно увидеть среднее время, необ- ходимое для выполнения модели более 20 раз. Например, на моем iPhone 6 это занимает в среднем около 0,2089 секунды и 0,0359 секунды в симуляторе iPhone 6. Наконец, после запуска приложения camera на устройстве iOS и наведения фотокамеры устройства на окружающие предметы будут показаны предметы, которые приложение видит и распознает в режиме реального времени. выполнение примеров приложений Tensor Flow Для android В случае Android имеется четыре примера приложений TensorFlow. Это TF Classify (Классификация), TF Detect (Обнар ужение), TF Speech (Речь) и TF Stylize (Стилизация), расположенные в папке tensorflow/examples/android . Са- мый простой способ запустить эти примеры – прос то открыть проект в указан- ной выше папке с помощью Android Studio, как показано в разделе «Настройка среды разработки Android Studio», а затем внести одно изменение, отредакти- ровав файл build.gradle проекта и заменив инструкцию def nativeBuildSystem = 'bazel' на def nativeBuildSystem = 'none' . Теперь подключите устройство Android к компьютеру и выполните сборку, установите и запустите приложение, выбрав Run (Выполнить) | Run android (Запу стить android) в среде Android Studio, в результате чего на вашем устрой- стве будут установлены четыре приложения для Android с именами TF Classify, TF Detect, TF Speech и TF Stylize. Приложение TF Classify так же, как и прило-\n--- Страница 40 ---\n38  Начало рабо ты с платформой TensorFlow Mobile жение camera для iOS, использует модель TensorFlow Inception v1 для класси- фицирования объектов в режиме реального времени с помощью фотокамеры устройства. Приложение TF Detect использует другую модель под названием однократного мультирамочного детектора (Single Shot Multibox Detector, SSD) 1 вместе с MobileNet, новым набором выпущенных компанией Google глубоко обучающихся моделей, которые ориентированы, в частности, на мобильные и встроенные устройства, для обнаружения объектов и отрисовки рамок на об-наруженных объектах. Приложение TF Speech использует еще одну глубоко обучающуюся модель (распознавания речи) для прослушивания и распознава-ния небольшого набора слов, таких как yes, no, left, right, stop и go. Приложение TF Stylize использует модель изменения стиля изображений, которые видит фотокамера. Для получения более подробной информации об этих приложе-ниях вы можете обратиться к документации по указанным примерам прило-жений платформы TensorFlow для Android на веб-странице https://github.com/ tensorflow/tensorflow/tree/master/tensorflow/examples/android . резюме В этой главе мы рассмотрели способы установки платформы TensorFlow 1.4 в Mac и Ubuntu, настройки затратно-эффективного графического процессора NVIDIA GPU в Ubuntu для ускоренной тренировки моделей и настройки сред разработки Xcode и Android Studio с целью разработки мобильных приложений ИИ. Мы также показали, как запускать несколько интересных примеров при-ложений с поддержкой TensorFlow для iOS и Android. В оставшейся части кни-ги мы более подробно займемся вопросами выполнения сборки и тренировки или вторичной тренировки каждой из этих моделей, используемых в прило-жениях, и многих других, в операционной системе Ubuntu с поддержкой GPU, и покажем, как развертывать модели в приложениях для iOS и Android и писать программный код для использования моделей в своих мобильных приложе-ниях ИИ. Теперь, когда все готово, нам очень не терпится отправиться в путь. Это будет захватывающее путешествие, путешествие, которым мы, безуслов-но, будем рады поделиться с нашими друзьями. Так почему бы не начать с на-ших самых лучших друзей? Давайте посмотрим, что требуется для того, чтобы создать приложение для распознавания пород собак. 1 Т ермин SSD (однократный мультирамочный детектор объектов) используется для описания архитектур, в которых применяется одна сверточная нейронная сеть (feedforward convolutional network) для непосредственного предсказания расположе-ния областей и их классов, без применения второго этапа классификации. – Прим. перев.",
      "debug": {
        "start_page": 26,
        "end_page": 40
      }
    },
    {
      "name": "Глава 2. Классифицирование изображений с помощью трансферного обучения 39",
      "content": "--- Страница 41 --- (продолжение)\nГлава 2 Классифицирование изображений с помощью трансферного обучения Описанные в предыдущей главе примеры приложений TensorFlow для iOS, simple и camera, а также приложение для Android, TF Classify, используют модель Inception v1, предварительно натренированную нейронную глубоко обуча ющуюся модель классифицирования изображений, размещенную в от - крытом доступе компанией Google. Данная модель натренирована на одной из крупнейших и наиболее известных баз данных изображений ImageNet (http://image-net.org) с более чем 10 миллионами изображений, аннотирован-ных для целого ряда классов объектов. Модель Inception может использоваться для классифицирования изображения в один из 1000 классов, перечисленных в синонимических рядах по адресу http://image-net.org/challenges/LSVRC/2014/ browse-synsets. Эти 1000 классов объектов среди многих видов объектов вклю-чают в себя довольно много пород собак. Однако ее точность распознавания пород собак не так высока, около 70%, потому что данная модель натрениро-вана распознавать очень широкий диапазон объектов, а не конкретный набор объектов, таких как породы собак. Что делать, если мы хотим улучшить ее точность и создать на нашем смарт - фоне мобильное приложение, в котором применялась бы улучшенная модель, и, к примеру, когда мы гуляем и видим интересную собаку, мы могли бы его использовать для определения породы этой собаки? В этой главе мы сначала обсудим вопрос, почему трансферное обучение (transfer learning) и вторичная тренировка предварительно натренированных глубоко обучающихся моделей для такой задачи классифицирования изобра-жений являются наиболее затратно-эффективным способом ее выполнения. Затем мы покажем, что именно требуется для вторичной тренировки неко-торых самых лучших моделей классифицирования изображений на хорошем наборе данных о собаках, а также как разворачивать и запускать вторично на-\nГлава 2 Классифицирование изображений с помощью трансферного обучения Описанные в предыдущей главе примеры приложений TensorFlow для iOS, simple и camera, а также приложение для Android, TF Classify, используют модель Inception v1, предварительно натренированную нейронную глубоко обуча ющуюся модель классифицирования изображений, размещенную в от - крытом доступе компанией Google. Данная модель натренирована на одной из крупнейших и наиболее известных баз данных изображений ImageNet (http://image-net.org) с более чем 10 миллионами изображений, аннотирован-ных для целого ряда классов объектов. Модель Inception может использоваться для классифицирования изображения в один из 1000 классов, перечисленных в синонимических рядах по адресу http://image-net.org/challenges/LSVRC/2014/ browse-synsets. Эти 1000 классов объектов среди многих видов объектов вклю-чают в себя довольно много пород собак. Однако ее точность распознавания пород собак не так высока, около 70%, потому что данная модель натрениро-вана распознавать очень широкий диапазон объектов, а не конкретный набор объектов, таких как породы собак. Что делать, если мы хотим улучшить ее точность и создать на нашем смарт - фоне мобильное приложение, в котором применялась бы улучшенная модель, и, к примеру, когда мы гуляем и видим интересную собаку, мы могли бы его использовать для определения породы этой собаки? В этой главе мы сначала обсудим вопрос, почему трансферное обучение (transfer learning) и вторичная тренировка предварительно натренированных глубоко обучающихся моделей для такой задачи классифицирования изобра-жений являются наиболее затратно-эффективным способом ее выполнения. Затем мы покажем, что именно требуется для вторичной тренировки неко-торых самых лучших моделей классифицирования изображений на хорошем наборе данных о собаках, а также как разворачивать и запускать вторично на-\n--- Страница 42 ---\n40  Клас сифицирование изображений с помощью трансферного обучения тренированные модели в примерах приложений для iOS и Android, которые мы рассмотрели в главе 1 «Начало работы с платформой TensorFlow Mobile». Кроме того, мы рассмотрим пошаговые инструкции по добавлению поддержки TensorFlow в приложения для iOS и Android, создаваемые на языках програм-мирования Objective-C или Swift. Резюмируя, в этой главе мы рассмотрим следующие ниже темы: транс ферное обучение – что это такое и почему; вторичная тренировка с использованием модели Inception v3; вторичная тренировка с использованием моделей MobileNet; испо льзование вторично натренированных моделей в примере прило- жения для iOS; испо льзование вторично натренированных моделей в примере прило- жения для Android; добавление поддержки платформы TensorFlow в свое собственное при- ложение для iOS; добавление поддержки платформы TensorFlow в свое собственное при- ложение для Android. трансферное обучение – что это такое и почему Мы, люди, не учимся новому с нуля. Вместо этого, сознательно или нет, мы максимально возможно используем то, чему научились раньше. Трансферное обучение в ИИ пытается сделать то же самое – это метод, который берет обыч- но небольшую часть крупной натренированной модели и повторно использует ее в новой модели для родственной задачи, тем самым избавляясь от необхо-димости доступа к очень крупным тренировочным данным и вычислительным ресурсам для тренировки исходной модели. В целом трансферное обучение в ИИ по-прежнему остается открытой задачей, так как во многих ситуациях, в которых человеку требуется всего несколько эмпирических примеров, чтобы научиться понимать что-то новое, искусственному интеллекту понадобится гораздо больше времени на его тренировку, пока он не научится. Тем не менее в области распознавания изображений трансферное обучение оказалось очень эффективным. Современные глубоко обучающиеся модели распознавания изображе- ний – это, как правило, глубокие нейронные сети, или, конкретнее, глубокие сверточные нейронные сети (Convolutional Neural Network, CNN) с много- численными слоями. Нижние слои такой CNN-сети отвечают за заучивание и распознавание более низкоуровневых признаков, таких как края, контуры и части изображения, в то время как последний слой определяет категорию изображения. Для разных типов объектов, таких как породы собак или разно-видности цветов, нам не нужно заново заучивать параметры или веса нижних слоев сети. На самом деле нам пришлось бы тренировать современную CNN-сеть на протяжении многих недель с нуля, пока она не заучит все веса, как пра-\n--- Страница 43 ---\nКлассифицирование изображений с помощью трансферного обучения  41 вило, миллионы или даже больше, прежде чем она начнет распознавать изо- бражения. Трансферное обучение в случае классифицирования изображений позволяет нам просто-напросто повторно натренировать последний слой та-кой CNN-сети с помощью нашего конкретного набора изображений, как пра-вило, менее чем за час, оставляя веса всех других слоев без изменения и дости-гая примерно такой же точности, как если бы мы тренировали всю сеть с нуля в течение нескольких недель. Второе основное преимущество трансферного обучения заключается в том, что для вторичной тренировки последнего уровня CNN-сети требуется лишь небольшое количество тренировочных данных. Если бы нам пришлось трени-ровать миллионы параметров глубокой CNN-сети с нуля, то нам понадобил-ся бы очень крупный объем тренировочных данных. А вот в случае вторичной тренировки, например для распознавания пород собак, нам потребуется всего 100+ изображений для каждой породы, чтобы построить модель с более высо-кой точностью классифицирования пород собак, чем исходная модель класси-фицирования изображений.  Ес ли вы незнакомы с CNN-сетями, то приглашаем посмотреть видеоролики и заметки на одном из лучших ресурсов по этой теме, Стэнфордском курсе cs231n «CNN for Visual Recognition» (CNN-сети для визуального распознавания) (http://cs23in.stanford.edu). Еще один хороший ресурс по CNN-сетям – это глава 6 онлайновой книги Майкла Ниль- сена (Michael Nielsen) «Neural Networks and Deep Learning» («Нейронные сети и глу-бокое обучение»): http://neuralnetworksanddeeplearning.com/chap6.html#introducing_ convolutional_networks. В следующих двух разделах мы будем использовать две самые лучшие пред- варительно натренированные CNN-модели TensorFlow, а также набор данных о породах собак, для вторичной тренировки моделей и генерирования более качественных моделей распознавания пород собак. Первая модель – Inception v3, более точная модель, чем Inception v1, оптимизирована по точности, но с бо-лее крупным размером. Другая модель, MobileNet, оптимизирована по раз-меру и эффективности для работы на мобильных устройствах. Подробный список предварительно натренированных моделей, которые поддерживаются платформой TensorFlow, можно найти по адресу https: //github.com/tensorflow/ models/tree/master/research/slim#pre-trained-models. вторичная тренировка с использованием моДели incePTion v3 В исходном коде TensorFlow, который мы установили в предыдущей главе, есть сценарий Python, tensorflow/examples/image_retraining/retrain.py , который мож - но использовать для вторичной тренировки моделей Inception v3 или MobileNet. Перед запуском сценария вторичной тренировки модели Inception v3 для рас - познавания пород собак сначала необходимо скачать набор данных о собаках\n--- Страница 44 ---\n42  Клас сифицирование изображений с помощью трансферного обучения Stanford Dogs Dataset (http://vision.stanford.edu/aditya86/ImageNetDogs), который содержит изображения 120 пород собак (требуется скачать только изображе-ния по ссылке, а не аннотации). Распакуйте скачанный файл изображений собак images.tar в папку ~/Down‑ loads , и в папке ~/Downloads/Images вы должны увидеть список папок, как показа- но на следующем ниже скриншоте. Каждая папка соответствует одной породе собак и содержит около 150 изображений (вам не нужно указывать метки яв-ным образом для изображений, так как имена папок используются для марки-ровки изображений, содержащихся в папках): Рисунок 2.1. Изображения набора Dogset, распределенные по папкам, или меткам пород собак  Вы можете скачать набор данных и запустить сценарий retrain.py в Mac, так как для вы- полнения сценария на относительно небольшом наборе данных (около 20 000 изобра- жений) потребуется не слишком много времени (меньше часа), но если вы сделаете это в Ubuntu с поддержкой GPU, настроенной, как описано в предыдущей главе, то сценарий завершит свою работу за несколько минут. Более того, вторичная тренировка с большим набором данных изображений в Mac может занять несколько часов, а то и дней, поэтому имеет смысл запускать его на машине с поддержкой GPU. Если исходить из того, что вы создали каталог /tf_file , а также каталог /tf_file/dogs_bottleneck , то команда вторичной тренировки модели выглядит следующим образом: python tensorflow/examples/image_retraining/retrain.py--model_dir=/tf_files/inception-v3--output_graph=/tf_files/dog_retrained.pb--output_labels=/tf_files/dog_retrained_labels.txt--image_dir ~/Downloads/Images--bottleneck_dir=/tf_files/dogs_bottleneck Пять используемых в ней параметров нуждаются в небольшом пояснении: параметр ‑‑modei_dir обозначает путь к каталогу, в который сценарий retarin.py автоматически должен скачать модель Inception v3, если в ука-занном каталоге ее еще нет;\n--- Страница 45 ---\nКлассифицирование изображений с помощью трансферного обучения  43 параметр ‑‑output_graph обозначает имя и путь вторично натренирован- ной модели; параметр ‑‑output_labels является файлом, состоящим из имен папок (меток) набора изображений, который затем используется вторично на- тренированной моделью для классифицирования новых изображений; параметр ‑‑image_dir является путем к набору изображений, используе- мому для вторичной тренировки модели Inception v3; параметр ‑‑bottieneck_dir используется для кеширования результатов, сгенерированных в слое bottleneck (бутылочном горлышке), слое перед последним слоем; последний слой выполняет классифицирование, ис - пользуя его результаты. Во время вторичной тренировки каждое изобра-жение используется несколько раз, но значения слоя bottleneck для изо-бражения остаются неизменными даже при будущих повторах сценария retrain.py. Поэтому первый запуск занимает гораздо больше времени, так как он должен создать результаты слоя bottleneck. Во время вторичной тренировки вы будете наблюдать 3 значения через каж - дые 10 шагов, всего по умолчанию 4000 шагов. Первые и последние 20 шагов и конечная тренировочная точность выглядят следующим образом: INFO: tensorflow:2018 -01-03 10:42:53.127219: Step 0: Train accuracy = 21.0% INFO: tensorflow:2018 -01-03 10:42:53.127414: Step 0: Cross entropy = 4.767182 INFO: tensorflow:2018 -01-03 10:42:55.384347: Step 0: Validation accuracy = 3.0% (N=100) INFO: tensorflow:2018 -01-03 10:43:11.591877: Step 10: Train accuracy = 34.0% INFO: tensorflow:2018 -01-03 10:43:11.592048: Step 10: Cross entropy = 4.704726 INFO: tensorflow:2018 -01-03 10:43:12.915417: Step 10: Validation accuracy = 22.0% (N=100) INFO: tensorflow:2018 -01-03 10:56:16.579971: Step 3990: Train accuracy = 93.0% INFO: tensorflow:2018 -01-03 10:56:16.580140: Step 3990: Cross entropy = 0.326892 INFO: tensorflow:2018 -01-03 10:56:16.692935: Step 3990: Validation accuracy = 89.0% (N=100) INFO: tensorflow:2018 -01-03 10:56:17.735986: Step 3999: Train accuracy = 93.0% INFO: tensorflow:2018 -01-03 10:56:17.736167: Step 3999: Cross entropy = 0.379192 INFO: tensorflow:2018 -01-03 10:56:17.846976: Step 3999: Validation accuracy = 90.0% (N=100) INFO: tensorflow: Final test accuracy = 91.0% (N=2109) Тренировочная точность (train accuracy) – это точность классифицирова- ния на изображениях, которые нейронная сеть использовала для тренировки, и контрольная точность (validation accuracy) – точность на изображениях, ко- торые для тренировки нейронной сети не использовались. Поэтому контроль-ная точность является более надежной мерой того, насколько точной является модель, и она обычно должна быть немного меньше тренировочной точности, но ненамного, если тренировка сходится и выполняется хорошо, то есть если натренированная модель не слишком плотно подогнана под тренировочные данные (не подвержена переподгонке) и если она не подогнана под трениро-вочные данные плохо (не подвержена недоподгонке).\n--- Страница 46 ---\n44  Клас сифицирование изображений с помощью трансферного обучения Если тренировочная точность становится высокой, а контрольная точность остается низкой, то это означает, что модель переподогнана. Если же трениро- вочная точность остается низкой, то это говорит о том, что модель недоподо-гнана. Кроме того, перекрестная энтропия является значением функции поте-ри, и если вторичная тренировка выполняется хорошо, то это значение должно в целом становиться все меньше и меньше. И наконец, контрольная точность относится к изображениям, которые не использовались ни для тренировки, ни для контроля. Это, как правило, наиболее точное значение, которое мы мо-жем увидеть относительно вторично натренированной модели. Как показывают приведенные выше результаты, к концу вторичной трени- ровки мы видим, что контрольная точность аналогична тренировочной точно-сти (90% и 93% по сравнению с 3% и 21% в начале), а окончательная контроль-ная точность составляет 91%. Перекрестная энтропия также падает с 4,767 в начале до 0,379 в конце. Так что теперь у нас получилась довольно хорошая модель распознавания собак.  С целью дальнейшего повышения точности вы можете поиграть с другими параметра- ми сценария retrain.py , такими как шаги тренировки ( ‑‑how_many_training_steps ), ско- рость заучивания ( ‑‑learning_rate ) и улучшение графических данных ( ‑‑flip_left_right , ‑‑random_crop, ‑‑random_scale, ‑‑random_brightness ). Как правило, это самый утомитель- ный процесс, который сопряжен с большим объемом «грязной работы», как назвал ее Эндрю Нг (Andrew Ng), один из самых известных экспертов по глубокому обучению, в своем видеоролике Nuts and Bolts of Applying Deep Learning to speech («Составные ча-сти применения глубокого обучения к обработке речи») (видеоролик доступен по адре-су: https://www.youtube.com/watch?v=F1ka6a13S9l). Для оперативного тестирования вторично натренированной модели на пре- доставленном вами изображении (например, изображении лабрадора-ретри-вера в папке /tmp/iabi.jpg ) можно использовать еще один спенарий Python. Это сценарий label_image (пометить изображение), который вы можете запустить после того, как вы его соберете следующим образом: bazel build tensorflow/examples/image_retraining: label_imagebazel-bin/tensorflow/examples/label_image/label_image--graph=/tf_files/dog_retrained.pb--image=/tmp/lab1.jpg--input_layer=Mul--output_layer=final_result--labels=/tf_files/dog_retrained_labels.txt Вы увидите пять самых лучших результатов классифицирования, которые будут выглядеть следующим образом (поскольку сети варьируются случайным образом, вероятно, вы увидите не совсем то же самое): n02099712 labrador retriever (41): 0.75551n02099601 golden retriever (64): 0.137506n02104029 kuvasz (76): 0.0228538\n--- Страница 47 ---\nКлассифицирование изображений с помощью трансферного обучения  45 n02090379 redbone (32): 0.00943663 n02088364 beagle (20): 0.00672507 Значения ‑‑input_iayer (Mul) и ‑‑output_iayer (final_result) очень важны – они должны быть такими же, как и те, что определены в модели, для того чтобы классифицирование работало хоть мало-мальски. Если вам любопытно, как их можно получить (из графа, из модели, из файла dog_retrained.pb – вс е это на- звания одного и того же), то для этого есть два инструмента TensorFlow, кото-рые могут быть очень полезными. Первый называется summarize_graph (резю- мировать граф). Ниже показано, как его можно собрать и запустить: bazel build tensorflow/tools/graph_transforms: summarize_graphbazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=/tf_files/dog_retrained.pb Вы увидите сводные результаты, подобные следующим: No inputs spotted.No variables spotted.Found 1 possible outputs: (name=final_result, op=Softmax)Found 22067948 (22.07M) const parameters, 0 (0) variable parameters, and 99 control_edgesOp types used: 489 Const, 101 Identity, 99 CheckNumerics, 94 Relu, 94 BatchNormWithGlobalNormalization, 94 Conv2D, 11 Concat, 9 AvgPool, 5 MaxPool, 1 DecodeJpeg, 1 ExpandDims, 1 Cast, 1 MatMul, 1 Mul, 1 PlaceholderWithDefault, 1 Add, 1 Reshape, 1 ResizeBilinear, 1 Softmax, 1 Sub Тут имеется один возможный выход с именем final_result. К сожалению, иногда инструмент резюмирования графа summarize_graph не сообщает нам имя входа, так как, похоже, он путает его с узлами, используемыми для тренировки. После того как узлы, используемые только для тренировки, будут удалены, что как раз мы вскоре и обсудим, инструмент summarize_graph вернет правильное имя входа. Еще один инструмент называется TensorBoard, и он дает нам более полную картину графа модели. Если у вас платформа TensorFlow установлена непосредственно из двоичного файла, то вы можете легко и просто запустить локальный сервер TensorBoard, так как по умолчанию он установлен в папку /usr/iocai/bin. Но если вы установите TensorFlow из исходного кода, как мы это сделали ранее, то для сборки локального сервера TensorBoard вам следует выполнить следующие ниже команды: git clone https://github.com/tensorflow/tensorboardcd tensorboard/bazel build //tensorboard Теперь убедитесь, что у вас есть папка /tmp/retrained_iogs , созданная автома- тически при выполнении сценария retrain.py , и выполните команду bazel-bin/tensorboard/tensorboard --logdir /tmp/retrain_logs\n--- Страница 48 ---\n46  Клас сифицирование изображений с помощью трансферного обучения Затем запустите браузер и направьте его по URL-адресу http://locaihost:6006 . Сначала вы увидите график точности, как показано на следующем ниже скрин- шоте: Рисунок 2.2. Тренировочная и контрольная точность вторично натренированной модели Inception v3 График перекрестной энтропии на следующем ниже скриншоте выглядит так, как мы его описывали ранее, рассматривая результаты выполнения сце-нария retrain.py : Рисунок 2.3. Тренировочная и контрольная перекрестная энтропия вторично натренированной модели Inception v3 Теперь перейдите на вкладку Graphs (Графы), и вы увидите операцию с име- нем Mul и еще одну с именем final_result, как показано ниже:\n--- Страница 49 ---\nКлассифицирование изображений с помощью трансферного обучения  47 Рисунок 2.4. Узлы Mul и final_result во вторично натренированной модели На самом деле если вы предпочитаете немного повзаимодействовать с TensorFlow, то можете попробовать несколько строк кода на Python с целью выяснить имена выходного и входного слоев, как показано ниже в примере взаимодействия с IPython: In [1]: import tensorflow as tfIn [2]: g=tf.GraphDef()In [3]: g.ParseFromString(open(\"/tf_files/dog_retrained.pb\", \"rb\").read())In [4]: x=[n.name for n in g.node]In [5]: x[--1:]\n--- Страница 50 ---\n48  Клас сифицирование изображений с помощью трансферного обучения Out[5]: [u'final_result'] Обратите внимание, что этот фрагмент кода не всегда будет работать, по- скольку порядок следования узлов не гарантируется, но он часто предоставля- ет необходимую информацию или обеспечивает самопроверку. Теперь мы готовы заняться дальнейшей модификацией вторично натрени- рованной модели, в результате которой ее можно будет разворачивать и за-пускать на мобильных устройствах. Размер файла вторично натренированной модели dog_retrained.pb слишком велик, около 80 Мб, и он должен пройти два этапа оптимизации перед ее развертыванием на мобильных устройствах: 1) очистка от неиспользуемых узлов : удаление узлов модели, которые используются только во время тренировки, но не требуются во время вы-ведения заключения; 2) квантование модели: конвертирование всех 32-разрядных параметров модели с вещественным типом float в 8-разрядные значения. Это позво-лит уменьшить размер модели примерно до 25% от ее первоначального размера, сохраняя при этом точность выводимого заключения пример-но такой же.  Д окументация TensorFlow (https://www.tensorflow.org/performance/quantization) пред- лагает более подробную информацию о квантовании и о том, почему оно работает. Существует два способа выполнения двух предыдущих задач. Более старый способ подразумевает применение инструмента strip_unused (удалить неис - пользуемые), и новый способ связан с применением инструмента transform_ graph (трансформировать граф). Давайте посмотрим, как работает старый метод. Сначала выполните следу - ющие ниже команды, чтобы создать модель, в которой все неиспользуемые узлы удалены: bazel build tensorflow/python/tools: strip_unused bazel-bin/tensorflow/python/tools/strip_unused --input_graph=/tf_files/dog_retrained.pb --output_graph=/tf_files/stripped_dog_retrained.pb--input_node_names=Mul--output_node_names=final_result--input_binary=true Если выполнить приведенный выше код Python с выходным графом, то мож - но найти правильное имя входного слоя: In [1]: import tensorflow as tfIn [2]: g=tf.GraphDef()In [3]: g.ParseFromString(open(\"/tf_files/ stripped_dog_retrained.pb\", \"rb\").read())In [4]: x=[n.name for n in g.node]In [5]: x[0]Out[5]: [u'Mul']\n--- Страница 51 ---\nКлассифицирование изображений с помощью трансферного обучения  49 Теперь выполните следующую ниже команду квантования модели: python tensorflow/tools/quantization/quantize_graph.py --input=/tf_files/stripped_dog_retrained.pb --output_node_names=final_result --output=/tf_files/quantized_stripped_dogs_retrained.pb --mode=weights После этого повторно натренированная проквантованная и очищенная мо- дель quantized_stripped_dogs_retrained.pb будет готова к развертыванию и ис - пользованию в приложениях для iOS и Android. Как раз это мы и увидим в сле-дующих разделах настоящей главы. Другим способом очистки от неиспользуемых узлов и квантования моде- ли является использование инструмента transform_graph . Этот новый способ рекомендуется в TensorFlow 1.4. Он отлично работает со сценарием Python label_image, но все же вызывает неправильные результаты распознавания при развертывании в приложениях для iOS и Android. bazel build tensorflow/tools/graph_transforms: transform_graphbazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/tf_files/dog_retrained.pb--out_graph=/tf_files/transform_dog_retrained.pb--inputs='Mul'--outputs='final_result'--transforms=' strip_unused_nodes(type=float, shape=\"1,299,299,3\")fold_constants(ignore_errors=true)fold_batch_normsfold_old_batch_normsquantize_weights' На проверочных данных сценарий label_image с обеими моделями – кван - тованной quantized_stripped_dogs_retrained.pb и трансформированной transform_ dog_retrained.pd – рабо тает правильно, но только первая из них работает верно в приложениях для iOS и Android. Подробную документацию по инструменту трансформации графов смот - рите в файле README по адресу https: //github.com/tensorflow/tensorflow/blob/ master/tensorflow/tools/graph_transforms/README.md. вторичная тренировка с использованием моДелей MobileneT Очищенная и квантованная модель, сгенерированная в предыдущем разделе, по-прежнему превышает 20 Мб. Это связано с тем, что предварительно постро-енная модель Inception v3, используемая для вторичной тренировки, пред-ставляет собой крупномасштабную глубоко обучающуюся модель с более чем\n--- Страница 52 ---\n50  Клас сифицирование изображений с помощью трансферного обучения 25 миллионами параметров, и данная модель прежде всего создавалась не для мобильных устройств. В июне 2017 года компания Google выпустила модели MobileNet v1. Это в об- щей сложности 16 мобильных глубоко обучающихся моделей для TensorFlow. Эти модели имеют размер всего несколько мегабайт, от 0,47 млн до 4,24 млн па-раметров, достигая при этом приличной точности (чуть ниже модели Inception v3). Для получения дополнительной информации по данным моделям обрати-тесь к файлу README: https: //github.com/tensorflow/models/blob/master/research/ slim/nets/mobilenet_v1.md. Описанный в предыдущем разделе сценарий retrain.py также поддерживает вторичную тренировку на основе моделей MobileNet. Для этого просто выпол-ните следующую ниже команду: python tensorflow/examples/image_retraining/retrain.py --output_graph=/tf_files/dog_retrained_mobilenet10_224.pb--output_labels=/tf_files/dog_retrained_labels_mobilenet.txt--image_dir ~/Downloads/Images--bottleneck_dir=/tf_files/dogs_bottleneck_mobilenet--architecture mobilenet_1.0_224 Сгенерированный файл меток dog_retrained_labels_mobilenet.txt фактиче- ски является тем же самым, что и файл, генерируемый во время вторичной тренировки с использованием модели Inception v3. Параметр ‑‑architec‑ ture определяет одну из 16 моделей MobileNet, и значение mobilenet_i.0_224 озна чает использование модели, в которой 1.0 – это количество параметров (другие три возможных значения: 0.75, 0.50 и 0.25 (1.0 означает большин-ство параметров и дает высокую точность, но при этом самый большой раз-мер, и 0.25 – наоборо т)) и 224 – это размер входного изображения (другие три значения 192, 160 и 128). Если в конец значения параметра архитектуры добавить _quantized: –architecture mobilenet_1.0_224_quantized, то в этом слу - чае модель также будет проквантована, в результате чего получится вторич-но натренированная модель размером около 5.1 Мб. Неквантованная модель имеет размер около 17 Мб. Протестировать модель, сгенерированную ранее с помощью сценария label_ image, можно следующим образом: bazel-bin/tensorflow/examples/label_image/label_image--graph=/tf_files/dog_retrained_mobilenet10_224.pb--image=/tmp/lab1.jpg--input_layer=input--output_layer=final_result--labels=/tf_files/dog_retrained_labels_mobilenet.txt--input_height=224--input_width=224--input_mean=128--input_std=128\n--- Страница 53 ---\nКлассифицирование изображений с помощью трансферного обучения  51 n02099712 labrador retriever (41): 0.824675 n02099601 golden retriever (64): 0.144245n02104029 kuvasz (76): 0.0103533n02087394 rhodesian ridgeback (105): 0.00528782n02090379 redbone (32): 0.0035457 Обратите внимание, что при выполнении сценария label_image входной слой input_layer получает имя input . Мы можем найти это имя с помощью интерак - тивного кода IPython или инструмента резюмирования графа summarize_graph , который мы встречали ранее: bazel-bin/tensorflow/tools/graph_transforms/summarize_graph--in_graph=/tf_files/dog_retrained_mobilenet10_224.pbFound 1 possible inputs: (name=input, type=float(1), shape=[1,224,224,3])No variables spotted.Found 1 possible outputs: (name=final_result, op=Softmax)Found 4348281 (4.35M) const parameters, 0 (0) variable parameters, and 0 control_edgesOp types used: 92 Const, 28 Add, 27 Relu6, 15 Conv2D, 13 Mul, 13 DepthwiseConv2dNative, 10 Dequantize, 3 Identity, 1 MatMul, 1 BiasAdd, 1 Placeholder, 1 PlaceholderWithDefault, 1 AvgPool, 1 Reshape, 1 Softmax, 1 Squeeze Итак, встает вполне закономерный вопрос: когда мы должны использовать модель Inception v3, а когда модели MobileNet на мобильных устройствах? В тех случаях, когда вы хотите достичь максимально возможной точности, вы должны и можете использовать модель, вторично натренированную на осно-ве модели Inception v3. Если же главным соображением является скорость, то вы должны рассмотреть возможность использования модели, вторично натренированной на основе одной из моделей MobileNet с наименьшим ко-личеством параметров и размером входного изображения за счет некоторой потери точности. Инструмент benchmark_model (оценить производительность модели) даст вам точную оценку производительности модели. Сначала его нужно собрать сле-дующим образом: bazel build -c opt tensorflow/tools/benchmark: benchmark_model Затем запустите его относительно модели, вторично натренированной на основе моделей Inception v3 или MobileNet v1: bazel-bin/tensorflow/tools/benchmark/benchmark_model--graph=/tf_files/quantized_stripped_dogs_retrained.pb--input_layer=\"Mul\"--input_layer_shape=\"1,299,299,3\"--input_layer_type=\"float\"--output_layer=\"final_result\"--show_run_order=false--show_time=false--show_memory=false--show_summary=true\n--- Страница 54 ---\n52  Клас сифицирование изображений с помощью трансферного обучения Вы получите довольно длинный результат, и в конце будет строка с оценоч- ным количеством флопов, к примеру FLOPs: 11.42 B, означая, что модели, вто- рично натренированной на основе Inception v3, потребуется около 11 B флопов (операций с плавающей точкой, floating-point operations) на то, чтобы вывес - ти свое заключение. Быстродействие IPhone 6 достигает порядка 2 B флопов, поэтому для выполнения модели на IPhone 6 потребуется около 5–6 секунд. Другие современные смартфоны могут работать с быстродействием до 10 B флопов. Заменив файл графа на модель dog_retrained_mobiieneti0_224.pb , вторично натренированную на основе одной из моделей MobileNet, и перезапустив ин-струмент benchmark_tool , вы увидите, что оценочное количество флопов будет около 1.14 B, что примерно в 10 раз быстрее. использование вторично натренированных моДелей в примере приложения Для ios В простом примере для iOS, который мы увидели в главе 1 «Начало работы с платформой TensorFlow Mobile», применяется модель Inception v1. Для того чтобы наше приложение использовало модель, вторично натренированную на Inception v3, и модель, вторично натренированную на MobileNet, и при этом показывало более высокую точность распознавания пород собак, нам нуж - но внести несколько изменений в приложение. Давайте сначала посмотрим, что нужно, чтобы применить вторично натренированную модель quantized_ stripped_dogs_retrained.pd в приложении для iOS. 1. Дваж ды щелкните на файле tf_simple_example.xcworkspace в папке tensor‑ flow/examples/ios/simple , в результате чего приложение будет открыто в среде разработки Xcode. 2. Перетащит е в папку data проекта файл модели quantized_stripped_dogs_re‑ trained.pd , файл меток dog_retrained_labels.txt и файл изображения lab1. jpg, который мы использовали для тестирования сценария label_image , убедившись, что флажки установлены напротив Copy items if needed (Копировать элементы, если это необходимо) и Add to targets (Добавить в цели), как показано на следующем ниже скриншоте:\n--- Страница 55 ---\nКлассифицирование изображений с помощью трансферного обучения  53 Рисунок 2.5. Добавление в приложение файла вторично натренированной модели и файла меток 3. Ще лкните на файле RunModelViewController.mm в среде Xcode, который ис - пользует API C++ платформы TensorFlow для обработки входного изо- бражения, его прогона через модель Inception v1 и получения результата классифицирования изображения, и замените в нем следующие строки: NSString* network_path = FilePathForResourceName(@\"tensorflow_inception_graph\", @\"pb\");NSString* labels_path = FilePathForResourceName(@\"imagenet_comp_graph_label_strings\", @\"txt\");NSString* image_path = FilePathForResourceName(@\"grace_hopper\", @\"jpg\"); на приведенные ниже с правильными именами файла модели, файла меток и файла тестового изображения: NSString* network_path = FilePathForResourceName(@\"quantized_stripped_dogs_retrained\", @\"pb\");NSString* labels_path = FilePathForResourceName(@\"dog_retrained_labels\", @\"txt\");NSString* image_path = FilePathForResourceName(@\"lab1\", @\"jpg\"); 4. Т акже в файле RunModelViewController.mm измените значение 224 в инструк - ции const int wanted_width = 224 ; и инструкции const int wanted_height = 224; на 299 и значение в инструкции const float input_mean = 117.0f ; и инструк - ции const float input_std = 1.0f ; на 128.0f . Это необходимо для того, что- бы входное изображение соответствовало размеру, требуемому нашей мо делью, вторично натренированной на основе Inception v3 (в отличие от v1). 5. Изменит е значения имен входных и выходных узлов с: std:: string input_layer = \"input\";std:: string output_layer = \"output\";\n--- Страница 56 ---\n54  Клас сифицирование изображений с помощью трансферного обучения на приведенные ниже правильные значения: std:: string input_layer = \"Mul\"; std:: string output_layer = \"final_result\"; 6. Нак онец, вы можете отредактировать файл dog_retrained_labels.txt и удалить ведущую цепочку символов nxxxx в каждой строке (например, чтобы удалить префикс n02099712 в строке n02099712 labrador retriever) – в Mac вы можете сделать это, удерживая нажатой клавишу Option, а за- тем сделав выбор блока и удаление – так результаты распознавания бу - дут более удобочитаемыми. Теперь запустите приложение и нажмите кнопку Run Model (Запу стить мо- дель) в консольном окне Xcode или в окне редактирования приложения, и вы увидите следующие ниже результаты распознавания, которые вполне согласу - ются с результатами выполнения сценария label_image : Predictions: 41 0.645 labrador retriever 64 0.195 golden retriever 76 0.0261 kuvasz 32 0.0133 redbone 20 0.0127 beagle Для того чтобы использовать модель dog_retrained_mobilenet10_224.pb , по- вторно натренированную на основе MobileNet (mobilenet_1.0_224_quantized), мы проделаем шаги, аналогичные предыдущим, и в шагах 2 и 3 будем исполь-зовать файл dog_retrained_mobilenet10_224.pb , но в шаге 4 мы должны оставить строки const int wanted_width = 224; и const int wanted_height = 224; без изме- нений, а только внести изменения в строки const float input_mean и const float input_std , поменяв на 128. Наконец, в шаге 5 мы должны использовать строки std:: string input_layer = \"input\"; и std:: string output_layer = \"final_result\"; . Эти параметры аналогичны параметрам, используемым в сценарии label_image для модели dog_retrained_mobilenet10_224.pb . Запустите приложение еще раз, и вы увидите, что самые лучшие результаты распознавания будут похожими. использование вторично натренированных моДелей в примере приложения Для android Применить нашу модель, вторично натренированную на Inception v3, и мо-дель, вторично натренированную на MobileNet, в приложении TF Classify для Android тоже довольно просто. Выполните следующие ниже действия, чтобы протестировать обе вторично натренированные модели. 1. Откройт е пример приложения TensorFlow для Android, расположенный в папке tensorfiow/exampies/Android , с помощью среды разработки Android Studio.\n--- Страница 57 ---\nКлассифицирование изображений с помощью трансферного обучения  55 2. Перетащит е две вторично натренированные модели, quantized_stripped_ dogs_retrained.pb и dog_retrained_mobilenet10_224.pb , а также файл меток dog_retrained_labels.txt в папку ресурсов assets приложения. 3. Откройт е файл ClassifierActivity.java и для использования модели, вто- рично натренированной на модели Inception v3, замените следующий программный код: private static final int INPUT_SIZE = 224;private static final int IMAGE_MEAN = 117;private static final float IMAGE_STD = 1;private static final String INPUT_NAME = \"input\";private static final String OUTPUT_NAME = \"output\"; приведенными ниже строками: private static final int INPUT_SIZE = 299;private static final int IMAGE_MEAN = 128;private static final float IMAGE_STD = 128;private static final String INPUT_NAME = \"Mul\";private static final String OUTPUT_NAME = \"final_result\";private static final String MODEL_FILE = \"file:///android_asset/quantized_stripped_dogs_retrained.pb\";private static final String LABEL_FILE =\"file:///android_asset/dog_retrained_labels.txt\"; 4. Либо для использования модели, вторично натренированной на модели MobileNet, замените указанный выше программный код на следующие ниже строки: private static final int INPUT_SIZE = 224;private static final int IMAGE_MEAN = 128;private static final float IMAGE_STD = 128;private static final String INPUT_NAME = \"input\";private static final String OUTPUT_NAME = \"final_result\";private static final String MODEL_FILE = \"file:///android_asset/dog_retrained_mobilenet10_224.pb\";private static final String LABEL_FILE = \"file:///android_asset/dog_retrained_labels.txt\"; 5. По дключите устройство Android к компьютеру и запустите на нем при- ложение. Затем нажмите на приложении TF Classify и наведите фото-камеру на несколько фотографий собак, и вы увидите самые лучшие ре-зультаты на экране. Вот и все, что нужно для того, чтобы применить две вторично натрениро- ванные модели в приложениях TensorFlow для iOS и Android. Теперь, когда вы увидели, как использовать наши вторично натренированные модели в при-мерах приложений, следующее, что вы, возможно, захотите узнать, – это как добавлять поддержку платформы TensorFlow в свое собственное новое или сущест вующее приложение для iOS или Android, чтобы начать внедрять мощь\n--- Страница 58 ---\n56  Клас сифицирование изображений с помощью трансферного обучения ИИ в свои собственные мобильные приложения. Это то, что мы обсудим под- робно в оставшейся части данной главы. Добавление платформы Tensor Flow в свое собственное приложение Для ios В более ранней версии добавлять платформу TensorFlow в свое собственное приложение было очень утомительно, и эта работа требовала использования процесса ручной сборки платформы и других настроек, производимых вруч-ную. В платформе TensorFlow 1.4 данный процесс стал довольно прост, и тем не менее на веб-сайте платформы TensorFlow подробные шаги данного про-цесса не очень хорошо задокументированы. Кроме того, отсутствует доку - ментация по использованию платформы TensorFlow в приложениях для iOS на языке Swift; все примеры приложений TensorFlow для iOS написаны на язы-ке Objective-С, который обращается к API C++ платформы TensorFlow. Давайте посмотрим, что тут можно усовершенствовать. Добавление платформы Tensor Flow в свое собственное приложение Для ios на языке objec Tive-c Сначала выполните следующие ниже действия, чтобы добавить платформу TensorFlow с функционалом классифицирования изображений в свое прило-жение для iOS на языке Objective-C (мы начнем с нового приложения, но вы можете смело пропустить первый шаг, если вам нужно добавить платформу TensorFlow в существующее приложение). 1. 1. В среде Xcode выберите File (Файл) | New (Создать) | Project… (Про - ект), выберите шаблон проекта Single View App (Прос тое приложение с одним экраном), затем Next (Далее), введите HelloTensorFlow в качестве имени продукта, выберите Objective-C в качестве языка программиро- вания, затем нажмите Next (Далее) и, перед тем как нажать Create (Соз- дать), выберите местоположение проекта. Закройте окно проекта в сре-де Xcode (поскольку мы откроем файл рабочего пространства проекта из-за использования им модуля позднее). 2. Откройт е окно терминала и командой cd перейдите в местоположение своего проекта, а затем создайте новый файл с именем Podfile со следу - ющим ниже содержимым: target 'HelloTensorFlow'pod 'TensorFlow ‑experimental' 3. Выпо лните команду pod install , чтобы скачать и установить модуль TensorFlow.\n--- Страница 59 ---\nКлассифицирование изображений с помощью трансферного обучения  57 4. Откройт е файл HelloTensorFlow.xcworkspace в среде Xcode, затем перета- щите два файла ( ios_image_load.mm и ios_image_load.h ), которые занима- ются загрузкой изображений, из каталога tensorflow/examples/ios/simple с примерами TensorFlow для iOS в папку проекта HelloTensorFlow . 5. 5. Перетащите в папку проекта обе модели, quantized_stripped_dogs_re‑ trained.pb и dog_retrained_mobilenet10_224.pb , файл меток dog_retrained_la‑ bels.txt и несколько файлов тестовых изображений, после чего вы долж - ны увидеть что-то вроде этого: Рисунок 2.6. Добавление служебных файлов, файлов моделей, файла меток и файлов изображений 6. Переименуйт е файл ViewController.m в ViewController.mm , так как в этом файле мы будем смешивать программный код C++ и Objective-C, связан- ный с вызовом API C++ платформы TensorFlow, обработкой ввода изобра-жений и обработкой выведения заключения. Затем перед объявлением @interface ViewController добавьте следующие ниже директивы #include и прототип функции (функциональное описание внешней функции): #include <fstream>#include <queue>#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/public/session.h\"#include \"ios_image_load.h\" NSString* RunInferenceOnImage(int wanted_width, int wanted_height, std:: string input_ layer, NSString *model);\n--- Страница 60 ---\n58  Клас сифицирование изображений с помощью трансферного обучения 7. В конце файла ViewController.mm добавьте следующий ниже программ- ный код, скопированный из файла tensorflow/example/ios/simple/RunModel‑ ViewController.mm с небольшим изменением функции RunInferenceOnImage для приема разных вторично натренированных моделей с разными раз- мерами входа и именами входного слоя: namespace { class IfstreamInputStream: public:: google:: protobuf:: io:: CopyingInputStream { static void GetTopN( bool PortableReadFileToProto(const std:: string& file_name, NSString* FilePathForResourceName(NSString* name, NSString* extension) { NSString* RunInferenceOnImage(int wanted_width, int wanted_height, std:: string input_layer, NSString *model) { 8. По-пре жнему находясь в файле ViewController.mm , в конце метода viewDidLoad сначала добавьте программный код, который добавляет надпись, информирующую пользователей о том, что они могут делать с приложением: UILabel *lbl = [[UILabel alloc] init];[lbl setTranslatesAutoresizingMaskIntoConstraints: NO];lbl.text = @\"Tap Anywhere\";[self.view addSubview: lbl]; Затем добавьте ограничения для центрирования надписи на экране: NSLayoutConstraint *horizontal = [NSLayoutConstraint constraintWithItem: lbl attribute: NSLayoutAttributeCenterXrelatedBy: NSLayoutRelationEqual toItem: self.viewattribute: NSLayoutAttributeCenterX multiplier:1 constant:0]; NSLayoutConstraint *vertical = [NSLayoutConstraint constraintWithItem: lbl attribute: NSLayoutAttributeCenterYrelatedBy: NSLayoutRelationEqual toItem: self.viewattribute: NSLayoutAttributeCenterY multiplier:1 constant:0]; [self.view addConstraint: horizontal]; [self.view addConstraint: vertical]; Наконец, добавьте распознаватель жестов: UITapGestureRecognizer *recognizer = [[UITapGestureRecognizer alloc] initWithTarget: self action:@selector(tapped:)];[self.view addGestureRecognizer: recognizer];\n--- Страница 61 ---\nКлассифицирование изображений с помощью трансферного обучения  59 9. В обработчике касаний сначала создаются два действия оповещения alert , позволяющих пользователю выбирать вторично натренирован- ную модель: UIAlertAction* inceptionV3 = [UIAlertAction actionWithTitle:@\"Inception v3 Retrained Model\" style: UIAlertActionStyleDefault handler:^(UIAlertAction * action) { NSString *result = RunInferenceOnImage(299, 299, \"Mul\", @\"quantized_stripped_dogs_retrained\");[self showResult: result]; }];UIAlertAction* mobileNet = [UIAlertAction actionWithTitle:@\"MobileNet 1.0 Retrained Model\" style: UIAlertActionStyleDefault handler:^(UIAlertAction * action) { NSString *result = RunInferenceOnImage(224, 224, \"input\", @\"dog_retrained_mobilenet10_224\");[self showResult: result]; }]; Затем создайте действие none и добавьте все три действия alert в кон- троллер предупреждений alert и представьте его: UIAlertAction* none = [UIAlertAction actionWithTitle:@\"None\" style: UIAlertActionStyleDefault handler:^(UIAlertAction * action) {}]; UIAlertController* alert = [UIAlertController alertControllerWithTitle:@\"Pick a Model\" message: nil preferredStyle: UIAlertControllerStyleAlert];[alert addAction: inceptionV3];[alert addAction: mobileNet];[alert addAction: none];[self presentViewController: alert animated: YES completion: nil]; 10. Р езультат вывода отображается в виде еще одного контроллера опове- щений в методе showResult : ‑(void) showResult:(NSString *)result { UIAlertController* alert = [UIAlertController alertControllerWithTitle:@\"Inference Result\" message: result preferredStyle: UIAlertControllerStyleAlert];UIAlertAction* action = [UIAlertAction actionWithTitle:@\"OK\" style: UIAlertActionStyleDefault handler: nil];[alert addAction: action];[self presentViewController: alert animated: YES completion: nil]; } Корневой код, связанный с вызовом платформы TensorFlow, находится в методе RunInferenceOnImage . Он слегка модифицирован на основе приложе- ния TensorFlow simple для iOS и состоит в том, что сначала создается сеанс TensorFlow и граф: tensorflow:: Session* session_pointer = nullptr;tensorflow:: Status session_status = tensorflow:: NewSession(options, &session_pointer);\n--- Страница 62 ---\n60  Клас сифицирование изображений с помощью трансферного обучения std:: unique_ptr<tensorflow:: Session> session(session_pointer); tensorflow:: GraphDef tensorflow_graph;NSString* network_path = FilePathForResourceName(model, @\"pb\");PortableReadFileToProto([network_path UTF8String], &tensorflow_graph);tensorflow:: Status s = session ‑>Create(tensorflow_graph); Затем загружается файл меток и файл изображения, а потом данные изобра- жения конвертируются в соответствующие тензорные данные: NSString* labels_path = FilePathForResourceName(@\"dog_retrained_labels\", @\"txt\"); NSString* image_path = FilePathForResourceName(@\"lab1\", @\"jpg\");std:: vector<tensorflow:: uint8> image_data = LoadImageFromFile([image_path UTF8String], &image_width, &image_height, &image_channels);tensorflow:: Tensor image_tensor(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({1, wanted_height, wanted_width, wanted_channels}));auto image_tensor_mapped = image_tensor.tensor<float, 4>();tensorflow:: uint8* in = image_data.data();float* out = image_tensor_mapped.data();for (int y = 0; y < wanted_height; ++y) { const int in_y = (y * image_height) / wanted_height; } И наконец, вызывается метод run сеанса TensorFlow с тензорными данными изображения и именем входного слоя, принимается возвращенный результат и обрабатывается для получения пяти самых лучших окончательных результа-тов со значениями достоверности, превышающими порог: std:: vector<tensorflow:: Tensor> outputs;tensorflow:: Status run_status = session ‑>Run({{input_layer, image_tensor}},{output_layer}, {}, &outputs); tensorflow:: Tensor* output = &outputs[0];const int kNumResults = 5;const float kThreshold = 0.01f;std:: vector<std:: pair<float, int> > top_results;GetTopN(output ‑>flat<float>(), kNumResults, kThreshold, &top_results); В остальной части настоящей книги мы выполним реализацию разных версий метода RunInferenceOnxxx для запуска различных моделей с разными входами. Поэтому если вы не до конца понимаете некоторые части приве-денного выше программного кода, то не переживайте. Через пару-тройку приложений вы будете чувствовать себя комфортно и уже сможете написать свою собственную логику выведения заключения для новой пользователь-ской модели. Кроме того, полноценное приложение для iOS, HelloTensorFlow, находится в репозитории исходного кода примеров книги.\n--- Страница 63 ---\nКлассифицирование изображений с помощью трансферного обучения  61 Теперь запустите приложение в симуляторе или на реальном устройстве iOS. Сначала вы увидите следующее ниже диалоговое окно с просьбой выбрать вто- рично натренированную модель: Рисунок 2.7. Выбор другой вторично натренированной модели для выведения заключения После выбора модели вы увидите результаты заключения: Рисунок 2.8. Результаты заключения на основе разных вторично натренированных моделей\n--- Страница 64 ---\n62  Клас сифицирование изображений с помощью трансферного обучения Обратите внимание, что модель, вторично натренированная на основе MobileNet, работает намного быстрее, примерно одну секунду на iPhone 6, чем модель, вторично натренированная на основе Inception v3, которая работает около семи секунд на том же самом iPhone. Добавление платформы Tensor Flow в свое собственное приложение Для ios на языке swiFT Язык Swift стал одним из самых элегантных современных языков программи-рования с момента своего рождения в июне 2014 года. Поэтому для некоторых разработчиков будет приятно и полезно интегрировать современную плат - форму TensorFlow в свое современное приложение для iOS на языке Swift. Шаги для этого аналогичны шагам для приложения на языке Objective-C, но с опре-деленным трюком, связанным с языком Swift. Если вы уже выполнили шаги в части, связанной с языком Objective-С, то обнаружите, что некоторые шаги повторяются. Тем не менее ниже предоставлены все шаги полностью для тех читателей, кто мог пропустить раздел, посвященный языку Objective-C, и пе-рейти к языку Swift непостредственно. 1. В среде Xcode выберите File (Файл) | New (Создать) | Project… (Проек т), выберите шаблон проекта Single View App (Прос тое приложение с од- ним экраном), затем Next (Далее), введите HelloTensorFiow_swift в качест - ве имени продукта, выберите Swift в качестве языка, нажмите кнопку Next (Далее) и, перед тем как нажать Create (Создать), выберите место- положение проекта. Закройте окно проекта в среде Xcode (поскольку мы откроем файл рабочего пространства проекта из-за использования им модуля позднее). 2. Откройт е окно терминала и командой cd перейдите в местоположение своего проекта, а затем создайте новый файл с именем Podfile со следу - ющим ниже содержимым: target 'HelloTensorFlow_Swift'pod 'TensorFlow ‑experimental' 3. Выпо лните команду pod install , чтобы скачать и установить модуль TensorFlow. 4. Откройт е файл HelloTensorFlow_Swift.xcworkspace в среде Xcode, затем пе- ретащите два файла ( ios_image_load.mm и ios_image_load.h ), которые зани- маются загрузкой изображений, из каталога ten‑sorflow/examples/ios/sim‑ ple с примерами TensorFlow для iOS в папку проекта HelloTensorFlow_Swift . Когда вы добавите оба файла в проект, то увидите окно сообщения, как показано на следующем ниже скриншоте, запрашивающее, не хотите ли вы настроить связующий заголовок Objective-C, который необходим про-граммному коду Swift для вызова кода C++ или Objective-C. Поэтому на-жмите кнопку Create Bridging Header (Создать связующий заголовок):\n--- Страница 65 ---\nКлассифицирование изображений с помощью трансферного обучения  63 Рисунок 2.9. Создание связующего заголовка при добавлении файла C++ 5. Кроме того, перетащите в папку проекта обе модели, quantized_stripped_ dogs_retrained.pb и dog_retrained_mobilenet10_224.pb , файл меток dog_re‑ trained_labels.txt и несколько файлов тестовых изображений, после чего вы должны увидеть что-то вроде этого: Рисунок 2.10. Добавление служебных файлов, файлов моделей, файла меток и файлов изображений\n--- Страница 66 ---\n64  Клас сифицирование изображений с помощью трансферного обучения 6. С оздайте новый файл с именем Runinference.h со следующим ниже про- граммным кодом (один из трюков заключается в том, что мы должны применить класс Objective-C в качестве обертки метода RunInferenceOn‑ Image на следующем шаге, чтобы наш программный код на языке Swift смог выполнить его косвенный вызов; в противном случае произойдет ошибка сборки): #import <Foundation/Foundation.h>@interface RunInference_Wrapper: NSObject ‑‑ (NSString *)run_inference_wrapper:(NSString *)name; @end 7. С оздайте еще один файл с именем RunInference.mm , который начинается со следующих ниже директив include и прототипа функции: #include <fstream>#include <queue>#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/public/session.h\"#include \"ios_image_load.h\" NSString* RunInferenceOnImage(int wanted_width, int wanted_height, std:: string input_ layer, NSString *model); 8. Д обавьте в файл RunInference.mm следующий ниже программный код реа- лизации обертки RunInference_Wrapper , определенной в его файле .h: @implementation RunInference_Wrapper ‑ (NSString *)run_inference_wrapper:(NSString *)name { if ([name isEqualToString:@\"Inceptionv3\"]) return RunInferenceOnImage(299, 299, \"Mul\", @\"quantized_stripped_dogs_retrained\"); else return RunInferenceOnImage(224, 224, \"input\", @\"dog_retrained_mobilenet10_224\"); }@end 9. В конце файла RunInference.mm добавьте точно такие же методы, что и в файле ViewController.mm в разделе языка Objective-C, слегка отлича- ющиеся от методов в файле tensorflow/example/ios/simple/RunModelViewCon‑ troller.mm : class IfstreamInputStream: public namespace { class IfstreamInputStream: public:: google:: protobuf:: io:: CopyingInputStream { static void GetTopN( bool PortableReadFileToProto(const std:: string& file_name, NSString* FilePathForResourceName(NSString* name, NSString* extension) {\n--- Страница 67 ---\nКлассифицирование изображений с помощью трансферного обучения  65 NSString* RunInferenceOnImage(int wanted_width, int wanted_height, std:: string input_ layer, NSString *model) { 10. Т еперь откройте файл ViewController.swift и в конце метода viewDidLoad сначала добавьте программный код, который добавляет надпись, инфор-мирующую пользователей о том, что они могут делать с приложением: let lbl = UILabel()lbl.translatesAutoresizingMaskIntoConstraints = falselbl.text = \"Tap Anywhere\"self.view.addSubview(lbl) Затем ограничения для центрирования надписи на экране: let horizontal = NSLayoutConstraint(item: lbl, attribute:.centerX, relatedBy:.equal, toItem: self.view, attribute:.centerX, multiplier: 1, constant: 0) let vertical = NSLayoutConstraint(item: lbl, attribute:.centerY, relatedBy:.equal, toItem: self.view, attribute:.centerY, multiplier: 1, constant: 0) self.view.addConstraint(horizontal) self.view.addConstraint(vertical) Наконец, добавьте распознаватель жестов: let recognizer = UITapGestureRecognizer(target: self, action: #selector(ViewController.tapped(_:)))self.view.addGestureRecognizer(recognizer) 11. В обработчике касаний мы сначала добавляем действие оповещения, по- зволяющее пользователю выбрать вторично натренированную модель Inception v3: let alert = UIAlertController(title: \"Pick a Model\", message: nil, preferredStyle:.actionSheet)alert.addAction(UIAlertAction(title: \"Inception v3 Retrained Model\", style:.default) {action in let result = RunInference_Wrapper().run_inference_wrapper(\"Inceptionv3\")let alert2 = UIAlertController(title: \"Inference Result\", message: result, preferredStyle:.actionSheet)alert2.addAction(UIAlertAction(title: \"OK\", style:.default) {action2 in})self.present(alert2, animated: true, completion: nil) }) Затем создаем еще одно действие для вторично натренированной моде-ли MobileNet, а также действие none, перед тем как его представить: alert.addAction(UIAlertAction(title: \"MobileNet 1.0 Retrained Model\", style:.default) {action in let result = RunInference_Wrapper().run_inference_wrapper(\"MobileNet\")\n--- Страница 68 ---\n66  Клас сифицирование изображений с помощью трансферного обучения let alert2 = UIAlertController(title: \"Inference Result\", message: result, preferredStyle:.actionSheet)alert2.addAction(UIAlertAction(title: \"OK\", style:.default) {action2 in}) self.present(alert2, animated: true, completion: nil)})alert.addAction(UIAlertAction(title: \"None\", style:.default) {action in}) self.present(alert, animated: true, completion: nil) 12. Откройт е файл HelloTensorFlow_Swift ‑Bridging ‑Header.h и добавьте в него одну строку с директивой: #include \"RunInference.h\" . Теперь запустите приложение в симуляторе, и вы увидите сообщение кон- троллера с просьбой выбрать модель: Рисунок 2.11. Выбор вторично натренированной модели для выведения заключения И результаты заключения для разных вторично натренированных моделей:\n--- Страница 69 ---\nКлассифицирование изображений с помощью трансферного обучения  67 Рисунок 2.12. Результаты заключения на основе разных вторично натренированных моделей Вот и все. Теперь, когда вы знаете, что именно требуется для того, чтобы добавлять мощные модели TensorFlow в свои приложения для iOS, не важно, написаны они на языке Objective-C или Swift, нет никаких причин, которые помешали бы вам добавлять ИИ в свои мобильные приложения, если только Android не является вашим всем. Но вы уже знаете, что мы, безусловно, поза-ботимся и об Android’е. Добавление платформы Tensor Flow в свое собственное приложение Для android Оказывается, добавлять поддержку платформы TensorFlow в свое собственное приложение для Android проще, чем в iOS. Давайте перейдем непосредственно к соответствующим шагам. 1. Если у вас уже есть существующее приложение для Android, то этот шаг можете пропустить. В противном случае в Android Studio выберите File (Файл) | New (Создать) | New Project… (Новый проект) и примите все значения по умолчанию перед нажатием кнопки Finish (Завершить).\n--- Страница 70 ---\n68  Клас сифицирование изображений с помощью трансферного обучения 2. Откройт е файл build.gradle (Module: app) и добавьте строку compile 'org. tensorflow: tensorflow ‑android:+' в конце секции зависимостей { }; . 3. Выпо лните сборку файла gradle, и внутри подпапок месторасположения app/build/intermediates/transforms/mergeJniLibs/debug/0/lib каталога ваше- го приложения вы увидите нативную библиотеку TensorFlow libtensor‑ flow_inference.so , с которой код на Java обменивается данными. 4. Если это новый проект, то вы можете создать папку ресурсов assets, сначала переключившись на Packages (Пак еты), затем щелкнув правой кнопкой мыши на app (прило жении) и выбрав New (Новый) | Folder (Папка) | Assets Folder (Папка ресурсов), как показано на следующем ниже скриншоте, и переключившись с Packages (Пакетов) на Android: Рисунок 2.13. Добавление папки ресурсов assets в новый проект 5. Перетащит е в папку assets два файла с повторно натренированными моделями и файл меток, а также несколько тестовых изображений, как показано ниже:\n--- Страница 71 ---\nКлассифицирование изображений с помощью трансферного обучения  69 Рисунок 2.14. Добавление файлов моделей, файла меток и тестовых изображений в папку ресурсов assets 6. У держивая нажатой кнопку Option, перетащите файлы TensorFlowImage‑ Classifier.java и Classifier.java из папки tensorflow/examples/android/src/ org/tensorflow/demo в папку Java вашего проекта, как показано ниже: Рисунок 2.15. Добавление в проект файлов классификатора TensorFlow 7. Откройт е файл MainActivity , сначала создайте константы, связанные с моделью, повторно натренированной на MobileNet, – размер входного изображения, имена узлов, имя файла модели и имя файла меток: private static final int INPUT_SIZE = 224; private static final int IMAGE_MEAN = 128;\n--- Страница 72 ---\n70  Клас сифицирование изображений с помощью трансферного обучения private static final float IMAGE_STD = 128; private static final String INPUT_NAME = \"input\";private static final String OUTPUT_NAME = \"final_result\"; private static final String MODEL_FILE = \"file:///android_asset/dog_retrained_ mobilenet10_224.pb\";private static final String LABEL_FILE = \"file:///android_asset/dog_retrained_labels.txt\";private static final String IMG_FILE = \"lab1.jpg\"; 8. Т еперь внутри метода onCreate сначала создайте экземпляр классифика- тора: Classifier classifier = TensorFlowImageClassifier.create( getAssets(),MODEL_FILE,LABEL_FILE,INPUT_SIZE,IMAGE_MEAN,IMAGE_STD,INPUT_NAME,OUTPUT_NAME); Затем прочитайте наше тестовое изображение из папки assets , измените его размер, как обозначено моделью, и вызовите метод выведения заключения recognizeImage : Bitmap bitmap = BitmapFactory.decodeStream(getAssets().open(IMG_FILE));Bitmap croppedBitmap = Bitmap.createScaledBitmap(bitmap, INPUT_SIZE, INPUT_SIZE, true);final List<Classifier.Recognition> results = classifier.recognizeImage(croppedBitmap); Для простоты мы не добавили программный код, связанный с пользова- тельским интерфейсом приложения для Android, но вы можете задать точку останова в строке после получения результатов и выполнить отладку приложе-ния; вы увидите результаты, как показано на следующем ниже снимке экрана: Рисунок 2.16. Результаты распознавания с использованием повторно натренированной модели MobileNet Если вы переключитесь на использование модели, повторно натрениро- ванной на Inception v3, изменив значение константы MODEL_FILE на quantized_ stripped_dogs_retrained.pb , значение константы INPUT_SIZE на 299 и значение\n--- Страница 73 ---\nКлассифицирование изображений с помощью трансферного обучения  71 константы INPUT_NAME на Mul, а затем отладите приложение, то получите резуль- таты, как показано ниже: Рисунок 2.17. Результаты распознавания с использованием модели, вторично натренированной на основе Inception v3 Теперь, когда вы знаете, как добавлять TensorFlow и вторично натренирован- ные модели в свои собственные приложения для iOS и Android, у вас не долж - но возникнуть никаких сложностей, если вы захотите добавить функционал, не связанный с TensorFlow, такой как использование фотокамеры мобильного устройства, для того чтобы сделать снимок собаки и распознать ее породу. резюме В этой главе мы сначала дали краткий обзор того, что такое трансферное обуче ние и почему мы можем и должны его использовать для вторичной тре- нировки предварительно натренированных глубоко обучающихся моделей классифицирования изображений. Затем мы представили подробную инфор-мацию о том, как вторично натренировать модель на основе моделей Inception v3 и MobileNet, чтобы мы могли более точно распознавать наших самых луч-ших друзей. После этого мы впервые показали, как использовать вторично на-тренированные модели в примерах приложений TensorFlow для iOS и Android, а затем предоставили пошаговые инструкции о том, как добавлять поддержку платформы TensorFlow в свое собственное приложение для iOS, написанное на языке Objective-C или Swift, и свое приложение для Android. Теперь, когда у нас есть наши самые лучшие друзья и мы познакомились с несколькими хорошими и чистыми трюками, мы знаем, что есть много дру - гих неожиданностей, хороших и плохих. В следующей главе мы узнаем, как быть еще умнее и как распознавать любые интересующие нас объекты на изо-бражении и локализовывать их с помощью вашего смартфона в любое время и в любом месте.",
      "debug": {
        "start_page": 41,
        "end_page": 73
      }
    },
    {
      "name": "Глава 3. Обнаружение и локализация объектов 72",
      "content": "--- Страница 74 --- (продолжение)\nГлава 3 Обнаружение и локализация объектов Обнаружение объектов уходит на один шаг дальше, чем классифицирование изображений, которое мы рассмотрели в предыдущей главе. Классифициро-вание изображений возвращает просто метку класса, соответствующую изо-бражению, в то время как обнаружение объектов возвращает список иденти-фицированных на изображении объектов вместе с ограничительной рамкой для каждого идентифицированного объекта. Современные алгоритмы обна-ружения объектов применяют глубокое обучение для построения моделей, ко-торые могут использоваться для обнаружения и локализации всевозможных объектов на одном изображении. В последние несколько лет один за другим стали появляться более быстрые и точные алгоритмы обнаружения объектов, и в июне 2017 года компания Google выпустила API Tensorflow обнаружения объектов (TensorFlow Object Detection API), который включает в себя несколько ведущих алгоритмов обнаружения объектов. В этой главе мы сначала дадим краткий обзор технологии обнаружения объ- ектов: процесса создания эффективной глубоко обучающейся модели для об-наружения объектов, а затем обсудим вопрос применения такой модели для выведения заключения. Затем мы подробно рассмотрим работу API TensorFlow обнаружения объектов и то, как использовать несколько его моделей для вы-вода и как их тренировать вторично с использованием своего собственного набора данных. После этого мы покажем, как применять предварительно на-тренированные модели обнаружения объектов, а также вторично натрени-рованные модели в своем приложении для iOS. Мы рассмотрим некоторые мощные советы, которые позволят создавать пользовательскую библиотеку TensorFlow вручную для исправления проблемы, связанной с работой моду - ля TensorFlow; это поможет вам подготовиться к работе с любыми моделями, поддерживаемыми платформой TensorFlow, которые описаны в оставшейся части данной книги. В этой главе отсутствует пример приложения обнаруже-ния объектов для Android, так как исходный код TensorFlow уже поставляется с хорошим примером применения данной технологии с использованием как\nГлава 3 Обнаружение и локализация объектов Обнаружение объектов уходит на один шаг дальше, чем классифицирование изображений, которое мы рассмотрели в предыдущей главе. Классифициро-вание изображений возвращает просто метку класса, соответствующую изо-бражению, в то время как обнаружение объектов возвращает список иденти-фицированных на изображении объектов вместе с ограничительной рамкой для каждого идентифицированного объекта. Современные алгоритмы обна-ружения объектов применяют глубокое обучение для построения моделей, ко-торые могут использоваться для обнаружения и локализации всевозможных объектов на одном изображении. В последние несколько лет один за другим стали появляться более быстрые и точные алгоритмы обнаружения объектов, и в июне 2017 года компания Google выпустила API Tensorflow обнаружения объектов (TensorFlow Object Detection API), который включает в себя несколько ведущих алгоритмов обнаружения объектов. В этой главе мы сначала дадим краткий обзор технологии обнаружения объ- ектов: процесса создания эффективной глубоко обучающейся модели для об-наружения объектов, а затем обсудим вопрос применения такой модели для выведения заключения. Затем мы подробно рассмотрим работу API TensorFlow обнаружения объектов и то, как использовать несколько его моделей для вы-вода и как их тренировать вторично с использованием своего собственного набора данных. После этого мы покажем, как применять предварительно на-тренированные модели обнаружения объектов, а также вторично натрени-рованные модели в своем приложении для iOS. Мы рассмотрим некоторые мощные советы, которые позволят создавать пользовательскую библиотеку TensorFlow вручную для исправления проблемы, связанной с работой моду - ля TensorFlow; это поможет вам подготовиться к работе с любыми моделями, поддерживаемыми платформой TensorFlow, которые описаны в оставшейся части данной книги. В этой главе отсутствует пример приложения обнаруже-ния объектов для Android, так как исходный код TensorFlow уже поставляется с хорошим примером применения данной технологии с использованием как\n--- Страница 75 ---\nОбнаружение и локализация объектов  73 предварительно натренированной модели TensorFlow обнаружения объектов, так и модели YOLO, которую мы рассмотрим в этой главе. Мы также покажем, как в своем приложении для iOS использовать еще одну ведущую модель об-наружения объектов, YOLO v2. Резюмируя, в этой главе мы рассмотрим следу - ющие темы: обнар ужение объектов: краткий обзор; настройка API TensorFlow обнаружения объектов; вторичная тренировка моделей SSD-MobileNet и моделей на основе бо- лее быстрого RCNN-детектора; испо льзование моделей обнаружения объектов в iOS; испо льзование YOLO2, еще одной модели обнаружения объектов. обнаружение объектов – краткий обзор С момента инновационного прорыва в технологии нейронных сетей в 2012 году, когда глубокая модель на основе CNN-сети под названием AlexNet выиграла ежегодный конкурс по распознаванию изображений ImageNet, рез-ко сократив частоту ошибок, многие исследователи в области компьютерного зрения и обработки естественного языка начали использовать преимущества глубоко обучающихся моделей. Современные реализации обнаружения объ-ектов на основе глубокого обучения основаны на сверточных нейронных сетях (CNN-сетях) и строятся поверх предварительно натренированных моделей, таких как AlexNet, Google Inception, либо поверх еще одной популярной сети VGG. У этих CNN-сетей обычно натренированы миллионы параметров, и они могут конвертировать входное изображение в набор признаков, которые могут далее использоваться для таких задач, как классифицирование изображений, рассмотренное нами в предыдущей главе, и обнаружение объектов, среди про-чих задач, связанных с компьютерным зрением. В 2014 году был предложен новейший детектор объектов под названи- ем RCNN (Regions with CNN features, участки с CNN-признаками), в котором сеть AlexNet была вторично натренирована набором помеченных данных для обнаружения объектов, и он показал большое улучшение точности, по срав-нению с традиционными методами обнаружения. RCNN-детектор сочетает в себе метод «предложений участков» (region proposals), который генерирует около 2000 возможных участков-кандидатов, и выполняет CNN-сеть в каждом из этих участков для классифицирования и предсказания ограничительной рамки. Затем он объединяет эти результаты для создания результата обнару - жения. Процесс тренировки RCNN-детектора довольно сложен и занимает не-сколько дней. Скорость выведения заключения также довольно медленная, за-нимающая на GPU почти минуту для одного изображения. С тех пор как был предложен RCNN-детектор, один за другим стали выхо- дить более эффективные алгоритмы обнаружения объектов: быстрый RCNN-детектор (Fast RCNN), более быстрый RCNN-детектор (Faster RCNN), детектор\n--- Страница 76 ---\n74  Обнар ужение и локализация объектов YOLO (You Only Look Once, вы смотрите только раз), SSD-детектор (Single Shot Multibox Detector, однократный многорамочный детектор) и детектор YOLO v2.  В 2014 году Андрей Карпати (Andrej Karpathy) написал хорошее введение в RCNN-детектор «Playing around with RCNN, State of the Art Object Detector» («Игрушки с RCNN – совре- менный детектор объектов») (https: //cs.stanford.edu/people/karpathy/rcnn). В рамках Стэн- фордского курса CS231n Джастина Джонсона (Justin Johnson) есть хорошая видеолекция «Spatial Localization and Detection» («Пространственная локализация и обнаружение»), посвященная обнаружению объектов с подробной информацией о RCNN-детекторе, быст - ром RCNN-детекторе, более быстром RCNN-детекторе и YOLO-детекторе. SSD-детектор подробно описывается на веб-странице https: //github.com/weiliu89/caffe/tree/ssd. Детек - тору YOLO2 посвящен замечательный веб-сайт https: //pjreddie.com/darknet/yolo . Быстрый RCNN-детектор (Faster RCNN) значительно улучшает процесс тре- нировки и сроки выведения заключения (10 часов на тренировку и 2.x секунд на заключение). Это достигается путем применения CNN-сети ко всему вход-ному изображению, а не к тысячам предложенных участков, и потом занятия предложениями участков. Более быстрый RCNN-детектор повышает скорость выведения заключения, приближаясь к реальному времени (0.2 сек), благодаря использованию сети предложений участков, делая уже более ненужным трудо-емкий процесс обработки предложений участков после тренировки. В отличие от семейства алгоритмов обнаружения на основе RCNN-детектора, детекторы SSD и YOLO – это однократные методы, то есть они применяют одну CNN-сеть к полному входному изображению без использования предложений участков и классифицирования участков. Это делает оба метода очень быстры-ми, а их средняя прецизионность (Mean Average Precision, mAP) составляет око-ло 80%, опережая более быстрый RCNN-детектор 1. Если вы впервые слышите об этих методах, то вы, вероятно, почувствуете себя немного растерянным. Но как разработчику, заинтересованному во внед - рении технологии ИИ в мобильные приложения, вам не нужно разбираться во всех тонкостях настройки глубокой нейросетевой архитектуры и трени-ровки моделей обнаружения объектов; вы просто должны знать, как все это использовать, и, если необходимо, выполнять вторичную тренировку предва-рительно натренированных моделей, а также знать, как использовать предва-рительно натренированные и вторично натренированные модели в приложе-ниях для iOS и Android.  Ес ли вы действительно заинтересованы в глубоком обучении и хотите знать все по - дробнос ти того, как работает каждый детектор, чтобы решить, какой из них использо- вать, то вы обязательно должны прочитать статьи по каждому методу и попытаться вос - 1 mAP (Mean Average Precision) – это метрический показатель для измерения точности детекторов объектов, таких как более быстрый RCNN-детектор, SSD-детектор и т. д., является средним значением максимальной прецизионности при различных значе- ниях полноты. См. https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection -45c121a31173 – Прим. перев.\n--- Страница 77 ---\nОбнаружение и локализация объектов  75 произвести процесс тренировки самостоятельно. Это будет долгий, но полезный путь. Но если же вы хотите последовать совету Андрея Карпати: «не будь героем» (отыщите видеоролик в YouTube по запросу «deep learning for computer vision Andrej» (глубокое обучение для компьютерного зрения Андрей), то вы можете «взять лучшее из того, что работает, скачать предварительно натренированную модель, возможно, добавить/уда-лить некоторые части и настроить ее для вашего приложения». Именно этот подход мы здесь и будем использовать. Прежде чем мы начнем рассматривать то, что лучше всего работает с TensorFlow, давайте кратко рассмотрим наборы данных. Для тренировки об-наружения объектов используется три основных набора данных: PASCAL VOC (http://host.robots.ox.ac.uk/pascal/VOC), ImageNet (http://image-net.org) и Microsoft COCO ( http://c ocodataset.org); количество классов у них соответственно – 20, 200 и 80. Большинство предварительно натренированных моделей, которые в на-стоящее время поддерживаются API TensorFlow обнаружения объектов, тре-нируются на 80-классовом наборе данных МС COCO (для получения полного списка предварительно натренированных моделей и наборов данных, на кото-рых они натренированы, обратитесь к https: //github.com/tensorflow/models/blob/ master/research/object_detection/g3doc/detection_model_zoo.md). Хотя мы не будем выполнять тренировку с нуля, вы увидите частые упо- минания форматов данных PASCAL VOC и MS COCO, а также 20 или 80 общих классов, которые они охватывают, при вторичной тренировке или использова-нии натренированных моделей. В последнем разделе этой главы мы протести-руем натренированную на данных VOC и COCO модель YOLO. настройка aPi T ensor Flow обнаружения объектов API TensorFlow обнаружения объектов (TensorFlow Object Detection API) подроб-но описан на его официальном веб-сайте https: //github.com/tensorflow/models/ tree/master/research/object_detection, и вы должны обязательно обратиться к его руководству «Quick Start: Jupyter notebook for off-the-shelf inference» («Краткое руководство: блокнот Jupyter для готовых заключений»), которое дает неко-торое представление о том, как применять на языке Python хорошую пред-варительно натренированную модель обнаружения объектов. Правда, стоит отметить, что документация там разбросана по многим страницам, и это иног - да затрудняет ориентацию в ней. В этом и следующих разделах мы упростим официальную документацию, реорганизовав важные детали, задокументиро-ванные в различных местах, и добавив дополнительные примеры и пояснения в программный код. Мы также предложим два пошаговых руководства: 1) как настроить API и использовать его предварительно натренированные модели для выведения стандартного заключения; 2) как выполнять вторичную тренировку предварительно натренирован- ных моделей с помощью данного API для более конкретных задач обна-ружения.\n--- Страница 78 ---\n76  Обнар ужение и локализация объектов быстрая установка и пример Для того чтобы установить и реализовать обнаружение объектов, выполните следующие действия. 1. В корневом каталоге исходного кода TensorFlow, созданном в главе 1 «Начало работы с платформой TensorFlow Mobile», следует клонировать репозиторий моделей TensorFlow, который содержит API TensorFlow об-наружения объектов в качестве одной из своих исследовательских мо-делей: git clone https://github.com/tensorflow/models 2. У становите библиотеки Python matpiotlib , pillow , lxml и jupyter . В Ubuntu или Mac вы можете выполнить: sudo pip install pillowsudo pip install lxmlsudo pip install jupytersudo pip install matplotlib 3. Перейдит е в каталог models/research и выполните следующую ниже ко- манду: protoc object_detection/protos/*.proto --python_out=. В результате в каталоге object_detection/protos будут скомпилированы все файлы механизма сериализации Protobuf, тем самым будет подготовлен API TensorFlow обнаружения объектов к работе. Файл Protobuf, или протоколь-ный буфер (Protocol Buffer), представляет собой автоматизированный способ сериа лизации и извлечения структурированных данных. Он легче и эффек - тивнее, чем XML. От вас требуется только написать файл с расширением.proto, в котором описывается структура ваших данных, а затем применить компиля-тор protoc для создания кода, автоматически анализирующего и кодирующего данные файла protobuf. Обратите внимание, что параметр ‑‑python_out задает язык создаваемого кода. В следующем разделе этой главы, когда мы обсудим вопрос использования модели в iOS, мы будем применять компилятор protoc с параметром ‑‑cpp_out , чтобы код генерировался в C++. Для получения пол- ной документации по протокольным буферам обратитесь на https://developers. google.com/protocol-buffers. 4. По-пре жнему находясь внутри папки models/research , выполните коман- ду export PYTHONPATH=$PYTHONPATH:'pwd':'pwd'/slim , а затем команду python object_detection/builders/model_builder_test.py для верификации, что все работает. 5. Запу стите команду jupyter notebook и направьте браузер по URL-адресу http://localhost:8888 . Сначала щелкните на папке object_detection , затем выберите блокнот object_detection_tutorial.ipynb и выполните демон- страционный пример последовательно, ячейка за ячейкой.\n--- Страница 79 ---\nОбнаружение и локализация объектов  77 использование преДварительно натренированных моДелей Давайте теперь рассмотрим приведенные в блокноте на языке Python основ- ные компоненты применения предварительно натренированных моделей TensorFlow обнаружения объектов для выведения заключения. Вначале зада-ются некоторые ключевые константы: MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'MODEL_FILE = MODEL_NAME + '.tar.gz'DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')NUM_CLASSES = 90 Программный код блокнота скачивает и использует предварительно на- тренированную модель обнаружения объектов ssd_mobilenet_v1_coco_2017_11_17 (построенную с помощью SSD-детектора, которого мы кратко коснулись в пре-дыдущем разделе, поверх модели MobileNet CNN, которую мы рассмотрели в предыдущей главе). Полный список предварительно натренированных моде-лей с поддержкой API TensorFlow обнаружения объектов содержится на стра-нице коллекции моделей TensorFlow (так называемого «зоопарка» моделей): https: //github.com/tensorflow/models/blob/master/research/object_detection/g3doc/ detection_model_zoo.md, и большинство из них натренировано на наборе дан-ных MS COCO. Точной моделью, используемой для выведения заключения, является файл frozen_inference_graph.pb (в скачанном архиве ssd_mobilenet_v1_ coco_2017_11_17.tar.gz ), модель используется для стандартного выведения за- ключения, а также для вторичной тренировки. Файл mscoco_label_map.pbtxt , расположенный в папке models/research/object_ detection/data/mscoco_label_map.pbtxt , имеет 90 (NUM_CLASSES) элементов для типов объектов, которые модель ssd_mobiienet_vi_coco_20i7_ii_i7 может обна- руживать. Его первые и последние два элемента следующие: item { name: \"/m/01g317\"id: 1display_name: \"person\" }item { name: \"/m/0199g\"id: 2display_name: \"bicycle\" } item { name: \"/m/03wvsk\"\n--- Страница 80 ---\n78  Обнар ужение и локализация объектов id: 89 display_name: \"hair drier\" }item { name: \"/m/012xff\"id: 90display_name: \"toothbrush\" } Ранее в шаге 3 мы упомянули файлы сериализации Protobuf. Файл proto, string_int_label_map.proto , который описывает данные в файле mscoco_label_map. pbtxt , расположен в папке models/research/object_detection/protos и имеет следу - ющее ниже содержимое: syntax = \"proto2\";package object_detection.protos;message StringIntLabelMapItem { optional string name = 1;optional int32 id = 2;optional string display_name = 3; }; message StringIntLabelMap { repeated StringIntLabelMapItem item = 1; }; По сути дела, компилятор protoc создает код на основе файла string_int_ label_map.proto , и этот код можно использовать для эффективной сериализа- ции данных файла mscoco_label_map.pbtxt . Позже, когда CNN-сеть обнаруживает объект и возвращает целочисленный идентификатор, он может быть конвер- тирован в имя name или отображаемое имя display_name для чтения человеком. После того как модель будет скачана, распакована и загружена в память, также загружается файл со словарем меток, и несколько тестовых изображе-ний, расположенных в папке готовых к использованию тестовых изображений models/research/object_detection/test_images , куда вы можете добавить любое выбранное вами тестовое изображение для тестирования обнаружения объек - тов. Далее определяются соответствующие входной и выходной тензоры: with detection_graph.as_default(): with tf.Session(graph=detection_graph) as sess: image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')num_detections = detection_graph.get_tensor_by_name('num_detections:0') Опять же, если вам интересно, откуда берутся имена входного и выходно- го тензоров в SSD-модели, скачанной и сохраненной в models/research/object_\n--- Страница 81 ---\nОбнаружение и локализация объектов  79 detection/ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb , вы можете воспользоваться следующим ниже программным кодом в IPython: import tensorflow as tf g=tf.GraphDef()g.ParseFromString(open(\"object_detection/ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb\",\"rb\").read())x=[n.name for n in g.node]x[ ‑‑4:] x[:5]The last two statements will return:[u'detection_boxes', u'detection_scores',u'detection_classes',u'num_detections'] and[u'Const', u'Const_1', u'Const_2', u'image_tensor', u'ToFloat'] Еще один способ – испо льзовать инструмент резюмирования графа summarize_ graph , описанный в предыдущей главе: bazel‑bin/tensorflow/tools/graph_transforms/summarize_graph ‑‑in_graph= models/research/ object_detection/ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb В результате на выходе будет сгенерировано следующее: Found 1 possible inputs: (name=image_tensor, type=uint8(4), shape=[?,?,?,3])No variables spotted.Found 4 possible outputs: (name=detection_boxes, op=Identity) (name=detection_scores, op=Identity (name=detection_classes, op=Identity) (name=num_detections, op=Identity) После загрузки каждого тестового изображения выполняется фактическое обнаружение: image = Image.open(image_path)image_np = load_image_into_numpy_array(image)image_np_expanded = np.expand_dims(image_np, axis=0)(boxes, scores, classes, num) = sess.run( [detection_boxes, detection_scores, detection_classes, num_detections],feed_dict={image_tensor: image_np_expanded}) Наконец, обнаруженные результаты визуализируются с помощью библиоте- ки matplotlib . Если вы используете два заданных по умолчанию тестовых изо- бражения, поставляемых вместе с репозиторием tensorflow/models , то увидите результаты, как на рис. 3.1:\n--- Страница 82 ---\n80  Обнар ужение и локализация объектов Рисунок 3.1. Обнаруженные объекты с ограничительными рамками и показателями достоверности В разделе «Использование моделей обнаружения объектов в iOS» мы увидим, как использовать ту же самую модель и извлекать одинаковый обнаруженный результат на устройстве iOS. Вы также можете протестировать другие предварительно натренированные модели из упомянутого ранее «зоопарка» моделей обнаружения TensorFlow. Например, если вы используете модель faster_rcnn_inception_v2_coco , за- менив в блокноте object_detection_tutorial.ipynb имя модели MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17' на MODEL_NAME = 'faster_rcnn_inception_v2_\n--- Страница 83 ---\nОбнаружение и локализация объектов  81 coco_2017_11_08' , которую вы получите из URL-адреса на странице «зоопарка» моделей TensorFlow, или на MODEL_NAME = ' faster_rcnn_resnet101_coco_2017_11_08' , вы можете увидеть аналогичные результаты обнаружения с двумя другими моделями на основе более быстрого RCNN-детектора, но они требуют больше времени. Кроме того, применение инструмента резюмирования графа summarize_graph к двум моделям faster_rcnn генерирует одинаковую информацию о входе и вы- ходе: Found 1 possible inputs: (name=image_tensor, type=uint8(4), shape=[?,?,?,3])Found 4 possible outputs: (name=detection_boxes, op=Identity) (name=detection_scores, op=Identity) (name=detection_classes, op=Identity) (name=num_detections, op=Identity) Как правило, модели на основе MobileNet являются самыми быстрыми, но менее точными (с меньшим значением показателя mAP), чем другие боль-шие модели на основе Inception- или Resnet-CNN. Кстати, файлы скачанных мо-делей ssd_mobilenet_v1_coco , faster_rcnn_inception_v2_coco_2017_11_08 и faster_rcnn_ resneti0i_coco_20i7_ii_08 имеют размеры соответственно 76 Мб, 149 Мб и 593 Мб. Как мы увидим позже, модели на основе MobileNet, такие как ssd_ mobiienet_vi_ coco, работают на мобильных устройствах намного быстрее, и иногда большая модель, такая как faster_rcnn_resneti0i_coco_20i7_ii_08 , попросту «вылетает» на старом iPhone. Будем надеяться, что проблемы, которые у вас есть, могут быть решены с помощью модели на основе MobileNet, либо вторично натре-нированной модели MobileNet, либо будущей версии модели ssd_mobilenet , ко- торая, безусловно, предложит еще более высокую точность, хотя модель ssd_mo‑ biienet версии v1 уже достаточно хороша для многих случаев использования. вторичная тренировка моДелей на основе ssd-M obileneT и более быстрого rcnn- Детектора В некоторых задачах предварительно натренированные модели TensorFlow обнаружения объектов, безусловно, работают хорошо. Но иногда может потре-боваться использовать собственный аннотированный набор данных (с ограни-чительными рамками вокруг объектов или частей объектов, представляющих особый для вас интерес) и вторично натренировать существующую модель, чтобы она могла более точно определять другой набор классов объектов. Мы будем использовать тот же самый набор данных о домашних питомцах Oxford-IIIT Pets, чье описание приведено на сайте API Tensorflow обнаружения объектов, для вторичной тренировки двух существующих моделей на локаль-ном компьютере вместо использования службы Google Cloud, рассматривае-мой в документации. Мы также добавим объяснение по каждому шагу, когда это необходимо. Ниже приводится пошаговое руководство по вторичной тре-нировке модели TensorFlow обнаружения объектов с использованием набора данных Oxford-IIIT Pets.\n--- Страница 84 ---\n82  Обнар ужение и локализация объектов 1. В окне терминала, предпочтительно в нашей Ubuntu с поддержкой гра- фического процессора (GPU), который ускоряет процесс вторичной тре- нировки, выполните команду cd models/research firs , а затем выполни- те следующие ниже команды, чтобы скачать набор данных (архивные файлы images.tar.gz и annotations.tar.gz имеют размеры соответственно 800 Мб и 38 Мб): wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gzwget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gztar -xvf images.tar.gztar -xvf annotations.tar.gz 2. Выпо лните следующую ниже команду, чтобы конвертировать набор дан- ных в формат TFRecords: python object_detection/dataset_tools/create_pet_tf_record.py \\ --label_map_path=object_detection/data/pet_label_map.pbtxt \\--data_dir='pwd' \\--output_dir='pwd' Эта команда создаст два файла TFRecord с именами pet_train_with_masks. record (268MB) и pet_val_with_masks.record (110МБ) в каталоге models/ research . Формат TFRecord – это интересный двоичный формат, включаю- щий в себя все данные, которые приложение TensorFlow может использо-вать для тренировки или контроля, он является обязательным форматом файла, если вы хотите вторично натренировать свой собственный набор данных с помощью API TensorFlow обнаружения объектов. 3. Скачайт е и распакуйте модель ssd_mobilenet_v1_coco и модель faster_rcnn_ resnet101_coco в каталог models/research , если вы этого не сделали в преды- дущем разделе во время тестирования работы блокнота на языке Python с программным кодом обнаружения объектов: wget http://storage.googleapis.com/download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gztar -xvf ssd_mobilenet_v1_coco_2017_11_17.tar.gzwget http://storage.googleapis.com/download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_11_06_2017.tar.gztar -xvf faster_rcnn_resnet101_coco_11_06_2017.tar.gz 4. Заменит е пять вхождений слов PATH_TO_BE_CONFIGURED (путь, подлежащий настройке) в файле object_detection/samples/configs/faster_rcnn_resnet101_ pets.config , чтобы они приняли следующий ниже вид: fine_tune_checkpoint: \"faster_rcnn_resnet101_coco_11_06_2017/model.ckpt\" train_input_reader: {tf_record_input_reader {input_path: \"pet_train_with_masks.record\"}\n--- Страница 85 ---\nОбнаружение и локализация объектов  83 label_map_path: \"object_detection/data/pet_label_map.pbtxt\" }eval_input_reader: {tf_record_input_reader {input_path: \"pet_val_with_masks.record\"}label_map_path: \"object_detection/data/pet_label_map.pbtxt\" } Конфигурационный файл faster_rcnn_resnet101_pets.config используется для указания расположений файлов контрольных точек модели и содер-жит натренированные веса модели, файлы TFRecords для тренировки и контроля, которые генерируются на шаге 2, и помеченные элементы 37 классов домашних животных, подлежащих обнаружению. Первый и последний элементы object_detection/data/pet_label_map.pbtxt следую- щие: item {id: 1name: 'Abyssinian'} item {id: 37name: 'yorkshire_terrier'} 5. Аналогичным образом измените пять вхождений слов PATH_TO_BE_ CONFIGURED в конфигурационном файле object_detection/samples/configs/ ssd_mobilenet_v1_pets.config , чтобы они приняли следующий вид: fine_tune_checkpoint: \"object_detection/ssd_mobilenet_v1_coco_2017_11_17/model.ckpt\"train_input_reader: {tf_record_input_reader {input_path: \"pet_train_with_masks.record\"}label_map_path: \"object_detection/data/pet_label_map.pbtxt\"}eval_input_reader: {tf_record_input_reader {input_path: \"pet_val_with_masks.record\"}label_map_path: \"object_detection/data/pet_label_map.pbtxt\" } 6. С оздайте новый каталог train_dir_faster_rcnn , затем выполните команду вторичной тренировки:\n--- Страница 86 ---\n84  Обнар ужение и локализация объектов python object_detection/train.py \\ ‑‑logtostderr \\ ‑‑pipeline_config_path=object_detection/samples/configs/faster_rcnn_resnet101_pets.config \\ ‑‑train_dir=train_dir_faster_rcnn В операционной системе с поддержкой GPU требуется менее 25 000 тре- нировочных шагов, чтобы достичь значения потери 0,2 или около того от первоначальной потери 5,0: tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7845pciBusID: 0000:01:00.0totalMemory: 7.92GiB freeMemory: 7.44GiBINFO: tensorflow: global step 1: loss = 5.1661 (15.482 sec/step)INFO: tensorflow: global step 2: loss = 4.6045 (0.927 sec/step)INFO: tensorflow: global step 3: loss = 5.2665 (0.958 sec/step) INFO: tensorflow: global step 25448: loss = 0.2042 (0.372 sec/step)INFO: tensorflow: global step 25449: loss = 0.4230 (0.378 sec/step)INFO: tensorflow: global step 25450: loss = 0.1240 (0.386 sec/step) 7. Нажмит е CTRL+C для завершения работы сценария вторичной трени- ровки примерно после 20 000 шагов (около 2 часов). Создайте новый ка- талог train_dir_ssd_mobilenet и выполните команду: python object_detection/train.py \\ ‑‑logtostderr \\ ‑‑pipeline_config_path=object_detection/samples/configs/ssd_mobilenet_v1_pets.config \\ ‑‑train_dir=train_dir_ssd_mobilenet Результаты тренировки должны выглядеть следующим образом: INFO: tensorflow: global step 1: loss = 136.2856 (23.130 sec/step)INFO: tensorflow: global step 2: loss = 126.9009 (0.633 sec/step)INFO: tensorflow: global step 3: loss = 119.0644 (0.741 sec/step) INFO: tensorflow: global step 22310: loss = 1.5473 (0.460 sec/step)INFO: tensorflow: global step 22311: loss = 2.0510 (0.456 sec/step)INFO: tensorflow: global step 22312: loss = 1.6745 (0.461 sec/step) Вы можете увидеть, что повторная тренировка модели SSD_Mobilenet на- чинается и заканчивается более крупным значением потери, чем у мо-дели Faster_RCNN . 8. Завершит е предыдущий сценарий вторичной тренировки примерно че- рез 20 000 тренировочных шагов. Затем создайте новый каталог eval_dir и выполните оценивающий сценарий: python object_detection/eval.py \\ ‑‑logtostderr \\ ‑‑pipeline_config_path=object_detection/samples/configs/faster_rcnn_resnet101_pets.config \\\n--- Страница 87 ---\nОбнаружение и локализация объектов  85 ‑‑checkpoint_dir=train_dir_faster_rcnn \\ ‑‑eval_dir=eval_dir 9. Откройт е еще одно окно терминала. Командой cd перейдите в корневой каталог TensorFlow и потом в папку models/research , а затем выполните команду tensorboard ‑‑logdir=. . В браузере откройте URL-адрес http://lo‑ calhost: 6006 , и вы увидите график потери, как показано на рис. 3.2: Рисунок 3.2. Общий тренд потери во время вторичной тренировки модели обнаружения объектов Вы также увидите несколько результатов оценки, как показано на рис. 3.3. Рисунок 3.3. Оценивание результатов обнаружения изображений во время вторичной тренировки модели обнаружения объектов 10. Аналогичным образом вы можете выполнить сценарий оценивания мо- дели ssD_MobileNet , а затем использовать сервер TensorBoard для просмот - ра общег о тренда потери и результатов оценивания изображений: python object_detection/eval.py \\\n--- Страница 88 ---\n86  Обнар ужение и локализация объектов ‑‑logtostderr \\ ‑‑pipeline_config_path=object_detection/samples/configs/ssd_mobilenet_v1_pets.config \\ ‑‑checkpoint_dir=train_dir_ssd_mobilenet \\ ‑‑eval_dir=eval_dir_mobilenet 11. Вы можете сгенерировать вторично натренированные графы с по мощью следующих ниже команд: python object_detection/export_inference_graph.py \\ ‑‑input_type image_tensor \\ ‑‑pipeline_config_path object_detection/samples/configs/ssd_mobilenet_v1_pets.config \\ ‑‑trained_checkpoint_prefix train_dir_ssd_mobilenet/model.ckpt‑21817 \\ ‑‑output_directory output_inference_graph_ssd_mobilenet.pb python object_detection/export_inference_graph.py \\ ‑‑input_type image_tensor \\ ‑‑pipeline_config_path object_detection/samples/configs/faster_rcnn_resnet101_pets.config \\ ‑‑trained_checkpoint_prefix train_dir_faster_rcnn/model.ckpt‑24009 \\ ‑‑output_directory output_inference_graph_faster_rcnn.pb Вам нужно заменить значение параметра ‑‑trained_checkpoint_prefix (21 817 и 24 009, приведенные выше) на собственные значения контрольных точек. Таким образом, теперь у вас есть две вторично натренированные модели об- наружения объектов, output_inference_graph_ssd_mobilenet.pb и output_ inference_ graph_faster_rcnn.pb , готовые к использованию в программном коде Python (блокнота Jupyter из предыдущего раздела) либо в мобильных приложениях. Без каких-либо дальнейших задержек давайте перейдем к мобильному миру и посмотрим, как применить предварительно натренированные и вторично натренированные модели, которые у нас есть. использование моДелей обнаружения объектов в ios В предыдущей главе мы показали вам, как использовать экспериментальный модуль TensorFlow для быстрого добавления поддержки платформы TensorFlow в приложение для iOS. Экспериментальный модуль TensorFlow работает от - лично для таких моделей, как Inception и MobileNet, либо моделей, вторично натренированных на их основе. Но если вы используете экспериментальный модуль TensorFlow, по крайней мере на момент написания этой книги (январь 2018 г.) с моделью SSD_MobileNet , то при загрузке файла графа ssd_mobilenet вы, вероятно, получите приведенное ниже сообщение об ошибке, которое говорит о незарегистрированном типе операции: Could not create TensorFlow Graph: Not found: Op type not registered 'NonMaxSuppressionV2' Если экспериментальный модуль TensorFlow не будет обновлен, чтобы включить «незарегистрированные определения операций», то единствен-ный способ исправить эти проблемы – создать пользовательскую библиоте-\n--- Страница 89 ---\nОбнаружение и локализация объектов  87 ку TensorFlow для iOS путем ее ручной сборки из исходного кода TensorFlow, и именно по этой причине мы показали в главе 1 «Начало работы с платфор- мой TensorFlow Mobile», как получать исходный код платформы TensorFlow и устанавливать платформу из исходного кода. Давайте посмотрим на шаги сборки своей собственной библиотеки TensorFlow для iOS и ее применения для создания нового приложения TensorFlow для iOS. ручная сборка библиотек Tensor Flow Для ios Для ручной сборки собственных библиотек TensorFlow для iOS следует выпол-нить следующие ниже действия. 1. Откройт е новый терминал в своем Mac, командой cd перейдите в корне- вой каталог с исходным кодом, то есть в ~/tensorflow‑1.4.0 , в случае если вы распаковали исходный zip-архив TensorFlow 1.4 в свой домашний ка-талог. 2. Выпо лните сценарий оболочки tensorflow/contrib/makefile/build_all_ios. sh, который займет от 20 минут до почти часа, в зависимости от скорости вашего Mac. После успешного завершения процесса сборки будут созда-ны три библиотеки: tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf ‑lite.a tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.atensorflow/contrib/makefile/gen/lib/libtensorflow ‑core.a Первые две библиотеки занимаются данными сериализации protobuf, о ко- торых мы говорили ранее. Последняя из них является универсальной статиче-ской библиотекой iOS.  Ес ли после следующих ниже шагов вы выполните приложение и в консоли Xcode обна- ружите ошибку «Invalid argument: No OpKernel was registered to support Op ‘Less’ with these attrs. Registered devices: [CPU], Registered kernels: device=’CPU’; T in [DT_FLOAT]», то перед выполнением шага 2 вам нужно изменить файл tensorflow/contrib/makefile/ Makefile (обратитесь к разделу «Сборка пользовательской библиотеки TensorFlow для iOS» в главе 7 «Распознавание рисунков с помощью CNN- и LSTM-сетей» за более подроб- ной информацией). Данная ошибка может не отобразиться при использовании более новых версий платформы TensorFlow. использование библиотек Tensor Flow Для ios в приложении Для того чтобы задействовать библиотеки в своем собственном приложении, выполните следующие ниже действия. 1. В среде Xcode выберите File (Файл) | New (Создать) | Project… (Проек т), выберите шаблон проекта Single View App (Прос тое приложение с од- ним экраном), затем введите TFObjectDetectionAPI в качестве имени про-\n--- Страница 90 ---\n88  Обнар ужение и локализация объектов дукта и выберите Objective-C в качестве языка программирования (если вы хотите использовать язык Swift, то обратитесь к предыдущей главе относительно того, как добавлять поддержку TensorFlow в приложение для iOS на языке Swift, и внесите необходимые показанные здесь изме-нения), затем выберите расположение проекта и нажмите Create (Соз- дать). 2. В проекте TFObjectDetectionAPI щелкните на PROJECT [имя проекта] и в разделе Build Settings (Параметры сборки) нажмите + и Add User- Defined Setting (Добавить пользовательские настройки), затем в поле TENSORFLOW_ROOT введите путь к корневому каталогу с исходным кодом TensorFlow (например, $HOME/tensorflow‑1.4 ), как показано на рис. 3.4. Этот пользовательский параметр будет применяться в других параметрах, чтобы позже упростить вам изменение настроек проекта, если вы захо-тите обратиться к более новому исходному коду платформы TensorFlow. Рисунок 3.4 Добавление пути TENSORFLOW_ROOT в пользовательских настройках 3. Ще лкните на Targets (Цели) и в разделе Build Settings (Параметры сбор- ки) найдите поле Other Linker Flags (Другие флаги компоновщика). До- бавьте туда следующее значение: -force_load $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf-lite.a $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/downloads/nsync/builds/lipo.ios.c++11/nsync.a Первый параметр ‑force_load необходим, потому что он гарантирует, что будут скомпонованы конструкторы C++, которые требуются для плат - формы TensorFlow, в противном случае после сборки и запуска приложе-ния у вас по-прежнему возникнет ошибка, связанная с незарегистриро-ванными сеансами.Последняя библиотека предназначена для nsync, библиотеки C, которая экспортирует mutex и другие методы синхронизации (https: //github.com/ google/nsync). Она представлена в новых версиях платформы TensorFlow. 4. Найдит е поле Header Search Paths (Маршр уты доступа к заголовкам) и добавьте туда следующее ниже значение:\n--- Страница 91 ---\nОбнаружение и локализация объектов  89 $(TENSORFLOW_ROOT) $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/downloads/protobuf/src $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/downloads $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/downloads/eigen $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/gen/proto После этого вы увидите что-то вроде того, что показано на рис. 3.5. Рисунок 3.5. Добавление всех параметров сборки, связанных с платформой TensorFlow, в раздел цели Target 5. В разделе Build Phases (Фазы сборки) целевого объекта добавьте Accelerate framework (Ускорить платформу) в поле Link Binary with Libraries (Скомпоновать двоичный файл с библиотеками), как на рис. 3.6. Рисунок 3.6. Добавление ускорения платформы 6. Вернит есь в терминал, который вы использовали для создания библио- тек TensorFlow для iOS, найдите следующие ниже две строки программ-ного кода в файле tensorflow/core/platform/default/mutex.h : #include \"nsync_cv.h\"#include \"nsync_mu.h\" Затем измените их на следующие: #include \"nsync/public/nsync_cv.h\"#include \"nsync/public/nsync_mu.h\" Вот и все, что нужно, чтобы добавить ваши вручную собранные библиотеки TensorFlow в приложение для iOS.\n--- Страница 92 ---\n90  Обнар ужение и локализация объектов  Загр узка моделей TensorFlow обнаружения объектов в свое собственное приложе- ние с библиотеками TensorFlow ручной сборки из более поздней версии TensorFlow, такой как 1.4, не будет выдавать ошибку, которую можно увидеть при использовании экспериментального модуля TensorFlow или библиотек ручной сборки из более ран-ней версии. Причиной тому является файл с именем tf_op_files.txt , который располо- жен в папке tensorflow/contrib/makefile и используется для установления того, какие определения операций должны быть собраны и включены в библиотеку TensorFlow. В версии TensorFlow 1.4 в этом файле задано больше операций, чем в предыдущих версиях. Например, файл tf_op_files.txt в TensorFlow 1.4 имеет строку, tensorflow/ core/kernels/non_max_ suppression_op.cc , которая задает операцию NonMaxSuppressionV2 , и поэтому в собранной нами вручную библиотеке эта операция определена, предот - вращая ошибку «Could not create TensorFlow Graph: Not found: Op type not registered ‘NonMaxSuppressionV2», которую мы увидим при использовании экспериментального модуля TensorFlow. Если в будущем вы столкнетесь с ошибкой Op type not registered (Тип операции не зарегистрирован) или подобной, то вы можете ее исправить, добавив в файл tf_op_files.txt правильный файл исходного кода, который задает операцию, а затем снова запустив сценарий оболочки build_all_ios.sh , чтобы создать новый файл libtensorflow ‑core.a . Добавление функционала обнаружения объектов в приложение Для ios Теперь выполните следующие ниже действия, чтобы добавить файлы модели, файл меток и программный код в приложение, и затем запустите его, чтобы увидеть обнаружение объектов в действии. 1. Перетащит е в проект TFObjectDetectionAPI три графа модели обнаружения объектов, ssd_mobilenet_v1_frozen_inference_graph.pb , faster_rcnn_ inceptionv2_ frozen_inference_graph.pb и faster_rcnn_resnet101_frozen_ inference_graph.pb , которые мы скачали (и переименовали здесь для уточнения) в предыду - щем разделе, а также файл со словарем меток mscoco_label_map.pbtxt и пару тестовых снимков. 2. Добавьте в проект файлы ios_image_load.mm и его файлы .h из примера приложения TensorFlow simple для iOS или приложения для iOS, создан-ного в последней главе. 3. Скачайт е протокольные буферы версии 3.4.0 с веб-страницы https:// github.com/google/protobuf/releases (в Mac это файл protoc‑3.4.0 ‑osx‑ x86_64.zip ). Для работы с библиотекой TensorFlow 1.4 требуется именно версия 3.4.0. Более поздние версии TensorFlow могут потребовать более поздней версии протокольных буферов. 4. Исходя из того, что скачанный файл распакован вами в каталог ~/ Downloads , откройте окно терминала и выполните следующие ниже ко- манды: cd <TENSORFLOW_ROOT>/models/research/object_detection/protos\n--- Страница 93 ---\nОбнаружение и локализация объектов  91 ~/Downloads/protoc -3.4.0-osx-x86_64/bin/protoc string_int_label_map.proto --cpp_out=<path_ to_your_TFObjectDetectionAPI_project>, the same location as your code files and the three graph files. 5. Пос ле завершения команды компилятора protoc вы увидите, что в ис - ходном каталоге проекта сгенерировано два файла: string_int_label_map. pb.cc и string_int_label_map.pb.h. Добавьте оба файла в проект в среде разработки Xcode. 6. В среде разработки Xcode переименуйте файл ViewController.m в ViewController.mm, как мы сделали в предыдущей главе, затем так же, как в файле ViewController.mm приложения HelloTensorFlow главы 2 «Клас- сифицирование изображений с помощью трансферного обучения», добавьте три действия UIAlertAction обработчика касаний для трех моделей обна- ружения объектов, которые мы добавили в проект и будем тестировать. Все файлы проекта теперь должны выглядеть как на рис. 3.7. Рисунок 3.7. Файлы проекта TFObjectDetectionAPI 7. Про должайте добавлять остальной программный код в файл viewcontroliier.mm. В метод viewDidLoad добавьте код, который программ- но создает новый элемент управления UIImageView для отображения тес - тового изображения и обнаруженных результатов, после того как вы- брана конкретная модель для работы на тестовом изображении, а затем добавьте следующие ниже реализации функций: NSString* FilePathForResourceName(NSString* name, NSString* extension)int LoadLablesFile(const string pbtxtFileName, object_detection:: protos:: StringIntLabelMap *imageLabels)string GetDisplayName(const object_detection:: protos:: StringIntLabelMap* labels, int index)Status LoadGraph(const string& graph_file_name, std:: unique_ptr<tensorflow:: Session>* session)void DrawTopDetections(std:: vector<Tensor>& outputs, int image_width, int image_height)void RunInferenceOnImage(NSString *model)\n--- Страница 94 ---\n92  Обнар ужение и локализация объектов Мы объясним эти реализации функций после следующего шага. Кроме того, вы можете получить весь исходный код в папке ch3/ios репозито- рия исходного кода книги. 8. Запу стите приложение в симуляторе или на устройстве iOS. Сначала вы увидите изображение на экране. Нажмите в любом месте, и вы увидите диалоговое окно выбора модели. Выберите модель SSD MobileNet, и при-мерно через 1 секунду, либо через пять секунд на iPhone 6, вы получите результаты обнаружения, которые будут нарисованы на изображении. Модель Faster RCNN Inception v2 занимает больше времени (около 5 се-кунд в симуляторе и более 20 на iPhone 6); эта модель также является более точной, чем модель SSD MobileNet, захватывая один объект соба-ки, который пропустила модель SSD MobileNet. Последняя модель, Faster RCNN Resnet 101, занимает около 20 секунд в симуляторе iOS, но вылета-ет на iPhone 6 из-за ее размера. На рис. 3.8 приведены результаты. Рисунок 3.8. Выполнение приложения с разными моделями и результатами обнаружения\n--- Страница 95 ---\nОбнаружение и локализация объектов  93 Возвращаясь к функциям на шаге 7, функция FilePathForResourceName явля- ется вспомогательной функцией, используемой для возврата пути к файлу ресурса: файл mscoco_label_map.pbtxt , который определяет идентификатор, внутреннее имя и отображаемое имя 90 обнаруживаемых классов объектов, файлы модельных графов и тестовые изображения. Ее реализация такая же, как и та, которую мы видели в приложении HelloTensorFlow в предыдущей главе. Функции LoadLablesFile и GetDisplayName используют API Google Protobuf для загрузки и разбора файла mscoco_label_map.pbtxt и возвращают отображаемое имя идентификатора обнаруженного объекта. Функция LoadGraph пытается загрузить один из трех выбранных пользовате- лем файлов моделей и возвращает состояние загрузки. Две функции RunInferenceOnImage и DrawTopDetections являются ключевыми. Как мы увидели в разделе «Настройка API TensorFlow обнаружения объектов», инструмент резюмирования графа summarize_graph показывает следующую ниже информацию о трех предварительно натренированных моделях обнару - жения объектов, которые мы использовали в приложении (обратите внимание на тип uint8 ): Found 1 possible inputs: (name=image_tensor, type=uint8(4), shape=[?,?,?,3]) Вот почему тензор изображения, который подается в нашу модель, должен создаваться с типом uint8 , а не с типом float . В противном случае при запуске модели вы получите ошибку. Также обратите внимание, что, когда мы исполь-зуем цикл for для конвертирования данных image_data в тензор image_tensor с типом Tensor , которого метод Run объекта Session в API C++ TensorFlow ожи- дает на входе, мы не используем input_mean и input_std , как мы это делали при применении моделей классифицирования изображений (для получения более подробного сравнения обратитесь к реализации функции RunInferenceOnImage приложения HelloTensorFlow в главе 2 «Классифицирование изображений с по- мощью трансферного обучения»). И мы знаем, что имеется четыре выхода, а именно detection_boxes (ограничительные рамки), detection_scores (оценки достоверности обнаружения), detection_classes (классы обнаружения) и num_ detections (количество обнаружений), поэтому функция RunInferenceOnImage имеет следующий ниже программный код для подачи изображения на вход модели и получения четырех результатов на выходе: tensorflow:: Tensor image_tensor(tensorflow:: DT_UINT8, tensorflow:: TensorShape({1, image_height, image_width, wanted_channels})); auto image_tensor_mapped = image_tensor.tensor<uint8, 4>(); tensorflow:: uint8* in = image_data.data();uint8* c_out = image_tensor_mapped.data();for (int y = 0; y < image_height; ++y) { tensorflow:: uint8* in_row = in + (y * image_width * image_channels); uint8* out_row = c_out + (y * image_width * wanted_channels);\n--- Страница 96 ---\n94  Обнар ужение и локализация объектов for (int x = 0; x < image_width; ++x) { tensorflow:: uint8* in_pixel = in_row + (x * image_channels); uint8* out_pixel = out_row + (x * wanted_channels); for (int c = 0; c < wanted_channels; ++c) { out_pixel[c] = in_pixel[c]; } } } std:: vector<Tensor> outputs;Status run_status = session ‑>Run({{\"image_tensor\", image_tensor}}, {\"detection_boxes\", \"detection_scores\", \"detection_classes\", \"num_detections\"}, {}, &outputs); Для того чтобы нарисовать ограничительные рамки на обнаруженных объ- ектах, мы передаем тензорный вектор outputs в функцию DrawTopDetections , в которой используется следующий ниже программный код разбора векто-ра outputs для получения значений четырех выходных результатов и обхода в цик ле каждого обнаруженного объекта, получая значения ограничительной рамки (границы слева, сверху, справа, снизу) и отображаемое имя обнаружен-ного идентификатора объекта. Это позволяет написать программный код для отрисовки ограничительной рамки с именем: auto detection_boxes = outputs[0].flat<float>();auto detection_scores = outputs[1].flat<float>();auto detection_classes = outputs[2].flat<float>();auto num_detections = outputs[3].flat<float>()(0); LOG(INFO) << \"num_detections: \" << num_detections << \", detection_scores size: \" << detection_scores.size() << \", detection_classes size: \" << detection_classes.size() << \", detection_boxes size: \" << detection_boxes.size(); for (int i = 0; i < num_detections; i++) { float left = detection_boxes(i * 4 + 1) * image_width; float top = detection_boxes(i * 4 + 0) * image_height; float right = detection_boxes(i * 4 + 3) * image_width; float bottom = detection_boxes((i * 4 + 2)) * image_height; string displayName = GetDisplayName(&imageLabels, detection_classes(i)); LOG(INFO) << \"Detected \" << i << \": \" << displayName << \", \" << score << \", (\" << left << \", \" << top << \", \" << right << \", \" << bottom << \") \"; } При выполнении со вторым тестовым изображением на рис. 3.1 и демон- страционным изображением, показанным на веб-сайте API Tensorflow обна-ружения объектов, приведенные выше строки LOG(INFO) выдадут следующую ниже информацию: num_detections: 100, detection_scores size: 100, detection_classes size: 100, detection_boxes size: 400\n--- Страница 97 ---\nОбнаружение и локализация объектов  95 Detected 0: person, 0.916851, (533.138, 498.37, 553.206, 533.727) Detected 1: kite, 0.828284, (467.467, 344.695, 485.3, 362.049)Detected 2: person, 0.779872, (78.2835, 516.831, 101.287, 560.955)Detected 3: kite, 0.769913, (591.238, 72.0729, 676.863, 149.322) Вот и все, что нужно, чтобы применить существующую предварительно на- тренированную модель обнаружения объектов в приложении для iOS. А что же насчет использования в iOS наших вторично натренированных моделей обна-ружения объектов? Оказывается, это почти то же самое, что и использование предварительно натренированных моделей, – вам не надо вносить никаких изменений в input_size , input_mean , input_std и input_name , как мы делали в пре- дыдущей главе во время работы со вторично натренированными моделями классификации изображений. Вам просто нужно выполнить следующие ниже шаги. 1. Добавить в проект TFObjectDetectionAPI вторично натренированную мо- дель, например файл output_inference_graph_ssd_mobilenet.pb , созданный в предыдущем разделе, файл со словарем меток, использованным для вторичной тренировки модели, например pet_label_map.pbtxt , и допол- нительно несколько новых тестовых изображений, характерных для вто-рично натренированной модели. 2. В файле ViewController.mm вызвать функцию RunInferenceOnImage со вто- рично натренированной моделью. 3. По-пре жнему находясь в ViewController.mm , вызвать функцию LoadLablesFile([FilePathForResourceName(@\"pet_label_map\",@\"pbtxt\") UTF8String], &imageLabels) ; внутри функции DrawTopDetections . Вот и все. Запустите приложение, и вы увидите, что обнаруженные резуль- таты будут более точно соответствовать вторично натренированной модели. Например, используя предыдущую вторично натренированную модель, сге-нерированную в результате вторичной тренировки с использованием набора данных Oxford Pets, мы ожидаем, что ограничительные рамки будут распола-гаться вокруг области головы, а не всего тела, и это именно то, что мы видим на тестовом изображении, показанном на рис. 3.9.\n--- Страница 98 ---\n96  Обнар ужение и локализация объектов Рисунок 3.9. Сравнение результатов обнаружения предварительно натренированной и вторично натренированной моделей использование Yolo2 – еще оДной моДели обнаружения объектов Как мы уже упоминали в первом разделе, YOLO2 (https: //pjreddie.com/darknet/ yolo) – это еще одна замечательная модель обнаружения объектов, использую- щая другой подход из семейства RCNN-детекторов. Данный подход применя- ет единую нейронную сеть для разделения входного изображения на участки фиксированного размера (но без предложений участков, как в методах семей-ства RCNN-детекторов) и предсказывает ограничительные рамки, классы и ве-роятности каждого участка. Пример приложения TensorFlow для Android имеет пример программно- го кода для использования предварительно натренированной модели YOLO, но пример для iOS отсутствует. Поскольку YOLO2 является одной из самых быст - рых моделей обнаружения объектов, а также довольно точной (сравните ее по- казатели mAP с моделями на основе SSD-детектора на ее веб-сайте), стоит взгля-нуть на то, что требуется для того, чтобы ее применить в приложении для iOS. Для тренировки своих моделей YOLO использует уникальную нейронную сеть с открытым исходным кодом Darknet (https: //pjreddie.com/darknet). Кроме того, есть еще одна библиотека, darkflow (https: //github.com/thtrieu/darkfiow), которая может конвертировать нейросетевые веса моделей YOLO, натрениро-ванные с помощью сети Darknet, в формат графа TensorFlow, а также выпол-нять вторичную тренировку предварительно натренированных моделей.\n--- Страница 99 ---\nОбнаружение и локализация объектов  97 Для построения моделей YOLO2 в формате платформы TensorFlow снача- ла следует получить репозиторий библиотеки darkflow по адресу https: //github. com/thtrieu/darkflow. Поскольку для этого требуется Python 3 и TensorFlow 1.0 (вполне возможно, что Python 2.7 и TensorFlow 1.4 или более поздней версии тоже будут работать), мы воспользуемся дистрибутивом Anaconda, чтобы уста-новить новую среду TensorFlow 1.0 с поддержкой Python 3: conda create --name tf1.0_p35 python=3.5source activate tf1.0_p35conda install -c derickl tensorflow Также выполните команду conda install ‑c menpo opencv3 , чтобы установить OpenCV 3, еще одну библиотеку, от которой зависит библиотека darkflow. Теперь перейдите в каталог библиотеки darkflow и выполните команду pip install . для установки библиотеки darkflow. Далее, нам нужно загрузить веса предварительно натренированных моде- лей YOLO. Мы попробуем две крошечные модели Tiny-YOLO, имеющие сверх - высокое быстродействие, но меньшую точность, чем полные модели YOLO. Программный код iOS для запуска крошечных моделей Tiny-YOLO и полных моделей YOLO примерно одинаков, поэтому мы просто покажем вам, как за-пускать крошечные модели Tiny-YOLO. Вы можете скачать веса и конфигурационные файлы модели tiny-yolo-voc (натренированной с использованием набора данных с 20 объектами/классами PASCAL VOC) и модели tiny-yolo (натренированной с использованием набора данных с 80 объектами/классами MS COCO) с официального веб-сайта YOLO2 или репозитория библиотеки darkflow. Теперь выполните следующие ниже ко-манды, чтобы конвертировать веса в файлы графа TensorFlow: flow --model cfg/tiny-yolo-voc.cfg --load bin/tiny-yolo-voc.weights --savepbflow --model cfg/tiny-yolo.cfg --load bin/tiny-yolo.weights --savepb Два сгенерированных файла tiny‑yolo‑voc.pb и tiny‑yolo.pb будут находиться в каталоге built_graph . Теперь перейдите в корневой каталог исходного кода TensorFlow и выполните следующие ниже команды для создания квантован-ных моделей, как мы делали в предыдущей главе: python tensorflow/tools/quantization/quantize_graph.py --input=darkflow/built_graph/tiny-yolo.pb --output_node_names=output --output=quantized_tiny-yolo.pb --mode=weightspython tensorflow/tools/quantization/quantize_graph.py --input=darkflow/built_graph/tiny-yolo-voc.pb --output_node_names=output --output=quantized_tiny-yolo-voc.pb --mode=weights Выполните следующие действия, чтобы увидеть, как использовать две моде- ли YOLO в нашем приложении для iOS. 1. Перетащит е в проект TFObjectDetectionAPI оба файла, quantized_tiny ‑yolo‑ voc.pb и quantized_tiny ‑yolo.pb .\n--- Страница 100 ---\n98  Обнар ужение и локализация объектов 2. Д обавьте два новых действия в контроллер ViewController.mm ; при запуске приложения вы увидите модели, доступные для запуска, как показано на рис. 3.10. Рисунок 3.10. Добавление двух моделей YOLO в приложение для iOS 3. Д обавьте следующий ниже программный код обработки входного изо- бражения в тензор, подаваемый во входной узел, и запустите сеанс TensorFlow с модельным графом YOLO, загруженным для генерирования выходных данных обнаружения: tensorflow:: Tensor image_tensor(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({1, wanted_height, wanted_width, wanted_channels}));auto image_tensor_mapped = image_tensor.tensor<float, 4>();tensorflow:: uint8* in = image_data.data();float* out = image_tensor_mapped.data();for (int y = 0; y < wanted_height; ++y) { out_pixel[c] = in_pixel[c] / 255.0f; }std:: vector<tensorflow:: Tensor> outputs;tensorflow:: Status run_status = session->Run({{\"input\", image_tensor}}, {\"output\"}, {}, &outputs); Обратите внимание на небольшое, но важное отличие в инструкциях for‑ioop и session‑>Run от программного кода, используемого для классифицирования\n--- Страница 101 ---\nОбнаружение и локализация объектов  99 изображений предыдущей главы и для обнаружения объектов с применени- ем других моделей, показанных ранее в этой главе (мы не показали фрагмент кода внутри многоточий ( ), как и в тех двух примерах). Для того чтобы кон- вертация данных изображения была правильной, вам нужно либо разобраться в деталях модели, либо изучить рабочий пример на Python, для Android или для iOS плюс, конечно же, разобраться в необходимой отладке. Для того чтобы пра-вильно установить имена входных и выходных узлов, вы можете применить инструмент резюмирования графа summarize_graph или фрагмент кода Python, который мы показывали уже неоднократно. 4. Пере дайте выходной результат в функцию YoloPostProcess , которая ана- логична программному коду постобработки в файле tensorflow/examples/ android/src/org/tensorflow/demo/TensorFlowYoloDetector.java примера для Android: tensorflow:: Tensor* output = &outputs[0];std:: vector<std:: pair<float, int> > top_results;YoloPostProcess(model, output ‑>flat<float>(), &top_results); Мы не будем здесь показывать остальную часть программного кода. Вы можете обратиться к полноценному приложению для iOS в папке ch3/ios репозитория исходного кода. 5. Запу стите приложение и выберите YOLO2 Tiny VOC или YOLO2 Tiny COCO, и вы увидите аналогичную скорость, но менее точные результаты обнаружения, чем при использовании модели SSD MobileNet v1. Модели TensorFlow и YOLO2 обнаружения объектов работают в мобильных устройствах довольно быстро, хотя модели TensorFlow на основе MobileNet и крошечные модели YOLO2 менее точны. Более крупные модели на основе более быстрого RNN-детектора и полные модели YOLO2 более точны, но они занимают гораздо больше времени или даже не будут работать на мобиль-ных устройствах вообще. Таким образом, лучший способ добавить быстрое обнаружение объектов в мобильные приложения – испо льзовать модели SSD MobileNet или Tiny-YOLO2 или модели, вторично натренированные на их основе и точно настроенные. Будущие выпуски моделей, скорее всего, будут иметь еще более высокую производительность и точность. Благодаря знаниям, полученным в этой главе, вы сможете быстро включать обнаружение объектов в приложения для iOS. резюме В этой главе мы сначала дали краткий обзор различных методов обнаруже-ния объектов на основе глубокого обучения. Затем мы рассмотрели в деталях, как использовать API TensorFlow обнаружения объектов (TensorFlow Object Detection API) для выведения стандартного заключения с помощью предва-рительно натренированных моделей и как вторично тренировать предвари-тельно натренированные модели TensorFlow обнаружения объектов на языке\n--- Страница 102 ---\n100  Обнар ужение и локализация объектов Python. Мы также предоставили подробные инструкции о том, как выполнять ручную сборку библиотеки TensorFlow для iOS, создавать новое приложение для iOS с помощью такой библиотеки и как использовать в iOS уже существу - ющие и вторично натренированные модели SSD MobileNet и модели Faster RCNN. Наконец, мы показали, что требуется для того, чтобы применить в сво-ем собственном приложении для iOS еще одну мощную модель обнаружения объектов, YOLO2. В следующей главе в центре внимания будет наша третья задача – компью- терное зрение, и мы внимательнее рассмотрим вопросы, связанные с трени-ровкой и построением на языке Python и в платформе TensorFlow увлекатель-ной глубоко обучающейся модели и ее применением в своем собственном приложении для iOS и Android, которое позволит нам добавлять в изображе-ния художественные стили.",
      "debug": {
        "start_page": 74,
        "end_page": 102
      }
    },
    {
      "name": "Глава 4. Трансформирование рисунков с помощью художественных стилей 101",
      "content": "--- Страница 103 --- (продолжение)\nГлава 4 Трансформирование рисунков с помощью художественных стилей С тех пор, как в 2012 году после победы нейросети AlexNet в конкурсе ImageNet популярность глубоких нейронных сетей резко пошла в гору, исследователи в области ИИ стали применять технологию глубокого обучения, включая пред-варительно натренированные глубокие модели на основе CNN-сетей, ко все большему числу проблемных областей. Что может быть креативнее, чем соз-давать художественные творения? Одна из идей, которая была предложена и реализована, называется нейронным переносом стиля, или нейростиле-вым переносом (neural-style transfer), который позволяет вам воспользовать-ся предварительно натренированными глубокими нейросетевыми моделями и переносить стиль изображения или, например, художественный стиль Ван Гога или Моне на другое изображение, такое как фотография в вашем профи-ле или фотография вашей любимой собаки, создавая тем самым изображение, в котором смешивается содержимое вашего изображения и стиль живописно-го шедевра. На самом деле уже существует приложение для iOS под названием Prisma, получившее в 2016 году приз «Лучшее приложение года», которое де-лает именно это; всего за несколько секунд оно трансформирует ваши фото-графии с помощью любого выбранного вами стиля. В этой главе мы сначала дадим краткий обзор трех методов нейронного пере- носа стиля, исходного, значительно улучшенного и еще более улучшенного. За-тем мы подробно рассмотрим, как использовать второй метод для тренировки модели быстрого нейронного переноса стиля, которая может быть применена в ваших смартфонах iOS и Android для достижения такого же результата, кото-рый имеет приложение Prisma. Далее мы воспользуемся этой моделью в при-ложении для iOS и для Android, подробно описав весь процесс создания таких приложений с нуля. Наконец, мы дадим вам краткое введение в проект с от - крытым исходным кодом TensorFlow Magenta, который вы можете использо-\nГлава 4 Трансформирование рисунков с помощью художественных стилей С тех пор, как в 2012 году после победы нейросети AlexNet в конкурсе ImageNet популярность глубоких нейронных сетей резко пошла в гору, исследователи в области ИИ стали применять технологию глубокого обучения, включая пред-варительно натренированные глубокие модели на основе CNN-сетей, ко все большему числу проблемных областей. Что может быть креативнее, чем соз-давать художественные творения? Одна из идей, которая была предложена и реализована, называется нейронным переносом стиля, или нейростиле-вым переносом (neural-style transfer), который позволяет вам воспользовать-ся предварительно натренированными глубокими нейросетевыми моделями и переносить стиль изображения или, например, художественный стиль Ван Гога или Моне на другое изображение, такое как фотография в вашем профи-ле или фотография вашей любимой собаки, создавая тем самым изображение, в котором смешивается содержимое вашего изображения и стиль живописно-го шедевра. На самом деле уже существует приложение для iOS под названием Prisma, получившее в 2016 году приз «Лучшее приложение года», которое де-лает именно это; всего за несколько секунд оно трансформирует ваши фото-графии с помощью любого выбранного вами стиля. В этой главе мы сначала дадим краткий обзор трех методов нейронного пере- носа стиля, исходного, значительно улучшенного и еще более улучшенного. За-тем мы подробно рассмотрим, как использовать второй метод для тренировки модели быстрого нейронного переноса стиля, которая может быть применена в ваших смартфонах iOS и Android для достижения такого же результата, кото-рый имеет приложение Prisma. Далее мы воспользуемся этой моделью в при-ложении для iOS и для Android, подробно описав весь процесс создания таких приложений с нуля. Наконец, мы дадим вам краткое введение в проект с от - крытым исходным кодом TensorFlow Magenta, который вы можете использо-\n--- Страница 104 ---\n102  Трансформирование рисунков с помощью художественных стилей вать для создания новых приложений-генераторов музыки и художественных произведений на основе глубокого обучения, и покажем вам, как применять одну из предварительно натренированных моделей переноса стиля, создан-ную на основе последних научных достижений в области нейронного переноса стиля, включая 26 поразительных художественных стилей, в ваших приложе-ниях для iOS и Android, чтобы достичь еще более высокой производительности и результатов. Резюмируя, в этой главе мы рассмотрим следующие темы: нейронный перенос с тиля – краткий обзор; тренировка мо делей быстрого нейронного переноса стиля; испо льзование моделей быстрого нейронного переноса стиля в iOS; применение мо делей быстрого нейронного переноса стиля в Android; испо льзование многостилевой модели TensorFlow Magenta в iOS; применение мног остилевой модели TensorFlow Magenta в Android. нейронный перенос стиля – краткий обзор Оригинальная идея и алгоритм использования глубокой нейронной сети для слияния содержимого изображения со стилем другого изображения была опуб - ликована в статье под названием «A Neural Algorithm of Artistic Style» («Ней- ронный алгоритм художественного стиля») (https: //arxiv.org/abs/1508.06576), вышедшей летом 2015 года. Она была основана на предварительно натре-нированной глубокой CNN-модели под названием VGG -19 (https: //arxiv.org/ pdf/1409.1556.pdf), победителе конкурса по распознаванию изображений ImageNet 2014 года, имеющей 16 сверточных слоев, или карт признаков, пред-ставляющих различные уровни содержимого изображения. В этом исходном методе окончательное изображение с перенесенным стилем сначала иниции-руется как изображение белого шума в сочетании с содержимым изображения. Функция потери содержимого определяется как вычисляемая на основе квад - ратичной ошибки потеря конкретного набора признаковых представлений на сверточном слое, conv4_2, контентного изображения и результирующего изображения после их подачи в сеть VGG -19. Функция потери стиля вычис - ляет общую разницу ошибок стилевого изображения и результирующего изо-бражения на пяти различных сверточных слоях. Общая потеря определяется как сумма потери содержимого и потери стиля. Во время тренировки потери минимизируются, и в результате получается изображение, в котором смеши-вается содержимое одного изображения и стиль другого. Хотя результат оригинального алгоритма нейронного переноса стиля был довольно удивительным, его производительность была очень низкой – тре- нировка была частью процесса генерирования изображений с перенесенным стилем, и обычно для получения хороших результатов требовалось нескольких минут на GPU и около часа на CPU.  Ес ли вы заинтересованы в подробностях оригинального алгоритма, то можете почитать статью наряду с изучением хорошо документированной реализации на языке Python,\n--- Страница 105 ---\nТрансформирование рисунков с помощью художественных стилей  103 размещенной на Github по адресу https: //github.com/logO/neural-style-painting/blob/ master/art.py . Мы не будем обсуждать этот оригинальный алгоритм, поскольку его невоз- можно выполнить на мобильном устройстве, но в целом данный алгоритм занимателен и поучителен в том плане, что он позволяет получить более четкое понимание того, как использовать предварительно натренированные глубокие CNN-модели для различных задач компьютерного зрения. Вполне естественным образом в 2016 году был опубликован новый алгоритм, который оказался «на три порядка быстрее», «Perceptual Losses for Real-Time Style Transfer and Super-Resolution» («Перцептивные потери для переноса сти-ля в реальном времени и сверхразрешающей способности») (https: //cs.stanford. edu/people/jcjohns/eccv16/) Джастина Джонсона (Justin Johnson) и др. В нем используется отдельный тренировочный процесс и определяются более оп-тимальные функции потери, которые сами по себе являются глубокими ней-ронными сетями. После тренировки, которая может занять несколько часов на GPU, как мы увидим в следующем разделе, использование натренирован-ной модели для генерирования изображения с перенесенным стилем проис - ходит почти в реальном времени на компьютере и занимает всего несколько секунд на смартфоне. Вместе с тем по-прежнему остается один недостаток использования этого алгоритма быстрого нейронного переноса: модель можно натренировать толь-ко для определенного стиля, и поэтому, для того чтобы в вашем приложении ис - пользовать разные стили, вы должны натренировать эти стили один за другим, создавая одну модель для каждого стиля. В 2017 году была опубликована новая статья под названием «A Learned Representation For Artistic Style» («Заучиваемое представление о художественном стиле») (https: //arxiv.org/abs/1610.07629), в ко - торой было показано, что единая глубокая нейросетевая модель может обоб-щать множество различных стилей. Проект TensorFlow Magenta (https: //github. com/tensorflow/magenta/tree/master/magenta/models/image_stylization) включает в себя предварительно натренированные модели с многочисленными стилями, и в последних двух разделах этой главы мы увидим, насколько легко можно применять такую модель в приложениях для iOS и Android с целью создания мощных и удивительных художественных эффектов. тренировка моДелей быстрого нейронного переноса стиля В этом разделе мы покажем вам, как тренировать модели на основе алгорит - ма быстрого нейронного переноса стиля с помощью платформы TensorFlow. Для выполнения тренировки такой модели выполните следующие ниже дей-ствия. 1. В терминале вашего Mac или желательно в Ubuntu с поддержкой GPU выполните клонирование репозитория git clone https://github.com/ jeffxtang/fast ‑style‑transfer , который представляет собой ответвление\n--- Страница 106 ---\n104  Трансформирование рисунков с помощью художественных стилей хорошей реализации в TensorFlow алгоритма быстрого переноса стиля Джонсона, модифицированного под применение натренированной мо-дели в приложениях для iOS или Android. 2. Командой cd перейдите в каталог fast‑style‑transfer , затем запустите сценарий оболочки setup.sh для скачивания предварительно натрениро- ванного файла модели VGG -19, а также тренировочного набора данных MS COCO, о котором мы упоминали в предыдущей главе. Обратите вни-мание, что скачивание больших файлов может занять несколько часов. 3. Выпо лните следующие ниже команды, чтобы создать файлы контроль- ных точек с итогами тренировки, используя стилевое изображение под названием starry_night.jpg и контентное изображение под названием wwi.jpg : mkdir checkpointsmkdir test_dirpython style.py --style images/starry_night.jpg --test images/ww1.jpg --test-dir test_dir --content-weight 1.5e1 --checkpoint-dir checkpoints --checkpoint-iterations 1000 --batch-size 10 В каталоге images имеется несколько других стилевых изображений, ко- торые можно использовать для создания различных файлов контроль-ных точек. Используемое здесь стилевое изображение starry_night.jpg (Звездная ночь) – это знаменитая картина Винсента Ван Гога, которая показана на рис. 4.1. Рисунок 4.1. Использование картины Ван Гога в качестве стилевого изображения\n--- Страница 107 ---\nТрансформирование рисунков с помощью художественных стилей  105 В Ubuntu с поддержкой Nvidia GTX 1070 GPU, которую мы настроили в главе 1 «Начало работы с платформой TensorFlow Mobile», вся трени- ровка занимает около пяти часов, которая, безусловно, займет гораздо больше времени на CPU.  Сценарий был первоначально написан для платформы TensorFlow 0.12, но позже из- менен для работы с платформой TensorFlow 1.1 и перепроверен, чтобы в равной мере хорошо работать в среде платформы TensorFlow 1.4 с языком Python 2.7, которую мы настроили ранее. 4. Откройт е файл evaluate.py в текстовом редакторе и раскомментируйте следующие две строки программного кода (в строках 158 и 159): # saver = tf.train.Saver()# saver.save(sess, \"checkpoints_ios/fns.ckpt\") 5. Выпо лните следующую ниже команду, чтобы создать новую контроль- ную точку с входным изображением img_placeholder и изображением preds с перенесенным стилем: python evaluate.py --checkpoint checkpoints \\ --in-path examples/content/dog.jpg \\--out-path examples/content/dog-output.jpg 6. Выпо лните следующую ниже команду, чтобы создать файл графа TensorFlow, объединяющий определение графа и веса в контрольной точке. В результате будет создан файл .pb размером около 6,7 МБ: python freeze.py --model_folder=checkpoints_ios --output_graph fst_frozen.pb 7. При условии что у вас есть каталог /tf_files , скопируйте файл fst_ frozen. pb, сгенерированный в папке /tf_files . Комадой cd перейдите непосред- ственно в ваш корневой каталог с исходным кодом TensorFlow; скорее всего, это будет папка ~/tensorflow‑1.4 . Затем выполните следующую ниже команду, чтобы создать квантованную модель из файла .pb (мы рассмот - рели квантование в главе 2 «Классифицирование изображений с помощью трансферного обучения»): bazel-bin/tensorflow/tools/quantization/quantize_graph \\--input=/tf_files/fst_frozen.pb \\--output_node_names=preds \\--output=/tf_files/fst_frozen_quantized.pb \\--mode=weights В результате размер файла с замороженным графом будет сокращен с 6,7 МБ до 1,7 МБ, то есть если вы поместите в свое приложение 5 0 моделей для 50 различных стилей, то суммарный размер составит около 85 Мб. В сентябре 2017 года компания Apple объявила, что лимит скачивания на одно мобиль-ное приложение через интернет увеличен до 150 Мб, поэтому пользователи\n--- Страница 108 ---\n106  Трансформирование рисунков с помощью художественных стилей без проблем смогут скачивать приложения по сотовой сети с более чем 50 раз- личными стилями. Вот и все, что требуется для тренировки и квантования модели быстрого нейронного переноса с использованием стилевого и входного изображений. Вы можете проверить сгенерированные изображения в каталоге test_dir , соз - данном на шаге 3, и посмотреть эффекты переноса стиля. При необходимости вы можете поиграть с настройками гиперпараметров, задокументированными на веб-странице https: //github.com/jeffxtang/fast-style-transfer/blob/master/docs. md#style, и сможете увидеть различные и, надо надеяться, более качественные эффекты переноса стиля. Одно важное замечание, прежде чем мы узнаем, как применять эти модели в наших приложениях для iOS и Android. Вам нужно записать точные шири-ну и высоту изображения, указанные в качестве значения параметра ‑‑in‑path в шаге 5, и использовать эти значения ширины и высоты в своем программном коде для iOS или Android (вскоре вы увидите, как это сделать). В противном случае вы получите ошибку несоответствия размеров Conv2DCustomBackpropInput: Size of out_backprop doesn't match computed при запуске модели в своем прило- жении. использование моДелей быстрого нейронного переноса стиля в ios Оказывается, у нас не будет проблем с использованием сгенерированного в шаге 7 файла модели fst_frozen_quantized.pb в приложении для iOS, создан- ном с помощью экспериментального модуля TensorFlow, как было показано в главе 2 «Классифицирование изображений с помощью трансферного обучения». Однако модулю TensorFlow (по состоянию на январь 2018 г.) не удастся загру - зить файл предварительно натренированной многостилевой модели из про-екта TensorFlow Magenta, который мы будем использовать в следующем далее разделе этой главы, – при попытке загрузить файл многостилевой модели будет выдано следующее ниже сообщение об ошибке, говорящее о невозможности создать граф TensorFlow из-за незарегистрированной реализации операции: Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'Mul' with these attrs. Registered devices: [CPU], Registered kernels: device='CPU'; T in [DT_FLOAT] [[Node: transformer/expand/conv1/mul_1 = Mul[T=DT_INT32](transformer/expand/conv1/mul_1/x, transformer/expand/conv1/strided_slice_1)]] В главе 3 «Обнаружение и локализация объектов» мы говорили о ее причи- не и о том, как использовать библиотеки TensorFlow ручной сборки для ис - правления такой ошибки. Поскольку мы будем применять обе модели в одном приложении, то создадим новое приложение для iOS, используя более мощные библиотеки TensorFlow ручной сборки.\n--- Страница 109 ---\nТрансформирование рисунков с помощью художественных стилей  107 Добавление и тестирование с моДелями быстрого нейронного переноса Если вы не выполнили ручную сборку библиотек TensorFlow, то вам сначала необходимо вернуться к предыдущей главе. Затем выполните следующие да-лее действия, чтобы добавить в приложение для iOS поддержку TensorFlow и файлы модели быстрого нейронного переноса стиля, а затем протестировать приложение. 1. Если у вас уже есть приложение iOS, куда добавлены библиотеки TensorFlow ручной сборки, то этот шаг можно пропустить. В против-ном случае, подобно тому, как мы делали в предыдущей главе, создай-те новое приложение для iOS на языке Objective-C с именем NeuralSty‑ leTransfer либо в существующем приложении в разделе Build Settings («Параметры сборки») раздела PROJECT создайте новый пользователь- ский параметр под названием TENSORFLOW_ROOT со значением $HOME/tensor‑ flow‑1.4.0 , при условии что у вас там установлена платформа TensorFlow 1.4.0, а затем в подразделе Build Settings (Параметры сборки) раздела TARGETS (Цели) введите значение в поле Other Linker Flags (Другие флаги компоновщика): --force_load $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf-lite.a $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/downloads/nsync/builds/lipo.ios.c++11/nsync.a Затем в поле Header Search Paths (Маршр уты поиска заголовков) задайте маршруты: $(TENSORFLOW_ROOT) $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/downloads/protobuf/src $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/downloads $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/downloads/eigen $(TENSORFLOW_ROOT)/tensorflow/contrib/makefile/gen/proto 2. Перетащит е в папку вашего проекта файл fst_frozen_quantized.pb и не- сколько тестовых изображений. Скопируйте в проект те же файлы ios_image_load.mm и .h, которые мы использовали в предыдущих гла- вах из предыдущих приложений для iOS, либо из папки приложения NeuralStyleTransfer внутри папки Ch4/ios репозитория исходного когда книги. 3. Переименуйт е файл ViewController.m в ViewController.mm и замените его и ViewController.h на файлы ViewController.h и .mm из папки Ch4/ios/ NeuralStyleTransfer . Мы подробно рассмотрим корневой фрагмент про- граммного кода после тестового запуска приложения. 4. Запу стите приложение в симуляторе iOS или на устройстве iOS, и вы уви- дите изображение собаки, как показано на рис. 4.2.\n--- Страница 110 ---\n108  Трансформирование рисунков с помощью художественных стилей Рисунок 4.2. Исходное изображение собаки до применения стиля 5. Прик оснитесь к надписи Fast Style Transfer (Быс трый перенос стиля), и через несколько секунд вы увидите новую картинку, как на рис. 4.3, с перенесенным стилем «Звездной ночи».\n--- Страница 111 ---\nТрансформирование рисунков с помощью художественных стилей  109 Рисунок 4.3. Как будто Ван Гог нарисовал вашу любимую собаку Вы можете легко построить другие модели с разными стилями, выбирая в качестве стилевых изображений свои любимые фотографии и следуя ин- струкциям в предыдущем разделе. Затем выполните действия, описанные в этом разделе, чтобы применить модели в приложении для iOS. Если вы заин-тересованы разобраться в том, как происходит тренировка модели, то вам сле-дует обратиться к программному коду в репозитории GitHub из предыдущего раздела. Давайте подробно рассмотрим программный код для iOS, в котором данная модель используется для совершения волшебства. анализ программного коДа ios с использованием моДелей быстрого нейронного переноса В контроллере представления ViewCcontroller.mm есть несколько фрагментов программного кода, которые уникальны для предобработки входного изобра-жения и постобработки изображения с перенесенным стилем.\n--- Страница 112 ---\n110  Трансформирование рисунков с помощью художественных стилей 1. Двум константам, wanted_width с требуемой шириной и wanted_height с требуемой высотой, задаются те же самые значения, что и ширина и высота изображения examples/content/dog.jpg из репозитория в шаге 5: const int wanted_width = 300;const int wanted_height = 400; 2. Д ля загрузки и запуска нашей модели быстрого нейронного переноса в потоке вне пользовательского интерфейса и для отправки изображе-ния в поток пользовательского интерфейса для отображения на экране после генерирования изображения с перенесенным стилем использует - ся диспетчерская очередь iOS: dispatch_async(dispatch_get_global_queue(0, 0), ^{ UIImage *img = imageStyleTransfer(@\"fst_frozen_quantized\");dispatch_async(dispatch_get_main_queue(), ^{ _lbl.text = @\"Tap Anywhere\";_iv.image = img; }); }); 3. Д ля конвертирования входных данных изображения определяется 3-мерный тензор вещественных чисел (с плавающей точкой): tensorflow:: Tensor image_tensor(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({wanted_height, wanted_width, wanted_channels}));auto image_tensor_mapped = image_tensor.tensor<float, 3>(); 4. Имя входного узла и имя выходного узла, отправляемые в метод TensorFlow Session‑>Run, определяются так же, как и во время тренировки модели: std:: string input_layer = \"img_placeholder\";std:: string output_layer = \"preds\";std:: vector<tensorflow:: Tensor> outputs;tensorflow:: Status run_status = session ‑>Run({{input_layer, image_tensor}} {output_layer}, {}, &outputs); 5. Пос ле завершения работы модели и отправки назад выходного тензора, содержащего значения RGB в диапазоне от 0 до 255, сначала необходимо вызвать служебную функцию tensorToUIImage для конвертирования дан- ных тензора в буфер RGB: UIImage *imgScaled = tensorToUIImage(model, output ‑>flat<float>(), image_width, image_ height); static UIImage* tensorToUIImage(NSString *model, const Eigen:: TensorMap<Eigen:: Tensor<float, 1, Eigen:: RowMajor>, Eigen:: Aligned>& outputTensor, int image_width, int image_height) { const int count = outputTensor.size();\n--- Страница 113 ---\nТрансформирование рисунков с помощью художественных стилей  111 unsigned char* buffer = (unsigned char*)malloc(count); for (int i = 0; i < count; ++i) { const float value = outputTensor(i); int n;if (value < 0) n = 0;else if (value > 255) n = 255;else n = (int)value;buffer[i] = n; } 6. Зат ем мы конвертируем буфер в экземпляр объекта UIImage перед изме- нением его размера и возвратом для отображения на экране: UIImage *img = [ViewController convertRGBBufferToUIImage: buffer withWidth: wanted_width withHeight: wanted_height];UIImage *imgScaled = [img scaleToSize: CGSizeMake(image_width, image_height)];return imgScaled; Полноценный код и приложение находятся в папке Ch4/ios/NeuralStyleTrans‑ fer репозитория. использование моДелей быстрого нейронного переноса стиля в android В главе 2 «Классифицирование изображений с помощью трансферного обуче- ния» мы дали описание того, как добавлять поддержку платформы TensorFlow в свое собственное приложение для Android, но без какого-либо пользователь-ского интерфейса. Давайте создадим новое приложение для Android с исполь-зованием моделей быстрого переноса стиля, которые мы натренировали ранее и использовали в iOS. Поскольку данное приложение для Android предоставляет прекрасную возможность применить минимальный TensorFlow-ориентированный про-граммный код, пользовательский интерфейс Android и поточный код для за-пуска полноценного приложения с поддержкой модели TensorFlow, мы прой-демся по каждой строке программного кода с нуля, чтобы помочь вам глубже разобраться в том, что требуется для разработки приложения TensorFlow для Android с нуля. 1. В Android Studio выберите File (Файл) | New (Создать) | New Project… (Новый проект) и введите FastNeuralTransfer в качестве имени прило- жения. Примите все значения по умолчанию перед нажатием кнопки Finish (Завершить). 2. Создайте новую папку ресурсов assets , как показано на рис. 2.13, и пере- тащите в папку assets натренированные модели быстрого нейронного переноса из приложения для iOS, если вы протестировали его в преды-дущем разделе, либо из папки /tf_fiies , как в шаге 7 раздела «Тренировка\n--- Страница 114 ---\n112  Трансформирование рисунков с помощью художественных стилей моделей быстрого нейронного переноса», вместе с несколькими тестовы- ми изображениями. 3. В файл build.gradle приложения добавьте строку compile 'org.tensorflow: tensorflow ‑android:+' в конце секции зависимостей. 4. Откройт е файл res/layout/activity_main.xml , удалите оттуда заданный по умолчанию элемент управления TextView и сначала добавьте элемент управления ImageView , который будет показывать изображение до и после переноса стиля: <ImageView android: id=\"@+id/imageview\" android: layout_width=\"match_parent\"android: layout_height=\"match_parent\"app: layout_constraintBottom_toBottomOf=\"parent\"app: layout_constraintLeft_toLeftOf=\"parent\"app: layout_constraintRight_toRightOf=\"parent\"app: layout_constraintTop_toTopOf=\"parent\"/> 5. Д обавьте элемент управления Button , который будет инициировать дей- ствие переноса стиля: <Button android: id=\"@+id/button\"android: layout_width=\"wrap_content\"android: layout_height=\"wrap_content\"android: text=\"Style Transfer\"app: layout_constraintBottom_toBottomOf=\"parent\"app: layout_constraintHorizontal_bias=\"0.502\"app: layout_constraintLeft_toLeftOf=\"parent\"app: layout_constraintRight_toRightOf=\"parent\"app: layout_constraintTop_toTopOf=\"parent\"app: layout_constraintVertical_bias=\"0.965\" /> 6. В файле MainActivity.java приложения сначала введите наш самый важ - ный импорт: import org.tensorflow.contrib.android.TensorFlowInferenceInterface; Интерфейс TensorFlowInferenceInterface предоставляет интерфейс Java для доступа к нативным для платформы TensorFlow прикладным интер-фейсам (API) выведения заключений. Затем убедитесь, что класс Main‑ Activity реализует интерфейс Runnable , так как нам нужно, чтобы наше приложение откликалось, загружало и запускало модель TensorFlow в рабочем потоке. 7. В начале к ласса следующим образом определите шесть констант: private static final String MODEL_FILE = \"file:///android_asset/fst_frozen_quantized.pb\";private static final String INPUT_NODE = \"img_placeholder\";private static final String OUTPUT_NODE = \"preds\";\n--- Страница 115 ---\nТрансформирование рисунков с помощью художественных стилей  113 private static final String IMAGE_NAME = \"pug1.jpg\"; private static final int WANTED_WIDTH = 300;private static final int WANTED_HEIGHT = 400; В качестве значения константы MODEL_FILE вы можете использовать лю- бой из ваших файлов с натренированной моделью. Значения констант INPUT_NODE и OUTPUT_NODE совпадают со значениями, заданными в трени- ровочном сценарии Python и используемыми в приложении iOS. Кон-станты WANTED_WIDTH и WANTED_HEIGHT – э то, опять-таки, те же значения, что и ширина и высота изображения в ‑‑in‑path, которые мы использова- ли на шаге 5 раздела «Тренировка моделей быстрого нейронного переноса стиля». 8. Объявите четыре экземплярные переменные: private ImageView mImageView;private Button mButton;private Bitmap mTransferredBitmap;private TensorFlowInferenceInterface mInferenceInterface; Экземплярные переменные mImageView и mButton будут установлены, ис - пользуя простой метод findViewById внутри метода onCreate . Экземплярная переменная mTransferredBitmap будет содержать объект Bitmap переноси- мого изображения. Благодаря ей элемент пользовательского интерфей-са, на который ссылается переменная mImageView , будет показывать его на экране. Экземплярная переменная mInferenceInterface используется для загрузки нашей модели TensorFlow, подачи входного изображения в модель, запуска модели и возврата результата выведения заключения. 9. Создайте экземпляр обработчика событий Handler для обработки зада- чи отображения результирующего изображения с перенесенным стилем в главном потоке, после того как наш поток TensorFlow выведения за-ключения отправит сообщение в экземпляр обработчика. После этого мы также создадим удобное всплывающее сообщение Toast : Handler mHandler = new Handler() { @Override public void handleMessage(Message msg) { mButton.setText(\"Style Transfer\"); String text = (String)msg.obj; Toast.makeText(MainActivity.this, text, Toast.LENGTH_SHORT).show(); mImageView.setImageBitmap(mTransferredBitmap); }}; 10. Вну три метода onCreate мы свяжем элемент управления ImageView в ма- кетном xml-файле с экземплярной переменной mImageView , загрузим в объект Bitmap наше тестовое изображение из папки assets и покажем его в элементе управления ImageView :\n--- Страница 116 ---\n114  Трансформирование рисунков с помощью художественных стилей mImageView = findViewById(R.id.imageview); try { AssetManager am = getAssets(); InputStream is = am.open(IMAGE_NAME); Bitmap bitmap = BitmapFactory.decodeStream(is); mImageView.setImageBitmap(bitmap); } catch (IOException e) { e.printStackTrace(); } 11. Аналогичным образом настройте переменную mButton , а также настрой- те слушателя касаний, который при касании на кнопку будет создавать и запускать новый поток, вызывающий метод run: mButton = findViewById(R.id.button);mButton.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { mButton.setText(\"Processing \"); Thread thread = new Thread(MainActivity.this); thread.start(); } }); 12. В методе run потока мы сначала объявляем три массива и выделяем для них соответствующие участки оперативной памяти: массив intValues со- держит значения пикселов тестового изображения, при этом значение каждого пиксела представлено в 32-разрядном формате ARGB (альфа, красный, зеленый, синий); массив floatValues содержит значения крас - ного, зеленого и синего для каждого пиксела по отдельности, как и ожи-дается моделью, поэтому он имеет размер в три раза больше, чем массив intValues , и массив outputValues имеет такой же размер, что и floatValues , но содержит выходные значения модели: public void run() { int[] intValues = new int[WANTED_WIDTH * WANTED_HEIGHT]; float[] floatValues = new float[WANTED_WIDTH * WANTED_HEIGHT * 3]; float[] outputValues = new float[WANTED_WIDTH * WANTED_HEIGHT * 3]; Затем мы получаем растровые данные тестового изображения, шкалируем их, приводя их в соответствие размеру изображений, использованных для тре-нировки, и затем загружаем прошкалированные пикселы растрового изобра-жения в массив intValues и конвертируем их в вещественные floatValues : Bitmap bitmap = BitmapFactory.decodeStream(getAssets().open(IMAGE_NAME));Bitmap scaledBitmap = Bitmap.createScaledBitmap(bitmap, WANTED_WIDTH, WANTED_HEIGHT, true);scaledBitmap.getPixels(intValues, 0, scaledBitmap.getWidth(), 0, 0, scaledBitmap.getWidth(), scaledBitmap.getHeight());\n--- Страница 117 ---\nТрансформирование рисунков с помощью художественных стилей  115 for (int i = 0; i < intValues.length; i++) { final int val = intValues[i]; floatValues[i*3] = ((val >> 16) & 0xFF); floatValues[i*3+1] = ((val >> 8) & 0xFF); floatValues[i*3+2] = (val & 0xFF); } Обратите внимание, что val , или каждый элемент пиксельного массива intValues , является 32-разрядным целым числом, содержащим ARGB в каж - дой из 8-разрядных областей. Мы применяем поразрядный сдвиг вправо (для красного и зеленого) и операцию поразрядного «И» для извлечения красного, зеленого и синего значений каждого пиксела, игнорируя значение альфа са-мого левого 8-го разряда в элементе intValues . В результате floatValues[i*3] , floatValues[i*3+1] и floatValues[i*3+2] содержат значения соответственно красного, зеленого и синего цветов пиксела. Теперь мы создаем новый экземпляр объекта TensorFlowInferenceInterface , передавая его с экземпляром AssetManager и именем файла модели из папки assets , и используем экземпляр объекта TensorFlowInferenceInterface для пере- дачи входного узла input_node с конвертированным массивом floatValues . Если модель ожидает более одного входного узла, то можно просто вызвать метод feed несколько раз. Затем мы запускаем модель, передавая строковый массив имен выходных узлов. Здесь в случае с нашей моделью быстрого переноса сти-ля мы имеем всего один входной и один выходной узел. Наконец, мы получаем выходные значения модели путем передачи имени выходного узла. Если ожи-дается получение нескольких выходных узлов, то можно вызвать метод fetch несколько раз: AssetManager assetManager = getAssets();mInferenceInterface = new TensorFlowInferenceInterface(assetManager, MODEL_FILE);mInferenceInterface.feed(INPUT_NODE, floatValues, WANTED_HEIGHT, WANTED_WIDTH, 3);mInferenceInterface.run(new String[] {OUTPUT_NODE}, false);mInferenceInterface.fetch(OUTPUT_NODE, outputValues); Сгенерированные моделью выходные значения outputValues в каждом эле- менте содержат одно из 8-разрядных значений красного, зеленого и синего в диапазоне от 0 до 255, и мы сначала применяем к значениям красного и зе-леного операцию поразрядного сдвига влево, но с другими размерами сдвига (16 и 8), а затем с помощью поразрядной операции «ИЛИ» объединяем 8-раз-рядное значение альфа (значение 0xFF) с 8-разрядными значениями RGB и со-храняем результат в массиве intValues : for (int i=0; i < intValues.length; ++i) { intValues[i] = 0xFF000000 | (((int) outputValues[i*3]) << 16) | (((int) outputValues[i*3+1]) << 8) | ((int) outputValues[i*3+2]);\n--- Страница 118 ---\n116  Трансформирование рисунков с помощью художественных стилей Затем мы создаем новый экземпляр объекта Bitmap и устанавливаем значе- ния его пикселов с помощью массива intValues , шкалируем растровое изобра- жение, приводя его к исходному размеру тестового изображения, и сохраняем прошкалированное растровое изображение в переменной mTransferredBitmap : Bitmap outputBitmap = scaledBitmap.copy(scaledBitmap.getConfig(), true);outputBitmap.setPixels(intValues, 0, outputBitmap.getWidth(), 0, 0, outputBitmap.getWidth(), outputBitmap.getHeight());mTransferredBitmap = Bitmap.createScaledBitmap(outputBitmap, bitmap.getWidth(), bitmap.getHeight(), true); Наконец, мы отправляем сообщение обработчику нашего основного пото- ка о том, что пришло время показать изображение с перенесенным стилем на экране: Message msg = new Message();msg.obj = \"Tranfer Processing Done\";mHandler.sendMessage(msg); Таким образом, в общей сложности в менее чем 100 строках программно- го кода у вас получилось полноценное приложение для Android, которое вы-полняет перенос стиля на предоставленное вами изображение. Запустив это приложение на своем устройстве Android или в виртуальном устройстве, вы сначала увидите предоставленное вами тестовое изображение вместе с кноп-кой. Коснитесь кнопки – и через несколько секунд вы увидите изображение с перенесенным стилем, как показано на рис. 4.4. Рисунок 4.4. Исходное изображение и изображение с перенесенным стилем на Android\n--- Страница 119 ---\nТрансформирование рисунков с помощью художественных стилей  117 Одна из проблем, которую мы имеем с моделями быстрого нейронного пере- носа стиля, заключается в том, что даже если после квантования каждая модель составляет всего 1,7 Мб, нам все равно необходимо выполнять тренировку от - дельно каждого стиля, и каждая натренированная модель может поддерживать перенос всего одного стиля. К счастью, есть отличное решение этой проблемы. использование многостилевой моДели Tensor Flow MaGenT a в ios Проект TensorFlow Magenta (https: //github.com/tensorfiow/magenta) позволяет ис - пользовать более 10 предварительно натренированных моделей для генери-рования новых музыкальных и графических композиций. В этом и следующих разделах мы сосредоточимся на использовании моделей стилизации изобра-жений Magenta. Вы можете перейти по ссылке и установить Magenta на вашем компьютере, хотя, для того чтобы использовать их модели переноса стилей изо-бражений в ваших мобильных приложениях, вам вовсе не нужно устанавливать Magenta. Предварительно натренированная модель переноса стиля, реализо-ванная на основе работы «A Learned Representation for Artistic Style» («Заучива-емые представления о художественном стиле») 2017 года, устраняет ограниче-ние на наличие у модели только одного стиля и позволяет включать в один файл модели многочисленные стили, при этом вы можете использовать любую ком-бинацию этих стилей. Вы можете взглянуть на демонстрационный пример, раз-мещенный в Github по адресу https: //github.com/tensorflow/magenta/tree/master/ magenta/models/image_stylization. Вместе с тем использовать непосредственно в вашем мобильном приложении две предварительно натренированные и за-фиксированные в контрольной точке модели, которые доступны для скачи-вания, не получится из-за некоторых ошибок NaN (не число), содержащихся в файлах контрольных точек. Мы не будем подробно рассматривать шаги, ко-торые необходимо предпринять, для того чтобы удалить эти числа и сгенери-ровать файл модели с расширением.pb, который может использоваться в ваших приложениях (если интересно, вы можете обратиться к веб-странице на Github https: //github.com/tensorflow/tensorflow/issues/9678 по этому поводу). Вместо это- го мы просто воспользуемся файлом предварительно натренированной модели stylize_quantized.pb , включенным в пример TensorFlow для Android в папке ten‑ sorflow/examples/android/assets , чтобы увидеть, как это работает. Если вы действительно хотите тренировать свои собственные модели, то можете выполнить шаги, перечисленные под заголовком «Training a Model» («Тренировка модели») в указанной выше ссылке image_stylization. Но имейте в виду, что для скачивания набора данных ImageNet требуется не менее 500 Гб свободного дискового пространства и мощные GPU. Скорее всего, после того как вы увидите программный код и результаты в этом или следующем разделе, вы останетесь очень довольны эффектами переноса стиля, реализованными предварительно натренированной моделью stylize_quantized.pb .\n--- Страница 120 ---\n118  Трансформирование рисунков с помощью художественных стилей Выполните следующие далее действия, чтобы применить и запустить много- стилевую модель в нашем приложении для iOS, созданном ранее в этой главе. 1. В среде разработки Xcode перетащите файл stylize_quantized.pb из пап- ки tensorflow/examples/android/assets в вашу папку приложений apps . 2. Добавьте новый объект UIAlertAction в обработчик касаний, используя тот же объект dispatch_async , который мы использовали для загрузки и обработки моделей быстрого переноса стиля: UIAlertAction* multi_style_transfer = [UIAlertAction actionWithTitle:@\"Multistyle Transfer\" style: UIAlertActionStyleDefault handler:^(UIAlertAction * action) { _lbl.text = @\"Processing \"; _iv.image = [UIImage imageNamed: image_name]; dispatch_async(dispatch_get_global_queue(0, 0), ^{ UIImage *img = imageStyleTransfer(@\"stylize_quantized\"); dispatch_async(dispatch_get_main_queue(), ^{ _lbl.text = @\"Tap Anywhere\"; _iv.image = img; }); }); }]; 3. Заменит е значения входного и выходного слоев input_layer и output_ layer правильными значениями, соответствующими новой модели, и до- бавьте новое имя входного узла style_num (эти значения взяты из приме- ра программного кода для Android в файле stylizeActivity.java ; однако, для того чтобы узнать их имена, вы также можете воспользоваться ин-струментом резюмирования графа summarize_graph , локальным сервером TensorBoard или фрагментом программного кода, который мы уже по-казывали в предыдущих главах): std:: string input_layer = \"input\";std:: string style_layer = \"style_num\";std:: string output_layer = \"transformer/expand/conv3/conv/Sigmoid\"; 4. В отличие от моделей быстрого переноса стиля, многостилевая мо- дель здесь ожидает в качестве входного изображения 4-мерный тензор вещест венных чисел: tensorflow:: Tensor image_tensor(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({1, wanted_height, wanted_width, wanted_channels}));auto image_tensor_mapped = image_tensor.tensor<float, 4>(); 5. Мы также должны определить стилевой тензор style_tensor в качестве еще одного объекта типа Tensor с формой (NUM_STYLES*1) , где NUM_STYLES определяется в начале файла ViewController.mm как const int NUM_STYLES = 26;. Число 26 – это количество стилей, встроенных в файл модели styl‑ ize_quantized.pb , в рамках которых вы можете запускать приложение TF Stylize для Android и просматривать 26 результатов, как показано\n--- Страница 121 ---\nТрансформирование рисунков с помощью художественных стилей  119 на рис. 4.5. Обратите внимание, что 20-е изображение в левом нижнем углу – это уже знакомая нам «Звездная ночь» Ван Гога: Рисунок 4.5. 26 стилевых изображений в многостилевой модели tensorflow:: Tensor style_tensor(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({NUM_STYLES, 1}));auto style_tensor_mapped = style_tensor.tensor<float, 2>();float* out_style = style_tensor_mapped.data();for (int i = 0; i < NUM_STYLES; i++) { out_style[i] = 0.0 / NUM_STYLES;}out_style[19] = 1.0; Сумма всех значений в массиве out_style должна равняться 1, а окон- чательное изображение с перенесенным стилем будет представлять со-бой смесь стилей с учетом значений, указанных в массиве out_style . На- пример, в приведенном выше программном коде будет использоваться только стиль «Звездная ночь» (индекс массива 19 соответствует 20-му изображению в списке стилевых изображений на рис. 4.5).Если требуется равное сочетание изображения звездной ночи и изобра-жения в правом верхнем углу, то вам нужно заменить последнюю строку в предыдущем блоке кода на следующую: out_style[4] = 0.5;out_style[19] = 0.5;\n--- Страница 122 ---\n120  Трансформирование рисунков с помощью художественных стилей Если вы хотите равное сочетание всех 26 стилей, измените показанный выше цикл for на нижеследующий и не назначайте других значений лю-бому элементу out_style : for (int i = 0; i < NUM_STYLES; i++) { out_style[i] = 1.0 / NUM_STYLES;} Позже на рис. 4.8 и 4.9 вы увидите эффекты переноса стилей для всех трех этих настроек. 6. Изменит е вызов session ‑>Run на следующую далее строку, чтобы отпра- вить в модель тензор изображения и тензор стиля: tensorflow:: Status run_status = session ‑>Run({{input_layer, image_tensor}, {style_layer, style_tensor}}, {output_layer}, {}, &outputs); Вот и все изменения, которые необходимо внести для запуска приложения в iOS с использованием многостилевой модели. Теперь запустите приложение, и вы увидите что-то вроде того, что изображено на рис. 4.6. Рисунок 4.6. Вывод на экран исходного изображения\n--- Страница 123 ---\nТрансформирование рисунков с помощью художественных стилей  121 Прикоснитесь в любом месте, и вы увидите два варианта переноса стиля, как показано на рис. 4.7. Рисунок 4.7. Вывод на экран выбора из двух типов моделей Результаты двух изображений с перенесенными стилями, где out_style[19] = 1.0;, показаны на рис. 4.8.\n--- Страница 124 ---\n122  Трансформирование рисунков с помощью художественных стилей Рисунок 4.8. Результаты переноса стиля двумя разными моделями (быстрый перенос стиля – слева, многостилевой – справа) Результаты использования одинакового сочетания изображения «Звездной ночи» и верхнего правого изображения на рис. 4.5, а также равного сочетания всех 26 стилей показаны на рис. 4.9.\n--- Страница 125 ---\nТрансформирование рисунков с помощью художественных стилей  123 Рисунок 4.9. Результаты различных смешений разных стилей (наполовину «Звездная ночь», наполовину другой стиль – слева, смесь всех 26 стилей – справа) На iPhone 6 многостилевая модель работает около 5 секунд, примерно в 2–3 раза быстрее модели быстрого переноса стиля. использование многостилевой моДели Tensor Flow MaGenT a в android Хотя пример приложения TensorFlow для Android уже содержит программный код, в котором используется многостилевая модель (на самом деле в нашем приложении для iOS предыдущего раздела мы использовали модель из приме-ра приложения для Android), TensorFlow-ориентированный программный код в примере приложения перемешан с большим объемом программного кода, связанного с пользовательским интерфейсом, и все это находится в файле stylizeActivity.java , состоящем из более чем 600 строк. Вы также можете по- смотреть программный код TensorFlow переноса стиля, предназначенный для Android на Codelab (https: //codelabs.developers.google.com/codelabs/tensorflow- style-transfer-android/index.html), правда, программный код там примерно та-\n--- Страница 126 ---\n124  Трансформирование рисунков с помощью художественных стилей кой же, как и пример приложения TensorFlow для Android. Поскольку у нас уже есть минималистская реализация приложения для Android с использованием модели TensorFlow быстрого переноса стиля, было бы интересно посмотреть, каким образом можно было бы изменить всего несколько строк кода, что-бы получить мощное приложение по многостилевому переносу стиля. Такой способ также должен оказаться интуитивно понятнее и поможет разобраться в том, как добавлять замечательную модель TensorFlow в существующее при-ложение для Android. Итак, вот что требуется, для того чтобы применить многостилевую модель переноса в приложение для Android, которое мы создали ранее. 1. Перетащит е файл stylize_quantized.pb из папки tensorflow/examples/an‑ droid/assets в папку ресурсов assets нашего приложения для Android. 2. В среде разработки Android Studio откройте файл MainActivity.java , най- дите следующие ниже три строки кода: private static final String MODEL_FILE = \"file:///android_asset/fst_frozen_quantized.pb\";private static final String INPUT_NODE = \"img_placeholder\";private static final String OUTPUT_NODE = \"preds\"; А затем замените их следующими ниже четырьмя строками: private static final int NUM_STYLES = 26;private static final String MODEL_FILE = \"file:///android_asset/stylize_quantized.pb\";private static final String INPUT_NODE = \"input\";private static final String OUTPUT_NODE = \"transformer/expand/conv3/conv/Sigmoid\"; Здесь значения такие же, что и в приложении для iOS, которое мы созда-ли в предыдущем разделе. Если вы занимаетесь разработкой приложе-ний только для Android и пропустили предыдущий раздел, посвященный iOS, то просто прочитайте краткое объяснение шага 3 в предыдущем раз-деле для iOS. 3. Заменит е следующий далее фрагмент программного кода, который по- дает входное изображение в модель быстрого переноса стиля и обраба-тывает выходное изображение: mInferenceInterface.feed(INPUT_NODE, floatValues, WANTED_HEIGHT, WANTED_WIDTH, 3);mInferenceInterface.run(new String[] {OUTPUT_NODE}, false);mInferenceInterface.fetch(OUTPUT_NODE, outputValues);for (int i = 0; i < intValues.length; ++i) { intValues[i] = 0xFF000000 | (((int) outputValues[i * 3]) << 16) | (((int) outputValues[i * 3 + 1]) << 8) | ((int) outputValues[i * 3 + 2]); } на фрагмент кода, который сначала устанавливает массив styleVals (если вы запутались и не знаете, что из себя представляет массив styleVals\n--- Страница 127 ---\nТрансформирование рисунков с помощью художественных стилей  125 и как могут быть установлены значения в данном массиве, обратитесь к примечаниям в шаге 5 предыдущего раздела): final float[] styleVals = new float[NUM_STYLES];for (int i = 0; i < NUM_STYLES; ++i) { styleVals[i] = 0.0f / NUM_STYLES; }styleVals[19] = 0.5f;styleVals[4] = 0.5f; Затем указанный программный код передает в модель тензор входного изображения и тензор со значением стиля и запускает модель для из-влечения изображения с перенесенным стилем: mInferenceInterface.feed(INPUT_NODE, floatValues, 1, WANTED_HEIGHT, WANTED_WIDTH, 3);mInferenceInterface.feed(\"style_num\", styleVals, NUM_STYLES);mInferenceInterface.run(new String[] {OUTPUT_NODE}, false);mInferenceInterface.fetch(OUTPUT_NODE, outputValues); Наконец, он обрабатывает данные на выходе: for (int i=0; i < intValues.length; ++i) { intValues[i] = 0xFF000000 | (((int) (outputValues[i*3] * 255)) << 16) | (((int) (outputValues[i*3+1] * 255)) << 8) | ((int) (outputValues[i*3+2] * 255)); } Обратите внимание, что многостилевая модель возвращает в массив outputValues массив вещественных чисел, каждое из которых находится в диа- пазоне от 0.0 до 1.0, поэтому перед применением операции поразрядного сдвига влево мы должны умножить каждое из них на 255, в результате чего получим значения красного и зеленого цветов, а затем мы должны применить поразрядное «ИЛИ», чтобы назначить каждому элементу массива intValues окончательное значение в формате ARGB. Вот и все, что нужно для того, чтобы добавить многостилевую модель в авто- номное приложение для Android. Теперь давайте запустим данное приложение и поиграем с другим тестовым изображением, но теми же тремя комбинация-ми значений стиля, что и в приложении для iOS. При условии что здесь, как и во фрагменте программного кода на шаге 3, стилевые изображения 20 и 5 одинаково перемешаны, исходное изображе-ние и изображение с перенесенным стилем будут иметь вид, как показано на рис. 4.10.\n--- Страница 128 ---\n126  Трансформирование рисунков с помощью художественных стилей Рисунок 4.10. Исходное контентное изображение и изображение с перенесенным стилем с сочетанием 5-го изображения и 20-го изображения «Звездной ночи». Если заменить следующие ниже две строки программного кода: styleVals[19] = 0.5f; styleVals[4] = 0.5f; на одну строку кода styleVals[19] = 1.5f; либо заменить следующий ниже фрагмент программного кода: for (int i = 0; i < NUM_STYLES; ++i) { styleVals[i] = 0.0f / NUM_STYLES; }styleVals[19] = 0.5f;styleVals[4] = 0.5f; на фрагмент программного кода: for (int i = 0; i < NUM_STYLES; ++i) { styleVals[i] = 1.0f / NUM_STYLES; } то вы увидите эффекты, которые показаны на рис. 4.11.\n--- Страница 129 ---\nТрансформирование рисунков с помощью художественных стилей  127 Рисунок 4.11. Изображения, стилизованные только одним стилем «Звездной ночи» Ван Гога и всеми 26 стилями, перемешанными в равной степени Похоже, что мы, разработчики мобильных приложений, тоже можем быть отличными художниками, при условии что нам помогает несколько мощных моделей TensorFlow и знания о том, как их использовать в мобильных прило-жениях, как было описано здесь. резюме В этой главе мы сначала дали краткий обзор различных методов нейронного переноса стиля, разработанных начиная с 2015 года. Затем мы показали, как натренировать модель переноса стиля второго поколения, которая достаточно быстро работает на мобильных устройствах, в течение всего нескольких се-кунд. После этого мы обсудили вопрос использования такой модели в прило-жении для iOS и приложении для Android, созданных с нуля с минималистским подходом и состоящих из менее чем 100 строк программного кода. Наконец, мы поговорили о том, как использовать многостилевую модель TensorFlow Magenta нейронного переноса стиля в приложениях для iOS и для Android, ко-\n--- Страница 130 ---\n128  Трансформирование рисунков с помощью художественных стилей торая включает в себя 26 удивительных художественных стилей в одной не- большой модели. В следующей главе мы рассмотрим еще одну задачу, которая считается ин- теллектуальной, когда она проявляется нами, людьми, или нашими самыми лучшими друзьями: способность распознавать голосовые команды. Кто не хо-тел бы, чтобы наши собаки понимали такие команды, как «сидеть», «рядом», «нет», или чтобы наши милые детки реагировали на «да», «стоп» или «иди»? Давайте посмотрим, как создаются мобильные приложения, которые ведут себя аналогичным образом.",
      "debug": {
        "start_page": 103,
        "end_page": 130
      }
    },
    {
      "name": "Глава 5. Понимание простых речевых команд 129",
      "content": "--- Страница 131 --- (продолжение)\nГлава 5 Понимание простых речевых команд В наши дни голосовые службы, такие как Apple Siri, Amazon Alexa, Google Assistant и Google Translate, становятся все более популярными, так как в опре-деленных сценариях для нас голос – это самый естественный и эффективный способ отыскивать информацию или выполнять задачи. Многие из этих голо-совых служб расположены в облаке, потому что речь пользователя может быть довольно длинной и произвольной, а автоматическое распознавание речи (automatic speech recognition, ASR) представляет собой очень сложную задачу и требует большой вычислительной мощи. На самом деле автоматическое рас - познавание речи стало возможным в естественных и шумных условиях лишь в последние годы, и случилось это благодаря инновационному прорыву в глу - боком обучении. Однако в некоторых случаях имеет смысл распознавать простые речевые ко- манды в автономном режиме прямо на устройстве. Например, для того чтобы управлять движением робота с помощью компьютера Raspberry Pi, вам не нуж - ны сложные голосовые команды, и ASR прямо на устройстве не только работает быстрее, чем облачное решение, но оно также всегда доступно, даже в среде без доступа к сети. Распознавание простых речевых команд прямо на устройстве также может сэкономить пропускную способность сети, отправляя на сервер лишь сложную речь пользователя, и обрабатывая локально, когда произносит - ся определенная четкая команда пользователя. В этой главе мы сначала дадим обзор технологий ASR, охватывающих как со- временные глубоко обучающиеся системы, так и самые лучшие проекты с от - крытым исходным кодом. Затем мы обсудим вопрос тренировки и вторичной тренировки модели TensorFlow распознавания простых речевых команд, та-ких как «left» (влево), «right» (вправо), «up» (вверх), «down» (вниз), «stop» (стоп) и «go» (вперед). Далее мы воспользуемся натренированной моделью для созда-ния простого приложения для Android, а затем двух полноценных приложений для iOS, одно из которых будет реализовано на языке Objective-C, а другое – на языке Swift. В предыдущих двух главах мы не рассматривали приложение\nГлава 5 Понимание простых речевых команд В наши дни голосовые службы, такие как Apple Siri, Amazon Alexa, Google Assistant и Google Translate, становятся все более популярными, так как в опре-деленных сценариях для нас голос – это самый естественный и эффективный способ отыскивать информацию или выполнять задачи. Многие из этих голо-совых служб расположены в облаке, потому что речь пользователя может быть довольно длинной и произвольной, а автоматическое распознавание речи (automatic speech recognition, ASR) представляет собой очень сложную задачу и требует большой вычислительной мощи. На самом деле автоматическое рас - познавание речи стало возможным в естественных и шумных условиях лишь в последние годы, и случилось это благодаря инновационному прорыву в глу - боком обучении. Однако в некоторых случаях имеет смысл распознавать простые речевые ко- манды в автономном режиме прямо на устройстве. Например, для того чтобы управлять движением робота с помощью компьютера Raspberry Pi, вам не нуж - ны сложные голосовые команды, и ASR прямо на устройстве не только работает быстрее, чем облачное решение, но оно также всегда доступно, даже в среде без доступа к сети. Распознавание простых речевых команд прямо на устройстве также может сэкономить пропускную способность сети, отправляя на сервер лишь сложную речь пользователя, и обрабатывая локально, когда произносит - ся определенная четкая команда пользователя. В этой главе мы сначала дадим обзор технологий ASR, охватывающих как со- временные глубоко обучающиеся системы, так и самые лучшие проекты с от - крытым исходным кодом. Затем мы обсудим вопрос тренировки и вторичной тренировки модели TensorFlow распознавания простых речевых команд, та-ких как «left» (влево), «right» (вправо), «up» (вверх), «down» (вниз), «stop» (стоп) и «go» (вперед). Далее мы воспользуемся натренированной моделью для созда-ния простого приложения для Android, а затем двух полноценных приложений для iOS, одно из которых будет реализовано на языке Objective-C, а другое – на языке Swift. В предыдущих двух главах мы не рассматривали приложение\n--- Страница 132 ---\n130  Понимание прос тых речевых команд для iOS на языке Swift с использованием моделей TensorFlow, и эта глава яв- ляется самым подходящим местом для повторения пройденного и укрепле-ния нашего понимания способов построения приложений TensorFlow для iOS на языке Swift. Резюмируя, в этой главе мы рассмотрим следующие темы: распознавание ре чи – краткий обзор; тренировка прос той модели распознавания команд; испо льзование простой модели распознавания речи в Android; применение простой модели распознавания речи в iOS на языке Objective-C; испо льзование простой модели распознавания речи в iOS на языке Swift. распознавание речи – краткий обзор Первые практические независимые от говорящего системы распознавания речи с большой лексикой и непрерывным распознаванием речи появились в 1990-х годах. В начале 2000-х годов механизмы распознавания речи, пред-лагаемые ведущими стартапами Nuance и SpeechWorks, приводили в движе-ние многие сетевые голосовые службы первого поколения, такие как TellMe, AOL по телефону и BeVocal. Создававшиеся тогда системы распознавания речи были чаще всего основаны на традиционных скрытых марковских моделях (Hidden Markov Model, HMM) и требовали наличия вручную написанной грам-матики и тихой окружающей среды, помогающей механизму распознавания работать с более высокой точностью. Современные механизмы распознавания речи могут в значительной степе- ни понимать любые высказывания людей в шумной окружающей среде и ос - нованы на сквозном глубоком обучении, в особенности на еще одном типе глу - бокой нейронной сети, более подходящей для обработки естественного языка, так называемой рекуррентной нейронной сети (recurrent neural network, RNN). В отличие от традиционного распознавания речи на основе HMM, где для соз-дания и тонкой настройки разработанных вручную признаков, а также акусти-ческих и языковых моделей необходим человеческий опыт, сквозные системы распознавания речи на основе RNN-сети транскрибируют аудиовход непо-средственно в текст, без необходимости конвертировать аудиовход в фонети-ческое представление для дальнейшей обработки.  RNN-с еть позволяет обрабатывать последовательности на входе и/или выходе, потому что эта сеть по своей конструкции может запоминать предыдущие элементы во входной последовательности или генерировать последовательность на выходе. Это делает RNN-сеть более подходящей для распознавания речи (где входом является произносимая пользователями последовательность слов), аннотирования изображений (где выходом является предложение на естественном языке, состоящее из ряда слов), генерирования текста и предсказания временных рядов. Если вы незнакомы с RNN-сетью, то обязатель-но загляните в блог-пост Андрея Карпати «The Unreasonable Effectiveness of Recurrent\n--- Страница 133 ---\nПонимание простых речевых команд  131 Neural Networks» («Необоснованная эффективность рекуррентных нейронных сетей») (http://karpathy.github.io/2015/05/21/rnn-effectiveness). Позже в этой книге мы также подробно рассмотрим некоторые модели на основе RNN-сетей. Первая исследовательская работа по сквозному распознаванию речи с по- мощью RNN-сети была опубликована в 2014 году (http://proceedings.mlr.press/v32/graves14.pdf), где использовался слой коннекционной временной класси-фикации (Connectionist Temporal Classification, СТС). Позже в 2014 году компа-ния Baidu выпустила систему Deep Speech (https: //arxiv.org/abs/1412.5567), одну из первых коммерческих систем, построенных с использованием сквозной RNN-сети на основе CTC-слоя, но работающую с огромными наборами данных и достигающую значительно более низкой частоты ошибок в шумной среде, чем традиционные системы ASR. Если вам любопытно, то вы можете посмот - реть на реализацию механизма Deep Speech в TensorFlow (https: //github.com/ mozilla/DeepSpeech), но генерируемая ею модель требует слишком много ресур-сов для работы на мобильном телефоне. Эта проблема является характерной для подобных систем, основанных на CTC-слое. Во время развертывания она требует крупной языковой модели, чтобы исправлять сгенерированные тек - стовые ошибки, частично вызванные природой RNN-сети (если вам интересно, почитайте указанный выше блог-пост по RNN-сети, чтобы получить некоторое представление о том, почему это происходит). Новые системы распознавания речи в 2015 и 2016 годах использовали ана- логичные сквозные методы RNN-сетей, но заменили CTC-слой моделью, ос - нованной на внимании (https: //arxiv.org/pdf/1508.01211.pdf), вследствие чего отпала необходимость в большой языковой модели при запуске модели, что позволило развертывать их на мобильных устройствах с ограниченной па-мятью. В данной версии книги мы не будем исследовать такую возможность и рассматривать способы использования новейших передовых моделей ASR в мобильных приложениях. Вместо этого мы начнем с более простой модели распознавания речи, которая, по нашему опыту, будет хорошо работать в мо-бильных устройствах. Для того чтобы добавить возможность автономного распознавания речи в мобильные приложения, вы также можете использовать один из двух веду - щих проектов распознавания речи с открытым исходным кодом: CMU Sphinx (https: //cmusphinx.github.io), который был начат около 20 лет назад, но по-прежнему находится в стадии активной разработки. В слу - чае создания приложений для Android с применением распознавания речи вы можете использовать карманную версию PocketSphinx, создан-ную специально для Android (https: //github.com/cmusphinx/pocketsphinx- android). В случае создания приложения для iOS с применением рас - познавания речи вы можете воспользоваться платформой OpenEars (https://www.politepix.com/openears), бесплатным SDK, который исполь-зует CMU PocketSphinx для создания офлайнового распознавания речи и речевого воспроизведения текста (TTS) в приложениях для iOS;\n--- Страница 134 ---\n132  Понимание прос тых речевых команд Kaldi (https: //github.com/kaldi-asr/kaldi), который был начат в 2009 году и был весьма активным в последнее время, со 165 участниками по со- стоянию на январь 2018 года. Для того чтобы попробовать его на Android, вы можете обратиться к блог-посту: http://jcsilva.github.io/2017/03/18/ compile-kaldi-android. Что касается iOS, то следует ознакомиться с приве-денным в ссылке прототипом использования Kaldi в iOS: https: //github. com/keenresearch/keenasr-ios-poc. Поскольку данная книга посвящена применению платформы TensorFlow на мобильных устройствах и среди прочих интеллектуальных задач платфор-ма TensorFlow может использоваться для построения мощных моделей обра-ботки изображений, речи и текста, в остальной части главы мы сосредоточим-ся на способах применения платформы TensorFlow для тренировки простой модели распознавания речи и ее использовании в мобильных приложениях. тренировка простой моДели распознавания команД В этом разделе мы кратко изложим шаги, используемые в хорошо написан-ном руководстве TensorFlow по простому распознаванию звука (https://www.tensorflow.org/versions/master/tutorials/audio_recognition), а также добавим не-сколько советов, которые могут оказаться для вас полезными во время трени-ровки модели. Простая модель распознавания речевых команд, которую мы построим, сможет распознавать 10 слов: «yes», «no», «up», «down», «left», «right», «on» (включить), «off» (выключить), «stop» и «go». Она также сможет обнаруживать тишину. Если она не найдет тишины и ни одного из этих 10 слов, то сгенери-рует «unknown» (неизвестно). Набор речевых команд, который мы позднее за-грузим при запуске сценария tensorflow/example/speech_commands/train.py и бу- дем использовать для тренировки модели, на самом деле содержит еще 20 слов в дополнение к этим 10 словам: «zero» (ноль), «two» (два), «three» (три), …, «ten» (десять) (все эти 20 слов называются корневыми словами) плюс 10 вспомога-тельных слов: «bed» (кровать), «bird» (птица), «cat» (кот), «dog» (собака), «happy» (счастливый), «house» (дом), «marvin» (Марвин), «sheila» (Шейла), «tree» (дере-во) и «wow» (вау). Корневые слова просто содержатся в большем количестве за-писанных файлов .wav (около 2350), чем вспомогательные слова (около 1750).  Набор речевых команд взят из веб-сайта записи речи Open Speech Recording (https: //aiyprojects.withgoogle.com/open_speech_recording). Вы должны его попробовать и, возможно, добавить несколько минут своих собственных записей, тем самым внеся свою лепту в улучшение его работы, а также получить представление о том, как со-брать свой собственный набор речевых команд, если это необходимо. Существует также конкурс Kaggle (https://www.kaggle.com/c/tensorflow-speech-recognition-challenge), по-священный применению набора данных для построения модели, и там вы можете по-черпнуть еще больше информации о речевых моделях и получить советы.\n--- Страница 135 ---\nПонимание простых речевых команд  133 Модель, которая будет натренирована и использована в мобильных прило- жениях, основана на «бумажных» сверточных нейронных сетях (paper CNN), использующихся для определения ключевых слов малого размера (http: //www. isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf), а это сильно от - личается от большинства других крупномасштабных моделей распознавания речи на основе RNN-сетей. То, что модели на основе CNN-сети применимы и даже осуществимы для распознавания речи, представляет интерес, посколь-ку в случае распознавания простых речевых команд мы можем конвертировать звуковой сигнал размером с короткое временное окно в изображение или, точ-нее, спектрограмму, то есть частотные распределения звукового сигнала в те-чение этого временного окна (см. ссылку на руководство TensorFlow в начале этого раздела относительно примера изображения спектрограммы, созданной с помощью сценария wav_to_spectrogram ). Другими словами, мы можем транс - формировать звуковой сигнал из его исходного представления во временной области в представление в частотной области. Самым лучшим алгоритмом, ко-торый выполняет эту трансформацию, является дискретное преобразование Фурье (ДПФ, DFT), а быстрое преобразование Фурье (БПФ, FFT) – это просто эффективный алгоритм реализации DFT.  Бу дучи разработчиком мобильных приложений, вам, вероятно, не потребуется разби- раться в DFT и FFT. Но зато вы лучше оцените то, как происходит тренировка модели при использовании в мобильных приложениях, зная, что за кулисами тренировки прос - той модели TensorFlow распознавания речевых команд, которую мы собираемся рас - смотреть, среди прочих, безусловно, лежит применение алгоритма FFT, одного из 10 самых лучших алгоритмов XX века, который делает возможным тренировку модели распознавания речевых команд на основе CNN-сети. В качестве занимательного и ин-туитивно понятного руководства по DFT вы можете почитать приведенную по ссыл-ке статью: http://practicalcryptography.com/miscellaneous/machine-learning/intuitive- guide-discrete-fourier-transform. Теперь давайте выполним следующие далее шаги, для того чтобы натрени- ровать простую модель распознавания речевых команд. 1. В терминале с помощью команды cd перейдите в ваш корневой каталог исходного кода TensorFlow. Скорее всего, это будет ~/tensorflow‑1.4.0 . 2. Выпо лните следующую ниже команду, чтобы скачать набор речевых ко- манд, о котором мы говорили ранее: python tensorflow/examples/speech_commands/train.py При этом вы можете применить целый ряд параметров: параметр ‑‑ wanted_words по умолчанию равен 10 корневым словам, начиная со слова «yes»; с помощью этого параметра вы можете добавить до-полнительные слова, которые могут быть распознаны вашей моделью. Для тренировки набора своих голосовых команд используйте параметр ‑‑data_uri . Для отключения скачивания набора голосовых команд и до- ступа к своему собственному набору используется параметр ‑‑data_\n--- Страница 136 ---\n134  Понимание прос тых речевых команд dir=<путь_к_своему_набору> , где каждая команда должна быть именована как отдельная папка, которая должна содержать 1000–2000 аудиокли- пов длительностью до 1 секунды; если все аудиоклипы дольше, то соот - ветственно вы можете изменить значение параметра ‑‑clip_ duration_ms . За более подробной информацией обратитесь к исходному коду сце-нария train.py и руководству TensorFlow по простому распознаванию звука. 3. Если в сценарии train.py вы принимаете все параметры по умолчанию, то после скачивания набора речевых команд объемом 1,48 Гб вся тре-нировка, состоящая из 18 000 шагов, в Ubuntu с поддержкой GTX -1070 GPU займет около 90 минут. После завершения тренировки в папке /tmp/speech_commands_train вы увидите список файлов контрольных то- чек, а также файл определения графа conv.pbtxt и файл меток с именем conv_labels.txt , который содержит список команд (тот же, что и в пара- метре ‑‑wanted_words по умолчанию, или плюс два дополнительных слова «_silence» (тишина) и «_unknown» (неизвестно) в начале файла): -rw-rw-r-- 1 jeff jeff 75437 Dec 9 21:08 conv.ckpt -18000.meta -rw-rw-r-- 1 jeff jeff 433 Dec 9 21:08 checkpoint -rw-rw-r-- 1 jeff jeff 3707448 Dec 9 21:08 conv.ckpt -18000.data -00000-of -00001 -rw-rw-r-- 1 jeff jeff 315 Dec 9 21:08 conv.ckpt -18000.index -rw-rw-r-- 1 jeff jeff 75437 Dec 9 21:08 conv.ckpt -17900.meta -rw-rw-r-- 1 jeff jeff 3707448 Dec 9 21:08 conv.ckpt -17900.data -00000-of -00001 -rw-rw-r-- 1 jeff jeff 315 Dec 9 21:08 conv.ckpt -17900.index -rw-rw-r-- 1 jeff jeff 75437 Dec 9 21:07 conv.ckpt -17800.meta -rw-rw-r-- 1 jeff jeff 3707448 Dec 9 21:07 conv.ckpt -17800.data -00000-of -00001 -rw-rw-r-- 1 jeff jeff 315 Dec 9 21:07 conv.ckpt -17800.index -rw-rw-r-- 1 jeff jeff 75437 Dec 9 21:07 conv.ckpt -17700.meta -rw-rw-r-- 1 jeff jeff 3707448 Dec 9 21:07 conv.ckpt -17700.data -00000-of -00001 -rw-rw-r-- 1 jeff jeff 315 Dec 9 21:07 conv.ckpt -17700.index -rw-rw-r-- 1 jeff jeff 75437 Dec 9 21:06 conv.ckpt -17600.meta -rw-rw-r-- 1 jeff jeff 3707448 Dec 9 21:06 conv.ckpt -17600.data -00000-of -00001 -rw-rw-r-- 1 jeff jeff 315 Dec 9 21:06 conv.ckpt -17600.index -rw-rw-r-- 1 jeff jeff 60 Dec 9 19:41 conv_labels.txt -rw-rw-r-- 1 jeff jeff 121649 Dec 9 19:41 conv.pbtxt Файл conv_labels.txt содержит следующие ниже команды: _silence__unknown_yesnoupdownleftrightonoff\n--- Страница 137 ---\nПонимание простых речевых команд  135 stop go Теперь выполните показанную далее команду, чтобы объединить файл определения графа и файл контрольной точки в один файл модели, ко-торую можно использовать в мобильных приложениях: python tensorflow/examples/speech_commands/freeze.py \\--start_checkpoint=/tmp/speech_commands_train/conv.ckpt -18000 \\ --output_file=/tmp/speech_commands_graph.pb 4. При необходимости перед развертыванием файла модели speech_ commands_graph.pb в мобильных приложениях вы можете оперативно его протестировать с помощью следующей ниже команды: python tensorflow/examples/speech_commands/label_wav.py \\--graph=/tmp/speech_commands_graph.pb \\--labels=/tmp/speech_commands_train/conv_labels.txt \\--wav=/tmp/speech_dataset/go/9d171fee_nohash_1.wav Вы увидите такой результат: go (score = 0.48427)no (score = 0.17657)_unknown_ (score = 0.08560) 5. Применит е инструмент резюмирования графа summarize_graph , чтобы узнать имена входных и выходных узлов: bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=/tmp/speech_commands_graph.pb Результат должен быть следующим: Found 1 possible inputs: (name=wav_data, type=string(7), shape=[])No variables spotted.Found 1 possible outputs: (name=labels_softmax, op=Softmax) К сожалению, он будет правильным только для имени выхода и не по- казывает возможных входов. Не помогает и запуск на выполнение сервера TensorBoard: tensorboard ‑‑logdir /tmp/retrain_logs и открытие браузера с URL- адресом http://localhost:6006 для взаимодействия с графом. Но наш неболь- шой фрагмент программного кода, показанный в предыдущих главах, который служит для выяснения имен входов и выходов, нам поможет – ниж е показано взаимодействие с IPython: In [1]: import tensorflow as tfIn [2]: g=tf.GraphDef()In [3]: g.ParseFromString(open(\"/tmp/speech_commands_graph.pb\",\"rb\").read())In [4]: x=[n.name for n in g.node]In [5]: xOut[5]:\n--- Страница 138 ---\n136  Понимание прос тых речевых команд [u'wav_data', u'decoded_sample_data', u'AudioSpectrogram', u'MatMul', u'add_2', u'labels_softmax'] Итак, мы видим, что возможными входами являются wav_data и decoded_ sample_data . Нам пришлось бы углубиться в программный код тренировки мо- дели, чтобы выяснить, какие имена входов мы должны использовать, если бы мы не увидели комментария в файле freeze.py : «Результирующий граф име- ет вход для данных в формате WAV с именем wav_data , вход для необработан- ных данных PCM (в виде вещественных в диапазоне от –1.0 до 1.0) с именем decoded_ sample_data и выход с именем labels_softmax \". На самом деле если го- ворить об этой модели, то приложение TF Speech, часть которого мы видели в главе 1 «Начало работы с платформой TensorFlow Mobile», является хорошим примером приложения TensorFlow для Android, в котором специально опреде-ляются эти имена входов и выходов. В некоторых главах далее в этой книге вы увидите, как заглянуть в исходный код тренировки модели, когда необходимо узнать важные имена входных и выходных узлов, с помощью или без помощи наших трех методов. Будем надеяться, что к тому времени, когда вы прочте-те эту книгу, инструмент резюмирования графа summarize_graph будет в доста- точной мере усовершенствован и способен предоставлять нам точные имена входных и выходных узлов. Теперь самое время, для того чтобы применить нашу новую модель в мо- бильных приложениях. использование простой моДели распознавания речи в android Пример приложения TensorFlow распознавания простых речевых команд для Android, расположенный в папке tensorflow/example/android , имеет программ- ный код, который делает аудиозапись и выполняет распознавание. Этот про-граммный код находится в файле SpeechActivity.java . Он исходит из того, что приложение должно быть всегда готово к новым звуковым командам. Хотя в некоторых случаях это, безусловно, имеет смысл, такой подход также приво-дит к более сложному программному коду, по сравнению с программным ко-дом, который будет делать запись и распознавание только после того, как поль-зователь касается кнопки. Именно так работает Siri компании Apple. В этом разделе мы покажем вам, как создавать приложение для Android и добавлять минимально возможный программный код для записи речевых команд поль-зователей и вывода результатов распознавания. Все это поможет вам легче ин-тегрировать модель в свое собственное приложение для Android. Но если вам\n--- Страница 139 ---\nПонимание простых речевых команд  137 нужно охватывать случаи, когда речевые команды всегда должны записывать- ся и распознаваться автоматически, то вы должны обратиться к упомянутому выше примеру приложения TensorFlow для Android. созДание нового приложения с использованием моДели Выполните следующие далее действия, для того чтобы создать полноценное новое приложение для Android, использующее модель speech_commands_graph.pb , которую мы построили в предыдущем разделе. 1. Создайте новое приложение для Android с именем AudioRecognition , при- няв все значения по умолчанию, как в предыдущих главах, а затем до-бавьте строку compile 'org.tensorflow: tensorflow ‑android:+' в конце сек - ции зависимостей файла build.gradle приложения. 2. Добавьте элемент <uses-permission android: name=»android.permission. RECORD_AUDIO» /> в файл AndroidManifest.xml, чтобы разрешить при-ложению вести запись звука. 3. Создайте новую папку ресурсов assets , затем перетащите в папку assets файлы speech_commands_graph.pb и conv_actions_labels.txt , созданные в шагах 2 и 3 в предыдущем разделе. 4. Включите в файл activity_main.xml три элемента пользовательского ин- терфейса. Первый элемент управления TextView предназначен для ото-бражения результата распознавания: <TextView android: id=\"@+id/textview\" android: layout_width=\"wrap_content\" android: layout_height=\"wrap_content\" android: text=\"\" android: textSize=\"24sp\" android: textStyle=\"bold\" app: layout_constraintBottom_toBottomOf=\"parent\" app: layout_constraintLeft_toLeftOf=\"parent\" app: layout_constraintRight_toRightOf=\"parent\" app: layout_constraintTop_toTopOf=\"parent\" /> Второй элемент управления TextView должен показывать 10 определен- ных по умолчанию команд, которые мы натренировали с помощью сце-нария Python train.py в шаге 2 предыдущего раздела: <TextView android: layout_width=\"wrap_content\" android: layout_height=\"wrap_content\" android: text=\"yes no up down left right on off stop go\" app: layout_constraintBottom_toBottomOf=\"parent\" app: layout_constraintHorizontal_bias=\"0.50\" app: layout_constraintLeft_toLeftOf=\"parent\" app: layout_constraintRight_toRightOf=\"parent\"\n--- Страница 140 ---\n138  Понимание прос тых речевых команд app: layout_constraintTop_toTopOf=\"parent\" app: layout_constraintVertical_bias=\"0.25\" /> Последний элемент управления пользовательским интерфейсом – это кнопка Button , которая при касании начинает запись звука в течение одной секунды, а затем отправляет запись в нашу модель для распо - знавания: <Button android: id=\"@+id/button\" android: layout_width=\"wrap_content\" android: layout_height=\"wrap_content\" android: text=\"Start\" app: layout_constraintBottom_toBottomOf=\"parent\" app: layout_constraintHorizontal_bias=\"0.50\" app: layout_constraintLeft_toLeftOf=\"parent\" app: layout_constraintRight_toRightOf=\"parent\" app: layout_constraintTop_toTopOf=\"parent\" app: layout_constraintVertical_bias=\"0.8\" /> 5. Откройт е файл MainActivity.java , сначала создайте определение класса MainActivity implements Runnable . Затем добавьте следующие ниже кон- станты, устанавливающие имя модели, имя метки, имена входа и вы- хода: private static final String MODEL_FILENAME = \"file:///android_asset/speech_commands_graph.pb\";private static final String LABEL_FILENAME = \"file:///android_asset/conv_actions_labels.txt\";private static final String INPUT_DATA_NAME = \"decoded_sample_data:0\";private static final String INPUT_SAMPLE_RATE_NAME = \"decoded_sample_data:1\";private static final String OUTPUT_NODE_NAME = \"labels_softmax\"; 6. Об ъявите четыре экземплярные переменные: private TensorFlowInferenceInterface mInferenceInterface;private List<String> mLabels = new ArrayList<String>();private Button mButton;private TextView mTextView; 7. В методе onCreate мы сначала создаем экземпляры для mButton и mTextView , затем настраиваем обработчик событий касания кнопки, который сна-чала изменяет заголовок кнопки, а потом запускает поток для записи и распознавания: mButton = findViewById(R.id.button);mTextView = findViewById(R.id.textview);mButton.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { mButton.setText(\"Listening \"); Thread thread = new Thread(MainActivity.this);\n--- Страница 141 ---\nПонимание простых речевых команд  139 thread.start(); } }); В конце метода onCreate мы читаем содержимое файла меток построчно и сохраняем каждую строку в списке массива mLabels . 8. В начале метода public void run() , запускаемого при касании кнопки «Start», добавьте программный код, который сначала получает мини- мальный размер буфера для создания объекта Android AudioRecord для осуществления аудиозаписи, а затем использует значение bufferSize , чтобы создать новый экземпляр объекта AudioRecord с частотой дискре- тизации (SAMPLE_RATE) 16 000 и форматом 16-бит/моно, то есть типом необработанного звука, который наша модель ожидает на входе, после чего данный метод наконец начинает аудиозапись из экземпляра объ-екта AudioRecord : int bufferSize = AudioRecord.getMinBufferSize(SAMPLE_RATE, AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT);AudioRecord record = new AudioRecord(MediaRecorder.AudioSource.DEFAULT, SAMPLE_RATE, AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT, bufferSize); if (record.getState()!= AudioRecord.STATE_INITIALIZED) return; record.startRecording();  В Android для записи звука имеется два класса: MediaRecorder и AudioRecord. Класс MediaRecorder проще в использовании, чем класс AudioRecord, но он сохраняет сжатые аудиофайлы согласно Android API до уровня 24 (Android 7.0), который поддерживает запись сырого звука. Согласно https://developer.android.com/about/dashboards/index. html, по состоянию на январь 2018 года на рынке существует более 70% устройств Android, которые по-прежнему работают в Android более ранних версий, чем версия 7.0. Вы, вероятно, предпочли бы не нацеливать свое приложение на Android 7.0 или выше. Кроме того чтобы декодировать сжатый звук, записанный объектом класса MediaRecorder, вы должны использовать MediaCodec, который довольно сложен в ис - пользовании. Класс AudioRecord, хотя и предоставляет низкоуровневый API, на самом деле идеально подходит для записи сырых необработанных данных, которые затем от - правляются в модель распознавания речевых команд для обработки. 9. С оздайте два массива из 16-разрядных коротких (с типом short) целых чисел audioBuffer и recordingBuffer . В случае 1-секундной аудиозаписи каждый раз, после того как объект AudioRecord считывает и заполняет массив audioBuffer , фактически считываемые данные будут добавляться в массив recordingBuffer : long shortsRead = 0;int recordingOffset = 0;short[] audioBuffer = new short[bufferSize / 2];short[] recordingBuffer = new short[RECORDING_LENGTH];while (shortsRead < RECORDING_LENGTH) {// 1 second of recording\n--- Страница 142 ---\n140  Понимание прос тых речевых команд int numberOfShort = record.read(audioBuffer, 0, audioBuffer.length); shortsRead += numberOfShort; System.arraycopy(audioBuffer, 0, recordingBuffer, recordingOffset, numberOfShort); recordingOffset += numberOfShort; } record.stop();record.release(); 10. Пос ле завершения записи мы сначала меняем надпись на кнопке на Recognizing (Распознавание): runOnUiThread(new Runnable() { @Override public void run() { mButton.setText(\"Recognizing \"); } }); Затем конвертируем массив с типом short recordingBuffer в массив с ти- пом float, при этом укладывая каждый элемент массива float в диапазон от –1.0 и 1.0, так как наша модель ожидает вещественные числа между –1.0 и 1.0: float[] floatInputBuffer = new float[RECORDING_LENGTH];for (int i = 0; i < RECORDING_LENGTH; ++i) { floatInputBuffer[i] = recordingBuffer[i] / 32767.0f; } Создайте новый интерфейс TensorFlowInferenceInterface , как мы делали в предыдущих главах, затем вызовите его метод feed с именами и зна- чениями двух входных узлов, один из которых является частотой дис - кретизации, а другой – необрабо танными аудиоданными, хранящимися в массиве floatInputBuffer : AssetManager assetManager = getAssets();mInferenceInterface = new TensorFlowInferenceInterface(assetManager, MODEL_FILENAME); int[] sampleRate = new int[] {SAMPLE_RATE}; mInferenceInterface.feed(INPUT_SAMPLE_RATE_NAME, sampleRate); mInferenceInterface.feed(INPUT_DATA_NAME, floatInputBuffer, RECORDING_LENGTH, 1); После этого мы вызываем метод run для запуска механизма выведе- ния заключения на модели, а затем получаем выходные оценки до- стоверности для каждой из 10 речевых команд и элементы «unknown» и «silence»: String[] outputScoresNames = new String[] {OUTPUT_NODE_NAME};mInferenceInterface.run(outputScoresNames);\n--- Страница 143 ---\nПонимание простых речевых команд  141 float[] outputScores = new float[mLabels.size()]; mInferenceInterface.fetch(OUTPUT_NODE_NAME, outputScores); 12. Массив outputScores соответствует списку mLabels , поэтому мы можем легко найти самую лучшую оценку и получить имя ее команды: float max = outputScores[0];int idx = 0;for (int i=1; i<outputScores.length; i++) { if (outputScores[i] > max) { max = outputScores[i]; idx = i; } }final String result = mLabels.get(idx); Наконец, мы показываем результат в элементе управления Textview и ме- няем надпись на кнопке на «Start», предоставляя пользователям возмож - ность снова начать записывать звук и распознавать речевые команды: runOnUiThread(new Runnable() { @Override public void run() { mButton.setText(\"Start\"); mTextView.setText(result); } });\n--- Страница 144 ---\n142  Понимание прос тых речевых команд выво Д результатов распознавания Теперь запустите приложение в устройстве Android. Вы увидите начальный экран, как показано на рис. 5.1. Рисунок 5.1. Начальный экран после запуска приложения Нажмите на кнопку Start и начните произносить одну из 10 команд, по- казанных сверху. Вы увидите, что название кнопки изменится на Listening… (Прос лушивание), а потом на Recognizing… (Распознавание), как показано на рис. 5.2.\n--- Страница 145 ---\nПонимание простых речевых команд  143 Рисунок 5.2. Прослушивание для записи звука и распознавание записанного звука И в середине экрана почти в реальном времени будет выведен распознан- ный результат, как показано на рис. 5.3.\n--- Страница 146 ---\n144  Понимание прос тых речевых команд Рисунок 5.3. Распознанные речевые команды Весь процесс распознавания завершается практически мгновенно, а ис - пользуемая для распознавания модель speech_commands_graph.pb составляет все- го примерно 3.7 Мб. Конечно, она поддерживает только 10 речевых команд, но размер не изменится, даже если будут поддерживаться десятки команд при использовании параметра ‑‑wanted_words сценария train.py или вашего соб- ственного набора, как мы обсуждали в разделе, посвященном тренировке дан-ной модели. Надо признать, что показанные здесь скриншоты приложения не такие кра- сочные и привлекательные, как в предыдущей главе (картинка стоит тысячи слов), но распознавание речи, безусловно, может делать нечто, что художник не в состоянии, например выдавать голосовые команды для управления дви-жением робота. Полноценный исходный код приложения находится в папке Ch5/android ре- позитория книги на Github . Теперь давайте посмотрим, как, используя данную модель, создать приложение для iOS. Для того чтобы модель работала коррект - но, нам придется предпринять несколько сложных шагов, связанных со сбор-\n--- Страница 147 ---\nПонимание простых речевых команд  145 кой библиотеки TensorFlow для iOS и подготовкой аудиоданных для модели. Решением этих задач мы и займемся. использование простой моДели распознавания речи в ios на языке objec Tive-c Если вы пробовали работать с приложениями для iOS в предыдущих трех главах, то вы, вероятно, предпочитаете использовать библиотеку TensorFlow ручной сборки, чем экспериментальный модуль TensorFlow, поскольку с биб - лиотекой ручной сборки у вас больше контроля над тем, какие операции TensorFlow могут быть добавлены для успешной работы ваших моделей, и этот более широкий контроль также является одной из причин, почему мы решили сосредоточиться на платформе TensorFlow Mobile вместо TensorFlow Lite в гла-ве 1 «Начало работы с платформой TensorFlow Mobile». Поэтому, несмотря на то что вы, возможно, попробуете применить экспе- риментальный модуль TensorFlow, когда будете читать эту книгу, чтобы про-верить, был ли данный модуль обновлен для поддержки всех используемых в модели определений операций, в дальнейшем, начиная с этого места в книге, мы в наших приложениях для iOS будем всегда пользоваться библиотеками TensorFlow ручной сборки (см. шаги 1 и 2 раздела «Использование моделей об-наружения объектов в iOS» главы 3 «Обнаружение и локализация объектов» в от- ношении того, как это делается). созДание нового приложения с использованием моДели Теперь выполните следующие далее действия, чтобы создать новое приложе-ние для iOS с целью применения модели распознавания речевых команд. 1. Создайте в среде Xcode новое приложение на языке Objective-C под назва- нием AudioRecognition и настройте проект на использование биб лиотек TensorFlow ручной сборки, как описано в шаге 1 раздела «Использование моделей быстрого нейронного переноса стиля в iOS» главы 4 «Трансфор- мирование рисунков с помощью художественных стилей». Также добавьте библиотеки AudioToolbox.framework , AVFoundation.framework и Accelerate. framework в поле Link Binary With Libraries (Скомпоновать двоичный файл с библиотеками) раздела Target. 2. Перетащит е в проект файл модели speech_commands_graph.pb . 3. Изменит е расширение файла ViewController.m на mm, затем добавьте сле- дующие ниже заголовки, используемые при записи и обработке звука: #import <AVFoundation/AVAudioRecorder.h>#import <AVFoundation/AVAudioSettings.h>#import <AVFoundation/AVAudioSession.h>#import <AudioToolbox/AudioToolbox.h>\n--- Страница 148 ---\n146  Понимание прос тых речевых команд Также добавьте заголовки для TensorFlow: #include <fstream> #include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/public/session.h\" Теперь определите константу частоты дискретизации звука SAMPLE_RATE , C-указатель на массив вещественных чисел, содержащий аудиоданные, которые будут отправлены в модель, сигнатуру нашей ключевой функ - ции audioRecognition и два свойства, содержащих путь к записанному файлу и экземпляр AVAudioRecorder iOS. Мы также должны позволить кон- троллеру ViewController реализовать метод-делегат AudioRecorderDelegate , чтобы тот знал, когда запись закончена: const int SAMPLE_RATE = 16000;float *floatInputBuffer;std:: string audioRecognition(float* floatInputBuffer, int length);@interface ViewController () <AVAudioRecorderDelegate>@property (nonatomic, strong) NSString *recorderFilePath;@property (nonatomic, strong) AVAudioRecorder *recorder;@end Мы не будем здесь приводить фрагмент кода, который программно соз-дает два элемента управления пользовательским интерфейсом: кнопку, которая при касании начинает запись звука в течение 1 секунды, а затем отправляет звук в нашу модель для распознавания, и надпись, которая показывает результаты распознавания. Однако в следующем разделе мы все же покажем немного программного кода пользовательского интер-фейса на языке Swift, для того чтобы освежить память. 4. Внутри обработчика кнопки UIControlEventTouchUpInside мы сначала соз- даем экземпляр AVAudioSession , устанавливаем его категорию на запись и делаем его активным: AVAudioSession *audioSession = [AVAudioSession sharedInstance];NSError *err = nil;[audioSession setCategory: AVAudioSessionCategoryPlayAndRecord error:&err];if(err){ NSLog(@\"audioSession:%@\", [[err userInfo] description]); return; }[audioSession setActive: YES error:&err];if(err){ NSLog(@\"audioSession:%@\", [[err userInfo] description]); return; } Затем создаем словарь настроек записи:\n--- Страница 149 ---\nПонимание простых речевых команд  147 NSMutableDictionary *recordSetting = [[NSMutableDictionary alloc] init]; [recordSetting setValue:[NSNumber numberWithInt: kAudioFormatLinearPCM] forKey: AVFormatIDKey];[recordSetting setValue:[NSNumber numberWithFloat: SAMPLE_RATE] forKey: AVSampleRateKey];[recordSetting setValue:[NSNumber numberWithInt: 1] forKey: AVNumberOfChannelsKey];[recordSetting setValue:[NSNumber numberWithInt:16] forKey: AVLinearPCMBitDepthKey];[recordSetting setValue:[NSNumber numberWithBool: NO] forKey: AVLinearPCMIsBigEndianKey];[recordSetting setValue:[NSNumber numberWithBool: NO] forKey: AVLinearPCMIsFloatKey];[recordSetting setValue:[NSNumber numberWithInt: AVAudioQualityMax] forKey: AVEncoderAudioQualityKey]; Наконец, в обработчике касаний кнопки мы определяем, где сохранить записанный звук, создаем экземпляр AVAudioRecorder , устанавливаем его метод-делегат и начинаем запись в течение 1 секунды: self.recorderFilePath = [NSString stringWithFormat:@\"%@/recorded_file.wav\", [NSHomeDirectory() stringByAppendingPathComponent:@\"tmp\"]];NSURL *url = [NSURL fileURLWithPath:_recorderFilePath];err = nil;_recorder = [[AVAudioRecorder alloc] initWithURL: url settings: recordSetting error:&err];if(!_recorder){ NSLog(@\"recorder:%@\", [[err userInfo] description]); return; }[_recorder setDelegate: self];[_recorder prepareToRecord];[_recorder recordForDuration:1]; 5. В методе-делегате AVAudioRecorderDelegate , audioRecorderDidFinishRe‑ cording , мы используем расширенные службы аудиофайлов Apple, ко- торые предназначены для чтения и записи сжатых и линейных PCM-аудиофайлов, чтобы загрузить записанный звук, конвертировать его в формат, ожидаемый моделью, и прочитать аудиоданные в память. Мы не будем здесь показывать эту часть программного кода, которая в основном основана на блог-посте по ссылке: https: //batmobile.blogs. ilrt.org/loading-audio-file-on-an-iphone/. После этой обработки входной буфер floatInputBuffer указывает на необработанные выборки звука. Теперь мы можем передать данные в наш метод audioRecognition в ра- бочем потоке и показать результат в потоке пользовательского интер-фейса: dispatch_async(dispatch_get_global_queue(0, 0), ^{ std:: string command = audioRecognition(floatInputBuffer, totalRead); delete [] floatInputBuffer; dispatch_async(dispatch_get_main_queue(), ^{ NSString *cmd = [NSString stringWithCString: command.c_str() encoding:[NSString defaultCStringEncoding]]; [_lbl setText: cmd];\n--- Страница 150 ---\n148  Понимание прос тых речевых команд [_btn setTitle:@\"Start\" forState: UIControlStateNormal]; }); }); 6. Вну три метода audioRecognition мы сначала определяем массив строк C++ с 10 командами для распознавания, а также двумя специальными значениями, \"_silence_\" и \" _unknown_\" : std:: string commands[] = {\"_silence_\", \"_unknown_\", \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"}; После стандартной настройки объектов TensorFlow Session , Status и GraphDef , в соответствии с тем, как мы делали в приложениях для iOS в предыдущих главах, мы считываем файл модели и пытаемся с ним соз-дать сеанс Session TensorFlow: NSString* network_path = FilePathForResourceName(@\"speech_commands_graph\", @\"pb\"); PortableReadFileToProto([network_path UTF8String], &tensorflow_graph);tensorflow:: Status s = session ‑>Create(tensorflow_graph); if (!s.ok()) { LOG(ERROR) << \"Could not create TensorFlow Graph: \" << s; return \"\"; } Если сеанс успешно создан, мы определяем имена двух входных узлов и одного выходного узла для модели: std:: string input_name1 = \"decoded_sample_data:0\";std:: string input_name2 = \"decoded_sample_data:1\";std:: string output_name = \"labels_softmax\"; 7. Д ля имени \"decoded_sample_data: 0\" нам нужно отправить значение час - тоты дискретизации в виде скаляра (в противном случае вы получите ошибку при вызове метода run сеанса TensorFlow); тензор определяется в API C++ платформы TensorFlow следующим образом: tensorflow:: Tensor samplerate_tensor(tensorflow:: DT_INT32, tensorflow:: TensorShape());samplerate_tensor.scalar<int>()() = SAMPLE_RATE; Для имени \"decoded_sample_data:1\" аудиоданные в вещественных чис - лах должны быть конвертированы из массива floatInputBuffer в тензор audio_tensor , аналогично тому, как тензор image_tensor был определен в предыдущих главах: tensorflow:: Tensor audio_tensor(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({length, 1}));auto audio_tensor_mapped = audio_tensor.tensor<float, 2>();float* out = audio_tensor_mapped.data();for (int i = 0; i < length; i++) {\n--- Страница 151 ---\nПонимание простых речевых команд  149 out[i] = floatInputBuffer[i]; } Теперь мы можем запустить модель, как раньше, с входами и получить выход: std:: vector<tensorflow:: Tensor> outputScores;tensorflow:: Status run_status = session ‑>Run({{input_name1, audio_tensor}, {input_name2, samplerate_tensor}},{output_name}, {}, &outputScores);if (!run_status.ok()) { LOG(ERROR) << \"Running model failed: \" << run_status; return \"\"; } 8. Мы делаем простой разбор выходов модели outputScores и возвраща- ем самую лучшую оценку. OutputScores – э то вектор тензора платфор- мы TensorFlow, первый элемент которого содержит 12 значений оценок для 12 возможных результатов распознавания. Эти 12 значений оценок можно получить с помощью метода flat и определить максимальную оценку: tensorflow:: Tensor* output = &outputScores[0];const Eigen:: TensorMap<Eigen:: Tensor<float, 1, Eigen:: RowMajor>, Eigen:: Aligned>& prediction = output ‑>flat<float>(); const long count = prediction.size();int idx = 0;float max = prediction(0);for (int i = 1; i < count; i++) { const float value = prediction(i); printf(\"%d:%f\", i, value); if (value > max) { max = value; idx = i; } } return commands[idx]; Единственное, что нам осталось еще сделать, прежде чем приложение сможет записывать любой звук, – это создать новое свойство конфиденци- альности – описание использования микрофона в файле приложения Info. plist – и установить значение свойства в нечто вроде «слушать и распознавать голосовые команды». Теперь запустите приложение в эмуляторе iOS (если ваша среда разработ - ки Xcode старше версии 9.2 и версия симулятора iOS старше 10.0, то вам, воз- можно, придется выполнить приложение на вашем фактическом устройстве iOS, поскольку, скорее всего, вы не сможете записывать звук в симуляторе iOS версии ранее 10.0) или на iPhone. Сперва вы увидите начальный экран с кноп-\n--- Страница 152 ---\n150  Понимание прос тых речевых команд кой Start в центре. Затем коснитесь кнопки и произнесите одну из 10 команд. Результат распознавания должен появиться вверху, как показано на рис. 5.4. Рисунок 5.4. Начальный экран и результат распознавания Вообще-то, результат распознавания должен появиться, но на самом деле этого не произойдет, потому что в панели вывода Xcode вы получите сообще- ние об ошибке, говорящей о невозможности создать граф TensorFlow из-за не-зарегистрированного типа операции: Could not create TensorFlow Graph: Not found: Op type not registered 'DecodeWav' in binary running on XXX's ‑MacBook‑Pro.local. Make sure the Op and Kernel are registered in the binary running in this process. исправление ошибок загрузки моДели с помощью файла TF_oP_Files.TXT Мы уже видели такого рода злополучные ошибки в предыдущих главах, и если вы не знаете, что она означает на самом деле, то на ее поиск и исправление может уйти много времени и усилий. Операции TensorFlow состоят из двух частей: определения, которое называется «ops» (что немного сбивает с тол-\n--- Страница 153 ---\nПонимание простых речевых команд  151 ку, так как «op» может означать либо определение и реализацию, либо просто определение), расположенного в папке tensorflow/core/ops , и реализации, кото- рая называется ядрами kernels и расположена в папке tensorflow/core/kernels . В папке tensorflow/contrib/makefile имеется файл с именем tf_op_files.txt , ко- торый содержит список определений и реализаций операций ops. Этот спи-сок должен быть встроен в библиотеки TensorFlow для iOS, когда вы создаете библиотеку вручную. Файл tf_op_files.txt должен включать все файлы опре- делений операций, как описано в документации TensorFlow «Preparing models for mobile deployment» («Подготовка моделей для мобильного развертывания») (https://www.tensorflow.org/mobile/prepare_models), так как они занимают очень мало места. Однако на момент выхода версий TensorFlow 1.4 и 1.5 файл tf_op_ files.txt содержал не все определения операций. Поэтому, когда мы видим ошибку «Op type not registered» (Определение операции не зарегистрировано), нам нужно выяснить, какие файлы определений и реализаций операций отве-чают за эту операцию. В нашем случае тип операции называется Decodewav . Для того чтобы получить соответствующую информацию, мы можем выполнить следующие ниже две команды оболочки: $ grep 'REGISTER.*\"DecodeWav\"' tensorflow/core/ops/*.cctensorflow/core/ops/audio_ops.cc: REGISTER_OP(\"DecodeWav\") $ grep 'REGISTER.*\"DecodeWav\"' tensorflow/core/kernels/*.cc tensorflow/core/kernels/decode_wav_op.cc: REGISTER_KERNEL_BUILDER(Name(\"DecodeWav\").Device(DEVICE_CPU), DecodeWavOp); В файле tf_op_files.txt платформы TensorFlow 1.4 уже имеется строка с тек - стом tensorflow/core/kernels/decode_wav_op.cc , но, будьте уверены, у нас эта сро- ка tensorflow/core/ops/audio_ops.cc отсутствует. От нас требуется лишь добавить строку tensorflow/core/ops/audio_ops.cc в любое место файла tf_op_files.txt и выполнить сценарий оболочки tensorflow/contrib/makefile/build_all_ios.sh , как мы делали в главе 3 «Обнаружение и локализация объектов», чтобы заново собрать библиотеку TensorFlow для iOS. Затем снова запустите приложение для iOS и начните нажимать кнопку Start и произносить голосовые команды для распознавания или нераспознавания до тех пор, пока вам не надоест.  Описание процесса исправления ошибки «Тип операции не зарегистрирован» является самым главным выводом из этой главы, поскольку он может сэкономить вам уйму вре-мени, когда вы будете работать с другими моделями TensorFlow в будущем. Но прежде чем вы забежите вперед, чтобы увидеть, какая еще новая мо- дель искусственного интеллекта TensorFlow будет рассмотрена и использова-на в следующей главе, давайте уделим внимание другим разработчикам iOS, которые предпочитают использовать более новый и, по крайней мере, для них более крутой язык Swift.\n--- Страница 154 ---\n152  Понимание прос тых речевых команд использование простой моДели распознавания речи в ios на языке swiFT В главе 2 «Классифицирование изображений с помощью трансферного обучения» мы создали приложение для iOS на языке Swift, которое использует модуль TensorFlow. Давайте теперь создадим новое приложение на Swift, которое ис - пользует библиотеки TensorFlow для iOS, собранные нами вручную в преды-дущем разделе, и применим модель распознавания речевых команд в нашем приложении Swift. 1. В среде разработки Xcode создайте новый проект для iOS с одним экра- ном и настройте проект так же, как было показано в шагах 1 и 2 преды-дущего раздела, за исключением того, что теперь нужно выбрать язык программирования Swift. 2. В среде разработки Xcode выберите File (Файл) | New (Создать) | File… (Файл) и затем выберите файл Swift. Введите имя RunInference . Вы уви- дите окно сообщения с вопросом: «Вы хотите настроить связующий за-головок Objective-C?» Щелкните на Create Bridging Header (Создать свя- зующий заголовок). Переименуйте файл RunInference.m в RunInfence.mm , поскольку мы будем смешивать наш программный код на языках C, C++ и Objective-C, для того чтобы выполнить обработку и распознавание зву - ка после его записи. В приложении на языке Swift мы используем язык Objective-C, потому что для вызова кода C++ платформы TensorFlow из Swift у нас должен быть класс Objective-C в качестве обертки для кода на C++. 3. Создайте файл заголовка RunInference.h и добавьте в него следующий программный код: @interface RunInference_Wrapper: NSObject ‑ (NSString *)run_inference_wrapper:(NSString*)recorderFilePath; @end Ваше приложение в среде Xcode должно выглядеть, как на рис. 5.5.\n--- Страница 155 ---\nПонимание простых речевых команд  153 Рисунок 5.5. Проект приложения для iOS на языке Swift 4. Откройт е файл ViewController.swift . Добавьте следующий далее про- граммный код в верхнюю часть данного файла сразу после инструкции import UIKit : import AVFoundation let _lbl = UILabel() let _btn = UIButton(type:.system)var _recorderFilePath: String! Затем приведите вид контроллера ViewController в соответствие с тем, что показано ниже (фрагмент кода, который определяет ограничение NSLayoutConstraint для кнопки _btn и надписи _lbl и вызывает метод addConstraint , не показан): class ViewController: UIViewController, AVAudioRecorderDelegate { var audioRecorder: AVAudioRecorder! override func viewDidLoad() { super.viewDidLoad() _btn.translatesAutoresizingMaskIntoConstraints = false _btn.titleLabel?.font = UIFont.systemFont(ofSize:32) _btn.setTitle(\"Start\", for:.normal) self.view.addSubview(_btn) _btn.addTarget(self, action:#selector(btnTapped), for:.touchUpInside) _lbl.translatesAutoresizingMaskIntoConstraints = false self.view.addSubview(_lbl) 5. Д обавьте обработчик касания кнопки и внутри него сначала запросите разрешение у пользователя на запись: @objc func btnTapped() { _lbl.text = \" \"\n--- Страница 156 ---\n154  Понимание прос тых речевых команд _btn.setTitle(\"Listening \", for:.normal) AVAudioSession.sharedInstance().requestRecordPermission () { [unowned self] allowed in if allowed { print(\"mic allowed\") } else { print(\"denied by user\") return } } Затем создайте экземпляр AudioSession и установите для него катего- рию record (запись) и статус active (активный), как мы делали в версии на языке Objective-C: let audioSession = AVAudioSession.sharedInstance()do { try audioSession.setCategory(AVAudioSessionCategoryRecord) try audioSession.setActive(true) } catch { print(\"recording exception\") return } Теперь определите параметры для использования экземпляра AVAudioRecorder : let settings = [ AVFormatIDKey: Int(kAudioFormatLinearPCM), AVSampleRateKey: 16000, AVNumberOfChannelsKey: 1, AVLinearPCMBitDepthKey: 16, AVLinearPCMIsBigEndianKey: false, AVLinearPCMIsFloatKey: false, AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue ] as [String: Any] Задайте путь к файлу для сохранения записанного звука, создайте эк - земпляр AVAudioRecorder , установите его метод-делегат и начните запись в течение 1 секунды: do { _recorderFilePath = NSHomeDirectory().stringByAppendingPathComponent(path: \"tmp\").stringByAppendingPathComponent(path: \"recorded_file.wav\") audioRecorder = try AVAudioRecorder(url: NSURL.fileURL(withPath: _recorderFilePath), settings: settings) audioRecorder.delegate = self audioRecorder.record(forDuration: 1) } catch let error { print(\"error:\" + error.localizedDescription) }\n--- Страница 157 ---\nПонимание простых речевых команд  155 6. В конце файла ViewController.swift добавьте метод-делегат AVAudioRecorderDelegate , audioRecorderDidFinishRecording , с приведенной ниже реализацией, которая в основном вызывает обертку run_inference_ wrapper для постобработки и распознавания звука: func audioRecorderDidFinishRecording(_ recorder: AVAudioRecorder, successfully flag: Bool) { _btn.setTitle(\"Recognizing \", for:.normal) if flag { let result = RunInference_Wrapper().run_inference_wrapper(_recorderFilePath) _lbl.text = result } else { _lbl.text = \"Recording error\" } _btn.setTitle(\"Start\", for:.normal) } В файл AudioRecognition_Swift ‑Bridging ‑Header.h добавьте директиву #include \"RunInference.h , для того чтобы заработал предыдущий про- граммный код Swift, RunInference_Wrapper().run_inference_wrapper(_ recorderFilePath) . 7. В файле RunInference.mm внутрь метода run_inference_wrapper скопируйте программный код из ViewController.mm приложения на языке Objective-C AudioRecognition , описанного в шагах 5–8 предыдущего раздела, который конвертирует записанный и сохраненный звук в формат, принимаемый моделью TensorFlow, а затем отправляет его вместе с частотой дискрети-зации в модель для получения результата распознавания: @implementation RunInference_Wrapper ‑ (NSString *)run_inference_wrapper:(NSString*)recorderFilePath { } Если вы действительно хотите перенести как можно больше программного кода на язык Swift, то можете переписать программный код конвертирования аудиофайлов с языка C на язык Swift (за более подробной информацией обра-титесь к https://developer.apple.com/documentation/audiotoolbox/extended_audio_ file_services). Существует также несколько неофициальных проектов с откры-тым исходным кодом, которые предоставляют Swift-обертку официального API C++ платформы TensorFlow. Но для простоты и правильного баланса мы оста-вим выведение заключения из модели TensorFlow как есть, а в данном приме-ре также чтение и конвертирование аудиофайла, то есть на C++ и Objective-C, которые работают совместно с программным кодом на языке Swift, чья зада-ча – управлять интерфейсом и аудиозаписью и инициировать вызов аудио - обрабо тки и распознавания.\n--- Страница 158 ---\n156  Понимание прос тых речевых команд Вот и все, что нужно для создания приложения для iOS на языке Swift, в ко- тором используется модель распознавания речевых команд. Теперь вы можете запустить его в своем симуляторе iOS или на фактическом устройстве и уви-деть те же самые результаты, что и в версии на языке Objective-C. резюме В этой главе мы сначала дали краткий обзор технологии распознавания речи и того, как строятся современные системы автоматического распознавания речи (ASR) с использованием методов сквозного глубокого обучения. Затем мы обсудили вопрос тренировки модели TensorFlow распознавания простых рече-вых команд и представили пошаговые инструкции по использованию такой модели в приложении для Android, а также в приложениях для iOS на языках Objective-C и Swift. Мы также обсудили вопрос исправления распространенной ошибки загрузки модели в iOS, обнаружив отсутствующий файл TensorFlow с описанием операции (op) или реализации операции (kernel), добавив его и выполнив повторную сборку библиотеки TensorFlow для iOS. Технология ASR предназначена для преобразования речи в текст. В следу - ющей главе мы рассмотрим еще одну модель, которая на выход подает текст, и этот текст будет представлять полные предложения на естественном языке вместо простых команд, как в этой главе. Мы расскажем, как создавать модель для преобразования изображения нашего старого доброго друга в текст и как использовать такую модель в мобильных приложениях. Наблюдение за тем, что вы видите вокруг себя, и описание этого на естественном языке требует истинного человеческого интеллекта. Непревзойденным мастером в этом деле является Шерлок Холмс; мы, конечно, еще не так хороши, как Холмс, но давай-те посмотрим, с чего мы можем начать.",
      "debug": {
        "start_page": 131,
        "end_page": 158
      }
    },
    {
      "name": "Глава 6. Описание изображений на естественном языке 157",
      "content": "--- Страница 159 --- (продолжение)\nГлава 6 Описание изображений на естественном языке Если классифицирование изображений и обнаружение объектов уже сами по себе являются интеллектуальными задачами, то описание изображения на естественном языке, безусловно, является намного более сложной задачей, требующей гораздо большего интеллекта, – прос то подумайте на мгновение о том, как по мере взросления с самых первых дней своей жизни мы сначала учимся распознавать объекты и определять их местоположение и только поз-же, в трехлетнем возрасте, учимся рассказывать о том, что видим на картинке. Официальный термин для описания изображения на естественном языке – анно тирование (или титрование) изображения. В отличие от распознавания речи, которое имеет долгую историю исследований и разработок, аннотирова-ние изображений (на полном естественном языке, а не только с участием клю-чевых слов) имело лишь короткую, но захватывающую историю исследований из-за своей сложности и благодаря инновационному прорыву в технологии глубокого обучения в 2012 году. В этой главе мы сначала рассмотрим работу модели аннотирования изобра- жений, основанную на глубоком обучении, которая одержала победу в конкур-се Microsoft COCO 2015 Image Captioning по аннотированию изображений (где используется крупномасштабный набор данных для обнаружения, сегменти-рования и аннотирования объектов, кратко рассмотренный в главе 3 «Обнару- жение и локализация объектов»). Затем мы резюмируем шаги, необходимые для тренировки модели в TensorFlow, и подробно рассмотрим процесс подготов-ки и оптимизации такой сложной модели для развертывания на мобильных устройствах. После этого мы продемонстрируем вам пошаговые инструкции по созданию приложений для iOS и Android, в которых такая модель приме-няется для генерирования предложений на естественном языке с описанием изображения. Поскольку данная модель включает в себя компьютерное зре-ние и обработку естественного языка, вы впервые увидите совместную рабо-ту сетей CNN и RNN, двух основных глубоких нейросетевых архитектур, и то, как писать программный код доступа к натренированным сетям и выведения\nГлава 6 Описание изображений на естественном языке Если классифицирование изображений и обнаружение объектов уже сами по себе являются интеллектуальными задачами, то описание изображения на естественном языке, безусловно, является намного более сложной задачей, требующей гораздо большего интеллекта, – прос то подумайте на мгновение о том, как по мере взросления с самых первых дней своей жизни мы сначала учимся распознавать объекты и определять их местоположение и только поз-же, в трехлетнем возрасте, учимся рассказывать о том, что видим на картинке. Официальный термин для описания изображения на естественном языке – анно тирование (или титрование) изображения. В отличие от распознавания речи, которое имеет долгую историю исследований и разработок, аннотирова-ние изображений (на полном естественном языке, а не только с участием клю-чевых слов) имело лишь короткую, но захватывающую историю исследований из-за своей сложности и благодаря инновационному прорыву в технологии глубокого обучения в 2012 году. В этой главе мы сначала рассмотрим работу модели аннотирования изобра- жений, основанную на глубоком обучении, которая одержала победу в конкур-се Microsoft COCO 2015 Image Captioning по аннотированию изображений (где используется крупномасштабный набор данных для обнаружения, сегменти-рования и аннотирования объектов, кратко рассмотренный в главе 3 «Обнару- жение и локализация объектов»). Затем мы резюмируем шаги, необходимые для тренировки модели в TensorFlow, и подробно рассмотрим процесс подготов-ки и оптимизации такой сложной модели для развертывания на мобильных устройствах. После этого мы продемонстрируем вам пошаговые инструкции по созданию приложений для iOS и Android, в которых такая модель приме-няется для генерирования предложений на естественном языке с описанием изображения. Поскольку данная модель включает в себя компьютерное зре-ние и обработку естественного языка, вы впервые увидите совместную рабо-ту сетей CNN и RNN, двух основных глубоких нейросетевых архитектур, и то, как писать программный код доступа к натренированным сетям и выведения\n--- Страница 160 ---\n158  Описание изображ ений на естественном языке из них нескольких заключений в iOS и Android. Резюмируя, в этой главе мы рассмотрим следующие темы: анно тирование изображений – как оно рабо тает; тренировка и замораживание мо дели аннотирования изображений; транс формация и оптимизация модели аннотирования изображений; испо льзование модели аннотирования изображений в iOS; применение мо дели аннотирования изображений в Android. аннотирование изображений – как оно работает Модель, которая победила в первом конкурсе аннотирования изображений MS COCO Image Captioning в 2015 году, описана в статье «Show and Tell: Lessons learned from the 2015 MS COCO Image Captioning Challenge» («Показать и рас - сказать: уроки, извлеченные из конкурса MS COCO 2015 по аннотированию изображений» (https: //arxiv.org/pdf/1609.06647.pdf)). Прежде чем мы поговорим о процессе тренировки, который также хорошо освещается на веб-сайте до-кументации модели TensorFlow im2txt по адресу https: //github.com/tensorflow/ models/tree/master/research/im2txt, давайте сначала получим общее представ-ление о том, как данная модель работает. Это также поможет вам понять про-граммный код тренировки и выведения заключения на Python, а еще про-граммный код выведения заключения в iOS и Android, который вы увидите далее в этой главе. Тренировка победившей модели Show and Tell выполняется с использова- нием сквозного метода, аналогичного применяемому в новейших глубоко обучающихся моделях распознавания речи, которые мы кратко рассмотре-ли в предыдущей главе. Данная модель использует набор данных MSCOCO Image Captioning 2014, доступный для скачивания по адресу http://c ocodataset. org/#download, который имеет более 82k тренировочных изображений и опи-сывающих их целевых предложений на естественном языке. Данная модель натренирована максимизировать вероятность выведения целевого предло-жения на естественном языке для каждого входного изображения. В отличие от других более сложных методов тренировки, использующих несколько под-систем, сквозной (end-to-end) метод элегантен, проще и может достигать са-мых современных результатов. Для обработки и представления входного изображения в модели Show and Tell используется предварительно натренированная модель Inception v3, та же самая модель, которую мы применяли для вторичной тренировки нашей мо-дели классифицирования изображений пород собак в главе 2 «Классифициро- вание изображений с помощью трансферного обучения». Последний скрытый слой CNN-модели Inception v3 используется в качестве представления вход-ного изображения. Из-за особенностей CNN-модели более ранние слои сети захватывают более элементарную информацию об изображении, а более позд-ние слои фиксируют понимание изображения более высокого уровня. Следо-\n--- Страница 161 ---\nОписание изображений на естественном языке  159 вательно, используя для представления изображения последний скрытый слой входного изображения, мы можем лучше подготовить вывод на естественном языке с помощью высокоуровневых понятий. В конце концов, мы обычно на-чинаем описывать рисунок общими словами «человек» или «поезд», а не вы-ражениями типа «нечто с острым краем». Для представления каждого слова в выводе на целевом естественном языке используется метод встраивания слов. Встраивание слов (word embedding) 1 – это попросту векторные представления слов. На веб-сайте TensorFlow име- ется хорошее руководство по данной теме (https://www.tensorflow.org/tutorials/word2veс) с описанием построения модели, получающей векторные представ-ления слов. Теперь, располагая представлением входного изображения и выходных слов (каждая такая пара образует тренировочный пример), самой лучшей трениро-вочной моделью, которая может использоваться для максимизации вероят - ности получения каждого слова w в целевом выводе с учетом входного изо-бражения и предыдущих слов перед словом w, является модель предсказания последовательности (sequence model) 2 на основе RNN-сети, или, точнее, тип модели на основе RNN-сети, который называется долгой краткосрочной памя-тью (Long Short Term Memory, LSTM). LSTM-модель хорошо известна тем, что решает проблему исчезающих и взрывающихся градиентов, присущих обыч-ным моделям на основе RNN-сетей. Для того чтобы лучше разобраться в LSTM-модели, вам следует обратиться к приведенному в ссылке популярному блог-посту, http://c olah.github.io/posts/2015–08-Understanding-LSTMs.  Понятие градиента используется в процессе обратного распространения ошибки для обновления весов сети, которое позволяет ей учиться генерировать более оптимальные выходные данные. Если вы незнакомы с процессом обратного распространения ошиб-ки, одним из самых фундаментальных и мощных алгоритмов в нейронных сетях, то вы обязательно должны потратить немного времени на его осмысление – прос то погуглите «backprop», и первые пять результатов вас не разочаруют. Исчезающий градиент означа-ет, что в процессе заучивания во время обратного распространения ошибки в глубокой нейронной сети веса сети в более ранних слоях едва обновляются, поэтому сеть никог - да не сходится. Взрывающийся градиент означает, что эти веса обновляются слишком резко, заставляя сеть сильно расходиться. Поэтому если кто-то замкнут в себе и ничему 1 Вс траивание, или включение, слов (word embedding) – это собирательный термин для набора методов моделирования языка и заучивания признаков при обработке естественного языка, где словам или фразам словаря ставятся в соответствие век - торы реальных чисел. Концептуально оно связано с математическим включением из пространства с одной размерностью в расчете на слово в непрерывное векторное пространство с гораздо более высокой размерностью. См. https://en.w ikipedia.org/wiki/ Word_embedding, а также https: //ru.w ikipedia.org/wiki/Вложение. – Прим. перев. 2 См. блог-пост «Gentle Introduction to Models for Sequence Prediction with Recurrent Neural Networks» («Осторожное введение в модели предсказания последователь-ностей на основе RNN-сети»): https://machinelearningmastery .com/models-sequence- prediction-recurrent-neural-networks/. – Прим. перев.\n--- Страница 162 ---\n160  Описание изображ ений на естественном языке не учится или если кто-то сходит с ума по новым вещам так же быстро, как он теряет к ним интерес, знайте, что они страдают проблемой градиента. После тренировки CNN- и LSTM-модели могут использоваться вместе для выведения заключения: при наличии входного изображения модель может оценить вероятность каждого слова и, следовательно, предсказать, какие из n первых самых лучших слов, скорее всего, подойдут для генерирования вы-ходного предложения. Затем с учетом входного изображения и n первых са- мых лучших слов могут быть сгенерированы n следующих самых лучших слов, и данный процесс продолжается до тех пор, пока у нас не будет иметься пол-ное предложение к моменту, когда модель вернет специфическое слово конца предложения или когда будет достигнуто определенное количество слов ге-нерируемого предложения (во избежание излишней многословности модели). Использование n самых лучших слов (имеется в виду n самых лучших пред- ложений на завершающей стадии) при каждой генерации слов называется лу - чевым поиском (beam search). Когда n, так называемый размер луча, равен 1, он становится жадным поиском, или поиском по первому наилучшему совпа-дению (best-first search), который основан исключительно на лучшем значе-нии вероятности среди всех возможных слов, возвращаемых моделью. Про-цесс тренировки и выведения заключения из официальной модели TensorFlow im2txt в следующем далее разделе использует лучевой поиск с размером луча 3 и реализован на языке Python; для сравнения, приложения для iOS и Android, которые мы разработаем, используют более простой жадный поиск, или по-иск по первому наилучшему совпадению. Вы увидите, какой из этих методов генерирует самые лучшие аннотации. тренировка и замораживание моДели аннотирования изображений В этом разделе мы сначала подведем итоги процесса тренировки модели Show and Tell под названием im2txt, описанной в https: //github.com/tensorflow/models/ tree/master/research/im2txt. Данный процесс будет дополнен несколькими сове-тами, которые помогут вам получше в нем разобраться. Затем мы покажем не-которые ключевые изменения в программном коде на языке Python, который поставляется с проектом модели im2txt, для того чтобы заморозить модель и тем самым подготовить ее к использованию на мобильных устройствах. тренировка и тестирование генерирования аннотаций Если вы отслеживали ход изложения материала в разделе «Настройка API Tensorflow обнаружения объектов» главы 3 «Обнаружение и локализация объ- ектов», то у вас уже установлена папка im2txt . В противном случае при по-\n--- Страница 163 ---\nОписание изображений на естественном языке  161 мощи команды cd перейдите в корневой каталог исходного кода платформы TensorFlow и затем выполните: git clone https://github.com/tensorflow/models Одна из библиотек Python, которую вы, вероятно, не установили, будет Natural Language ToolKit (NLTK), одна из самых популярных библиотек Python для обработки естественного языка. Просто зайдите на ее веб-сайт по адресу http: //www.nltk.org для получения инструкций по установке. Теперь выполните следующие ниже действия для тренировки модели.1. Настройте месторасположение для сохранения набора тренировочных и контрольных данных аннотирования изображений 2014 MSCOCO, от - крыв терминал и выполнив: MSCOCO_DIR=\"${HOME}/im2txt/data/mscoco\" Обратите внимание, что хотя сырой набор данных 2014 MSCOCO, кото-рый должен быть скачан и сохранен, имеет размер около 20 Гб, он бу - дет конвертирован в формат TFRecord (мы уже использовали формат TFRecord в главе 3 «Обнаружение и локализация объектов» для конверти- рования набора данных обнаружения объектов), который необходим для выполнения следующего далее тренировочного сценария, и вследствие этого увеличится приблизительно на 100 Гб данных. Таким образом, для тренировки вашей собственной модели аннотирования изображений с помощью проекта TensorFlow im2txt вам потребуется в общей слож - ности около 140 Гб. 2. Перейдит е к месторасположению исходного кода im2txt, скачайте и об- работайте набор данных MSCOCO: cd <your_tensorflow_root>/models/research/im2txtbazel build //im2txt: download_and_preprocess_mscocobazel ‑bin/im2txt/download_and_preprocess_mscoco \"${MSCOCO_DIR}\" После завершения сценария download_and_preprocess_mscoco все файлы тренировочных, контрольных и тестовых данных в формате TFRecord вы найдете в папке $MSCOCO_DIR . В папке $MSCOCO_DIR также имеется сгенерированный файл с именем word_counts.txt . Он имеет в общей сложности 11 518 слов, и каждая строка состоит из слова, пробела и количества появлений слова в наборе дан-ных. В данном файле хранятся только те слова, количество которых рав-но или больше 4, а также специальные слова, такие как начало и конец предложения (представленных соответственно как <s> и </s>). Позже вы увидите, как конкретно использовать и выполнять разбор данного файла в наших приложениях для iOS и Android для генерирования аннотаций. 3. Получите зафиксированный в контрольной точке файл Inception v3, вы- полнив следующие ниже команды:\n--- Страница 164 ---\n162  Описание изображ ений на естественном языке INCEPTION_DIR=\"${HOME}/im2txt/data\" mkdir ‑p ${INCEPTION_DIR} cd ${INCEPTION_DIR}wget \"http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\"tar ‑xvf inception_v3_2016_08_28.tar.gz ‑C ${INCEPTION_DIR} rm inception_v3_2016_08_28.tar.gz После этого в папке ${HOME}/im2txt/data вы увидите файл с именем inception_v3.ckpt, как показано тут: jeff@AiLabby:~/im2txt/data$ ls ‑lt inception_v3.ckpt ‑rw‑r‑‑‑‑‑ 1 jeff jeff 108816380 Aug 28 2016 inception_v3.ckpt 4. Т еперь мы готовы натренировать нашу модель с помощью следующих ниже команд: INCEPTION_CHECKPOINT=\"${HOME}/im2txt/data/inception_v3.ckpt\"MODEL_DIR=\"${HOME}/im2txt/model\"cd <your_tensorflow_root>/models/research/im2txtbazel build ‑c opt //im2txt/ bazel‑bin/im2txt/train \\ ‑‑input_file_pattern=\"${MSCOCO_DIR}/train ‑?????‑of‑00256\" \\ ‑‑inception_checkpoint_file=\"${INCEPTION_CHECKPOINT}\" \\ ‑‑train_dir=\"${MODEL_DIR}/train\" \\ ‑‑train_inception=false \\ ‑‑number_of_steps=1000000 Даже на GPU, таком как наш Nvidia GTX 1070, установленном в главе 1 «Начало работы с платформой TensorFlow Mobile», все 1M шагов, ука- занных в приведенном выше параметре ‑‑number_of_steps , займут бо- лее 5 дней и ночей, так как для выполнения 50K шагов требуется около 6,5 часа. К счастью, как вы вскоре увидите, даже с шагом около 50K ре-зультаты аннотирования изображений уже будут довольно хорошими. Также обратите внимание на то, что вы можете отменить работу трени-ровочного сценария train в любое время, а затем повторно запустить его, и сценарий начнется с последней сохраненной контрольной точки; контрольные точки по умолчанию сохраняются каждые 10 минут, поэто-му в худшем случае вы потеряете только 10 минут тренировки.После нескольких часов тренировки отмените работу приведенного выше сценария train и посмотрите, куда указывает параметр ‑‑train_ dir. Вы увидите там что-то вроде этого (по умолчанию сохраняются пять наборов файлов контрольных точек, но мы приводим здесь только три): ls -lt $MODEL_DIR/train-rw-rw-r-- 1 jeff jeff 2171543 Feb 6 22:17 model.ckpt -109587.meta -rw-rw-r-- 1 jeff jeff 463 Feb 6 22:17 checkpoint -rw-rw-r-- 1 jeff jeff 149002244 Feb 6 22:17 model.ckpt -109587.data -00000-of -00001 -rw-rw-r-- 1 jeff jeff 16873 Feb 6 22:17 model.ckpt -109587.index -rw-rw-r-- 1 jeff jeff 2171543 Feb 6 22:07 model.ckpt -109332.meta\n--- Страница 165 ---\nОписание изображений на естественном языке  163 -rw-rw-r-- 1 jeff jeff 16873 Feb 6 22:07 model.ckpt -109332.index -rw-rw-r-- 1 jeff jeff 149002244 Feb 6 22:07 model.ckpt -109332.data -00000-of -00001 -rw-rw-r-- 1 jeff jeff 2171543 Feb 6 21:57 model.ckpt -109068.meta -rw-rw-r-- 1 jeff jeff 149002244 Feb 6 21:57 model.ckpt -109068.data -00000-of -00001 -rw-rw-r-- 1 jeff jeff 16873 Feb 6 21:57 model.ckpt -109068.index -rw-rw-r-- 1 jeff jeff 4812699 Feb 6 14:27 graph.pbtxt Вы видите, что каждый набор файлов контрольных точек ( model. ckpt‑109068.*, model.ckpt‑109332.*, model.ckpt‑109587.* ) генерируется каж - дые 10 минут. Файл graph.pbtxt – э то файл определения графа моде- ли (в текстовом формате). Файл model.ckpt ‑??????.meta также содержит определение графа модели и, помимо этого, еще некоторые другие мета- данные, относящиеся к конкретной контрольной точке, например model. ckpt‑109587.data‑00000 ‑of‑00001 (обратите внимание, что его размер со- ставляет почти 150 Мб, потому что там сохраняются все параметры сети). 5. Теперь протестируем генерирование аннотаций: CHECKPOINT_PATH=\"${HOME}/im2txt/model/train\"VOCAB_FILE=\"${HOME}/im2txt/data/mscoco/word_counts.txt\"IMAGE_FILE=\"${HOME}/im2txt/data/mscoco/raw ‑data/val2014/COCO_val2014_000000224477.jpg\" bazel build ‑c opt //im2txt: run_inference bazel‑bin/im2txt/run_inference \\ ‑‑checkpoint_path=${CHECKPOINT_PATH} \\ ‑‑vocab_file=${VOCAB_FILE} \\ ‑‑input_files=${IMAGE_FILE} Значение пути CHECKPOINT_PATH эквивалентно пути, указанному в парамет - ре ‑‑train_dir . Сценарий run_inference сгенерирует что-то вроде этого (результаты могут отличаться, поскольку они зависят от того, сколько тренировочных шагов было выполнено): Captions for image COCO_val2014_000000224477.jpg:0) a man on a surfboard riding a wave. (p=0.015135)1) a person on a surfboard riding a wave. (p=0.011918)2) a man riding a surfboard on top of a wave. (p=0.009856) Весьма впечатляюще. Разве не было бы еще более впечатляюще, если бы мы могли запускать эту модель на наших смартфонах? Но прежде чем мы сможем это сделать, нам нужно выполнить несколько дополнительных шагов из-за от - носительно большей сложности модели, а также из-за того, как были написаны сценарии train и run_inference на языке Python. замораживание моДели аннотирования изображений В главе 4 «Трансформирование рисунков с помощью художественных стилей» и главе 5 «Понимание простых речевых команд» мы использовали две немного разные версии сценария под названием freeze.py , объединяющие натрениро- ванные веса сети с определением графа сети в самодостаточный файл моде-\n--- Страница 166 ---\n164  Описание изображ ений на естественном языке ли, который мы можем использовать на мобильных устройствах. Платформа TensorFlow поставляется с более универсальной версией сценария заморозки под названием freeze_graph.py , расположенным в папке tensorflow/python/tools , который можно использовать для построения файла модели. Для того чтобы он заработал, вам нужно предоставить ему, по крайней мере, четыре парамет - ра (чтобы увидеть все доступные параметры, просмотрите файл tensorflow/py‑ thon/tools/freeze_graph.py ): параметры ‑‑input_graph или ‑‑input_meta_graph: файл определения гра- фа модели. Например, в выводе команды ls ‑lt $MODEL_DIR/train в шаге 4 предыдущего раздела model.ckpt‑109587.meta – э то файл метаграфа, со- держащий определение графа модели и другие метаданные, связанные с контрольной точкой, и graph.pbtxt – э то просто определение графа мо- дели; параметр ‑‑input_checkpoint : файл конкретной контрольной точки, на- пример modei.ckpt‑109587 . Обратите внимание, что вы не указываете пол- ное имя файла контрольной точки большого размера model.ckpt‑109587. data‑00000 ‑of‑00001 ; параметр ‑‑output_graph : путь к файлу замороженной модели – испо льзу - ется на мобильных устройствах; параметр ‑‑output_node_names : список имен выходных узлов, разделенных запятыми, который сообщает инструменту freeze_graph , какая часть мо- дели и весов должна быть включена в замороженную модель, в резуль-тате чего узлы и веса, не требующиеся для генерирования конкретных имен выходных узлов, пропускаются. Так как же узнать обязательные имена выходных узлов, а также имена вход- ных узлов, тоже необходимых для выведения заключения, для этой модели, как мы уже видели в приложениях для iOS и Android в предыдущих главах? Поскольку мы уже применяли сценарий run_inference , который генерировал аннотацию для нашего тестового изображения, мы можем взглянуть на то, как он выводит свое заключение. Перейдите в свою папку с исходным кодом im2txt , models/research/im2txt/ im2txt : вы можете открыть ее в каком-нибудь хорошем редакторе, к примеру в редакторе Atom или Sublime Text, либо интегрированной среде разработки на Python, такой как PyCharm и Spyder, либо просто открыть его (https: //github. com/tensorflow/models/tree/master/research/im2txt/im2txt) из своего браузера. В сценарии run_inference.ру имеется вызов функции build_graph_from_config сценария inference_utils/inference_wrapper_base.py . Данная функция, в свою очередь, вызывает метод build_model в сценарии inference_wrapper.py , который далее вызывает метод сборки модели build_model в сценарии show_and_tell_ model.py . Метод build наконец вызывает, среди прочего, метод build_input , име- ющий следующий программный код: if self.mode == \"inference\": image_feed = tf.placeholder(dtype=tf.string, shape=[], name=\"image_feed\")\n--- Страница 167 ---\nОписание изображений на естественном языке  165 input_feed = tf.placeholder(dtype=tf.int64, shape=[None], # batch_size name=\"input_feed\") Метод build_model выглядит следующим образом: if self.mode == \"inference\": tf.concat(axis=1, values=initial_state, name=\"initial_state\") state_feed = tf.placeholder(dtype=tf.float32, shape=[None, sum(lstm_cell.state_size)], name=\"state_feed\") tf.concat(axis=1, values=state_tuple, name=\"state\") tf.nn.softmax(logits, name=\"softmax\") Следовательно, именами входных узлов должны быть три заполнителя с именами image_feed , input_feed и state_feed , в то время как именами выход- ных узлов должны быть initial_state , state и softmax . Кроме того, определен- ные в сценарии inference_wrapper.py два метода подтверждают наше расследо- вание – первый мет од выглядит так: def feed_image(self, sess, encoded_image): initial_state = sess.run(fetches=\"lstm/initial_state:0\", feed_dict={\"image_feed:0\": encoded_image}) return initial_state Таким образом, мы предоставляем image_feed и получаем назад initial_state (префикс lstm/ просто означает, что узел находится в области действия lstm ). Второй метод следующий: def inference_step(self, sess, input_feed, state_feed): softmax_output, state_output = sess.run( fetches=[\"softmax:0\", \"lstm/state:0\"], feed_dict={ \"input_feed:0\": input_feed, \"lstm/state_feed:0\": state_feed, }) return softmax_output, state_output, None Мы подаем input_feed и state_feed и получаем обратно softmax и state . Всего три входных узла и три выходных узла. Обратите внимание, что эти узлы создаются, только если режим mode установ- лен в «inference», так как сценарий show_and_tell_model.py используется в сце- нариях train.py и run_inference.py . Это означает, что файл определения графа и веса модели, расположенные в папке, указанной в параметре ‑‑ checkpoint_ path, сгенерированные с помощью сценария train на шаге 5, будут изменены после запуска сценария run_inference.py . И как же нам сохранить обновленные файлы определения графа и контрольных точек?\n--- Страница 168 ---\n166  Описание изображ ений на естественном языке Оказывается, в сценарии run_inference.py , после того как сеанс TensorFlow создан, имеется вызов функции restore_fn(sess) для загрузки файла конт - рольной точки, и этот вызов определен в сценарии inference_utils/inference_ wrapper_base.py : def _restore_fn(sess): saver.restore(sess, checkpoint_path) При достижении вызова метода saver.restore после запуска сценария run_ inference.py будет сделано обновление определения графа, и поэтому мы мо- жем сохранить там новую контрольную точку и файл графа, переделав функ - цию _restore_fn следующим образом: def _restore_fn(sess): saver.restore(sess, checkpoint_path) saver.save(sess, \"model/image2text\") tf.train.write_graph(sess.graph_def, \"model\", 'im2txt4.pbtxt') tf.summary.FileWriter(\"logdir\", sess.graph_def) Строка tf.train.write_graph(sess.graph_def, \"model\", 'im2txt4.pbtxt') не обя- зательная, поскольку во время сохранения нового файла контрольной точки путем вызова saver.save также генерируется метафайл, который может ис - пользоваться сценарием freeze_graph.py вместе с файлом контрольной точки. Но он генерируется здесь для тех, кто хотел бы все видеть в текстовом форма-те, либо тех, кто предпочитает использовать файл определения графа с пара-метром ‑‑in_graph при замораживании модели. Последняя строка tf.summary. FileWriter(\"logdir\", sess.graph_def) также является необязательной, но она создает файл событий, который можно визуализировать с помощью серве-ра TensorBoard. Так что после повторного запуска сценария run_ inference. py (не забудьте сначала выполнить команду bazel build ‑c opt //im2txt: run_ inference , в случае если вы не выполняете сценарий run_inference.py непосред- ственно в среде Python) при наличии этих изменений в своем каталоге модели вы увидите следующие новые файлы контрольных точек и новый файл опре-деления графа: jeff@AiLabby:~/tensorflow -1.5.0/models/research/im2txt$ ls -lt model -rw-rw-r-- 1 jeff jeff 2076964 Feb 7 12:33 image2text.pbtxt -rw-rw-r-- 1 jeff jeff 1343049 Feb 7 12:33 image2text.meta -rw-rw-r-- 1 jeff jeff 77 Feb 7 12:33 checkpoint -rw-rw-r-- 1 jeff jeff 149002244 Feb 7 12:33 image2text.data -00000-of -00001 -rw-rw-r-- 1 jeff jeff 16873 Feb 7 12:33 image2text.index И в своем каталоге logdir : jeff@AiLabby:~/tensorflow -1.5.0/models/research/im2txt$ ls -lt logdir total 2124-rw-rw-r-- 1 jeff jeff 2171623 Feb 7 12:33 events.out.tfevents.1518035604.AiLabby\n--- Страница 169 ---\nОписание изображений на естественном языке  167  Выпо лнение команды bazel build для сборки сценария Python платформы TensorFlow является необязательным. Вы можете выполнить сценарий Python непосредственно. На- пример, мы можем выполнить спенарий python tensorflow/python/tools/freeze_graph. py без его сборки командой bazel build tensorflow/python/tools: freeze_graph и за- тем bazel‑bin/tensorflow/python/tools/freeze_graph . Но имейте в виду, что при непо- средственном выполнении сценария Python будет использоваться версия платформы TensorFlow, установленная с помощью менеджера пакетов pip, которая может отличаться от версии, скачанной в качестве исходного кода и собранной командой bazel build . Это может быть причиной некоторых запутанных ошибок, поэтому убедитесь, что вы знаете версию платформы TensorFlow, используемую для запуска данного сценария. Кроме того, перед запуском инструмента на основе C++ сначала вы должны выполнить его сборку с помощью bazel . Например, инструмент transform_graph , который мы вскоре увидим в действии, реализуется в файле transform_graph.cc , расположенном в папке tensorflow/ tools/graph_transforms . Еще один важный инструмент под названием convert_graphdef_ memmapped_format , который мы позже будем использовать в нашем приложении для iOS, тоже реализован на C++ и расположен в папке tensorflow/contrib/util . Пока мы находимся здесь, давайте быстро воспользуемся сервером TensorBoard, чтобы взглянуть на то, как выглядит наш граф, – выпо лни- те команду tensorboard ‑‑logdir logdir и направьте свой браузер по адресу http:// localhost:6006 . На рис. 6.1 показаны имена трех выходных узлов ( softmax вверху, lstm/initial_state и lstm/state вверху выделенного красного прямо - угольника) и имя одного входного узла ( state_feed внизу):\n--- Страница 170 ---\n168  Описание изображ ений на естественном языке Рисунок 6.1. Схема, показывающая имена трех выходных узлов и имя одного входного узла На рис. 6.2 показано имя одного дополнительного входного узла image_feed :\n--- Страница 171 ---\nОписание изображений на естественном языке  169 Рисунок 6.2. Схема показывает имя одного дополнительного входного узла image_feed Наконец, на рис. 6.3 показано имя последнего входного узла input_feed : Рисунок 6.3. Схема показывает имя последнего входного узла input_feed Конечно же, существует еще много других деталей, которые мы не сможем и не будем здесь освещать. Тем не менее вы получили общую картину и, что не менее важно, достаточно подробностей, для того чтобы двигаться дальше. Теперь работа сценария заморозки freeze_graph.py должна пойти как по маслу (каламбур): python tensorflow/python/tools/freeze_graph.py ‑‑input_meta_graph=/home/jeff/ tensorflow‑1.5.0/models/research/im2txt/model/image2text.meta ‑‑input_checkpoint=/ home/jeff/tensorflow‑1.5.0/models/research/im2txt/model/image2text ‑‑output_graph=/tmp/\n--- Страница 172 ---\n170  Описание изображ ений на естественном языке image2text_frozen.pb ‑‑output_node_names=\"softmax, lstm/initial_state, lstm/state\" ‑‑input_ binary=true Обратите внимание, что мы здесь используем файл метаграфа и назначили параметру ‑‑input_binary значение true , так как по умолчанию оно равно false . Это означает, что инструмент freeze_graph ожидает, что входной граф или файл метаграфа находится в текстовом формате. В качестве входа можно использовать текстовый файл графа, и в этом случае нет необходимости предоставлять параметр ‑‑input_binary : python tensorflow/python/tools/freeze_graph.py ‑‑input_graph=/home/jeff/tensorflow‑1.5.0/ models/research/im2txt/model/image2text.pbtxt ‑‑input_checkpoint=/home/jeff/ tensorflow‑1.5.0/models/research/im2txt/model/image2text ‑‑output_graph=/tmp/image2text_ frozen2.pb ‑‑output_node_names=\"softmax, lstm/initial_state, lstm/state\" Размеры обоих выходных файлов графа image2text_frozen.pb и image2text_ frozen2.pb будут немного отличаться, но они будут вести себя совершенно оди- наково при использовании на мобильных устройствах после их трансформа- ции и, возможно, оптимизации. трансформация и оптимизация моДели аннотирования изображений Если вы действительно не можете больше ждать и теперь решили попробо-вать свежезамороженную «горячую» модель в своем приложении для iOS или Android, то вы, конечно же, можете это сделать, но получите фатальную ошиб-ку No OpKernel was registered to support Op 'DecodeJpeg' with these attrs , которая заставит вас пересмотреть свое решение. исправление ошибок в трансформированных моДелях Как правило, вы можете воспользоваться инструментом очистки strip_unused. py, расположенным в том же месте, что и сценарий заморозки freeze_graph.py в папке tensorflow/python/tools , для удаления операции DecodeJpeg , не входящей в корневую библиотеку TensorFlow (за дополнительной информацией обрати-тесь к https://www.tensorflow.org/mobile/prepare_models#removing_training-only_ nodes), но поскольку входной узел image_feed требует операции декодирования (рис. 6.2), инструмент strip_unused не будет рассматривать операцию Decodejpeg как неиспользуемую, и она не будет удалена. Вы можете перепроверить сами, сначала выполнив команду strip_unused следующим образом: bazel‑bin/tensorflow/python/tools/strip_unused ‑‑input_graph=/tmp/image2text_frozen. pb ‑‑output_graph=/tmp/image2text_frozen_stripped.pb ‑‑input_node_names=\"image_feed,\n--- Страница 173 ---\nОписание изображений на естественном языке  171 input_feed, lstm/state_feed\" ‑‑output_node_names=\"softmax, lstm/initial_state, lstm/state\" ‑‑input_binary=True Затем загрузив выходной граф в IPython и выведя список первых несколь- ких узлов, как здесь: import tensorflow as tf g=tf.GraphDef()g.ParseFromString(open(\"/tmp/image2text_frozen_stripped\", \"rb\").read())x=[n.name for n in g.node]x[:6] Результат будут выглядеть следующим образом: [u'image_feed',u'input_feed',u'decode/DecodeJpeg',u'convert_image/Cast',u'convert_image/y',u'convert_image'] Вторым возможным решением задачи исправления ошибки в вашем при- ложении для iOS являются добавление незарегистрированной реализации операции (op) в файл tf_op_files и повторная сборка библиотеки TensorFlow для iOS, как мы делали в главе 5 «Понимание простых речевых команд». Пло- хая новость заключается в том, что поскольку в TensorFlow нет реализации функционала DecodeJpeg , то и нет никакого способа добавить характерную для TensorFlow реализацию операции DecodeJpeg в файл tf_op_files . Подсказка для исправления этой досадной неприятности фактически дана на рис. 6.2, где узел convert_image используется в качестве декодированной версии входа image_feed . Если быть точнее, то следует нажать на узлах Cast и decode графа TensorBoard, как показано на рис. 6.4, и вы увидите из инфор- мационных панелей TensorBoard справа, что входом и выходом из Cast (с име- нем convert_image/cast ) являются decode/DecodeJpeg и convert_image , а входом и выходом из decode – image_feed и convert_image/cast :\n--- Страница 174 ---\n172  Описание изображ ений на естественном языке Рисунок 6.4. Просмотр узлов decode и convert_image В действительности в сценарии im2txt/ops/image_processing.py имеется стро- ка image = tf.image.convert_image_dtype(image, dtype=tf.float32) , которая конвер- тирует декодированное изображение в вещественные числа. Давайте заменим image_feed на convert_image/cast (имя показано в TensorBoard), а также выход из предыдущего фрагмента кода и снова запустим strip_unused : bazel‑bin/tensorflow/python/tools/strip_unused ‑‑input_graph=/tmp/image2text_frozen.pb ‑‑output_graph=/tmp/image2text_frozen_stripped.pb ‑‑input_node_names=\"convert_image/Cast, input_feed, lstm/state_feed\" ‑‑output_node_names=\"softmax, lstm/initial_state, lstm/state\" ‑‑input_binary=True Теперь выполните этот фрагмент кода еще раз следующим образом: g.ParseFromString(open(\"/tmp/image2text_frozen_stripped\", \"rb\").read()) x=[n.name for n in g.node]x[:6] И на выходе больше не будет узла decode/DecodeJpeg : [u'input_feed',u'convert_image/Cast',u'convert_image/y',u'convert_image',u'ExpandDims_1/dim',u'ExpandDims_1'] Если мы применим файл нашей новой модели, image2text_frozen_stripped. pb, в приложении для iOS или Android, то сообщение No OpKernel was registered\n--- Страница 175 ---\nОписание изображений на естественном языке  173 to support Op 'DecodeJpeg' with these attrs. наконец-то исчезнет, однако при этом возникнет еще одна ошибка: Not a valid TensorFlow Graph serialization: Input 0 of node ExpandDims_6 was passed float from input_feed:0 incompatible with expected int64 , сообщающая о несовместимости типов. Если вы прошли хоро- ший лабораторный курс Google TensorFlow под названием «TensorFlow для поэтов 2» (https: //codelabs.developers.google.com/codelabs/tensorflow-for-poets -2), то можете вспомнить, что существует еще один инструмент под названием optimize_for_inference (оптимизировать для выведения заключения), который делает нечто похоже, что и инструмент strip_unused , и он хорошо работает для задачи классификации изображений в лабораторном курсе codelab . Его можно выполнить следующим образом: bazel build tensorflow/python/tools: optimize_for_inferencebazel-bin/tensorflow/python/tools/optimize_for_inference \\--input=/tmp/image2text_frozen.pb \\--output=/tmp/image2text_frozen_optimized.pb \\--input_names=\"convert_image/Cast, input_feed, lstm/state_feed\" \\--output_names=\"softmax, lstm/initial_state, lstm/state\" Но загрузка результирующего файла модели image2text_frozen_optimized.pb в iOS или Android приведет к той же самой ошибке Input 0 of node ExpandDims_6 was passed float from input_feed:0 incompatible with expected int64 (0 несовместим с типом int64). По всей видимости, в то время как в этой главе мы всеми силами стараемся хотя бы чуть-чуть научиться тому, что умеет делать Холмс, кто-то хочет, чтобы мы сначала были похожими на Холмса. Если вы пробовали инструменты strip_unused или optimize_for_inference на других моделях, которые мы встречали в предыдущих главах, то они работа-ют безупречно. Оказывается, оба этих инструмента на основе Python, которые, хотя и включены в официальные релизы TensorFlow 1.4 и 1.5, имеют несколько ошибок при оптимизации некоторых более сложных моделей. Обновленным и правильным инструментом является инструмент transform_graph на основе C++, который теперь является официальным инструментом, рекомендован-ным на веб-сайте TensorFlow Mobile (https://www.tensorflow.org/mobile). Для того чтобы избавиться от ошибки float incompatible with the int64 (тип float несо- вместим с типом int64) при развертывании на мобильных устройствах, выпол-ните следующие ниже команды: bazel build tensorflow/tools/graph_transforms: transform_graph bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\ --in_graph=/tmp/image2text_frozen.pb \\--out_graph=/tmp/image2text_frozen_transformed.pb \\--inputs=\"convert_image/Cast, input_feed, lstm/state_feed\" \\--outputs=\"softmax, lstm/initial_state, lstm/state\" \\--transforms=' strip_unused_nodes(type=float, shape=\"299,299,3\")\n--- Страница 176 ---\n174  Описание изображ ений на естественном языке fold_constants(ignore_errors=true, clear_output_shapes=true) fold_batch_norms fold_old_batch_norms' Мы не будем здесь вдаваться в подробности всех вариантов параметра ‑‑ transforms . Они полностью задокументированы на веб-странице https: //github. com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms. В общем и целом параметр ‑‑transforms правильно избавляет от неиспользуемых узлов, таких как DecodeJpeg , в случае с нашей моделью, а также выполняет несколько других оптимизаций. Теперь, если вы загрузите файл image2text_frozen_transformed.pb в приложе- ние для iOS или Android, то ошибка несовместимости исчезнет. Конечно, мы еще не написали никакого реального программного кода для iOS и Android, но мы знаем, что модель работоспособна и готова к тому, чтобы порадовать нас своей работой. Все это хорошо, но может быть еще лучше. оптимизация трансформированной моДели Поистине последним шагом, и к тому же решающим, в особенности при вы-полнении сложной замороженной и трансформированной модели, такой как та, которую мы натренировали на истертом устройстве iOS, является исполь-зование еще одного инструмента под названием convert_graphdef_memmapped_ format (конвертировать описание графа в формат memmapped), расположен- ного в папке tensorflow/contrib/util . Данный инструмент предназначен для конвертирования замороженной и трансформированной модели в формат memmapped. Файл в формате memmapped позволяет современным операци-онным системам, таким как iOS и Android, отображать файл непосредственно в основную память, поэтому нет необходимсти выделять память для файла, а запись данных назад на диск отсутствует, так как данные файла предназна-чены только для чтения, что в итоге дает существенный прирост в произво-дительности. Что еще более важно, iOS не воспринимает использование файла memmapped как потребление памяти, поэтому, когда присутствует слишком большое дав-ление на память, приложение, в котором используется файл memmapped, даже крупного размера, не будет убито операционной системой iOS из-за его по-требления большого объема памяти и не завершится аварийно. На самом деле, как мы скоро увидим в следующем далее разделе, если трансформированная версия нашего файла модели не будет конвертирована в формат memmapped, приложение завершится аварийно на старых мобильных устройствах, таких как iPhone 6, и в этом случае конвертация является обязательной. Команда сборки и запуска инструмента довольно прямолинейна: bazel build tensorflow/contrib/util: convert_graphdef_memmapped_format bazel‑bin/tensorflow/contrib/util/convert_graphdef_memmapped_format \\\n--- Страница 177 ---\nОписание изображений на естественном языке  175 ‑‑in_graph=/tmp/image2text_frozen_transformed.pb \\ ‑‑out_graph=/tmp/image2text_frozen_transformed_memmapped.pb В следующем далее разделе мы покажем вам, как использовать конверти- рованный в формат memmapped файл модели image2text_frozen_transformed_ memmapped.pb в приложении для iOS. Данный инструмент также может быть применен в Android с использованием нативного программного кода, но из-за ограничений по времени мы не сможем охватить его в этой главе. Мы прошли несколько дополнительных миль, чтобы получить сложную мо- дель аннотирования изображений, наконец-то готовую для наших мобильных приложений. Пришло время оценить простоту использования данной модели. На самом деле использование модели – это больше, чем просто один вызов метода session ‑>Run в iOS или вызов метода mInferenceInterface.run в Android, как мы делали во всех предыдущих главах. Выведение заключения из изобра-жения на входе в естественно-языковое предложение на выходе, как вы убеди-лись, глядя на то, как в предыдущем разделе работает сценарий run_ inference . py, состоит из нескольких вызовов метода run применительно к модели. LSTM- модели работают по принципу «продолжайте посылать мне новые данные на вход, и (на основе моего предыдущего состояния и вывода) я отправлю вам обратно следующее состояние и вывод». Если говорить о простоте, то мы про-демонстрируем вам минимальный чистый программный код, необходимый для создания приложений с использованием модели описания изображений на естественном языке для iOS и Android. Благодаря этому вы сможете при не-обходимости легко интегрировать такую модель и ее программный код выве-дения заключения в свои собственные приложения. использование моДели аннотирования изображений в ios Поскольку CNN-часть модели основана на модели Inception v3, той самой, которую мы использовали в главе 2 «Классифицирование изображений с по- мощью трансферного обучения», мы можем и будем использовать более прос - той модуль TensorFlow для создания нашего приложения для iOS на языке Objective-C. Следуйте приведенным здесь инструкциям, чтобы увидеть, как ис - пользовать файлы модели image2text_frozen_transformed.pb и модели image2text_ frozen_transformed_memmapped.pb в новом приложении для iOS. 1. Аналогично первым четырем шагам в главе 2 «Классифицирование изо- бражений с помощью трансферного обучения», в разделе «Добавление поддержки платформы TensorFlow в приложение для iOS на языке Objective-C» создайте новый проект iOS с именем Image2Text и добавьте новый файл с именем Podfile со следующим содержимым: target 'Image2Text' pod 'TensorFlow ‑experimental'\n--- Страница 178 ---\n176  Описание изображ ений на естественном языке Затем выполните команду pod install в терминале и откройте файл Image2Text.xcworkspace . В среде Xcode перетащите файлы ios_image_load.h , ios_image_load.mm , tensorflow_utils.h и tensorflow_utils.mm из приложения TensorFlow camera для iOS, расположенного в папке tensorflow/examples/ ios/camera проекта Image2Text . Мы уже использовали файлы ios_image_ load.* до этого, а вот файлы tensorflow_utils.* применяются здесь в основ- ном для загрузки файла модели в формате memmapped. В файле утилит tensorflow_utils.mm имеется два метода, LoadModel и LoadMemoryMappedModel : один загружает модель, как обычно, не в формате memmapped, а другой загружает модель в формате memmapped. Если вам интересно, то вы мо-жете взглянуть на реализацию метода LoadMemoryMappedModel в указанном файле. Вы также найдете полезной соответствующую документацию по адресу https://www.tensorflow.org/mobile/optimizing#reducing_model_ loading_time_andor_memory_footprint. 2. Добавьте два файла модели, которые мы создали в конце последнего раз- дела, файл word_counts.txt , сгенерированный на шаге 2 подраздела «Тре- нировка и тестирование генерирования аннотаций», а также несколько тестовых изображений – мы сохранили и используем четыре изображе- ния из верхней части страницы модели TensorFlow im2txt (https: //github. com/tensorflow/models/tree/master/research/im2txt), поэтому мы можем сравнить результаты аннотирования наших моделей с результатами, созданными моделью, предположительно натренированной на гораздо большем количестве шагов. Также переименуйте файл ViewController.m в .mm, и с этого момента с целью завершения приложения мы будем рабо- тать только с файлом контроллера ViewController.mm . Теперь ваш проект Xcode Image2Text должен выглядеть, как на рис. 6.5.\n--- Страница 179 ---\nОписание изображений на естественном языке  177 Рисунок 6.5. Настройка приложения Image2Text для iOS, также демонстрирует реализацию метода LoadMemoryMappedModel 3. Откройт е файл ViewController.mm и добавьте ряд констант Objective-C и C++ следующим образом: static NSString* MODEL_FILE = @\"image2text_frozen_transformed\"; static NSString* MODEL_FILE_MEMMAPPED = @\"image2text_frozen_transformed_memmapped\";static NSString* MODEL_FILE_TYPE = @\"pb\";static NSString* VOCAB_FILE = @\"word_counts\";static NSString* VOCAB_FILE_TYPE = @\"txt\";static NSString *image_name = @\"im2txt4.png\";const string INPUT_NODE1 = \"convert_image/Cast\";const string OUTPUT_NODE1 = \"lstm/initial_state\";const string INPUT_NODE2 = \"input_feed\";const string INPUT_NODE3 = \"lstm/state_feed\";const string OUTPUT_NODE2 = \"softmax\";const string OUTPUT_NODE3 = \"lstm/state\"; const int wanted_width = 299;\n--- Страница 180 ---\n178  Описание изображ ений на естественном языке const int wanted_height = 299; const int wanted_channels = 3; const int CAPTION_LEN = 20; const int START_ID = 2;const int END_ID = 3;const int WORD_COUNT = 12000;const int STATE_COUNT = 1024; Они все говорят сами за себя и должны выглядеть уже знакомыми, если вы следили за изложением материала в данной главе, за исключением, вероятно, последних пяти констант: CAPTION_LEN – э то максимальное ко- личество слов, которые мы хотим создать в аннотации, а START_ID – это идентификатор стартового слова предложения, <S> , определенного как номер строки в файле word_counts.txt ; так, 2 означает слово во второй строке, 3 – в третьей строке. Первые несколько строк файла word_counts. txt выглядят так: a 969108<S> 586368</S> 586368. 440479on 213612of 202290 WORD_COUNT – э то общее количество слов, которое модель допускает. По каждому вызову процедуры выведения заключения, которую вы вскоре увидите, модель возвращает в общей сложности 12 000 вероят - ностных оценок, а также 1024 значения состояния LSTM-модели. 4. Добавьте несколько глобальных переменных и сигнатуру функции: unique_ptr<tensorflow:: Session> session;unique_ptr<tensorflow:: MemmappedEnv> tf_memmapped_env; std:: vector<std:: string> words;UIImageView *_iv; UILabel *_lbl; NSString* generateCaption(bool memmapped); Этот элементарный код, связанный с пользовательским интерфейсом, аналогичен программному коду приложения для iOS в главе 2 «Класси- фицирование изображений с помощью трансферного обучения». Его суть в том, что после запуска приложения вы касаетесь в любом месте экрана и выбираете одну из двух моделей, и вверху появится результат анноти-рования изображения. Когда пользователь выбирает модель memmapped в действии alert , выполняется следующий ниже программный код:\n--- Страница 181 ---\nОписание изображений на естественном языке  179 dispatch_async(dispatch_get_global_queue(0, 0), ^{ NSString *caption = generateCaption(true); dispatch_async(dispatch_get_main_queue(), ^{ _lbl.text = caption; }); }); Если выбрана модель без memmapped, то используется функция generateCaption(false) . 5. В конце метода viewDidLoad добавьте программный код загрузки фай- ла word_counts.txt и построчно сохраните слова в векторе на языке Objective-C и C++: NSString* voc_file_path = FilePathForResourceName(VOCAB_FILE, VOCAB_FILE_TYPE); if (!voc_file_path) { LOG(FATAL) << \"Couldn't load vocabuary file: \" << voc_file_path; }ifstream t;t.open([voc_file_path UTF8String]);string line;while(t){ getline(t, line); size_t pos = line.find(\" \"); words.push_back(line.substr(0, pos)); }t.close(); 6. В остальном нам нужно только реализовать функцию generateCaption . Внутри нее сначала загрузите нужную модель: tensorflow:: Status load_status;if (memmapped) load_status = LoadMemoryMappedModel(MODEL_FILE_MEMMAPPED, MODEL_FILE_TYPE, &session, &tf_memmapped_env); else load_status = LoadModel(MODEL_FILE, MODEL_FILE_TYPE, &session); if (!load_status.ok()) { return @\"Couldn't load model\"; } 7. Зат ем примените аналогичный код обработки изображений с целью подготовки тензора изображения для подачи в модель: int image_width;int image_height;int image_channels;NSArray *name_ext = [image_name componentsSeparatedByString:@\".\"];NSString* image_path = FilePathForResourceName(name_ext[0], name_ext[1]);std:: vector<tensorflow:: uint8> image_data = LoadImageFromFile([image_path UTF8String], &image_width, &image_height, &image_channels);\n--- Страница 182 ---\n180  Описание изображ ений на естественном языке tensorflow:: Tensor image_tensor(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({wanted_ height, wanted_width, wanted_channels}));auto image_tensor_mapped = image_tensor.tensor<float, 3>();tensorflow:: uint8* in = image_data.data();float* out = image_tensor_mapped.data();for (int y = 0; y < wanted_height; ++y) { const int in_y = (y * image_height) / wanted_height; tensorflow:: uint8* in_row = in + (in_y * image_width * image_channels); float* out_row = out + (y * wanted_width * wanted_channels); for (int x = 0; x < wanted_width; ++x) { const int in_x = (x * image_width) / wanted_width; tensorflow:: uint8* in_pixel = in_row + (in_x * image_channels); float* out_pixel = out_row + (x * wanted_channels); for (int c = 0; c < wanted_channels; ++c) { out_pixel[c] = in_pixel[c]; } } } 8. Т еперь мы можем отправить изображение в модель и получить возвра- щенный тензорный вектор initial_state , содержащий 1200 значений (STATE_COUNT) : vector<tensorflow:: Tensor> initial_state; if (session.get()) { tensorflow:: Status run_status = session ‑>Run({{INPUT_NODE1, image_tensor}}, {OUTPUT_ NODE1}, {}, &initial_state); if (!run_status.ok()) { return @\"Getting initial state failed\"; } } 9. Опре делите тензоры input_feed и state_feed и назначьте их значениям соответственно идентификатор начального слова и возвращаемые зна- чения initial_state : tensorflow:: Tensor input_feed(tensorflow:: DT_INT64, tensorflow:: TensorShape({1,}));tensorflow:: Tensor state_feed(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({1, STATE_COUNT})); auto input_feed_map = input_feed.tensor<int64_t, 1>(); auto state_feed_map = state_feed.tensor<float, 2>();input_feed_map(0) = START_ID;auto initial_state_map = initial_state[0].tensor<float, 2>();for (int i = 0; i < STATE_COUNT; i++){ state_feed_map(0, i) = initial_state_map(0, i); }\n--- Страница 183 ---\nОписание изображений на естественном языке  181 10. Создайте цикл for по длине CAPTION_LEN и внутри цикла сначала создайте тензорные векторы output_feed и output_states , затем передайте в мо- дель input_feed и state_feed , которые мы установили ранее, и запустите модель. Она вернет тензорный вектор output , который состоит из тензо- ра softmax и тензора new_state : vector<int> captions; for (int i=0; i<CAPTION_LEN; i++) { vector<tensorflow:: Tensor> output; tensorflow:: Status run_status = session ‑>Run({{INPUT_NODE2, input_feed}, {INPUT_NODE3, state_feed}}, {OUTPUT_NODE2, OUTPUT_NODE3}, {}, &output); if (!run_status.ok()) { return @\"Getting LSTM state failed\"; } else { tensorflow:: Tensor softmax = output[0]; tensorflow:: Tensor state = output[1]; auto softmax_map = softmax.tensor<float, 2>(); auto state_map = state.tensor<float, 2>(); 11. Т еперь найдите идентификатор слова с наибольшей вероятностью (зна- чение softmax ). Если этот идентификатор является конечным словом, то завершите цикл for . В противном случае добавьте идентификатор слова с максимальным значением softmax в вектор captions . Обратите внимание, что здесь мы используем жадный поиск, всегда выбирая сло-ва с максимальной вероятностью, вместо лучевого поиска с размером 3, как в сценарии run_inference.py . В конце цикла for обновите значение input_feed идентификатором максимального слова и значением state_ feed с ранее возвращенным значением состояния state , перед тем как снова подать в модель два входа для получения значений softmax всех следующих слов и следующего значения состояния state : float max_prob = 0.0f;int max_word_id = 0;for (int j = 0; j < WORD_COUNT; j++){ if (softmax_map(0, j) > max_prob) { max_prob = softmax_map(0, j); max_word_id = j; } } if (max_word_id == END_ID) break; captions.push_back(max_word_id); input_feed_map(0) = max_word_id; for (int j = 0; j < STATE_COUNT; j++){\n--- Страница 184 ---\n182  Описание изображ ений на естественном языке state_feed_map(0, j) = state_map(0, j); } }  Наверное, мы еще ни разу не объясняли так подробно, как получать и устанавливать зна- чение тензора платформы TensorFlow на языке C++. Но если вы внимательно просмот - рели программный код, приведенный в книге, то теперь знаете, как это делать. Очень похоже на то, как учится RNN-сеть: если вас натренировали на достаточном объеме при- меров кода, то вы сможете писать осмысленный программный код. Подытоживая ска-занное, сначала вы определяете переменную, имеющую тип Tensor, с заданным типом данных и формой, затем вызываете метод tensor класса Tensor, передавая C++-версию типа данных и размерность формы, чтобы создать переменную-словарь для тензора. Пос ле этого вы можете использовать этот словарь для получения или установки значе- ний тензора. 12. Нак онец, просто обойдите вектор captions в цикле и конвертируйте в слово каждый хранящийся в векторе идентификатор слова, после чего добавьте слово в строковое значение sentence , игнорируя начальный и конечный идентификаторы, а затем верните предложение, которое, надо надеяться, будет на разумном естественном языке: NSString *sentence = @\"\";for (int i=0; i<captions.size(); i++) { if (captions[i] == START_ID) continue; if (captions[i] == END_ID) break; sentence = [NSString stringWithFormat:@\"%@%s\", sentence, words[captions[i]].c_str()]; } return sentence; Вот и все, что требуется для запуска модели в приложении для iOS. Теперь запустите приложение в симуляторе или на устройстве iOS, коснитесь и выбе- рите модель, как показано на рис. 6.6.\n--- Страница 185 ---\nОписание изображений на естественном языке  183 Рисунок 6.6. Выполнение приложения Image2Text для iOS и выбор модели В симуляторе iOS выполнение модели, не находящейся в формате memmapped, занимает более 10 секунд и около 5 секунд – в случае модели в фор- мате memmapped. На iPhone 6 выполнение модели в формате memmapped тоже занимает около 5 секунд, но при выполнении модели не в формате memmapped происходит аварийный сбой приложения из-за большого файла модели и не-хватки памяти. Что же касается результатов, то на рис. 6.7 показаны результаты для четырех тестовых изображений:\n--- Страница 186 ---\n184  Описание изображ ений на естественном языке Рисунок 6.7. Результаты аннотирования изображений Рисунок 6.8 показывает результаты на сайте TensorFlow im2txt, и вы можете видеть, что результаты нашего более простого жадного поиска выглядят до- вольно хорошо; однако в случае изображения жирафа, похоже, наша модель или программный код выведения заключения не умеют хорошо считать. После проделанной в этой главе работы, надо надеяться, вы немного развлечетесь во время совершенствования процесса тренировки или выведения модельного заключения:\n--- Страница 187 ---\nОписание изображений на естественном языке  185 Рисунок 6.8. Пример результатов аннотирования на веб-сайте модели TensorFlow im2txt Пришло время уделить должное внимание разработчикам на Android, перед тем как мы перейдем к нашей следующей интеллектуальной задаче. использование моДели аннотирования изображений в android Следуя тем же соображениям простоты, мы разработаем новое приложение для Android с минимальным пользовательским интерфейсом и сосредоточим- ся на том, как применять эту модель в Android. 1. Создайте новое приложение для Android под названием Image2Text , до- бавьте строку compile 'org.tensorflow: tensorflow ‑android:+' в конце секции зависимостей файла build.gradle вашего приложения. Создай-\n--- Страница 188 ---\n186  Описание изображ ений на естественном языке те папку ресурсов assets и перетащите в нее файл модели image2text_ frozen_transformed.pb , файл word_counts.txt и несколько файлов тестовых изображений. 2. Добавьте элемент управления ImageView и элемент управления Button в файл activity_main.xml : <ImageView android: id=\"@+id/imageview\" android: layout_width=\"match_parent\" android: layout_height=\"match_parent\" app: layout_constraintBottom_toBottomOf=\"parent\" app: layout_constraintHorizontal_bias=\"0.0\" app: layout_constraintLeft_toLeftOf=\"parent\" app: layout_constraintRight_toRightOf=\"parent\" app: layout_constraintTop_toTopOf=\"parent\" app: layout_constraintVertical_bias=\"1.0\"/> <Button android: id=\"@+id/button\" android: layout_width=\"wrap_content\" android: layout_height=\"wrap_content\" android: text=\"DESCRIBE ME\" app: layout_constraintBottom_toBottomOf=\"parent\" app: layout_constraintHorizontal_bias=\"0.5\" app: layout_constraintLeft_toLeftOf=\"parent\" app: layout_constraintRight_toRightOf=\"parent\" app: layout_constraintTop_toTopOf=\"parent\" app: layout_constraintVertical_bias=\"1.0\"/> > 3. Откройт е файл MainActivity.java , реализуйте в нем интерфейс Runnable , а затем добавьте следующие ниже константы, последние пять из кото- рых описаны в предыдущем разделе, в то время как другие не требуют пояснений: private static final String MODEL_FILE = \"file:///android_asset/image2text_frozen_transformed.pb\";private static final String VOCAB_FILE = \"file:///android_asset/word_counts.txt\";private static final String IMAGE_NAME = \"im2txt1.png\"; private static final String INPUT_NODE1 = \"convert_image/Cast\"; private static final String OUTPUT_NODE1 = \"lstm/initial_state\";private static final String INPUT_NODE2 = \"input_feed\";private static final String INPUT_NODE3 = \"lstm/state_feed\";private static final String OUTPUT_NODE2 = \"softmax\";private static final String OUTPUT_NODE3 = \"lstm/state\"; private static final int IMAGE_WIDTH = 299; private static final int IMAGE_HEIGHT = 299;\n--- Страница 189 ---\nОписание изображений на естественном языке  187 private static final int IMAGE_CHANNEL = 3; private static final int CAPTION_LEN = 20; private static final int WORD_COUNT = 12000;private static final int STATE_COUNT = 1024;private static final int START_ID = 2;private static final int END_ID = 3; И следующие ниже экземплярные переменные и реализацию обработ - чика: private ImageView mImageView;private Button mButton; private TensorFlowInferenceInterface mInferenceInterface; private String[] mWords = new String[WORD_COUNT];private int[] intValues;private float[] floatValues; Handler mHandler = new Handler() { @Override public void handleMessage(Message msg) { mButton.setText(\"DESCRIBE ME\"); String text = (String)msg.obj; Toast.makeText(MainActivity.this, text, Toast.LENGTH_LONG).show(); mButton.setEnabled(true); }}; 4. В методе onCreate сначала добавьте программный код, который показы- вает тестовое изображение в элементе управления ImageView и обраба- тывает событие касания кнопки: mImageView = findViewById(R.id.imageview); try { AssetManager am = getAssets(); InputStream is = am.open(IMAGE_NAME); Bitmap bitmap = BitmapFactory.decodeStream(is); mImageView.setImageBitmap(bitmap); } catch (IOException e) { e.printStackTrace(); } mButton = findViewById(R.id.button); mButton.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { mButton.setEnabled(false); mButton.setText(\"Processing \"); Thread thread = new Thread(MainActivity.this);\n--- Страница 190 ---\n188  Описание изображ ений на естественном языке thread.start(); } }); Затем добавьте программный код, который построчно считывает файл word_counts.txt и сохраняет каждое слово в массиве mWords : String filename = VOCAB_FILE.split(\"file:///android_asset/\")[1]; BufferedReader br = null;int linenum = 0;try { br = new BufferedReader(new InputStreamReader(getAssets().open(filename))); String line; while ((line = br.readLine())!= null) { String word = line.split(\" \")[0]; mWords[linenum++] = word; } br.close(); } catch (IOException e) { throw new RuntimeException(\"Problem reading vocab file!\", e); } 5. Т еперь в методе public void run() , запускаемом при возникновении события onClick кнопки DESCRIBE ME (Описать меня), добавьте про- граммный код изменения размера тестового изображения, прочитайте значения пикселов из растрового изображения с измененным размером и затем конвертируйте их в вещественные числа – мы видели такой про- граммный код в трех предыдущих главах: intValues = new int[IMAGE_WIDTH * IMAGE_HEIGHT];floatValues = new float[IMAGE_WIDTH * IMAGE_HEIGHT * IMAGE_CHANNEL]; Bitmap bitmap = BitmapFactory.decodeStream(getAssets().open(IMAGE_NAME)); Bitmap croppedBitmap = Bitmap.createScaledBitmap(bitmap, IMAGE_WIDTH, IMAGE_HEIGHT, true);croppedBitmap.getPixels(intValues, 0, IMAGE_WIDTH, 0, 0, IMAGE_WIDTH, IMAGE_HEIGHT);for (int i = 0; i < intValues.length; ++i) { final int val = intValues[i]; floatValues[i * IMAGE_CHANNEL + 0] = ((val >> 16) & 0xFF); floatValues[i * IMAGE_CHANNEL + 1] = ((val >> 8) & 0xFF); floatValues[i * IMAGE_CHANNEL + 2] = (val & 0xFF); } 6. С оздайте экземпляр TensorFlowInferenceInterface , который загружает файл модели, и выведите первое заключение с помощью модели, пере-дав ей значения изображений, а затем извлеките возвращенный резуль-тат в initialState : AssetManager assetManager = getAssets();mInferenceInterface = new TensorFlowInferenceInterface(assetManager, MODEL_FILE);\n--- Страница 191 ---\nОписание изображений на естественном языке  189 float[] initialState = new float[STATE_COUNT]; mInferenceInterface.feed(INPUT_NODE1, floatValues, IMAGE_WIDTH, IMAGE_HEIGHT, 3);mInferenceInterface.run(new String[] {OUTPUT_NODE1}, false);mInferenceInterface.fetch(OUTPUT_NODE1, initialState); 7. Присвойт е первому значению массива input_feed стартовый идентифи- катор и первому значению массива state_feed возвращенное значение initialState : long[] inputFeed = new long[] {START_ID};float[] stateFeed = new float[STATE_COUNT * inputFeed.length];for (int i=0; i < STATE_COUNT; i++) { stateFeed[i] = initialState[i]; } Как можно убедиться, благодаря реализации в Android интерфейса TensorFlowInferenceInterface получать и устанавливать значения тензо- ра и выводить заключение в Android проще, чем в iOS. Прежде чем мы начнем повторно использовать inputFeed и stateFeed , чтобы выводить заключения с помощью модели, мы создаем список captions из пар це- лых и вещественных чисел, где целое число – это идентификатор слова с максимальным значением softmax (среди всех значений softmax, воз-вращаемых моделью для каждого вызова функции выведения заключе-ния) и вещественное число – это значение softmax слова. Для хранения слова с максимальным значением softmax из каждого заключения мож - но было бы использовать простой вектор, но использование списка пар облегчает переход от метода жадного поиска к методу лучевого поиска: List<Pair<Integer, Float>> captions = new ArrayList<Pair<Integer, Float>>(); 8. В цикле for по длине аннотации мы подаем значения input_feed и state_ feed, которые мы установили выше, а затем извлекаем возвращенные значения softmax и newstate : for (int i=0; i<CAPTION_LEN; i++) { float[] softmax = new float[WORD_COUNT * inputFeed.length]; float[] newstate = new float[STATE_COUNT * inputFeed.length]; mInferenceInterface.feed(INPUT_NODE2, inputFeed, 1); mInferenceInterface.feed(INPUT_NODE3, stateFeed, 1, STATE_COUNT); mInferenceInterface.run(new String[]{OUTPUT_NODE2, OUTPUT_NODE3}, false); mInferenceInterface.fetch(OUTPUT_NODE2, softmax); mInferenceInterface.fetch(OUTPUT_NODE3, newstate); 9. Т еперь создайте еще один список пар целых и вещественных чисел, до- бавьте идентификатор и значение softmax каждого слова в список и от - сортируйте список по убыванию: List<Pair<Integer, Float>> prob_id = new ArrayList<Pair<Integer, Float>>();for (int j = 0; j < WORD_COUNT; j++) {\n--- Страница 192 ---\n190  Описание изображ ений на естественном языке prob_id.add(new Pair(j, softmax[j])); } Collections.sort(prob_id, new Comparator<Pair<Integer, Float>>() { @Override public int compare(final Pair<Integer, Float> o1, final Pair<Integer, Float> o2) { return o1.second > o2.second? ‑‑1: (o1.second == o2.second? 0: 1); } }); 10. Ес ли слово с максимальной вероятностью является концом слова, то мы прекращаем цикл; в противном случае мы добавляем пару в список captions , а затем обновляем input_feed идентификатором слова с мак - симальным значением softmax и значениями state_feed возвращенных состояний, чтобы продолжить и вывести следующее заключение: if (prob_id.get(0).first == END_ID) break;captions.add(new Pair(prob_id.get(0).first, prob_id.get(0).first));inputFeed = new long[] {prob_id.get(0).first}; for (int j=0; j < STATE_COUNT; j++) { stateFeed[j] = newstate[j]; } } 11. Нак онец, мы перебираем все пары в списке captions и добавляем каждое слово, исключая начальное и конечное, в строковое значение sentence , после чего оно возвращается через обработчик для вывода пользовате-лю результата на естественном языке: String sentence = \"\";for (int i=0; i<captions.size(); i++) { if (captions.get(i).first == START_ID) continue; if (captions.get(i).first == END_ID) break; sentence = sentence + \" \" + mWords[captions.get(i).first]; } Message msg = new Message(); msg.obj = sentence;mHandler.sendMessage(msg); Запустите приложение в виртуальном или реальном устройстве Android. На то, чтобы увидеть результат, уйдет около 10 секунд. Вы можете поэкспери-ментировать с четырьмя разными тестовыми изображениями, показанными в предыдущем разделе, и увидеть результаты на рис. 6.9.\n--- Страница 193 ---\nОписание изображений на естественном языке  191 Рисунок 6.9. Результаты аннотирования в Android Некоторые результаты немного отличаются от результатов в iOS и результа- тов на сайте TensorFlow im2txt. Но все они выглядят довольно прилично. По- мимо этого, выполнение версии модели не в формате memmapped на отно-сительно старом устройстве Android, таком как Nexus 5, работает нормально. Но было бы неплохо загрузить модель в Android в формате memmapped, чтобы увидеть значительное увеличение производительности. Эту задачу мы рас - смотрим в одной из последующих глав книги. На этом мы завершаем пошаговый процесс построения приложения для Android с использованием мощной модели аннотирования изображений. Не-зависимо от того, работаете ли вы с приложением для iOS или Android, вы смо-жете легко интегрировать нашу натренированную модель и программный код выведения заключения в свои собственные приложения или вернуться к про-цессу тренировки, чтобы выполнить тонкую настройку модели, а затем под-готовить и оптимизировать более качественную модель для использования в своих мобильных приложениях. резюме В этой главе мы сначала обсудили особенности технологии аннотирова- ния изображений, приводимой в движение современным сквозным глубоким обуче нием, а затем кратко подытожили задачу тренировки такой модели с по- мощью модельного проекта TensorFlow im2txt. Мы подробно обсудили, как найти правильные имена входных и выходных узлов, как заморозить модель, а затем использовать новейший инструмент трансформации графа и инстру - мент конвертации в формат memmapped, для того чтобы исправить некоторые неприятные ошибки при загрузке модели в мобильные устройства. После это-го мы продемонстрировали подробные инструкции по созданию приложения\n--- Страница 194 ---\n192  Описание изображ ений на естественном языке для iOS и Android с помощью данной модели и выводу новых заключений с по- мощью LSTM RNN-компонента модели. Просто невероятно, что после тренировки на десятках тысяч примеров ан- нотаций изображений и на основе современных CNN- и LSTM-сетей мы можем построить и использовать модель, которая способна генерировать разумное описание изображения на естественном языке на наших мобильных устрой-ствах. Нетрудно представить, какие полезные приложения могут быть построе - ны поверх него. Ну, разве мы не похожи на Холмса? Определенно нет. Но ведь мы на правильном пути? Надеемся. Мир ИИ чрезвычайно увлекателен и одно-временно сложен, но до тех пор, пока мы продолжаем неуклонно продвигаться вперед и улучшать свой собственный процесс познания и самообучения, из-бегая проблем исчезающего и взрывающегося градиента, есть хороший шанс, что в один прекрасный день мы сможем создать модель, похожую на Холмса, и применить ее в мобильных приложениях в любое время и в любом месте. Прочитав эту длинную главу с практическим применением нейросетевой CNN- и LSTM-моделей, мы заслуживаем некоторого развлечения. В следующей главе вы увидите, как применять еще одну CNN- и LSTM-модели для разра-ботки забавных приложений для iOS и Android, которые позволяют рисовать объекты, а затем их распознавать. Для быстрого ознакомления с онлайновой версией такой игры, и чтобы просто развлечься, перейдите по ссылке на веб-сайт https://quickdraw.withgoogle.com.",
      "debug": {
        "start_page": 159,
        "end_page": 194
      }
    },
    {
      "name": "Глава 7. Распознавание рисунков с помощью CNN- и LSTM-сетей 193",
      "content": "--- Страница 195 --- (продолжение)\nГлава 7 Распознавание рисунков с помощью CNN- и LSTM-сетей В предыдущей главе мы увидели мощь применения глубоко обучающейся мо- дели, в которой CNN-сеть интегрована с LSTM RNN-сетью для генерирования аннотации изображения на естественном языке. Если ИИ, работающий на глу - боком обучении, похож на новое электричество, то мы, безусловно, рассчи-тываем увидеть применение таких гибридных нейросетевых моделей в самых разных областях. Что будет полной противоположностью серьезному прило-жению, такому как аннотирование изображений? Ну, конечно же, забавное приложение для рисования, такое как Quick Draw (https://quickdraw.withgoogle.com, а также https://quickdraw.withgoogle.com/data), в котором применяется мо- дель, натренированная и основанная на 50 миллионах рисунков, распределен-ных по 345 категориям, и в котором выполняется классификация новых ри-сунков на эти категории. Звучит неплохо. К тому же существует официальное руководство TensorFlow (https://www.tensorflow.org/tutorials/recurrent_quickdraw), посвященное тому, как строить такую модель, что, без всякого сомнения, по-может нам осуществить быстрый старт. Оказывается, что задача применения модели, построенной с помощью это- го руководства, в iOS и Android предлагает отличную возможность: углубить наше понимание того, как выявлять правильные имена вход- ных и выходных узлов модели с целью надлежащей подготовки модели для мобильных приложений; испо льзовать дополнительные методы исправления ошибок загрузки новой модели и выведения ею заключений в iOS; впервые создать собственную библиотеку TensorFlow для Android, кото- рая позволит исправлять ошибки загрузки новой модели и ее предска-заний в Android; увидеть больше примеров того, как подавать на вход модели TensorFlow данные в ожидаемом формате, а также получать и обрабатывать полу - ченные из нее данные в iOS и Android.\nГлава 7 Распознавание рисунков с помощью CNN- и LSTM-сетей В предыдущей главе мы увидели мощь применения глубоко обучающейся мо- дели, в которой CNN-сеть интегрована с LSTM RNN-сетью для генерирования аннотации изображения на естественном языке. Если ИИ, работающий на глу - боком обучении, похож на новое электричество, то мы, безусловно, рассчи-тываем увидеть применение таких гибридных нейросетевых моделей в самых разных областях. Что будет полной противоположностью серьезному прило-жению, такому как аннотирование изображений? Ну, конечно же, забавное приложение для рисования, такое как Quick Draw (https://quickdraw.withgoogle.com, а также https://quickdraw.withgoogle.com/data), в котором применяется мо- дель, натренированная и основанная на 50 миллионах рисунков, распределен-ных по 345 категориям, и в котором выполняется классификация новых ри-сунков на эти категории. Звучит неплохо. К тому же существует официальное руководство TensorFlow (https://www.tensorflow.org/tutorials/recurrent_quickdraw), посвященное тому, как строить такую модель, что, без всякого сомнения, по-может нам осуществить быстрый старт. Оказывается, что задача применения модели, построенной с помощью это- го руководства, в iOS и Android предлагает отличную возможность: углубить наше понимание того, как выявлять правильные имена вход- ных и выходных узлов модели с целью надлежащей подготовки модели для мобильных приложений; испо льзовать дополнительные методы исправления ошибок загрузки новой модели и выведения ею заключений в iOS; впервые создать собственную библиотеку TensorFlow для Android, кото- рая позволит исправлять ошибки загрузки новой модели и ее предска-заний в Android; увидеть больше примеров того, как подавать на вход модели TensorFlow данные в ожидаемом формате, а также получать и обрабатывать полу - ченные из нее данные в iOS и Android.\n--- Страница 196 ---\n194  Распознавание рисунков с помощью CNN- и LSTM-сетей Кроме того, в процессе работы со всеми эти утомительными, но важными деталями, благодаря которым модель сможет работать как по волшебству, соз- давая классификации рисунков, вы просто получите удовольствие от рисова-ния закорючек на своих устройствах iOS и Android. Итак, в этой главе мы рассмотрим следующие темы: классификация рисунков – как э то работает; тренировка и по дготовка модели классификации рисунков; испо льзование модели классификации рисунков в iOS; применение мо дели классификации рисунков в Android. классификация рисунков – как это работает Модель классификации рисунков, встроенная в руководство TensorFlow (https://www.tensorflow.org/tutorials/recurrent_quickdraw), сначала принимает на входе пользовательский рисунок, представленный в виде списка точек, и конвертирует нормализованный ввод в тензор дельт последовательных то-чек вместе с информацией о том, является каждая точка началом нового штри-ха или нет. Затем с целью классификации пользовательского рисунка она про-пускает тензор через несколько сверточных и LSTM-слоев и, наконец, через слой softmax, как показано на рис. 7.1.\n--- Страница 197 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  195 Рисунок 7.1. Режим классификации рисунка Надписи: входные данные; последовательность точек; 1-мерные свертки; LSTM-слой; Softmax-слой; С-классы В отличие от API двумерного сверточного слоя tf.layers.conv2d , который на входе принимает 2-мерное изображение, API одномерного сверточного слоя tf.layers.conv1d используется здесь для временной свертки, в частности, рисунка. По умолчанию в модели классификации рисунков используются три одномерных сверточных слоя, каждый из которых имеет 48, 64 и 96 фильтров, длины которых соответственно равны 5, 5 и 3. После сверточных слоев соз-даются 3 LSTM-слоя с 128 прямыми узлами BasicLSTMCell и 128 обратными узлами BasicLSTMCell в расчете на слой, а затем используются для создания динамической двунаправленной рекуррентной нейронной сети, выход кото-рой отправляется в конечный полносвязный (плотный) слой для вычисления логитов (ненормализованных логарифмов вероятностей).\n--- Страница 198 ---\n196  Распознавание рисунков с помощью CNN- и LSTM-сетей  Ес ли у вас отсутствует прочное понимание всех этих подробностей, то не беспокойтесь; для разработки мощных мобильных приложений с использованием модели, построен- ной другими специалистами, вы не обязаны понимать все подробности. Однако в следу-ющей главе мы детально обсудим вопрос создания RNN-модели с нуля для предсказа-ния биржевой цены, и благодаря этому вы будете иметь более четкое понимание всего, что связано с RNN-сетью. Простая и элегантная модель и ее реализация на языке Python подробно описаны в упомянутом ранее забавном руководстве, чей исходный код рас - положен в папке tutorials/rnn/quickdraw в репозитории https: //github.com/ tensorflow/models. Единственное, что стоит отметить, прежде чем перейти к следующему разделу, – это то, что программный код построения данной модели, ее тренировки, оценивания и предсказания на ее основе, в отличие от программного кода, который вы видели в предыдущих главах, использу - ет высокоуровневый API TensorFlow под названием Estimator (https://www. tensorflow.org/api_docs/python/tf/estimator/Estimator), или, точнее, пользователь-ский оценщик Estimator . Если вы интересуетесь подробностями реализации данной модели, то обязательно должны взглянуть на руководство по созданию и использованию пользовательского оценщика (https://www.tensorflow.org/get_started/custom_estimators) и полезный исходный код, прилагаемый к данному руководству в файле models/samples/core/get_started/custom_estimator.py в папке https: //github.com/tensorflow/models репозитория. В принципе, сначала вы реа - лизу ете функцию, которая определяет вашу модель, определяет меры поте- ри и точности, а также устанавливает оптимизатор и операцию тренировки training_op , затем вы создаете экземпляр класса tf.estimator.Estimator и вы- зываете его методы train , evaluate и predict . Как вы вскоре увидите, исполь- зование оценщика Estimator упрощает построение, тренировку и выведение результата с помощью нейросетевых моделей, но поскольку это высокоуров-невый API, он также усложняет некоторые задачи более низкого уровня, такие как поиск имен входных и выходных узлов для выведения заключения на мо-бильных устройствах. тренировка , преД сказание и поДготовка моДели классификации рисунков Натренировать модель довольно просто, сложнее подготовить модель для раз-вертывания на мобильном устройстве. Перед началом тренировки обязатель-но клонируйте репозиторий модели TensorFlow ( https://github.com/ tensorflow/ models ) в корневой каталог TensorFlow, как мы делали в предыдущих двух гла- вах. Затем скачайте тренировочный набор данных классификации рисунков с http: //download.tensorflow.org/data/quickdraw_tutorial_dataset_v1.tar.gz, кото - рый составляет около 1,1 Гб. Создайте новую папку с именем rnn_ tutorial_ data и распакуйте в нее архив tar.gz . Вы увидите 10 тренировочных файлов\n--- Страница 199 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  197 TFRecord и 10 оценочных файлов TFRecord, а также два файла с расширением .classes , имеющих одинаковое содержимое и представляющих собой простой текст для 345 категорий, которые можно использовать для классификации на- бора данных на такие категории, как «sheep» (овцы), «skull» (череп), «donut» (пончик) и «apple» (яблоко). тренировка моДели классификации рисунков Для того чтобы натренировать модель, откройте терминал, потом командой cd перейдите в папку tensorflow/models/tutorials/rnn/quickdraw , а затем выполните следующий ниже сценарий: python train_model.py \\ ‑‑training_data=rnn_tutorial_data/training.tfrecord ‑?????‑of‑????? \\ ‑‑eval_data=rnn_tutorial_data/eval.tfrecord ‑?????‑of‑????? \\ ‑‑model_dir quickdraw_model/ \\ ‑‑classes_file=rnn_tutorial_data/training.tfrecord.classes По умолчанию тренировочные шаги составляют 100k, и на нашем GTX 1070 GPU на них уходит порядка 6 часов, чтобы закончить тренировку до конца. Пос - ле завершения тренировки в каталоге модели вы увидите хорошо знакомый список файлов (остальные четыре набора файлов model.ckpt* пропущены): ls ‑lt quickdraw_model/ ‑rw‑rw‑r‑‑ 1 jeff jeff 164419871 Feb 12 05:56 events.out.tfevents.1518422507.AiLabby ‑rw‑rw‑r‑‑ 1 jeff jeff 1365548 Feb 12 05:56 model.ckpt‑100000.meta ‑rw‑rw‑r‑‑ 1 jeff jeff 279 Feb 12 05:56 checkpoint ‑rw‑rw‑r‑‑ 1 jeff jeff 13707200 Feb 12 05:56 model.ckpt‑100000.data‑00000 ‑of‑00001 ‑rw‑rw‑r‑‑ 1 jeff jeff 2825 Feb 12 05:56 model.ckpt‑100000.index ‑rw‑rw‑r‑‑ 1 jeff jeff 2493402 Feb 12 05:47 graph.pbtxt drwxr‑xr‑x 2 jeff jeff 4096 Feb 12 00:11 eval Если выполнить команду tensorboard ‑‑logdir quickdraw_model и запустить ло- кальный сервер TensorBoard по адресу http://localhost:6006 в браузере, то вы увидите, что точность составляет около 0,55, а потеря – около 2,0. Если вы про- должите тренировку в течение еще около 200k шагов, то точность улучшится примерно до 0,65, а потеря снизится до 1,3, как показано на рис. 7.2.\n--- Страница 200 ---\n198  Распознавание рисунков с помощью CNN- и LSTM-сетей Рисунок 7.2. Точность и потеря модели после 300k шагов тренировки Теперь можно запустить инструмент заморозки freeze_graph.py , как мы делали в предыдущей главе, с целью создания файла модели для мобильных устройств. Но прежде чем мы это сделаем, давайте сначала посмотрим, как эта модель используется на языке Python для выведения нескольких заключений, аналогично тому, как мы делали со сценарием run_inference.py в предыдущей главе. преДсказание с помощью моДели классификации рисунков Взгляните на файл train_model.py в папке models/tutorial/rnn/quickdraw . Когда он начинает выполняться, в функции create_estimator_and_specs создается эк - земпляр оценщика Estimator : estimator = tf.estimator.Estimator( model_fn=model_fn, config=run_config, params=model_params) Ключевым параметром, передаваемым в класс Estimator , является модель- ная функция под названием model_fn , которая определяет: функции для получения входного тензора и создания сверточных слоев, RNN-слоев и конечного слоя; программный к од вызова этих функций для построения модели; потерю, оптимизатор и предсказание. Прежде чем вернуть экземпляр tf.estimator.EstimatorSpec , функция model_fn также имеет параметр mode , который может иметь одно из следующих трех зна- чений: tf.estimator.ModeKeys.TRAIN ; tf.estimator.ModeKeys.EVAL ; tf.estimator.ModeKeys.PREDICT .\n--- Страница 201 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  199 Сценарий train_model.py реализован таким образом, что он поддерживает режимы TRAIN и EVAL, но вы не можете непосредственно выводить заключение (классифицировать рисунок) с конкретным рисунком на входе. Для того чтобы проверить предсказание с помощью конкретных входных данных, выполните следующие ниже действия. 1. Сделайте копию сценария train_model.ру и переименуйте новый файл в predict.py – так вы сможете поэкспериментировать с предсказанием более свободно. 2. В файле predict.py определите входную функцию для предсказания с пе- ременной features , представляющей данные рисунка (дельты последова- тельных точек, где третье число указывает на то, является точка началом штриха или нет), ожидаемые моделью на входе: def predict_input_fn(): def _input_fn(): features = {'shape': [[16, 3]], 'ink': [[ ‑‑ 0.23137257, 0.31067961, 0., ‑‑ 0.05490196, 0.1116505, 0., 0.00784314, 0.09223297, 0., 0.19215687, 0.07766992, 0., 0.12156862, 0.05825245, 0., 0., ‑‑0.06310678, 1., 0., 0., 0., 0., 0., 0., ]]} features['shape'].append(features['shape'][0]) features['ink'].append(features['ink'][0]) features=dict(features) dataset = tf.data.Dataset.from_tensor_slices(features) dataset = dataset.batch(FLAGS.batch_size) return dataset.make_one_shot_iterator().get_next() return _input_fn Мы не показываем все значения точек, но они создаются с использова-нием демонстрационного примера с данными для слова cat, приводимо-го в руководстве TensorFlow «RNN for Drawing Classification» (RNN-сеть для классификации рисунков), в котором применена функция parse_line (за подробностями обратитесь к руководству или сценарию create_ dataset.py в папке models/tutorials/rnn/quickdraw ). Кроме того, обратите внимание на то, что мы используем метод make_ one_shot_iterator объекта tf.data.Dataset для создания однократного итератора, который возвращает пример из набора данных (в данном\n--- Страница 202 ---\n200  Распознавание рисунков с помощью CNN- и LSTM-сетей случае у нас всего один пример в наборе данных). Работая с большим набором данных, модель получает данные во время тренировки и оце-нивания точно таким же образом – именно поэтому позже вы увидите операцию OneShotIterator на графе модели. 3. В функции main вызовите метод predict оценщика, который генерирует предсказания для заданного признака, а затем распечатайте предсказа-ние, как показано ниже: predictions = estimator.predict(input_fn=predict_input_fn())print(next(predictions)['argmax']) 4. В функции model_fn после инструкции logits = _add_fc_layers(final_state) добавьте следующий ниже программный код: argmax = tf.argmax(logits, axis=1) if mode == tf.estimator.ModeKeys.PREDICT: predictions = { 'argmax': argmax, 'softmax': tf.nn.softmax(logits), 'logits': logits, } return tf.estimator.EstimatorSpec(mode, predictions=predictions) Теперь, если выполнить сценарий predict.py , вы получите идентификатор класса с максимальным значением, предназначенным для входных данных на шаге 2. Располагая базовым пониманием того, как выполнять предсказание с по - мощью модели, построенной с использованием высокоуровневого API Estimator , мы теперь готовы выполнить заморозку модели, которая позволит ее использовать на мобильных устройствах, а это требует от нас сначала вы-яснить, какими должны быть имена выходных узлов. поДготовка моДели классификации рисунков Давайте воспользуемся сервером TensorBoard, чтобы увидеть, что мы мо-жем найти. В разделе графов GRAPHS просмотровой области нашей модели в TensorBoard вы видите, как показано на рис. 7.3, что выделенный красным цветом узел BiasAdd является входом в операцию ArgMax, используемую для расчета точности, а также входом в операцию softmax. Для инструмента freeze_ graph мы можем использовать операцию SparseSof tmaxCrossEntropyWithLogits (на рис. 7.3 она показана сокращенно как SparseSiftnaxCr…) или же dense/BiasAdd в качестве имени выходного узла, однако, для того чтобы четче видеть выход конечного плотного слоя, а также результат ArgMax, мы будем использо-вать ArgMax и dense/BiasAdd в качестве имен двух выходных узлов:\n--- Страница 203 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  201 Рисунок 7.3. Возможные имена выходных узлов модели Выполните следующий ниже сценарий в вашем корневом каталоге TensorFlow для получения замороженного графа, не забыв подставить в зна- чения параметров ‑‑input_graph и ‑‑input_checkpoint путь вашего файла graph. pbtxt и префикс последней контрольной точки модели: python tensorflow/python/tools/freeze_graph.py ‑‑input_graph=/tmp/graph.pbtxt ‑‑input_ checkpoint=/tmp/model.ckpt‑314576 ‑‑output_graph=/tmp/quickdraw_frozen_dense_biasadd_ argmax.pb ‑‑output_node_names=\"dense/BiasAdd, ArgMax\" Вы увидите, что будет успешно создан файл модели quickdraw_frozen_dense_ biasadd_argmax.pb. Однако если вы попробуете загрузить модель в свой iOS или Android, то получите сообщение об ошибке Could not create TensorFlow Graph: Not found: Op type not registered 'OneShotIterator' in binary. Make sure the Op and Kernel are registered in the binary running in this process . В предыдущем подразделе мы уже говорили о том, что означает однократный итератор OneShotIterator . Возвращаясь назад к разделу TensorBoard GRAPHS, мы видим, как показано на рис. 7.4, что однократный итератор OneShotIterator выделен красным цветом, а также показан в правой информационной панели. На пару уровней выше имеется одна операция изменения формы Reshape , ко- торая служит входом в первый сверточный слой:\n--- Страница 204 ---\n202  Распознавание рисунков с помощью CNN- и LSTM-сетей Рисунок 7.4. Выяснение возможных имен входных узлов  Вы, возможно, задаетесь вопросом, почему нельзя просто исправить ошибку Not found: Op type not registered 'OneShotIterator' тем методом, который мы использовали раньше, то есть сначала выяснить, какой исходный файл содержит определение операции (op), применив команду grep 'REGISTER.*\" OneShotIterator\"'tensorflow/core/ops/*.cc (и уви- дев результат как tensorflow/core/ops/dataset_ops.cc: REGISTER_OP(\"OneShotIterator\") ), а затем добавив строку tensorflow/core/ops/dataset_ops.cc в файл tf_op_files.txt и собрав библиотеку TensorFlow заново. Даже если бы этот прием был возможен, то он только усложнил бы все решение, так как сейчас нам нужно подавать в модель не кон-кретный рисунок пользователя в виде точек, а некие данные, связанные с однократным итератором OneShotIterator . Кроме того, еще одним уровнем выше на правой стороне (рис. 7.5) имеет - ся еще одна операция, Squeeze , которая действует как вход для подграфа rnn_ classification :\n--- Страница 205 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  203 Рисунок 7.5. Дальнейшее исследование для определения имен входных узлов Нас не интересует операция Shape , которая находится справа от Reshape , поскольку она фактически является выходом из подграфа rnn_classification . Поэтому основная идея всего этого исследования заключается в том, что мы можем использовать Reshape и Squeeze как два входных узла, а затем с по- мощью инструмента transform_graph , который мы видели в предыдущей главе, у нас должно получиться исключить узлы ниже Reshape и Squeeze , в том числе OneShotIterator . Теперь выполните следующие ниже команды в корневом каталоге TensorFlow: bazel‑bin/tensorflow/tools/graph_transforms/transform_graph ‑‑in_graph=/tmp/quickdraw_ frozen_dense_biasadd_argmax.pb ‑‑out_graph=/tmp/quickdraw_frozen_strip_transformed.pb ‑‑inputs=\"Reshape, Squeeze\" ‑‑outputs=\"dense/BiasAdd, ArgMax\" ‑‑transforms=' strip_unused_nodes(name=Squeeze, type_for_name=int64, shape_for_name=\"8\", name=Reshape, type_for_name=float, shape_for_name=\"8,16,3\")' Здесь мы используем более расширенный формат функции strip_unused_ nodes : для каждого имени входного узла ( Reshape и Squeeze ) мы указываем его конкретный тип и форму, чтобы позже избежать ошибки загрузки мо-дели. Дополнительные сведения о функции strip_unused_nodes инструмента transform_graph вы можете найти в его документации по адресу https: //github. com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms. Теперь при загрузке модели в iOS или Android ошибка OneShotIterator ис- чезнет. Но как вы уже, вероятно, привыкли ожидать, возникает новая ошибка, говорящая о несовместимости типов: Could not create TensorFlow Graph: Invalid argument: Input 0 of node IsVariableInitialized was passed int64 from global_step:0 incompatible with expected int64_ref .\n--- Страница 206 ---\n204  Распознавание рисунков с помощью CNN- и LSTM-сетей Сначала нам нужно узнать больше об операции IsVariableInitialized . Если мы вернемся на вкладку GRAPHS сервера TensorBoard, то увидим, что с левой стороны имеется операция IsVariableInitialized , выделенная красным цветом и показанная в информационной панели справа, где в качестве входных дан- ных выступает global_step (рис. 7.6). Даже если мы не знаем точно, для чего она используется, мы можем сказать наверняка, что она не имеет ничего общего с выводимым моделью заключе-нием, которая просто-напросто ожидает некоторых данных на входе (рис. 7.4 и рис. 7.5) и генерирует классификацию рисунков на выходе (рис. 7.3). Рисунок 7.6. Выяснение узлов, вызывающих ошибку загрузки модели, но не связанных с выведением моделью заключения Итак, каким же образом можно избавиться от global_step , а также от других родственных узлов cond , которые из-за их изолированности не будут удалены инструментом transform graph ? К счастью, сценарий freeze_graph поддержива- ет обработку такой ситуации, и это задокументировано в его исходном коде на https: //github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/ freeze_graph.py. Мы можем применить параметр variable_names_blacklist сце- нария, чтобы указать узлы, которые должны быть удалены в замороженной модели: python tensorflow/python/tools/freeze_graph.py ‑‑input_graph=/tmp/graph.pbtxt ‑‑input_ checkpoint=/tmp/model.ckpt‑314576 ‑‑output_graph=/tmp/quickdraw_frozen_long_blacklist.pb ‑‑output_node_names=\"dense/BiasAdd, ArgMax\" ‑‑variable_names_blacklist=\"IsVariableInitializ ed, global_step, global_step/Initializer/zeros, cond/pred_id, cond/read/Switch, cond/read, cond/Switch_1, cond/Merge\" Здесь мы просто выводим список узлов внутри областей действия global_ step и cond. Теперь снова запустите инструмент transform_graph :\n--- Страница 207 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  205 bazel‑bin/tensorflow/tools/graph_transforms/transform_graph ‑‑in_graph=/tmp/quickdraw_ frozen_long_blacklist.pb ‑‑out_graph=/tmp/quickdraw_frozen_long_blacklist_strip_ transformed.pb ‑‑inputs=\"Reshape, Squeeze\" ‑‑outputs=\"dense/BiasAdd, ArgMax\" ‑‑transforms=' strip_unused_nodes(name=Squeeze, type_for_name=int64, shape_for_name=\"8\", name=Reshape, type_for_name=float, shape_for_name=\"8,16,3\")' Загрузите результирующий файл модели quickdraw_frozen_long_ blacklist_ strip_transformed.pb в iOS или Android, и вы больше не увидите ошибку IsVariableInitialized . И конечно же, вы увидите еще одну ошибку – как в iOS, так и в Android. Загрузка указанной выше модели приведет вот к такой ошибке: Couldn't load model: Invalid argument: No OpKernel was registered to support Op 'RefSwitch' with these attrs. Registered devices: [CPU], Registered kernels: device='GPU'; T in [DT_FLOAT] device='GPU'; T in [DT_INT32]device='GPU'; T in [DT_BOOL]device='GPU'; T in [DT_STRING]device='CPU'; T in [DT_INT32]device='CPU'; T in [DT_FLOAT]device='CPU'; T in [DT_BOOL] [[Node: cond/read/Switch = RefSwitch[T=DT_INT64, _class=[\"loc:@global_step\"], _output_ shapes=[[], []]](global_step, cond/pred_id)]] Для того чтобы исправить эту ошибку, мы должны выполнить сборку поль- зовательских библиотек TensorFlow для iOS и Android, в каждом случае по-разному. Прежде чем мы обсудим, как это сделать в следующих разделах, посвященных iOS и Android, давайте сначала сделаем еще одну вещь: конвер-тируем модель в версию в формате memmapped, что позволит загружать ее быстрее и потреблять меньше памяти в iOS: bazel‑bin/tensorflow/contrib/util/convert_graphdef_memmapped_format \\ ‑‑in_graph=/tmp/quickdraw_frozen_long_blacklist_strip_transformed.pb \\ ‑‑out_graph=/tmp/quickdraw_frozen_long_blacklist_strip_transformed_memmapped.pb использование моДели классификации рисунков в ios Для того чтобы исправить упомянутую выше ошибку RefSwitch, которая будет происходить независимо от того, используете ли вы модуль TensorFlow, как мы это делали в главе 2 «Классифицирование изображений с помощью трансферно-го обучения» и в главе 6 «Описание изображений на естественном языке», либо библиотеку TensorFlow ручной сборки, как в других главах, мы должны задей-ствовать какой-то новый трюк. Данная ошибка возникает из-за того, что для операции RefSwitch требуется тип данных Int64, но он не является одним из за-регистрированных типов данных, встроенных в библиотеку TensorFlow, по-скольку по умолчанию, с целью максимального сокращения размера библио -\n--- Страница 208 ---\n206  Распознавание рисунков с помощью CNN- и LSTM-сетей теки, включены только распространенные типы данных. Мы можем исправить эту ситуацию на языке Python со стороны построения модели, но здесь мы по-кажем вам, как это исправить со стороны iOS, что может оказаться полезным, когда у вас нет доступа к исходному коду построения модели. сборка пользовательской библиотеки Tensor Flow Для ios Откройте файл Makefile из tensorflow/contrib/makefile/Makefile и затем, если вы используете TensorFlow 1.4, отыщите раздел IOS_ARCH . Для каждой архитектуры (5 в общей сложности: ARMv7, ARMV7S, ARM64, I386, x86_64) измените значение ‑D__ANDROID_TYPES_SLIM__ на ‑D__ANDROID_TYPES_FULL__ . Файл Makefile в TensorFlow 1.5 (или 1.6/1.7) немного отличается, хотя он по-прежнему находится в той же папке. В случае версий 1.5/1.6/1.7 найдите значение ANDROID_TYPES_SLIM и по- меняйте на ANDROID_TYPES_FULL . Теперь выполните повторную сборку библио- теки TensorFlow, запустив ценарий оболочки tensorflow/contrib/makefile/build_ all_ios.sh. После этого при загрузке файла модели ошибка Refswitch исчезнет. Размер приложения, собранного с помощью библиотеки TensorFlow с полной поддержкой типов данных, составит около 70 Мб, по сравнению с размером приложения 3 7 Мб при сборке с типами данных slim по умолчанию. В довершение ко всему во время загрузки модели возникнет еще одна ошибка: Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op'RandomUniform' with these attrs. Registered devices: [CPU], Registered kernels: <no registeredkernels>. К счастью, вы должны быть уже хорошо знакомы с тем, как исправлять по- добного рода ошибку, если вы прочитали предыдущие главы. Вот краткое из-ложение: сначала выясните, какие файлы операций и ядра (op и kernel) опре-деляют и реализуют операцию, а затем проверьте, включен ли файл операций или ядра в файл tf_op_files.txt. Хотя бы один должен отсутствовать, что как раз и вызывает ошибку. Теперь просто добавьте файл операций или ядра в файл tf_op_files.txt и выполните повторную сборку библиотеки. В нашем случае выполните следующие ниже команды: grep RandomUniform tensorflow/core/ops/*.ccgrep RandomUniform tensorflow/core/kernels/*.cc И на выходе вы увидите вот эти файлы: tensorflow/core/ops/random_grad.cctensorflow/core/ops/random_ops.cc:tensorflow/core/kernels/random_op.cc\n--- Страница 209 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  207 Из этого списка файл tensorflow/contrib/makefile/tf_op_files.txt имеет толь- ко первые два элемента, поэтому просто добавьте последний, tensorflow/core/ kernels/random_op.cc , в конец файла tf_op_files.txt и выполните сценарий обо- лочки tensorflow/contrib/makefile/build_all_ios.sh еще раз. Наконец-таки, все ошибки при загрузке модели исчезнут, и мы можем на- чать получать удовольствие от реализации логики приложения для обработки рисунков пользователей, конвертирования точек в ожидаемый моделью фор-мат и получения результатов классификации. разработка приложения Для ios с использованием моДели Создайте новый проект Xcode на языке Objective-C, затем перетащите файлы tensorflow_util.h и tensorflow_util.mm из проекта iOS Image2Text , соз - данного в предыдущей главе. Кроме того, перетащите два файла модели, quickdraw_ frozen_long_blacklist_strip_transformed.pb и quickdraw_frozen_long_ blacklist_strip_transform и файл training.tfrecord.classes из папки models/ tutorials/rnn/quickdraw/rnn_tutorial_data в проект QuickDraw и переименуйте файл training.tfrecord.classes в classes.txt . Также переименуйте файл ViewController.m в ViewController.mm и закоммен- тируйте определение функции GetTopN в файле tensorflow_util.h и его реализа- цию в файле tensorflow_util.mm , поскольку мы реализуем модифицированную версию контроллера ViewController.mm . Теперь ваш проект должен выглядеть, как показано на рис. 7.7. Рисунок 7.7. Проект QuickDraw в среде Xcode с исходным содержимым файла ViewController.mm\n--- Страница 210 ---\n208  Распознавание рисунков с помощью CNN- и LSTM-сетей Теперь нам осталось доработать один только контроллер ViewController.mm , и на этом мы нашу миссию закончим. 1. После установки основных констант и переменных и двух прототипов функций, как показано на рис. 7.6, в методе viewDidLoad контроллера ViewController инстанцируйте элементы управления пользовательского интерфейса UIButton , UILabel и UIImageView . Каждый элемент управления был задан с несколькими ограничениями NSLayoutConstraint (для получе- ния полного листинга обратитесь к репозиторию исходного кода). Про- граммный код, связанный с элементом управления UIImageView , выгля- дит следующим образом: iv = [[UIImageView alloc] init];_iv.contentMode = UIViewContentModeScaleAspectFit;[_iv setTranslatesAutoresizingMaskIntoConstraints: NO];[self.view addSubview:_iv]; Элемент управления UIImageView будет использоваться для отображения рисунка пользователя с помощью UIBezierPath . Кроме того, инициали- зируйте два массива, используемых для хранения каждой последующей точки и всех точек, нарисованных пользователем: _allPoints = [NSMutableArray array];_consecutivePoints = [NSMutableArray array]; 2. Пос ле касания кнопки с начальной надписью «Start» пользователь мо- жет приступить к рисованию, при этом надпись на кнопке изменится на «Restart», а также выполнится еще несколько переопределений: ‑ (IBAction)btnTapped:(id)sender { _canDraw = YES; [_btn setTitle:@\"Restart\" forState: UIControlStateNormal]; [_lbl setText:@\"\"]; _iv.image = [UIImage imageNamed:@\"\"]; [_allPoints removeAllObjects]; } 3. Д ля обработки рисунка пользователя мы сначала реализуем метод touchesBegan : ‑ (void) touchesBegan:(NSSet *)touches withEvent:(UIEvent *)event { if (!_canDraw) return; [_consecutivePoints removeAllObjects]; UITouch *touch = [touches anyObject]; CGPoint point = [touch locationInView: self.view]; [_consecutivePoints addObject:[NSValue valueWithCGPoint: point]]; _iv.image = [self createDrawingImageInRect:_iv.frame]; } Затем метод touchesMoved :\n--- Страница 211 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  209 ‑ (void) touchesMoved:(NSSet *)touches withEvent:(UIEvent *)event { if (!_canDraw) return; UITouch *touch = [touches anyObject]; CGPoint point = [touch locationInView: self.view]; [_consecutivePoints addObject:[NSValue valueWithCGPoint: point]]; _iv.image = [self createDrawingImageInRect:_iv.frame]; } И наконец, метод touchesEnd : ‑ (void) touchesEnded:(NSSet *)touches withEvent:(UIEvent *)event { if (!_canDraw) return; UITouch *touch = [touches anyObject]; CGPoint point = [touch locationInView: self.view]; [_consecutivePoints addObject:[NSValue valueWithCGPoint: point]]; [_allPoints addObject:[NSArray arrayWithArray:_consecutivePoints]]; [_consecutivePoints removeAllObjects]; _iv.image = [self createDrawingImageInRect:_iv.frame]; dispatch_async(dispatch_get_global_queue(0, 0), ^{ std:: string classes = getDrawingClassification(_allPoints); dispatch_async(dispatch_get_main_queue(), ^{ NSString *c = [NSString stringWithCString: classes.c_str() encoding:[NSString defaultCStringEncoding]]; [_lbl setText: c]; }); }); } Программный код здесь довольно понятен, за исключением двух мето-дов, createDrawingImageInRect и getDrawingClassification , которые мы рас - смотрим далее. 4. Мет од createDrawingImageInRect использует методы moveToPoint и addLineToPoint объекта UIBezierPath для вывода на экран рисунка поль- зователя. Сначала по событиям касания он подготавливает все завер-шенные штрихи, при этом все точки хранятся в массиве _allPoints : ‑ (UIImage *)createDrawingImageInRect:(CGRect)rect { UIGraphicsBeginImageContextWithOptions(CGSizeMake(rect.size.width, rect.size.height), NO, 0.0); UIBezierPath *path = [UIBezierPath bezierPath]; for (NSArray *cp in _allPoints) { bool firstPoint = TRUE; for (NSValue *pointVal in cp) { CGPoint point = pointVal.CGPointValue; if (firstPoint) { [path moveToPoint: point];\n--- Страница 212 ---\n210  Распознавание рисунков с помощью CNN- и LSTM-сетей firstPoint = FALSE; } else [path addLineToPoint: point]; } } Затем он подготавливает все точки текущего незавершенного штриха, хранящиеся в массиве _consecutivePoints : bool firstPoint = TRUE;for (NSValue *pointVal in _consecutivePoints) { CGPoint point = pointVal.CGPointValue; if (firstPoint) { [path moveToPoint: point]; firstPoint = FALSE; } else [path addLineToPoint: point]; } Наконец, он выполняет фактическую прорисовку и возвращает рисунок как UIImage , который будет показан с помощью элемента управления UIImageView : path.lineWidth = 6.0;[[UIColor blackColor] setStroke];[path stroke]; UIImage *image = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext();return image;} 5. Мет од GetDrawingClassification сначала использует тот же самый про- граммный код, что и в предыдущей главе, который служит для загрузки модели или ее версии в формате memmapped: std:: string getDrawingClassification(NSMutableArray *allPoints) { if (!_modelLoaded) { tensorflow:: Status load_status; if (USEMEMMAPPED) { load_status = LoadMemoryMappedModel(MODEL_FILE_MEMMAPPED, MODEL_FILE_TYPE, &tf_session, &tf_memmapped_env); } else { load_status = LoadModel(MODEL_FILE, MODEL_FILE_TYPE, &tf_session); }\n--- Страница 213 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  211 if (!load_status.ok()) { LOG(FATAL) << \"Couldn't load model: \" << load_status; return \"\"; } _modelLoaded = YES; } Затем он получает общее количество точек и выделяет массив ве- щественных чисел, перед тем как вызвать еще одну функцию normalizeScreenCoordinates , которую мы вскоре рассмотрим, для того что- бы конвертировать точки в ожидаемый моделью формат: if ([allPoints count] == 0) return \"\";int total_points = 0;for (NSArray *cp in allPoints) { total_points += cp.count; } float *normalized_points = new float[total_points * 3]; normalizeScreenCoordinates(allPoints, normalized_points); Далее мы определяем имена входных и выходных узлов и создаем тен-зор, содержащий общее количество точек: std:: string input_name1 = \"Reshape\";std:: string input_name2 = \"Squeeze\";std:: string output_name1 = \"dense/BiasAdd\";std:: string output_name2 = \"ArgMax\"const int BATCH_SIZE = 8;tensorflow:: Tensor seqlen_tensor(tensorflow:: DT_INT64, tensorflow:: TensorShape({BATCH_SIZE}));auto seqlen_mapped = seqlen_tensor.tensor<int64_t, 1>();int64_t* seqlen_mapped_data = seqlen_mapped.data();for (int i=0; i<BATCH_SIZE; i++) { seqlen_mapped_data[i] = total_points; } Обратите внимание, что мы должны использовать тот же размер паке-та BATCH_SIZE , что и BATCH_SIZE при запуске сценария train_model.py для тренировки модели. По умолчанию он равняется 8. Здесь создается еще один тензор, содержащий все конвертированные значения точек: tensorflow:: Tensor points_tensor(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({8, total_points, 3}));auto points_tensor_mapped = points_tensor.tensor<float, 3>();float* out = points_tensor_mapped.data();for (int i=0; i<BATCH_SIZE; i++) { for (int j=0; j<total_points*3; j++) out[i*total_points*3+j] = normalized_points[j]; }\n--- Страница 214 ---\n212  Распознавание рисунков с помощью CNN- и LSTM-сетей 6. Т еперь запускаем модель и получаем ожидаемые результаты: std:: vector<tensorflow:: Tensor> outputs; tensorflow:: Status run_status = tf_session ‑>Run({{input_name1, points_tensor}, {input_ name2, seqlen_tensor}}, {output_name1, output_name2}, {}, &outputs);if (!run_status.ok()) { LOG(ERROR) << \"Getting model failed:\" << run_status; return \"\"; } tensorflow:: string status_string = run_status.ToString(); tensorflow:: Tensor* logits_tensor = &outputs[0]; 7. Применяем модифицированную версию функции GetTopN и разбираем логиты для получения самых лучших результатов: const int kNumResults = 5;const float kThreshold = 0.1f;std:: vector<std:: pair<float, int> > top_results;const Eigen:: TensorMap<Eigen:: Tensor<float, 1, Eigen:: RowMajor>, Eigen:: Aligned>& logits = logits_tensor ‑>flat<float>(); GetTopN(logits, kNumResults, kThreshold, &top_results);string result = \"\";for (int i=0; i<top_results.size(); i++) { std:: pair<float, int> r = top_results[i]; if (result == \"\") result = classes[r.second]; else result += \", \" + classes[r.second]; } 8. Изменяем GetTopN путем конвертации значений логитов в значения softmax и возвращаем значения softmax с их позициями: float sum = 0.0;for (int i = 0; i < CLASS_COUNT; ++i) { sum += expf(prediction(i)); } for (int i = 0; i < CLASS_COUNT; ++i) { const float value = expf(prediction(i)) / sum; if (value < threshold) { continue; } top_result_pq.push(std:: pair<float, int>(value, i)); if (top_result_pq.size() > num_results) { top_result_pq.pop(); } }\n--- Страница 215 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  213 9. Наконец, функция normalizeScreenCoordinates конвертирует все точки ко- ординат экрана, фиксируемые в событиях касания, в дельта-разницы. Ее реализация в значительной степени дублирует метод Python parse_line в https: //github.com/tensorflow/models/blob/master/tutorials/rnn/quickdraw/ create_dataset.py: void normalizeScreenCoordinates(NSMutableArray *allPoints, float *normalized) { float lowerx=MAXFLOAT, lowery=MAXFLOAT, upperx= ‑‑MAXFLOAT, uppery= ‑‑MAXFLOAT; for (NSArray *cp in allPoints) { for (NSValue *pointVal in cp) { CGPoint point = pointVal.CGPointValue; if (point.x < lowerx) lowerx = point.x; if (point.y < lowery) lowery = point.y; if (point.x > upperx) upperx = point.x; if (point.y > uppery) uppery = point.y; } } float scalex = upperx ‑‑ lowerx; float scaley = uppery ‑‑ lowery; int n = 0; for (NSArray *cp in allPoints) { int m=0; for (NSValue *pointVal in cp) { CGPoint point = pointVal.CGPointValue; normalized[n*3] = (point.x ‑‑ lowerx) / scalex; normalized[n*3+1] = (point.y ‑‑ lowery) / scaley; normalized[n*3+2] = (m ==cp.count‑1? 1: 0); n++; m++; } } for (int i=0; i<n‑1; i++) { normalized[i*3] = normalized[(i+1)*3] ‑‑ normalized[i*3]; normalized[i*3+1] = normalized[(i+1)*3+1] ‑‑ normalized[i*3+1]; normalized[i*3+2] = normalized[(i+1)*3+2]; } } Теперь вы можете запустить приложение в симуляторе или на устройстве iOS, начать рисовать и увидеть, что модель думает о том, что вы рисуете. На рис. 7.8 показано несколько рисунков и результатов классификации – не са- мые лучшие рисунки, но весь процесс работает, как по маслу!\n--- Страница 216 ---\n214  Распознавание рисунков с помощью CNN- и LSTM-сетей Рисунок 7.8. Рисунок и результаты классификации в iOS использование моДели классификации рисунков в android Самое время посмотреть, как загружать и применять модель в Android. В пре- дыдущих главах мы добавляли поддержку платформы TensorFlow путем ис - пользования файла build.gradle приложений Android и добавления строки compile 'org.tensorflow: tensorflow ‑android:+' . По сравнению с iOS, где нам при- ходится выполнять сборку собственной библиотеки TensorFlow, которая тре-бовалась для исправления различных ошибок загрузки или запуска модели (например, в главе 3 «Обнаружение и локализация объектов», главе 4 «Трансфор- мирование рисунков с помощью художественных стилей» и в главе 5 «Понимание простых речевых команд»), используемая по умолчанию библиотека TensorFlow для Android имеет улучшенную поддержку зарегистрированных операций и типов данных, которые могут быть, поскольку Android является для Google гражданином первого сорта, тогда как iOS – второго сорта, хоть и находящимся на втором месте. Вся суть в том, что когда мы работаем со всевозможными удивительными моделями в реальном мире, столкновение с неизбежным становится просто вопросом времени, и мы предстаем перед необходимостью выполнять сборку библиотеки TensorFlow для Android вручную, для того чтобы исправить какие-то ошибки, которые библиотека TensorFlow по умолчанию просто не в состоя-нии обработать. И ошибка No OpKernel was registered to support Op 'RefSwitch' with these attrs – о дна из таких ошибок. Для оптимистичных разработчиков это просто означает еще одну возможность добавить новые трюки в свой ар-сенал навыков.\n--- Страница 217 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  215 сборка пользовательской библиотеки Tensor Flow Для android Выполните следующие далее действия, чтобы вручную собрать пользователь- скую библиотеку TensorFlow для Android. 1. В корневом каталоге TensorFlow есть файл WORKSPACE . Отредактируйте в нем параметры android_sdk_repository и android_ndk_repository следу - ющим образом (замените версию build_tools_version и пути SDK и NDK на свои собственные): android_sdk_repository( name = \"androidsdk\", api_level = 23, build_tools_version = \"26.0.1\", path = \"$HOME/Library/Android/sdk\", ) android_ndk_repository( name=\"androidndk\", path=\"$HOME/Downloads/android ‑ndk‑r15c\", api_level=14) 2. Ес ли вы также работали с приводимыми в книге приложениями для iOS и поменяли в файле tensorflow/core/platform/default/mutex.h директивы #include \"nsync_cv.h\" и #include \"nsync_mu.h\" на директивы #include \"nsync/ public/nsync_cv.h\" и #include \"nsync/public/nsync_mu.h\" , как показано в гла- ве 3 «Обнаружение и локализация объектов», то вам необходимо вернуть их в прежний вид, для того чтобы вы успешно смогли выполнить сбор-ку библиотеки TensorFlow в Android (позднее, когда вы будете работать в Xcode над приложением для iOS, используя библиотеку TensorFlow руч-ной сборки, вам нужно перед обоми заголовками добавить nsync/public ).  Вне сение изменений и их отмена в файле tensorflow/core/platform/default/mutex.h , разумеется, не является идеальным решением. Предполагалось, что это будет просто обходным маневром. Поскольку файл должен быть изменен, только когда вы начинаете использовать библиотеку TensorFlow ручной сборки в iOS или же когда вы выполняете сборку пользовательской библиотеки TensorFlow, мы пока можем с этим смириться. 3. Выпо лните следующую далее команду, чтобы создать собственную биб - лиотеку TensorFlow в том случае, если ваш виртуальный эмулятор или устройство Android поддерживают процессор x86: bazel build ‑c opt ‑‑copt=\"‑D__ANDROID_TYPES_FULL__\" //tensorflow/contrib/android: libtensorflow_inference.so \\ ‑‑crosstool_top=//external: android/crosstool \\ ‑‑host_crosstool_top=@bazel_tools//tools/cpp: toolchain \\ ‑‑cpu=x86_64\n--- Страница 218 ---\n216  Распознавание рисунков с помощью CNN- и LSTM-сетей Если же ваше устройство Android поддерживает armeabi-v7a, как и боль- шинство устройств Android, то выполните вот эту команду: bazel build ‑c opt ‑‑copt=\"‑D__ANDROID_TYPES_FULL__\" //tensorflow/contrib/android: libtensorflow_inference.so \\ ‑‑crosstool_top=//external: android/crosstool \\ ‑‑host_crosstool_top=@bazel_tools//tools/cpp: toolchain \\ ‑‑cpu=armeabi ‑v7a  При использовании библиотеки ручной сборки в приложении для Android необходимо сообщить приложению, для каких наборов инструкций CPU так называемого двоично-го интерфейса приложения (Application Binary Interface, ABI) библиотека создается. ОС Android поддерживает две основные категории ABI: ARM и X86, и armeabi-v7a является самым популярным ABI в Android. Для того чтобы узнать, какой ABI использует ваше устройство или эмулятор, выполните команду adb -s <идентификатор_устройства> shell getprop ro.product.cpu.abi . Например, эта команда возвращает armeabi-v7a для моего планшета Nexus 7 и x86_64 для моего эмулятора. Если ваш виртуальный эмулятор поддерживает x86_64, то, возможно, вы захотите собрать обе категории с целью быстрого тестирования в эму - ляторе во время разработки и окончательного тестирования произво-дительности на устройстве.После завершения сборки вы увидите, что файл нативной библиоте-ки TensorFlow libtensorflow_inference.so будет сгенерирован в папке bazel‑bin/tensorflow/contrib/android . Перетащите его в папку приложе- ния в android/app/src/main/jniLibs/armeabi ‑v7a или android/app/src/main/ jniLibs/x86_64 , как показано на рис. 7.9. Рисунок 7.9. Файл нативной библиотеки TensorFlow 4. Произве дите сборку интерфейса Java для собственной библиотеки TensorFlow, выполнив команду: bazel build //tensorflow/contrib/android: android_tensorflow_inference_java В результате в папке bazelbin/tensorflow/contrib/android будет создан файл libandroid_tensorflow_inference_java.jar . Переместите файл в папку android/app/ lib, как показано на рис. 7.10.\n--- Страница 219 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  217 Рисунок 7.10. Интерфейсный файл Java для библиотеки TensorFlow Теперь мы готовы запрограммировать и протестировать модель в Android. разработка приложения Для android с целью применения моДели Выполните следующие ниже действия, чтобы создать новое приложение для Android с привлечением библиотеки TensorFlow и модели, которую мы создали ранее. 1. В Android Studio создайте новое приложение для Android с именем QuickDraw , приняв все значения по умолчанию. Затем в файле build. gradle приложения добавьте строку compile files('libs/libandroid_tensorflow_ inference_java.jar') в конце секции зависимостей. Создайте новую папку ресурсов assets , как мы делали раньше, и перетащите туда файл модели quickdraw_frozen_long_blacklist_strip_transformed.pb и файл classes.txt . 2. Создайте новый класс Java с именем QuickDrawView , который расширяет класс View , и задайте поля и его конструктор следующим образом: public class QuickDrawView extends View { private Path mPath; private Paint mPaint, mCanvasPaint; private Canvas mCanvas; private Bitmap mBitmap; private MainActivity mActivity; private List<List<Pair<Float, Float>>> mAllPoints = new ArrayList<List<Pair<Float, Float>>>(); private List<Pair<Float, Float>> mConsecutivePoints = new ArrayList<Pair<Float, Float>>(); public QuickDrawView(Context context, AttributeSet attrs) { super(context, attrs); mActivity = (MainActivity) context; setPathPaint(); } Массив mAllPoints используется для хранения списка последовательных точек mConsecutivePoints . Элемент управления QuickDrawView используется в макете основного действия, для того чтобы показывать рисунок поль-зователя.\n--- Страница 220 ---\n218  Распознавание рисунков с помощью CNN- и LSTM-сетей 3. Опре делите метод setPathPaint следующим образом: private void setPathPaint() { mPath = new Path(); mPaint = new Paint(); mPaint.setColor(0xFF000000); mPaint.setAntiAlias(true); mPaint.setStrokeWidth(18); mPaint.setStyle(Paint.Style.STROKE); mPaint.setStrokeJoin(Paint.Join.ROUND); mCanvasPaint = new Paint(Paint.DITHER_FLAG); } Добавьте два важнейших метода создания объектов растрового изобра- жения Bitmap и холста Canvas и покажите рисунок пользователя на хол- сте: @Overrideprotected void onSizeChanged(int w, int h, int oldw, int oldh) { super.onSizeChanged(w, h, oldw, oldh); mBitmap = Bitmap.createBitmap(w, h, Bitmap.Config.ARGB_8888); mCanvas = new Canvas(mBitmap); } @Override protected void onDraw(Canvas canvas) { canvas.drawBitmap(mBitmap, 0, 0, mCanvasPaint); canvas.drawPath(mPath, mPaint); } 4. Переопре деленный метод onTouchEvent используется для заполнения массивов mConsecutivePoints и mAllPoints , вызова метода drawPath холста, инвалидации рисунка (вызов метода onDraw ) и (всякий раз, когда штрих завершается действием MotionEvent.ACTION_UP ) запуска нового потока, в котором используется модель классификации рисунков: @Overridepublic boolean onTouchEvent(MotionEvent event) { if (!mActivity.canDraw()) return true; float x = event.getX(); float y = event.getY(); switch (event.getAction()) { case MotionEvent.ACTION_DOWN: mConsecutivePoints.clear(); mConsecutivePoints.add(new Pair(x, y)); mPath.moveTo(x, y); break; case MotionEvent.ACTION_MOVE: mConsecutivePoints.add(new Pair(x, y));\n--- Страница 221 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  219 mPath.lineTo(x, y); break; case MotionEvent.ACTION_UP: mConsecutivePoints.add(new Pair(x, y)); mAllPoints.add(new ArrayList<Pair<Float, Float>> (mConsecutivePoints)); mCanvas.drawPath(mPath, mPaint); mPath.reset(); Thread thread = new Thread(mActivity); thread.start(); break; default: return false; } invalidate(); return true; } 5. Опре делите два публичных метода, которые будут вызываться классом MainActivity для получения всех точек и обнуления рисунка после каса- ния пользователем кнопки перезапуска Restart: public List<List<Pair<Float, Float>>> getAllPoints() { return mAllPoints; } public void clearAllPointsAndRedraw() { mBitmap = Bitmap.createBitmap(mBitmap.getWidth(), mBitmap.getHeight(), Bitmap.Config.ARGB_8888); mCanvas = new Canvas(mBitmap); mCanvasPaint = new Paint(Paint.DITHER_FLAG); mCanvas.drawBitmap(mBitmap, 0, 0, mCanvasPaint); setPathPaint(); invalidate(); mAllPoints.clear(); } 6. Т еперь откройте класс MainActivity , назначьте ему реализацию Runnable и добавьте туда следующие ниже поля: public class MainActivity extends AppCompatActivity implements Runnable { private static final String MODEL_FILE = \"file:///android_asset/quickdraw_frozen_long_ blacklist_strip_transformed.pb\" private static final String CLASSES_FILE = \"file:///android_asset/classes.txt\"; private static final String INPUT_NODE1 = \"Reshape\"; private static final String INPUT_NODE2 = \"Squeeze\"; private static final String OUTPUT_NODE1 = \"dense/BiasAdd\";\n--- Страница 222 ---\n220  Распознавание рисунков с помощью CNN- и LSTM-сетей private static final String OUTPUT_NODE2 = \"ArgMax\"; private static final int CLASSES_COUNT = 345; private static final int BATCH_SIZE = 8; private String[] mClasses = new String[CLASSES_COUNT]; private QuickDrawView mDrawView; private Button mButton; private TextView mTextView; private String mResult = \"\"; private boolean mCanDraw = false; private TensorFlowInferenceInterface mInferenceInterface; 7. В главном макетном файле activity_main.xml создайте элемент управ- ления QuickDrawView в дополнение к элементам управления TextView и Button , как мы делали раньше: <com.ailabby.quickdraw.QuickDrawView android: id=\"@+id/drawview\" android: layout_width=\"fill_parent\" android: layout_height=\"fill_parent\" app: layout_constraintBottom_toBottomOf=\"parent\" app: layout_constraintLeft_toLeftOf=\"parent\" app: layout_constraintRight_toRightOf=\"parent\" app: layout_constraintTop_toTopOf=\"parent\"/> 8. Вернит есь в класс MainActivity и в его методе onCreate свяжите иденти- фикаторы элементов пользовательского интерфейса с полями, настрой- те слушателя касаний для кнопки Start/Restart, а затем прочитайте файл classes.txt в строковый массив: @Overrideprotected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); mDrawView = findViewById(R.id.drawview); mButton = findViewById(R.id.button); mTextView = findViewById(R.id.textview); mButton.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { mCanDraw = true; mButton.setText(\"Restart\"); mTextView.setText(\"\"); mDrawView.clearAllPointsAndRedraw(); } });\n--- Страница 223 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  221 String classesFilename = CLASSES_FILE.split(\"file:///android_asset/\")[1]; BufferedReader br = null; int linenum = 0; try { br = new BufferedReader(new InputStreamReader(getAssets().open(classesFilename))); String line; while ((line = br.readLine())!= null) { mClasses[linenum++] = line; } br.close(); } catch (IOException e) { throw new RuntimeException(\"Problem reading classes file!\", e); } } 9. Зат ем вызовите синхронизированный метод classifyDrawing из метода run поточного объекта Thread: public void run() { classifyDrawing(); } private synchronized void classifyDrawing() { try { double normalized_points[] = normalizeScreenCoordinates(); long total_points = normalized_points.length / 3; float[] floatValues = new float[normalized_points.length*BATCH_SIZE]; for (int i=0; i<normalized_points.length; i++) { for (int j=0; j<BATCH_SIZE; j++) floatValues[j*normalized_points.length + i] = (float)normalized_points[i]; } long[] seqlen = new long[BATCH_SIZE]; for (int i=0; i<BATCH_SIZE; i++) seqlen[i] = total_points; Метод normalizeScreenCoordinates , который будет реализован чуть ниже, конвертирует точки рисунка пользователя в ожидаемый моделью фор- мат. Значения floatValues и seqlen будут поступать в модель в качестве входных данных. Обратите внимание, что здесь мы должны использо-вать вещественный тип float для floatValues и тип long для seqlen , поскольку модель ожидает на входе именно этих типов данных (float и int64). В противном случае при использовании модели возникнет ошибка выполнения. 10. Создайте интерфейс Java к библиотеке TensorFlow, чтобы загрузить мо- дель, передать данные на вход модели и получить данные на выходе из нее:\n--- Страница 224 ---\n222  Распознавание рисунков с помощью CNN- и LSTM-сетей AssetManager assetManager = getAssets(); mInferenceInterface = new TensorFlowInferenceInterface(assetManager, MODEL_FILE); mInferenceInterface.feed(INPUT_NODE1, floatValues, BATCH_SIZE, total_points, 3); mInferenceInterface.feed(INPUT_NODE2, seqlen, BATCH_SIZE); float[] logits = new float[CLASSES_COUNT * BATCH_SIZE]; float[] argmax = new float[CLASSES_COUNT * BATCH_SIZE]; mInferenceInterface.run(new String[] {OUTPUT_NODE1, OUTPUT_NODE2}, false); mInferenceInterface.fetch(OUTPUT_NODE1, logits);mInferenceInterface.fetch(OUTPUT_NODE1, argmax); 11. Нормализуйт е полученные вероятности logits и отсортируйте их в убы- вающем порядке: double sum = 0.0;for (int i=0; i<CLASSES_COUNT; i++) sum += Math.exp(logits[i]); List<Pair<Integer, Float>> prob_idx = new ArrayList<Pair<Integer, Float>>();for (int j = 0; j < CLASSES_COUNT; j++) { prob_idx.add(new Pair(j, (float)(Math.exp(logits[j]) / sum))); } Collections.sort(prob_idx, new Comparator<Pair<Integer, Float>>() { @Override public int compare(final Pair<Integer, Float> o1, final Pair<Integer, Float> o2) { return o1.second > o2.second? ‑‑1: (o1.second == o2.second? 0: 1); } }); Получите пять самых лучших результатов и покажите их в элементе управления TextView : mResult = \"\";for (int i=0; i<5; i++) { if (prob_idx.get(i).second > 0.1) { if (mResult == \"\") mResult = \"\" + mClasses[prob_idx.get(i).first]; else mResult = mResult + \", \" + mClasses[prob_idx.get(i).first]; } } runOnUiThread( new Runnable() { @Override public void run() { mTextView.setText(mResult); } });\n--- Страница 225 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  223 12. Наконец, реализуйте метод normalizeScreenCoordinates , который был прос - то перенесен из реализации для iOS: private double[] normalizeScreenCoordinates() { List<List<Pair<Float, Float>>> allPoints = mDrawView.getAllPoints(); int total_points = 0; for (List<Pair<Float, Float>> cp: allPoints) { total_points += cp.size(); } double[] normalized = new double[total_points * 3]; float lowerx=Float.MAX_VALUE, lowery=Float.MAX_VALUE, upperx= ‑‑Float.MAX_VALUE, uppery= ‑‑Float.MAX_VALUE; for (List<Pair<Float, Float>> cp: allPoints) { for (Pair<Float, Float> p: cp) { if (p.first < lowerx) lowerx = p.first; if (p.second < lowery) lowery = p.second; if (p.first > upperx) upperx = p.first; if (p.second > uppery) uppery = p.second; } } float scalex = upperx ‑‑ lowerx; float scaley = uppery ‑‑ lowery; int n = 0; for (List<Pair<Float, Float>> cp: allPoints) { int m = 0; for (Pair<Float, Float> p: cp) { normalized[n*3] = (p.first ‑‑ lowerx) / scalex; normalized[n*3+1] = (p.second ‑‑ lowery) / scaley; normalized[n*3+2] = (m ==cp.size() ‑1? 1: 0); n++; m++; } } for (int i=0; i<n‑1; i++) { normalized[i*3] = normalized[(i+1)*3] ‑‑ normalized[i*3]; normalized[i*3+1] = normalized[(i+1)*3+1] ‑ normalized[i*3+1]; normalized[i*3+2] = normalized[(i+1)*3+2]; } return normalized; } Запустите приложение в эмуляторе или на устройстве Android и наслаждай- тесь нанесением закорючек и результатами их классификации. Вы должны увидеть что-то вроде того, что изображено на рис. 7.11.\n--- Страница 226 ---\n224  Распознавание рисунков с помощью CNN- и LSTM-сетей Рисунок 7.11. Рисунок и результаты классификации в Android Теперь, когда вы увидели весь процесс тренировки быстрой модели рисова- ния и использовали ее в своих приложениях для iOS и Android, вы, безусловно, можете отрегулировать параметры тренировки, чтобы сделать модель точнее, а также усовершенствовать мобильные приложения и в результате получать больше удовольствия. Последний совет, прежде чем мы закончим увлекательную экскурсию по этой главе. Если вы создадите собственную библиотеку TensorFlow для Android с неправильным ABI, вы все равно сможете создавать и запускать при-ложение из Android Studio, но вы получите ошибку времени выполнения java. lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK . Это означает, что у вас отсутствует правильная нативная библиотека TensorFlow в папке jniLibs вашего приложения (рис. 7.9). Для того чтобы узнать, отсутствует ли файл в конкретной подпапке ABI внутри jniLibs , вы можете открыть Device File Explorer (Файловый менеджер устрой- ства) из Android Studio | View (Вид) | Tool Windows (Инс трументальные окна), а затем последовательно выбрать data | app | package | lib, как показано на рис. 7.12. Если же вы предпочитаете командную строку, то, для того чтобы это узнать, вы можете воспользоваться инструментом adb.\n--- Страница 227 ---\nРаспознавание рисунков с помощью CNN- и LSTM-сетей  225 Рис. 7.12. Проверка наличия файла нативной библиотеки TensorFlow с помощью файлового менеджера устройства резюме В этой главе мы сначала дали краткое описание работы модели классифика- ции рисунков, а затем обсудили вопрос тренировки такой модели с по мощью высокоуровневого API TensorFlow Estimator. Мы также обсудили вопрос на-писания программного кода Python для создания предсказаний с помощью натренированной модели, а затем подробно рассмотрели способы поис - ка правильных имен входных и выходных узлов и правильной заморозки и трансформации модели с целью их использования в мобильных приложе-ниях. Мы также предложили новый метод сборки новой пользовательской библиотеки TensorFlow для iOS и пошаговое руководство по сборке пользо-вательской библиотеки TensorFlow для Android с целью исправления ошибок времени выполнения при использовании модели. Наконец, мы показали программный код iOS и Android, который захватывает и показывает рису - нок пользователя, конвертирует его в ожидаемые моделью данные, а также обрабатывает и представляет результаты классификации, возвращаемые мо делью. Надеюсь, вы узнали столько же, сколько получили удовольствия во время этой долгой экскурсии. До сих пор, кроме пары моделей из других проектов с открытым исходным кодом, все модели, которые мы использовали в наших приложениях для iOS и Android, предварительно натренированные либо вторично натренированные нами, исходили из проекта TensorFlow с открытым исходным кодом, который, безусловно, предлагает широкий список мощных моделей, тренировка некото-рых из которых выполнялась в течение многих недель на мощных GPU. Однако если вы заинтересованы в построении собственной модели с нуля, а также если вы еще немного путаетесь в работе мощной модели на основе RNN-сети и ее понятиях, использованных и применявшихся в этой главе, то следующая гла-ва станет именно той, которая вам будет нужна: мы расскажем о том, как по-строить свою собственную RNN-модель с нуля и использовать ее в мобильных приложениях, для того чтобы получить еще один вид занимательного время-\n--- Страница 228 ---\n226  Распознавание рисунков с помощью CNN- и LSTM-сетей препровождения – получения дохода от торговли на фондовом рынке, – или, по крайней мере, мы попробуем применить все наши силы, для того чтобы это было именно так. Конечно, никто не может гарантировать, что вы всегда будете зарабатывать деньги с каждой биржевой сделки, но давайте хотя бы по-смотрим, как наша RNN-модель сможет помочь поднять наши шансы.",
      "debug": {
        "start_page": 195,
        "end_page": 228
      }
    },
    {
      "name": "Глава 8. Предсказание биржевой цены с помощью RNN-сети 227",
      "content": "--- Страница 229 --- (продолжение)\nГлава 8 Предсказание биржевой цены с помощью RNN-сети Если в предыдущей главе вы получили удовольствие от рисования закорючек и построения (а также выполнения) модели распознавания рисунков на мо-бильных устройствах, то вам, возможно, так же понравится зарабатывать деньги на фондовом рынке или же серьезно задуматься, когда и если у вас не получится. С одной стороны, биржевые цены представляют собой данные временных рядов, то есть последовательности дискретных данных, и самый лучший метод глубокого обучения, способный обращаться с данными времен-ных рядов, – это рекуррентная нейронная сеть (RNN), которую мы использова- ли в последних двух главах. Орелиен Жерон (Aurelien Geron) в своем бестселле-ре «Hands-On Machine Learning with ScikitLearn and TensorFlow» («Практическое машинное обучение с помощью Scikit-Learn и TensorFlow») предложил исполь-зовать RNN-сеть для «анализа данных временных рядов, таких как биржевые цены, и предоставления рекомендаций в отношении того, когда следует вхо-дить в сделку на покупку или продажу». С другой стороны, иные исследовате-ли считают, что исторические ценовые отметки, достигнутые биржевым акти-вом, не могут предсказывать его будущую доходность, и случайно выбранный инвес тиционный портфель будет приносить такой же результат, что и тот, который был тщательно отобран экспертами. На самом деле Франсуа Шолле (Francois Chollet), автор библиотеки Keras, очень популярной высокоуровневой библиотеки глубокого обучения, работающей поверх TensorFlow, и нескольких других библиотек, в своем бестселлере «Deep Learning with Python» («Глубокое обучение с помощью Python») заявил, что применять RNN-сеть только с обще-доступными данными с целью выигрыша на фондовых рынках «очень сложно, и вы, вероятно, потратите свое время и ресурсы, так ничего и не добившись». Таким образом, с риском «скорее всего» потратить наши время и ресурсы впустую, но с уверенностью, что мы, по крайней мере, узнаем больше о RNN-сети и почему существует или отсутствует возможность предсказывать бир-жевую цену лучше, чем случайная стратегия с доходностью 50%, мы прежде всего дадим краткий обзор того, как использовать RNN-сеть для предсказания\nГлава 8 Предсказание биржевой цены с помощью RNN-сети Если в предыдущей главе вы получили удовольствие от рисования закорючек и построения (а также выполнения) модели распознавания рисунков на мо-бильных устройствах, то вам, возможно, так же понравится зарабатывать деньги на фондовом рынке или же серьезно задуматься, когда и если у вас не получится. С одной стороны, биржевые цены представляют собой данные временных рядов, то есть последовательности дискретных данных, и самый лучший метод глубокого обучения, способный обращаться с данными времен-ных рядов, – это рекуррентная нейронная сеть (RNN), которую мы использова- ли в последних двух главах. Орелиен Жерон (Aurelien Geron) в своем бестселле-ре «Hands-On Machine Learning with ScikitLearn and TensorFlow» («Практическое машинное обучение с помощью Scikit-Learn и TensorFlow») предложил исполь-зовать RNN-сеть для «анализа данных временных рядов, таких как биржевые цены, и предоставления рекомендаций в отношении того, когда следует вхо-дить в сделку на покупку или продажу». С другой стороны, иные исследовате-ли считают, что исторические ценовые отметки, достигнутые биржевым акти-вом, не могут предсказывать его будущую доходность, и случайно выбранный инвес тиционный портфель будет приносить такой же результат, что и тот, который был тщательно отобран экспертами. На самом деле Франсуа Шолле (Francois Chollet), автор библиотеки Keras, очень популярной высокоуровневой библиотеки глубокого обучения, работающей поверх TensorFlow, и нескольких других библиотек, в своем бестселлере «Deep Learning with Python» («Глубокое обучение с помощью Python») заявил, что применять RNN-сеть только с обще-доступными данными с целью выигрыша на фондовых рынках «очень сложно, и вы, вероятно, потратите свое время и ресурсы, так ничего и не добившись». Таким образом, с риском «скорее всего» потратить наши время и ресурсы впустую, но с уверенностью, что мы, по крайней мере, узнаем больше о RNN-сети и почему существует или отсутствует возможность предсказывать бир-жевую цену лучше, чем случайная стратегия с доходностью 50%, мы прежде всего дадим краткий обзор того, как использовать RNN-сеть для предсказания\n--- Страница 230 ---\n228  Предсказание биржевой цены с помощью RNN-сети биржевой цены, затем обсудим вопрос построения RNN-модели с помощью API TensorFlow, которая будет предсказывать биржевую цену, и рассмотрим построение LSTM RNN-модели с помощью более легкого в использовании API Keras для той же цели. Мы проверим, смогут ли такие модели превзойти слу - чайную стратегию покупки или продажи. Если мы почувствуем, что наши мо-дели поднимают наши шансы одержать победу на фондовом рынке, да и прос - то ради ноу-хау, то мы глянем на то, как выполнить заморозку и подготовку этих моделей TensorFlow и Keras для работы в приложениях для iOS и Android. Если модель способна поднять наши шансы, то наши мобильные приложения, работающие с этими моделями, смогут помогать нам при любом удобном слу - чае, когда мы захотим принять решение о покупке или продаже. Чувствуете легкий мандраж и неуверенность? Добро пожаловать на фондовый рынок. Резюмируя, в этой главе мы рассмотрим следующие ниже темы: RNN-с еть и предсказание биржевой цены: что это такое и как это дела- ется; испо льзование RNN API TensorFlow для предсказания биржевой цены; применение RNN LSTM API Keras для предсказания биржевой цены; выпо лнение моделей TensorFlow и Keras в iOS; выпо лнение моделей TensorFlow и Keras в Android. rnn- сеть и преД сказание биржевой цены – что это такое и как это Делается Сети прямого распространения сигнала, такие как полносвязные (плотные) сети, не имеют памяти и рассматривают каждый вход как единое целое. На-пример, изображение, представленное в виде вектора пикселов, обрабатыва-ется сетью прямого распространения сигнала за один шаг. Но данные времен-ных рядов, такие как биржевые цены за последние 10 или 20 дней, лучше всего обрабатываются сетью с памятью. Допустим, что цены за последние 10 дней представлены как X 1, X2, …, X10, причем X1 является самой старой ценой, а X10 – самой последней, тогда все 10-дневные цены можно рассматривать как один вход-последовательность, и когда RNN-сеть обрабатывает такой вход, выпол-няются следующие шаги. 1. Конкретная RNN-ячейка, связанная с первым элементом X1 в последова- тельности, обрабатывает X1 и получает выход y1. 2. Еще одна RNN-ячейка, связанная со следующим элементом, Х2, во вход- ной последовательности, использует Х2 и предыдущий выход, y1, чтобы получить следующий выход, y2. 3. Данный процесс повторяется: при использовании RNN-ячейки для обра- ботки элемента Хi во входной последовательности на временном шаге i предыдущий выход, уi–1, на временном шаге i – 1 используется совместно с Xi, для того чтобы сгенерировать новый выход, yi, на временном шаге i.\n--- Страница 231 ---\nПредсказание биржевой цены с помощью RNN-сети  229 Таким образом, каждый выход yi на временном шаге i содержит информа- цию обо всех элементах входной последовательности вплоть до временного шага i включительно: X1, X2,…, Xi-1 и Xi. Во время тренировки RNN-сети пред- сказанные цены, y1, y2,…, y9 и y10, на каждом временном шаге сравниваются с истинными целевыми ценами на каждом временном шаге, то есть с X2, X3,…, X10 и X11, и, следовательно, функция потери определяется и используется для оптимизации при обновлении параметров сети. После того как тренировка за-вершена, X 11 используется во время этапа предсказания в качестве предсказа- ния для входной последовательности, X1, X2,…, X10. Именно поэтому мы говорим, что RNN-сеть имеет память. И похоже, есть разумное зерно в том, чтобы применить RNN-сеть к данным биржевых цен, потому что в интуитивном плане биржевая цена сегодня (завтра, послезавтра и т. д.), по всей вероятности, будет зависеть от цен в предыдущие N дней. LSTM-сеть – это просто-напросто один из типов RNN-сети, решающий ши- роко известную проблему исчезающего градиента в RNN-сети, с которой мы познакомились в главе 6 «Описание изображений на естественном языке». В принципе, если во время тренировки модели на основе RNN-сети временные шаги входной последовательности для RNN-сети слишком длинные, то при ис - пользовании алгоритма обратного распространения для обновления сетевых весов более раннего временного шага можно получить значения градиента, равные 0, в результате чего сеть учиться не будет. Например, когда в качестве входных данных мы используем цены за 50 дней и если 50 или даже 40 вре-менных шагов оказываются слишком продолжительным сроком, то обычную RNN-сеть натренировать невозможно. Эта проблема решается с помощью LSTM-сети путем добавления долгосрочного состояния, которое решает, какая информация может быть отброшена, а какая должна сохраняться и перено-ситься через многие временные шаги.  Еще один тип RNN-сети, который хорошо решает проблему исчезающего градиента, на- зывается вентильным рекуррентным блоком (Gated Recurrent Unit, GRU), который не-много упрощает стандартные модели на основе LSTM-сети и становится все более по-пулярным. И API платформы TensorFlow, и API библиотеки Keras поддерживают базовые модели на основе RNN- и LSTM/GRU-сетей. В следующих двух разделах вы увидите конкретные API TensorFlow и Keras для использования RNN-сети и стандартной LSTM-сети, и вы получите возможность просто менять «LSTM» на «GRU» в программном коде и сравнивать результаты использования модели на основе GRU с моделями на основе RNN и LSTM. Общепринято использовать три метода, которые делают LSTM-модели ре- зультативнее: стековая укладка LSTM-слоев (stacking) и увеличение числа нейронов в слоях: это обычно приводит к более мощным и точным сетевым мо-делям, в случае если отсутствует переподгонка к тренировочным дан-ным. Если вы еще не «поиграли» на спортивной площадке TensorFlow\n--- Страница 232 ---\n230  Предсказание биржевой цены с помощью RNN-сети (http://playground.tensorflow.org), то вы определенно должны это сделать, чтобы ее прочувствовать; испо льзование отсева (dropout) для решения проблемы переподгонки. Отсев означает случайное исключение скрытых и входных блоков в слое; применение двунаправленных RNN-сетей, которые обрабатывают каж - дую входную последовательность в двух направлениях (прямое направ-ление и обратное), надеясь обнаружить закономерности, которые могут быть упущены из виду в случае с обычной однонаправленной RNN-сетью. Все эти методы реализованы и могут быть легко доступны как в API TensorFlow, так и в API Keras. Так как же нам проверить возможность предсказания биржевой цены с по- мощью RNN- и LSTM-сетей? На основе бесплатного API по адресу https://www. alphavantage.co мы извлечем ежедневные данные о биржевой цене определен-ного символа (тикера) биржевого актива, разберем данные на тренировочный и тестовый наборы, затем будем непрерывно подавать пакеты тренировочных данных (каждый с 20 временными шагами, то есть с ценами за 20 дней под-ряд) в RNN/LSTM-модель, натренируем и протестируем модель и потом по-смотрим, насколько точной такая модель может оказаться на тестовом наборе данных. Мы протестируем оба API: и TensorFlow, и Keras – и сравним различия между обычными RNN- и LSTM-моделями. Мы также проведем тестирование с участием трех несколько отличающихся входных и выходных последователь-ностей, чтобы увидеть, какая из них является лучшей, по следующим направ-лениям: предсказание цены на один день вперед на основе последних N дней; предсказание цены на M дней вперед на основе последних N дней; предсказание на основе сдвига последних N дней на один временной шаг и использование последнего результата предсказанной последова-тельности в качестве цены, предсказываемой на следующий день. Теперь давайте погрузимся в API TensorFlow RNN и программирование. Наша задача – натренировать модель, способную предсказывать биржевую цену, и затем посмотреть, насколько точной она может быть. использование aPi T ensor Flow rnn Для преД сказания биржевой цены Прежде всего вам нужно получить бесплатный ключ API по адресу https://www. alphavantage.co, который даст вам возможность получать данные биржевых цен любого символа/тикера. После того как вы получите свой ключ API, откройте терминал и выполните следующую ниже команду (после замены <ваш_ключ_api> на свой собственный ключ), чтобы получить ежедневные данные об ак - циях Amazon (amzn) и Google (goog), либо подставьте любой другой символ, который вас интересует:\n--- Страница 233 ---\nПредсказание биржевой цены с помощью RNN-сети  231 curl -o daily_amzn.csv \"https://www.alphavantage.co/query?function=TIME_SERIES_ DAILY&symbol=amzn&apikey=<ваш_ключ_api>&datatype=csv&outputsize=full\" curl -o daily_goog.csv \"https://www.alphavantage.co/query?function=TIME_SERIES_ DAILY&symbol=goog&apikey=<ваш_ключ_api>&datatype=csv&outputsize=full\" В результате будут сгенерированы файлы daily_amzn.csv и daily_goog.csv , в ко - торых верхняя строка будет выступать в качестве заголовка в виде «timestamp, open, high, low, close, volume» (метка времени, цена открытия, максимальная цена, минимальная цена, цена закрытия, объем), а остальные строки будут со-держать ежедневную информацию об акциях. Нас интересуют только цены за-крытия (close), поэтому выполните следующую ниже команду, чтобы получить все цены закрытия: cut -d ',' -f 5 daily_amzn.csv | tail -n +2 > amzn.txt cut -d ',' -f 5 daily_goog.csv | tail -n +2 > goog.txt По состоянию на 26 февраля 2018 года количество строк в файлах amzn.txt и goog.txt составляло соответственно 4566 и 987, которые означают количество дней, когда торговались акции компаний Amazon или Google. Теперь давайте посмотрим на полный программный код Python, в котором используется API RNN платформы TensorFlow для тренировки и предсказания с помощью дан-ной модели. тренировка rnn- моДели в Tensor Flow 1. Импор тируйте необходимые пакеты Python и определите несколько констант: import numpy as npimport tensorflow as tffrom tensorflow.contrib.rnn import *import matplotlib.pyplot as plt num_neurons = 100 num_inputs = 1num_outputs = 1symbol = 'goog' # amznepochs = 500seq_len = 20learning_rate = 0.001 Numpy (http: //www.numpy.org) – это самая популярная библиотека Python для работы с n-мерными массивами, и Matplotlib (https: //matplotlib. org) – ведущая библиотека Python для построения 2D-графиков. Мы будем использовать библиотеку numpy для обработки набора данных и библиотеку Matplotlib для визуализации биржевой цены и предсказа-\n--- Страница 234 ---\n232  Предсказание биржевой цены с помощью RNN-сети ний. Константа num_neurons – э то количество нейронов, которые ячей- ка RNN-сети (RNN-ячейка) имеет на каждом временном шаге – каждый нейрон получает в качестве входного элемента входную последователь- ность на текущем временном шаге и выход из предыдущего временного шага. Константы num_inputs и num_outputs определяют количество входов и выходов на каждом временном шаге. На каждом временном шаге мы будем подавать одну биржевую цену из 20-дневной входной последо-вательности в RNN-ячейку с num_neurons нейронами и на каждом шаге будем ожидать получения одной предсказанной биржевой цены на вы-ходе. Константа seq_len – э то количество временных шагов. Таким об- разом, в качестве входной последовательности мы будем использовать 20-дневные цены на акции компании Google и отправлять такие вход-ные данные в RNN-ячейку со 100 нейронами. 2. Откройт е и прочитайте текстовый файл со всеми ценами, выполните разбор цен на список вещественных чисел float, поменяйте порядок сле-дования элементов списка на обратный, для того чтобы сначала начи-налась старая цена, затем добавляйте в массив numpy result значения в количестве seq_len+1 (первые seq_len значений будут входной после- довательностью, подаваемой в RNN-ячейку, последние seq_len значений будут целевой выходной последовательностью), начиная с первого эле-мента в списке и всякий раз сдвигая на 1 до самого конца списка: f = open(symbol + '.txt', 'r').read()data = f.split('\\n')[:-1] # избавиться от последнего '', чтобы сработало float(n) data.reverse()d = [float(n) for n in data] result = [] for i in range(len(d) ‑‑ seq_len ‑‑ 1): result.append(d[i: i + seq_len + 1]) result = np.array(result) 3. Р езультирующий массив теперь содержит весь набор данных для нашей модели, но нам нужно дополнительно обработать его в формате, ожида-емом API RNN. Давайте сначала разделим его на тренировочный набор (90% всего набора данных) и тестовый набор (10%): row = int(round(0.9 * result.shape[0]))train = result[: row,:]test = result[row:,:] Затем перетасуем тренировочный набор случайным образом, согласно стандартной практике тренировки машинно-обучающейся модели: np.random.shuffle(train)\n--- Страница 235 ---\nПредсказание биржевой цены с помощью RNN-сети  233 Теперь мы сформулируем входные последовательности тренировочного и тестового наборов, X_train и x_test , а также выходные целевые по- следовательности тренировочного и тестового наборов, y_train и y_test . Обратите внимание, что больше X и малое y являются общепринятыми обозначениями, используемыми в машинном обучении для представле-ния соответственно входа и целевого выхода: X_train = train[:,:-1] # все строки со всеми столбцами, кроме последнего X_test = test[:,:-1] # каждая строка содержит seq_len + 1 столб цов y_train = train[:, 1:]y_test = test[:, 1:] Наконец, изменим формат четырех массивов на трехмерный (размер па-кета, количество временных шагов и количество входов или выходов), завершив на этом подготовку наборов данных для тренировки и тести-рования: X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], num_inputs))X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], num_inputs))y_train = np.reshape(y_train, (y_train.shape[0], y_train.shape[1], num_outputs))y_test = np.reshape(y_test, (y_test.shape[0], y_test.shape[1], num_outputs)) Обратите внимание, что все размеры X_train.shape[1], X_test.shape[1] , y_train.shape[1] и y_test.shape[1] имеют одинаковое значение seq_len . 4. Мы готовы построить модель. Создайте два заполнителя, в которые во время тренировки будут подаваться наборы X_train и y_train и набор X_test во время тестирования: X = tf.placeholder(tf.float32, [None, seq_len, num_inputs])y = tf.placeholder(tf.float32, [None, seq_len, num_outputs]) Примените объект-ячейку BasicRNNCell для создания RNN-ячейки, каждая из которых содержит num_neurons нейронов для каждого временного шага: cell = tf.contrib.rnn.OutputProjectionWrapper( tf.contrib.rnn.BasicRNNCell(num_units=num_neurons, activation=tf.nn.relu), output_size=num_outputs) outputs, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32) Обертка OutputProjectionWrapper используется для надстройки полносвяз- ного слоя поверх выходных данных в каждой ячейке, поэтому на каждом временном шаге выходные данные RNN-ячейки, которые были бы по-следовательностью, состоящей из num_neurons значений, сводятся до од- ного значения. Именно так RNN-сеть выдает одно значение для каждого значения во входной последовательности на каждом временном шаге, или выдает общее количество seq_len значений для каждой входной по- следовательности, состоящей из seq_len значений в расчете на каждый экземпляр.\n--- Страница 236 ---\n234  Предсказание биржевой цены с помощью RNN-сети Метод dynamic_rnn используется для перебора RNN-ячеек в цикле по всем временным шагам в общей сложности seq_len шагов (определенных в форме x) и возвращает два значения: список выходов на каждом вре- менном шаге и конечное состояние сети. Мы будем использовать ре- формированное (reshaped) значение первого возвращаемого значения outputs для определения функции потери. 5. Завершит е определение модели стандартным образом, указав тензор предсказаний, потерю, оптимизатор и тренировочную операцию: preds = tf.reshape(outputs, [1, seq_len], name=\"preds\")loss = tf.reduce_mean(tf.square(outputs ‑‑ y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss) Обратите внимание, что тензор предсказаний preds будет использо- ваться в качестве имени выходного узла, когда мы будем использовать инструмент freeze_graph , чтобы подготовить модель для развертывания на мобильном устройстве. Он также будет использоваться в iOS и Android для запуска модели и выполнения ею предсказаний. Как вы сами видите, иметь такую информацию до того, как мы начнем тренировку модели, безусловно приятно, и это преимущество идет вместе с моделью, кото-рую мы построили с нуля. 6. Начнит е тренировочный процесс. Для каждой эпохи мы загружаем дан- ные X_train и y_train и выполняем тренировочную операцию training_ op, минимизируя потерю loss , затем мы сохраняем файлы контрольных точек модели и распечатываем значение потери через каждые 10 эпох: init = tf.global_variables_initializer()saver = tf.train.Saver() with tf.Session() as sess: init.run() count = 0 for _ in range(epochs): n=0 sess.run(training_op, feed_dict={X: X_train, y: y_train}) count += 1 if count% 10 == 0: saver.save(sess, \"/tmp/\" + symbol + \"_model.ckpt\") loss_val = loss.eval(feed_dict={X: X_train, y: y_train}) print(count, \"loss:\", loss_val) Если выполнить приведенный выше программный код, то вы увидите результат, подобный следующему: (10, 'loss:', 243802.61)(20, 'loss:', 80629.57)\n--- Страница 237 ---\nПредсказание биржевой цены с помощью RNN-сети  235 (30, 'loss:', 40018.996) (40, 'loss:', 28197.496)(50, 'loss:', 24306.758) (460, 'loss:', 93.095985)(470, 'loss:', 92.864082)(480, 'loss:', 92.33461)(490, 'loss:', 92.09893)(500, 'loss:', 91.966286)  Вы можете заменить ячейку BasicRNNCell в шаге 4 на ячейку BasicLSTMCell и запус - тить программный код тренировки, правда, скорость тренировки с помощью ячейки BasicLSTMCell будет намного медленнее, и после 500 эпох значение потери все еще будет довольно большим. В этом разделе мы не будем больше экспериментировать с ячейкой BasicLSTMCell, однако для сравнения вы увидите подробное использование стековой укладки LSTM-слоев, отсева и двунаправленных RNN-сетей в следующем раз-деле, посвященном использованию Keras. тестирование моДели Tensor Flow rnn Для того чтобы увидеть, является ли значение потери вполне приемлемым пос ле 500 эпох, давайте добавим следующий ниже программный код, исполь- зуя тестовый набор данных, который вычисляет количество правильных пред-сказаний среди всех тестовых примеров (под правильным мы подразумеваем, что предсказываемая цена растет или снижается в том же направлении, что и целевая цена, относительно цены предыдущего дня): correct = 0y_pred = sess.run(outputs, feed_dict={X: X_test})targets = []predictions = []for i in range(y_pred.shape[0]): input = X_test[i] target = y_test[i] prediction = y_pred[i] targets.append(target[ ‑‑1][0]) predictions.append(prediction[ ‑‑1][0]) if target[ ‑‑1][0] >= input[ ‑‑1][0] and prediction[ ‑‑1][0] >= input[ ‑‑1][0]: correct += 1 elif target[ ‑‑1][0] < input[ ‑‑1][0] and prediction[ ‑‑1][0] < input[ ‑‑1][0]: correct += 1 Теперь мы можем визуализировать соотношение правильных предсказа- ний, воспользовавшись для этого методом plot :\n--- Страница 238 ---\n236  Предсказание биржевой цены с помощью RNN-сети total = len(X_test) xs = [i for i, _ in enumerate(y_test)] plt.plot(xs, predictions, 'r ‑', label='prediction') plt.plot(xs, targets, 'b ‑', label='true') plt.legend(loc=0)plt.title(\"%s ‑‑%d/%d=%.2f%%\"%(symbol, correct, total, 100*float(correct)/total)) plt.show() Выполнив теперь этот программный код, мы получим график, который бу - дет выглядеть, как на рис. 8.1, с соотношением правильных предсказаний, рав-ным 56.25%. Рисунок 8.1. Правильность предсказания биржевой цены моделью, натренированной с помощью TensorFlow RNN Обратите внимание, что при каждом выполнении этого тренировочного и тестового программного кода вы, скорее всего, будете получать соотноше-ние, которое будет немного отличаться. В результате точной настройки гипер-параметров модели вы можете получить соотношение, равное более 60%, что, безусловно, выглядит лучше, чем случайное предсказание. Если вы – оптимис т, то вы, возможно, подумаете, что это, по крайней мере, хоть какой-то результат, причем лучше, чем 50% (56.25%), и, вероятно, захотите увидеть, как эта модель работает на мобильном устройстве. Но давайте сначала посмотрим, сможем ли мы построить более оптимальную модель с помощью замечательной библио-теки Keras, – прежде чем мы это сделаем, давайте заморозим натренирован- ную модель TensorFlow, выполнив команду:\n--- Страница 239 ---\nПредсказание биржевой цены с помощью RNN-сети  237 python tensorflow/python/tools/freeze_graph.py --input_meta_graph=/tmp/amzn_model.ckpt.meta --input_checkpoint=/tmp/amzn_model.ckpt --output_graph=/tmp/amzn_tf_frozen.pb --output_node_names=\"preds\" --input_binary=true использование aPi rnn lsTM библиотеки Keras Для преД сказания биржевой цены Keras – э то очень простая в использовании высокоуровневая библиотека Python для глубокого обучения, работающая поверх других популярных библиотек глубокого обучения, включая TensorFlow, Theano и CNTK. Как вы вскоре уви-дите, библиотека Keras намного упрощает создание моделей и эксперименти-рование с ними. Для того чтобы установить и использовать библиотеку Keras вместе с платформой TensorFlow в качестве бэкенда, лучше всего сначала на-строить виртуальную среду virtualenv: sudo pip install virtualenv Затем, если на вашем компьютере размещен исходный код TensorFlow 1.4 и приложения для iOS и Android, то выполните следующие ниже команды; при этом применяйте пользовательскую библиотеку TensorFlow 1.4: cdmkdir ~/tf14_kerasvirtualenv --system-site-packages ~/tf14_keras/cd ~/tf14_keras/source ./bin/activateeasy_install -U pippip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow -1.4.0- py2-none-any.whlpip install keras Если же на вашем компьютере размещен исходный код TensorFlow 1.5, то вы должны установить платформу TensorFlow 1.5 с библиотекой Keras, потому что модель, создаваемая с использованием библиотеки Keras, должна иметь ту же самую версию платформы TensorFlow, которая используется мобильными приложениями TensorFlow. В противном случае возникает ошибка при попыт - ке загрузить модель: cdmkdir ~/tf15_kerasvirtualenv --system-site-packages ~/tf15_keras/cd ~/tf15_keras/source ./bin/activateeasy_install -U pippip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow -1.5.0- py2-none-any.whlpip install keras\n--- Страница 240 ---\n238  Предсказание биржевой цены с помощью RNN-сети Если ваша ОС не Mac или ваш компьютер имеет GPU, то вам следует заме- нить URL-адрес Python’овского пакета TensorFlow на соответствующий, кото- рый вы можете найти на веб-странице https://www.tensorflow.org/install. тренировка lsTM- моДели в библиотеке Keras Давайте теперь посмотрим, что нужно, чтобы построить и натренировать LSTM-модель для предсказания биржевой цены в библиотеке Keras. Прежде всего следует импортировать несколько библиотек и определить несколько констант: import kerasfrom keras import backend as Kfrom keras.layers.core import Dense, Activation, Dropoutfrom keras.layers.recurrent import LSTMfrom keras.layers import Bidirectionalfrom keras.models import Sequentialimport matplotlib.pyplot as plt import tensorflow as tf import numpy as np symbol = 'amzn' epochs = 10num_neurons = 100seq_len = 20pred_len = 1shift_pred = False Константа shift_pred используется для того, чтобы указать, будем мы пред- сказывать выходную последовательность цен относительно только одной выходной цены или нет. Если да, то мы предсказываем X 2, X3,…, Xn+1, исходя из входной последовательности X1, X2, X3,…, Xn, как мы делали в предыдущем разделе, когда использовали API платформы TensorFlow. Если константа shift_ pred имеет значение False , то мы предсказываем вперед на длину pred_len в выходных данных на основе входной последовательности X1, X2,…, Xn. Напри- мер, если pred_len равняется 1, то мы предсказываем XN+1, и если длина pred_len равняется 3, то мы предсказываем Xn+1, Xn+2, и Xn+3, что вполне резонно, так как мы хотели бы знать, будет ли цена продолжать расти 3 дня подряд или только 1 день, а потом 2 дня будет снижаться. Теперь создадим метод, модифицированный на основе программного кода загрузки данных из предыдущего раздела, который готовит соответствую-щие тренировочный и тестовый наборы данных на основе значений pred_len и shift_pred : def load_data(filename, seq_len, pred_len, shift_pred): f = open(filename, 'r').read()\n--- Страница 241 ---\nПредсказание биржевой цены с помощью RNN-сети  239 data = f.split('\\n')[: ‑1] # get rid of the last '' so float(n) works data.reverse() d = [float(n) for n in data] lower = np.min(d) upper = np.max(d) scale = upper ‑lower normalized_d = [(x ‑lower)/scale for x in d] result = [] if shift_pred: pred_len = 1 for i in range((len(normalized_d) ‑‑ seq_len ‑‑ pred_len)/pred_len): result.append(normalized_d[i*pred_len: i*pred_len + seq_len + pred_len]) result = np.array(result) row = int(round(0.9 * result.shape[0])) train = result[: row,:] test = result[row:,:] np.random.shuffle(train) X_train = train[:,: ‑pred_len] X_test = test[:,: ‑pred_len] if shift_pred: y_train = train[:, 1:] y_test = test[:, 1:] else: y_train = train[:, ‑‑ pred_len:] y_test = test[:, ‑‑ pred_len:] X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1)) X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1)) return [X_train, y_train, X_test, y_test, lower, scale] Обратите внимание, что здесь мы также применяем нормализацию, исполь- зуя тот же метод нормализации, что и в предыдущей главе, для того чтобы убе- диться, что она улучшает нашу модель. Мы также возвращаем значения lower и scale , поскольку они необходимы для денормализации при выполнении предсказаний с помощью натренированной модели. Теперь мы можем вызвать функцию load_data и получить наборы трениро- вочных и тестовых данных, а также значения lower и scale : X_train, y_train, X_test, y_test, lower, scale = load_data(symbol + '.txt', seq_len, pred_len, shift_pred) Полный программный код построения модели выглядит следующим обра- зом:\n--- Страница 242 ---\n240  Предсказание биржевой цены с помощью RNN-сети model = Sequential() model.add(Bidirectional(LSTM(num_neurons, return_sequences=True, input_shape=(None, 1)), input_shape=(seq_len, 1)))model.add(Dropout(0.2)) model.add(LSTM(num_neurons, return_sequences=True)) model.add(Dropout(0.2)) model.add(LSTM(num_neurons, return_sequences=False)) model.add(Dropout(0.2)) if shift_pred: model.add(Dense(units=seq_len)) else: model.add(Dense(units=pred_len)) model.add(Activation('linear')) model.compile(loss='mse', optimizer='rmsprop') model.fit( X_train, y_train, batch_size=512, epochs=epochs, validation_split=0.05) print(model.output.op.name) print(model.input.op.name) Этот программный код в значительной степени не требует пояснений и про- ще, чем код построения модели в платформе TensorFlow, даже со вновь добав-ленными объектами Bidirectional , Dropout , validation_split и стековой уклад- кой LSTM-слоев. Обратите внимание, что параметр return_sequences в вызове LSTM должен иметь значение True, для того чтобы на выходе из LSTM была полная выходная последовательность, а не только последний результат в вы-ходной последовательности, если только она не является последним уложен-ным слоем. Последние две инструкции print распечатают имя входного узла (bidirectional_1_input) и имя выходного узла (activation_1/Identity), необходи-мые при замораживании модели и запуске ее на мобильном устройстве. Теперь если выполнить приведенный выше программный код, то вы увиди- те вот такой результат: 824/824 [==============================] -- 7s 9ms/step -- loss: 0.0833 -- val_loss: 0.3831 Epoch 2/10824/824 [==============================] -- 2s 3ms/step -- loss: 0.2546 -- val_loss: 0.0308 Epoch 3/10824/824 [==============================] -- 2s 2ms/step -- loss: 0.0258 -- val_loss: 0.0098 Epoch 4/10\n--- Страница 243 ---\nПредсказание биржевой цены с помощью RNN-сети  241 824/824 [==============================] -- 2s 2ms/step -- loss: 0.0085 -- val_loss: 0.0035 Epoch 5/10 824/824 [==============================] -- 2s 2ms/step -- loss: 0.0044 -- val_loss: 0.0026 Epoch 6/10824/824 [==============================] -- 2s 2ms/step -- loss: 0.0038 -- val_loss: 0.0022 Epoch 7/10824/824 [==============================] -- 2s 2ms/step -- loss: 0.0033 -- val_loss: 0.0019 Epoch 8/10824/824 [==============================] -- 2s 2ms/step -- loss: 0.0030 -- val_loss: 0.0019 Epoch 9/10824/824 [==============================] -- 2s 2ms/step -- loss: 0.0028 -- val_loss: 0.0017 Epoch 10/10824/824 [==============================] -- 2s 3ms/step -- loss: 0.0027 -- val_loss: 0.0019 Потеря во время тренировки и потеря во время тестирования печатаются с помощью простого вызова метода model.fit . тестирование моДели Keras rnn Теперь самое время сохранить контрольную точку модели и применить тес - товый набор данных для вычисления количества правильных предсказаний в том смысле, как мы объяснили в предыдущем разделе: saver = tf.train.Saver()saver.save(K.get_session(), '/tmp/keras_' + symbol + '.ckpt')predictions = []correct = 0total = pred_len*len(X_test)for i in range(len(X_test)): input = X_test[i]y_pred = model.predict(input.reshape(1, seq_len, 1))predictions.append(scale * y_pred[0][ ‑‑1] + lower) if shift_pred:if y_test[i][ ‑‑1] >= input[ ‑‑1][0] and y_pred[0][ ‑‑1] >= input[ ‑‑1][0]: correct += 1elif y_test[i][ ‑‑1] < input[ ‑‑1][0] and y_pred[0][ ‑‑1] < input[ ‑‑1][0]: correct += 1else:for j in range(len(y_test[i])):if y_test[i][j] >= input[ ‑‑1][0] and y_pred[0][j] >= input[ ‑‑1][0]: correct += 1elif y_test[i][j] < input[ ‑‑1][0] and y_pred[0][j] < input[ ‑‑1][0]: correct += 1 Мы вызываем метод model.predict , главным образом чтобы получить пред- сказание для каждого экземпляра в x_test , и используем его с истинным зна- чением и ценой предыдущего дня, чтобы увидеть, является ли предсказание\n--- Страница 244 ---\n242  Предсказание биржевой цены с помощью RNN-сети правильным с точки зрения направления. Наконец, мы строим график, состо- ящий из истинных цен тестируемого набора данных и предсказаний: y_test = scale * y_test + lowery_test = y_test[:, ‑‑1] xs = [i for i, _ in enumerate(y_test)] plt.plot(xs, y_test, 'g ‑', label='true') plt.plot(xs, predictions, 'r ‑', label='prediction') plt.legend(loc=0)if shift_pred:plt.title(\"%s ‑‑ epochs=%d, shift_pred=True, seq_len=%d:%d/%d=%.2f%%\"%(symbol, epochs, seq_len, correct, total, 100*float(correct)/total))else:plt.title(\"%s ‑‑ epochs=%d, lens=%d,%d:%d/%d=%.2f%%\"%(symbol, epochs, seq_len, pred_len, correct, total, 100*float(correct)/total))plt.show() И вы увидите результат, как показано на рис. 8.2. Рисунок 8.2. Предсказание биржевой цены с использованием двунаправленных и уложенных в стек LSTM-слоев Keras Можно легко уложить в стек дополнительные LSTM-слои или поиграть с на- стройками гиперпараметров, такими как скорость заучивания и коэффициент отсева, а также многими константами. Вместе с тем мы не нашли существен-ной разницы в соотношении правильности в случае разных значений pred_ien и shift_pred . Возможно, на данном этапе нам стоит просто удовлетвориться 60%-ным соотношением правильности и заняться применением моделей, на-тренированных в TensorFlow и Keras, в наших iOS и Android позднее мы можем попробовать усовершенствовать данную модель, но сейчас полезнее выяснить,\n--- Страница 245 ---\nПредсказание биржевой цены с помощью RNN-сети  243 нет ли у нас каких-либо проблем с использованием наших RNN-моделей, на- тренированных с помощью TensorFlow и Keras.  Как отметил Франсуа Шолле: «Глубокое обучение – это больше искусство, чем наука… каждая задача уникальна, и вам придется испытывать и оценивать разные стратегии эмпирически. В настоящее время нет теории, которая бы заранее точно указывала, что именно нужно сделать, чтобы оптимально решить задачу. Вы должны постоянно про-бовать». Надо надеяться, что мы обеспечили вам хорошую отправную точку для усо-вершенствования моделей предсказания биржевой цены с использованием платформы TensorFlow и библиотеки Keras API. Последнее, что нам нужно сделать в этом разделе, – это заморозить модель Keras из контрольной точки. Поскольку мы установили TensorFlow и Keras в нашей виртуальной среде и TensorFlow является единственной установлен-ной и поддерживаемой в виртуальной среде библиотекой глубокого обуче-ния, библиотека Keras использует TensorFlow в качестве бэкенда и генериру - ет контрольную точку в формате TensorFlow с помощью вызова saver.save(K. get_session(), '/tmp/keras_' + symbol + '.ckpt') . Теперь выполните следующую ниже команду, чтобы заморозить контрольную точку (напомним, что мы по-лучаем выходное имя output_node_name в результате выполнения инструкции print(model.input.op.name) во время тренировки): python tensorflow/python/tools/freeze_graph.py --input_meta_graph=/tmp/keras_amzn.ckpt.meta --input_checkpoint=/tmp/keras_amzn.ckpt --output_graph=/tmp/amzn_keras_frozen.pb --output_node_names=\"activation_1/Identity\" --input_binary=true Вследствие того, что наша модель довольно проста и понятна, мы испытаем две замороженные модели непосредственно на мобильном устройстве без ис - пользования инструмента transform_graph , как мы делали в предыдущих двух главах. выполнение моДелей Tensor Flow и Keras в ios Мы не будем утомлять вас, повторяя шаг настройки проекта. Просто следуйте тому, что мы делали раньше, и создайте новый проект на языке Objective-C под названием StockPrice , в котором будет использоваться библиотека TensorFlow ручной сборки (если вам нужна подробная информация, то обратитесь к разде-лу iOS главы 7 «Распознавание рисунков с помощью CNN- и LSTM-сетей»). Затем добавьте в проект два модельных файла amzn_tf_frozen.pb и amzn_keras_frozen. pb, и у вас получится свой собственный проект StockPrice в среде разработки Xcode, как показано на рис. 8.3.\n--- Страница 246 ---\n244  Предсказание биржевой цены с помощью RNN-сети Рисунок 8.3. Приложение для iOS в среде Xcode с использованием моделей, натренированных в TensorFlow и Keras В файле ViewController.mm сначала объявите несколько переменных и одну константу: unique_ptr<tensorflow:: Session> tf_session; UITextView *_tv;UIButton *_btn;NSMutableArray *_closeprices;const int SEQ_LEN = 20; Затем создайте обработчик касания кнопки, который позволит пользовате- лю делать выбор между моделью TensorFlow и моделью Keras (кнопка создает - ся в методе viewDidLoad , как и раньше): ‑ (IBAction)btnTapped:(id)sender { UIAlertAction* tf = [UIAlertAction actionWithTitle:@\"Use TensorFlow Model\" style: UIAlertActionStyleDefault handler:^(UIAlertAction * action) { [self getLatestData: NO]; }]; UIAlertAction* keras = [UIAlertAction actionWithTitle:@\"Use Keras Model\" style: UIAlertActionStyleDefault handler:^(UIAlertAction * action) { [self getLatestData: YES]; }]; UIAlertAction* none = [UIAlertAction actionWithTitle:@\"None\" style: UIAlertActionStyleDefault handler:^(UIAlertAction * action) {}]; UIAlertController* alert = [UIAlertController alertControllerWithTitle:@\"RNN Model Pick\" message: nil preferredStyle: UIAlertControllerStyleAlert]; [alert addAction: tf]; [alert addAction: keras]; [alert addAction: none];\n--- Страница 247 ---\nПредсказание биржевой цены с помощью RNN-сети  245 [self presentViewController: alert animated: YES completion: nil]; } Метод getLatestData сначала выполняет URL-запрос и получает компактную версию данных из API Alpha Vantage. В случае компактной версии будут воз- вращены последние 100 точек ежедневных биржевых данных по компании Amazon. Затем указанный метод выполняет разбор результата и сохраняет по-следние 20 цен закрытия в массиве _closeprices : ‑(void)getLatestData:(BOOL)useKerasModel { NSURLSession *session = [NSURLSession sharedSession]; [[session dataTaskWithURL:[NSURL URLWithString:@\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=amzn&apikey=<your_api_key>&datatype=csv&outputsize=compact\"] completionHandler:^(NSData *data, NSURLResponse *response, NSError *error) { NSString *stockinfo = [[NSString alloc] initWithData: data encoding: NSASCIIStringEncoding]; NSArray *lines = [stockinfo componentsSeparatedByString:@\"\\n\"]; _closeprices = [NSMutableArray array]; for (int i=0; i<SEQ_LEN; i++) { NSArray *items = [lines[i+1] componentsSeparatedByString:@\",\"]; [_closeprices addObject: items[4]]; } if (useKerasModel) [self runKerasModel]; else [self runTFModel]; }] resume]; } Метод runTFModel определен следующим образом: ‑ (void) runTFModel { tensorflow:: Status load_status; load_status = LoadModel(@\"amzn_tf_frozen\", @\"pb\", &tf_session); tensorflow:: Tensor prices(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({1, SEQ_LEN, 1})); auto prices_map = prices.tensor<float, 3>(); NSString *txt = @\"Last 20 Days:\\n\"; for (int i = 0; i < SEQ_LEN; i++){ prices_map(0, i,0) = [_closeprices[SEQ_LEN ‑i‑1] floatValue]; txt = [NSString stringWithFormat:@\"%@%@\\n\", txt, _closeprices[SEQ_LEN ‑i‑1]];\n--- Страница 248 ---\n246  Предсказание биржевой цены с помощью RNN-сети } std:: vector<tensorflow:: Tensor> output; tensorflow:: Status run_status = tf_session ‑>Run({{\"Placeholder\", prices}}, {\"preds\"}, {}, &output); if (!run_status.ok()) { LOG(ERROR) << \"Running model failed:\" << run_status; } else { tensorflow:: Tensor preds = output[0]; auto preds_map = preds.tensor<float, 2>(); txt = [NSString stringWithFormat:@\"%@\\nPrediction with TF RNN model:\\n%f\", txt, preds_map(0, SEQ_LEN‑1)]; dispatch_async(dispatch_get_main_queue(), ^{ [_tv setText: txt]; [_tv sizeToFit]; }); } } preds_map(0, SEQ_LEN‑1) – э то цена на следующий день, предсказанная на ос - нове последних 20 дней; Placeholder – э то имя входного узла, определенного в X = tf.placeholder(tf.float32, [None, seq_len, num_inputs]) на шаге 4 под- раздела «Тренировка RNN-модели в TensorFlow». После того как предсказание моделью сгенерировано, мы показываем его в элементе управления TextView вместе с ценами за последние 20 дней. Метод runKeras определен аналогичным образом, но с денормализацией и другими именами входных и выходных узлов. Поскольку наша модель Keras натренирована выводить только одну предсказываемую цену вместо последо-вательности цен seq_len , то для получения предсказания мы используем preds_ map(0,0) : ‑ (void) runKerasModel { tensorflow:: Status load_status; load_status = LoadModel(@\"amzn_keras_frozen\", @\"pb\", &tf_session); if (!load_status.ok()) return; tensorflow:: Tensor prices(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({1, SEQ_LEN, 1})); auto prices_map = prices.tensor<float, 3>(); float lower = 5.97; float scale = 1479.37; NSString *txt = @\"Last 20 Days:\\n\"; for (int i = 0; i < SEQ_LEN; i++){ prices_map(0, i,0) = ([_closeprices[SEQ_LEN ‑i‑1] floatValue] ‑ lower)/scale;\n--- Страница 249 ---\nПредсказание биржевой цены с помощью RNN-сети  247 txt = [NSString stringWithFormat:@\"%@%@\\n\", txt, _closeprices[SEQ_LEN ‑i‑1]]; } std:: vector<tensorflow:: Tensor> output; tensorflow:: Status run_status = tf_session ‑>Run({{\"bidirectional_1_input\", prices}}, {\"activation_1/Identity\"}, {}, &output); if (!run_status.ok()) { LOG(ERROR) << \"Running model failed:\" << run_status; } else { tensorflow:: Tensor preds = output[0]; auto preds_map = preds.tensor<float, 2>(); txt = [NSString stringWithFormat:@\"%@\\nPrediction with Keras RNN model:\\n%f\", txt, scale * preds_map(0,0) + lower]; dispatch_async(dispatch_get_main_queue(), ^{ [_tv setText: txt]; [_tv sizeToFit]; }); } } Если запустить приложение сейчас и коснуться кнопки Predict (Предска- зать), то появится сообщение о выборе модели (рис. 8.4).\n--- Страница 250 ---\n248  Предсказание биржевой цены с помощью RNN-сети Рисунок 8.4. Выбор между RNN-моделями TensorFlow и Keras Если вы выберете модель TensorFlow, то получите следующее сообщение об ошибке: Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'Less' with these attrs. Registered devices: [CPU], Registered kernels:device='CPU'; T in [DT_FLOAT][[Node: rnn/while/Less = Less[T=DT_INT32, _output_shapes=[[]]](rnn/while/Merge, rnn/while/Less/Enter)]] Если же вы выберете модель Keras, то может возникнуть несколько другая ошибка: Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'Less' with these attrs. Registered devices: [CPU], Registered kernels:device='CPU'; T in [DT_FLOAT][[Node: bidirectional_1/while_1/Less = Less[T=DT_INT32, _output_shapes=[[]]](bidirectional_1/while_1/Merge, bidirectional_1/while_1/Less/Enter)]] Мы уже встречали подобного рода ошибки в предыдущей главе; там она была связана с операцией RefSwitch , и мы знаем, что исправление такой ошибки заключается в создании библиотеки TensorFlow с включением всех\n--- Страница 251 ---\nПредсказание биржевой цены с помощью RNN-сети  249 типов данных ‑D__ANDROID_TYPES_FULL__ . Если вы не видите этих ошибок, зна- чит, вы выполнили сборку такой библиотеки, когда изучали приложение для iOS в предыдущей главе; в противном случае следуйте инструкциям в начале подраздела «Сборка пользовательской библиотеки TensorFlow для iOS» в раз-деле «Применение модели классификации рисунков в iOS» предыдущей гла-вы, чтобы выполнить сборку новой библиотеки TensorFlow, а затем запустите приложение снова. Теперь выберите модель TensorFlow, и вы увидите результаты, как на рис. 8.5. Рисунок 8.5. Предсказание RNN-модели TensorFlow При использовании модели Keras предсказание будет другим, как показано на рис. 8.6.\n--- Страница 252 ---\n250  Предсказание биржевой цены с помощью RNN-сети Рисунок 8.6. Предсказание RNN-модели Keras Без дальнейших исследований мы не можем сказать точно, какая модель ра- ботает лучше, но вот в чем мы можем быть уверены точно, так это в том, что обе наши RNN-модели, натренированные с нуля с использованием API платформы TensorFlow и библиотеки Keras, и демонстрирующие точность, близкую к 60%, работают нормально в iOS, а это, возможно, стоит наших усилий, так как мы пытаемся построить модель, которая, по мнению многих экспертов, останется на уровне результативности, которую можно получить при случайном гадании. И более того, в процессе работы над моделями мы узнали несколько новых ин-тересных вещей – применение платформы TensorFlow и библиотеки Keras для построения RNN-моделей и запуск их в iOS. Перед тем как мы продолжим наше путешествие в следующей главе, осталась всего одна вещь: как насчет исполь-зования этих моделей в Android? Будут ли у нас новые препятствия? выполнение моДелей Tensor Flow и Keras в android Оказывается, использование моделей в Android – это все равно, что прогул- ка по пляжу, – нам даже не нужно использовать пользовательскую библиотеку TensorFlow, собранную специально для Android, как мы это делали в предыду -\n--- Страница 253 ---\nПредсказание биржевой цены с помощью RNN-сети  251 щей главе, хотя в iOS нам действительно пришлось применять пользователь- скую библиотеку TensorFlow (не модуль TensorFlow по состоянию на февраль 2018 г.). Библиотека TensorFlow для Android, собираемая с помощью строки compile 'org.tensorflow: tensorflow ‑android:+' в файле build.gradle , должна иметь более полную поддержку типов данных для операции Less , чем в моду - ле TensorFlow для iOS. Для того чтобы протестировать модели в Android, создайте новое приложе- ние для Android под названием StockPrice и добавьте два модельных файла в папку ресурсов assets . Затем добавьте в макет пару элементов управления Button и Textview и определите несколько полей и констант в файле MainActivity. java: private static final String TF_MODEL_FILENAME = \"file:///android_asset/amzn_tf_frozen.pb\";private static final String KERAS_MODEL_FILENAME = \"file:///android_asset/amzn_keras_frozen.pb\";private static final String INPUT_NODE_NAME_TF = \"Placeholder\";private static final String OUTPUT_NODE_NAME_TF = \"preds\";private static final String INPUT_NODE_NAME_KERAS = \"bidirectional_1_input\";private static final String OUTPUT_NODE_NAME_KERAS = \"activation_1/Identity\";private static final int SEQ_LEN = 20;private static final float LOWER = 5.97f;private static final float SCALE = 1479.37f; private TensorFlowInferenceInterface mInferenceInterface;private Button mButtonTF; private Button mButtonKeras;private TextView mTextView;private boolean mUseTFModel;private String mResult; Придайте обработчику события onCreate следующий вид: protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); mButtonTF = findViewById(R.id.tfbutton); mButtonKeras = findViewById(R.id.kerasbutton); mTextView = findViewById(R.id.textview); mTextView.setMovementMethod(new ScrollingMovementMethod()); mButtonTF.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { mUseTFModel = true; Thread thread = new Thread(MainActivity.this); thread.start(); }\n--- Страница 254 ---\n252  Предсказание биржевой цены с помощью RNN-сети }); mButtonKeras.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { mUseTFModel = false; Thread thread = new Thread(MainActivity.this); thread.start(); } }); Остальная часть программного кода находится в методе run, запуска- емом в рабочем потоке при касании кнопки TF PREDICTION или KERAS PREDICTION, который не требует какого-то особого объяснения, кроме того что использование модели Keras требует нормализации и денормализации до и после выполнения модели: public void run() { runOnUiThread( new Runnable() { @Override public void run() { mTextView.setText(\"Getting data \"); } }); float[] floatValues = new float[SEQ_LEN]; try { URL url = new URL(\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=amzn&apikey=4SOSJM2XCRIB5IUS&datatype=csv&outputsize=compact\"); HttpURLConnection urlConnection = (HttpURLConnection) url.openConnection(); InputStream in = new BufferedInputStream(urlConnection.getInputStream()); Scanner s = new Scanner(in).useDelimiter(\"\\\\n\"); mResult = \"Last 20 Days:\\n\"; if (s.hasNext()) s.next(); // get rid of the first title line List<String> priceList = new ArrayList<>(); while (s.hasNext()) { String line = s.next(); String[] items = line.split(\",\"); priceList.add(items[4]); } for (int i=0; i<SEQ_LEN; i++) mResult += priceList.get(SEQ_LEN ‑i‑1) + \"\\n\"; for (int i=0; i<SEQ_LEN; i++) { if (mUseTFModel) floatValues[i] = Float.parseFloat(priceList.get(SEQ_LEN ‑i‑1));\n--- Страница 255 ---\nПредсказание биржевой цены с помощью RNN-сети  253 else floatValues[i] = (Float.parseFloat(priceList.get(SEQ_LEN ‑i‑1)) ‑‑ LOWER) / SCALE; } AssetManager assetManager = getAssets(); mInferenceInterface = new TensorFlowInferenceInterface(assetManager, mUseTFModel? TF_ MODEL_FILENAME: KERAS_MODEL_FILENAME); mInferenceInterface.feed(mUseTFModel? INPUT_NODE_NAME_TF: INPUT_NODE_NAME_KERAS, floatValues, 1, SEQ_LEN, 1); float[] predictions = new float[mUseTFModel? SEQ_LEN: 1]; mInferenceInterface.run(new String[] {mUseTFModel? OUTPUT_NODE_NAME_TF: OUTPUT_NODE_NAME_KERAS}, false); mInferenceInterface.fetch(mUseTFModel? OUTPUT_NODE_NAME_TF: OUTPUT_NODE_NAME_KERAS, predictions); if (mUseTFModel) { mResult += \"\\nPrediction with TF RNN model:\\n\" + predictions[SEQ_LEN ‑‑ 1]; } else { mResult += \"\\nPrediction with Keras RNN model:\\n\" + (predictions[0] * SCALE + LOWER); } runOnUiThread( new Runnable() { @Override public void run() { mTextView.setText(mResult); } }); } catch (Exception e) { e.printStackTrace(); } } Теперь запустите приложение и коснитесь кнопки TF PREDICTION, и вы увидите результаты, как на рис. 8.7.\n--- Страница 256 ---\n254  Предсказание биржевой цены с помощью RNN-сети Рисунок 8.7. Использование модели TensorFlow для предсказания цен на акции компании Amazon Касание кнопки KERAS PREDICTION даст вам результаты, как на рис. 8.8.\n--- Страница 257 ---\nПредсказание биржевой цены с помощью RNN-сети  255 Рисунок 8.8. Использование модели Keras для предсказания цен на акции компании Amazon резюме В этой главе мы начали с того, что презрели невозможное и попытались пре- взойти фондовый рынок, используя API RNN платформы TensorFlow и библио - теки Keras для предсказания биржевой цены. Сначала мы обсудили в общем плане концепцию моделей на основе RNN- и LSTM-сетей и как их использовать для предсказания биржевой цены. Затем мы построили две RNN-модели с нуля с использованием платформы TensorFlow и библиотеки Keras, достигнув почти 60% правильности на тестовых данных. Наконец, мы рассмотрели вопрос за-морозки моделей и их использования в iOS и Android, исправив возможную ошибку во время выполнения в iOS при помощи пользовательской библиотеки TensorFlow. Если вы немного разочарованы тем, что мы не построили модель с 80- или 90%-ным соотношением правильности предсказания, то вы можете продол-жить процесс по принципу «продолжайте пробовать», чтобы опытным пу - тем ответить на вопрос, возможно ли вообще предсказывать биржевую цену с таким соотношением правильности. Так или иначе, но навыки, которые вы\n--- Страница 258 ---\n256  Предсказание биржевой цены с помощью RNN-сети получили в процессе построения, тренировки и тестирования RNN-модели с использованием API TensorFlow и Keras и их запуска в iOS и Android, вы, безу словно, унесете с собой. Если вы заинтересованы и стремитесь к победе на фондовом рынке с ис - пользованием технологий глубокого обучения, то давайте поработаем в сле-дующей главе над моделями на основе генеративно-состязательных сетей, или GAN-сетей (Generative Adversarial Networks). Такая сеть пытается одержать верх над соперником, который может отличать реальные и поддельные дан-ные, и чем дальше, тем ближе генерируемые ею данные выглядят как реаль-ные, тем самым позволяя ее одурачивать соперника. GAN-сети на самом деле воспринимаются некоторыми ведущими исследователями в области глубокого обучения как самая интересная и захватывающая идея в глубоком обучении за последние 10 лет.",
      "debug": {
        "start_page": 229,
        "end_page": 258
      }
    },
    {
      "name": "Глава 9. Генерирование и улучшение изображений с помощью GAN -сети 257",
      "content": "--- Страница 259 --- (продолжение)\nГлава 9 Генерирование и улучшение изображений с помощью GAN-сети Некоторые специалисты считают, что с тех пор, как в 2012 году глубокое обуче- ние пошло резко в гору, не было более интересной и многообещающей идеи, чем генеративно-состязательная сеть (Generative Adversarial Network, GAN), кото-рую в 2014 году разработал и опубликовал Ян Гудфеллоу (Ian Goodfellow) в своей статье, которая так и называется «Generative Adversarial Networks» (https: //arxiv. org/abs/1406.2661). На самом деле Янн Лекун (Yann LeCun), директор по иссле-дованиям в области ИИ компании Facebook и один из исследователей-перво-проходцев в области глубокого обучения, называет GAN-сети и состязательную тренировку «самой интересной идеей за последние 10 лет в области машинного обучения». Учитывая все это, просто невозможно представить, что данная тема не будет здесь раскрыта, чтобы разобраться, чем вызван такой интерес к GAN-сетям и как строить модели на основе GAN-сетей и запускать их в iOS и Android. В этой главе мы сначала дадим краткий обзор того, что такое GAN-сеть, как она работает и почему она имеет такой большой потенциал. Затем мы рас - смотрим две модели на основе GAN-сети: одну – базовую GAN-модель, кото- рая может использоваться для генерирования человекоподобных рукописных цифр, и другую – более продвинутую GAN-модель, которая может улучшать изображения с низким разрешением до изображений с высоким разрешением. Мы покажем вам, как строить и тренировать такие модели на языке Python и в платформе TensorFlow и как готовить модели для развертывания на мо-бильных устройствах. Затем мы представим приложения для iOS и Android с полноценным исходным кодом, в которых используются модели для созда-ния рукописных цифр и улучшения изображений. К концу главы вы должны быть готовы к дальнейшему изучению всех видов моделей на основе GAN-сетей или начать создавать свои собственные модели и разбираться в том, как запускать их в своих мобильных приложениях.\nГлава 9 Генерирование и улучшение изображений с помощью GAN-сети Некоторые специалисты считают, что с тех пор, как в 2012 году глубокое обуче- ние пошло резко в гору, не было более интересной и многообещающей идеи, чем генеративно-состязательная сеть (Generative Adversarial Network, GAN), кото-рую в 2014 году разработал и опубликовал Ян Гудфеллоу (Ian Goodfellow) в своей статье, которая так и называется «Generative Adversarial Networks» (https: //arxiv. org/abs/1406.2661). На самом деле Янн Лекун (Yann LeCun), директор по иссле-дованиям в области ИИ компании Facebook и один из исследователей-перво-проходцев в области глубокого обучения, называет GAN-сети и состязательную тренировку «самой интересной идеей за последние 10 лет в области машинного обучения». Учитывая все это, просто невозможно представить, что данная тема не будет здесь раскрыта, чтобы разобраться, чем вызван такой интерес к GAN-сетям и как строить модели на основе GAN-сетей и запускать их в iOS и Android. В этой главе мы сначала дадим краткий обзор того, что такое GAN-сеть, как она работает и почему она имеет такой большой потенциал. Затем мы рас - смотрим две модели на основе GAN-сети: одну – базовую GAN-модель, кото- рая может использоваться для генерирования человекоподобных рукописных цифр, и другую – более продвинутую GAN-модель, которая может улучшать изображения с низким разрешением до изображений с высоким разрешением. Мы покажем вам, как строить и тренировать такие модели на языке Python и в платформе TensorFlow и как готовить модели для развертывания на мо-бильных устройствах. Затем мы представим приложения для iOS и Android с полноценным исходным кодом, в которых используются модели для созда-ния рукописных цифр и улучшения изображений. К концу главы вы должны быть готовы к дальнейшему изучению всех видов моделей на основе GAN-сетей или начать создавать свои собственные модели и разбираться в том, как запускать их в своих мобильных приложениях.\n--- Страница 260 ---\n258  Г енерирование и улучшение изображений с помощью GAN -сети Резюмируя, в этой главе мы рассмотрим следующие темы: GAN-с еть – что это такое и почему; построение и тренировка моделей на основе GAN-сети с помощью TensorFlow; испо льзование GAN-моделей в iOS; применение GAN-мо делей в Android. Gan- сеть – что это такое и почему GAN-сети – это нейронные сети, которые учатся генерировать данные, ана- логичные реальным данным или данным в тренировочном наборе. Ключевая идея GAN-сети – это наличие генераторной и дискриминаторной сетей, кото- рые играют друг против друга: генератор пытается генерировать данные, ко-торые выглядят как реальные данные, а дискриминатор пытается различить, являются ли сгенерированные данные реальными (то есть пришли из извест - ных реальных данных) или поддельными (созданными генератором). Генера-тор и дискриминатор тренируются вместе, и в процессе тренировки генератор учится генерировать данные, которые все больше и больше похожи на реаль-ные, в то время как дискриминатор учится отличать реальные данные от под-дельных. Генератор учится, пытаясь добиться у дискриминатора максимально близкой к 1,0 вероятности, что выходные данные являются реальными, когда тому на вход подаются данные из генератора, в то время как дискриминатор учится, пытаясь достичь двух целей: сделать свою вероятность, что выходные данные являются реальными, максимально близкой к 0,0, когда ему на вход подаются данные генера-тора, что оказывается прямо противоположным цели генератора; сделать свою вероятность, что выходные данные являются реальными, максимально близкой к 1,0, когда ему на вход подаются реальные дан-ные.  В следующем разделе вы увидите подробный фрагмент кода, соответствующий задан- ному описанию генераторной и дискриминаторной сетей, а также их тренировочному процессу. Если вы хотите узнать больше о GAN-сетях в дополнение к нашему приводи-мому здесь сводному обзору, то в YouTube вы можете найти введение «Introduction to GANs» и посмотреть введение и обучающие видеоролики Иэна Гудфеллоу о GAN-сетях на конференции NIPS2016 (Neural Information Processing Systems, Системы нейроин-формационной обработки) и ICCV 2017 (International Conference on Computer Vision, Международная конференция по компьютерному зрению). На самом деле в YouTube имеется 7 видеороликов с записью семинаров по состязательной тренировке с конфе-ренции NIPS2016 и 12 учебных видеороликов по GAN-сетям с конференции ICCV 2017, которые помогут вам углубиться в данную тему. С учетом конкурирующих целей двух игроков, генератора и дискримина- тора, GAN-сеть представляет собой систему, которая ищет равновесие между двумя соперниками. Если оба игрока имеют неограниченные возможности\n--- Страница 261 ---\nГенерирование и улучшение изображений с помощью GAN -сети  259 и способны тренироваться оптимально, то равновесие Нэша, то есть стабиль- ное состояние, в котором ни один игрок не может получить выгоду, изменив только свою собственную стратегию (названо так в честь лауреата Нобелев-ской премии в области экономических наук за 1994 г. Джона Нэша (John Nash), о котором впоследствии был снят кинофильм «Игры разума»), соответствует состоянию генератора, генерирующего данные, которые выглядит так же, как реальные данные, и дискриминатора, который не способен отличить реальные данные от поддельных.  Ес ли вам интересно узнать больше о равновесии Нэша, погуглите «Khan academy Nash equilibrium» и посмотрите два забавных видеоролика Сэла Хана, посвященных ему. Сто-ит также почитать страницу Википедии о равновесии Нэша и статью «What is the Nash equilibrium and why does it matter?» («Что такое равновесие Нэша и почему оно имеет значение?») в журнале Economist explaining economics («Экономист объясняет экономи-ку») (https://www.economist.com/blogs/economist-explains/2016/09/economist-explains-economics). Интуитивное понимание базовой идеи, лежащей в основе GAN-сети, помо-жет вам лучше понять, почему она имеет большой потенциал. Потенциал генератора, способного генерировать данные, которые выглядят как реальные, означает, что на основе GAN-сетей можно разрабатывать целый ряд приложений, таких как: создание высококачественных изображений из изображений низкого качества; ретуширование изображений (восстановление потерянных или повреж - денных изображений); транс ляция изображений (например, преобразование из набросков в фотографии либо добавление или удаление объектов, таких как очки на человеческих лицах); генерирование изображений из текста (противоположная задача прило- жению Text2Image, которое мы рассматривали в главе 6 «Описание изо- бражений на естественном языке»); написание новос тных статей, которые выглядят как настоящие; генерирование звуковых волн, аналогичных звуку в тренировочном на- боре. В принципе, GAN-сети могут генерировать реалистичные изображения, тек - сты или аудиоданные из случайных входных данных; если у вас есть трениро-вочный набор исходных и целевых данных, то GAN-сети также могут генериро-вать данные, аналогичные целевым данным, из входных данных, аналогичных исходным. Именно эта универсальная функциональная особенность того, как генератор и дискриминатор в GAN-модели динамически сотрудничают, позво-ляя GAN-сетям порождать любой вид реалистичных выходных данных, делает GAN-сети весьма перспективными. Вместе с тем как раз по причине этой самой динамики или конкурирующих целей генератора и дискриминатора тренировка GAN-сетей до достижения равновесного состояния Нэша является сложной и трудной задачей. На самом\n--- Страница 262 ---\n260  Г енерирование и улучшение изображений с помощью GAN -сети деле эта задача по-прежнему остается открытой для исследований – Ян Гуд- феллоу в своем интервью «Герои глубокого обучения» с Эндрю Нг в августе 2017 года (поищите в Youtube по запросу ian goodfellow andrew ng) говорит, что если мы сможем сделать так, что GAN-сети станут так же надежны, как глубо-кое обучение, то мы увидим, как GAN-сети добьются гораздо большего успеха, в противном случае мы в конечном итоге заменим их другими формами гене-ративных моделей. Несмотря на трудности в тренировке GAN-сетей, уже был предложен це- лый ряд эффективных и известных приемов, которые вы можете использо-вать во время тренировки (https: //github.com/soumith/ganhacks), – мы не будем их здесь рассматривать, однако они могут оказаться полезными, если вы за-интересованы в донастройке модели, которую мы опишем в этой главе, или многих других GAN-моделей (https: //github.com/eriklindernoren/Keras-GAN), или в построении своей собственной GAN-модели. построение и тренировка Gan- моДелей с помощью Tensor Flow В общем случае GAN-модель имеет две нейронные сети: генератор G и дис - криминатор D. Входом в нее является x, то есть некие реальные данные из тре- нировочного набора, и z – случайный входной шум. В ходе обучения D(х) – это вероятность того, что x является реальным, и D пытается приблизить D(х) к 1; G(z) – это генерируемые выходные данные со случайными входными данны- ми z, и D пытается приблизить D(G(z)) к 0, но одновременно с этим G пытается приблизить D(G(z)) к 1. Теперь давайте сначала посмотрим, что мы можем сде-лать, чтобы построить базовую GAN-модель в платформе TensorFlow и на язы-ке Python, способную писать или генерировать рукописные цифры. базовая Gan- моДель генерирования рукописных цифр Тренировка модели генерирования рукописных цифр основана на репози-тории https: //github.com/jeffxtang/generative-adversarial-networks, являющемся ответвлением репозитория https: //github.com/jonbruner/generative-adversarial- networks с добавлением сценария, который показывает сгенерированные циф-ры и сохраняет натренированную модель TensorFlow с заполнителем входных данных, поэтому наши приложения для iOS и Android вполне могут использо-вать эту модель. Вы обязательно должны просмотреть блог-пост https://www. oreilly.com/learning/generative-adversarial-networks-for-beginners, где можно взять ссылку на исходный репозиторий, если вы чувствуете, что, перед тем как дви-гаться дальше, вам требуется элементарное понимание GAN-модели вместе с программным кодом. Прежде чем мы рассмотрим корневой фрагмент программного кода, кото- рый определяет генераторную и дискриминаторную сети и выполняет трени-\n--- Страница 263 ---\nГенерирование и улучшение изображений с помощью GAN -сети  261 ровку GAN-сети, давайте сначала выполним сценарии для тренировки и тес - тирования модели после клонирования репозитория и перехода в каталог репозитория: git clone https://github.com/jeffxtang/generative-adversarial-networks cd generative-adversarial-networks В этом ответвлении в сценарий gan‑script‑fast.py добавлен программный код сохранения контрольной точки, а также добавлен новый сценарий gan‑ script‑test.py для тестирования и сохранения новых контрольных точек с за- полнителем для случайных входных данных – поэтому модель, замороженная с помощью новой контрольной точки, может использоваться в приложениях для iOS и Android. Выполните команду python gan ‑script‑fast.py , чтобы произвести тренировку модели, которая на нашем GTX -1070 GPU в Ubuntu занимает менее одного часа. После завершения тренировки файлы контрольных точек будут сохранены в каталоге модели. Теперь выполните команду python gan ‑script‑test.py , чтобы увидеть некоторые сгенерированные рукописные цифры. Данный сценарий также считывает файлы контрольных точек из каталога модели, сохраненные при выполнении сценария gan‑script‑fast.py , и повторно сохраняет в каталоге newmodel обновленные файлы контрольных точек вместе с заполнителем для случайных входных данных: ls -lt newmodel-rw-r--r-- 1 jeffmbair staff 266311 Mar 5 16:43 ckpt.meta -rw-r--r-- 1 jeffmbair staff 65 Mar 5 16:42 checkpoint -rw-r--r-- 1 jeffmbair staff 69252168 Mar 5 16:42 ckpt.data -00000-of -00001 -rw-r--r-- 1 jeffmbair staff 2660 Mar 5 16:42 ckpt.index Следующий ниже фрагмент кода в сценарии gan‑script‑test.py показыва- ет имя входного узла ( z_placeholder ) и имя выходного узла ( Sigmoid_1 ), которые распечатываются инструкцией print(generated_images) : z_placeholder = tf.placeholder(tf.float32, [None, z_dimensions], name='z_placeholder') saver.restore(sess, 'model/ckpt')generated_images = generator(z_placeholder, 5, z_dimensions)print(generated_images)images = sess.run(generated_images, {z_placeholder: z_batch})saver.save(sess, \"newmodel/ckpt\") В сценарии gan‑script‑fast.py метод def discriminator(images, reuse_ variables=None) определяет дискриминаторную сеть, которая на входе прини- мает реальное рукописное изображение или изображение, сгенерированное генератором, проходит через типичную небольшую СNN-сеть с двумя свер-точными слоями conv2d , каждый из которых сопровождается активацией relu\n--- Страница 264 ---\n262  Г енерирование и улучшение изображений с помощью GAN -сети и редуцирующим слоем на основе функции average1, и двумя полносвязными слоями для вывода скалярного значения, которое содержит вероятность того, что входное изображение является реальным или фиктивным. Другой метод def generator(batch_size, z_dim) определяет генераторную сеть, которая при- нимает вектор случайного входного изображения и преобразует его в изобра-жение 28×28 с тремя сверточными слоями conv2d . Эти два метода теперь можно использовать для определения трех выводов: Gz, вывод из генератора при вводе случайного изображения: Gz = generator(batch_size, z_dimensions) ; Dx, вывод из дискриминатора при вводе реального изображения: Dx = discriminator(x_placeholder) ; Dg, вывод из дискриминатора при вводе Gz: Dg = discriminator(Gz, reuse_ variables=True) , – и трех функций потерь: d_loss_real , разница между Dx и 1: d_loss_real = tf.reduce_mean(tf. nn.sigmoid_cross_entropy_with_logits(logits = Dx, labels = tf.ones_like(Dx))) ; d_loss_fake , разница между Dg и 0: d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_ cross_entropy_with_logits(logits = Dg, labels = tf.zeros_like(Dg))) ; g_loss , разница между Dg и 1: g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_ entropy_with_logits(logits = Dg, labels = tf.ones_like(Dg))) . Обратите внимание, что дискриминатор пытается минимизировать потерю d_loss_fake , в то время как генератор пытается минимизировать потерю g_loss , разница между Dg в обоих случаях равняется соответственно 0 и 1. Наконец, теперь можно установить три оптимизатора для трех функций по- терь: d_trainer_fake , d_trainer_real и g_trainer , – опре деленных с помощью метода minimize оптимизатора tf.train.AdamOptimizer . Далее сценарий просто создает сеанс TensorFlow, тренирует генератор и дис - криминатор на 100 000 шагах, запуская три оптимизатора, со случайными изо- бражениями, подаваемыми на вход генератора, и реальными и фиктивными изображениями, подаваемыми на вход дискриминатора. После выполнения обоих сценариев, gan‑script‑fast.py и gan‑script‑test. py, скопируйте файлы контрольных точек из каталога newmodel в папку /tmp , а затем перейдите в корневой каталог исходного кода TensorFlow и выполните команду: python tensorflow/python/tools/freeze_graph.py \\--input_meta_graph=/tmp/ckpt.meta \\--input_checkpoint=/tmp/ckpt \\ 1 Р едуцирующий слой (pooling layer) используется для структурированного уменьше- ния размерности входных данных. В математическом смысле он принимает локаль- ное рецептивное поле и заменяет нелинейную активационную функцию в каждой части поля на функцию max, либо min, либо average, как в данном случае. В отечест - венной литературе термин pooling layer часто переводится как субдискретизирую- щий, или подвыборочный, слой. – Прим. перев.\n--- Страница 265 ---\nГенерирование и улучшение изображений с помощью GAN -сети  263 --output_graph=/tmp/gan_mnist.pb \\ --output_node_names=\"Sigmoid_1\" \\--input_binary=true В результате будет создана замороженная модель gan_mnist.pd , которую мы можем использовать в мобильных приложениях. Но прежде чем мы это сде-лаем, давайте взглянем на более продвинутую GAN-модель, которая способна улучшать изображения с низким разрешением. проДвинутая Gan- моДель улучшения разрешающей способности изображения Модель, которую мы будем использовать для улучшения размытых изо- бражений с низким разрешением, базируется на исследовательской работе «Image-to-Image Translation with Conditional Adversarial Networks» («Сквозная трансляция «изображение в изображение» с помощью условных состязатель-ных сетей») (https: //arxiv.org/abs/1611.07004) и ее реализации TensorFlow, pix2pix (https: //affinelayer.com/pix2pix/). В наше ответвление репозитория (https: //github. com/jeffxtang/pix2pix-tensorflow) мы добавили два сценария: сценарий tools/convert.py создает размытые изображения из обычных изображений; сценарий pix2pix_runinference.py добавляет заполнитель для ввода изо- бражения с низким разрешением и операцию для возврата улучшенного изображения, а также сохраняет новые файлы контрольных точек, кото-рые мы заморозим для создания файла модели, используемого на мо-бильных устройствах. В сущности, модель pix2pix использует GAN-сеть для отображения входно- го изображения в выходное изображение. Вы можете использовать различные типы входных и выходных изображений, в результате создавая множество ин-тересных трансляций изображений: карта в аэрофотосъемку; день в ночь; наброс ок в фото; черно-бе лые изображения в цветные изображения; повре жденные изображения в исходные изображения; изображ ения с низким разрешением в изображения с высоким разре- шением. Во всех случаях генератор трансформирует входное изображение в выход- ное изображение, пытаясь сделать выход похожим на реальное целевое изо-бражение, а дискриминатор принимает в качестве входных данных образец из тренировочного набора или выход из генератора и пытается определить, является ли он реальным изображением или произведен генератором. Естест - венно, генераторные и дискриминантные сети в модели pix2pix построены\n--- Страница 266 ---\n264  Г енерирование и улучшение изображений с помощью GAN -сети более изощренным способом, чем модель для генерации рукописных цифр, и в процессе тренировки также применяются некоторые приемы, которые де-лают процесс стабильным. Для получения более подробной информации вы можете почитать указанную ранее исследовательскую работу либо перейти по ссылке и посмотреть на реализацию в TensorFlow. Мы просто покажем вам, как настроить тренировочный набор и натренировать модель pix2pix для улуч-шения изображений с низким разрешением. 1. Выпо лните клонирование репозитория в своем терминале: git clone https://github.com/jeffxtang/pix2pix-tensorflowcd pix2pix-tensorflow 2. С оздайте новый каталог photos/original и скопируйте в него несколь- ко файлов изображений – например, мы скопировали в каталог photos/ original все фотографии лабрадора-ретривера из набора данных о соба- ках Stanford Dog Dataset (http://vision.stanford.edu/aditya86/ImageNetDogs), который мы использовали в главе 2 «Классифицирование изображений с помощью трансферного обучения». 3. Выпо лните команду со сценарем Python python tools/process.py ‑‑input_ dir photos/original ‑‑operation resize ‑‑output_dir photos/resized , которая изменит размеры изображений, находящихся в каталоге photos/original , и сохранит измененные изображения в каталоге photos/resized . 4. Выпо лните команду mkdir photos/blurry , а затем команду со сценарием python tools/convert.py , которая выполнит конвертацию изображения с измененными размерами в размытые, используя популярную команду библиотеки ImageMagick convert . Программный код сценария convert.py выглядит следующим образом: import osfile_names = os.listdir(\"photos/resized/\")for f in file_names: if f.find(\".png\")!= ‑‑1: os.system(\"convert photos/resized/\" + f + \" ‑blur 0x3 photos/blurry/\" + f) 5. Об ъедините все файлы из каталогов photos/resized и photos/blurry в пары и сохраните все парные изображения (одно изображение с измененным размером и другое размытое) в каталоге photos/resized_blurry : python tools/process.py --input_dir photos/resized --b_dir photos/blurry --operation combine --output_dir photos/resized_blurry 6. Выпо лните инструмент разделения python tools/split.py ‑‑dir photos/ resized_blurry для разбивки файлов на каталог train и каталог val. 7. Натренир уйте модель pix2pix , выполнив: python pix2pix.py \\ --mode train \\ --output_dir photos/resized_blurry/ckpt_1000 \\\n--- Страница 267 ---\nГенерирование и улучшение изображений с помощью GAN -сети  265 --max_epochs 1000 \\ --input_dir photos/resized_blurry/train \\ --which_direction BtoA Направление BtoA означает трансляцию из размытого изображения в исходное. На GTX -1070 GPU тренировка занимает около четырех часов, и результирующие файлы контрольных точек в каталоге photos/resized_ blurry/ckpt_1000 выглядят следующим образом: -rw-rw-r-- 1 jeff jeff 1721531 Mar 2 18:37 model -136000.meta -rw-rw-r-- 1 jeff jeff 81 Mar 2 18:37 checkpoint -rw-rw-r-- 1 jeff jeff 686331732 Mar 2 18:37 model -136000.data -00000-of -00001 -rw-rw-r-- 1 jeff jeff 10424 Mar 2 18:37 model -136000.index -rw-rw-r-- 1 jeff jeff 3807975 Mar 2 14:19 graph.pbtxt -rw-rw-r-- 1 jeff jeff 682 Mar 2 14:19 options.json 8. При необходимости вы можете выполнить сценарий в тестовом режиме и затем проверить результаты трансляции изображений в каталоге, ука-занном в параметре ‑‑output_dir : python pix2pix.py \\ --mode test \\ --output_dir photos/resized_blurry/output_1000 \\ --input_dir photos/resized_blurry/val \\ --checkpoint photos/resized_blurry/ckpt_1000 9. Выпо лните сценарий pix2pix_runinference.py , чтобы восстановить конт - рольную точку, сохраненную на шаге 7, создать новый заполнитель для ввода изображения, подать туда тестовое изображение ww.png , вывести результат трансляции как result.png и, наконец, сохранить новые файлы контрольных точек в каталоге newckpt : python pix2pix_runinference.py \\--mode test \\--output_dir photos/blurry_output \\--input_dir photos/blurry_test \\--checkpoint photos/resized_blurry/ckpt_1000 Фрагмент кода в следующем ниже сценарии pix2pix_runinference.py за- дает и распечатывает входные и выходные узлы: image_feed = tf.placeholder(dtype=tf.float32, shape=(1, 256, 256, 3), name=\"image_feed\")print(image_feed) # Tensor(\"image_feed:0\", shape=(1, 256, 256, 3), dtype=float32)with tf.variable_scope(\"generator\", reuse=True): output_image = deprocess(create_generator(image_feed, 3)) print(output_image) #Tensor(\"generator_1/deprocess/truediv:0\", shape=(1, 256, 256, 3), dtype=float32) Строка с инструкцией tf.variable_scope(\"generator\", reuse=True): очень важна, так как переменная generator должна быть в общем доступе, что-\n--- Страница 268 ---\n266  Г енерирование и улучшение изображений с помощью GAN -сети бы ей могли использоваться все натренированные значения параметров. В противном случае вы увидите причудливые результаты трансляции.В следующем ниже программном коде показано, как заполнить запол-нитель, запустить GAN-модель и сохранить в каталоге newckpt выходные данные генератора, а также файлы контрольных точек: if a.mode == \"test\": from scipy import misc image = misc.imread(\"ww.png\").reshape(1, 256, 256, 3) image = (image / 255.0) * 2 ‑1 result = sess.run(output_image, feed_dict={image_feed: image}) misc.imsave(\"result.png\", result.reshape(256, 256, 3)) saver.save(sess, \"newckpt/pix2pix\") На рис. 9.1 показано исходное тестовое изображение, его размытая версия и выходное изображение из генератора натренированной GAN-модели. Результат не идеален, но GAN-модель имеет более высокое раз-решение без эффекта размытости. Рисунок 9.1. Оригинальное, размытое и сгенерированное изображения 10. Т еперь скопируйте каталог newckpt в папку /tmp , и мы можем заморозить модель следующим образом: python tensorflow/python/tools/freeze_graph.py \\--input_meta_graph=/tmp/newckpt/pix2pix.meta \\--input_checkpoint=/tmp/newckpt/pix2pix \\--output_graph=/tmp/newckpt/pix2pix.pb \\--output_node_names=\"generator_1/deprocess/truediv\" \\--input_binary=true 11. Сг енерированный файл модели pix2pix.pb довольно большой, около 217 Мб, и приведет к аварийному сбою или вызовет ошибку нехватки памяти (Out of Memory, OOM) во время ее загрузки на устройстве iOS или Android. Мы должны трансформировать и конвертировать его в формат\n--- Страница 269 ---\nГенерирование и улучшение изображений с помощью GAN -сети  267 memmapped для iOS, как мы это делали со сложной моделью im2txt в гла- ве 6 «Описание изображений на естественном языке»: bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\--in_graph=/tmp/newckpt/pix2pix.pb \\--out_graph=/tmp/newckpt/pix2pix_transformed.pb \\--inputs=\"image_feed\" \\--outputs=\"generator_1/deprocess/truediv\" \\--transforms='strip_unused_nodes(type=float, shape=\"1,256,256,3\") fold_constants(ignore_errors=true, clear_output_shapes=true) fold_batch_norms fold_old_batch_norms' bazel‑bin/tensorflow/contrib/util/convert_graphdef_memmapped_format \\ ‑‑in_graph=/tmp/newckpt/pix2pix_transformed.pb \\ ‑‑out_graph=/tmp/newckpt/pix2pix_transformed_memmapped.pb Файл модели pix2pix_transformed_memmapped.pb теперь можно использо- вать в iOS. 12. Для того чтобы построить модель для Android, нам нужно проквантовать замороженную модель, чтобы уменьшить размер модели с 217 Мб до 54 Мб: bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\--in_graph=/tmp/newckpt/pix2pix.pb \\--out_graph=/tmp/newckpt/pix2pix_transformed_quantized.pb --inputs=\"image_feed\" \\--outputs=\"generator_1/deprocess/truediv\" \\--transforms='quantize_weights' Теперь давайте посмотрим, каким образом мы могли бы применить две GAN-модели в мобильных приложениях. использование Gan- моДелей в ios Если вы попытаетесь использовать модуль TensorFlow в своем приложение для iOS и загрузить файл gan_mnist.pb, то получите сообщение об ошибке: Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'RandomStandardNormal' with these attrs. Registered devices: [CPU], Registered kernels:<no registered kernels>[[Node: z_1/RandomStandardNormal = RandomStandardNormal[T=DT_INT32, _output_shapes=[[50,100]], dtype=DT_FLOAT, seed=0, seed2=0](z_1/shape)]] Проследите, чтобы ваш файл tensorflow/contrib/makefile/tf_op_files.txt имел строку tensorflow/core/kernels/random_op.cc , реализующую операцию RandomStandardNormal , и библиотека libtensorflow ‑core.a была собрана с по - мощью сценария оболочки tensorflow/contrib/makefile/build_all_ios.sh , после того как эта строка будет добавлена в файл tf_op_files.txt .\n--- Страница 270 ---\n268  Г енерирование и улучшение изображений с помощью GAN -сети Кроме того, при попытке загрузить файл модели pix2pix_transformed_ memmapped.pb даже в пользовательскую библиотеку TensorFlow, собранную с по- мощью TensorFlow 1.4, вы получите следующую ошибку: No OpKernel was registered to support Op 'FIFOQueueV2' with these attrs. Registered devices: [CPU], Registered kernels:<no registered kernels>[[Node: batch/fifo_queue = FIFOQueueV2[_output_shapes=[[]], capacity=32, component_types=[DT_STRING, DT_FLOAT, DT_FLOAT], container=\"\", shapes=[[], [256,256,1], [256,256,2]], shared_name=\"\"]()]] Вам нужно добавить строку tensorflow/core/kernels/fifo_queue_op.cc в файл tf_op_files.txt и заново собрать библиотеку iOS. Но если вы используете TensorFlow 1.5 или 1.6, то строка tensorflow/core/kernels/fifo_queue_op.cc уже в файл tf_op_files.txt добавлена. С каждой новой версией TensorFlow все больше ядер добавляется в файл tf_op_files.txt по умолчанию. После того как для наших моделей выполнена сборка библиотеки TensorFlow для iOS, давайте создадим в среде Xcode новый проект под названием GAN и на- строим платформу TensorFlow в проекте, как мы это делали в главе 8 «Пред- сказание биржевой цены с помощью RNN-сети» и в других главах, где модуль TensorFlow не использовался. Затем перетащите в проект два файла модели gan_mnist.pb и pix2pix_transformed_memmapped.pb и одно тестовое изображение. Кроме того, скопируйте файлы tensorflow_utils.h , tensorflow_utils.mm , ios_ image_load.h и ios_image_load.mm из проекта iOS в главе 6 «Описание изображе- ний на естественном языке» в проект GAN. Переименуйте файл ViewController.m в ViewController.mm . Теперь ваша среда Xcode должна выглядеть, как на рис. 9.2. Рисунок 9.2. Приложение GAN в среде Xcode\n--- Страница 271 ---\nГенерирование и улучшение изображений с помощью GAN -сети  269 Теперь мы создадим кнопку, которая при ее касании предложит пользова- телю выбрать модель для генерирования цифр или улучшения изображения: ‑ (IBAction)btnTapped:(id)sender { UIAlertAction* mnist = [UIAlertAction actionWithTitle:@\"Generate Digits\" style: UIAlertActionStyleDefault handler:^(UIAlertAction * action) { _iv.image = NULL; dispatch_async(dispatch_get_global_queue(0, 0), ^{ NSArray *arrayGreyscaleValues = [self runMNISTModel]; dispatch_async(dispatch_get_main_queue(), ^{ UIImage *imgDigit = [self createMNISTImageInRect:_iv.frame values: arrayGreyscaleValues]; _iv.image = imgDigit; }); }); }]; UIAlertAction* pix2pix = [UIAlertAction actionWithTitle:@\"Enhance Image\" style: UIAlertActionStyleDefault handler:^(UIAlertAction * action) { _iv.image = [UIImage imageNamed: image_name]; dispatch_async(dispatch_get_global_queue(0, 0), ^{ NSArray *arrayRGBValues = [self runPix2PixBlurryModel]; dispatch_async(dispatch_get_main_queue(), ^{ UIImage *imgTranslated = [self createTranslatedImageInRect:_iv.frame values: arrayRGBValues]; _iv.image = imgTranslated; }); }); }]; UIAlertAction* none = [UIAlertAction actionWithTitle:@\"None\" style: UIAlertActionStyleDefault handler:^(UIAlertAction * action) {}]; UIAlertController* alert = [UIAlertController alertControllerWithTitle:@\"Use GAN to\" message: nil preferredStyle: UIAlertControllerStyleAlert]; [alert addAction: mnist]; [alert addAction: pix2pix]; [alert addAction: none]; [self presentViewController: alert animated: YES completion: nil]; } Программный код здесь довольно прост. Основная функциональность при- ложения реализована в четырех методах: runMNISTModel , runPix2PixBlurryModel , createMNISTImageInRect и createTranslatedImageInRect .\n--- Страница 272 ---\n270  Г енерирование и улучшение изображений с помощью GAN -сети использование базовой Gan- моДели В методе runMNISTModel мы вызываем вспомогательный метод LoadModel для загрузки GAN-модели, а затем устанавливаем входной тензор равным 6 паке- там из 100 случайных чисел из нормального распределения (среднее значе-ние – 0.0 и стандартное отклонение – 1.0). Модель ожидает именно случайных входных данных с нормальным распределением. Вы можете изменить число 6 на любое другое и получить назад такое же количество сгенерированных цифр: ‑ (NSArray*) runMNISTModel { tensorflow:: Status load_status; load_status = LoadModel(@\"gan_mnist\", @\"pb\", &tf_session); if (!load_status.ok()) return NULL; std:: string input_layer = \"z_placeholder\"; std:: string output_layer = \"Sigmoid_1\"; tensorflow:: Tensor input_tensor(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({6, 100})); auto input_map = input_tensor.tensor<float, 2>(); unsigned seed = (unsigned)std:: chrono:: system_clock:: now().time_since_epoch().count(); std:: default_random_engine generator (seed); std:: normal_distribution<double> distribution(0.0, 1.0); for (int i = 0; i < 6; i++){ for (int j = 0; j < 100; j++){ double number = distribution(generator); input_map(i, j) = number; } } Остальная часть программного кода в методе runMNISTModel выполняет мо- дель, получает на выходе 6*28*28 вещественных чисел, представляющих зна-чение оттенков серого в каждом пикселе для каждого пакета изображения с размером 28*28, и вызывает метод createMNISTImageInRect для отображения чисел в контексте изображения с помощью UIBezierPath перед конвертацией контекста изображения в объект UIImage , который возвращается и выводится на экран с помощью элемента управления UIImageView : std:: vector<tensorflow:: Tensor> outputs;tensorflow:: Status run_status = tf_session ‑>Run({{input_layer, input_tensor}}, {output_layer}, {}, &outputs); if (!run_status.ok()) { LOG(ERROR) << \"Running model failed: \" << run_status; return NULL;\n--- Страница 273 ---\nГенерирование и улучшение изображений с помощью GAN -сети  271 } tensorflow:: string status_string = run_status.ToString();tensorflow:: Tensor* output_tensor = &outputs[0]; const Eigen:: TensorMap<Eigen:: Tensor<float, 1, Eigen:: RowMajor>, Eigen:: Aligned>& output = output_tensor ‑>flat<float>(); const long count = output.size();NSMutableArray *arrayGreyscaleValues = [NSMutableArray array]; for (int i = 0; i < count; ++i) { const float value = output(i); [arrayGreyscaleValues addObject:[NSNumber numberWithFloat: value]]; } return arrayGreyscaleValues; } Метод CreateMNISTImageInRect определяется следующим образом: мы исполь- зовали подобную технику в главе 7 «Распознавание рисунков с помощью CNN- и LSTM-сетей»: ‑ (UIImage *)createMNISTImageInRect:(CGRect)rect values:(NSArray*)greyscaleValues { UIGraphicsBeginImageContextWithOptions(CGSizeMake(rect.size.width, rect.size.height), NO, 0.0); int i=0; const int size = 3; for (NSNumber *val in greyscaleValues) { float c = [val floatValue]; int x = i%28; int y = i/28; i++; CGRect rect = CGRectMake(145+size*x, 50+y*size, size, size); UIBezierPath *path = [UIBezierPath bezierPathWithRect: rect]; UIColor *color = [UIColor colorWithRed: c green: c blue: c alpha:1.0]; [color setFill]; [path fill]; } UIImage *image = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); return image; } Для каждого пиксела мы рисуем небольшой прямоугольник шириной и вы- сотой, равной 3, с возвращаемым для пиксела значением оттенков серого.\n--- Страница 274 ---\n272  Г енерирование и улучшение изображений с помощью GAN -сети использование проДвинутой Gan- моДели В методе runPix2PixBlurryModel мы используем метод LoadMemoryMappedModel для загрузки файла модели pix2pix_transformed_memmapped.pd . Потом мы загружа- ем тестовое изображение и затем настраиваем входной тензор подобно тому, как мы делали в главе 4 «Трансформирование рисунков с помощью художествен- ных стилей»: ‑ (NSArray*) runPix2PixBlurryModel { tensorflow:: Status load_status; load_status = LoadMemoryMappedModel(@\"pix2pix_transformed_memmapped\", @\"pb\", &tf_session, &tf_memmapped_env); if (!load_status.ok()) return NULL; std:: string input_layer = \"image_feed\"; std:: string output_layer = \"generator_1/deprocess/truediv\"; NSString* image_path = FilePathForResourceName(@\"ww\", @\"png\"); int image_width; int image_height; int image_channels; std:: vector<tensorflow:: uint8> image_data = LoadImageFromFile([image_path UTF8String], &image_width, &image_height, &image_channels); Затем мы запускаем модель, получаем на выходе вещественные числа 256*256*3 (размер изображения составляет 256*256, а 3 – это значения RGB) и вызываем метод createTranslatedImageInRect для конвертации чисел в Ullmage : std:: vector<tensorflow:: Tensor> outputs;tensorflow:: Status run_status = tf_session ‑>Run({{input_layer, image_tensor}}, {output_layer}, {}, &outputs); if (!run_status.ok()) { LOG(ERROR) << \"Running model failed: \" << run_status; return NULL; }tensorflow:: string status_string = run_status.ToString();tensorflow:: Tensor* output_tensor = &outputs[0]; const Eigen:: TensorMap<Eigen:: Tensor<float, 1, Eigen:: RowMajor>, Eigen:: Aligned>& output = output_tensor ‑>flat<float>(); const long count = output.size(); // 256*256*3NSMutableArray *arrayRGBValues = [NSMutableArray array]; for (int i = 0; i < count; ++i) { const float value = output(i); [arrayRGBValues addObject:[NSNumber numberWithFloat: value]]; }\n--- Страница 275 ---\nГенерирование и улучшение изображений с помощью GAN -сети  273 return arrayRGBValues; Окончательный метод createTranslatedImageInRect определяется следующим образом, где все довольно понятно: ‑ (UIImage *)createTranslatedImageInRect:(CGRect)rect values:(NSArray*)rgbValues { UIGraphicsBeginImageContextWithOptions(CGSizeMake(wanted_width, wanted_height), NO, 0.0); for (int i=0; i<256*256; i++) { float R = [rgbValues[i*3] floatValue]; float G = [rgbValues[i*3+1] floatValue]; float B = [rgbValues[i*3+2] floatValue]; const int size = 1; int x = i%256; int y = i/256; CGRect rect = CGRectMake(size*x, y*size, size, size); UIBezierPath *path = [UIBezierPath bezierPathWithRect: rect]; UIColor *color = [UIColor colorWithRed: R green: G blue: B alpha:1.0]; [color setFill]; [path fill]; } UIImage *image = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); return image; } Теперь запустите приложение в симуляторе или на устройстве iOS, нажмите кнопку GAN и выберите Generate Digits (Сгенерировать цифры), и в резуль- тате вы увидите рукописные цифры, сгенерированные GAN-моделью, как по-казано на рис. 9.3.\n--- Страница 276 ---\n274  Г енерирование и улучшение изображений с помощью GAN -сети Рисунок 9.3. Выбор GAN-модели и сгенерированные моделью рукописные цифры Цифры очень похожи на реальные, написанные человеком, и выполнены они после тренировки базовой GAN-модели. Если вы вернетесь и просмотрите про- граммный код, который выполняет тренировку, остановитесь и на мгновение подумаете о том, как GAN-модель работает в целом, как генератор и дискри-минатор играют друг против друга и пытаются достичь состояния стабильного равновесия Нэша, в котором генератор может порождать похожие на реальные поддельные данные, которые дискриминатор не способен отличить от реаль-ных, то вы, вероятно, больше проникнитесь тем, насколько потрясающей GAN-сеть является или может оказаться. Теперь давайте выберем вариант Enhance Image (Улучшить изображение), и вы увидите результат, как на рис. 9.4, тот же результат, что и сгенериро-ванный тестовым программным кодом на языке Python, приводимым выше на рис. 9.1.\n--- Страница 277 ---\nГенерирование и улучшение изображений с помощью GAN -сети  275 Рисунок 9.4. Исходное размытое и улучшенное изображения в iOS Вы знаете правила игры. Пришло время уделить внимание ОС Android. использование Gan- моДелей в android Оказывается, что для запуска GAN-моделей в Android нам не нужно исполь- зовать пользовательскую библиотеку TensorFlow, как мы это делали в главе 7 «Распознавание рисунков с помощью CNN- и LSTM-сетей». Просто создайте новое приложение в среде Android Studio под названием GAN со всеми значениями по умолчанию, добавьте строку compile 'org.tensorflow: tensorflow ‑android:+' в файле build.gradle приложения, создайте новую папку ресурсов assets и ско- пируйте туда два файла GAN-модели и тестовое размытое изображение. Теперь ваш проект в среде Android Studio должен выглядеть так, как показа- но на рис. 9.5.\n--- Страница 278 ---\n276  Г енерирование и улучшение изображений с помощью GAN -сети Рисунок 9.5. Обзор приложения GAN в среде Android Studio с постоянными определениями Обратите внимание, что для простоты мы задаем размер пакета BATCH_SIZE равным 1. Вы можете легко присвоить ему любое число и получить назад это же количество результатов, как мы сделали в iOS. Кроме констант, определенных на рис. 9.5, мы создадим несколько экземплярных переменных: private Button mButtonMNIST;private Button mButtonPix2Pix;private ImageView mImageView;private Bitmap mGeneratedBitmap;private boolean mMNISTModel; private TensorFlowInferenceInterface mInferenceInterface; Макет приложения состоит из одного элемента управления ImageView и двух кнопок, как мы делали раньше, и они инстанцируются в методе onCreate : protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); mButtonMNIST = findViewById(R.id.mnistbutton); mButtonPix2Pix = findViewById(R.id.pix2pixbutton); mImageView = findViewById(R.id.imageview); try { AssetManager am = getAssets(); InputStream is = am.open(IMAGE_NAME); Bitmap bitmap = BitmapFactory.decodeStream(is); mImageView.setImageBitmap(bitmap); } catch (IOException e) { e.printStackTrace(); }\n--- Страница 279 ---\nГенерирование и улучшение изображений с помощью GAN -сети  277 Затем настройте два слушателя касаний для двух кнопок: mButtonMNIST.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { mMNISTModel = true; Thread thread = new Thread(MainActivity.this); thread.start(); } }); mButtonPix2Pix.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { try { AssetManager am = getAssets(); InputStream is = am.open(IMAGE_NAME); Bitmap bitmap = BitmapFactory.decodeStream(is); mImageView.setImageBitmap(bitmap); mMNISTModel = false; Thread thread = new Thread(MainActivity.this); thread.start(); } catch (IOException e) { e.printStackTrace(); } } });} При касании кнопки метод run выполняется в рабочем потоке: public void run() { if (mMNISTModel) runMNISTModel(); else runPix2PixBlurryModel(); } использование базовой Gan- моДели В методе runMNISTModel мы сначала подготовим случайные входные данные, которые подаются в модель: void runMNISTModel() { float[] floatValues = new float[BATCH_SIZE*100]; Random r = new Random(); for (int i=0; i<BATCH_SIZE; i++) { for (int j=0; i<100; i++) { double sample = r.nextGaussian();\n--- Страница 280 ---\n278  Г енерирование и улучшение изображений с помощью GAN -сети floatValues[i] = (float)sample; } } Затем подадим эти входные данные в модель, запустим модель и получим выходные значения, которые являются шкалированными значениями оттен- ков серого от 0,0 до 1,0, и конвертируем их в целые числа в диапазоне от 0 до 255: float[] outputValues = new float[BATCH_SIZE * 28 * 28];AssetManager assetManager = getAssets();mInferenceInterface = new TensorFlowInferenceInterface(assetManager, MODEL_FILE1); mInferenceInterface.feed(INPUT_NODE1, floatValues, BATCH_SIZE, 100); mInferenceInterface.run(new String[] {OUTPUT_NODE1}, false);mInferenceInterface.fetch(OUTPUT_NODE1, outputValues); int[] intValues = new int[BATCH_SIZE * 28 * 28]; for (int i = 0; i < intValues.length; i++) { intValues[i] = (int) (outputValues[i] * 255); } После этого при создании объекта Bitmap мы устанавливаем возвращенные и конвертированные значения оттенков серого для каждого пиксела: try { Bitmap bitmap = Bitmap.createBitmap(28, 28, Bitmap.Config.ARGB_8888); for (int y=0; y<28; y++) { for (int x=0; x<28; x++) { int c = intValues[y*28 + x]; int color = (255 & 0xff) << 24 | (c & 0xff) << 16 | (c & 0xff) << 8 | (c & 0xff); bitmap.setPixel(x, y, color); } } mGeneratedBitmap = Bitmap.createBitmap(bitmap); }catch (Exception e) { e.printStackTrace(); } Наконец, мы показываем растровое изображение в элементе управления ImageView основного потока пользовательского интерфейса: runOnUiThread( new Runnable() { @Override public void run() { mImageView.setImageBitmap(mGeneratedBitmap); } }); }\n--- Страница 281 ---\nГенерирование и улучшение изображений с помощью GAN -сети  279 Если запустить приложение сейчас, с пустой реализацией void runPix2PixBlurryModel() {}, чтобы избежать ошибки сборки, то вы увидите на- чальный экран и результат после касания кнопки GENERA TE DIGITS (Сгене- рировать цифры), как показано на рис. 9.6. Рисунок 9.6. Изображение сгенерированных цифр использование проДвинутой Gan- моДели Метод runPix2PixBlurryModel похож на программный код из предыдущих глав, где мы использовали входное изображение для подачи в наши модели. Сна- чала мы получаем значения RGB из растрового изображения и сохраняем их в вещественном массиве с типом float: void runPix2PixBlurryModel() { int[] intValues = new int[WANTED_WIDTH * WANTED_HEIGHT]; float[] floatValues = new float[WANTED_WIDTH * WANTED_HEIGHT * 3]; float[] outputValues = new float[WANTED_WIDTH * WANTED_HEIGHT * 3]; try { Bitmap bitmap = BitmapFactory.decodeStream(getAssets().open(IMAGE_NAME)); Bitmap scaledBitmap = Bitmap.createScaledBitmap(bitmap, WANTED_WIDTH, WANTED_HEIGHT, true);\n--- Страница 282 ---\n280  Г енерирование и улучшение изображений с помощью GAN -сети scaledBitmap.getPixels(intValues, 0, scaledBitmap.getWidth(), 0, 0, scaledBitmap. getWidth(), scaledBitmap.getHeight()); for (int i = 0; i < intValues.length; ++i) { final int val = intValues[i]; floatValues[i * 3 + 0] = (((val >> 16) & 0xFF) ‑‑IMAGE_MEAN) / IMAGE_STD; floatValues[i * 3 + 1] = (((val >> 8) & 0xFF) ‑‑IMAGE_MEAN) / IMAGE_STD; floatValues[i * 3 + 2] = ((val & 0xFF) ‑‑IMAGE_MEAN) / IMAGE_STD; } Затем мы запускаем модель с входными данными, получаем выходные зна- чения и конвертируем их в целочисленный массив, который потом использу - ется для установки пикселов нового растрового изображения: AssetManager assetManager = getAssets();mInferenceInterface = new TensorFlowInferenceInterface(assetManager, MODEL_FILE2);mInferenceInterface.feed(INPUT_NODE2, floatValues, 1, WANTED_HEIGHT, WANTED_WIDTH, 3);mInferenceInterface.run(new String[] {OUTPUT_NODE2}, false);mInferenceInterface.fetch(OUTPUT_NODE2, outputValues); for (int i = 0; i < intValues.length; ++i) { intValues[i] = 0xFF000000 | (((int) (outputValues[i * 3] * 255)) << 16) | (((int) (outputValues[i * 3 + 1] * 255)) << 8) | ((int) (outputValues[i * 3 + 2] * 255)); }Bitmap outputBitmap = scaledBitmap.copy(scaledBitmap.getConfig(), true); outputBitmap.setPixels(intValues, 0, outputBitmap.getWidth(), 0, 0, outputBitmap.getWidth(), outputBitmap.getHeight());mGeneratedBitmap = Bitmap.createScaledBitmap(outputBitmap, bitmap.getWidth(), bitmap.getHeight(), true); } catch (Exception e) { e.printStackTrace(); } Наконец, мы показываем растровое изображение в элементе управления ImageView основного пользовательского интерфейса: runOnUiThread( new Runnable() { @Override public void run() { mImageView.setImageBitmap(mGeneratedBitmap); } }); }\n--- Страница 283 ---\nГенерирование и улучшение изображений с помощью GAN -сети  281 Запустите приложение еще раз и нажмите кнопку ENHANCE IMAGE (Улуч- шить изображение). Через несколько секунд вы увидите улучшенное изобра- жение, как на рис. 9.7. Рисунок 9.7. Размытое и улучшенное изображения в Android Вот и все наше приложение для Android с использованием двух GAN-моделей. резюме В этой главе мы провели небольшую экскурсию по удивительному миру ге- неративно-состязательных сетей, или GAN-сетей. Мы рассмотрели, что такое GAN-сети и почему они так интересны – то, как генератор и дискриминатор играют друг против друга и пытаются победить друг друга, вероятно, звучит привлекательно для большинства людей. Затем мы подробно рассмотрели шаги тренировки базовой модели на основе GAN-сети и более продвинутую модель улучшения разрешающей способности изображения, а также их под-готовку для мобильных устройств. Наконец, мы показали вам, как создавать приложения для iOS и Android с помощью данных моделей. Если вы в восторге от всего процесса и полученных результатов, то наверняка захотите по дробно изучить GAN-сети, эту быстро развивающаяся область, где вскоре были раз- работаны новые типы GAN-сетей, которые преодолевают недостатки преды-\n--- Страница 284 ---\n282  Г енерирование и улучшение изображений с помощью GAN -сети дущих моделей, например те же исследователи, которые разработали модель pix2pix, требующую для тренировки парных изображений, как мы видели в подразделе «Продвинутая GAN-модель улучшения разрешающей способно-сти изображения», придумали новый тип GAN-сети под названием CycleGAN (https: //junyanz.github.io/CycleGAN), который устраняет требование наличия парных изображений. Если вы не удовлетворены качеством наших сгенериро-ванных цифр или улучшенного изображения, то вам, по-видимому, тоже стоит заняться дальнейшим изучением GAN-сетей, чтобы выяснить, каким образом можно было бы усовершенствовать GAN-модели. Как мы уже упоминали ра-нее, GAN-сети еще очень молоды, исследователи все еще упорно работают над тем, чтобы стабилизировать процесс тренировки, и ожидается гораздо боль-ший успех, если его удастся стабилизировать. По крайней мере, к настояще-му времени вы приобрели опыт быстрого развертывания GAN-модели в мо-бильных приложениях. И теперь вам самим решать, следует ли приглядывать за новейшими и наилучшими GAN-сетями и использовать их на мобильных устройствах или же отложить свою шляпу разработчика мобильных приложе-ний в сторону на некоторое время и, засучив рукава, заняться созданием но-вых или усовершенствованием существующих GAN-моделей. Если GAN-сети вызвали в сообществе глубокого обучения большое волне- ние, то достижение программы AlphaGo в 2016 и 2017 годах, одержавшей побе-ду над самыми маститыми игроками-людьми, определенно поразило всех, кто не живет в пещере. Более того, в октябре 2017 года был представлен AlphaGo Zero, новый алгоритм исключительно на основе самообучения по методу мак - симизации вознаграждения, без каких-либо человеческих знаний. Этот алго-ритм показал невероятный результат, победив программу AlphaGo с разгром-ным счетом 100:0; в декабре 2017 года был опубликован алгоритм AlphaZero, который, в отличие от алгоритмов AlphaGo и AlphaGo Zero, ориентированных исключительно на игру го, может добиваться «исключительной результатив-ности во многих трудноразрешимых областях». В следующей главе мы увидим, как применять новейший и самый невероятный алгоритм AlphaZero для по-строения и тренировки модели для игры в простую и забавную игру и как за-пускать такую модель на мобильных устройствах.",
      "debug": {
        "start_page": 259,
        "end_page": 284
      }
    },
    {
      "name": "Глава 10. Создание мобильного игрового AlphaZero-подобного приложения 283",
      "content": "--- Страница 285 --- (продолжение)\nГлава 10 Создание мобильного игрового AlphaZero-подобного приложения Несмотря на растущую популярность современного искусственного интеллек - та (ИИ), по существу, вызванного необычайным инновационным прорывом в области глубокого обучения в 2012 году, исторические события, связанные с победой программы AlphaGo компании Google DeepMind над Ли Седолем, 18-кратным чемпионом мира по игре го, со счетом 4:1 в марте 2016 года, а за-тем победа над Ке Цзе, занимающим в настоящее время 1-е место среди игро-ков в го, со счетом 3:0 в мае 2017 года, во многом способствовали тому, что ИИ стал притчей во языцех. Из-за сложности игры го все были абсолютно уверены в том, что, по крайней мере, еще целое десятилетие компьютерная программа не будет в состоянии побить лучших игроков в го. После матча между AlphaGo и Ке Цзе в мае 2017 года компания Google сняла программу AlphaGo с эксплуатации; DeepMind, стартап компании Google, при-обрел ее новаторскую технологию глубокого самообучения с подкреплением, и разработчик программы AlphaGo решил сосредоточить свои исследователь-ские усилия в других областях ИИ. Затем, что интересно, в октябре 2017 года стартап DeepMind опубликовал еще одну статью об игре го: «Mastering the Game of GO without Human Knowledge» («Освоение игры в го без человеческих знаний») (https://deepmind.com/research/publications/mastering-game-go-without-human-knowledge), описывающую усовершенствованный алгоритм под назва-нием AlphaGo Zero, который учится играть исключительно на основе самообу - чения с подкреплением во время самоигры (self-play) без опоры на какие-либо человеческие экспертные знания, такие как большое количество гроссмей-стерских партий, которые использовались в программе AlphaGo для трени-ровки модели. Самое удивительное то, что программа AlphaGo Zero нанесла разгромное поражение со счетом 100:0 программе AlphaGo, которая всего не-сколько месяцев тому назад усмирила лучшего в мире игрока-человека!\nГлава 10 Создание мобильного игрового AlphaZero-подобного приложения Несмотря на растущую популярность современного искусственного интеллек - та (ИИ), по существу, вызванного необычайным инновационным прорывом в области глубокого обучения в 2012 году, исторические события, связанные с победой программы AlphaGo компании Google DeepMind над Ли Седолем, 18-кратным чемпионом мира по игре го, со счетом 4:1 в марте 2016 года, а за-тем победа над Ке Цзе, занимающим в настоящее время 1-е место среди игро-ков в го, со счетом 3:0 в мае 2017 года, во многом способствовали тому, что ИИ стал притчей во языцех. Из-за сложности игры го все были абсолютно уверены в том, что, по крайней мере, еще целое десятилетие компьютерная программа не будет в состоянии побить лучших игроков в го. После матча между AlphaGo и Ке Цзе в мае 2017 года компания Google сняла программу AlphaGo с эксплуатации; DeepMind, стартап компании Google, при-обрел ее новаторскую технологию глубокого самообучения с подкреплением, и разработчик программы AlphaGo решил сосредоточить свои исследователь-ские усилия в других областях ИИ. Затем, что интересно, в октябре 2017 года стартап DeepMind опубликовал еще одну статью об игре го: «Mastering the Game of GO without Human Knowledge» («Освоение игры в го без человеческих знаний») (https://deepmind.com/research/publications/mastering-game-go-without-human-knowledge), описывающую усовершенствованный алгоритм под назва-нием AlphaGo Zero, который учится играть исключительно на основе самообу - чения с подкреплением во время самоигры (self-play) без опоры на какие-либо человеческие экспертные знания, такие как большое количество гроссмей-стерских партий, которые использовались в программе AlphaGo для трени-ровки модели. Самое удивительное то, что программа AlphaGo Zero нанесла разгромное поражение со счетом 100:0 программе AlphaGo, которая всего не-сколько месяцев тому назад усмирила лучшего в мире игрока-человека!\n--- Страница 286 ---\n284  Создание мобильного игрового AlphaZero-подобного приложения И это оказался лишь один-единственный шаг компании Google к более амбициозной цели по применению и улучшению лежащих в основе алго- ритма AlphaGo методов ИИ в других областях. В декабре 2017 года стартап DeepMind опубликовал еще одну статью «Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm» («Освоение игры в шахматы и сеги 1 путем самостоятельной игры с помощью универсального алгорит - ма самообуче ния с подкреплением») (ps: //arxiv.org/pdf/1712.01815.pdf), кото - рый обобщил программу AlphaGo Zero до одного алгоритма под названием AlphaZero и использовал данный алгоритм, для того чтобы быстро научиться играть в шахматы и сеги с нуля, начиная со случайной игры и без предметных знаний, кроме правил игры, и в результате за 24 часа достиг сверхчеловеческо-го уровня, одержав верх над чемпионами мира. В этой главе мы проведем для вас экскурсию по новейшим и самым неверо- ятным моделям AlphaZero, покажем вам, как построить и натренировать мо-дель AlphaZero для игры в простую, но забавную игру под названием «Четыре в ряд» (Connect 4) (https: //en.wikipedia.org/wiki/Connect_Four) с использованием TensorFlow и Keras, популярных высокоуровневых библиотек глубокого обуче - ния, которые мы использовали в главе 8 «Предсказание биржевой цены с по- мощью RNN-сети». Мы также расскажем, как использовать натренированную AlphaZero-подобную модель, чтобы выработать натренированную экспертную линию поведения, руководящую игровым процессом на мобильном устрой-стве, и предложим исходный код полноценных приложений для iOS и Android, которые играют в игру «Четыре в ряд» с использованием данной модели. Резюмируя, в этой главе мы рассмотрим следующие темы: алгоритм AlphaZero – как он рабо тает; построение и тренировка AlphaZero-подобной модели игры «Четыре в ряд»; испо льзование модели игры «Четыре в ряд» в iOS; применение мо дели игры в «Четыре в ряд» в Android. алгоритм alPhaZero – как он работает ? Алгоритм AlphaZero состоит из трех основных компонентов: глубокая сверточная нейронная сеть, которая принимает позицию на игровом поле (или состояние) в качестве входных данных и выводит значение в качестве предсказанного результата игры, исходя из позиции и линии поведения (так называемой политики), которая представляет собой список вероятностей ходов для каждого возможного действия из состояния входного игрового поля; универсальный алгоритм самообучения с подкреплением, который учится посредством самоигры с нуля без каких-либо конкретных пред- 1 С ёги (Shogi) – японская настольная логическая игра шахматного типа. – Прим. перев.\n--- Страница 287 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  285 метных знаний, кроме правил игры. Параметры глубокой нейронной сети заучиваются с помощью самообучения с подкреплением во вре-мя самоигры, минимизируя потерю между предсказанным значением и фактическим результатом самоигры и максимизируя сходство между предсказанной линией поведения и вероятностями результатов поиска, которые являются результатом следующего алгоритма; универсальный (предметно-независимый) алгоритм древовидного по- иска по методу Монте-Карло (Monte-Carlo Tree Search, МСТS)1, который симулирует партии самостоятельной игры от начала до конца, отбирая каждый шаг во время симуляции с учетом предсказанного значения и значений вероятностей линии поведения, возвращаемых из глубокой нейронной сети, а также того, как часто узел посещался, – время от вре- мени выбирая узел с низким количеством визитов. Такой подход в само-обучении с подкреплением, называемым разведкой (в отличие от при-нятия хода с высоким предсказанным значением и линией поведения, который называется эксплуатацией). Хороший баланс между разведкой и эксплуатацией может привести к более высоким результатам. Обучение с подкреплением, а точнее самообучение с максимизацией полу - чаемого вознаграждения, имеет долгую историю, начиная с 1960-х годов, когда этот термин был впервые использован в инженерной литературе. Но техноло-гический прорыв произошел в 2013 году, когда стартап DeepMind объединил са-мообучение, осуществляемое на основе максимизации вознаграждения, с глу - боким обучением и разработал глубоко обучающиеся приложения на основе самообучения с подкреплением, которые самостоятельно обучались играть в игры Atari с нуля, где необработанные пикселы выступали в качестве входных данных, и впоследствии могли побеждать людей. В отличие от контролируемо-го обучения (обучения с учителем), которое для тренировки требует наличия помеченных данных, как мы видели во многих моделях, построенных и при-менявшихся нами в предыдущих главах, в самообучении с подкреплением ис - пользуется метод проб и ошибок: агент взаимодействует со средой и получает вознаграждение (положительное или отрицательное) за каждое действие, ко-торое он предпринимает в каждом состоянии. В примере с играющей в шах - маты программой AlphaZero награда приходит только после партии в виде +1 в случае выигрыша, –1 – в случае проигрыша, 0 – в случае ничьи. Алгоритм самообучения с подкреплением в программе AlphaZero использует упоминав-шийся нами ранее градиентный спуск на потере с целью обновления парамет - ров глубокой нейронной сети, которая действует как функция универсальной аппроксимации для заучивания и кодирования игрового опыта. Результатом процесса заучивания или выполнения тренировки может быть линия поведения, генерируемая глубокой нейронной сетью, которая сообщает, 1 Древовидный поиск по методу Монте-Карло (Monte Carlo Tree Search) – это алгоритм эвристического поиска для некоторых видов процессов принятия решений, в пер- вую очередь тех, которые используются в игре. – Прим. перев.\n--- Страница 288 ---\n286  Создание мобильного игрового AlphaZero-подобного приложения какие действия следует предпринимать в любом состоянии, или функция стои - мости, которая отображает каждое состояние и каждое возможное действие из этого состояния в долгосрочное вознаграждение. Если заученная глубокой нейронной сетью линия поведения с использова- нием самообучения с подкреплением на основе самоигры является идеаль- ной, то во время игры нам может не понадобиться давать программе выпол-нять древовидный поиск по методу Монте-Карло (MCTS) – программа может просто всегда выбирать ход с максимальной вероятностью. Однако в сложных играх, таких как шахматы или го, идеальную линию поведения сгенерировать невозможно, поэтому требуется кооперация древовидного поиска MCTS с на-тренированной глубокой сетью, и эта кооперация будет направлять поиск наи-лучших возможных действий для каждого состояния игры.  Ес ли вы незнакомы с самообучением с подкреплением или древовидным поиском по методу Монте-Карло (MCTS), то в интернете имеется достаточно много инфор-мации по этим темам. Вы можете обратиться к классической книге Ричарда Саттона (Richard Sutton) и Эндрю Барто (Andrew Barto) «Reinforcement Learning: An Introduction» («Самообучение с подкреплением: введение»), которая публично доступна по адресу http://incompleteideas.net/book/the-book -2nd.html. Вы также можете посмотреть ви- деоролики курса по самообучению с подкреплением Дэвида Сильвера (David Silver), технического руководителя направления AlphaGo в стартапе DeepMind, в YouTube (по-иск «reinforcement learning David Silver»). Забавным и полезным инструментарием для самообучения с подкреплением является так называемый «тренажерный зал» OpenAI, или OpenAI Gym (https: //gym.openai.com). В последней главе книги мы подробно рас - смотрим самообучение с подкреплением и OpenAI Gym. Что касается древовидно-го поиска по методу Монте-Карло (MCTS), то обратитесь к веб-странице в Википе-дии, https: //en.wikipedia.org/wiki/Monte_Carlo_tree_search, а также к этому блог-посту: http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works. В следующем разделе мы рассмотрим реализацию алгоритма AlphaZero с помощью библиотеки Keras, где платформа TensorFlow будет выступать в ка-честве бэкенда, и построим и натренируем модель на основе данного алгорит - ма для игры «Четыре в ряд». Вы увидите, как выглядят архитектура и ключевой программный код библиотеки Keras для построения данной модели. тренировка и тестирование alPhaZero-поДобной моДели Для игры «ч етыре в ряД» Если вы никогда не играли в игру «Четыре в ряд», то сможете поиграть в нее бесплатно по адресу http: //www.connectfour.org. Это быстрая и веселая игра. Суть игры состоит в том, что два игрока по очереди набрасывают разноцвет - ные диски на решетку из шести строк на семь столбцов поверх столбца. Вновь брошенный диск ложится вниз столбца, если ни одного диска в этот столбец брошено не было, либо поверх последнего брошенного диска в этом столбце. Побеждает тот игрок, у кого первым окажется четыре диска подряд его цве-\n--- Страница 289 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  287 та в любом из трех возможных направлений (горизонтальном, вертикальном, диагональном). Модель AlphaZero для игры «Четыре в ряд» основана на репозитории https: //github.com/jeffxtang/DeepReinforcementLearning, являющемся ответвле- нием https: //github.com/AppliedDataSciencePartners/DeepReinforcementLearning с хорошим блог-постом «How to build your own AlphaZero AI using Python and Keras» («Как построить свой собственный ИИ на основе алгоритма AlphaZero с помощью Python и Keras» (https: //applied-data.science/blog/how-to-build-your- own-alphazero-ai-using-python-and-keras)), который вы, вероятно, должны про-читать, прежде чем двигаться дальше, для того что последующие шаги имели для вас больше смысла. тренировка моДели Прежде чем мы взглянем на некоторые корневые фрагменты программного кода, давайте сначала посмотрим на тренировку модели. Сначала получите ре-позиторий, выполнив в терминале следующее: git clone https://github.com/jeffxtang/DeepReinforcementLearning Затем настройте виртуальную среду (virtualenv) Keras и TensorFlow, если вы еще этого не сделали в главе 8 «Предсказание биржевой цены с помощью RNN- сети»: cdmkdir ~/tf_kerasvirtualenv --system-site-packages ~/tf_keras/cd ~/tf_keras/source ./bin/activateeasy_install -U pip #В Mac: pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow -1.4.0- py2-none-any.whl #В Ubuntu: pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu -1.4.0-cp27-none-linux_x86_64.whl easy_install ipythonpip install keras Вы также можете попробовать URL-адрес для скачивания TensorFlow 1.5–1.8 в приведенной выше команде pip install . Теперь откройте блокнот Jupyter run.ipynb , сначала перейдя командой cd в папку DeepReinforcementLearning , затем выполнив команду jupyter notebook , – в случае если вы увидите какие-либо ошибки, то в зависимости от вашей среды\n--- Страница 290 ---\n288  Создание мобильного игрового AlphaZero-подобного приложения вам нужно будет установить отсутствующие пакеты Python. В браузере открой- те URL-адрес http://localhost:8888/notebooks/run.ipynb и выполните первый блок кода в блокноте, чтобы загрузить все необходимые базовые библиотеки, и второй блок кода, чтобы начать тренировку, – программный код был напи- сан так, что тренировка будет выполняться бесконечно, поэтому вы можете отменить команду jupyter notebook после нескольких часов тренировки. Преж - де чем вы сможете увидеть первые версии моделей, созданных в следующем ниже каталоге (более новая версия, к примеру version0004.h5 , включает в себя более тонко настроенные веса, чем веса в более старых версиях, таких как version0001.h5 ), на неновом Mac потребуется подождать около часа: (tf_keras) MacBook-Air: DeepReinforcementLearning jeffmbair$ ls -lt run/models -rw-r--r-- 1 jeffmbair staff 3781664 Mar 8 15:23 version0004.h5 -rw-r--r-- 1 jeffmbair staff 3781664 Mar 8 14:59 version0003.h5 -rw-r--r-- 1 jeffmbair staff 3781664 Mar 8 14:36 version0002.h5 -rw-r--r-- 1 jeffmbair staff 3781664 Mar 8 14:12 version0001.h5 -rw-r--r-- 1 jeffmbair staff 656600 Mar 8 12:29 model.png Файлы с расширением .h5 – э то файлы модели Keras в формате HDF5, каж - дый из которых содержит в основном определение архитектуры модели, на- тренированные веса и настройки конфигурации тренировки. Позже вы увиди-те, как использовать файлы модели Keras для создания файлов контрольных точек TensorFlow, которые затем можно заморозить в файлы моделей, работа-ющих на мобильных устройствах. Файл model.png содержит подробный вид глубокой нейросетевой архитек - туры. Она довольно глубокая со многими остаточными блоками сверточных слоев с последующей пакетной нормализацией и ReLU-слоями для стабилиза-ции тренировки. Верхняя часть модели выглядит так, как показано на следую-щей ниже схеме (мы не будем показывать среднюю часть, так как она доволь-но большая, вам рекомендуется самим открыть файл model.png для изучения архитектуры).\n--- Страница 291 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  289 Рисунок 10.1. Первые слои глубокой остаточной сети Стоит отметить, что эта нейросеть называется остаточной сетью (residual network, ResNet). Данный термин был внедрен компанией Microsoft в 2015 году в своих победных записях на конкурсах ImageNet и COCO 2015. В остаточной сети ResNet используется тождественное отображение (стрелка с правой сто-роны на рис. 10.1), которое позволяет избежать более высокой ошибки трени-ровки, когда сети становятся глубже. Для получения дополнительной инфор-мации о ResNet вы можете обратиться к оригинальной статье под названием «Deep Residual Learning for Image Recognition» («Глубокое остаточное обуче-ние для распознавания изображений») (https: //arxiv.org/pdf/1512.03385v1.pdf) и блог-посту «Understanding Deep Residual Networks» («Объяснение глубоких остаточных сетей») – прос той платформе модульного самообучения, в которой пересмотрено то, что составляет современное положение дел (https: //blog.waya. ai/deep-residual-learning -9610bb62c355).\n--- Страница 292 ---\n290  Создание мобильного игрового AlphaZero-подобного приложения Последние слои глубокой сети показаны на рис. 10.2, и вы можете видеть, что после конечного остаточного блока и сверточных слоев с пакетной норма- лизацией и ReLU-слоями применяются плотные полносвязные слои для выво-да значений value_head и policy_head : Рисунок 10.2. Последние слои глубокой остаточной сети В последней части этого раздела вы увидите несколько фрагментов про- граммного кода на языке Python, в которых используется API библиотеки Keras, имеющий хорошую поддержку остаточных сетей ResNet, для построения такой сети. Давайте теперь посмотрим, насколько хороши эти модели, сначала дав им возможность поиграть друг против друга и после этого поиграть с нами.\n--- Страница 293 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  291 тестирование моДели Для того чтобы дать, к примеру, 4-й версии модели поиграть с версией 1, создайте новый каталожный путь, выполнив команду mkdir ‑p run_archive/ connect4/run0001/models , и скопируйте файлы *.h5 из папки run/models в каталог run0001/models . Затем внесите изменение в ваш сценарий play.py в каталоге DeepReinforcementLearning , как показано ниже: playMatchesBetweenVersions(env, 1, 1, 4, 10, lg.logger_tourney, 0) Первое значение среди параметров 1,1,4,10 означает запускаемую версию прогона ( run), поэтому 1 означает, что модели находятся в подпапке run0001/ models папки run_archive/connect4 . Второе и третье значения являются модель- ными версиями двух игроков, поэтому 1 и 4 означают, что версия 1 модели будет играть против версии 4. И последний параметр 10 – это количество раз или эпизодов игры. После выполнения команды со сценарием python play.py , который сыграет в игру, как указано выше, вы можете применить следующую ниже команду, чтобы узнать результат: grep WINS run/logs/logger_tourney.log |tail --10 В случае игры, где версия 4 играет против версии 1, вы увидите результаты, аналогичные следующим ниже, то есть они будут примерно на одинаковом уровне: 2018-03-14 23:55:21,001 INFO player2 WINS! 2018-03-14 23:55:58,828 INFO player1 WINS! 2018-03-14 23:56:43,778 INFO player2 WINS! 2018-03-14 23:56:51,981 INFO player1 WINS! 2018-03-14 23:57:00,985 INFO player1 WINS! 2018-03-14 23:57:30,389 INFO player2 WINS! 2018-03-14 23:57:39,742 INFO player1 WINS! 2018-03-14 23:58:19,498 INFO player2 WINS! 2018-03-14 23:58:27,554 INFO player1 WINS! 2018-03-14 23:58:36,490 INFO player1 WINS! В файле конфигурации config.py параметр MCTS_SIMS = 50 , то есть количество симуляций MCTS, оказывает сильное влияние на время игры. В каждом состоя-нии древовидный поиск MCTS делает MCTS_SIMS симуляций и вместе с натрени- рованной сетью выдает самый лучший ход. Поэтому если назначить параметру MCTS_SIMS значение 50, то сценарий play.py будет выполняться дольше, но это вовсе не означает, что игрок станет сильнее, в случае если натренированная модель недостаточно хороша. При игре с конкретной версией модели вы мо-жете назначить этому параметру другие значения и посмотреть, как это влияет на ее уровень прочности. Для того чтобы сыграть против какой-то конкретной версии вручную, внесите изменение в сценарий play.py :\n--- Страница 294 ---\n292  Создание мобильного игрового AlphaZero-подобного приложения playMatchesBetweenVersions(env, 1, 4, ‑‑1, 10, lg.logger_tourney, 0) Здесь –1 означает игрока-человека. Таким образом, приведенная выше стро- ка попросит вас (игрок 2) сыграть против игрока 1, то есть версии 4 модели. После того как вы теперь выполните сценарий python play.py , вы увидите при- глашение «Enter your chosen action:» («Введите свое выбранное действие:»); откройте еще один терминал, перейдите в каталог DeepReinforcementLearning , затем наберите команду tail ‑‑f run/logs/logger_tourney.log , и вы увидите игровую решетку, которая будет напечатана следующим образом: 2018-03-15 00:03:43,907 INFO ==================== 2018-03-15 00:03:43,907 INFO EPISODE1 OF 10 2018-03-15 00:03:43,907 INFO ==================== 2018-03-15 00:03:43,908 INFO player2 plays as X 2018-03-15 00:03:43,908 INFO -------------- 2018-03-15 00:03:43,908 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:03:43,908 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:03:43,908 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:03:43,909 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:03:43,909 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:03:43,909 INFO ['-', '-', '-', '-', '-', '-', '-'] Обратите внимание, что последние 6 строк представляют собой игровую решетку, состоящую из 6 строк на 7 столбцов: первая строка соответствует 7 цифрам действий: 0, 1, 2, 3, 4, 5, 6, вторая строка – цифрам действий 7, 8, 9, 10, 11, 12, 13, и т. д. – поэтому последняя строка соответствует цифрам действий 35, 36, 37, 38, 39, 40, 41. Теперь введите номер 38 в первом терминале, выполняющим сценарий play.py , и игрок 1 версии 4 модели, играющий как O, сделает свой ход, показав новые игровые решетки следующим образом: 2018-03-15 00:06:13,360 INFO action: 38 2018-03-15 00:06:13,364 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:06:13,365 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:06:13,365 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:06:13,365 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:06:13,365 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:06:13,365 INFO ['-', '-', '-', 'X', '-', '-', '-'] 2018-03-15 00:06:13,366 INFO -------------- 2018-03-15 00:06:15,155 INFO action: 31 2018-03-15 00:06:15,155 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:06:15,156 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:06:15,156 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:06:15,156 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:06:15,156 INFO ['-', '-', '-', 'O', '-', '-', '-'] 2018-03-15 00:06:15,156 INFO ['-', '-', '-', 'X', '-', '-', '-'] Продолжайте вводить новое действие, после того как игрок 1 сделает свой ход, до самого конца игры и, возможно, начала новой игры:\n--- Страница 295 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  293 2018-03-15 00:16:03,205 INFO action: 23 2018-03-15 00:16:03,206 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:16:03,206 INFO ['-', '-', '-', 'O', '-', '-', '-'] 2018-03-15 00:16:03,206 INFO ['-', '-', '-', 'O', 'O', 'O', '-'] 2018-03-15 00:16:03,207 INFO ['-', '-', 'O', 'X', 'X', 'X', '-'] 2018-03-15 00:16:03,207 INFO ['-', '-', 'X', 'O', 'X', 'O', '-'] 2018-03-15 00:16:03,207 INFO ['-', '-', 'O', 'X', 'X', 'X', '-'] 2018-03-15 00:16:03,207 INFO -------------- 2018-03-15 00:16:14,175 INFO action: 16 2018-03-15 00:16:14,178 INFO ['-', '-', '-', '-', '-', '-', '-'] 2018-03-15 00:16:14,179 INFO ['-', '-', '-', 'O', '-', '-', '-'] 2018-03-15 00:16:14,179 INFO ['-', '-', 'X', 'O', 'O', 'O', '-'] 2018-03-15 00:16:14,179 INFO ['-', '-', 'O', 'X', 'X', 'X', '-'] 2018-03-15 00:16:14,179 INFO ['-', '-', 'X', 'O', 'X', 'O', '-'] 2018-03-15 00:16:14,180 INFO ['-', '-', 'O', 'X', 'X', 'X', '-'] 2018-03-15 00:16:14,180 INFO -------------- 2018-03-15 00:16:14,180 INFO player2 WINS! 2018-03-15 00:16:14,180 INFO ==================== 2018-03-15 00:16:14,180 INFO EPISODE2 OF 5 Вот так выполняется ручное тестирование прочности конкретной версии модели. Понимание внешнего вида условного игрового поля также поможет вам позже разобраться в программном коде для iOS и Android. Если вы по-беждаете модель слишком легко, то можете сделать несколько вещей, для того чтобы попытаться улучшить модель: выпо лнять модель в блокноте Python run.ipynb (второй блок кода) в те- чение нескольких дней. В наших тестах после работы в течение пример-но одного дня на старом iMac версия 19 модели побеждает версию 1 и 4 со счетом 10:0 (напомним, что версии 1 и 4 находятся примерно на оди-наковом уровне); улучшить прочность формулы оценки, выполняемой на основе древо- видного поиска MCTS: для того чтобы выбирать следующий ход, про-цедура древовидного поиска MCTS во время симуляции использует оценку верхней границы доверия в дереве (Upper Confidence Tree, UCT), и формула в репозитории выглядит следующим образом (за подробной информацией обратитесь к блог-посту по адресу http://tim.hibal.org/blog/ alpha-zero-how-and-why-it-works и к официальной работе, посвященной AlphaZero): edge.stats['P'] * np.sqrt(Nb) / (1 + edge.stats['N']) Если мы сделаем эту формулу больше похожей на ту, которая применя-ется в DeepMind: edge.stats['P'] * np.sqrt(np.log(1+Nb) / (1 + edge.stats['N'])) то версия 19 будет выигрывать версию 1 с разгромным счетом 10:0 даже с параметром MCTS_SIMS, равным всего лишь 10;\n--- Страница 296 ---\n294  Создание мобильного игрового AlphaZero-подобного приложения т онко настроить модель глубокой нейронной сети для максимально точ- ной репликации алгоритма AlphaZero. Подробный разбор модели выходит за рамки настоящей книги, но давайте все-таки взглянем на то, как модель строится в библиотеке Keras, для того что- бы больше оценить ее достоинства, когда мы позже будем запускать ее в iOS и Android (вы можете взглянуть на остальную часть основного программного кода в сценариях agent.py , MCTS.ру и game.py и получить более четкое понима- ние того, как разворачивается игра). анализ программного коДа построения моДели В сценарии model.py импорт из библиотеки Keras выглядит следующим об- разом: from keras.models import Sequential, load_model, Modelfrom keras.layers import Input, Dense, Conv2D, Flatten, BatchNormalization, Activation, LeakyReLU, addfrom keras.optimizers import SGDfrom keras import regularizers Четыре ключевых метода построения модели имеют следующий вид: def residual_layer(self, input_block, filters, kernel_size)def conv_layer(self, x, filters, kernel_size)def value_head(self, x)def policy_head(self, x) Все они имеют один или более сверточных слоев Conv2d с последующей па- кетной нормализацией BatchNormalization и активацией LeakyReLU , как показа- но на рис. 10.1, правда, value_head и policy_head также имеют полносвязные слои, как показано на рис. 10.2, после сверточных слоев для генерирования предсказываемого значения и вероятностей линии поведения для входного состояния, о котором мы говорили ранее. В методе _build_model определены вход и выход модели: main_input = Input(shape = self.input_dim, name = 'main_input') vh = self.value_head(x) ph = self.policy_head(x) model = Model(inputs=[main_input], outputs=[vh, ph]) Глубокая нейронная сеть, а также потеря модели и оптимизатор также опре- делены в методе _build_model : if len(self.hidden_layers) > 1: for h in self.hidden_layers[1:]: x = self.residual_layer(x, h['filters'], h['kernel_size'])\n--- Страница 297 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  295 model.compile(loss={'value_head': 'mean_squared_error', 'policy_head': softmax_cross_ entropy_with_logits}, optimizer=SGD(lr=self.learning_rate, momentum = config.MOMENTUM), loss_weights={'value_head': 0.5, 'policy_head': 0.5}) Мы можем добавить инструкции print(vh) и print(ph) в сценарий model. py и узнать точные имена выходных узлов (имя входного узла указано как 'main_input' ), и тогда при выполнении команды python play.py будут выведены следующие две строки: Tensor(\"value_head/Tanh:0\", shape=(?, 1), dtype=float32)Tensor(\"policy_head/MatMul:0\", shape=(?, 42), dtype=float32) Они понадобятся нам при заморозке файлов контрольных точек TensorFlow и загрузке модели в мобильные приложения. заморозка моДели Сначала нам нужно создать файлы контрольных точек TensorFlow. Для этого просто раскомментируйте две строки для player1 и player2 в сценарии funcs.py и снова выполните команду python play.py : if player1version > 0: player1_network = player1_NN.read(env.name, run_version, player1version) player1_NN.model.set_weights(player1_network.get_weights()) # saver = tf.train.Saver() # saver.save(K.get_session(), '/tmp/alphazero19.ckpt') if player2version > 0: player2_network = player2_NN.read(env.name, run_version, player2version) player2_NN.model.set_weights(player2_network.get_weights()) # saver = tf.train.Saver() # saver.save(K.get_session(), '/tmp/alphazero_4.ckpt') Выглядит знакомо, поскольку мы делали что-то подобное в главе 8 «Пред- сказание биржевой цены с помощью RNN-сети». Убедитесь, что номер вер- сии совпадает, к примеру 19 или 4, в контрольных точках alphazero19.ckpt и alphazero_4.ckpt с тем, который определен в сценарии play.py , к примеру playMatchesBetweenVersions(env, 1, 19, 4, 10, lg.logger_tourney, 0) , а также с тем, который находится в каталоге run_archive/connect4/run0001/models , – в этом слу - чае обе версии, version0019.h5 и version0004.h5 , должны находиться там. После выполнения сценария play.py в каталоге /tmp будут сгенерированы файлы контрольных точек alphazero19 : -rw-r--r-- 1 jeffmbair wheel 99 Mar 13 18:17 checkpoint -rw-r--r-- 1 jeffmbair wheel 1345545 Mar 13 18:17 alphazero19.ckpt.meta -rw-r--r-- 1 jeffmbair wheel 7296096 Mar 13 18:17 alphazero19.ckpt.data -00000-of -00001 -rw-r--r-- 1 jeffmbair wheel 8362 Mar 13 18:17 alphazero19.ckpt.index\n--- Страница 298 ---\n296  Создание мобильного игрового AlphaZero-подобного приложения Теперь можно перейти в корневой каталог с исходным кодом TensorFlow и запустить сценарий freeze_graph : python tensorflow/python/tools/freeze_graph.py \\ --input_meta_graph=/tmp/alphazero19.ckpt.meta \\--input_checkpoint=/tmp/alphazero19.ckpt \\--output_graph=/tmp/alphazero19.pb \\--output_node_names=\"value_head/Tanh, policy_head/MatMul\" \\--input_binary=true Для простоты и по той причине, что эта модель имеет небольшой размер, мы не будем выполнять трансформацию графа и его конвертацию в формат memmapped, как мы делали в главе 6 «Описание изображений на естественном языке» и в главе 9 «Генерирование и улучшение изображений с помощью GAN- сети». Теперь мы готовы к применению модели на мобильном устройстве и написанию программного кода для игры «Четыре в ряд» на устройствах iOS и Android. использование моДели игры «ч етыре в ряД» в ios При наличии только что замороженной и, возможно, трансформированной и конвертированной в формат memmapped модели вы всегда можете ее протес - тировать с помощью модуля TensorFlow и проверить свое везение на предмет ее применения простым способом. В нашем случае при использовании модуля TensorFlow модель alphazero19.pb , которую мы сгенерировали, вызовет следу - ющую ниже ошибку при ее загрузке: Couldn't load model: Invalid argument: No OpKernel was registered to support Op 'Switch' with these attrs. Registered devices: [CPU], Registered kernels: device='GPU'; T in [DT_FLOAT] device='GPU'; T in [DT_INT32] device='GPU'; T in [DT_BOOL] device='GPU'; T in [DT_STRING] device='CPU'; T in [DT_INT32] device='CPU'; T in [DT_FLOAT] [[Node: batch_normalization_13/cond/Switch = Switch[T=DT_BOOL, _output_shapes=[[], []]](batch_normalization_1/keras_learning_phase, batch_normalization_1/keras_learning_phase)]] К настоящему времени вы должны уже знать, как исправлять подобного рода ошибки, так как эта тема обсуждалась в предыдущих главах. Напомним, что следует просто убедиться, что файл описания ядра для операции Switch включен в файл tensorflow/contrib/makefile/tf_op_files.txt . Вы можете выяс - нить, какой файл описания ядра относится к операции Switch, выполнив ко-манду grep 'REGISTER.*\"Switch\"' tensorflow/core/kernels/*.cc , которая должна показать tensorflow/core/kernels/control_flow_ops.cc . По умолчанию, начиная с TensorFlow версии 1.4, файл control_flow_ops.cc включен в файл tf_op_files.\n--- Страница 299 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  297 txt, поэтому от вас требуется только собрать пользовательскую библиотеку TensorFlow для iOS, выполнив сценарий оболочки tensorflow/contrib/makefile/ build_all_ios.sh . Если вы успешно выполнили приложение для iOS из преды- дущей главы, то эта библиотека уже находится в хорошем состоянии, и вам не нужно или не требуется снова выполнять данную трудоемкую команду. Теперь просто создайте новый проект iOS в среде Xcode с именем AlphaZero и перетащите в данный проект файлы tensorflow_utils.mm и tensorflow_utils.h из проекта iOS предыдущей главы, а также файл модели alphazeroi9.pb , сге - нерированный в предыдущем разделе. Переименуйте файл ViewController.m в ViewController.mm и добавьте несколько констант и переменных. Ваш проект должен выглядеть, как на рис. 10.3. Рисунок 10.3. Приложения AlphaZero для iOS в среде Xcode Нам необходимо использовать всего три компонента пользовательского ин- терфейса: элемент управления UIImageView , который показывает игровое поле и сыг ранные фиг уры; элемент правления UILabel , который показывает результат игры и запра- шивает действия пользователя; элемент правления UIButton для запуска (Play) или повтора (Replay) игры. Как и прежде, мы создаем и размещаем их программно в методе viewDidLoad .\n--- Страница 300 ---\n298  Создание мобильного игрового AlphaZero-подобного приложения Когда выполнено касание кнопки Play или Replay, вам нужно случайным образом принять решение, кто ходит первым, обнулить игровое поле, пред- ставленное в виде целочисленного массива, очистить два вектора, в которых хранятся наши ходы и ходы ИИ, и перерисовать исходную решетку игрового поля: int n = rand()% 2;aiFirst = (n==0);if (aiFirst) aiTurn = true;else aiTurn = false; for (int i=0; i<PIECES_NUM; i++) board[i] = 0; aiMoves.clear(); humanMoves.clear(); _iv.image = [self createBoardImageInRect:_iv.frame]; И затем запустить игру в рабочем потоке: dispatch_async(dispatch_get_global_queue(0, 0), ^{ std:: string result = playGame(withMCTS); dispatch_async(dispatch_get_main_queue(), ^{ NSString *rslt = [NSString stringWithCString: result.c_str() encoding:[NSString defaultCStringEncoding]]; [_lbl setText: rslt]; _iv.image = [self createBoardImageInRect:_iv.frame]; }); }); В методе playGame сначала проверьте, загружена ли наша модель, и загрузите ее, если нет: string playGame(bool withMCTS) { if (!_modelLoaded) { tensorflow:: Status load_status; load_status = LoadModel(MODEL_FILE, MODEL_FILE_TYPE, &tf_session); if (!load_status.ok()) { LOG(FATAL) << \"Couldn't load model: \" << load_status; return \"\"; } _modelLoaded = YES; } Если теперь наша очередь, то возвращайтесь и сообщите об этом. В против- ном случае конвертируйте состояние игрового поля в двоично отформатиро-ванные данные, ожидаемые моделью на входе:\n--- Страница 301 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  299 if (!aiTurn) return \"Tap the column for your move\"; int binary[PIECES_NUM*2]; for (int i=0; i<PIECES_NUM; i++) if (board[i] == 1) binary[i] = 1; else binary[i] = 0; for (int i=0; i<PIECES_NUM; i++) if (board[i] == ‑1) binary[42+i] = 1; else binary[PIECES_NUM+i] = 0; Например, если массив игрового поля состоит из [0 1 1-1 1-1 0 0 1-1 -1-1 1 0 0 1-1 1 -1 1 0 0 -1 -1 -1 1 -1 0 1 1 1 -1 -1 -1 -1 1 1 1 -1 1 1 -1], который пред- ставляет приведенное ниже состояние игрового поля ( \"X\" соответствует 1, 'O' соответствует –1 и '‑' соответствует 0): ['-', 'Х', 'Х', 'О', 'Х', 'О', '-']['-', 'Х', 'О', 'О', 'О', 'Х', '-']['-', 'Х', 'О', 'Х', 'О', 'Х', '-']['-', 'О', 'О', 'О', 'Х', 'О', '-']['Х', 'Х', 'Х', 'О', 'О', 'О', 'О']['Х', 'Х', 'Х', 'О', 'Х', 'Х', 'О'] то двоичный массив, созданный с помощью предыдущего фрагмента кода, будет [0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1], кодируя фигуры обоих игроков на игровом поле. Находясь по-прежнему в методе playGame , вызовите метод getProbs , который запускает замороженную модель с массивом binary на входе и возвращает ве- роятности линии поведения в массиве probs , а затем находит максимальное значение вероятности в линии поведения: float *probs = new float[PIECES_NUM];for (int i=0; i<PIECES_NUM; i++) probs[i] = ‑100.0; if (getProbs(binary, probs)) { int action = ‑1; float max = 0.0; for (int i=0; i<PIECES_NUM; i++) { if (probs[i] > max) { max = probs[i]; action = i; } } Причина, почему мы инициализируем все элементы массива probs значени- ями –100.0, заключается в том, что внутри метода getProbs , который мы вско- ре покажем, массив probs будет изменен только для разрешенных действий\n--- Страница 302 ---\n300  Создание мобильного игрового AlphaZero-подобного приложения на значения (все в диапазоне от –1.0 до 1.0), возвращаемые из линии пове- дения, поэтому значения probs для всех неправомерных действий останутся –100.0, и после функции softmax , которая делает вероятности неправомерных ходов практически нулевыми, мы можем оперировать только вероятностями правомерных ходов. Для того чтобы направлять ход искусственного интеллекта, мы используем только максимальное значение вероятности, не используя древовидного по-иска MCTS, который необходим, если мы хотим, чтобы ИИ был действительно сильным в сложной игре, такой как шахматы или го. Если линия поведения, возвращаемая из натренированной модели, идеальна, как мы упоминали ра-нее, то древовидный поиск нам не нужен. Вместо того чтобы показывать здесь все детали реализации древовидного поиска MCTS, мы для справки оставим его реализацию в репозитории исходного кода книги. Остальная часть программного кода в методе playGame обновляет игровое поле выбранным действием на основе максимальной вероятности из всех пра-вомерных ходов, возвращаемых моделью, вызывает вспомогательный метод printBoard для распечатки игрового поля в вашей панели вывода в среде Xcode для облегчения отладки, добавляет действие в вектор aiMoves для правильной перерисовки игрового поля и возвращает правильную информацию о статусе, если игра заканчивается. Назначение объекту aiTurn значения false приводит к тому, что обработчик событий касания, который вы вскоре увидите, примет жест касания человека за перемещение, которое человек намеревается сде-лать; если aiTurn равен true , то обработчик касаний будет игнорировать все жесты: board[action] = AI_PIECE;printBoard(board);aiMoves.push_back(action); delete []probs;if (aiWon(board)) return \"AI Won!\"; else if (aiLost(board)) return \"You Won!\";else if (aiDraw(board)) return \"Draw\";} else { delete []probs; }aiTurn = false;return \"Tap the column for your move\";} Вспомогательный метод printBoard выглядит следующим образом: void printBoard(int bd[]) { for (int i = 0; i<6; i++) { for (int j=0; j<7; j++) { cout << PIECE_SYMBOL[bd[i*7+j]] << \" \";\n--- Страница 303 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  301 } cout << endl; } cout << endl << endl; } И в панели вывода среды разработки Xcode он напечатает что-то вроде это- го: ‑ ‑ ‑ ‑ ‑ ‑ ‑ ‑ ‑ ‑ ‑ ‑ ‑ ‑ ‑ ‑ O ‑ ‑ ‑ ‑ X ‑ O ‑ ‑ ‑ O O O O X X ‑ X X X O O X ‑ X В ключевом методе getProbs сначала определите имена входных и выход- ных узлов, а затем подготовьте входной тензор, используя значения в массиве binary : bool getProbs(int *binary, float *probs) { std:: string input_name = \"main_input\"; std:: string output_name1 = \"value_head/Tanh\"; std:: string output_name2 = \"policy_head/MatMul\"; tensorflow:: Tensor input_tensor(tensorflow:: DT_FLOAT, tensorflow:: TensorShape({1,2,6,7})); auto input_mapped = input_tensor.tensor<float, 4>(); for (int i = 0; i < 2; i++) { for (int j = 0; j<6; j++) { for (int k=0; k<7; k++) { input_mapped(0, i, j, k) = binary[i*42+j*7+k]; } } } Теперь запустите модель с входными данными и получите выходные дан- ные: std:: vector<tensorflow:: Tensor> outputs;tensorflow:: Status run_status = tf_session ‑>Run({{input_name, input_tensor}}, {output_ name1, output_name2}, {}, &outputs);if (!run_status.ok()) { LOG(ERROR) << \"Getting model failed:\" << run_status; return false; }tensorflow:: Tensor* value_tensor = &outputs[0];tensorflow:: Tensor* policy_tensor = &outputs[1];\n--- Страница 304 ---\n302  Создание мобильного игрового AlphaZero-подобного приложения const Eigen:: TensorMap<Eigen:: Tensor<float, 1, Eigen:: RowMajor>, Eigen:: Aligned>& value = value_tensor ‑>flat<float>(); const Eigen:: TensorMap<Eigen:: Tensor<float, 1, Eigen:: RowMajor>, Eigen:: Aligned>& policy = policy_tensor ‑>flat<float>(); Установите значения вероятности только для разрешенных действий, а за- тем вызовите функцию softmax , чтобы получить сумму значений probs для разрешенных действий, равную 1: vector<int> actions;getAllowedActions(board, actions);for (int action: actions) { probs[action] = policy(action); } softmax(probs, PIECES_NUM); return true;} Функция getAllowedActions определена для получения разрешенных дей- ствий следующим образом: void getAllowedActions(int bd[], vector<int> &actions) { for (int i=0; i<PIECES_NUM; i++) { if (i>=PIECES_NUM‑7) { if (bd[i] == 0) actions.push_back(i); } else { if (bd[i] == 0 && bd[i+7]!= 0) actions.push_back(i); } } } И следующая ниже функция softmax довольно прямолинейна: void softmax(float vals[], int count) { float max = ‑FLT_MAX; for (int i=0; i<count; i++) { max = fmax(max, vals[i]); } float sum = 0.0; for (int i=0; i<count; i++) { vals[i] = exp(vals[i] ‑‑max); sum += vals[i]; } for (int i=0; i<count; i++) { vals[i] /= sum;\n--- Страница 305 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  303 } } Несколько других вспомогательных функций определено для проверки ста- туса окончания игры: bool aiWon(int bd[]) { for (int i=0; i<69; i++) { int sum = 0; for (int j=0; j<4; j++) sum += bd[winners[i][j]]; if (sum == 4*AI_PIECE) return true; } return false; } bool aiLost(int bd[]) { for (int i=0; i<69; i++) { int sum = 0; for (int j=0; j<4; j++) sum += bd[winners[i][j]]; if (sum == 4*HUMAN_PIECE) return true; } return false; }bool aiDraw(int bd[]) { bool hasZero = false; for (int i=0; i<PIECES_NUM; i++) { if (bd[i] == 0) { hasZero = true; break; } } if (!hasZero) return true; return false; }bool gameEnded(int bd[]) { if (aiWon(bd) || aiLost(bd) || aiDraw(bd)) return true; return false; } В обеих функциях, aiWon и aiLost , используется константный массив, в кото- ром определены все 69 возможных выигрышных позиций: int winners[69][4] = { {0,1,2,3}, {1,2,3,4},\n--- Страница 306 ---\n304  Создание мобильного игрового AlphaZero-подобного приложения {2,3,4,5}, {3,4,5,6}, {7,8,9,10}, {8,9,10,11}, {9,10,11,12}, {10,11,12,13}, {3,11,19,27}, {2,10,18,26}, {10,18,26,34}, {1,9,17,25}, {9,17,25,33}, {17,25,33,41}, {0,8,16,24}, {8,16,24,32}, {16,24,32,40}, {7,15,23,31}, {15,23,31,39}, {14,22,30,38}}; В обработчике событий касания сначала убедитесь, что сейчас очередь за че- ловеком. Затем проверьте, находятся ли значения точек касания в пределах области игрового поля, получите столбец, по которому было произведено каса-ние, на основе позиции касания и обновите массив игрового поля board и век - тор humanMoves ходов игрока-человека: ‑ (void) touchesEnded:(NSSet *)touches withEvent:(UIEvent *)event { if (aiTurn) return; UITouch *touch = [touches anyObject]; CGPoint point = [touch locationInView: self.view]; if (point.y < startY || point.y > endY) return; int column = (point.x ‑startX)/BOARD_COLUMN_WIDTH; for (int i=0; i<6; i++) if (board[35+column‑7*i] == 0) { board[35+column‑7*i] = HUMAN_PIECE; humanMoves.push_back(35+column‑7*i); break; }\n--- Страница 307 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  305 Остальная часть обработчика касания перерисовывает элемент управления ImageView , вызывая метод createBoardImageInRect , который использует Bezierpath для отрисовки или перерисовки игрового поля и всех участвующих фигур, про- веряет состояние игры и возвращает результат, если игра заканчивается, либо продолжает играть, если нет: _iv.image = [self createBoardImageInRect:_iv.frame];aiTurn = true;if (gameEnded(board)) { if (aiWon(board)) _lbl.text = @\"AI Won!\"; else if (aiLost(board)) _lbl.text = @\"You Won!\"; else if (aiDraw(board)) _lbl.text = @\"Draw\"; return; }dispatch_async(dispatch_get_global_queue(0, 0), ^{ std:: string result = playGame(withMCTS)); dispatch_async(dispatch_get_main_queue(), ^{ NSString *rslt = [NSString stringWithCString: result.c_str() encoding:[NSString defaultCStringEncoding]]; [_lbl setText: rslt]; _iv.image = [self createBoardImageInRect:_iv.frame]; }); });} Остальная часть программного кода для iOS находится в методе createBoard‑ ImageInRect , который использует методы moveToPoint и addLineToPoint из объекта uiBezierpath для отрисовки игрового поля: ‑ (UIImage *)createBoardImageInRect:(CGRect)rect { int margin_y = 170; UIGraphicsBeginImageContextWithOptions(CGSizeMake(rect.size.width, rect.size.height), NO, 0.0); UIBezierPath *path = [UIBezierPath bezierPath]; startX = (rect.size.width ‑‑7*BOARD_COLUMN_WIDTH)/2.0; startY = rect.origin.y+margin_y+30; endY = rect.origin.y ‑‑margin_y + rect.size.height; for (int i=0; i<8; i++) { CGPoint point = CGPointMake(startX + i * BOARD_COLUMN_WIDTH, startY); [path moveToPoint: point]; point = CGPointMake(startX + i * BOARD_COLUMN_WIDTH, endY); [path addLineToPoint: point]; } CGPoint point = CGPointMake(startX, endY); [path moveToPoint: point];\n--- Страница 308 ---\n306  Создание мобильного игрового AlphaZero-подобного приложения point = CGPointMake(rect.size.width ‑‑startX, endY); [path addLineToPoint: point]; path.lineWidth = BOARD_LINE_WIDTH; [[UIColor blueColor] setStroke]; [path stroke]; Метод bezierPathWithOvalInRect рисует все фигуры, перемещаемые ИИ и че- ловеком, – в зависимости от того, кто делает первый ход, он начинает рисовать фигуры по очереди, но в другом порядке: int columnPieces[] = {0,0,0,0,0,0,0}; if (aiFirst) { for (int i=0; i<aiMoves.size(); i++) { int action = aiMoves[i]; int column = action% 7; CGRect r = CGRectMake(startX + column * BOARD_COLUMN_WIDTH, endY ‑‑BOARD_COL UMN_WIDTH ‑ ‑BOARD_COLUMN_WIDTH * columnPieces[column], BOARD_COLUMN_WIDTH, BOARD_COLUMN_WIDTH); UIBezierPath *path = [UIBezierPath bezierPathWithOvalInRect: r]; UIColor *color = [UIColor redColor]; [color setFill]; [path fill]; columnPieces[column]++; if (i<humanMoves.size()) { int action = humanMoves[i]; int column = action% 7; CGRect r = CGRectMake(startX + column * BOARD_COLUMN_WIDTH, endY ‑‑BOARD_COLUMN_ WIDTH‑‑BOARD_COLUMN_WIDTH * columnPieces[column], BOARD_COLUMN_WIDTH, BOARD_COLUMN_ WIDTH); UIBezierPath *path = [UIBezierPath bezierPathWithOvalInRect: r]; UIColor *color = [UIColor yellowColor]; [color setFill]; [path fill]; columnPieces[column]++; } } }else { for (int i=0; i<humanMoves.size(); i++) { int action = humanMoves[i]; int column = action% 7; CGRect r = CGRectMake(startX + column * BOARD_COLUMN_WIDTH, endY ‑‑BOARD_COLUMN_WIDTH ‑‑ BOARD_COLUMN_WIDTH * columnPieces[column], BOARD_COLUMN_WIDTH, BOARD_COLUMN_WIDTH); UIBezierPath *path = [UIBezierPath bezierPathWithOvalInRect: r]; UIColor *color = [UIColor yellowColor]; [color setFill];\n--- Страница 309 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  307 [path fill]; columnPieces[column]++; if (i<aiMoves.size()) { int action = aiMoves[i]; int column = action% 7; CGRect r = CGRectMake(startX + column * BOARD_COLUMN_WIDTH, endY ‑‑BOARD_COLUMN_ WIDTH‑‑BOARD_COLUMN_WIDTH * columnPieces[column], BOARD_COLUMN_WIDTH, BOARD_COLUMN_ WIDTH); UIBezierPath *path = [UIBezierPath bezierPathWithOvalInRect: r]; UIColor *color = [UIColor redColor]; [color setFill]; [path fill]; columnPieces[column]++; } } } UIImage *image = UIGraphicsGetImageFromCurrentImageContext();UIGraphicsEndImageContext();return image;} Теперь запустите приложение, и вы увидите экраны, как на рис. 10.4.\n--- Страница 310 ---\n308  Создание мобильного игрового AlphaZero-подобного приложения Рисунок 10.4. Игра «Четыре в ряд» в iOS Сыграйте несколько партий с ИИ. На рис. 10.5 показаны возможные вариан- ты конца игры.\n--- Страница 311 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  309 Рисунок 10.5. Несколько результатов игры «Четыре в ряд» в iOS Прежде чем мы остановимся, давайте быстро взглянем на программный код для Android, который использует данную модель и играет в игру. использование моДели игры «ч етыре в ряД» в android Нет ничего удивительного в том, что нам не нужно применять пользователь- скую библиотеку в Android, как мы делали в главе 7 «Распознавание рисунков с помощью CNN- и LSTM-сетей», для загрузки модели. Просто создайте новое приложение в Android Studio с именем AlphaZero , скопируйте файл модели alphazero19.pb в недавно созданную папку ресурсов assets и добавьте строку compile 'org.tensorflow: tensorflow ‑android:+' в файл build.gradle приложения, как мы делали раньше. Сначала мы создадим новый класс BoardView , который расширяет класс View и отвечает за отрисовку игрового поля, а также фигур ИИ и пользователя: public class BoardView extends View { private Path mPathBoard, mPathAIPieces, mPathHumanPieces; private Paint mPaint, mCanvasPaint; private Canvas mCanvas;\n--- Страница 312 ---\n310  Создание мобильного игрового AlphaZero-подобного приложения private Bitmap mBitmap; private MainActivity mActivity; private static final float MARGINX = 20.0f; private static final float MARGINY = 210.0f; private float endY; private float columnWidth; public BoardView(Context context, AttributeSet attrs) { super(context, attrs); mActivity = (MainActivity) context; setPathPaint(); } Мы использовали три экземпляра объекта Path – mPathBoard , mPathAIPieces и mPathHumanPieces , – д ля того чтобы нарисовать игровое поле и ходы, которые делают соответственно ИИ и человек, с различными цветами. Функционал ри- сования в объекте BoardView реализован в методе onDraw с использованием методов moveTo и lineTo объекта Path и метода drawPath объекта Canvas : protected void onDraw(Canvas canvas) { canvas.drawBitmap(mBitmap, 0, 0, mCanvasPaint); columnWidth = (canvas.getWidth() ‑‑2*MARGINX) / 7.0f; for (int i=0; i<8; i++) { float x = MARGINX + i * columnWidth; mPathBoard.moveTo(x, MARGINY); mPathBoard.lineTo(x, canvas.getHeight() ‑MARGINY); } mPathBoard.moveTo(MARGINX, canvas.getHeight() ‑MARGINY); mPathBoard.lineTo(MARGINX + 7*columnWidth, canvas.getHeight() ‑MARGINY); mPaint.setColor(0xFF0000FF); canvas.drawPath(mPathBoard, mPaint); Если ИИ ходит первым, то мы начинаем отрисовку первого хода ИИ, затем первого хода человека, если таковые имеются, и чередуем отрисовку ходов ИИ и человека: endY = canvas.getHeight() ‑MARGINY; int columnPieces[] = {0,0,0,0,0,0,0};for (int i=0; i<mActivity.getAIMoves().size(); i++) { int action = mActivity.getAIMoves().get(i); int column = action% 7; float x = MARGINX + column * columnWidth + columnWidth / 2.0f; float y = canvas.getHeight() ‑MARGINY‑ columnWidth*columnPieces[column] ‑columnWidth/2.0f; mPathAIPieces.addCircle(x, y, columnWidth/2,\n--- Страница 313 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  311 Path.Direction.CW); mPaint.setColor(0xFFFF0000); canvas.drawPath(mPathAIPieces, mPaint); columnPieces[column]++; if (i<mActivity.getHumanMoves().size()) { action = mActivity.getHumanMoves().get(i); column = action% 7; x = MARGINX + column * columnWidth + columnWidth / 2.0f; y = canvas.getHeight() ‑MARGINY‑ columnWidth*columnPieces[column] ‑columnWidth/2.0f; mPathHumanPieces.addCircle(x, y, columnWidth/2, Path.Direction.CW); mPaint.setColor(0xFFFFFF00); canvas.drawPath(mPathHumanPieces, mPaint); columnPieces[column]++; } } Если человек ходит первым, то применяется аналогичный код отрисовки, как в коде для iOS. Внутри метода public Boolean onTouchEvent(MotionEvent event) объекта BoardView , который возвращает истину, если сейчас очередь ИИ, мы проверяем, в каком столбце было касание, и если столбец не заполнен шестью фигурами, то добавляем новый ход человека в вектор ходов humanMoves класса MainActivity и перерисовываем окно: public boolean onTouchEvent(MotionEvent event) { if (mActivity.getAITurn()) return true; float x = event.getX(); float y = event.getY(); switch (event.getAction()) { case MotionEvent.ACTION_DOWN: break; case MotionEvent.ACTION_MOVE: break; case MotionEvent.ACTION_UP: if (y < MARGINY || y > endY) return true; int column = (int)((x ‑MARGINX)/columnWidth); for (int i=0; i<6; i++) if (mActivity.getBoard()[35+column‑7*i] == 0) { mActivity.getBoard()[35+column‑7*i] = MainActivity.HUMAN_PIECE; mActivity.getHumanMoves().add(35+column‑7*i); break; }\n--- Страница 314 ---\n312  Создание мобильного игрового AlphaZero-подобного приложения invalidate(); После этого мы назначаем очередь ходить искусственному интеллекту и возвращаемся, если игра заканчивается; в противном случае запускаем но- вый поток, чтобы продолжить игру, предоставляя ИИ возможность сделать следующий ход на основе линии поведения модели, после чего человек может коснуться и выбрать свой следующий ход: mActivity.setAiTurn(); if (mActivity.gameEnded(mActivity.getBoard())) { if (mActivity.aiWon(mActivity.getBoard())) mActivity.getTextView().setText(\"AI Won!\"); else if (mActivity.aiLost(mActivity.getBoard())) mActivity.getTextView().setText(\"You Won!\"); else if (mActivity.aiDraw(mActivity.getBoard())) mActivity.getTextView().setText(\"Draw\"); return true; } Thread thread = new Thread(mActivity); thread.start(); break; default: return false; } return true; } Основной макет пользовательского интерфейса, определенный в файле activity_main.xml , состоит из трех элементов пользовательского интерфейса, элемента управления TextView , пользовательского объекта – игровог о поля BoardView и элемента управления Button : <TextView android: id=\"@+id/textview\" android: layout_width=\"wrap_content\" android: layout_height=\"wrap_content\" android: text=\"\" android: textAlignment=\"center\" android: textColor=\"@color/colorPrimary\" android: textSize=\"24sp\" android: textStyle=\"bold\" app: layout_constraintBottom_toBottomOf=\"parent\" app: layout_constraintHorizontal_bias=\"0.5\" app: layout_constraintLeft_toLeftOf=\"parent\" app: layout_constraintRight_toRightOf=\"parent\" app: layout_constraintTop_toTopOf=\"parent\" app: layout_constraintVertical_bias=\"0.06\"/> <com.ailabby.alphazero.BoardView\n--- Страница 315 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  313 android: id=\"@+id/boardview\" android: layout_width=\"fill_parent\" android: layout_height=\"fill_parent\" app: layout_constraintBottom_toBottomOf=\"parent\" app: layout_constraintLeft_toLeftOf=\"parent\" app: layout_constraintRight_toRightOf=\"parent\" app: layout_constraintTop_toTopOf=\"parent\"/> <Button android: id=\"@+id/button\" android: layout_width=\"wrap_content\" android: layout_height=\"wrap_content\" android: text=\"Play\" app: layout_constraintBottom_toBottomOf=\"parent\" app: layout_constraintHorizontal_bias=\"0.5\" app: layout_constraintLeft_toLeftOf=\"parent\" app: layout_constraintRight_toRightOf=\"parent\" app: layout_constraintTop_toTopOf=\"parent\" app: layout_constraintVertical_bias=\"0.94\" /> В файле MainActivity.java сначала определите несколько констант и полей: public class MainActivity extends AppCompatActivity implements Runnable { private static final String MODEL_FILE = \"file:///android_asset/alphazero19.pb\"; private static final String INPUT_NODE = \"main_input\"; private static final String OUTPUT_NODE1 = \"value_head/Tanh\"; private static final String OUTPUT_NODE2 = \"policy_head/MatMul\"; private Button mButton; private BoardView mBoardView; private TextView mTextView; public static final int AI_PIECE = ‑1; public static final int HUMAN_PIECE = 1; private static final int PIECES_NUM = 42; private Boolean aiFirst = false; private Boolean aiTurn = false; private Vector<Integer> aiMoves = new Vector<>(); private Vector<Integer> humanMoves = new Vector<>(); private int board[] = new int[PIECES_NUM]; private static final HashMap<Integer, String> PIECE_SYMBOL; static { PIECE_SYMBOL = new HashMap<Integer, String>(); PIECE_SYMBOL.put(AI_PIECE, \"X\");\n--- Страница 316 ---\n314  Создание мобильного игрового AlphaZero-подобного приложения PIECE_SYMBOL.put(HUMAN_PIECE, \"O\"); PIECE_SYMBOL.put(0, \" ‑\"); } private TensorFlowInferenceInterface mInferenceInterface; Затем определите все выигрышные позиции, как мы это сделали в версии приложения для iOS: private final int winners[][] = { {0,1,2,3}, {1,2,3,4}, {2,3,4,5}, {3,4,5,6}, {7,8,9,10}, {8,9,10,11}, {9,10,11,12}, {10,11,12,13}, {0,8,16,24}, {8,16,24,32}, {16,24,32,40}, {7,15,23,31}, {15,23,31,39}, {14,22,30,38}}; Несколько методов чтения и методов записи для класса BoardView : public boolean getAITurn() { return aiTurn; } public boolean getAIFirst() { return aiFirst; }public Vector<Integer> getAIMoves() { return aiMoves; }public Vector<Integer> getHumanMoves() { return humanMoves; }public int[] getBoard() { return board; }public void setAiTurn() { aiTurn = true; } И несколько вспомогательных функций для проверки состояний игры, кото- рые были перенесены непосредственно из программного кода для iOS:\n--- Страница 317 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  315 public boolean aiWon(int bd[]) { for (int i=0; i<69; i++) { int sum = 0; for (int j=0; j<4; j++) sum += bd[winners[i][j]]; if (sum == 4*AI_PIECE) return true; } return false; } public boolean aiLost(int bd[]) { for (int i=0; i<69; i++) { int sum = 0; for (int j=0; j<4; j++) sum += bd[winners[i][j]]; if (sum == 4*HUMAN_PIECE) return true; } return false; }public boolean aiDraw(int bd[]) { boolean hasZero = false; for (int i=0; i<PIECES_NUM; i++) { if (bd[i] == 0) { hasZero = true; break; } } if (!hasZero) return true; return false; }public boolean gameEnded(int[] bd) { if (aiWon(bd) || aiLost(bd) || aiDraw(bd)) return true; return false; } Метод getAllowedActions , также непосредственно перенесенный из про- граммного кода для iOS, назначает вектору actions все разрешенные действия для заданной позиции игрового поля: void getAllowedActions(int bd[], Vector<Integer> actions) { for (int i=0; i<PIECES_NUM; i++) { if (i>=PIECES_NUM‑7) { if (bd[i] == 0) actions.add(i); } else { if (bd[i] == 0 && bd[i+7]!= 0)\n--- Страница 318 ---\n316  Создание мобильного игрового AlphaZero-подобного приложения actions.add(i); } } } В методе onCreate создайте экземпляр трех элементов пользовательского интерфейса и задайте слушателя касаний кнопки, который случайным обра- зом будет решать, кто ходит первым. Кнопка также нажимается, когда поль-зователь хочет сыграть новую партию, поэтому нам нужно обнулить векторы aiMoves и humanMoves , перед тем как выполнить отрисовку игрового поля и за- пустить новый поток для игры: protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); mButton = findViewById(R.id.button); mTextView = findViewById(R.id.textview); mBoardView = findViewById(R.id.boardview); mButton.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { mButton.setText(\"Replay\"); mTextView.setText(\"\"); Random rand = new Random(); int n = rand.nextInt(2); aiFirst = (n==0); if (aiFirst) aiTurn = true; else aiTurn = false; if (aiTurn) mTextView.setText(\"Waiting for AI's move\"); else mTextView.setText(\"Tap the column for your move\"); for (int i=0; i<PIECES_NUM; i++) board[i] = 0; aiMoves.clear(); humanMoves.clear(); mBoardView.drawBoard(); Thread thread = new Thread(MainActivity.this); thread.start(); } }); }\n--- Страница 319 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  317 Поток запускает метод run , который далее вызывает метод playGame . Метод playGame сначала конвертирует позицию игрового поля в целочисленный мас - сив binary , который будет использоваться в качестве входных данных модели: public void run() { final String result = playGame(); runOnUiThread( new Runnable() { @Override public void run() { mBoardView.invalidate(); mTextView.setText(result); } }); } String playGame() { if (!aiTurn) return \"Tap the column for your move\"; int binary[] = new int[PIECES_NUM*2]; for (int i=0; i<PIECES_NUM; i++) if (board[i] == 1) binary[i] = 1; else binary[i] = 0; for (int i=0; i<PIECES_NUM; i++) if (board[i] == ‑1) binary[42+i] = 1; else binary[PIECES_NUM+i] = 0; Остальная часть метода playGame тоже почти напрямую перенесена из про- граммного кода для iOS. В нем вызывается метод getProbs , который получает максимальное вероятностное значение из всех разрешенных действий, ис - пользуя вероятностные значения, возвращаемые для всех действий, 42 в об- щей сложности, включая как правомерные, так и неправомерные, в линии по-ведения модели: float probs[] = new float[PIECES_NUM];for (int i=0; i<PIECES_NUM; i++) probs[i] = ‑100.0f; getProbs(binary, probs);int action = ‑1; float max = 0.0f;for (int i=0; i<PIECES_NUM; i++) { if (probs[i] > max) { max = probs[i]; action = i; } } board[action] = AI_PIECE;\n--- Страница 320 ---\n318  Создание мобильного игрового AlphaZero-подобного приложения printBoard(board); aiMoves.add(action); if (aiWon(board)) return \"AI Won!\"; else if (aiLost(board)) return \"You Won!\";else if (aiDraw(board)) return \"Draw\"; aiTurn = false; return \"Tap the column for your move\";} Метод getProbs загружает модель, если она не была загружена, запускает модель с текущим состоянием игрового поля на входе и получает линию по-ведения на выходе перед вызовом функции softmax , которая выдает для раз- решенных действий истинные значения вероятности, равные в сумме 1: void getProbs(int binary[], float probs[]) { if (mInferenceInterface == null) { AssetManager assetManager = getAssets(); mInferenceInterface = new TensorFlowInferenceInterface(assetManager, MODEL_FILE); } float[] floatValues = new float[2*6*7]; for (int i=0; i<2*6*7; i++) { floatValues[i] = binary[i]; } float[] value = new float[1]; float[] policy = new float[42]; mInferenceInterface.feed(INPUT_NODE, floatValues, 1, 2, 6, 7); mInferenceInterface.run(new String[] {OUTPUT_NODE1, OUTPUT_NODE2}, false); mInferenceInterface.fetch(OUTPUT_NODE1, value); mInferenceInterface.fetch(OUTPUT_NODE2, policy); Vector<Integer> actions = new Vector<>(); getAllowedActions(board, actions); for (int action: actions) { probs[action] = policy[action]; } softmax(probs, PIECES_NUM); } Метод softmax определен почти так же, как и в версии для iOS: void softmax(float vals[], int count) {\n--- Страница 321 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  319 float maxval = ‑Float.MAX_VALUE; for (int i=0; i<count; i++) { maxval = max(maxval, vals[i]); } float sum = 0.0f; for (int i=0; i<count; i++) { vals[i] = (float)exp(vals[i] ‑‑maxval); sum += vals[i]; } for (int i=0; i<count; i++) { vals[i] /= sum; } } Теперь запустите приложение в виртуальном или реальном устройстве Android и сыграйте в игру с программным приложением, и вы увидите началь- ный экран и некоторые результаты игры. Рисунок 10.6. Игровое поле и некоторые результаты игры в Android Во время игры в iOS и Android с приведенным выше программным кодом вы вскоре обнаружите, что линия поведения, возвращаемая моделью, не так сильна – основная причина в том, что древовидный поиск MCTS, который здесь не показан в подробностях ввиду ограниченного объема, не использу - ется вместе с глубокой нейросетевой моделью. Настоятельно рекомендуем вам изучить и реализовать MCTS самостоятельно или использовать нашу реализацию в репозитории исходного кода, прилагаемого к данной книге.\n--- Страница 322 ---\n320  Создание мобильного игрового AlphaZero-подобного приложения Вы также должны попробовать применить сетевую модель и MCTS к другим интересующим вас играм – в конце концов, в AlphaZero был использован универсальный алгоритм древовидного поиска MCTS и самообучения с под-креплением на основе самоигры без предметных знаний, чтобы обеспе-чить переносимость сверхчеловеческих результатов самообучения на дру - гие предметные области. Объединив древовидный поиск MCTS с глубокой нейросетевой моделью, вы сможете достичь того, чего достигла программа AlphaZero. резюме В этой главе мы представили удивительный мир AlphaZero, новейшее и самое большое достижение стартапа DeepMind по состоянию на де-кабрь 2017 года. Мы показали вам, как выполнять тренировку AlphaZero-подобной модели для игры «Четыре в ряд», используя мощный API библио-теки Keras с платформой TensorFlow в качестве бэкенда, и как тестировать и, возможно, совершенствовать такую модель. Затем мы выполнили замо-розку данной модели и подробно обсудили вопрос создания приложений для iOS и Android с использованием этой модели для игры «Четыре в ряд» с ИИ, работающим на основе данной модели. Это не точная копия моде-ли AlphaZero, способной побеждать гроссмейстеров по шахматам или го, но мы надеемся, что эта глава предоставила вам прочную основу и мотиви-ровала вас продолжить свою работу по тиражированию того, что программа AlphaZero сделала первой, и дальнейшему ее распространению на другие предметные области. Для этого потребуется много упорного труда, но это будет стоить того. Если вы впечатлены последними достижениями в области ИИ, такими как AlphaZero, скорее всего, вы также обнаружите новейшие программные реше-ния или инструментальные средства TensorFlow для мобильных платформ. Платформа TensorFlow Lite, как мы уже упоминали в главе 1 «Начало работы с платформой TensorFlow Mobile», является альтернативным решением плат - форме TensorFlow Mobile, на которой мы сосредоточились во всех предыдущих главах. По данным компании Google, платформа TensorFlow Lite в будущем со-бирается занять место платформы TensorFlow Mobile, хотя в настоящее время и в обозримом будущем платформа TensorFlow Mobile по-прежнему будет ис - пользоваться для производственных случаев. Хотя платформа TensorFlow Lite работает как в iOS, так и в Android, она также может пользоваться преимуществами API нейронных сетей Android (Android Neural Networks API) для аппаратного ускорения во время рабо-ты на устройствах Android. С другой стороны, разработчики для iOS могут использовать новейшую платформу машинного обучения Core ML компа-нии Apple, ориентированную на iOS11 или выше, которая поддерживает оптимальное выполнение множества мощных предварительно натрени-\n--- Страница 323 ---\nСоздание мобильного игрового AlphaZero-подобного приложения  321 рованных глубоко обучающихся моделей, а также моделей, построенных с использованием классических машинно-обучающихся алгоритмов и биб - лиотеки Keras, на мобильных устройствах с минимизированным размером двоичных файлов приложений. В следующей главе мы расскажем, как ис - пользовать платформы TensorFlow Lite и Core ML в приложениях для iOS и Android.",
      "debug": {
        "start_page": 285,
        "end_page": 323
      }
    },
    {
      "name": "Глава 11. Применение платформ T ensorFlow Lite и Core ML на мобильных устройствах 322",
      "content": "--- Страница 324 --- (продолжение)\nГлава 11 Применение платформ T ensorFlow Lite и Core ML на мобильных устройствах В предыдущих десяти главах мы применяли платформу TensorFlow Mobile для выполнения всевозможных мощных глубоко обучающихся моделей, постро-енных с помощью платформы TensorFlow и библиотеки Keras, на мобильных устройствах. Как мы уже упоминали в главе 1 «Начало работы с платформой TensorFlow Mobile», компания Google также предлагает платформу TensorFlow Lite, альтернативу платформе TensorFlow Mobile, для выполнения моделей на мобильных устройствах. Хотя она все еще находится в предварительной версии на момент выхода версии TensorFlow 1.8 и проведения конференции Google I/O 2018, она позиционируется компанией Google как инструмент, ко- торый «значительно упростит опыт разработчика при нацеливании модели на небольшие устройства». Поэтому стоит взглянуть на платформу TensorFlow Lite более подробно и быть готовыми к будущему. Если вы разработчик для iOS или работаете как с iOS, так и с Android, то еже- годная Всемирная конференция разработчиков Apple (WWDC) – это то собы- тие, которое вы не хотите пропустить. На конференции WWDC2017 компания Apple анонсировала новую платформу Core ML для поддержки работы как глу - боко обучающихся моделей, так и стандартных машинно-обучающихся мо-делей на iOS (и на всех других платформах ОС Appl: macOS, tvOS и watchOS). Платформа Core ML доступна начиная с операционной системы iOS11, которая уже занимает более 80% рыночной доли по состоянию на май 2018 года. Чтобы получить хотя бы базовое понимание того, что вы можете делать с помощью платформы Core ML в своих приложениях для iOS, безусловно, имеет смысл познакомиться с ней поближе. Таким образом, в этой главе мы рассмотрим и TensorFlow Lite, и Core ML, показав сильные и слабые стороны обеих платформ, и затронем следующие темы:\nГлава 11 Применение платформ T ensorFlow Lite и Core ML на мобильных устройствах В предыдущих десяти главах мы применяли платформу TensorFlow Mobile для выполнения всевозможных мощных глубоко обучающихся моделей, постро-енных с помощью платформы TensorFlow и библиотеки Keras, на мобильных устройствах. Как мы уже упоминали в главе 1 «Начало работы с платформой TensorFlow Mobile», компания Google также предлагает платформу TensorFlow Lite, альтернативу платформе TensorFlow Mobile, для выполнения моделей на мобильных устройствах. Хотя она все еще находится в предварительной версии на момент выхода версии TensorFlow 1.8 и проведения конференции Google I/O 2018, она позиционируется компанией Google как инструмент, ко- торый «значительно упростит опыт разработчика при нацеливании модели на небольшие устройства». Поэтому стоит взглянуть на платформу TensorFlow Lite более подробно и быть готовыми к будущему. Если вы разработчик для iOS или работаете как с iOS, так и с Android, то еже- годная Всемирная конференция разработчиков Apple (WWDC) – это то собы- тие, которое вы не хотите пропустить. На конференции WWDC2017 компания Apple анонсировала новую платформу Core ML для поддержки работы как глу - боко обучающихся моделей, так и стандартных машинно-обучающихся мо-делей на iOS (и на всех других платформах ОС Appl: macOS, tvOS и watchOS). Платформа Core ML доступна начиная с операционной системы iOS11, которая уже занимает более 80% рыночной доли по состоянию на май 2018 года. Чтобы получить хотя бы базовое понимание того, что вы можете делать с помощью платформы Core ML в своих приложениях для iOS, безусловно, имеет смысл познакомиться с ней поближе. Таким образом, в этой главе мы рассмотрим и TensorFlow Lite, и Core ML, показав сильные и слабые стороны обеих платформ, и затронем следующие темы:\n--- Страница 325 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  323 плат форма TensorFlow Lite – краткий обзор; испо льзование платформы TensorFlow Lite в iOS; применение плат формы TensorFlow Lite в Android; плат форма CoreML для iOS – краткий обзор; испо льзование платформы CoreML вместе с машинным обучением на основе Scikit-Learn; применение плат формы CoreML вместе с Keras и TensorFlow. платформа Tensor Flow liTe – краткий обзор Платформа TensorFlow Lite (https://www.tensorflow.org/mobile/tflite) – это легкое решение, позволяющее выполнять глубоко обучающиеся модели на мобиль- ных и встраиваемых устройствах. Если модель, построенная в TensorFlow или в Keras, может быть успешно конвертирована в формат TensorFlow Lite, новый модельный формат, основанный на кроссплатформенной библиотеке сериа-лизации FlatBuffers (https: //google.github.io/flatbuffers), похожей на механизм сериализации ProtoBuffers, о котором мы говорили в главе 3 «Обнаружение и локализация объектов», но быстрее и намного меньше по размеру, то можно ожидать, что модель будет выполняться с низким значением задержки и ма-лым размером двоичного файла. Базовый рабочий процесс использования TensorFlow Lite в мобильных приложениях выглядит следующим образом. 1. Построить и натренировать (или вторично натренировать) модель TensorFlow с помощью платформы TensorFlow или библиотеки Keras с TensorFlow в качестве бэкенда, как те, которые мы тренировали в пре-дыдущих главах.  Вы также можете выбрать готовую модель TensorFlow Lite, такую как модели MobileNet, доступные на https: //github.com/tensorflow/models/blob/master/research/slim/nets/ mobilenet_v1.md, которые мы использовали для вторичной тренировки в главе 2 «Клас - сифицирование изображений с помощью трансферного обучения». Каждый архивный модельный файл MobileNet tgz , который вы можете там скачать, содержит модель, конвертированную в формат платформы TensorFlow Lite. Например, архивный файл MobileNet_v1_1.0_224.tgz содержит файл mobilenet_v1_1.0_224.tflite , который можно использовать непосредственно на мобильном устройстве. Если вы используете такую предварительно построенную модель TensorFlow Lite, то шаги 2 и 3 можно пропустить. 2. С обрать инструмент конвертации TensorFlow Lite Converter. Если вы скачаете TensorFlow версии 1.5 или 1.6 с https: //github.com/tensorflow/ tensorflow/releases, то можете выполнить команду bazel build tensorflow/ contrib/lite/toco: toco в терминале из своего корневого каталога с ис - ходным кодом TensorFlow. Если вы используете более поздние версии или получили последний репозиторий TensorFlow, то у вас должно полу - читься это сделать с помощью команды build . Однако если это сделать не получилось, то обратитесь к документации по данному новому ре-лизу.\n--- Страница 326 ---\n324  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах 3. Испо льзовать инструмент конвертации TensorFlow Lite Converter для преобразования модели TensorFlow в модель в формате платформы TensorFlow Lite. Вы увидите подробный пример конвертации в следую-щем разделе. 4. Развернуть модель TensorFlow Lite в iOS или Android – в случае iOS для загрузки и выполнения модели использовать API C++; в случае Android для загрузки и выполнения модели использовать API Java, то есть оберт - ку вокруг API C++. В API C++ и API Java применяется специфичный для TensorFlow Lite класс-интерпретатор Interpreter , в отличие от класса-се- анса Session , который мы использовали в мобильных проектах TensorFlow ранее для выведения заключения на модели. В следующих двух разделах мы покажем вам оба варианта программного кода: код на C++ для iOS и код на Java для Android, в котором используется класс Interpreter .  Ес ли вы запустите модель TensorFlow Lite в Android и если устройство Android имеет версию Android 8.1 (уровень API 27) или выше и поддерживает аппаратное ускорение на основе специального нейросетевого оборудования, GPU или какого-то другого про-цессора цифровых сигналов, то класс Interpreter будет использовать специальный API нейронных сетей Android (Android Neural Networks API) (https://developer.android.com/ndk/guides/neuralnetworks/index.html) для ускорения работы модели. Например, теле-фон Google Pixel 2 имеет собственный чип, оптимизированный для обработки изобра-жений. Данный чип, который можно включить с помощью Android 8.1, поддерживает аппаратное ускорение. Давайте теперь посмотрим, как использовать платформу TensorFlow Lite в iOS. использование платформы Tensor Flow liTe в ios Прежде чем мы покажем, как создавать новое приложение для iOS и добавлять в него поддержку платформы TensorFlow Lite, давайте сначала взглянем на не-сколько примеров приложений TensorFlow для iOS с использованием платфор-мы TensorFlow Lite. выполнение примеров приложений Tensor Flow liTe Для ios Для iOS имеется два примера приложений TensorFlow Lite, simple и camera, по-хожих на приложения TensorFlow Mobile, с теми же именами, но реализован-ных в API TensorFlow Lite в официальных релизах TensorFlow 1.5–1.8 по адре-су https: //github.com/tensorflow/tensorflow/releases и, скорее всего, находящихся в репозитории самой последней версии TensorFlow. Для того чтобы подгото-вить и запустить эти два приложения, вы можете выполнить следующие ниже команды, аналогично описанным в разделе «Пример приложения для iOS»\n--- Страница 327 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  325 по адресу https: //github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ lite: cd tensorflow/contrib/lite/examples/ios ./download_models.shsudo gem install cocoapodscd camerapod installopen tflite_camera_example.xcworkspacecd /simplepod installopen simple.xcworkspace Теперь у вас будет два запускаемых проекта Xcode для iOS, simple и camera (с именами соответственно tflite_simple_example и tflite_camera_example в Xcode), которые вы можете установить и выполнить на устройствах iOS (при-ложение simple также может работать в симуляторе iOS).  Сценарий оболочки download_models.sh скачает zip-файл, содержащий файл модели mobilenet_quant_vi_224.tflite и файл меток labels.txt , после чего скопируйте их в ка- талоги simple/data и camera/data . Обратите внимание на то, что по каким-то причинам этот сценарий не включен в официальные релизы TensorFlow 1.5.0 и 1.6.0, и для того чтобы его получить, вам нужно будет выполнить команду git clone https://github.com/ tensorflow/tensorflow и клонировать последний исходный код (по состоянию на март 2018 года). В среде Xcode вы можете взглянуть на исходный код файла CameraExampleViewController.mm проекта tflite_camera_example и исходный код файла RunModelViewController.mm проекта tflite_simple_example , для того чтобы получить представление о том, как использовать API TensorFlow Lite для за-грузки и запуска моделей TensorFlow Lite. Прежде чем мы продемонстриру - ем вам пошаговые инструкции того, как создавать новое приложение для iOS и добавлять в него поддержку платформы TensorFlow Lite, которая позволит вам запускать готовые модели TensorFlow Lite, мы быстро покажем вам на кон-кретных цифрах одно из преимуществ использования платформы TensorFlow Lite, как мы уже упоминали ранее – размер двоичного файла приложения. Файл модели tensorflow_inception.graph.pb , используемый в примере прило- жения TensorFlow Mobile tf_camera_example и находящийся в папке tensorflow/ examples/ios/camera , имеет размер 95,7 Мб, в то время как файл модели TensorFlow Lite mobilenet_quant_v1_224.tflite , используемый в примере приложения TensorFlow Lite tflite_camera_example и расположенный в папке tensorflow/ contrib/lite/examples/ios/camera , составляет всего 4.3 Мб. Квантованная версия файла вторично натренированной модели Inception 3 для TensorFlow Mobile, как мы видели в приложении HelloTensorFlow в главе 2 «Классифицирование изо- бражений с помощью трансферного обучения», составляет около 22.4 Мб, тогда как файл вторично натренированной модели MobileNet для TensorFlow Mobile\n--- Страница 328 ---\n326  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах имеет размер 17.6 Мб. Подытоживая, ниже перечислены размеры четырех раз- личных типов моделей: модель Inception 3 для TensorFlow Mobile: 95.7 Мб; квант ованная и вторично натренированная модель Inception 3 для TensorFlow Mobile: 22.4 Мб; вторично натренированная модель MobileNet 1.0 224 для TensorFlow Mobile: 17.6 Мб; модель MobileNet 1.0 224 для TensorFlow Lite: 4.3 Мб. Если вы установите и выполните эти два приложения на своем iPhone, то в настройках своего iPhone вы увидите, что размер приложения tflite_ camera_ example составляет около 18,7 Мб, тогда как размер приложения tf_ camera_example будет порядка 44,2 Мб. Это правда, точность модели Inception 3 немного выше, чем модели MobileNet, но во многих случаях небольшую разницу в точности можно про- игнорировать. Кроме того, следует признать, что сегодня мобильные прило-жения могут легко переваривать несколько десятков мегабайт, и в некоторых случаях разница в 20 или 30 Мб в размере приложения не так уж важна. Одна-ко размер будет сказываться чувствительным образом на работе небольших встроенных устройств, и если мы сможем достичь примерно одинаковой точ-ности с более высокой скоростью и меньшим размером, минуя слишком много хлопот, то это всегда будет хорошей новостью для пользователей. использование готовой моДели Tensor Flow liTe в ios Выполните следующие ниже действия, для того чтобы создать новое приложе-ние для iOS и добавить в него поддержку TensorFlow Lite, используя предвари-тельно построенную модель TensorFlow Lite для классификации изображений. 1. В среде Xcode создайте новый проект для iOS с одним экраном под на- званием HelloTFLite , установите Objective-C в качестве языка програм- мирования, затем добавьте в проект файлы ios_image_load.mm и ios_ image_load.h из папки tensorflow/contrib/lite/examples/ios .  Ес ли вы предпочитаете в качестве языка программирования Swift, то после выполнения приведенных здесь шагов вы можете обратиться к главе 2 «Классифицирование изобра-жений с помощью трансферного обучения» или главе 5 «Понимание простых речевых команд», для того чтобы увидеть, как конвертировать приложение на языке Objective-C в приложение на языке Swift. Но имейте в виду, что программный код выведения заклю-чения в TensorFlow Lite все равно должен быть на языке C++, поэтому в конечном счете у вас будет смесь кода на Swift, Objective-C и C++, в котором программный код на Swift в основном будет отвечать за пользовательский интерфейс и пред- и постобработку про-цедуры TensorFlow Lite выведения заключения. 2. Д обавьте в проект файл модели и файл меток, сгенерированные с по- мощью предыдущего сценария оболочки download_models.sh из папки\n--- Страница 329 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  327 tensorflow/contrib/lite/examples/ios/simple/data , а также тестовые изобра- жения, к примеру lab1.jpg из папки с исходным кодом главы 2. 3. Закройт е проект и создайте новый файл с именем Podfile со следующим ниже содержимым: platform: ios, '8.0' target 'HelloTFLite' pod 'TensorFlowLite' Выполните команду pod install . Затем откройте файл HelloTFLite. xcworkspace в среде Xcode, переименуйте файл ViewController.m в ViewController.mm и добавьте необходимые заголовки C++ и заголовки TensorFlow Lite. В результате ваш проект Xcode должен выглядеть следу - ющим образом: Рисунок 11.1. Новый проект Xcode для iOS с использованием модуля TensorFlow Lite  Мы ограничимся показом использования в своих собственных приложениях для iOS только модуля TensorFlow Lite. Однако есть еще один способ добавить поддержку TensorFlow Lite в iOS, который аналогичен выполнению сборки пользовательской биб - лиотеки TensorFlow Mobile для iOS, что мы делали неоднократно в предыдущих главах. Для получения дополнительной информации о том, как собрать свою собственную биб - лиотеку TensorFlow Lite для iOS, обратитесь к документации по адресу https: //github. com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/ios.md.\n--- Страница 330 ---\n328  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах 4. Ск опируйте аналогичный программный код пользовательского интер- фейса из приложения HelloTensorFlow для iOS главы 2 «Классифицирова- ние изображений с помощью трансферного обучения» в файл ViewController. mm, в котором используется распознаватель жестов UITapGestureRecognizer для захвата жестов пользователя на экране, а затем вызовите метод RunInferenceOnImage , который загружает файл модели TensorFlow Lite: NSString* RunInferenceOnImage() { NSString* graph = @\"mobilenet_v1_1.0_224\"; std:: string input_layer_type = \"float\"; std:: vector<int> sizes = {1, 224, 224, 3}; const NSString* graph_path = FilePathForResourceName(graph, @\"tflite\"); std:: unique_ptr<tflite:: FlatBufferModel> model(tflite:: FlatBufferModel:: BuildFromFile([graph_path UTF8String])); if (!model) { NSLog(@\"Failed to mmap model%@.\", graph); exit(‑1); } 5. С оздайте экземпляр класса-интерпретатора Interpreter и задайте его входные данные: tflite:: ops:: builtin:: BuiltinOpResolver resolver;std:: unique_ptr<tflite:: Interpreter> interpreter;tflite:: InterpreterBuilder(*model, resolver)(&interpreter);if (!interpreter) { NSLog(@\"Failed to construct interpreter.\"); exit(‑1); }interpreter ‑>SetNumThreads(1); int input = interpreter ‑>inputs()[0]; interpreter ‑>ResizeInputTensor(input, sizes); if (interpreter ‑>AllocateTensors()!= kTfLiteOk) { NSLog(@\"Failed to allocate tensors.\"); exit(‑1); } В отличие от платформы TensorFlow Mobile, платформа TensorFlow Lite во время передачи на вход модели TensorFlow Lite данных для выведе-ния ею заключения вместо конкретных имен входных узлов использует interpreter ‑>inputs()[0] . 6. После загрузки файла меток labels.txt так же, как мы делали в приложе- нии HelloTensorFlow , предназначенное для классификации изображение загружается подобным образом, но при этом оно подготавливается с ис -\n--- Страница 331 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  329 пользованием метода typed_tensor класса-интерпретатора Interpreter платформы TensorFlow Lite вместо класса Tensor платформы TensorFlow Mobile и его метода. Рисунок 11.2 демонстрирует в сравнении программ- ный код TensorFlow Mobile и TensorFlow Lite для загрузки и обработки данных файла изображения. Рисунок 11.2. Программный код TensorFlow Mobile (слева) и TensorFlow Lite (справа) загрузки и обработка входного изображения 7. Вызовит е метод Invoke , определенный на интерпретаторе Interpreter , для запуска модели и метод typed_out_tensor для получения выходных данных модели, перед тем как вызывать вспомогательный метод GetTopN для получения лучших N результатов классификации. Разница в про-граммном коде между платформами TensorFlow Mobile и TensorFlow Lite для этого случая показана на рис. 11.3. Рисунок 11.3. Программный код TensorFlow Mobile (слева) и TensorFlow Lite (справа) запуска модели и получения выходных данных 8. Р еализуйте метод GetTopN аналогично методу в приложении HelloTensorFlow с использованием типа const float* prediction для TensorFlow Lite, вместо const Eigen:: TensorMap<Eigen:: Tensor<float, 1, Eigen:: RowMajor>, Eigen:: Aligned>& prediction для TensorFlow Mobile. Сравнение метода GetTopN в TensorFlow Mobile и TensorFlow Lite пока- зано на рис. 11.4.\n--- Страница 332 ---\n330  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах Рисунок 11.4. Программный код TensorFlow Mobile (слева) и TensorFlow Lite обработки выходных данных модели для возврата лучших результатов 9. Применит е простой контроллер оповещений UIAlertController для ото- бражения лучших результатов, при этом значения достоверности воз- вращаются моделью TensorFlow Lite, если они больше порога (установ-ленного на уровне 0.1f): ‑(void) showResult:(NSString *)result { UIAlertController* alert = [UIAlertController alertControllerWithTitle:@\"TFLite Model Result\" message: result preferredStyle: UIAlertControllerStyleAlert]; UIAlertAction* action = [UIAlertAction actionWithTitle:@\"OK\" style: UIAlertActionStyleDefault handler: nil]; [alert addAction: action]; [self presentViewController: alert animated: YES completion: nil]; } ‑(void)tapped:(UITapGestureRecognizer *)tapGestureRecognizer { NSString *result = RunInferenceOnImage(); [self showResult: result]; } Теперь запустите приложение для iOS и коснитесь экрана, чтобы запус - тить модель. Для тестового изображения lab1.jpg вы увидите результат модели, показанный на рис. 11.5.\n--- Страница 333 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  331 Рисунок 11.5. Лучшее изображение и результат выведения моделью заключения Вот так используется готовая (предварительно построенная) модель MobileNet для TensorFlow Lite в новом приложении для iOS. Теперь давайте по- смотрим, каким образом мы можем использовать вторично натренированную модель TensorFlow. использование вторично натренированной моДели Tensor Flow Для платформы Tensor Flow liTe в ios В главе 2 «Классифицирование изображений с помощью трансферного обучения» мы выполняли вторичную тренировку модели TensorFlow MobileNet для рас - познавания пород собак. Для того чтобы использовать такую модель вместе с TensorFlow Lite, мы должны сначала конвертировать ее в соответствующий формат с помощью инструмента TensorFlow Lite converter: bazel build tensorflow/contrib/lite/toco: toco bazel-bin/tensorflow/contrib/lite/toco/toco \\ --input_file=/tmp/dog_retrained_mobilenet10_224_not_quantized.pb \\ --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\ --output_file=/tmp/dog_retrained_mobilenet10_224_not_quantized.tflite \\ --inference_type=FLOAT \\ --input_type=FLOAT --input_array=input \\ --output_array=final_result --input_shape=1,224,224,3\n--- Страница 334 ---\n332  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах Мы должны применить параметры ‑‑input_array и ‑‑output_array для ука- зания имени входного и выходного узлов. Подробные параметры командной строки инструмента конвертации converter смотрите по адресу https: //github. com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md. После добавления конвертированного файла модели TensorFlow Lite dog_ retrained_mobilenet10_224_not_quantized.tflite , а также файла меток dog_ retrained_labels.txt из проекта HelloTensorFlow в проект Xcode следует на шаге 4 просто поменять строку NSString* graph = @\"mobilenet_v1_1.0_224\" ; на строку NSString* graph = @\"dog_retrained_mobilenet10_224_not_quantized\" ; и строку const int output_size = 1000 ; на строку const int output_size = 121 ; (напомним, что модель MobileNet классифицирует 1000 объектов, а наша вторично натрени-рованная модель собак классифицирует 121 породу собак), и затем снова за-пустить приложение, используя вторично натренированную модель в формате TensorFlow Lite. Результат будет примерно таким же. Таким образом, после того как мы успешно конвертировали вторично на- тренированную модель MobileNet платформы TensorFlow в модель платфор-мы TensorFlow Lite, применить ее довольно просто. А как же насчет всех этих пользовательских моделей, которые описываются в книге и в других местах? использование пользовательской моДели Tensor Flow liTe в ios В предыдущих главах мы натренировали целый ряд пользовательских мо-делей TensorFlow и выполнили их заморозку для мобильного применения. К сожалению, если вы попытаетесь применить инструмент конвертации bazel‑bin/ tensorflow/contrib/lite/toco/toco платформы TensorFlow Lite, собран- ный в предыдущем разделе, для того чтобы конвертировать модели из форма-та TensorFlow в формат TensorFlow Lite, то все ваши попытки потерпят неудачу, кроме вторично натренированной модели главы 2 «Классифицирование изобра- жений с помощью трансферного обучения», которую мы рассмотрели в преды-дущем подразделе. Большинство ошибок имеет тип «Converting unsupported operation» («Конвертирование неподдерживаемой операции»). Например, сле-дующая ниже команда пытается конвертировать модель обнаружения объек - тов TensorFlow главы 3 «Обнаружение и локализация объектов» в формат плат - формы TensorFlow Lite: bazel-bin/tensorflow/contrib/lite/toco/toco \\ --input_file=/tmp/ssd_mobilenet_v1_frozen_inference_graph.pb \\ --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\ --output_file=/tmp/ssd_mobilenet_v1_frozen_inference_graph.tflite --inference_type=FLOAT \\ --input_type=FLOAT --input_arrays=image_tensor \\ --output_arrays=detection_boxes, detection_scores, detection_classes, num_detections \\ --input_shapes=1,224,224,3\n--- Страница 335 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  333 Но в TensorFlow 1.6 вы получите множество ошибок, в том числе: Converting unsupported operation: TensorArrayV3 Converting unsupported operation: EnterConverting unsupported operation: EqualConverting unsupported operation: NonMaxSuppressionV2Converting unsupported operation: ZerosLike Следующая ниже команда пытается конвертировать модель нейронного пе- реноса стиля главы 4 «Трансформирование рисунков с помощью художественных стилей» в формат платформы TensorFlow Lite: bazel-bin/tensorflow/contrib/lite/toco/toco \\ --input_file=/tmp/stylize_quantized.pb \\ --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\ --output_file=/tmp/stylize_quantized.tflite --inference_type=FLOAT \\ --inference_type=QUANTIZED_UINT8 \\ --input_arrays=input, style_num \\ --output_array=transformer/expand/conv3/conv/Sigmoid \\ --input_shapes=1,224,224,3:26 Следующая далее команда пытается конвертировать модель главы 10 «Соз- дание мобильного игрового AlphaZero-подобного приложения»: bazel-bin/tensorflow/contrib/lite/toco/toco \\ --input_file=/tmp/alphazero19.pb \\ --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\ --output_file=/tmp/alphazero19.tflite --inference_type=FLOAT \\ --input_type=FLOAT --input_arrays=main_input \\ --output_arrays=value_head/Tanh, policy_head/MatMul \\ --input_shapes=1,2,6,7 Но вы также получите целый ряд ошибок с типом «Converting unsupported operation». Платформа TensorFlow Lite все еще находится в предварительной версии, по состоянию на март 2018 года и в TensorFlow 1.6, но в будущих релизах ожи-дается поддержка большего количества операций, поэтому если вы хотите по-пробовать TensorFlow Lite в TensorFlow 1.6, то должны в значительной степени ограничивать свои запросы предварительно натренированными и вторично натренированными моделями Inception и MobileNet, держа при этом руку на пульсе будущих версий TensorFlow Lite. Возможно, что все больше моделей TensorFlow, рассмотренных в предыдущих главах книги и в других местах, бу - дут успешно конвертированы в формат TensorFlow Lite в платформе TensorFlow 1.7 или к тому времени, когда вы прочтете эту книгу. Однако, по крайней мере сейчас, в случае пользовательской и сложной мо- дели, построенной с помощью платформы TensorFlow или библиотеки Keras, скорее всего, вы не сможете выполнить успешную конвертацию в формат TensorFlow Lite, поэтому вам следует придерживаться платформы TensorFlow\n--- Страница 336 ---\n334  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах Mobile, как мы уже обсуждали в предыдущих главах, если только вы не полны решимости заставить их работать с TensorFlow Lite и не против помочь доба-вить больше операций, которые должны поддерживаться в TensorFlow Lite, – в конце концов, ведь TensorFlow является проектом с открытым исходным ко- дом. Прежде чем мы закончим наше освещение платформы TensorFlow Lite, мы рассмотрим способы ее использования в Android. использование платформы Tensor Flow liTe в android Для простоты мы просто покажем, как добавлять в новое приложение для Android поддержку платформы TensorFlow Lite с предварительно построенной моделью TensorFlow Lite MobileNet, раскрывая по ходу некоторые полезные советы. Прежде всего, перед тем как перейти к рассмотрению шагов, необ-ходимых для того, чтобы задействовать TensorFlow Lite в новом приложении для Android, следует отметить, что уже есть пример приложения для Android с использованием платформы TensorFlow Lite, который вы можете запустить с помощью Android Studio (https://www.tensorflow.org/mobile/tflite/demo_android) на устройстве Android с уровнем API не менее 15 (версией не менее 4.0.3). Если вы успешно выполните сборку и запустите пример этого приложения, то вы должны увидеть объекты, распознаваемые с помощью фотокамеры устрой-ства и модели MobileNet для TensorFlow Lite, при наведении вашего устройства Android на окружающие предметы. Теперь выполните следующие ниже действия, чтобы создать новое прило- жение для Android и добавить поддержку платформы TensorFlow Lite для клас - сифицирования изображений, как это было в приложении HelloTensorFlow для Android в главе 2 «Классифицирование изображений с помощью трансферного обучения». 1. Создайте новый проект Android Studio и назовите приложение HelloTFLite . Установите значение минимального SDK равным API 15: Android 4.0.3 – и примит е все остальные значения по умолчанию. 2. Создайте новую папку ресурсов assets , перетащите файл модели TensorFlow Lite mobilenet_quant_v1_224.tflite и файл меток labels.txt из примера приложения, расположенного в папке tensorflow/contrib/ lite/java/demo/app/src/main/assets , а также тестовое изображение, в папке assets приложения HelloTFLite . 3. Перетащит е файл ImageClassifier.java из папки tensorflow/contrib/lite/ java/demo/app/src/main/java/com/example/android/tflitecamerademo в прило- жение HelloTFLite в Android Studio. Файл ImageClassifier.java содержит весь программный код, использующий API Java для TensorFlow Lite, ко-торый загружает и запускает модель TensorFlow Lite, и вскоре мы рас - смотрим его подробно.\n--- Страница 337 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  335 4. Откройт е файл build.gradle приложения, добавьте строку compile 'org. tensorflow: tensorflow ‑lite:0.1' в конец секции зависимостей, и следу - ющие далее три строки после секции buildTypes внутри секции android : aaptOptions { noCompress \"tflite\" } Это необходимо для того, чтобы избежать показанной ниже ошибки, ко- торую вы получите при запуске приложения: 10185‑‑10185/com.ailabby.hellotflite W/System.err: java.io.FileNotFoundException: This file can not be opened as a file descriptor; it is probably compressed03 ‑20 00:32:28.805 10185‑10185/com.ailabby.hellotflite W/System.err: at android.content.res. AssetManager.openAssetFd(Native Method)03 ‑20 00:32:28.806 10185‑10185/com.ailabby.hellotflite W/System.err: at android.content.res. AssetManager.openFd(AssetManager.java:390)03 ‑20 00:32:28.806 10185‑10185/com.ailabby.hellotflite W/System.err: at com.ailabby. hellotflite.ImageClassifier.loadModelFile(ImageClassifier.java:173) Теперь ваше приложение HelloTFLite в среде Android Studio должно вы- глядеть как на рис. 11.6. Рисунок 11.6. Новое приложение для Android с использованием платформы TensorFlow Lite и готовой модели MobileNet классифицирования изображений 5. Д обавьте элементы управления ImageView и Button в файл activity_main. xml, как мы делали неоднократно раньше. Затем в методе onCreate файла MainActivity.java назначьте элементу управления ImageView содержимое тестового изображения, назначьте слушателю касаний в кнопке Button\n--- Страница 338 ---\n336  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах запуск нового потока и инстанцируйте экземпляр ImageClassifier под названием classifier: private ImageClassifier classifier; @Override protected void onCreate(Bundle savedInstanceState) { try { classifier = new ImageClassifier(this); } catch (IOException e) { Log.e(TAG, \"Failed to initialize an image classifier.\"); } 6. Мет од run потока считывает данные тестового изображения в объект Bitmap , вызывает метод classifyFrame объекта ImageClassifier и показы- вает результат в виде всплывающего уведомления Toast : Bitmap bitmap = BitmapFactory.decodeStream(getAssets().open(IMG_FILE));Bitmap croppedBitmap = Bitmap.createScaledBitmap(bitmap, INPUT_SIZE, INPUT_SIZE, true);if (classifier == null) { Log.e(TAG, \"Uninitialized Classifier or invalid context.\"); return; }final String result = classifier.classifyFrame(croppedBitmap);runOnUiThread( new Runnable() { @Override public void run() { mButton.setText(\"TF Lite Classify\"); Toast.makeText(getApplicationContext(), result, Toast.LENGTH_LONG).show(); } }); Если вы запустите приложение сейчас, то увидите тестовое изображение и кнопку с надписью «TF Lite Classify». Коснитесь кнопки, и вы получите результат классифицирования, к примеру «Labrador retriever (лабрадор-ретривер): 0.86 pug (мопс): 0.05 dalmatian (далматинец): 0.04».Программный код, связанный с платформой TensorFlow Lite, в объекте ImageClassifier , который использует корневой класс org.tensorflow.lite. Interpreter и его метод run для запуска модели, выглядит следующим образом: import org.tensorflow.lite.Interpreter; public class ImageClassifier { private Interpreter tflite; private byte[][] labelProbArray = null;\n--- Страница 339 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  337 ImageClassifier(Activity activity) throws IOException { tflite = new Interpreter(loadModelFile(activity)); } String classifyFrame(Bitmap bitmap) { if (tflite == null) { Log.e(TAG, \"Image classifier has not been initialized; Skipped.\"); return \"Uninitialized Classifier.\"; } convertBitmapToByteBuffer(bitmap); tflite.run(imgData, labelProbArray); } И метод loadModelFile определен так: private MappedByteBuffer loadModelFile(Activity activity) throws IOException { AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_PATH); FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor()); FileChannel fileChannel = inputStream.getChannel(); long startOffset = fileDescriptor.getStartOffset(); long declaredLength = fileDescriptor.getDeclaredLength(); return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength); } Напомним, что в шаге 4 мы должны добавить в файл build.gradle строку noCompress \"tflite\" , в противном случае метод openFd вызовет ошибку. Данный метод возвращает версию модели в формате memmapped, и мы уже говорили об использовании инструмента convert_graphdef_memmapped_format для конвер- тирования моделей TensorFlow Mobile в формат memmapped в главе 6 «Описа- ние изображений на естественном языке» и в главе 9 «Генерирование и улучшение изображений с помощью GAN-сети». Вот и все, что требуется для того, чтобы загрузить и запустить готовую мо- дель TensorFlow Lite в новом приложении для Android. Если вы интересуетесь использованием вторично натренированных и конвертированных моделей TensorFlow Lite, как мы это делали в приложениях для iOS и для Android, либо пользовательской модели TensorFlow Lite, при условии что вы успешно по-лучите конвертированную версию такой модели, то можете попробовать их поверх приложения HelloTFLite . На этом мы отложим в сторону передовую платформу TensorFlow Lite и перейдем к еще одной потрясающей и высоко-значимой на международной конференции разработчиков (WWDC) теме для разработчиков iOS.\n--- Страница 340 ---\n338  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах платформа core Ml Для ios – краткий обзор Платформа Core ML компании Apple (https://developer.apple.com/documentation/ coreml) позволяет разработчикам приложений для iOS легко использовать на-тренированные машинно-обучающиеся модели в своих приложениях для iOS, работающих под управлением iOS11 или выше и собранных с помощью среды разработки Xcode 9 или выше. Вы можете скачать и использовать предвари-тельно натренированные модели, уже находящиеся в формате Core ML, предла-гаемые компанией Apple по адресу https://developer.apple.com/machine-learning, или инструмент для Python под названием coremltools, представляющий собой инструментарий сообщества разработчиков Core ML, по адресу https: //apple. github.io/coremltools, для конвертирования других машинно-обуча ющих ся мо- делей и глубоко обучающихся моделей в формат Core ML. Предварительно натренированные модели в формате Core ML включают в себя популярные модели MobileNet и Inception v3, а также более позднюю модель ResNet50 (мы кратко затронули тему остаточных сетей в главе 10 «Соз- дание мобильного игрового AlphaZero-подобного приложения»). Модели, которые могут быть конвертированы в формат Core ML, включают в себя глубоко обуча - ющиеся модели, построенные с помощью платформы Caffe и библиотеки Keras, а также традиционные машинно-обучающиеся модели, такие как линейно-ре-грессионные, опорно-векторные, деревьев решений, построенные с помощью библиотеки Scikit-Learn (http: //scikit-learn.org), очень популярной библиотеки машинного обучения для Python. Поэтому если вы хотите использовать традиционные машинно-обучающие - ся модели в iOS, то Scikit-Learn и Core ML – это, безусловно, верный путь. Хотя эта книга посвящена мобильной платформе TensorFlow, создание интеллекту - альных приложений иногда не требует глубокого обучения; в некоторых случа-ях применения имеет смысл задействовать классическое машинное обучение. Кроме того, поддержка моделей Scikit-Learn в платформе Core ML настолько гладкая, что мы просто не можем отказаться от возможности кратко их про-смотреть, чтобы знать, когда следует притормозить со своими навыками ис - пользования мобильной платформы TensorFlow, если того требует ситуация. Если вы хотите использовать предварительно натренированные модели MobileNet Core ML, то обратитесь к удобному демонстрационному примеру от компании Apple «Classifying Images with Vision and Core ML» («Классифици-рование изображений с привлечением компьютерного зрения и платформы Core ML») по адресу https://developer.apple.com/documentation/vision/classifying_ images_with_vision_and_core_ml, а также обязательно посмотрите видеороли-ки с международной конференции разработчиков WWDC2017, посвященные Core ML, перечисленные на веб-странице https://developer.apple.com/machine- learning. В следующих двух разделах мы покажем вам два руководства о том, как кон- вертировать и использовать модели Scikit-Learn и RNN-модель предсказания\n--- Страница 341 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  339 биржевой цены в библиотеке Keras с TensorFlow в качестве бэкенда, которую мы построили в главе 8 «Предсказание биржевой цены с помощью RNN-сети». Вы увидите полноценные приложения для iOS, созданные с нуля с исходным кодом на языках Objective-C и Swift для использования конвертированных мо-делей Core ML. Если выражение «с нуля» приводит вас в волнение и напомина-ет об AlphaZero, то вам, по всей видимости, понравилась предыдущая глава 10 «Создание мобильного игрового AlphaZero-подобного приложения». использование платформы core Ml с машинным обучением на основе библиотеки sciKiT-learn Линейная регрессия и опорно-векторные машины – это две из наиболее рас - пространенных классических машинно-обучающихся алгоритмов, которые, разумеется, поддерживаются библиотекой Scikit-Learn. Мы обсудим вопрос построения модели, которая предсказывает цены на жилье с использованием этих двух алгоритмов. построение и конвертирование моДелей sciKiT-learn Прежде всего давайте получим набор данных о ценах на жилье, доступный для скачивания по адресу https://wiki.csc.calpoly.edu/datasets/wiki/Houses. Скачан- ный файл RealEstate.csv выглядит так: MLS, Location, Price, Bedrooms, Bathrooms, Size, Price/SQ.Ft, Status132842, Arroyo Grande,795000.00,3,3,2371,335.30, Short Sale134364, Paso Robles,399000.00,4,3,2818,141.59, Short Sale135141, Paso Robles,545000.00,4,3,3032,179.75, Short Sale Мы будем использовать pandas (https: //pandas.pydata.org), популярную биб - лиотеку Python анализа данных с открытым исходным кодом для анализа csv- файла. Для того чтобы установить библиотеки Scikit-Learn и Pandas, выполните следующие ниже команды, предпочтительно из созданной ранее виртуальной среды TensorFlow и Keras: pip install scikit-learnpip install pandas Теперь введите следующий ниже программный код, чтобы прочитать и про- анализировать файл RealEstate.csv . В качестве входных данных используйте все строки в столбцах с четвертого по шестой (Bedrooms, Bathrooms и Size – спальни, ванные комнаты и размер), а все строки со столбцом три (Price – цена) – в качес тве целевого вывода: from sklearn.linear_model import LinearRegressionfrom sklearn.svm import LinearSVR\n--- Страница 342 ---\n340  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах import pandas as pd import sklearn.model_selection as ms data = pd.read_csv('RealEstate.csv') X, y = data.iloc[:, 3:6], data.iloc[:, 2] Разделите набор данных на тренировочный и тестовый наборы и натрени- руйте набор данных с помощью линейной регрессионной модели Scikit Learn, используя стандартный метод подгонки fit: X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.25) lr = LinearRegression() lr.fit(X_train, y_train) Протестируйте натренированную модель тремя новыми входами (3 спаль- ни, 2 ванные комнаты, 1560 квадратных футов и т. д.) с использованием метода предсказания predict : X_new = [[3, 2, 1560.0], [3, 2, 1680], [5, 3, 2120]] print(lr.predict(X_new)) В результате в качестве предсказываемой цены дома будет выведено три значения: [319289.9552276, 352603.45104977, 343770.57498118] . Для того чтобы натренировать модель с использованием опорно-векторных машин и протестировать ее с помощью ввода X_new , добавьте следующий ниже программный код: svm = LinearSVR(random_state=42)svm.fit(X_train, y_train) print(svm.predict(X_new)) В результате будут выведены цены на жилье, предсказанные с по - мощью модели опорно-векторных машин [298014.41462535, 320991.94354092, 404822.78465954] . Мы не будем обсуждать вопрос, какая из этих моделей лучше, как улучшить работу линейно-регрессионной модели или модели опорно- векторных машин, или как выбрать самую лучшую модель среди всех алго-ритмов, поддерживаемых библиотекой Scikit-Learn, – широкий диапазон не- плохих книг и интернет-ресурсов специально посвящен этим темам. Для того чтобы конвертировать две указанные модели Scikit-Learn, lr и svm, в формат Core ML, который можно использовать в ваших приложениях для iOS, вы должны сначала установить инструментарий Core ML (https: //github. com/apple/coremltools). Мы рекомендуем вам установить его, используя коман-ду pip install ‑U coremltools в виртуальной среде TensorFlow и Keras, которую\n--- Страница 343 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  341 мы создали и использовали в главе 8 «Предсказание биржевой цены с помощью RNN-сети» и главе 10 «Создание мобильного игрового AlphaZero-подобного при- ложения», так как мы будем также использовать ее для конвертирования нашей модели Keras в следующем далее разделе. Теперь выполните следующий ниже программный код с целью конвертации двух моделей Scikit-Learn в формат Core ML: import coremltoolscoreml_model = coremltools.converters.sklearn.convert(lr, [\"Bedrooms\", \"Bathrooms\", \"Size\"], \"Price\")coreml_model.save(\"HouseLR.mlmodel\") coreml_model = coremltools.converters.sklearn.convert(svm, [\"Bedrooms\", \"Bathrooms\", \"Size\"], \"Price\")coreml_model.save(\"HouseSVM.mlmodel\") Для получения более подробной информации об инструменте конвертации обратитесь к его онлайновой документации по адресу https: //apple.github.io/ coremltools/coremltools.converters.html. Теперь мы можем добавить обе эти мо-дели в приложение для iOS на языках Objective-C или Swift, однако тут мы пока-жем только пример на языке Swift; вы увидите примеры на языках Objective-C и Swift в следующем разделе, где будет применена модель Core ML предсказа-ния биржевой цены, конвертированная из модели Keras и TensorFlow. использование конвертированных в формат core Ml моДелей в ios После добавления в новый проект Xcode на языке Swift для iOS двух файлов модели Core ML, HouseLR.mlmodel и HouseSVM.mlmodel , модель HouseLR.mlmodel вы- глядит, как на рис. 11.7.\n--- Страница 344 ---\n342  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах Рисунок 11.7. Проект для iOS на языке Swift и линейно-регрессионная модель Core ML Другая модель, HouseSVM.mlmodel , выглядит точно так же, за исключением того, что имя машинно-обучающейся модели и класс модели изменены с HouseLR на HouseSVM . Добавьте следующий ниже программный код в класс ViewController в файле ViewController.swift : private let lr = HouseLR() private let svm = HouseSVM()override func viewDidLoad() { super.viewDidLoad() let lr_input = HouseLRInput(Bedrooms: 3, Bathrooms: 2, Size: 1560) let svm_input = HouseSVMInput(Bedrooms: 3, Bathrooms: 2, Size: 1560) guard let lr_output = try? lr.prediction(input: lr_input) else { return } print(lr_output.Price) guard let svm_output = try? svm.prediction(input: svm_input) else {\n--- Страница 345 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  343 return } print(svm_output.Price) } Все должно быть довольно просто. Запуск приложения приведет к распечат - ке: 319289.955227601 298014.414625352 Данный результат совпадает с первыми двумя числами в двух массивах, вы- веденных сценарием Python в предыдущем подразделе, так как мы использу - ем первое входное значение в переменной X_new программного кода Python для входа в модели предсказания HouseLR и HouseSVM. использование платформы core Ml с Keras и Tensor Flow Инструментарий coremitools также официально поддерживает конвертиро- ванные модели, построенные с помощью библиотеки Keras (смотрите ссыл-ку keras.convert на веб-странице https: //apple.github.io/coremltools/coremltools. converters.html). Последняя версия инструментария coremltools, 0.8 по состоя - нию на март 2018 года, работает с TensorFlow 1.4 и Keras 2.1.5, которые мы использовали для построения модели предсказания цен на акции в главе 8 «Предсказание биржевой цены с помощью RNN-сети». Существует два способа использования инструментария coremltools для создания модели в формате Core ML. Первый – вызвать методы convert и save инструментария coremltools непосредственно в программном коде Python Keras после выполнения трени-ровки модели. Например, добавьте последние ниже три строки кода в файл Python ch8/python/keras/train.py после выполнения подгонки model.fit : model.fit( X_train, y_train, batch_size=512, epochs=epochs, validation_split=0.05) import coremltoolscoreml_model = coremltools.converters.keras.convert(model)coreml_model.save(\"Stock.mlmodel\") Для конвертации нашей модели мы можем проигнорировать приведенное ниже предупреждение, появляющееся во время выполнения нового сценария: WARNING: root: Keras version 2.1.5 detected. Last version known to be fully compatible of Keras is 2.1.3.\n--- Страница 346 ---\n344  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах Когда вы перетащите сгенерированный файл Stock.mlmodel в проект Xcode 9.2 для iOS, он будет использовать имена, принятые по умолчанию для ввода, input1 , и вывода, output1 , как показано на рис. 11.8, что нормально в приложе- ниях для iOS как на языке Objective-C, так и на языке Swift. Рисунок 11.8. Модель Core ML предсказания биржевой цены, конвертированная из Keras и TensorFlow в приложении на языке Objective-C Другой способ использовать инструментарий coremltools для генериро- вания модели в формате Core ML – сначала сохранить модель, построенную с помощью библиотеки Keras, в формате Keras HDF5, который мы использова- ли в главе 10 «Создание мобильного игрового AlphaZero-подобного приложения», перед конвертированием в файлы контрольных точек AlphaZero TensorFlow. Для этого нужно лишь выполнить инструкцию model.save('stock.h5') . Затем вы можете применить следующий ниже фрагмент кода для конверти- рования модели Keras.h5 в модель Core ML: import coremltoolscoreml_model = coremltools.converters.keras.convert('stock.h5',input_names = ['bidirectional_1_input'],output_names = ['activation_1/Identity'])\n--- Страница 347 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  345 coreml_model.save('Stock.mlmodel') Обратите внимание, что здесь мы использовали те же входное и выходное имена, что и при заморозке файлов контрольных точек TensorFlow. Если вы перетащите модель Stock.mlmodel в проект на языке Objective-C, то в файле Stock.h автоматически будет сгенерирована ошибка из-за того, что дефект в среде разработки Xcode 9.2 не способен правильно справляться с символом \"/\" в выходном имени activation_1/Identity . Если же речь идет об объекте iOS на языке Swift, то автоматически сгенерированный файл Stock.swift правиль- но изменит символ \"/\" на символ \"_\", тем самым избегая ошибки компиляции, как показано на рис. 11.9 с программным приложением на языке Swift. Рисунок 11.9. Модель Core ML предсказания биржевой цены, конвертированная из Keras и TensorFlow в приложении на языке Swift Для того чтобы применить модель в приложении на языке Objective-C, создайте объект Stock и объект MLMultiArray с указанным типом и формой данных, а затем заполните массив входными данными и вызовите метод predictionFromFeatures , используя экземпляр StockInput , инициализированный данными массива MLMultiArray : #import \"ViewController.h\"#import \"Stock.h\"\n--- Страница 348 ---\n346  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах @interface ViewController () @end @implementation ViewController ‑ (void)viewDidLoad { [super viewDidLoad]; Stock *stock = [[Stock alloc] init]; double input[] = { 0.40294855, 0.39574954, 0.39789235, 0.39879138, 0.40368535, 0.41156033, 0.41556879, 0.41904324, 0.42543786, 0.42040193, 0.42384258, 0.42249741, 0.4153998, 0.41925279, 0.41295281, 0.40598363, 0.40289448, 0.44182321, 0.45822208, 0.44975226}; NSError *error = nil; NSArray *shape = @[@20, @1, @1]; MLMultiArray *mlMultiArray = [[MLMultiArray alloc] initWithShape:(NSArray*)shape dataType: MLMultiArrayDataTypeDouble error:&error]; for (int i = 0; i < 20; i++) { [mlMultiArray setObject:[NSNumber numberWithDouble: input[i]] atIndexedSubscript:(NSInteger)i]; } StockOutput *output = [stock predictionFromFeatures:[[StockInput alloc] initWithInput1: mlMultiArray] error:&error]; NSLog(@\"output =%@\", output.output1); } Мы использовали здесь жестко закодированные нормализованные входные данные и простой способ вывода информации NSLog только для демонстрации\n--- Страница 349 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  347 того, как использовать модель Core ML. Если вы сейчас запустите приложение, то увидите выходное значение 0.4486984312534332, которое после денормали-зации показывает цену акций, предсказываемую на следующий день. Версия приведенного выше программного кода на языке Swift выглядит сле- дующим образом: import UIKitimport CoreML class ViewController: UIViewController { private let stock = Stock() override func viewDidLoad() { super.viewDidLoad() let input = [ 0.40294855, 0.39574954, 0.45822208, 0.44975226] guard let mlMultiArray = try? MLMultiArray(shape:[20,1,1], dataType: MLMultiArrayDataType.double) else { fatalError(\"Unexpected runtime error. MLMultiArray\") } for (index, element) in input.enumerated() { mlMultiArray[index] = NSNumber(floatLiteral: element) } guard let output = try? stock.prediction(input: StockInput(bidirectional_1_input: mlMultiArray)) else { return } print(output.activation_1_Identity) } } Обратите внимание, что мы используем bidirectional_1_input и activation_1_ identity , как делали в приложении TensorFlow Mobile для iOS, с целью установ- ки входных данных и получения выходных данных. Если вы попытаетесь конвертировать модель AlphaZero, которую мы по- строили и натренировали в Keras в главе 10 «Создание мобильного игрового AlphaZero-подобного приложения», то вы получите ошибку ValueError: Unknown loss function: softmax_cross_entropy_with_logits , сообщающую о неизвест - ной функции потери. Если вы попытаетесь конвертировать другие модели TensorFlow, которые мы построили в книге, то самым лучшим неофициальным\n--- Страница 350 ---\n348  Применение платформ TensorFlow Lite и Core ML на мобильных устройствах инструментом, который можно применить, является конвертер TensorFlow в формат Core ML на https: //github.com/tf-coreml/tf-coreml. К сожалению, анало- гично платформе TensorFlow Lite, он поддерживает только ограниченный на-бор определений операций TensorFlow. В некоторых случаях причиной тому являются пределы Core ML, в других случаях – пределы конвертера tf-coreml. Мы не будем вдаваться в детали конвертирования моделей TensorFlow в моде-ли Core ML, однако, по крайней мере, вы увидели, как можно конвертировать и использовать традиционные машинно-обучающиеся модели, построенные с помощью библиотеки Scikit-Learn, а также RNN-модель, построенную с по-мощью библиотеки Keras, что, надо полагать, предоставит вам хорошую основу для создания и использования моделей Core ML в будущем. Разумеется, если вам нравится Core ML, то вы непременно должны постоянно следить за вы-ходом его будущих улучшенных версий, а также будущих релизов инструмен-тария coremltools и конвертера tf-coreml. Многое из того, что касается плат - формы Core ML, мы не рассмотрели – чтобы получить четкое представление обо всех функциональных возможностях платформы Core ML, обратитесь к полной документации по ее API на веб-странице https://developer.apple.com/ documentation/coreml/core_ml_api. резюме В этой главе мы рассмотрели два передовых инструмента, которые приме-няются к машинно-обучающимся и глубоко обучающимся моделям на мо-бильных и встраиваемых устройствах: платформы TensorFlow Lite и Core ML. Платформа TensorFlow Lite все еще находится в предварительной версии. Она имеет ограниченную поддержку операций TensorFlow и в своих будущих рели-зах будет поддерживать все больше и больше функциональных возможностей платформы TensorFlow, сохраняя при этом значение задержки и размер при-ложения небольшим. Мы предложили пошаговые инструкции по разработке приложений TensorFlow Lite для iOS и Android классифицирования изобра-жений с нуля. Платформа Core ML – это разработка компании Apple для раз- работчиков мобильных приложений, предназначенная для интегрирования машинного обучения в приложения для iOS. Она имеет большую поддержку конвертирования и использования классических машинно-обучающихся мо-делей, созданных в библиотеке Scikit-Learn, а также хорошую поддержку мо-делей на основе Keras. Мы также показали способы конвертирования моделей Scikit-Learn и Keras в модели Core ML и их применения в приложениях на язы-ках Objective-C и Swift. На сегодняшний день у платформ TensorFlow Lite и Core ML имеется несколько серьезных ограничений, которые приводят к тому, что они не способны конвертировать сложные модели TensorFlow и Keras, которые мы построили в этой книге. Но у них уже имеются свои варианты использова-ния, и в будущем они будут становиться только лучше. Самое большее, что мы можем сделать, – это быть в курсе их вариантов использования, их пределов\n--- Страница 351 ---\nПрименение платформ TensorFlow Lite и Core ML на мобильных устройствах  349 и их потенциала, с тем чтобы мы могли выбрать наиболее подходящие инстру - менты, сейчас или в будущем, для различных задач. В конце концов, у нас есть больше, чем просто молоток, поэтому не все должно выглядеть как гвоздь. В следующей и последней главе книги мы отберем несколько моделей, которые построили раньше, и добавим возможности самообучения с под-креплением – ключевую технологию, лежащую в основе успеха программ AlphaGo и AlphaZero и, по данным MIT Review, одну из 10 прорывных тех - нологий в 2017 году, – в потрясающую платформу Raspberry Pi, небольшой, недорогой, но мощный компьютер – кто не любит комбинаций из трех? Мы посмотрим, сколько интеллекта – зрения, слуха, ходьбы, равновесия и, ко- нечно же, самообучения – нам удастся запихнуть в крохотного робота, при- водимого в движение «малиновым пирогом» (Raspberry Pi) в одной главе. Если в наше время беспилотный автомобиль является одной из самых горя-чих технологий ИИ, то самоходный робот может быть одной из наших самых крутых игрушек дома.",
      "debug": {
        "start_page": 324,
        "end_page": 351
      }
    },
    {
      "name": "Глава 12. Разработка приложений T ensorFlow на компьютере Raspberry Pi 350",
      "content": "--- Страница 352 --- (продолжение)\nГлава 12 Разработка приложений T ensorFlow на компьютере Raspberry Pi Согласно Википедии, «Raspberry Pi – это серия небольших одноплатных компью теров, разработанных в Соединенном Королевстве Фондом Raspberry Pi для содействия преподаванию основ компьютерных наук в школах и в раз- вивающихся странах». Официальный сайт Raspberry Pi (https://www.raspberrypi.org) описывает его как «небольшой и доступный компьютер, который мож - но использовать для изучения программирования». Если вы никогда раньше не слышали или не использовали Raspberry Pi, то зайдите на его веб-сайт, и, ско-рее всего, вы быстро влюбитесь в этот милый небольшой гаджет. Мал, да удал – на самом деле разработчики TensorFlow сделали платформу TensorFlow до- ступной на компьютере Raspberry Pi с его ранних версий, примерно с середины 2016 года, так что мы можем запускать сложные модели TensorFlow на кро-шечном компьютере, который вы можете приобрести приблизительно за $35. Это, вероятно, выходит за рамки «преподавания основ информатики» или «из-учения программирования», но, с другой стороны, если мы подумаем обо всех этих стремительных достижениях в области мобильных устройств за послед-ние несколько лет, то мы не должны удивляться тому, как все более крупные функциональные возможности реализовывались во все меньших по размеру устройствах. В этой главе мы войдем в веселый и захватывающий мир Raspberry Pi, са- мого миниатюрного устройства, официально поддерживаемого платформой TensorFlow. Сначала мы расскажем, как получить и настроить новую плату Raspberry Pi 3 B, включая все необходимые аксессуары, используемые в этой главе, которые позволят ей видеть, слышать и говорить. Затем мы расскажем, как использовать базовый комплект робота GoPiGo Robot Base Kit (https://www.dexterindustries.com/shop/gopigo3-robot-base-kit/), для того чтобы превратить плату Raspberry Pi в робота, который может двигаться. После этого мы пре-\nГлава 12 Разработка приложений T ensorFlow на компьютере Raspberry Pi Согласно Википедии, «Raspberry Pi – это серия небольших одноплатных компью теров, разработанных в Соединенном Королевстве Фондом Raspberry Pi для содействия преподаванию основ компьютерных наук в школах и в раз- вивающихся странах». Официальный сайт Raspberry Pi (https://www.raspberrypi.org) описывает его как «небольшой и доступный компьютер, который мож - но использовать для изучения программирования». Если вы никогда раньше не слышали или не использовали Raspberry Pi, то зайдите на его веб-сайт, и, ско-рее всего, вы быстро влюбитесь в этот милый небольшой гаджет. Мал, да удал – на самом деле разработчики TensorFlow сделали платформу TensorFlow до- ступной на компьютере Raspberry Pi с его ранних версий, примерно с середины 2016 года, так что мы можем запускать сложные модели TensorFlow на кро-шечном компьютере, который вы можете приобрести приблизительно за $35. Это, вероятно, выходит за рамки «преподавания основ информатики» или «из-учения программирования», но, с другой стороны, если мы подумаем обо всех этих стремительных достижениях в области мобильных устройств за послед-ние несколько лет, то мы не должны удивляться тому, как все более крупные функциональные возможности реализовывались во все меньших по размеру устройствах. В этой главе мы войдем в веселый и захватывающий мир Raspberry Pi, са- мого миниатюрного устройства, официально поддерживаемого платформой TensorFlow. Сначала мы расскажем, как получить и настроить новую плату Raspberry Pi 3 B, включая все необходимые аксессуары, используемые в этой главе, которые позволят ей видеть, слышать и говорить. Затем мы расскажем, как использовать базовый комплект робота GoPiGo Robot Base Kit (https://www.dexterindustries.com/shop/gopigo3-robot-base-kit/), для того чтобы превратить плату Raspberry Pi в робота, который может двигаться. После этого мы пре-\n--- Страница 353 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  351 доставим самые простые рабочие шаги по настройке платформы TensorFlow 1.6 на компью тере Raspberry Pi и создадим примеры приложений для этого компью тера. Мы также обсудим вопрос интеграции модели классифициро- вания изображений, которую мы использовали в главе 2 «Классифицирование изображений с помощью трансферного обучения», с речевым воспроизведением текста (TTS), чтобы научить робота сообщать нам о том, что он распознает, и во-прос интеграции модели распознавания звука, которую мы использовали в гла-ве 5 «Понимание простых речевых команд», с API GoPiGo, чтобы дать вам возмож - ность использовать голосовые команды для управления движением робота. Наконец, мы покажем вам, как использовать платформу TensorFlow и ин- струментарий Python, библиотеку OpenAI Gym, предназначенную для разра-ботки и сравнения алгоритмов, обучающихся на основе максимизации воз-награждения, для того чтобы реализовать мощный обучающийся алгоритм в симулированной среде и подготовить нашего робота к передвижению и ба-лансированию в реальной физической среде. На конференции Google I/O 2016 проходил семинар под названием «Как соз- дать умного бота RasPi с помощью облачного API зрения и речи» (вы можете посмотреть видеоролик в YouTube). Там облачный API Google используется для классифицирования изображений вместе с распознаванием и синтезировани-ем речи. В этой главе на примере приложения, демонстрирующего мощь плат - формы TensorFlow на компьютере Raspberry Pi, мы увидим реализацию этих задач, а также самообучения с подкреплением, в автономном режиме прямо на устройстве. Резюмируя, в этой главе мы рассмотрим следующие темы, чтобы построить робота, который двигается, видит, слушает, говорит и учится: настройка компьютера Raspberry Pi и приведение его в движение; настройка платформы TensorFlow на компьютере Raspberry Pi; распознавание изображ ений и речевое воспроизведение текста; распознавание звука и движение робота; самооб учение с подкреплением на компьютере Raspberry Pi. настройка компьютера rasPberr Y Pi и привеДение его в Движение Серия небольших, одноплатных компьютеров Raspberry Pi состоит из плат Raspberry Pi 3 B+, 3 B, 2B, 1 B+, 1 A+, Zero и Zero W (для получения подробной информации по материнским платам обратитесь на веб-страницу https://www. raspberrypi.org/products/#buy-now-modal). Здесь мы будем использовать мате-ринскую плату Pi 3 B, которую вы можете приобрести примерно за 35 дол-ларов, перейдя по указанной выше ссылке или же на Amazon (https://www.amazon.com/gp/product/B01CD5VC92). Аксессуары, которые мы использовали и протестировали с материнской платой, вместе с их ценами выглядят следу - ющим образом:\n--- Страница 354 ---\n352  Разработка приложений TensorFlow на компьютере Raspberry Pi б лок питания CanaKit 5V 2.5A Raspberry Pi, цена около $10 (https://www. amazon.com/gp/product/B00MARDJZ4), для использования во время разра- ботки; мини-микрофон Kinobo-USB2.0, цена около $4 (https://www.amazon.com/ gp/product/B00IR8R7WQ), для записи голосовых команд; мини- динамик Ushonk USB, цена около $12 (https://www.amazon.com/gp/ product/B075M7FHM1), для воспроизведения синтезированного голоса; мини-фо токамера Arducam 5 Megapixels 1080p Sensor OV5647, цена око- ло $14 (https://www.amazon.com/gp/product/B012V1HEP4), для поддержки классифицирования изображений; карта памяти MicroSD емкостью 16 GB и адаптер, цена около $10 (https://www.amazon.com/gp/product/B00TDBLTWK), для хранения устано-вочных файлов Raspbian, официальной операционной системы Raspberry Pi и в качестве жесткого диска после установки; USB- диск, например SanDisk 32GB USB Drive, цена $9 (https://www.amazon. com/gp/product/B008AF380Q), который используется в качестве раздела подкачки (обратитесь к следующему разделу за подробностями), для того чтобы мы могли вручную собрать библиотеку TensorFlow, необхо-димую для сборки и запуска кода C++ TensorFlow; базовый комплект робота GoPiGo Robot Base Kit, цена $110 (https://www. amazon.com/gp/product/B00NYB3J0A или официальный сайт по адресу https://www.dexterindustries.com/shop), для превращения материнской платы Raspberry Pi в робота, который может двигаться. Вам также понадобятся кабель HDMI для подключения платы Raspberry Pi к монитору компьютера, USB-клавиатура и USB-мышь. В общей сложности, для того чтобы построить робота Raspberry Pi, который двигается, видит, слушает и говорит, нужно около $200, включая $110 за комплект GoPiGo. Хотя комплект GoPiGo выглядит немного дорогим, по сравнению с мощным компьютером Raspberry Pi, без него неподвижный компьютер Raspberry Pi, вероятно, поте-ряет большую часть своей привлекательности.  Ес ть давнишний блог-пост «Как построить робота за $100, который “видит”, и с помощью TensorFlow» (https://www.oreilly.com/learning/how-to-build-a-robot-that-sees-with -100- and-tensorflow), написанный Лукасом Бивальдом (Lukas Biewald) в сентябре 2016 года, рассказывающий, как применять платформу TensorFlow и компьютер Raspberry Pi 3 с другими составными деталями для создания робота, который видит и говорит. Увле-кательная статья – прочтит е обязательно. Мы же предлагаем, помимо распознавания голосовых команд и самообучения на основе максимизации вознаграждения, более подробные шаги по настройке Raspberry Pi 3 вместе с комплектом GoPiGo, удобным и рекомендуемым компанией Google инструментарием для превращения компьютера Raspberry Pi в робота, а также более свежую версию платформы TensorFlow 1.6. Теперь давайте сначала посмотрим, как настроить операционную систему Raspbian для материнской платы Raspberry Pi.\n--- Страница 355 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  353 настройка материнской платы rasPberr Y Pi Самый простой способ – следовать инструкциям по установке операционной системы Raspbian, приводимым на веб-странице https://www.raspberrypi.org/ learning/software-guide/quickstart, которые, в сущности, сводятся к простому трехступенчатому процессу: 1) скачать и установить утилиту SD Formatter (https://www.sdcard.org/ downloads/formatter_4/index.html) для Windows или Mac; 2) применить утилиту SD Formatter для форматирования карты памяти MicroSD; 3) скачать автономную ZIP-версию NOOBS-установщика (New Out Of Box Software, готового к применению), официального простого установщи-ка для операционной системы Raspbian, с https://www.raspberrypi.org/ downloads/noobs, распаковать ее, а затем перетащить все файлы в рас - пакованной папке NOOBS в отформатированную карту памяти MicroSD. Теперь извлеките карту памяти MicroSD и вставьте ее в материнскую плату Raspberry Pi. Подключите кабель HDMI к монитору, а также клавиатуру и мышь USB к плате. Подключите материнскую плату через блок питания и затем сле-дуйте инструкциям на экране для выполнения шагов установки операционной системы Raspbian, включая настройку сети Wifi. Вся установка занимает менее часа. После того как это сделано, вы можете открыть терминал и ввести ко-манду ifconfig , чтобы узнать IP-адрес материнской платы, а затем примените команду ssh pi@<ip_адрес_платы> из своего компьютера, чтобы получить к ней доступ, что, как мы увидим позже, очень удобно и необходимо для управления тестированием робота Raspberry Pi в движении – вы же не хотите или не смо- жете таскать клавиатуру, мышь и монитор вместе с платой, когда она шеве-лится? Однако протокол SSH по умолчанию не включен, поэтому при попытке под- ключиться к вашей плате Pi командой ssh в первый раз вы получите ошибку «SSH connection refused» (SSH-соединение отклонено). Самый быстрый способ включить данный протокол – выпо лнить следующие ниже две команды: sudo systemctl enable sshsudo systemctl start ssh После этого вы можете выполнить команду ssh, используя принятый по умолчанию пароль для входа в систему pi, то есть raspberry . Разумеется, вы можете изменить пароль, установленный по умолчанию, на новый, применив команду passwd . Теперь, когда у нас установлена операционная система Raspbian, давайте вставим мини-микрофон USB, мини-динамик USB и мини-фотокамеру в ма-теринскую плату Pi. Мини-микрофон и мини-динамик конфигурируются ав- томатически (по стандарту Plyg-and-Play). После того как они вставлены, вы можете применить команду aplay ‑l, чтобы узнать поддерживаемые устрой- ства воспроизведения звука:\n--- Страница 356 ---\n354  Разработка приложений TensorFlow на компьютере Raspberry Pi aplay -l **** List of PLAYBACK Hardware Devices ****card 0: Device_1 [USB2.0 Device], device 0: USB Audio [USB Audio] Subdevices: 1/1 Subdevice #0: subdevice #0 card 2: ALSA [bcm2835 ALSA], device 0: bcm2835 ALSA [bcm2835 ALSA] Subdevices: 8/8 Subdevice #0: subdevice #0 Subdevice #1: subdevice #1 Subdevice #2: subdevice #2 Subdevice #3: subdevice #3 Subdevice #4: subdevice #4 Subdevice #5: subdevice #5 Subdevice #6: subdevice #6 Subdevice #7: subdevice #7 card 2: ALSA [bcm2835 ALSA], device 1: bcm2835 ALSA [bcm2835 IEC958/HDMI] Subdevices: 1/1 Subdevice #0: subdevice #0 Материнская палата Pi также имеет аудиоразъем, который вы можете ис - пользовать для того, чтобы получать звук во время разработки. Но USB-динамик, конечно, удобнее. Для того чтобы узнать поддерживаемое записывающее устройство, приме- ните команду arecord ‑l: arecord -l**** List of CAPTURE Hardware Devices ****card 1: Device [USB PnP Sound Device], device 0: USB Audio [USB Audio] Subdevices: 1/1 Subdevice #0: subdevice #0 Теперь вы можете проверить аудиозапись с помощью следующей ниже ко- манды: arecord -D plughw:1,0 -d 3 test.wav Параметр ‑D указывает устройство ввода звука, и здесь он означает, что это автоконфигурируемое (Plyg-and-Play) устройство с картой 1, устройством 0, как показано в выходных данных команды arecord ‑l. Параметр ‑D задает длительность записи в секундах. Для воспроизведения записанного звука на USB-динамике сначала необхо- димо создать файл с именем .asoundrc в домашнем каталоге со следующим содержимым: pcm.!default { type plug slave { pcm \"hw:0,0\" }\n--- Страница 357 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  355 } ctl.!default { type hw card 0 } Обратите внимание, что \"hw: 0,0\" соответствует информации «карта 0, устройство 0», возвращаемой командой aplay ‑l для USB-динамика. Теперь вы можете протестировать воспроизведение записанного звука на динамик, воспользовавшись командой aplay test.wav .  Иног да после перезагрузки материнской платы Pi номер карты для USB-динамика из- меняется системой автоматически, и вы не услышите звука при выполнении aplay test. wav. В этом случае вы можете снова выполнить команду aplay ‑l, для того чтобы найти новый номер карты, установленный для USB-динамика, и соответственно обновить файл ~/.asoundrc . Если вы хотите настроить громкость динамика, то используйте команду amixer set PCM ‑‑ 100%, где 100% – максимальная громкость. Для того чтобы загрузить драйвер фотокамеры, выполните команду sudo modprobe bcm2835 ‑v4l2. После этого, чтобы убедиться, что фотокамера обнаруже- на, выполните команду vcgencmd get_camera , которая должна вернуть supported=1 detected=1 . Для того чтобы загружать драйвер фотокамеры всякий раз, когда выполняется начальная загрузка материнской платы, что как раз нам и требу - ется, выполните команду sudo vi /etc/modules и добавьте строку bcm2835 ‑v4l2 в конец /etc/modules (или же вы просто можете выполнить команду sudo bash ‑c \"echo 'bcm2835 ‑v4l2' >> /etc/modules\" ). Мы протестируем фотокамеру в следу - ющем разделе, когда будем выполнять пример TensorFlow классифицирования изображений. Вот и все, что требуется для того, чтобы настроить компьютер Raspberry Pi для наших задач. Теперь давайте посмотрим, как заставить его двигаться. привеДение компьютера rasPberr Y Pi в Движение GoPiGo – это популярный комплект инструментов для превращения пла- ты Raspberry Pi в движущегося робота. После покупки и получения базового комплекта робота GoPiGo Robot Base Kit, как мы упоминали ранее, следуйте инструкциям, приводимым на веб-странице https://www.dexterindustries.com/ GoPiGo/get-started-with-the-gopigo3-raspberry-pi-robot/1-assemble-gopigo3 , для того чтобы собрать его вместе с платой Pi. Это должно занять около одного или двух часов, в зависимости от того, смотрите ли вы одновременно баскет - больный турнир серии March Madness или игры плей-офф NBA. После того как вы закончите, ваш робот Raspberry Pi вместе со всеми аксес - суарами, которые мы перечислили ранее, должен выглядеть следующим об-разом.\n--- Страница 358 ---\n356  Разработка приложений TensorFlow на компьютере Raspberry Pi Рисунок 12.1. Робот Raspberry Pi с комплектом GoPiGo и фотокамерой, USB-динамиком и USB-микрофоном Теперь воспользуйтесь источником питания Raspberry Pi для включения ро- бота Pi в сеть и после его загрузки подключитесь к нему с помощью команды ssh pi@<ip_вашей_платы_pi> . Для того чтобы установить библиотеку Python GoPiGo, которая позволит нам использовать Python’овский API GoPiGo (http://gopigo3. readthedocs.io/en/master/api-basic.html) для управления роботом, выполните следующую ниже команду, исполняющую сценарий оболочки. Данная команда создаст новый каталог /home/pi/Dexter и установит туда все файлы библиотеки и прошивки: sudo sh -c \"curl -kL dexterindustries.com/update_gopigo3 | bash\" Вы также должны перейти в каталог ~/Dexter и выполнить следующий ниже сценарий оболочки, чтобы обновить прошивку платы GoPiGo: bash GoPiGo3/Firmware/gopigo3_flash_firmware.sh Теперь выполните команду sudo reboot для перезагрузки платы, с тем чтобы изменения вступили в силу. После перезагрузки материнской платы Pi вы мо-жете протестировать движение GoPiGo и Raspberry Pi из интерактивной среды IPython, которую вы можете установить с помощью команды sudo pip install ipython . Для того чтобы протестировать базовый API Python GoPiGo, сначала запус - тите IPython, а затем введите следующий далее программный код построчно:  Не забудьте положить ваш робот GoPiGo Raspberry Pi на безопасную поверхность, так как он начнет двигаться. Во время финального теста вы должны использовать для пи-тания робота батарейки GoPiGo, чтобы он мог свободно двигаться. Но на этапе разра-ботки и в первоначальных тестах вы определенно должны использовать адаптер пита-ния с целью экономии заряда батареек, если, конечно, вы не используете аккумулятор. Всегда будьте осторожны, когда кладете своего робота на стол, так как он может упасть, в случае если вы выполните команду, заставляющую его сделать ошибочный ход.\n--- Страница 359 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  357 import easygopigo3 as easy gpg3_obj = easy.EasyGoPiGo3() gpg3_obj.drive_cm(5) gpg3_obj.drive_cm( ‑5) gpg3_obj.turn_degrees(30)gpg3_obj.turn_degrees( ‑30) gpg3_obj.stop() Команда drive_cm перемещает робота вперед или назад, в зависимости от по- ложительного или отрицательного значения параметра. Команда turn_degrees поворачивает робота по часовой стрелке или против часовой стрелки, в зави-симости от положительного или отрицательного значения параметра. Таким образом, приведенный выше пример программного кода перемещает робота вперед на 5 см, затем назад на 5 см, поворачивает по часовой стрелке на 30 гра-дусов, затем против часовой стрелки на 30 градусов. Эти вызовы по умолча-нию являются блокирующими вызовами, поэтому они не будут возвращаться из вызываемой функции, пока робот не закончит движение. Для того чтобы сделать неблокирующий вызов, добавьте параметр False , например: gpg3_obj.drive_cm(5, False)gpg3_obj.turn_degrees(30, False) Вы также можете использовать forward , backward и многие другие вызовы API, как описано в http://gopigo3.readthedocs.io/en/master/api-basic.html, для управления движением робота, но в этой главе мы будем применять только команды drive_cm и turn_degrees . Теперь мы готовы задействовать TensorFlow, чтобы добавить роботу больше интеллекта. настройка платформы Tensor Flow на компьютере rasPberr Y Pi Для того чтобы использовать платформу TensorFlow на языке Python, как мы поступим в разделах «Распознавание звука» и «Самообучение с подкреплени-ем», мы можем установить ночную сборку TensorFlow 1.6 для Pi c веб-сайта непрерывной интеграции TensorFlow (http://ci.tensorflow.org/view/Nightly/job/nightly-pi/223/artifact/output-artifacts) 1: sudo pip install http://ci.tensorflow.org/view/Nightly/job/nightly-pi/lastSuccessfulBuild/artifact/output-artifacts/tensorflow -1.6.0-cp27-none-any.whl 1 Веб-сайт непрерывной интеграции официально признан устаревшим, поэтому данная и следующая ссылки не являются рабочими. Информацию о ночных сбор- ках и их установке на компьютер Raspberry Pi можно подчерпнуть на веб-странице https: //www .tensorflow .org/install/. – Прим. перев.\n--- Страница 360 ---\n358  Разработка приложений TensorFlow на компьютере Raspberry Pi Этот метод более распространен и подробно описан в неплохом блог-посте Питера Уордена (Pete Warden) «Cross-compiling TensorFlow for the Raspberry Pi» («Перекрестная компиляция TensorFlow для Raspberry Pi») (https: //petewarden. com/2017/08/20/cross-compiling-tensorflow-for-the-raspberry-pi). Более сложный метод заключается в использовании файла makefile , не- обходимого для сборки и использования библиотеки TensorFlow. В разде-ле Raspberry Pi официальной документации TensorFlow по файлу makefile (https: //github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) содержатся подробные шаги сборки библиотеки TensorFlow, но они могут не работать с каждым выходом новой версии TensorFlow. Описанные там шаги отлично работают с более ранней версией TensorFlow (0.10), но вызывают мно-го ошибок \"undefined reference to google:: protobuf\" (неопределенная ссылка на google:: protobuf) с TensorFlow 1.6. Следующие далее шаги были протестированы с версией TensorFlow 1.6, ко- торую можно скачать по адресу https: //github.com/tensorflow/tensorflow/releases/ tag/v1.6.0; вы, конечно, можете попробовать более новую версию на веб-странице релизов TensorFlow или клонировать самый свежий исходный код TensorFlow командой git clone https://github.com/tensorflow/tensorflow , и ис - править любые возможные сбои. После выполнения команды cd для перехода в корневой каталог TensorFlow с исходным кодом выполните следующие ниже команды: tensorflow/contrib/makefile/download_dependencies.shsudo apt-get install -y autoconf automake libtool gcc -4.8 g++-4.8 cd tensorflow/contrib/makefile/downloads/protobuf/./autogen.sh./configuremake CXX=g++-4.8sudo make installsudo ldconfig # refresh shared library cachecd / / / / export HOST_NSYNC_LIB='tensorflow/contrib/makefile/compile_nsync.sh'export TARGET_NSYNC_LIB=\"$HOST_NSYNC_LIB\" Убедитесь, что вы выполняете команду make CXX=g++ ‑4.8 вместо просто ко- манды make , как описано в официальной документации TensorFlow по файлу Makefile, потому что файл сериализации Protobuf должен быть скомпилирован той же версией компилятора gcc, который используется для сборки следующей далее библиотеки TensorFlow. Это требуется для того, чтобы исправить упомя-нутые выше ошибки \"undefined reference to google:: protobuf\" . Теперь попро- буйте собрать библиотеку TensorFlow с помощью следующей команды: make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI \\ OPTFLAGS=\"-Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize\" CXX=g++-4.8 После нескольких часов сборки вы, скорее всего, получите ошибку типа «virtual memory exhausted: Cannot allocate memory» («виртуальная память ис -\n--- Страница 361 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  359 черпана: не удается выделить память»), либо материнская плата Pi просто за- виснет из-за нехватки памяти. Для того чтобы исправить это, нам нужно на-строить подкачку (своп), потому что без нее, когда у приложения закончится память, приложение будет убито из-за паники ядра. Существует два способа настройки подкачки: файл подкачки и раздел подкачки. В ОС Raspbian исполь-зуется файл подкачки по умолчанию размером 100 Мб на SD-карте, как пока-зано здесь, с применением команды free : pi@raspberrypi:~/tensorflow -1.6.0 $ free -h total used free shared buff/cache availableMem: 927M 45M 843M 660K 38M 838MSwap: 99M 74M 25M Для того чтобы увеличить размер файла подкачки до 1 Гб, измените файл /etc/dphys ‑swapfile посредством команды sudo vi /etc/dphysswapfile , заменив CONF_SWAPSIZE=100 на CONF_SWAPSIZE=1024 , затем перезагрузите службу файла под- качки: sudo /etc/init.d/dphys-swapfile stopsudo /etc/init.d/dphys-swapfile start После этого команда free ‑h покажет, что общий объем подкачки составляет 1.0 Гб. Раздел подкачки создается на отдельном диске USB и является предпочти- тельным вариантом, потому что раздел подкачки не может быть фрагментиро-ван, а файл подкачки на SD-карте может быть легко фрагментирован, вызывая замедление доступа. Для того чтобы настроить раздел подкачки, подключите USB-накопитель без данных, которые вам нужны, к материнской плате Pi, за-тем выполните команду sudo blkid, и вы увидите что-то вроде этого: /dev/sda1: LABEL=\"EFI\" UUID=\"67E3 ‑‑17ED\" TYPE=\"vfat\" PARTLABEL=\"EFI System Partition\" PARTUUID=\"622fddad ‑da3c‑4a09 ‑b6b3‑‑11233a2ca1f6\" /dev/sda2: UUID=\"E67F‑6EAB\" TYPE=\"vfat\" PARTLABEL=\"NO NAME\" PARTUUID=\"a045107a‑9e7f‑47c7 ‑ 9a4b‑7400d8d40f8c\" /dev/sda2 – э то раздел, который мы будем использовать в качестве раздела подкачки. Теперь размонтируйте и отформатируйте его как раздел подкачки: sudo umount /dev/sda2sudo mkswap /dev/sda2mkswap: /dev/sda2: warning: wiping old swap signature.Setting up swapspace version 1, size = 29.5 GiB (31671701504 bytes)no label, UUID=23443cde -9483-4ed7-b151-0e6899eba9de При выполнении команды mkswap вы увидите универсальный уникальный идентификатор UUID; выполните команду sudo vi /etc/fstab , добавьте следу - ющую строку в файл fstab со значением UUID: UUID=<значение UUID> none swap sw, pri=5 0 0\n--- Страница 362 ---\n360  Разработка приложений TensorFlow на компьютере Raspberry Pi Сохраните и закройте файл fstab , а затем выполните команду sudo swapon ‑a. Теперь если вы снова выполните команду free ‑h, то увидите, что общий объем подкачки близок размеру USB-накопителя. Нам определенно не нужен весь этот размер для подкачки – на самом деле рекомендуемый максимальный размер подкачки для материнской платы Raspberry Pi 3 с памятью 1 GB состав- ляет 2 GB, но мы оставим его как есть, потому что мы просто хотим успешно собрать библиотеку TensorFlow. При любом из изменений параметров подкачки мы можем перезапустить команду make: make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI \\ OPTFLAGS=\"-Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize\" CXX=g++-4.8 После ее завершения библиотека TensorFlow будет сгенерирована как tensorflow/contrib/makefile/gen/lib/libtensorflow ‑core.a , которая должна быть уже вам знакома, если вы изучили предыдущие главы, где мы собирали библио - теку TensorFlow вручную. Теперь мы можем создать пример классифицирова- ния изображений, используя данную библиотеку. распознавание изображений и речевое воспроизвеДение текста В папке tensorflow/contrib/pi_examples имеется два примера приложе- ния TensorFlow для компьютера Raspberry Pi (https: //github.com/tensorflow/ tensorflow/tree/master/tensorflow/contrib/pi_examples): label_image и camera . Мы внесем изменения в пример приложения camera, чтобы интегрировать рече-вое воспроизведение текста (TTS). Благодаря этому приложение сможет го-ворить о распознанных им изображениях при перемещении. Прежде чем мы создадим и протестируем два приложения, нам нужно установить некоторые библиотеки и загрузить готовый файл модели TensorFlow Inception: sudo apt-get install -y libjpeg-devsudo apt-get install libv4l-dev curl https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015_ stripped.zip -o /tmp/inception_dec_2015_stripped.zip cd ~/tensorflow -1.6.0 unzip /tmp/inception_dec_2015_stripped.zip -d tensorflow/contrib/pi_examples/label_image/ data/ Для того чтобы создать приложения label_image и camera , выполните коман- ды: make -f tensorflow/contrib/pi_examples/label_image/Makefilemake -f tensorflow/contrib/pi_examples/camera/Makefile\n--- Страница 363 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  361 При создании приложений может возникнуть следующая ошибка: ./tensorflow/core/platform/default/mutex.h:25:22: fatal error: nsync_cv.h: No such file or directory#include \"nsync_cv.h\"^compilation terminated. Для того чтобы ее исправить, выполните команду sudo cp tensorflow/contrib/ makefile/downloads/nsync/public/nsync*.h /usr/include . Затем отредактируйте файл tensorflow/contrib/pi_examples/label_image/ Makefile либо файл tensorflow/contrib/pi_examples/camera/Makefile , добавьте сле- дующие библиотеки и включите пути перед повторным выполнением коман-ды make : -L$(DOWNLOADSDIR)/nsync/builds/default.linux.c++11 \\-lnsync \\ Для того чтобы протестировать оба приложения, запустите их напрямую: tensorflow/contrib/pi_examples/label_image/gen/bin/label_imagetensorflow/contrib/pi_examples/camera/gen/bin/camera Взгляните на исходный код C++, tensorflow/contrib/pi_examples/label_image/ label_image.cc и tensorflow/contrib/pi_examples/camera/camera.cc , и вы увидите, что там используется аналогичный код C++, что и в наших приложениях для iOS в предыдущих главах, который загружает файл модельного графа, подго-тавливает входной тензор, запускает модель и получает выходной тензор. По умолчанию пример camera также использует предварительно постро- енную модель Inception, распакованную в папке label_image/data . Но в случае вашей собственной конкретной задачи классифицирования изображений вы можете предоставить свою собственную модель, вторично натренированную на основе трансферного обучения, как мы делали в главе 2 «Классифицирование изображений с помощью трансферного обучения», используя параметр ‑‑graph при запуске двух примеров приложений. В целом голос является основным пользовательским интерфейсом робо- та Raspberry Pi для взаимодействия с нами. В идеале, мы должны запустить модель речевого воспроизведения текста (Text-to-Speech, TTS) с естествен-ным звучанием, приводимую в движение платформой TensorFlow, такую как WaveNet (https://deepmind.com/blog/wavenet-generative-model-raw-audio) или Tacotron (https: //github.com/keithito/tacotron), но запуск и развертывание подоб- ной модели выходит за рамки этой главы. Оказывается, мы можем применить гораздо более простую библиотеку TTS под названием Flite, разработанную в Университете Карнеги-Мелон (http: //www.festvox.org/flite), которая предлага- ет речевое воспроизведение текста на довольно приличном уровне, и для ее установки требуется всего одна простая команда: sudo apt ‑get install flite . Если вы хотите установить последнюю версию библиотеки Flite, чтобы полу -\n--- Страница 364 ---\n362  Разработка приложений TensorFlow на компьютере Raspberry Pi чить лучшее качество TTS, просто скачайте последнюю версию Flite по ссылке и соберите ее. Для того чтобы протестировать библиотеку Flite с нашим USB-динамиком, запустите библиотеку с параметром ‑t и текстом в двойных кавычках, к при- меру flite ‑t \"i recommend the ATM machine\". Если вам не нравится голос, уста- новленный по умолчанию, то вы можете найти другие поддерживаемые голо-са, выполнив flite ‑lv, в результате чего должны быть показаны доступные голоса: kal awb_time kal16 awb rms slt . Затем вы можете указать голос, исполь- зуемый для речевого воспроизведения текста: flite ‑voice rms ‑t \"i recommend the ATM machine\" . Для того чтобы приложение camera озвучивало распознанные им объекты, то есть вело себя так, как от него требуется, когда робот Raspberry Pi переме-щается, вы можете использовать вот эту простую конвейерную команду pipe : tensorflow/contrib/pi_examples/camera/gen/bin/camera | xargs -n 1 flite -t Вы, вероятно, услышите слишком много голоса. С целью точной настройки результата озвучивания классификации изображений можно также изменить файл camera.cc и добавить в функцию PrintTopLabels следующий ниже про- граммный код перед сборкой примера с помощью команды make ‑f tensorflow/ contrib/pi_examples/camera/Makefile : std:: string cmd = \"flite ‑voice rms ‑t \\\"\"; cmd.append(labels[label_index]);cmd.append(\"\\\"\");system(cmd.c_str()); Теперь, когда мы завершили задачи классификации изображений и синтеза речи, без использования каких-либо облачных API из упомянутого ранее семи-нара «Как создать умного бота RasPi с помощью облачного API зрения и речи» с демонстрацией облачного API, давайте посмотрим, как можно организовать распознавание звука в компьютере Raspberry Pi, используя ту же модель, кото-рую мы применяли в главе 5 «Понимание простых речевых команд». распознавание звука и Движение робота Для того чтобы применить предварительно натренированную модель распо - знавания звука из руководства TensorFlow (https://www.tensorflow.org/tutorials/ audio_recognition) или его вторично натренированной модели, которую мы описали ранее, мы повторно воспользуемся сценарием Python listen.py из https: //gist.github.com/aallan и добавим вызовы API GoPiGo для управления движением робота, после того как он распознает четыре основные звуковые команды: «left», «right», «go» и «stop». Остальные шесть команд, поддержива-емых предварительно натренированной моделью, – «yes», «no», «up», «down», «on» и «off» – не очень хорошо применимы в нашем примере, и если вы хотите, то можете применить вторично натренированную модель, как показано в гла-\n--- Страница 365 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  363 ве 5 «Понимание простых речевых команд», для поддержки других голосовых команд, которые соответствуют вашей конкретной задаче. Прежде чем выполнить сценарий, сначала скачайте предварительно на- тренированную модель распознавания звука с http: //download.tensorflow.org/ models/speech_commands_v0.01.zip и распакуйте ее в папку /tmp , например, или же скопируйте модель командой scp , которую мы использовали в гла- ве 5 «Понимание простых речевых команд», в каталог /tmp материнской платы Raspberry Pi, а затем выполните: python listen.py --graph /tmp/conv_actions_frozen.pb --labels /tmp/conv_actions_labels.txt -I plughw:1,0 Или же вы можете выполнить сценарий так: python listen.py --graph /tmp/speech_commands_graph.pb --labels /tmp/conv_actions_labels.txt -I plughw:1,0 Обратите внимание, что значение plughw value 1,0 должно соответствовать номеру карты и номеру устройства вашего USB-микрофона, который можно найти с помощью команды arecord ‑l, показанной ранее. Сценарий listen.py также поддерживает множество других параметров. На- пример, мы можем применить параметр ‑‑detection_threshold 0.5 вместо по- рога обнаружения, который по умолчанию равен 0.8. Давайте теперь взглянем на то, как работает сценарий listen.py , перед тем как мы добавим вызовы API GoPiGo, чтобы заставить робота двигаться. В сце-нарии listen.py используются модуль Python subprocess и его класс Popen , который порождает новый процесс запуска команды arecord с соответству - ющими параметрами. Класс Popen имеет атрибут stdout , конкретизирующий стандартный дескриптор выходного файла выполняемой команды arecord , ко- торый может использоваться для чтения записанных аудиобайтов. Программный код Python для загрузки графа натренированной модели вы- глядит следующим образом: with tf.gfile.FastGFile(filename, 'rb') as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) tf.import_graph_def(graph_def, name='') Сеанс TensorFlow создается с помощью инструкции tf.session() , и после загрузки графа и создания сеанса записанный звуковой буфер отправляется вмес те с частотой дискретизации в качестве входных данных в метод run се- анса TensorFlow, который возвращает результат распознавания: run(softmax_tensor, { self.input_samples_name_: input_data, self.input_rate_name_: self.sample_rate_ })\n--- Страница 366 ---\n364  Разработка приложений TensorFlow на компьютере Raspberry Pi Здесь тензор softmax_tensor определяется как get_tensor_by_name(self. output_name_) графа TensorFlow, а output_name_, input_samples_name_ и input_rate_ name_ определяются соответственно как labels_softmax , decoded_sample_data:0 и decoded_sample_data:1 , то есть с теми же именами, которые мы использовали в приложениях для iOS и Android в главе 5 «Понимание простых речевых ко- манд». В предыдущих главах язык Python главным образом использовался для тре- нировки и тестирования модели TensorFlow перед запуском модели в iOS, где уже применялся C++, или в Android, где использовался интерфейсный код Java для нативной библиотеки TensorFlow на C++. Для запуска моделей TensorFlow на компьютере Raspberry Pi вы можете решить использовать Python’овский API TensorFlow непосредственно либо через API C++ (как в примерах label_image и camera ), хотя обычно вы бы все же предпочли выполнять тренировку модели на более мощном компьютере. Для получения полной документации по API Python TensorFlow обратитесь на https://www.tensorflow.org/api_docs/python . Для того чтобы применить Python’овский API GoPiGo для перемещения ро- бота на основе голосовой команды, сначала добавьте следующие две строки в сценарий listen.py : import easygopigo3 as gpggpg3_obj = gpg.EasyGoPiGo3() Затем добавьте следующий ниже программный код в конец метода def add_ data: if current_top_score > self.detection_threshold_ and time_since_last_top > self.suppression_ms_: self.previous_top_label_ = current_top_label self.previous_top_label_time_ = current_time_ms is_new_command = True logger.info(current_top_label) if current_top_label==\"go\": gpg3_obj.drive_cm(10, False) elif current_top_label==\"left\": gpg3_obj.turn_degrees( ‑30, False) elif current_top_label==\"right\": gpg3_obj.turn_degrees(30, False) elif current_top_label==\"stop\": gpg3_obj.stop() Теперь положите вашего робота Raspberry Pi на пол, подключитесь к нему с помощью команды ssh с вашего компьютера и выполните следующий ниже сценарий: python listen.py --graph /tmp/conv_actions_frozen.pb --labels /tmp/conv_actions_labels.txt -I plughw:1,0 --detection_threshold 0.5\n--- Страница 367 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  365 Вы увидите результат, как показано ниже: INFO: audio: started recording INFO: audio:_silence_INFO: audio:_silence_ Затем вы можете произнести «left», «right», «stop», «go» и «stop», и увидите, что команды были распознаны и робот движется согласно им: INFO: audio: leftINFO: audio:_silence_INFO: audio:_silence_INFO: audio: rightINFO: audio:_silence_INFO: audio: stopINFO: audio:_silence_INFO: audio: goINFO: audio: stop Вы можете запустить приложение camera в отдельном терминале, и тогда одновременно с перемещением на основе ваших голосовых команд он будет распознавать новые изображения, которые он увидит, и будет говорить о ре-зультатах распознавания. Вот и все, что нужно для создания базового робо-та Raspberry Pi, который слушает, двигается, видит и говорит – как раз то, что делает демонстрационный пример на Google I/O 2016, но без использования облачных API. Это далеко не фантастический робот, который может понимать естественную человеческую речь, участвовать в увлекательных разговорах или выполнять полезные и нетривиальные задачи. Но если вы будете приводить его в действие с помощью предварительно натренированных, вторично на-тренированных или других мощных моделей TensorFlow, используя при этом всевозможные датчики, то вы, безусловно, сможете наращивать робота Pi, ко-торого мы создали, все большим и большим интеллектом и физической силой. После того как мы увидели, как запускаются предварительно натрениро- ванные и вторично натренированные модели TensorFlow на компьютере Pi, в следующем разделе мы покажем вам, как добавлять в робот мощную мо-дель, обучающуюся на основе максимизации подкрепления, построенную и натренированную с помощью TensorFlow. В конце концов, самообучение с подкреплением методом проб и ошибок и сама его природа взаимодействия с окружающей средой, в основе которого лежит максимизация получаемого вознаграждения, делают самообучение с подкреплением очень подходящим методом машинного обучения для роботов.\n--- Страница 368 ---\n366  Разработка приложений TensorFlow на компьютере Raspberry Pi самообучение с поДкреплением на компьютере rasPberr Y Pi «Тренажерный зал» OpenAI Gym (https: //gym.openai.com) представляет собой набор инструментов Python с открытым исходным кодом, который предлагает множество симулируемых сред, помогающих вам в разработке, сопоставле-нии и тренировке алгоритмов, обучающихся с подкреплением, поэтому вам не нужно покупать все эти датчики и тренировать своего робота в реальной среде, что может оказаться дорогостоящим как по времени, так и по деньгам. В этом разделе мы покажем вам, как разрабатывать, сравнивать и трениро-вать различные модели, которые обучаются с подкреплением, на компьютере Raspberry Pi с использованием TensorFlow в симулируемой среде OpenAI Gym под названием CartPole (https: //gym.openai.com/envs/CartPole-v0). Для того чтобы установить библиотеку OpenAI Gym, выполните следующие команды: git clone https://github.com/openai/gym.gitcd gymsudo pip install -e. Вы можете перепроверить, что у вас установлена платформа TensorFlow 1.6 и библиотека gym, выполнив команду pip list (в последней части раздела «На- стройка TensorFlow на компьютере Raspberry Pi» описано, как устанавливать платформу TensorFlow 1.6): pi@raspberrypi:~ $ pip listgym (0.10.4, /home/pi/gym)tensorflow (1.6.0) Либо вы можете запустить IPython, а затем импортировать TensorFlow и gym: pi@raspberrypi:~ $ ipythonPython 2.7.9 (default, Sep 17 2016, 20:26:04) IPython 5.5.0--An enhanced Interactive Python. In [1]: import tensorflow as tfIn [2]: import gymIn [3]: tf.__version__ Out[3]: '1.6.0' In [4]: gym.__version__ Out[4]: '0.10.4' Теперь все готово к использованию платформы TensorFlow и набора инстру - ментов gym, которые позволят нам построить интересную модель, обучающу - юся с подкреплением и работающую на компьютере Raspberry Pi.\n--- Страница 369 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  367 описание симулируемой среДы carTPole CartPole (задача стабилизации перевернутого маятника)1 – э то среда, которую можно использовать для того, чтобы натренировать робота оставаться в рав- новесии, к примеру если он несет предмет и хочет положить его в движении. Из-за объема этой главы мы будем строить модели, которые работают только в симулируемой среде CartPole, но эта модель и то, как она построена и натре-нирована, безусловно, может быть применена к реальной физической среде, аналогичной CartPole. В симулируемой среде CartPole стержень прикреплен к тележке, которая двигается горизонтально вдоль трека. Вы можете приложить к тележке дей-ствие 1 (ускорение вправо) или 0 (ускорение влево). В самом начале стержень находится в вертикальном положении, и цель – не дать ему упасть. Вознаграж - дение в размере 1 предоставляется за каждый шаг времени, когда стержень остается в вертикальном положении. Эпизод заканчивается, когда стержень отклоняется более чем на 15 градусов от вертикали или тележка перемещается более чем на 2,4 единицы от центра. На рис. 12.2 приведено изображение по-зиций тележки и положений стержня в симулируемой среде CartPole. Рисунок 12.2. Разные позиции тележки и положения стержня в симулируемой среде CartPole (задача стабилизации перевернутого маятника) Теперь давайте поиграем с симулируемой средой CartPole. Сначала создайте новую среду и выясните, какие действия агент может предпринимать в этой среде: env = gym.make(\"CartPole ‑v0\") env.action_space# Discrete(2)env.action_space.sample()# 0 or 1 1 Инт уитивно понятное объяснение симулируемой среды CartPole с gif-анимацией вы найдете в статье, приведенной по ссылке https://habr .com/company/newprolab/ blog/343834/. – Прим. перев.\n--- Страница 370 ---\n368  Разработка приложений TensorFlow на компьютере Raspberry Pi Каждое наблюдение (состояние) состоит из четырех значений тележки: го- ризонтальная позиция, скорость, угол стержня и угловая скорость: obs=env.reset() obs# array([0.04052535, 0.00829587, ‑0.03525301, ‑0.00400378]) Каждый шаг (действие) в среде приводит к новому наблюдению, возна- граждению за выполненное действие, был ли эпизод выполнен (если это так, то дальнейшие шаги предпринять невозможно) и некоторой дополнительной информации: obs, reward, done, info = env.step(1)obs# array([0.04069127, 0.2039052, ‑0.03533309, ‑0.30759772]) Напомним, что действие (или шаг) 1 означает движение вправо, а действие 0 – влево. Для того чтобы увидеть, как долго может длиться эпизод, когда вы продолжаете перемещать тележку вправо, выполните в цикле следующее: while not done: obs, reward, done, info = env.step(1) print(obs) #[0.08048328 0.98696604 ‑0.09655727 ‑1.54009127] #[0.1002226 1.18310769 ‑0.12735909 ‑1.86127705] #[0.12388476 1.37937549 ‑0.16458463 ‑2.19063676] #[0.15147227 1.5756628 ‑0.20839737 ‑2.52925864] #[0.18298552 1.77178219 ‑0.25898254 ‑2.87789912] Давайте теперь вручную пройдемся по серии действий от начала до кон- ца и распечатаем первое значение наблюдения (горизонтальная позиция) и треть е значение (угол стержня в градусах от вертикали), поскольку эти два значения определяют, выполнен эпизод или нет. Прежде всего надо обнулить среду и ускорить тележку вправо несколько раз подряд: import numpy as np obs=env.reset() obs[0], obs[2]*360/np.pi# (0.008710582898326602, 1.4858315848689436) obs, reward, done, info = env.step(1) obs[0], obs[2]*360/np.pi# (0.009525842685697472, 1.5936049816642313) obs, reward, done, info = env.step(1) obs[0], obs[2]*360/np.pi# (0.014239775393474322, 1.040038643681757)\n--- Страница 371 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  369 obs, reward, done, info = env.step(1) obs[0], obs[2]*360/np.pi# (0.0228521194217381, ‑0.17418034908781568) Вы видите, что значение позиции тележки все увеличивается, когда она перемещается вправо, а вертикальный градус стержня все уменьшается, и по-следний шаг показывает отрицательный градус. Это означает, что стержень движется в левую сторону от центра. Все это станет понятным, стоит только на-рисовать в вашем сознании небольшую яркую картину вашей любимой собаки, толкающей тележку со стержнем. Теперь измените действие, ускорив тележку влево (0) несколько раз подряд: obs, reward, done, info = env.step(0)obs[0], obs[2]*360/np.pi# (0.03536432554326476, ‑2.0525933052704954) obs, reward, done, info = env.step(0)obs[0], obs[2]*360/np.pi# (0.04397450935915654, ‑3.261322987287562) obs, reward, done, info = env.step(0)obs[0], obs[2]*360/np.pi# (0.04868738508385764, ‑3.812330822419413) obs, reward, done, info = env.step(0)obs[0], obs[2]*360/np.pi# (0.04950617929263011, ‑3.7134404042580687) obs, reward, done, info = env.step(0)obs[0], obs[2]*360/np.pi# (0.04643238384389254, ‑2.968245724428785) obs, reward, done, info = env.step(0)obs[0], obs[2]*360/np.pi# (0.039465670006712444, ‑1.5760901885345346) Сначала вы, возможно, удивитесь, увидев, что действие 0 приводит к тому, что позиции ( obs[0] ) продолжают несколько раз увеличиваться, однако на- помним, что тележка движется со скоростью, и одно или несколько действий по перемещению тележки в другом направлении не уменьшат значения по-зиции сразу. Но если вы продолжите двигать тележку влево, то увидите, как показано на последних двух предыдущих шагах, что позиция тележки начи-нает уменьшаться (по направлению влево). Теперь продолжите действие 0, и вы увидите, что позиция становится все меньше и меньше, с отрицательным значением, означающим, что тележка входит в левую часть от центра, а угол стержня становится все больше:\n--- Страница 372 ---\n370  Разработка приложений TensorFlow на компьютере Raspberry Pi obs, reward, done, info = env.step(0) obs[0], obs[2]*360/np.pi# (0.028603948219811447, 0.46789197320636305) obs, reward, done, info = env.step(0) obs[0], obs[2]*360/np.pi# (0.013843572459953138, 3.1726728882727504) obs, reward, done, info = env.step(0) obs[0], obs[2]*360/np.pi# ( ‑0.00482029774222077, 6.551160678086707) obs, reward, done, info = env.step(0)obs[0], obs[2]*360/np.pi# ( ‑0.02739315127299434, 10.619948631208114) Как мы уже говорили ранее, симулируемая среда CartPole определена таким образом, что эпизод «заканчивается, когда стержень отклоняется более чем на 15 градусов от вертикали», поэтому давайте сделаем еще несколько ходов и на этот раз также распечатаем значение переменной done1: obs, reward, done, info = env.step(0)obs[0], obs[2]*360/np.pi, done# ( ‑0.053880356973985064, 15.39896478042983, False) obs, reward, done, info = env.step(0)obs[0], obs[2]*360/np.pi, done# ( ‑0.08428612474261402, 20.9109976051126, False) obs, reward, done, info = env.step(0)obs[0], obs[2]*360/np.pi, done# ( ‑0.11861214326416822, 27.181070460526062, True) obs, reward, done, info = env.step(0)# WARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' ‑‑any further steps are undefined behavior. Существует некоторая задержка, когда среда решает, что она должна вернуть True для переменной done , – х отя первые два шага уже возвращают градус, кото- рый больше 15 (напомним, эпизод заканчивается, когда стержень отклоняется более чем на 15 градусов от вертикали), вы все равно еще можете предпринять действие 0 в среде. Третий шаг возвращает done как True , и еще один (последний) шаг в среде приведет к предупреждению, поскольку среда уже завершила эпизод. 1 Перево д предупреждения WARN : вы вызываете 'step ()' , даже если эта среда уже вер- нула done = True . Вы всегда должны вызывать 'reset()' после получения 'done = True' – дальнейшие действия не определены. – Прим. перев.\n--- Страница 373 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  371 Для симулируемой среды CartPole значение вознаграждения, возвращаемое при каждом вызове шага, всегда равняется 1, а информация всегда {}. Вот и все, что вам надо знать о симулируемой среде CartPole. Теперь, когда мы пони-маем, как работает среда CartPole, давайте посмотрим, какие линии поведе-ния (политики) мы можем придумать, чтобы в каждом состоянии (наблюде-нии) мы могли давать линии поведения сообщать нам, какое действие (шаг) предпринять, для того чтобы мы могли удерживать стержень в вертикальном положении как можно дольше, другими словами, чтобы мы могли максими-зировать наши вознаграждения. Тут стоит отметить, что линия поведения в самообучении с подкреплением – это просто функция, которая в качестве входных данных принимает состояние, в котором агент находится, и выводит действие, которое агент должен предпринять дальше, с целью максимизации стоимостей или долгосрочных вознаграждений. начало с простой интуитивно понятной линии повеДения Совершенно очевидно, если всякий раз предпринимать одинаковое действие (все нули или единицы), то стержень не сможет удерживаться в вертикальном положении слишком долго. Для того чтобы сравнить с базовым вариантом, вы-полните следующий ниже программный код, и вы увидите средние вознаграж - дения за 1000 эпизодов, когда во время каждого эпизода применяется одно и то же действие: # single_minded_policy.py (однобокая линия поведения) import gym import numpy as np env = gym.make(\"CartPole ‑v0\") total_rewards = [] for _ in range(1000): rewards = 0 obs = env.reset() action = env.action_space.sample() while True: obs, reward, done, info = env.step(action) rewards += reward if done: break total_rewards.append(rewards) print(np.mean(total_rewards))# 9.36\n--- Страница 374 ---\n372  Разработка приложений TensorFlow на компьютере Raspberry Pi Так, среднее вознаграждение за все 1000 эпизодов составляет около 10. Об- ратите внимание, что инструкция env.action_space.sample() выполняет выбор- ку действия со значением 0 или 1. Это то же самое, если делать случайную вы- борку из 0 или 1. Вы можете в этом убедиться, вычислив сумму np.sum([env. action_space.sample() for _ in range(10000)]) , в результате чего получите значе- ние, близкое к 5000. Для того чтобы увидеть, как может работать другая линия поведения, лучше давайте применим простую и интуитивно-разумную линию поведения, кото-рая принимает действие 1 (переместить тележку вправо), когда градус стерж - ня – положительный (справа от вертикали), и действие 0 (переместить тележку влево), когда градус стержня – отрицательный (слева от вертикали). Эта линия поведения имеет смысл, поскольку это, вероятно, именно то, что мы будем де-лать, для того чтобы сохранить равновесие стержня тележки как можно дольше: # simple_policy.py (простая линия поведения) import gym import numpy as np env = gym.make(\"CartPole ‑v0\") total_rewards = [] for _ in range(1000): rewards = 0 obs = env.reset() while True: action = 1 if obs[2] > 0 else 0 obs, reward, done, info = env.step(action) rewards += reward if done: break total_rewards.append(rewards) print(np.mean(total_rewards))# 42.19 Среднее вознаграждение за 1000 эпизодов теперь составляет 42, что являет - ся неплохим улучшением, по сравнению с 9.36. Давайте теперь посмотрим, сможем ли мы разработать еще более совершен- ную, еще более изощренную линию поведения. Напомним, что линия пове-дения – это просто функция, или отображение из состояния в действие. Одна из вещей, которую мы узнали с момента возрождения нейронных сетей за по-следние несколько лет, заключается в том, что если не понятно, как определять сложные функции, такие как линия поведения в самообучении с подкреплени-ем, то следует подумать о нейронной сети, которая является универсальным аппроксиматором функций (обратитесь к разделу «A visual proof that neural\n--- Страница 375 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  373 nets can compute any function» («Наглядное доказательство того, что нейрон- ные сети могут вычислить любую функцию») Майкла Нельсона (Michael Nelson) на веб-странице http://neuralnetworksanddeeplearning.com/chap4.html).  В предыдущей главе мы рассмотрели программы AlphaGo и AlphaZero. Стоить заме- тить, что Джим Флеминг (Jim Fleming) написал интересный блог-пост под названием «Before AlphaGo there was TD-Gammon» («До программы AlphaGo существовала про-грамма TD-Gammon») (https: //medium.com/jim-fleming/before-alphago-there-was-td- gammon -13deff866197), где говорится о программе, которая была первым приложени- ем самообучения на основе максимизации вознаграждения. Она тренировала саму себя, используя нейронную сеть в качестве оценочной функции, чтобы победить чемпионов по нардам. В указанном блог-посте и в книге «Reinforcement Learning: An Introduction» («Обучение с подкреплением: введение») Саттона и Барто имеются подробные описа-ния программы TD-Gammon; а если же вы хотите узнать побольше об использовании нейронных сетей в качестве мощной универсальной функции, то вы также можете по-гуглить «Temporal Difference Learning and TD-Gammon» («Временное разностное обуче-ние и TD-Gammon») и скачать оригинальную статью. использование нейронных сетей Для построения более оптимальной линии повеДения Давайте сначала посмотрим, как создать случайную линию поведения с исполь-зованием простой полносвязной (плотной) нейронной сети, которая на входе принимает 4 значения в наблюдении, использует скрытый слой из 4 нейронов и выводит вероятность действия 0, на основе которого агент может попробо-вать предпринять следующее действие из 0 и 1: # nn_random_policy.py import tensorflow as tf import numpy as npimport gym env = gym.make(\"CartPole ‑v0\") num_inputs = env.observation_space.shape[0] inputs = tf.placeholder(tf.float32, shape=[None, num_inputs])hidden = tf.layers.dense(inputs, 4, activation=tf.nn.relu)outputs = tf.layers.dense(hidden, 1, activation=tf.nn.sigmoid)action = tf.multinomial(tf.log(tf.concat([outputs, 1 ‑outputs], 1)), 1) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) total_rewards = [] for _ in range(1000):\n--- Страница 376 ---\n374  Разработка приложений TensorFlow на компьютере Raspberry Pi rewards = 0 obs = env.reset() while True: a = sess.run(action, feed_dict={inputs: obs.reshape(1, num_inputs)}) obs, reward, done, info = env.step(a[0][0]) rewards += reward if done: break total_rewards.append(rewards) print(np.mean(total_rewards)) Обратите внимание, что мы используем функцию tf.multinomial для выбор- ки действия, опираясь на вероятностное распределение действия между 0 и 1, определенного соответственно как outputs и 1‑outputs (сумма двух вероятно- стей равна 1). Среднее значение общего количества вознаграждений составит порядка 20, что лучше, чем однобокая линия поведения, но хуже, чем простая интуитивная линия поведения из предыдущего подраздела. Эта случайная ли-ния поведения генерируется нейронной сетью без какой-либо предваритель-ной тренировки. Для того чтобы натренировать сеть, мы применяем функцию сигмоидаль- ной перекрестной энтропии tf.nn.sigmoid_cross_entropy_with_logits , которая определяет функцию потери между выходными данными сети и требуемым действием y_target , определенным с помощью базовой простой линии пове- дения из предыдущего подраздела, поэтому мы ожидаем, что эта нейросетевая линия поведения достигнет примерно тех же преимуществ, что и базовая ли-ния поведения, не связанная с нейронной сетью: # nn_simple_policy.py import tensorflow as tf import numpy as npimport gym env = gym.make(\"CartPole ‑v0\") num_inputs = env.observation_space.shape[0] inputs = tf.placeholder(tf.float32, shape=[None, num_inputs])y = tf.placeholder(tf.float32, shape=[None, 1])hidden = tf.layers.dense(inputs, 4, activation=tf.nn.relu)logits = tf.layers.dense(hidden, 1)outputs = tf.nn.sigmoid(logits)action = tf.multinomial(tf.log(tf.concat([outputs, 1 ‑outputs], 1)), 1) cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)optimizer = tf.train.AdamOptimizer(0.01)training_op = optimizer.minimize(cross_entropy)\n--- Страница 377 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  375 with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for _ in range(1000): obs = env.reset() while True: y_target = np.array([[1. if obs[2] < 0 else 0.]]) a, _ = sess.run([action, training_op], feed_dict={inputs: obs.reshape(1, num_inputs), y: y_target}) obs, reward, done, info = env.step(a[0][0]) if done: break print(\"training done\") Мы определяем выходы outputs как функцию sigmoid от логитов ( outputs = tf.nn.sigmoid(logits) )1, то есть как вероятность действия 0, а затем применяем tf.multinomial для выборки действия. Обратите внимание, что для трениров- ки сети мы используем стандартный оптимизатор tf.train.AdamOptimizer и его метод minimize . Для проверки результативности линии поведения выполните следующий ниже программный код: total_rewards = [] for _ in range(1000): rewards = 0 obs = env.reset() while True: y_target = np.array([1. if obs[2] < 0 else 0.]) a = sess.run(action, feed_dict={inputs: obs.reshape(1, num_inputs)}) obs, reward, done, info = env.step(a[0][0]) rewards += reward if done: break total_rewards.append(rewards) print(np.mean(total_rewards)) Среднее значение общего количества вознаграждений составит порядка 40, примерно столько же, что и при использовании простой линии поведения без нейронной сети, а это именно то, чего мы и ожидали, поскольку для трениров-ки сети мы специально использовали простую линию поведения, с y: y_target в тренировочной фазе. Теперь все готово к тому, чтобы разведать способ реализации метода взятия градиента линии поведения, опираясь на нее, который радикально улучшит 1 Д ля справки: логит (logit) – это ненормализованный логарифм вероятности. – Прим. перев.\n--- Страница 378 ---\n376  Разработка приложений TensorFlow на компьютере Raspberry Pi работу нашей нейронной сети, принося вознаграждения, которые будут в не- сколько раз крупнее. Основная идея градиента линии поведения заключается в том, что, для того чтобы натренировать нейронную сеть генерировать более оптимальную ли-нию поведения, когда единственным знанием, которое агент получает от окру - жающей среды, являются вознаграждения, получаемые при выполнении дей-ствия, находясь в том или ином конкретном состоянии (то есть для тренировки мы не можем использовать контролируемое обучение), мы можем принять два новых механизма: диск онтированные вознаграждения: в стоимости каждого действия должны учитываться будущие вознаграждения за действие. Например, действие, которое получает немедленное вознаграждение, 1, но закан-чивает эпизод через два действия (шага), должно иметь меньше долго-срочных вознаграждений, чем действие, которое получает немедленное вознаграждение, 1, но заканчивает эпизод на 10 шагов позже. Типичная формула дисконтированных вознаграждений за действие – это сумма его немедленного вознаграждения и произведения каждого из его буду - щих вознаграждений на ставку дисконтирования, возведенную в степень по количеству шагов в будущее. Таким образом, если последовательность действий до конца эпизода имеет награды 1, 1, 1, 1, 1, то дисконтированные награды за первое действие составляют 1+(1*ставка_дисконта)+(1*ставка_дисконта**2)+(1*ставка_дисконта**3)+(1*ставка_дисконта**4) 1; выпо лнить тестовый прогон текущей линии поведения и посмотреть, ка- кие действия ведут к увеличению дисконтированных вознаграждений, затем обновить градиенты текущей линии поведения (потери весов) с помощью дисконтированных вознаграждений таким образом, чтобы действие с более высокими дисконтированными вознаграждениями пос ле обновления сети имело более высокую вероятность быть выбран- ным в следующий раз. Повторить такие тестовые прогоны и обновлять процесс многократно, для того чтобы натренировать нейронную сеть бо-лее оптимальной линии поведения. Для получения более подробного изложения данной темы и пошагового руководства по градиентам линии поведения обратитесь к блог-посту Анд - рея Карпати «Deep Reinforcement Learning: Pong from Pixels» («Глубокое обуче - ние с подкреплением: игра в пинг-понг из пикселов» (http://karpathy.github. io/2016/05/31/rl)). Давайте теперь посмотрим, как реализовать в платформе TensorFlow градиент линии поведения для нашей задачи о симулируемой сре-де CartPole со стабилизацией перевернутого маятника. Сначала импортируйте библиотеки Python tensorflow , numpy и gym и опре- делите вспомогательный метод, который вычисляет нормализованные и дис - контированные вознаграждения: 1 См. https: //ru.w ikipedia.org/wiki/Дисконтированная_стоимость. – Прим. перев.\n--- Страница 379 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  377 import tensorflow as tf import numpy as npimport gym def normalized_discounted_rewards(rewards): dr = np.zeros(len(rewards)) dr[‑1] = rewards[ ‑1] for n in range(2, len(rewards)+1): dr[‑n] = rewards[ ‑n] + dr[ ‑n+1] * discount_rate return (dr ‑‑dr.mean()) / dr.std() Например, если ставка дисконта discount_rate составляет 0,95, то дисконти- рованное вознаграждение за первое действие в списке вознаграждений [1,1,1] равняется 1 + 1*0.95 + 1*0.95**2 = 2.8525, а дисконтированные вознаграждения за второй и последний элементы равняются 1.95 и 1; дисконтированное возна-граждение за первое действие в списке вознаграждений [1,1,1,1,1] равняется 1 + 1*0.95 + 1*0.95**2 + 1*0.95**3 + 1*0.95**4 = 4.5244, а для остальных действий 3.7099, 2.8525, 1.95 и 1. Нормализованные дисконтированные вознаграждения для списков [1,1,1] и [1,1,1,1,1] составляют [1.2141, 0.0209, –1.2350] и [1.3777, 0.7242, 0.0362, –0.6879, –1.4502]. Каждый нормализованный дисконтирован-ный список находится в убывающем порядке, то есть чем дольше длится дей-ствие (до конца эпизода), тем больше его награда. Далее, создайте симулируемую среду CartPole библиотеки gym, опреде- лите гиперпараметры скорости заучивания learning_rate и ставки дисконта discount_rate и постройте сеть с четырьмя входными нейронами, четырьмя скрытыми нейронами и одним выходным нейроном, как и раньше: env = gym.make(\"CartPole ‑v0\") learning_rate = 0.05discount_rate = 0.95 num_inputs = env.observation_space.shape[0] inputs = tf.placeholder(tf.float32, shape=[None, num_inputs])hidden = tf.layers.dense(inputs, 4, activation=tf.nn.relu)logits = tf.layers.dense(hidden, 1)outputs = tf.nn.sigmoid(logits)action = tf.multinomial(tf.log(tf.concat([outputs, 1 ‑outputs], 1)), 1) prob_action_0 = tf.to_float(1 ‑action) cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=prob_action_0)optimizer = tf.train.AdamOptimizer(learning_rate) Обратите внимание, что мы больше не используем функцию minimize , как делали это в предыдущем примере простой нейронной сети, потому что нам необходимо настроить градиенты вручную, для того чтобы учесть дисконти-\n--- Страница 380 ---\n378  Разработка приложений TensorFlow на компьютере Raspberry Pi рованные вознаграждения за каждое действие. Для этого нам необходимо сна- чала применить метод compute_gradients , затем обновить градиенты, как мы хотим, и, наконец, вызвать метод apply_gradients (метод minimize , который мы должны использовать большую часть времени, фактически за кадром вызы-вает методы compute_gradients и apply_gradients – смо трите https: //github.com/ tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py для по-лучения дополнительной информации). Итак, давайте теперь вычислим градиенты перекрестно-энтропийной поте- ри для параметров сети (весов и смещений) и настроим градиентные заполни-тели, которые позже будут заполняться значениями, учитывающими как вы-численные градиенты, так и дисконтированные вознаграждения за действия, выполняемые с использованием текущей линии поведения во время тестового прогона: gvs = optimizer.compute_gradients(cross_entropy)gvs = [(g, v) for g, v in gvs if g!= None]gs = [g for g, _ in gvs] gps = [] gvs_feed = []for g, v in gvs: gp = tf.placeholder(tf.float32, shape=g.get_shape()) gps.append(gp) gvs_feed.append((gp, v)) training_op = optimizer.apply_gradients(gvs_feed) Возвращенное из инструкции optimizer.compute_gradients(cross_entropy) зна- чение gvs представляет собой список кортежей, и каждый кортеж состоит из градиента (перекрестной энтропии cross_entropy для тренируемой пере- менной) и тренируемой переменной. Например, если вы посмотрите на пере-менную gvs после выполнения всей программы, то увидите что-то вроде этого: [(<tf.Tensor 'gradients/dense/MatMul_grad/tuple/control_dependency_1:0' shape=(4, 4) dtype=float32>, <tf.Variable 'dense/kernel:0' shape=(4, 4) dtype=float32_ref>), (<tf.Tensor 'gradients/dense/BiasAdd_grad/tuple/control_dependency_1:0' shape=(4,) dtype=float32>, <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32_ref>), (<tf.Tensor 'gradients/dense_2/MatMul_grad/tuple/control_dependency_1:0' shape=(4, 1) dtype=float32>, <tf.Variable 'dense_1/kernel:0' shape=(4, 1) dtype=float32_ref>), (<tf.Tensor 'gradients/dense_2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32_ref>)] Обратите внимание, что ядро (kernel) – это просто другое имя для веса и (4, 4), (4,), (4, 1) и (1,) – это формы весов и смещений для первого (из входно- го в скрытый) и второго (из скрытого в выходной) слоев. Если вы выполните\n--- Страница 381 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  379 данный сценарий несколько раз в IPython, то по умолчанию граф объекта tf будет содержать тренируемые переменные из предыдущих прогонов, поэто- му если вы не вызовете tf.reset_default_graph() , то для удаления устаревших тренируемых переменных, которые не возвращают градиентов, вы должны применить инструкцию gvs = [(g, v) for g, v in gvs if g!= None] (допол- нительные сведения о методе computer_gradients можно получить в разделе compute_gradients по ссылке https://www.tensorflow.org/api_docs/python/tf/train/ AdamOptimizer#compute_gradients). Теперь сыграйте в несколько игр и сохраните вознаграждения и значения градиента: with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for _ in range(1000): rewards, grads = [], [] obs = env.reset() # использование текущей линии поведения для тестирования игры while True: a, gs_val = sess.run([action, gs], feed_dict={inputs: obs.reshape(1, num_inputs)}) obs, reward, done, info = env.step(a[0][0]) rewards.append(reward) grads.append(gs_val) if done: break После тестовой игры обновите градиенты дисконтированными возна- граждениями и натренируйте сеть (напомним, что тренировочная операция training_op определена как optimizer.apply_gradients(gvs_feed) ): # обновить градиенты и выполнить тренировку nd_rewards = normalized_discounted_rewards(rewards)gp_val = {}for i, gp in enumerate(gps): gp_val[gp] = np.mean([grads[k][i] * reward for k, reward in enumerate(nd_rewards)], axis=0) sess.run(training_op, feed_dict=gp_val) Наконец, после 1000 итераций тестовой игры и обновлений мы можем про- тестировать натренированную модель: total_rewards = [] for _ in range(100): rewards = 0 obs = env.reset() while True:\n--- Страница 382 ---\n380  Разработка приложений TensorFlow на компьютере Raspberry Pi a = sess.run(action, feed_dict={inputs: obs.reshape(1, num_inputs)}) obs, reward, done, info = env.step(a[0][0]) rewards += reward if done: break total_rewards.append(rewards) print(np.mean(total_rewards)) Обратите внимание, что теперь мы используем натренированную сеть ли- нии поведения и применяем sess.run , для того чтобы получить следующее дей- ствие с текущим наблюдением в качестве входных данных. Выходное среднее значение общего количества вознаграждений составит около 200, что является огромным улучшением, по сравнению с нашей простой интуитивной линией поведения, с использованием или без использования нейронной сети. Кроме того, натренированную модель после тренировки можно сохранить с помощью объекта tf.train.Saver , как мы делали неоднократно в предыдущих главах: saver = tf.train.Saver()saver.save(sess, \"./nnpg.ckpt\") Затем ее можно перезагрузить в отдельной тестовой программе с помощью следующей инструкции: with tf.Session() as sess: saver.restore(sess, \"./nnpg.ckpt\") Все предыдущие реализации линии поведения выполняются на компьютере Raspberry Pi, даже та, которая использует платформу TensorFlow для трениров-ки модели градиента линии поведения на основе самообучения с подкрепле-нием, которая занимает около 15 минут до полного завершения. Вот суммарное количество вознаграждений, возвращенных после выполнения каждой линии поведения, которую мы рассмотрели, на компьютере Raspberry Pi: pi@raspberrypi:~/mobiletf/ch12 $ python single_minded_policy.py 9.362 pi@raspberrypi:~/mobiletf/ch12 $ python simple_policy.py 42.535pi@raspberrypi:~/mobiletf/ch12 $ python nn_random_policy.py 21.182pi@raspberrypi:~/mobiletf/ch12 $ python nn_simple_policy.py 41.852pi@raspberrypi:~/mobiletf/ch12 $ python nn_pg.py 199.116\n--- Страница 383 ---\nРазработка приложений TensorFlow на компьютере Raspberry Pi  381 Теперь у вас есть мощная нейросетевая модель линии поведения, которая может помочь вашему роботу сохранять равновесие. Данная модель всесто- ронне протестирована в симулированной среде, и вы можете использовать ее в реальной физической среде, разумеется, после замены данных, возвра-щаемых API симулируемой среды, данными реальной информационной сре-ды – но программный код для построения и тренировки нейросетевой модели на основе самообучения с подкреплением, безусловно, может быть легко ис - пользован многократно. резюме В этой главе мы сначала подробно рассмотрели шаги настройки компью- тера Raspberry Pi с учетом всех необходимых аксессуаров и операционной си-стемы, а также инструментария GoPiGo, который превращает материнскую плату Raspberry Pi в движущегося робота. Затем мы рассмотрели вопрос уста-новки платформы TensorFlow на компьютер Raspberry Pi и сборки библиотеки TensorFlow, а также задачу интегрирования речевого воспроизведения текста (TTS) с классификацией изображений и распознавания звуковых команд с API GoPiGo, в результате чего мы получили робота Raspberry Pi, который движется, видит, слушает и говорит, без использования облачных интерфейсов. Нако-нец, мы представили инструментарий OpenAI Gym для самообучения с под-креплением и показали, как построить и натренировать мощную нейросете-вую модель на основе самообучения с подкреплением, используя платформу TensorFlow, которая способна поддерживать равновесие вашего робота в симу - лируемой среде.\n--- Страница 384 ---\nПослесловие Настало время попрощаться. В этой книге мы начали с трех предварительно натренированных моделей TensorFlow для классифицирования изображений, обнаружения объектов и нейронного переноса стиля и подробно обсудили во-прос вторичной тренировки моделей и их использования в приложениях для iOS и Android. Затем мы рассмотрели три интересные модели из руководств TensorFlow, построенных с использованием языка Python, – распознавание звука, аннотирование изображений и распознавание набросков – и показали, как вторично тренировать и запускать эти модели на мобильном устройстве. После этого мы разработали RNN-модели с нуля для предсказания биржевой цены в TensorFlow и Keras, две GAN-модели для распознавания цифр и транс - ляции пикселов и AlphaZero-подобную модель для игры «Четыре в ряд», вместе с приложениями для iOS и Android, во всех этих случаях используя платфор-му TensorFlow. Затем мы рассмотрели возможность использования платфор-мы TensorFlow Lite, а также платформы Core ML компании Apple со стандарт - ными машинно-обучающимися моделями и конвертированными моделями TensorFlow, показывая их потенциал, а также их пределы. Наконец, мы иссле-довали задачу построения робота Raspberry Pi с использованием платформы TensorFlow, который движется, видит, слушает, говорит и учится на основе мощного алгоритма самообучения с подкреплением. Мы показали приложения на языках Objective-C и Swift для iOS с исполь- зованием модуля TensorFlow, а также библиотеки TensorFlow ручной сборки и приложения для Android с использованием готовой к применению библио-теки TensorFlow, а также библиотеки ручной сборки, предназначенной для ис - правления всевозможных ошибок, которые могут возникнуть при развертыва-нии и запуске моделей TensorFlow на мобильном устройстве. Мы рассмотрели очень много тем, и одновременно с этим еще осталось так много нераскрытого. Новые версии платформы TensorFlow выходят с большой скоростью. Также быстро становятся доступными новые модели TensorFlow, в которых реализованы последние научные разработки. Основная цель этой книги – показать вам достаточный объем работоспособных приложений для iOS и Android, в которых задействованы всевозможные виды интеллектуаль-ных моделей TensorFlow, а также практические советы по устранению и отлад-ке неполадок, которые помогут вам быстро разворачивать и запускать свои любимые модели TensorFlow на мобильных устройствах в составе ваших сног - сшибательных мобильных приложений на основе ИИ. Если вы хотите построить свою собственную замечательную модель, ис - пользуя платформу TensorFlow или библиотеку Keras с реализацией алгорит - мов и сетей, которые будоражат ваше воображение больше всего, то после прочтения книги вам придется продолжить свое самообучение, поскольку мы\n--- Страница 385 ---\nПослесловие  383 не показали вам во всех подробностях, как это сделать, но надо надеяться, что мы мотивировали вас в достаточной мере, чтобы вы начали свое одиночное путешествие с полученной из книги уверенностью, что, как только у вас полу - чится построить и натренировать модели, вы уже точно будете знать, каким образом можно быстро развернуть и запустить их на мобильном устройстве, в любое время и в любом месте. Что касается того, какое путешествие предпринять и какие задачи ИИ ре- шать, то совет Яна Гудфеллоу в его интервью Эндрю Нг, вероятно, говорит луч-ше всего: спросите себя, что будет для вас лучше предпринять в дальнейшем и какой путь лучше для вас подходит: самообучение с подкреплением, не-контролируемое обучение или генеративно-состязательная сеть. Независимо от того, что вы выберете, это будет отличный путь, полный волнений и, ко-нечно же, подкрепленный тяжелой работой, а навыки, которые вы получили из книги, будут похожи на ваш смартфон, готовый быть полезным для вас в лю-бое время и ожидающий от вас, что вы сделаете свое милое, умное и крошеч-ное устройство еще милее и умнее.\n--- Страница 386 ---\nКниги издательства «ДМК Пресс» можно заказать в торгово-издательском холдинге «Планета Альянс» наложенным платежом, выслав открытку или письмо по почтовому адресу: 115487, г. Москва, 2-й Нагатинский пр-д, д. 6А. При оформлении заказа следует указать адрес (полностью), по которому должны быть высланы книги; фамилию, имя и отчество получателя. Желательно также указать свой телефон и электронный адрес. Эти книги вы можете заказать и в интернет-магазине: www.a-planeta.ru. Оптовые закупки: тел. (499) 782-38-89 . Электронный адрес: books@alians-kniga.ru. Джефф Танг Умные мобильные проекты с T ensorflow 10+ приложений искусственного интеллекта, построенных с помощью платформ TensorFlow Mobile и TensorFlow Lite для iOS, Android и Raspberry Pi Главный редактор Мовчан Д. А. dmkpress@gmail.com Перевод Денисов Д., Липова А., Мукосеев А. Корректор Синяева Г. И. Верстка Арифулин Г. Р. Дизайн обложки Мовчан А. Г. Формат 70×90 1/16. Гарнитура «PT Serif». Печать офсетная. Усл. печ. л. 28,08. Тираж 200 экз. Веб-сайт издательства: www.dmkpress.com",
      "debug": {
        "start_page": 352,
        "end_page": 386
      }
    }
  ]
}
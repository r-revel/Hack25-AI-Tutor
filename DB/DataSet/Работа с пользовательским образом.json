{
  "title": "Работа с пользовательским образом",
  "chapters": [
    {
      "name": "Работа с пользовательским образом",
      "content": "Практические руководства Evolution    \n\n # Работа с пользовательским образом   Эта статья полезна?          \nС помощью этого руководства вы научитесь обрабатывать данные, применяя пользовательский образ Spark.\nВы примените пользовательский образ, включающий библиотеки для работы с Object Storage и библиотеку NumPy.\nДля обработки данных вы используете скрипт, который объединит информацию о заказах из двух таблиц в единую витрину данных, найдет среднюю стоимость заказа и подсчитает разницу с ней для каждого заказа.\nВы будете использовать следующие сервисы:\n- Managed Spark — сервис, который позволяет развернуть кластерное вычислительное решение на основе Apache Spark для распределенной обработки данных.\n- Object Storage — сервис для хранения данных любого типа и объема. Будет использоваться в качестве хранилища для скриптов.\n- Artifact Registry —  сервис для хранения и распространения артефактов.\nШаги:\n1. Подготовьте файлы с данными.\n2. Подготовьте скрипт задачи.\n3. Подготовьте образ в Artifact Registry.\n4. Создайте задачу Managed Spark.\n5. Наблюдайте за ходом выполнения задачи.\n\n## Перед началом работы\n1. Зарегистрируйтесь в личном кабинете Cloud.ru. Если вы уже зарегистрированы, войдите под своей учетной записью.\n2. Создайте бакет Object Storage, в котором будут храниться необходимые файлы и логи.\n3. Настройте DNS-сервер и подсеть.\n4. Создайте кластер Data Platform, в котором будет размещен инстанс.\n5. Скачайте и установите root-сертификат на устройство.\n6. Создайте пароль и добавьте его в Secret Manager. Этот секрет станет паролем для доступа к интерфейсу Managed Spark.\n7. Создайте инстанс Managed Spark.\n8. Создайте реестр Artifact Registry, в котором будет храниться пользовательский образ Managed Spark.\n\n## 1. Подготовьте файлы с данными\nНа этом шаге вы загрузите в хранилище Object Storage файлы с данными для обработки.\n1. Скачайте CSV-таблицы client-spark-image.csv и sales-spark-image.csv: нажмите Скачать в правом верхнем углу.\n2. В ранее созданном бакете Object Storage создайте папку input.\n3. Загрузите CSV-таблицы в папку input.\n\n## 2. Подготовьте скрипт задачи\nНа этом шаге вы загрузите в хранилище Object Storage файл, содержащий скрипт для обработки данных из CSV-таблицы.\n1. Скопируйте скрипт и назовите файл script-spark-image.py.\n\n```\nimport numpy as npimport time\nfrom pyspark.sql import SparkSessionfrom pyspark.sql.types import FloatTypefrom pyspark.sql.functions import lit, udf\nbucket_name = 'your-bucket-name'\nspark = (SparkSession.builder         .appName(\"sales\")         .getOrCreate()         )\n# Read the source data from csvdf_sales = spark.read \\.format(\"csv\") \\.option(\"header\", \"true\") \\.option(\"inferSchema\", \"true\") \\.option(\"delimiter\", \";\") \\.load(f\"s3a://{bucket_name}/input/sales-spark-image.csv\")\ndf_client = spark.read \\.format(\"csv\") \\.option(\"header\", \"true\") \\.option(\"inferSchema\", \"true\") \\.option(\"delimiter\", \";\") \\.load(f\"s3a://{bucket_name}/input/client-spark-image.csv\")\n# get average cost for all salesnp_arr = np.array(df_sales.select('sales').collect())avg = np.average(np_arr)print(f'Average cost: {avg}')# define UDF\n@udf(returnType=FloatType())def calc_diff_avg(avg, val):      return val - avg# Create result with sale price and diff between sale price and average price\ndf_result = df_sales \\.join(df_client, df_sales.order_number ==  df_client.order_number,\"inner\") \\.select( \\      df_client.order_number, \\      df_client.order_date, \\      df_client.phone, \\      df_client.address_line1, \\      df_client.address_line2, \\      df_client.city, \\      df_client.state, \\      df_client.postal_code, \\      df_client.country, \\      df_client.territory, \\      df_client.contact_last_name, \\      df_client.contact_first_name, \\      df_client.deal_size, \\      df_client.car, \\      df_sales.sales, \\      calc_diff_avg(lit(avg), df_sales.sales).alias(\"diff_with_avg\") \\)\n# Write the result to csv filedf_result.write.mode('overwrite').option(\"header\",\"true\").csv(f\"s3a://{bucket_name}/output/sales\")\n```\n2. В строке bucket_name = 'your-bucket-name' замените your-bucket-name на название бакета Object Storage.\n3. В ранее созданном бакете Object Storage создайте папку jobs.\n4. Загрузите скрипт в папку jobs.\nВ результате получится следующая структура бакета с файлами:\n- <bucket>\n- input\n- sales-spark-image.csv\n- client-spark-image.csv\n- jobs\n- script-spark-image.py\n\n## 3. Подготовьте образ в Artifact Registry\nНа этом шаге вы подготовите пользовательский образ Managed Spark и загрузите его в сервис Artifact Registry.\n1. Создайте Dockerfile для сборки образа.\n```\nFROM apache/spark:3.5.0-scala2.12-java11-python3-ubuntu\n# add S3 libsRUN curl https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar -o /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jarRUN curl https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar -o /opt/spark/jars/hadoop-aws-3.3.4.jar\nARG spark_uid=rootUSER ${spark_uid}\n# install compatible numpy versionRUN pip install numpy==1.21.6\n```\n2. Чтобы собрать образ, выполните команду:\n```\ndocker build . --tag <IMAGE-NAME>:<TAG> --platform linux/amd64\n```\n\nГде:\n- <IMAGE-NAME> — имя образа.\n- <TAG> — тег образа.\n3. Откройте сервис Artifact Registry.\n4. Создайте репозиторий.\n5. Загрузите образ.\n\n## 4. Создайте задачу Managed Spark\nНа этом шаге вы запустите задачу Managed Spark с использованием подготовленного скрипта.\nДля продолжения работы убедитесь, что статус инстанса Managed Spark изменился на «Готов».\n1. Перейдите в сервис Managed Spark.\n2. Откройте созданный ранее инстанс.\n3. Перейдите на вкладку Задачи.\n4. Нажмите Создать задачу.\n5. В блоке Общие параметры введите название задачи, например spark-image-sales.\n6. В блоке Образ:\n1. Выберите Пользовательский.\n2. Под полем URI образа нажмите Выбрать из реестра и выберите добавленный ранее образ.\n7. В блоке Скрипт приложения:\n\n- В поле Тип запускаемой задачи выберите Python.\n- В поле Путь к запускаемому файлу укажите путь к файлу script-spark-image.py.\nВ данном случае путь s3a://{bucket_name}/jobs/script-spark-image.py, где {bucket_name} — название созданного бакета Object Storage.\n8. Нажмите Создать.\nЗадача Managed Spark начнет выполняться и отобразится на странице инстанса во вкладке Задачи.\n\n## 5. Наблюдайте за ходом выполнения задачи\nНа этом шаге вы будете наблюдать за ходом выполнения задачи, просматривая информацию, поступающую в логи.\nВы можете посмотреть логи задачи, когда задача находится в статусах «Выполняется» и «Готово», то есть как в процессе выполнения, так и по завершению задачи.\n\n### Перейдите к логам\n1. В строке задачи нажмите  и выберите Перейти к логам.\n2. Используйте фильтр, чтобы найти логи, например, за определенное время.\n\n### Перейдите в Spark UI\n1. Откройте инстанс Managed Spark.\n2. Во вкладке Задачи нажмите Spark UI. В соседней вкладке откроется интерфейс Spark UI.\n3. Вернитесь в инстанс и откройте вкладку Информация.\n4. Скопируйте данные из блока Настройки доступа.\n5. Введите данные инстанса:\n- Username — значение поля Пользователь.\n- Password — значение секрета в поле Пароль.\nВ интерфейсе Spark UI вы найдете информацию о ходе выполнения задачи.\n\n## Результат\nКогда задача перейдет в статус «Выполнено», откройте файловый менеджер Object Storage.\nВ бакете появится новая папка output, в которой будет храниться сводная таблица данных.\nВы применили пользовательский образ Managed Spark  и скрипт для обработки данных и получили объединенную таблицу со всеми данными.\n\n  [© 2025 Cloud.ru](https://cloud.ru)",
      "debug": {
        "start_page": 1,
        "end_page": 1
      }
    }
  ]
}
{
  "title": "Создание бота для суммаризации чатов и каналов в Telegram на LangChain и Foundation Models",
  "chapters": [
    {
      "name": "Создание бота для суммаризации чатов и каналов в Telegram на LangChain и Foundation Models",
      "content": "Практические руководства Evolution    \n\n # Создание бота для суммаризации чатов и каналов в Telegram на LangChain и Foundation Models   Эта статья полезна?          \nС помощью этого руководства вы познакомитесь с проектом evo-foundation-models-tg-bot-lab — Telegram-ботом, который демонстрирует, как интегрировать языковую модель при помощи фреймворка LangChain и сервиса Foundation Models.\nБот автоматически логирует сообщения чатов и выполняет интеллектуальный анализ: составляет краткие изложения диалогов и извлекает из них задачи.\nВы будете использовать следующие сервисы:\n- Foundation Models — сервис для доступа к API популярных фундаментальных моделей машинного обучения с открытым исходным кодом.\n- Artifact Registry для хранения, совместного использования и управления Docker-образами и Helm-чартами.\n- Container Apps — сервис для запуска контейнерных приложений в облаке. Не требует знания Kubernetes и создания виртуальных машин.\n- Object Storage — объектное S3-хранилище с бесплатным хранением файлов, объемом до 15 ГБ.\n- Docker — система контейнеризации.\n- Telegram — чат-платформа.\n- LangChain — фреймворк для создания AI-ориентированных приложений.\nШаги:\n1. Клонируйте или скачайте репозиторий кода с GitHub.\n2. Ознакомьтесь с архитектурой кода и интеграции с AI-моделями.\n3. Соберите образ и присвойте тег.\n4. Загрузите Docker-образ в реестр.\n5. Зарегистрируйте Telegram-бота.\n6. Сгенерируйте API-ключ для доступа к Foundation Models.\n7. Создайте и запустите контейнер с чат-ботом.\n8. Создайте Object Storage и ключи доступа.\n9. Проверьте работоспособность развернутого чат-бота.\n\n## Перед началом работы\n1. Зарегистрируйтесь в личном кабинете Cloud.ru.\nЕсли вы уже зарегистрированы, войдите под своей учетной записью.\n2. Подготовьте среду Container Apps и Artifact Registry, если не сделали этого ранее.\n\n## 1. Клонируйте или скачайте репозиторий кода с GitHub\nКлонируйте или скачайте код из репозитория.\n```\ngit clone https://github.com/cloud-ru/evo-foundation-models-tg-bot-lab.git\n```\n\n## 2. Ознакомьтесь с архитектурой кода и интеграции с AI-моделями\n\n### Архитектура интеграции\nПроект использует модульную архитектуру с четким разделением ответственности:\n```\nchat_bot/├── assistant.py         # Main class for working with AI├── models/              # Pydantic models for type hinting│   ├── ai_config.py     # AI configuration│   ├── summary_response.py│   ├── task_extraction_response.py│   └── task.py├── prompts/             # Prompt templates│   ├── summary.txt│   └── task_extraction.txt└── formatter.py         # Message formatting\n```\n\n#### Модель конфигурации\n```\n# chat_bot/models/ai_config.pyfrom pydantic import BaseModel, Field, validator\nclass AIConfig(BaseModel):    \"\"\"Model for AI configuration.\"\"\"\n    api_key: str = Field(..., description=\"AI API key\")    model: str = Field(\"t-tech/T-pro-it-2.0\", description=\"AI model name\")    base_url: Optional[str] = Field(None, description=\"Custom AI base URL\")    temperature: float = Field(0.3, ge=0.0, le=2.0, description=\"Generation temperature\")    max_tokens: int = Field(500, gt=0, description=\"Maximum tokens for generation\")\n    @classmethod    @validator(\"api_key\")    def validate_api_key(cls, v: str) -> str:        \"\"\"Validate that API key is not empty.\"\"\"        if not v or not v.strip():            raise ValueError(\"api_key cannot be empty\")        return v.strip()\n```\n\n#### Инициализация LangChain модели и интеграция с Foundation Models\n```\n# chat_bot/assistant.pyfrom langchain_openai import ChatOpenAIfrom pydantic import SecretStr\ndef _init_llm(self) -> None:    \"\"\"Initialize the language model.\"\"\"    try:        # Initialize with required parameters        self.llm = ChatOpenAI(            api_key=SecretStr(self.config.api_key),            model=self.config.model,            temperature=self.config.temperature,            base_url=self.config.base_url,        )        logger.info(            f\"Initialized AI model: {self.config.model} \"            f\"(temp: {self.config.temperature}, max_tokens: {self.config.max_tokens})\"        )    except Exception as e:        logger.error(f\"Failed to initialize AI model: {e}\")        raise\n```\n\nКлючевые особенности:\n- Использование SecretStr для безопасного хранения API-ключа.\n- Валидация конфигурации через Pydantic.\n- Поддержка кастомных базовых URL для различных AI-провайдеров.\n- Настраиваемые параметры генерации (temperature, max_tokens).\n\n#### Загрузка промптов из файлов\n```\ndef _load_prompts(self) -> None:    \"\"\"Load prompt templates from files.\"\"\"    try:        prompts_dir = Path(__file__).parent / \"prompts\"        summary_prompt_file = prompts_dir / \"summary.txt\"        task_extraction_prompt_file = prompts_dir / \"task_extraction.txt\"\n        # Load summary prompt        if summary_prompt_file.exists():            with open(summary_prompt_file, \"r\", encoding=\"utf-8\") as f:                summary_template = f.read()\n            self.summary_prompt = ChatPromptTemplate.from_template(summary_template)            logger.info(\"Loaded summary prompt from file\")        else:            # Fallback to default prompt            self.summary_prompt = ChatPromptTemplate.from_template(                \"You are an assistant for creating brief chat summaries. \"                \"Please provide your response in Russian.\\n\\n{messages}\\n\\n\"                \"Create a brief summary in Russian.\"            )    except Exception as e:        logger.error(f\"Failed to load prompts: {e}\")        # Fallback to default prompts\n```\n\n#### Пример промпта для создания кратких изложений\n```\n# chat_bot/prompts/summary.txtYou are an assistant for creating brief chat summaries.\nYour task is to analyze messages from a chat and create a brief but informative summary in Russian.\nThe summary should include:- Main discussion topics- Key points- Number of participants- Overall tone of the conversation\nBe concise but informative.Use telegram emojis for better readability. You can add max one emoji.Don't change or translate names, use exact name provided.A name consists of the First Name and Last Name. Don't show patronymic in the assignee name.Use bullets for main discussion topics formatting.Use line breaks for identation formatting.Format message for easy reading in telegram.\nYou will provide your response in a structured format with two fields:1. \"thoughts\" - Your reasoning process and analysis of the messages (in Russian)2. \"summary\" - The final Russian summary, formatted for Telegram\nHere are the chat messages:{messages}\nAnalyze the messages and provide your thoughts and summary in Russian.\n```\n\nПреимущества такого подхода:\n- Промпты хранятся отдельно от кода.\n- Легко редактировать и версионировать.\n- Поддержка fallback промптов.\n- Четкие инструкции для AI модели.\n\n#### Модели для структурированного вывода\n```\n# chat_bot/models/summary_response.pyclass SummaryOutput(BaseModel):    \"\"\"    Structured output schema for summary generation from chat messages.\n    This model is used with LangChain's structured output feature to ensure    the AI model returns properly formatted summary data.    \"\"\"\n    thoughts: str = Field(        ...,        description=\"The AI's reasoning process and thoughts about the messages before creating the summary. This should be in Russian.\",    )    summary: str = Field(        ...,        description=\"The actual summary of the chat messages. This should be concise and in Russian.\",    )\n# chat_bot/models/task_extraction_response.pyclass TaskExtractionOutput(BaseModel):    \"\"\"    Structured output schema for task extraction from chat messages.    \"\"\"\n    tasks: List[Task] = Field(        default_factory=list,        description=\"List of tasks extracted from the chat messages. If no tasks are found, return an empty list.\",    )\nclass Task(BaseModel):    \"\"\"Represents a task extracted from chat messages.\"\"\"\n    assignee: str = Field(..., description=\"The person assigned to the task\")    title: str = Field(..., description=\"The title/description of the task\")    deadline: Optional[datetime] = Field(        None, description=\"Optional deadline date/time for the task\"    )\n```\n\n#### Использование структурированного вывода\n```\nasync def summarize(self, messages_input: Union[str, Dict[str, Any], MessagesData]) -> SummaryResponse:    \"\"\"Summarize messages using LangChain's structured output.\"\"\"    import time    start_time = time.time()\n    try:        # Handle different input types        if isinstance(messages_input, str):            data = json.loads(messages_input)            messages_data = MessagesData(**data)        elif isinstance(messages_input, dict):            messages_data = MessagesData(**messages_input)        elif isinstance(messages_input, MessagesData):            messages_data = messages_input        else:            raise ValueError(\"Input must be either a JSON string, dictionary, or MessagesData object\")\n        # Format messages for summarization        formatted_messages = MessageFormatter.format_messages_for_summary(messages_data)\n        # Create the prompt        prompt = self.summary_prompt.format(messages=formatted_messages)\n        # Create model with structured output        model_with_structure = self.llm.with_structured_output(SummaryOutput)\n        # Generate summary response using structured output        structured_output: SummaryOutput = await model_with_structure.ainvoke(prompt)\n        processing_time = time.time() - start_time        logger.info(\"Successfully generated summary\")\n        return SummaryResponse(            summary=structured_output.summary,            success=True,            error_message=None,            processing_time=processing_time,        )    except Exception as e:        logger.error(f\"Failed to generate summary: {e}\")        return SummaryResponse(            summary=\"\",            success=False,            error_message=f\"Ошибка при создании сводки: {str(e)}\",            processing_time=time.time() - start_time,        )\n```\n\nКлючевые преимущества структурированного вывода:\n- Гарантированная типизация ответов.\n- Валидация данных через Pydantic.\n- Предсказуемый формат ответов.\n- Упрощенная обработка результатов.\n\n## 3. Соберите образ и присвойте тег\nПеред сборкой образа, убедитесь, что Docker Desktop запущен и пользователь авторизован в приложении.\nСоберите образ и присвойте тег, используя команду:\n\n```\ndocker build -t evo-foundation-models-tg-bot-lab .docker tag evo-foundation-models-tg-bot-lab <registry-name>.cr.cloud.ru/evo-foundation-models-tg-bot-lab:latest\n```\n\nГде <registry-name> — имя реестра, созданного при подготовке среды.\n\n## 4. Загрузите Docker-образ в реестр\n1. Загрузите образ в реестр Artifact Registry, выполнив команду:\n```\ndocker push <registry-name>.cr.cloud.ru/evo-foundation-models-tg-bot-lab:latest\n```\n\nГде <registry-name> — имя реестра, созданного при подготовке среды.\n2. В личном кабинете перейдите в сервис Artifact Registry и убедитесь, что образ загружен.\n\n## 5. Зарегистрируйте Telegram-бота\n1. В Telegram найдите BotFather.\n2. Выполните команду /newbot.\n3. Задайте название (name) и имя пользователя (username) для бота.\nИмя пользователя должно оканчиваться на ...Bot или ..._bot.\nНапример:\n- name — new-bot\n- username — botforlabbot\nВ результате вы получите токен.\nСохраните его — он потребуется на следующих этапах.\n4. С помощью команды /setuserpic установите иконку для вашего бота.\n\n## 6. Сгенерируйте API-ключ для доступа к Foundation Models\n1. На верхней панели слева нажмите  и перейдите в раздел Пользователи, на вкладку Сервисные аккаунты.\n2. Нажмите на название сервисного аккаунта, который будете использовать для отправки запроса к модели.\n3. Перейдите на вкладку API-ключи.\n4. Нажмите Создать API-ключ.\n5. Введите название и описание API-ключа, которое поможет в будущем идентифицировать его среди других ключей.\n6. Заполните параметры API-ключа:\n- Сервисы — Foundation Models.\n- Время действия — срок действия API-ключа и часовой пояс.\nВы можете установить значение от одного дня до одного года с текущей даты.\nЕсли параметр не задан, срок действия ключа устанавливается на максимальное значение — один год.\nС целью повышения уровня безопасности рекомендуется выставлять средние значения, например 90 дней.\n- Интервал работы ключа — один или несколько интервалов времени, в которые можно использовать API-ключ.\n7. Нажмите Создать.\n8. Сохраните Key Secret.\nПосле закрытия окна получить его будет нельзя.\nСозданный API-ключ появится в списке ключей в статусе «Активен».\nПодробнее о работе с API-ключом.\n\n## 7. Создайте Object Storage и ключи доступа\n1. Создайте бакет в Object Storage со следующими параметрами:\n- Название: tg-bot-lab\n- Глобальное название: tg-bot-lab\n- Класс хранения по умолчанию: Стандартный\n- Максимальный размер: 10 ГБ\n2. Перейдите в раздел Object Storage API.\nСохраните значения ID тенанта и Регион.\n3. Убедитесь, что в личном кабинете на странице сервиса Object Storage отображается бакет tg-bot-lab.\n4. Создайте сервисный аккаунт пользователя со следующими параметрами:\n- Название: tg-bot-lab-object-storage\n- Описание: Аккаунт пользователя Object Storage\n- Проект: Пользователь сервисов\n- Сервисы: оставьте список пустым\n- Evolution Object Storage Роли: s3e.viewer, s3e.editor\n5. Сгенерируйте ключи доступа для сервисного аккаунта.\n6. Сохраните Secret ID и Secret Key для обоих ключей.\n\n## 8. Создайте и запустите контейнер\n1. Перейдите в сервис Container Apps через меню в левом верхнем углу экрана.\n2. Нажмите Создать.\n3. Заполните поля и активируйте опции:\n1. Название контейнера — глобально уникальное имя, на базе которого формируется адрес вашего приложения в домене \\*.containers.cloud.ru.\n2. URI образа — выберите образ, загруженный в Artifact Registry на шаге 4.\n3. Порт контейнера — порт контейнера, который должен совпадать с портом вашего приложения.\nВ этой лабораторной работе мы используем порт 8080.\n4. vCPU/RAM — количество vCPU и RAM, которые выделяются для каждого экземпляра контейнера при обработке вызова.\nВыберите минимальную конфигурацию.\n5. Минимальное и Максимальное количество экземпляров при масштабировании сервиса.\nУстановите минимальное и максимальное количество экземпляров в значении 1, чтобы приложение всегда оставалось активным.\n6. Переменные — добавьте следующие переменные:\n\n- TELEGRAM_BOT_TOKEN — токен Telegram-бота, полученный на шаге 5\n- AI_API_KEY — токен сервиса Foundation Models, полученный на шаге 6\n- AI_MODEL — название AI-модели для нашего сервиса.\nИспользуйте значение RefalMachine/RuadaptQwen2.5-32B-Pro-Beta\n- AI_BASE_URL — https://foundation-models.api.cloud.ru/v1/\n- AI_TEMPERATURE — 0.5\n- AI_MAX_TOKENS — 1000\n- OBJECT_STORAGE_BUCKET_NAME — tg-bot-lab.\nНазвание бакета, созданного на шаге 7.\n- OBJECT_STORAGE_ACCESS_KEY_ID — ключ для доступа к бакету Object Storage, полученный на шаге 7\n- OBJECT_STORAGE_SECRET_ACCESS_KEY — секрет для доступа к бакету Object Storage, полученный на шаге 7\n- OBJECT_STORAGE_REGION — ru-central-1\n- OBJECT_STORAGE_ROOT_DIR — chat_logs\n- OBJECT_STORAGE_ENDPOINT_URL — https://s3.cloud.ru\n7. Активируйте опцию Автоматическое развертывание, чтобы каждый раз после загрузки в Artifact Registry новой версии образа на стороне Container Apps автоматически создавалась новая ревизия контейнера.\n4. Нажмите Создать.\nКонтейнер будет запущен в течение нескольких секунд.\n5. Дождитесь, когда контейнер и ревизия перейдут в статус «Выполняется».\n\n## 9. Проверьте работоспособность развернутого чат-бота\n1. Добавьте чат-бота в закрытый канал или чат в Telegram с ролью администратор.\n2. Напишите несколько сообщений в канал или чат.\n3. Выполните команду /summary.\nДождитесь ответа от чат-бота с суммаризацией вашей переписки.\n4. Выполните команду /tasks.\nДождитесь ответа от чат-бота со списком задач.\n\n## Результат\nВ ходе выполнения практической работы вы получили практический опыт интеграции LLM-моделей из сервиса Foundation Models в Telegram-экосистему, освоили приемы безопасной работы с ключами и конфигурацией, а также убедились, что сервис Foundation Models существенно упрощает создание production-ready AI-сервисов.\n\n  [© 2025 Cloud.ru](https://cloud.ru)",
      "debug": {
        "start_page": 1,
        "end_page": 1
      }
    }
  ]
}